



















































Multi-style Generative Reading Comprehension


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2273–2284
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2273

Multi-Style Generative Reading Comprehension

Kyosuke Nishida1, Itsumi Saito1, Kosuke Nishida1,
Kazutoshi Shinoda2∗, Atsushi Otsuka1, Hisako Asano1, Junji Tomita1

1NTT Media Intelligence Laboratory, NTT Corporation 2The University of Tokyo
kyosuke.nishida@acm.org

Abstract

This study tackles generative reading compre-
hension (RC), which consists of answering
questions based on textual evidence and nat-
ural language generation (NLG). We propose
a multi-style abstractive summarization model
for question answering, called Masque. The
proposed model has two key characteristics.
First, unlike most studies on RC that have fo-
cused on extracting an answer span from the
provided passages, our model instead focuses
on generating a summary from the question
and multiple passages. This serves to cover
various answer styles required for real-world
applications. Second, whereas previous stud-
ies built a specific model for each answer style
because of the difficulty of acquiring one gen-
eral model, our approach learns multi-style an-
swers within a model to improve the NLG ca-
pability for all styles involved. This also en-
ables our model to give an answer in the tar-
get style. Experiments show that our model
achieves state-of-the-art performance on the
Q&A task and the Q&A + NLG task of MS
MARCO 2.1 and the summary task of Nar-
rativeQA. We observe that the transfer of the
style-independent NLG capability to the target
style is the key to its success.

1 Introduction

Question answering has been a long-standing re-
search problem. Recently, reading comprehension
(RC), a challenge to answer a question given tex-
tual evidence provided in a document set, has re-
ceived much attention. Current mainstream stud-
ies have treated RC as a process of extracting an
answer span from one passage (Rajpurkar et al.,
2016, 2018) or multiple passages (Joshi et al.,
2017; Yang et al., 2018), which is usually done
by predicting the start and end positions of the an-
swer (Yu et al., 2018; Devlin et al., 2018).

∗Work done during an internship at NTT.

0

1

0

1

10 weeks </s>

it  takes 10 weeks to  get  new york state tax refund .    </s>

Question: “how long to get nys tax refund”

Generate from Voc.
Copy from Question
Copy from Passages

M
ix

tu
re

 w
e

ig
h

ts

[NLG]

[Q&A]

Figure 1: Visualization of how our model generates
an answer on MS MARCO. Given an answer style
(top: NLG, bottom: Q&A), the model controls the
mixture of three distributions for generating words
from a vocabulary and copying words from the ques-
tion and multiple passages at each decoding step.

The demand for answering questions in natural
language is increasing rapidly, and this has led to
the development of smart devices such as Alexa.
In comparison with answer span extraction, how-
ever, the natural language generation (NLG) capa-
bility for RC has been less studied. While datasets
such as MS MARCO (Bajaj et al., 2018) and Nar-
rativeQA (Kociský et al., 2018) have been pro-
posed for providing abstractive answers, the state-
of-the-art methods for these datasets are based on
answer span extraction (Wu et al., 2018; Hu et al.,
2018). Generative models suffer from a dearth of
training data to cover open-domain questions.

Moreover, to satisfy various information needs,
intelligent agents should be capable of answer-
ing one question in multiple styles, such as well-
formed sentences, which make sense even without
the context of the question and passages, and con-
cise phrases. These capabilities complement each
other, but previous studies cannot use and control
different styles within a model.

In this study, we propose Masque, a genera-
tive model for multi-passage RC. It achieves state-
of-the-art performance on the Q&A task and the
Q&A + NLG task of MS MARCO 2.1 and the
summary task of NarrativeQA. The main contri-



2274

butions of this study are as follows.

Multi-source abstractive summarization. We
introduce the pointer-generator mechanism (See
et al., 2017) for generating an abstractive answer
from the question and multiple passages, which
covers various answer styles. We extend the mech-
anism to a Transformer (Vaswani et al., 2017)
based one that allows words to be generated from
a vocabulary and to be copied from the question
and passages.

Multi-style learning for style control and trans-
fer. We introduce multi-style learning that en-
ables our model to control answer styles and im-
proves RC for all styles involved. We also ex-
tend the pointer-generator to a conditional decoder
by introducing an artificial token corresponding to
each style, as in (Johnson et al., 2017). For each
decoding step, it controls the mixture weights over
three distributions with the given style (Figure 1).

2 Problem Formulation

This paper considers the following task:

PROBLEM 1. Given a question with J words xq =
{xq1, . . . , x

q
J}, a set of K passages, where the

k-th passage is composed of L words xpk =
{xpk1 , . . . , x

pk
L }, and an answer style label s, an RC

model outputs an answer y = {y1, . . . , yT } condi-
tioned on the style.

In short, given a 3-tuple (xq, {xpk}, s), the sys-
tem predicts P (y). The training data is a set of
6-tuples: (xq, {xpk}, s, y, a, {rpk}), where a and
{rpk} are optional. Here, a is 1 if the question is
answerable with the provided passages and 0 oth-
erwise, and rpk is 1 if the k-th passage is required
to formulate the answer and 0 otherwise.

3 Proposed Model

We propose a Multi-style Abstractive Summa-
rization model for QUEstion answering, called
Masque. Masque directly models the conditional
probability p(y|xq, {xpk}, s). As shown in Fig-
ure 2, it consists of the following modules.

1. The question-passages reader (§3.1) models
interactions between the question and passages.

2. The passage ranker (§3.2) finds passages rele-
vant to the question.

3. The answer possibility classifier (§3.3) identi-
fies answerable questions.

Masked 
Multi-Head 
Attention 

Shared
Encoder

Block 

Shared
Encoder

Block 

Shared
Encoder

Block 

Modeling
Encoder

Block 

Modeling
Encoder

Block 

Modeling 
Encoder

Block 

Multi-Head 
Attention 

Multi-Head 
Attention 

Add & Norm

Feed 
Forward 

Add & Norm

Highway 

Glove&ELMo 

Add & Norm
Dual Attention

Concat

Add & Norm

Highway 

Glove&ELMo 

Highway 

Glove&ELMo 

Highway 

Glove&ELMo 

Additive 
Attention 

Additive 
Attention 

Combined 
Attention 

Multi-source Pointer-Generator

Passage 1 Passage K Question Style + Answer (Shiftted) 

8x

3x

5x 2x

Answer Possibility Answer Word Sequence 

Passage Ranker

Classifier

query 

query 

query 

query 

Decoder

Reader

Relevance 

Figure 2: Masque model architecture.

4. The answer sentence decoder (§3.4) outputs
an answer sentence conditioned on the target style.

Our model is based on multi-source abstractive
summarization: the answer that it generates can be
viewed as a summary from the question and pas-
sages. The model also learns multi-style answers
together. With these two characteristics, we aim
to acquire the style-independent NLG ability and
transfer it to the target style. In addition, to im-
prove natural language understanding in the reader
module, our model considers RC, passage rank-
ing, and answer possibility classification together
as multi-task learning.

3.1 Question-Passages Reader

The reader module is shared among multiple an-
swer styles and the three task-specific modules.

3.1.1 Word Embedding Layer
Let xq and xpk represent one-hot vectors (of size
V ) for words in the question and the k-th pas-
sage. First, this layer projects each of the vec-
tors to a dword-dimensional vector with a pre-
trained weight matrix W e ∈ Rdword×V such as
GloVe (Pennington et al., 2014). Next, it uses con-
textualized word representations via ELMo (Pe-
ters et al., 2018), which allows our model to use
morphological clues to form robust representa-
tions for out-of-vocabulary words unseen in train-
ing. Then, the concatenation of the word and con-



2275

textualized vectors is passed to a two-layer high-
way network (Srivastava et al., 2015) to fuse the
two types of embeddings, as in (Seo et al., 2017).
The highway network is shared by the question
and passages.

3.1.2 Shared Encoder Layer
This layer uses a stack of Transformer blocks,
which are shared by the question and passages,
on top of the embeddings provided by the word
embedding layer. The input of the first block is
immediately mapped to a d-dimensional vector
by a linear transformation. The outputs of this
layer are Epk ∈ Rd×L for each k-th passage, and
Eq ∈ Rd×J for the question.

Transformer encoder block. The block con-
sists of two sub-layers: a self-attention layer and
a position-wise feed-forward network. For the
self-attention layer, we adopt the multi-head atten-
tion mechanism (Vaswani et al., 2017). Following
GPT (Radford et al., 2018), the feed-forward net-
work consists of two linear transformations with a
GELU (Hendrycks and Gimpel, 2016) activation
function in between. Each sub-layer is placed in-
side a residual block (He et al., 2016). For an in-
put x and a given sub-layer function f , the output
is LN(f(x) + x), where LN indicates the layer
normalization (Ba et al., 2016). To facilitate these
residual connections, all sub-layers produce a se-
quence of d-dimensional vectors. Note that our
model does not use any position embeddings in
this block because ELMo gives the positional in-
formation of the words in each sequence.

3.1.3 Dual Attention Layer
This layer uses a dual attention mechanism to fuse
information from the question to the passages as
well as from the passages to the question.

It first computes a similarity matrix Upk ∈
RL×J between the question and the k-th passage,
as done in (Seo et al., 2017), where

Upklj = w
a>[Epkl ;E

q
j ;E

pk
l � E

q
j ]

indicates the similarity between the l-th word of
the k-th passage and the j-th question word. The
wa ∈ R3d are learnable parameters. The �
operator denotes the Hadamard product, and the
[; ] operator denotes vector concatenation across
the rows. Next, the layer obtains the row and
column normalized similarity matrices Apk =
softmaxj(U

pk>) and Bpk = softmaxl(Upk). It

then uses DCN (Xiong et al., 2017) to obtain dual
attention representations, Gq→pk ∈ R5d×L and
Gp→q ∈ R5d×J :

Gq→pk = [Epk ; Āpk ; ¯̄Apk ;Epk � Āpk ;Epk � ¯̄Apk ]
Gp→q = [Eq; B̄; ¯̄B;Eq � B̄;Eq � ¯̄B].

Here, Āpk = EqApk , B̄pk = EpkBpk , ¯̄Apk =
B̄pkApk , ¯̄Bpk = ĀpkBpk , B̄ = maxk(B̄pk), and
¯̄B = maxk(

¯̄Bpk).

3.1.4 Modeling Encoder Layer
This layer uses a stack of the Transformer en-
coder blocks for question representations and ob-
tains M q ∈ Rd×J from Gp→q. It also uses an-
other stack for passage representations and obtains
Mpk ∈ Rd×L from Gq→pk for each k-th pas-
sage. The outputs of this layer, M q and {Mpk},
are passed on to the answer sentence decoder; the
{Mpk} are also passed on to the passage ranker
and the answer possibility classifier.

3.2 Passage Ranker

The ranker maps the output of the modeling layer,
{Mpk}, to the relevance score of each passage. It
takes the output for the first word, Mpk1 , which
corresponds to the beginning-of-sentence token, to
obtain the aggregate representation of each pas-
sage sequence. Given wr ∈ Rd as learnable pa-
rameters, it calculates the relevance of each k-th
passage to the question as

βpk = sigmoid(wr>Mpk1 ).

3.3 Answer Possibility Classifier

The classifier maps the output of the modeling
layer to a probability for the answer possibility. It
also takes the output for the first word,Mpk1 , for all
passages and concatenates them. Givenwc ∈ RKd
as learnable parameters, it calculates the answer
possibility for the question as

P (a) = sigmoid(wc>[Mp11 ; . . . ;M
pK
1 ]).

3.4 Answer Sentence Decoder

Given the outputs provided by the reader mod-
ule, the decoder generates a sequence of an-
swer words one element at a time. It is auto-
regressive (Graves, 2013), consuming the previ-
ously generated words as additional input at each
decoding step.



2276

3.4.1 Word Embedding Layer
Let y represent one-hot vectors of the words in the
answer. This layer has the same components as
the word embedding layer of the reader module,
except that it uses a unidirectional ELMo to ensure
that the predictions for position t depend only on
the known outputs at positions previous to t.

Artificial tokens. To be able to use multiple an-
swer styles within a single system, our model in-
troduces an artificial token corresponding to the
style at the beginning of the answer (y1), as done
in (Johnson et al., 2017; Takeno et al., 2017). At
test time, the user can specify the first token to
control the style. This modification does not re-
quire any changes to the model architecture. Note
that introducing the token at the decoder prevents
the reader module from depending on the answer
style.

3.4.2 Attentional Decoder Layer
This layer uses a stack of Transformer decoder
blocks on top of the embeddings provided by the
word embedding layer. The input is immedi-
ately mapped to a d-dimensional vector by a lin-
ear transformation, and the output is a sequence of
d-dimensional vectors: {s1, . . . , sT }.

Transformer decoder block. In addition to the
encoder block, this block consists of the second
and third sub-layers after the self-attention block
and before the feed-forward network, as shown in
Figure 2. As in (Vaswani et al., 2017), the self-
attention sub-layer uses a sub-sequent mask to pre-
vent positions from attending to subsequent posi-
tions. The second and third sub-layers perform the
multi-head attention over M q and Mpall , respec-
tively. The Mpall is the concatenated outputs of
the encoder stack for the passages,

Mpall = [Mp1 , . . . ,MpK ] ∈ Rd×KL.

Here, the [, ] operator denotes vector concatenation
across the columns. This attention for the concate-
nated passages produces attention weights that are
comparable between passages.

3.4.3 Multi-source Pointer-Generator
Our extended mechanism allows both words to
be generated from a vocabulary and words to be
copied from both the question and multiple pas-
sages (Figure 3). We expect that the capability
of copying words will be shared among answer
styles.

Additive 
Attention 

Additive 
Attention 

Final distribution 

Mixing weights

Feed- 
Forward 

Feed- 
Forward 

Context vec.Attention 

weights 

Voc. dist. 

Passage 
Representations 

Question 
Representations 

Decoder 
t-th state 

Attention 

dist. 

query
key,Êvalue 

Figure 3: Multi-source pointer-generator mechanism.
For each decoding step t, mixture weights λv, λq, λp

for the probability of generating words from the vo-
cabulary and copying words from the question and the
passages are calculated. The three distributions are
weighted and summed to obtain the final distribution.

Extended vocabulary distribution. Let the ex-
tended vocabulary, Vext, be the union of the com-
mon words (a small subset of the full vocabulary,
V , defined by the input-side word embedding ma-
trix) and all words appearing in the input question
and passages. P v then denotes the probability dis-
tribution of the t-th answer word, yt, over the ex-
tended vocabulary. It is defined as:

P v(yt) = softmax(W
2>(W 1st + b

1)),

where the output embedding W 2 ∈ Rdword×Vext is
tied with the corresponding part of the input em-
bedding (Inan et al., 2017), and W 1 ∈ Rdword×d
and b1 ∈ Rdword are learnable parameters. P v(yt)
is zero if yt is an out-of-vocabulary word for V .

Copy distributions. A recent Transformer-
based pointer-generator randomly chooses one of
the attention-heads to form a copy distribution;
that approach gave no significant improvements in
text summarization (Gehrmann et al., 2018).

In contrast, our model uses an additional atten-
tion layer for each copy distribution on top of the
decoder stack. For the passages, the layer takes st
as the query and outputs αpt ∈ RKL as the atten-
tion weights and cpt ∈ Rd as the context vectors:

epkl = w
p> tanh(W pmMpkl +W

psst + b
p),

αpt = softmax([e
p1 ; . . . ; epK ]), (1)

cpt =
∑

l α
p
tlM

pall
l ,

where wp, bp ∈ Rd and W pm,W ps ∈ Rd×d are
learnable parameters. For the question, our model



2277

uses another identical layer and obtains αqt ∈ RJ
and cqt ∈ Rd. As a result, P q and P p are the copy
distributions over the extended vocabulary:

P q(yt) =
∑

j:xqj=yt
αqtj ,

P p(yt) =
∑

l:x
pk(l)
l =yt

αptl,

where k(l) means the passage index correspond-
ing to the l-th word in the concatenated passages.

Final distribution. The final distribution of yt is
defined as a mixture of the three distributions:

P (yt) = λ
vP v(yt) + λ

qP q(yt) + λ
pP p(yt),

λv, λq, λp = softmax(Wm[st; c
q
t ; c

p
t ] + b

m),

where Wm ∈ R3×3d and bm ∈ R3 are learnable
parameters.

3.4.4 Combined Attention
In order not to attend words in irrelevant passages,
our model introduces a combined attention. While
the original technique combined word and sen-
tence level attentions (Hsu et al., 2018), our model
combines the word and passage level attentions.
The word attention, Eq. 1, is re-defined as

αptl =
αptlβ

pk(l)∑
l′ α

p
tl′β

pk(l′)
.

3.5 Loss Function
We define the training loss as the sum of losses via

L(θ) = Ldec + γrankLrank + γclsLcls

where θ is the set of all learnable parameters, and
γrank and γcls are balancing parameters.

The loss of the decoder, Ldec, is the negative
log likelihood of the whole target answer sentence
averaged over Nable answerable examples:

Ldec = −
1

Nable

∑
(a,y)∈D

a

T

∑
t

logP (yt),

where D is the training dataset. The losses of the
passage ranker, Lrank, and the answer possibility
classifier, Lcls, are the binary cross entropy be-
tween the true and predicted values averaged over
all N examples:

Lrank = −
1

NK

∑
k

∑
rpk∈D

(
rpk log βpk+

(1− rpk ) log(1− βpk )

)
,

Lcls = −
1

N

∑
a∈D

(
a logP (a)+

(1− a) log(1− P (a))

)
.

Dataset Subset Train Dev. Eval.
ALL 808,731 101,093 101,092

MS MARCO ANS 503,370 55,636 –
NLG 153,725 12,467 –

NarrativeQA Summary 32,747 3,461 10,557

Table 1: Numbers of questions used in the experiments.

4 Experiments on MS MARCO 2.1

We evaluated our model on MS MARCO 2.1 (Ba-
jaj et al., 2018). It is the sole dataset providing ab-
stractive answers with multiple styles and serves
as a great test bed for building open-domain QA
agents with the NLG capability that can be used in
smart devices. The details of our setup and output
examples are in the supplementary material.

4.1 Setup
Datasets. MS MARCO 2.1 provides two tasks
for generative open-domain QA: the Q&A task
and the Q&A + Natural Language Generation
(NLG) task. Both tasks consist of questions sub-
mitted to Bing by real users, and each question
refers to ten passages. The dataset also includes
annotations on the relevant passages, which were
selected by humans to form the final answers, and
on whether there was no answer in the passages.

Answer styles. We associated the two tasks with
two answer styles. The NLG task requires a well-
formed answer that is an abstractive summary of
the question and passages, averaging 16.6 words.
The Q&A task also requires an abstractive answer
but prefers it to be more concise than in the NLG
task, averaging 13.1 words, and many of the an-
swers do not contain the context of the question.
For the question “tablespoon in cup”, a reference
answer in the Q&A task is “16,” while that in the
NLG task is “There are 16 tablespoons in a cup.”

Subsets. In addition to the ALL dataset, we pre-
pared two subsets for ablation tests as listed in Ta-
ble 1. The ANS set consisted of answerable ques-
tions, and the NLG set consisted of the answerable
questions and well-formed answers, so that NLG
⊂ ANS ⊂ ALL. We note that multi-style learning
enables our model to learn from different answer
styles of data (i.e., the ANS set), and multi-task
learning with the answer possibility classifier en-
ables our model to learn from both answerable and
unanswerable data (i.e., the ALL set).

Training and Inference. We trained our model
with mini-batches consisting of multi-style an-



2278

NLG Q&A
Model R-L B-1 R-L B-1
BiDAFa 16.91 9.30 23.96 10.64
Deep Cascade QAb 35.14 37.35 52.01 54.64
S-Net+CES2Sc 45.04 40.62 44.96 46.36
BERT+Multi-PGNetd 47.37 45.09 48.14 52.03
Selector+CCGe 47.39 45.26 50.63 52.03
VNETf 48.37 46.75 51.63 54.37
Masque (NLG; single) 49.19 49.63 48.42 48.68
Masque (NLG; ensemble) 49.61 50.13 48.92 48.75
Masque (Q&A; single) 25.66 36.62 50.93 42.37
Masque (Q&A; ensemble) 28.53 39.87 52.20 43.77
Human Performance 63.21 53.03 53.87 48.50

Table 2: Performance of our and competing models on
the MS MARCO V2 leaderboard (4 March 2019). aSeo
et al. (2017); bYan et al. (2019); cShao (unpublished), a
variant of Tan et al. (2018); dLi (unpublished), a model
using Devlin et al. (2018) and See et al. (2017); eQian
(unpublished); fWu et al. (2018). Whether the compet-
ing models are ensemble models or not is unreported.

swers that were randomly sampled. We used a
greedy decoding algorithm and did not use any
beam search or random sampling, because they
did not provide any improvements.

Evaluation metrics and baselines. ROUGE-L
and BLEU-1 were used to evaluate the models’
RC performance, where ROUGE-L is the main
metric on the official leaderboard. We used the
reported scores of extractive (Seo et al., 2017; Yan
et al., 2019; Wu et al., 2018), generative (Tan et al.,
2018), and unpublished RC models at the submis-
sion time.

In addition, to evaluate the individual contribu-
tions of our modules, we used MAP and MRR for
the ranker and F1 for the classifier, where the pos-
itive class was the answerable questions.

4.2 Results

Does our model achieve state-of-the-art on the
two tasks with different styles? Table 2 shows
the performance of our model and competing
models on the leaderboard. Our ensemble model
of six training runs, where each model was trained
with the two answer styles, achieved state-of-the-
art performance on both tasks in terms of ROUGE-
L. In particular, for the NLG task, our single model
outperformed competing models in terms of both
ROUGE-L and BLEU-1.

Does multi-style learning improve the NLG
performance? Table 3 lists the results of an ab-
lation test for our single model (controlled with

Model Train R-L B-1
Masque (NLG style; single) ALL 69.77 65.56
w/o multi-style learning (§3.4.2) NLG 68.20 63.95
↪→ w/o Transformer (§3.1.2, §3.4.2) NLG 67.13 62.96
w/o passage ranker (§3.2) NLG 68.05 63.82
w/o possibility classifier (§3.3) ANS 69.64 65.41

Masque w/ gold passage ranker ALL 78.70 78.14

Table 3: Ablation test results on the NLG dev. set. The
models were trained with the subset listed in “Train”.

Model Train MAP MRR
Bing (initial ranking) - 34.62 35.00
Masque (single) ALL 69.51 69.96
w/o answer decoder (§3.4) ALL 67.03 67.49
w/o multi-style learning (§3.4.2) NLG 65.51 65.59
w/o possibility classifier (§3.3) ANS 69.08 69.54

Table 4: Passage ranking results on the ANS dev. set.

the NLG style) on the NLG dev. set1. Our model
trained with both styles outperformed the model
trained with the single NLG style. Multi-style
learning enabled our model to improve its NLG
performance by also using non-sentence answers.

Does the Transformer-based pointer-generator
improve the NLG performance? Table 3
shows that our model also outperformed the model
that used RNNs and self-attentions instead of
Transformer blocks as in MCAN (McCann et al.,
2018). Our deep decoder captured the multi-hop
interaction among the question, the passages, and
the answer better than a single-layer LSTM de-
coder could.

Does joint learning with the ranker and classi-
fier improve NLG performance? Furthermore,
Table 3 shows that our model (jointly trained with
the passage ranker and answer possibility classi-
fier) outperformed the model that did not use the
ranker and classifier. Joint learning thus had a reg-
ularization effect on the question-passages reader.

We also confirmed that the gold passage ranker,
which can perfectly predict the relevance of pas-
sages, significantly improved the RC performance.
Passage ranking will be a key to developing a sys-
tem that can outperform humans.

Does joint learning improve the passage rank-
ing performance? Table 4 lists the passage
ranking performance on the ANS dev. set2. The

1We confirmed with the organizer that the dev. results
were much better than the test results, but there was no prob-
lem.

2This evaluation requires our ranker to re-rank 10 pas-
sages. It is not the same as the Passage Re-ranking task.



2279

1.0

0.9

0.8

0.7

0.6

0.5

0.4

P
re
c
is
io
n

0.0 0.2 0.4 0.6 0.8 1.0
Recall

F1=0.9

0.7

0.50.30.1

Figure 4: Precision-recall curve for answer possibility
classification on the ALL dev. set.

0

5

10

15

20

25

30

yesno which when who where all how what other why

Prediction (Q&A)

Reference (Q&A)

Prediction (NLG)

Reference (NLG)

L
e

n
g

th

Figure 5: Lengths of answers generated by Masque
broken down by the answer style and query type on the
NLG dev. set. The error bars indicate standard errors.

ranker shares the question-passages reader with
the answer decoder, and this sharing contributed to
improvements over the ranker trained without the
answer decoder. Also, our ranker outperformed
the initial ranking provided by Bing by a signifi-
cant margin.

Does our model accurately identify answerable
questions? Figure 4 shows the precision-recall
curve for answer possibility classification on the
ALL dev. set. Our model identified the answer-
able questions well. The maximum F1 score was
0.7893, where the threshold of answer possibility
was 0.4411. This is the first report on answer pos-
sibility classification with MS MARCO 2.1.

Does our model control answer lengths with
different styles? Figure 5 shows the lengths of
the answers generated by our model broken down
by the answer style and query type. The generated
answers were relatively shorter than the reference
answers, especially for the Q&A task, but well
controlled with the target style for every query
type. The short answers degraded our model’s
BLEU scores in the Q&A task (Table 2) because
of BLEU’s brevity penalty (Papineni et al., 2002).

5 Experiments on NarrativeQA

Next, we evaluated our model on Narra-
tiveQA (Kociský et al., 2018). It requires under-
standing the underlying narrative rather than re-
lying on shallow pattern matching. Our detailed
setup and output examples are in the supplemen-
tary material.

5.1 Setup

We only describe the settings specific to this ex-
periment.

Datasets. Following previous studies, we used
the summary setting for the comparisons with the
reported baselines, where each question refers to
one summary (averaging 659 words), and there is
no unanswerable questions. Our model therefore
did not use the passage ranker and answer possi-
bility classifier.

Answer styles. The NarrativeQA dataset does
not explicitly provide multiple answer styles. In
order to evaluate the effectiveness of multi-style
learning, we used the NLG subset of MS MARCO
as additional training data. We associated the
NarrativeQA and NLG datasets with two answer
styles. The answer style of NarrativeQA (NQA) is
different from that of MS MARCO (NLG) in that
the answers are short (averaging 4.73 words) and
contained frequently pronouns. For instance, for
the question “Who is Mark Hunter?”, a reference
is “He is a high school student in Phoenix.”

Evaluation metrics and baselines. BLEU-1
and 4, METEOR, and ROUGE-L were used in
accordance with the evaluation in the dataset pa-
per (Kociský et al., 2018). We used the reports
of top-performing extractive (Seo et al., 2017; Tay
et al., 2018; Hu et al., 2018) and generative (Bauer
et al., 2018; Indurthi et al., 2018) models.

5.2 Results

Does our model achieve state-of-the-art perfor-
mance? Table 5 shows that our single model,
trained with two styles and controlled with the
NQA style, pushed forward the state-of-the-art by
a significant margin. The evaluation scores of the
model controlled with the NLG style were low be-
cause the two styles are different. Also, our model
without multi-style learning (trained with only the
NQA style) outperformed the baselines in terms of
ROUGE-L. This indicates that our model architec-



2280

Model B-1 B-4 M R-L
BiDAFa 33.72 15.53 15.38 36.30
DECAPROPb 42.00 23.42 23.42 40.07
MHPGM+NOICc 43.63 21.07 19.03 44.16
ConZNetd 42.76 22.49 19.24 46.67
RMR+A2De 50.4 26.5 N/A 53.3
Masque (NQA) 54.11 30.43 26.13 59.87

w/o multi-style learning 48.70 20.98 21.95 54.74
Masque (NLG) 39.14 18.11 24.62 50.09
Masque (NQA; valid.)f 52.78 28.72 25.38 58.94

Table 5: Performance of our and competing models on
the NarrativeQA test set. aSeo et al. (2017); bTay et al.
(2018); cBauer et al. (2018); dIndurthi et al. (2018);
eHu et al. (2018). fResults on the NarrativeQA valida-
tion set.

ture itself is powerful for natural language under-
standing in RC.

6 Related Work and Discussion

Transfer and multi-task learning in RC. Re-
cent breakthroughs in transfer learning demon-
strate that pre-trained language models perform
well on RC with minimal modifications (Peters
et al., 2018; Devlin et al., 2018; Radford et al.,
2018, 2019). In addition, our model also uses
ELMo (Peters et al., 2018) for contextualized em-
beddings.

Multi-task learning is a transfer mechanism
to improve generalization performance (Caruana,
1997), and it is generally applied by sharing
the hidden layers between all tasks, while keep-
ing task-specific layers. Wang et al. (2018) and
Nishida et al. (2018) reported that the sharing of
the hidden layers between the multi-passage RC
and passage ranking tasks was effective. Our re-
sults also showed the effectiveness of the sharing
of the question-passages reader module among the
RC, passage ranking, and answer possibility clas-
sification tasks.

In multi-task learning without task-specific lay-
ers, Devlin et al. (2018) and Chen et al. (2017)
improved RC performance by learning multiple
datasets from the same extractive RC setting. Mc-
Cann et al. (2018) and Yogatama et al. (2019) in-
vestigated multi-task and curriculum learning on
many different NLP tasks; their results were below
task-specific RC models. Our multi-style learning
does not use style-specific layers; instead uses a
style-conditional decoder.

Generative RC. S-Net (Tan et al., 2018) used
an extraction-then-synthesis mechanism for multi-

passage RC. The models proposed by McCann
et al. (2018), Bauer et al. (2018), and Indurthi
et al. (2018) used an RNN-based pointer-generator
mechanism for single-passage RC. Although these
mechanisms can alleviate the lack of training data,
large amounts of data are still required. Our multi-
style learning will be a key technique enabling
learning from many RC datasets with different
styles.

In addition to MS MARCO and NarrativeQA,
there are other datasets that provide abstractive
answers. DuReader (He et al., 2018), a Chinese
multi-document RC dataset, provides longer doc-
uments and answers than those of MS MARCO.
DuoRC (Saha et al., 2018) and CoQA (Reddy
et al., 2018) contain abstractive answers; most of
the answers are short phrases.

Controllable text generation. Many studies
have been carried out in the framework of style
transfer, which is the task of rephrasing a text so
that it contains specific styles such as sentiment.
Recent studies have used artificial tokens (Sen-
nrich et al., 2016; Johnson et al., 2017), varia-
tional auto-encoders (Hu et al., 2017), or adver-
sarial training (Fu et al., 2018; Tsvetkov et al.,
2018) to separate the content and style on the en-
coder side. On the decoder side, conditional lan-
guage modeling has been used to generate out-
put sentences with the target style. In addition,
output length control with conditional language
modeling has been well studied (Kikuchi et al.,
2016; Takeno et al., 2017; Fan et al., 2018). Our
style-controllable RC relies on conditional lan-
guage modeling in the decoder.

Multi-passage RC. The simplest approach is to
concatenate the passages and find the answer from
the concatenation, as in (Wang et al., 2017). Ear-
lier pipelined models found a small number of rel-
evant passages with a TF-IDF based ranker and
passed them to a neural reader (Chen et al., 2017;
Clark and Gardner, 2018), while more recent mod-
els have used a neural re-ranker to more accurately
select the relevant passages (Wang et al., 2018;
Nishida et al., 2018). Also, non-pipelined models
(including ours) consider all the provided passages
and find the answer by comparing scores between
passages (Tan et al., 2018; Wu et al., 2018). The
most recent models make a proper trade-off be-
tween efficiency and accuracy (Yan et al., 2019;
Min et al., 2018).



2281

RC with unanswerable question identification.
The previous work of (Levy et al., 2017; Clark and
Gardner, 2018) outputted a no-answer score de-
pending on the probability of all answer spans. Hu
et al. (2019) proposed an answer verifier to com-
pare an answer with the question. Sun et al. (2018)
jointly learned an RC model and an answer veri-
fier. Our model introduces a classifier on top of the
question-passages reader, which is not dependent
on the generated answer.

Abstractive summarization. Current state-of-
the-art models use the pointer-generator mecha-
nism (See et al., 2017). In particular, content se-
lection approaches, which decide what to sum-
marize, have recently been used with abstractive
models. Most methods select content at the sen-
tence level (Hsu et al., 2018; Chen and Bansal,
2018) or the word level (Pasunuru and Bansal,
2018; Li et al., 2018; Gehrmann et al., 2018). Our
model incorporates content selection at the pas-
sage level in the combined attention.

Query-based summarization has rarely been
studied because of a lack of datasets. Nema et al.
(2017) proposed an attentional encoder-decoder
model; however, Saha et al. (2018) reported that
it performed worse than BiDAF on DuoRC. Has-
selqvist et al. (2017) proposed a pointer-generator
based model; however, it does not consider copy-
ing words from the question.

7 Conclusion

This study sheds light on multi-style generative
RC. Our proposed model, Masque, is based on
multi-source abstractive summarization and learns
multi-style answers together. It achieved state-
of-the-art performance on the Q&A task and
the Q&A + NLG task of MS MARCO 2.1 and
the summary task of NarrativeQA. The key to
its success is transferring the style-independent
NLG capability to the target style by use of
the question-passages reader and the conditional
pointer-generator decoder. In particular, the capa-
bility of copying words from the question and pas-
sages can be shared among the styles, while the
capability of controlling the mixture weights for
the generative and copy distributions can be ac-
quired for each style. Our future work will involve
exploring the potential of our multi-style learning
towards natural language understanding.

References
Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.

2016. Layer normalization. Computing Research
Repository (CoRR), arXiv:1607.06450. Version 1.

Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,
Jianfeng Gao, Xiaodong Liu, Rangan Majumder,
Andrew McNamara, Bhaskar Mitra, Tri Nguyen,
Mir Rosenberg, Xia Song, Alina Stoica, Saurabh
Tiwary, and Tong Wang. 2018. MS MARCO: A
human generated machine reading comprehension
dataset. Computing Research Repository (CoRR),
arXiv:1611.09268. Version 3.

Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018.
Commonsense for generative multi-hop question an-
swering tasks. In Empirical Methods in Natural
Language Processing (EMNLP), pages 4220–4230.

Richard Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41–75.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. In Association for Computa-
tional Linguistics (ACL), pages 1870–1879.

Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-
tive summarization with reinforce-selected sentence
rewriting. In Association for Computational Lin-
guistics (ACL), pages 675–686.

Christopher Clark and Matt Gardner. 2018. Simple
and effective multi-paragraph reading comprehen-
sion. In Association for Computational Linguistics
(ACL), pages 845–855.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. Computing Research Repository (CoRR),
arXiv:1810.04805. Version 1.

Angela Fan, David Grangier, and Michael Auli. 2018.
Controllable abstractive summarization. In Work-
shop on Neural Machine Translation and Genera-
tion (NMT@ACL), pages 45–54.

Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan
Zhao, and Rui Yan. 2018. Style transfer in text:
Exploration and evaluation. In Association for the
Advancement of Artificial Intelligence (AAAI), pages
663–670.

Sebastian Gehrmann, Yuntian Deng, and Alexander M.
Rush. 2018. Bottom-up abstractive summarization.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 4098–4109.

Alex Graves. 2013. Generating sequences with recur-
rent neural networks. Computing Research Reposi-
tory (CoRR), arXiv:1308.0850. Version 5.

Johan Hasselqvist, Niklas Helmertz, and Mikael
Kågebäck. 2017. Query-based abstractive summa-
rization using neural networks. arXiv, 1712.06100.

https://www.aclweb.org/anthology/D18-1454
https://www.aclweb.org/anthology/D18-1454
http://aclweb.org/anthology/P17-1171
http://aclweb.org/anthology/P17-1171
https://www.aclweb.org/anthology/P18-1063
https://www.aclweb.org/anthology/P18-1063
https://www.aclweb.org/anthology/P18-1063
https://www.aclweb.org/anthology/P18-1078
https://www.aclweb.org/anthology/P18-1078
https://www.aclweb.org/anthology/P18-1078
https://www.aclweb.org/anthology/W18-2706
https://www.aclweb.org/anthology/D18-1443


2282

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Computer Vision and Pattern Recognition
(CVPR), pages 770–778.

Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao,
Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu,
Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng
Wang. 2018. DuReader: a chinese machine read-
ing comprehension dataset from real-world applica-
tions. In Workshop on Machine Reading for Ques-
tion Answering (MRQA@ACL), pages 37–46.

Dan Hendrycks and Kevin Gimpel. 2016. Bridging
nonlinearities and stochastic regularizers with gaus-
sian error linear units. Computing Research Reposi-
tory (CoRR), arXiv:1606.08415. Version 2.

Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui
Min, Jing Tang, and Min Sun. 2018. A unified
model for extractive and abstractive summarization
using inconsistency loss. In Association for Compu-
tational Linguistics (ACL), pages 132–141.

Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang,
Dongsheng Li, Nan Yang, and Ming Zhou. 2018.
Attention-guided answer distillation for machine
reading comprehension. In Empirical Methods
in Natural Language Processing (EMNLP), pages
2077–2086.

Minghao Hu, Furu Wei, Yuxing Peng, Zhen Huang,
Nan Yang, and Ming Zhou. 2019. Read + Verify:
Machine reading comprehension with unanswerable
questions. In Association for the Advancement of
Artificial Intelligence (AAAI).

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P. Xing. 2017. Toward con-
trolled generation of text. In International Con-
ference on Machine Learning (ICML), pages 1587–
1596.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying word vectors and word classifiers: A
loss framework for language modeling. In Inter-
national Conference on Learning Representations
(ICLR).

Sathish Reddy Indurthi, Seunghak Yu, Seohyun Back,
and Heriberto Cuayáhuitl. 2018. Cut to the chase:
A context zoom-in network for reading comprehen-
sion. In Empirical Methods in Natural Language
Processing (EMNLP), pages 570–575.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Viégas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. 2017.
Google’s multilingual neural machine translation
system: Enabling zero-shot translation. Transac-
tions of the Association for Computational Linguis-
tic (TACL), 5:339–351.

Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com-
prehension. In Association for Computational Lin-
guistics (ACL), pages 1601–1611.

Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya
Takamura, and Manabu Okumura. 2016. Control-
ling output length in neural encoder-decoders. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1328–1338.

Tomás Kociský, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, Gábor Melis, and
Edward Grefenstette. 2018. The NarrativeQA read-
ing comprehension challenge. Transactions of the
Association for Computational Linguistic (TACL),
6:317–328.

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Computational Natural
Language Learning (CoNLL), pages 333–342.

Chenliang Li, Weiran Xu, Si Li, and Sheng Gao.
2018. Guiding generation for abstractive text sum-
marization based on key information guide network.
In North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT), pages 55–60.

Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,
and Richard Socher. 2018. The natural language
decathlon: Multitask learning as question answer-
ing. Computing Research Repository (CoRR),
arXiv:1806.08730. Version 1.

Sewon Min, Victor Zhong, Richard Socher, and Caim-
ing Xiong. 2018. Efficient and robust question an-
swering from minimal context over documents. In
Association for Computational Linguistics (ACL),
pages 1725–1735.

Preksha Nema, Mitesh M. Khapra, Anirban Laha, and
Balaraman Ravindran. 2017. Diversity driven atten-
tion model for query-based abstractive summariza-
tion. In Association for Computational Linguistics
(ACL), pages 1063–1072.

Kyosuke Nishida, Itsumi Saito, Atsushi Otsuka, Hisako
Asano, and Junji Tomita. 2018. Retrieve-and-
read: Multi-task learning of information retrieval
and reading comprehension. In Conference on
Information and Knowledge Management (CIKM),
pages 647–656.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Association for
Computational Linguistics (ACL), pages 311–318.

Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-
reward reinforced summarization with saliency and
entailment. In North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), pages 646–653.

https://www.aclweb.org/anthology/W18-2605
https://www.aclweb.org/anthology/W18-2605
https://www.aclweb.org/anthology/W18-2605
https://www.aclweb.org/anthology/P18-1013
https://www.aclweb.org/anthology/P18-1013
https://www.aclweb.org/anthology/P18-1013
https://www.aclweb.org/anthology/D18-1232
https://www.aclweb.org/anthology/D18-1232
https://www.aclweb.org/anthology/D18-1054
https://www.aclweb.org/anthology/D18-1054
https://www.aclweb.org/anthology/D18-1054
https://www.aclweb.org/anthology/Q17-1024
https://www.aclweb.org/anthology/Q17-1024
http://aclweb.org/anthology/P17-1147
http://aclweb.org/anthology/P17-1147
http://aclweb.org/anthology/P17-1147
https://www.aclweb.org/anthology/D16-1140
https://www.aclweb.org/anthology/D16-1140
https://www.aclweb.org/anthology/Q18-1023
https://www.aclweb.org/anthology/Q18-1023
https://www.aclweb.org/anthology/K17-1034
https://www.aclweb.org/anthology/K17-1034
https://www.aclweb.org/anthology/N18-2009
https://www.aclweb.org/anthology/N18-2009
https://www.aclweb.org/anthology/P18-1160
https://www.aclweb.org/anthology/P18-1160
https://www.aclweb.org/anthology/P17-1098
https://www.aclweb.org/anthology/P17-1098
https://www.aclweb.org/anthology/P17-1098
https://www.aclweb.org/anthology/P02-1040
https://www.aclweb.org/anthology/P02-1040
https://www.aclweb.org/anthology/N18-2102
https://www.aclweb.org/anthology/N18-2102
https://www.aclweb.org/anthology/N18-2102


2283

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), pages 2227–
2237.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. Technical re-
port, OpenAI.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. Techni-
cal report, OpenAI.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for SQuAD. In Association for Computational
Linguistics (ACL), pages 784–789.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
and Percy Liang. 2016. SQuAD: 100,000+ ques-
tions for machine comprehension of text. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 2383–2392.

Siva Reddy, Danqi Chen, and Christopher D. Manning.
2018. CoQA: A conversational question answering
challenge. Computing Research Repository (CoRR),
arXiv:1808.07042. Version 1.

Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and
Karthik Sankaranarayanan. 2018. DuoRC: Towards
complex language understanding with paraphrased
reading comprehension. In Association for Compu-
tational Linguistics (ACL), pages 1683–1693.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Association for Computa-
tional Linguistics (ACL), pages 1073–1083.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Controlling politeness in neural machine
translation via side constraints. In North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 35–40.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In International
Conference on Learning Representations (ICLR).

Rupesh Kumar Srivastava, Klaus Greff, and
Jürgen Schmidhuber. 2015. Highway net-
works. Computing Research Repository (CoRR),
arXiv:1505.00387. Version 2.

Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018.
U-Net: Machine reading comprehension with unan-
swerable questions. Computing Research Reposi-
tory (CoRR), arXiv:1810.06638. Version 1.

Shunsuke Takeno, Masaaki Nagata, and Kazuhide Ya-
mamoto. 2017. Controlling target features in neural
machine translation via prefix constraints. In Work-
shop on Asian Translation (WAT@IJCNLP), pages
55–63.

Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du,
Weifeng Lv, and Ming Zhou. 2018. S-Net: From
answer extraction to answer synthesis for machine
reading comprehension. In Association for the Ad-
vancement of Artificial Intelligence (AAAI), pages
5940–5947.

Yi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian
Su. 2018. Densely connected attention propagation
for reading comprehension. In Advances in Neural
Information Processing Systems (NeurIPS), pages
4911–4922.

Yulia Tsvetkov, Alan W. Black, Ruslan Salakhutdi-
nov, and Shrimai Prabhumoye. 2018. Style transfer
through back-translation. In Association for Com-
putational Linguistics (ACL), pages 866–876.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 6000–6010.

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerald
Tesauro, Bowen Zhou, and Jing Jiang. 2018. R3:
Reinforced reader-ranker for open-domain question
answering. In Association for the Advancement of
Artificial Intelligence (AAAI), pages 5981–5988.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Association for Computational Linguis-
tics (ACL), pages 189–198.

Hua Wu, Haifeng Wang, Sujian Li, Wei He, Yizhong
Wang, Jing Liu, Kai Liu, and Yajuan Lyu. 2018.
Multi-passage machine reading comprehension with
cross-passage answer verification. In Association
for Computational Linguistics (ACL), pages 1918–
1927.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for question
answering. In International Conference on Learn-
ing Representations (ICLR).

Ming Yan, Jiangnan Xia, Chen Wu, Bin Bi, Zhongzhou
Zhao, Ji Zhang, Luo Si, Rui Wang, Wei Wang, and
Haiqing Chen. 2019. A deep cascade model for
multi-document reading comprehension. In Associ-
ation for the Advancement of Artificial Intelligence
(AAAI).

http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://www.aclweb.org/anthology/N18-1202
https://www.aclweb.org/anthology/N18-1202
https://www.aclweb.org/anthology/P18-2124
https://www.aclweb.org/anthology/P18-2124
https://aclweb.org/anthology/D16-1264
https://aclweb.org/anthology/D16-1264
https://www.aclweb.org/anthology/P18-1156
https://www.aclweb.org/anthology/P18-1156
https://www.aclweb.org/anthology/P18-1156
https://www.aclweb.org/anthology/P17-1099
https://www.aclweb.org/anthology/P17-1099
https://www.aclweb.org/anthology/N16-1005
https://www.aclweb.org/anthology/N16-1005
https://www.aclweb.org/anthology/W17-5702
https://www.aclweb.org/anthology/W17-5702
https://www.aclweb.org/anthology/P18-1080
https://www.aclweb.org/anthology/P18-1080
https://www.aclweb.org/anthology/P17-1018
https://www.aclweb.org/anthology/P17-1018
https://www.aclweb.org/anthology/P17-1018
https://www.aclweb.org/anthology/P18-1178
https://www.aclweb.org/anthology/P18-1178


2284

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. HotpotQA: A
dataset for diverse, explainable multi-hop question
answering. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 2369–2380.

Dani Yogatama, Cyprien de Masson d’Autume, Jerome
Connor, Tomás Kociský, Mike Chrzanowski, Ling-
peng Kong, Angeliki Lazaridou, Wang Ling,
Lei Yu, Chris Dyer, and Phil Blunsom. 2019.
Learning and evaluating general linguistic intelli-
gence. Computing Research Repository (CoRR),
arXiv:1901.11373. Version 1.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V.
Le. 2018. QANet: Combining local convolution
with global self-attention for reading comprehen-
sion. In International Conference on Learning Rep-
resentations (ICLR).

https://www.aclweb.org/anthology/D18-1259
https://www.aclweb.org/anthology/D18-1259
https://www.aclweb.org/anthology/D18-1259

