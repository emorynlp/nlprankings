



















































Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Context Vector Disambiguation for Bilingual Lexicon Extraction from
Comparable Corpora

Dhouha Bouamor
CEA, LIST, Vision and

Content Engineering Laboratory,
91191 Gif-sur-Yvette CEDEX

France
dhouha.bouamor@cea.fr

Nasredine Semmar
CEA, LIST, Vision and Content

Engineering Laboratory,
91191 Gif-sur-Yvette

CEDEX France
nasredine.semmar@cea.fr

Pierre Zweigenbaum
LIMSI-CNRS,

F-91403 Orsay CEDEX
France

pz@limsi.fr

Abstract
This paper presents an approach that ex-
tends the standard approach used for bilin-
gual lexicon extraction from comparable
corpora. We focus on the unresolved prob-
lem of polysemous words revealed by the
bilingual dictionary and introduce a use of
a Word Sense Disambiguation process that
aims at improving the adequacy of con-
text vectors. On two specialized French-
English comparable corpora, empirical ex-
perimental results show that our method
improves the results obtained by two state-
of-the-art approaches.

1 Introduction

Over the years, bilingual lexicon extraction from
comparable corpora has attracted a wealth of re-
search works (Fung, 1998; Rapp, 1995; Chiao
and Zweigenbaum, 2003). The basic assumption
behind most studies is a distributional hypothe-
sis (Harris, 1954), which states that words with a
similar meaning are likely to appear in similar con-
texts across languages. The so-called standard ap-
proach to bilingual lexicon extraction from com-
parable corpora is based on the characterization
and comparison of context vectors of source and
target words. Each element in the context vector
of a source or target word represents its associa-
tion with a word which occurs within a window
of N words. To enable the comparison of source
and target vectors, words in the source vectors are
translated into the target language using an exist-
ing bilingual dictionary.

The core of the standard approach is the bilin-
gual dictionary. Its use is problematic when a word
has several translations, whether they are synony-
mous or polysemous. For instance, the French

word action can be translated into English as
share, stock, lawsuit or deed. In such cases, it
is difficult to identify in flat resources like bilin-
gual dictionaries which translations are most rel-
evant. The standard approach considers all avail-
able translations and gives them the same impor-
tance in the resulting translated context vectors in-
dependently of the domain of interest and word
ambiguity. Thus, in the financial domain, trans-
lating action into deed or lawsuit would introduce
noise in context vectors.

In this paper, we present a novel approach that
addresses the word polysemy problem neglected
in the standard approach. We introduce a Word
Sense Disambiguation (WSD) process that iden-
tifies the translations of polysemous words that
are more likely to give the best representation of
context vectors in the target language. For this
purpose, we employ five WordNet-based semantic
similarity and relatedness measures and use a data
fusion method that merges the results obtained by
each measure. We test our approach on two spe-
cialized French-English comparable corpora (fi-
nancial and medical) and report improved results
compared to two state-of-the-art approaches.

2 Related Work

Most previous works addressing the task of bilin-
gual lexicon extraction from comparable corpora
are based on the standard approach. In order to
improve the results of this approach, recent re-
searches based on the assumption that more the
context vectors are representative, better is the
bilingual lexicon extraction were conducted. In
these works, additional linguistic resources such
as specialized dictionaries (Chiao and Zweigen-
baum, 2002) or transliterated words (Prochasson
et al., 2009) were combined with the bilingual dic-

759



tionary to translate context vectors. Few works
have however focused on the ambiguity problem
revealed by the seed bilingual dictionary. (Hazem
and Morin, 2012) propose a method that filters the
entries of the bilingual dictionary on the base of
a POS-Tagging and a domain relevance measure
criteria but no improvements have been demon-
strated. Gaussier et al. (2004) attempted to solve
the problem of word ambiguities in the source and
target languages. They investigated a number of
techniques including canonical correlation analy-
sis and multilingual probabilistic latent semantic
analysis. The best results, with an improvement of
the F-Measure (+0.02 at Top20) were reported for
a mixed method. Recently, (Morin and Prochas-
son, 2011) proceed as the standard approach but
weigh the different translations according to their
frequency in the target corpus. Here, we propose a
method that differs from Gaussier et al. (2004) in
this way: If they focus on words ambiguities on
source and target languages, we thought that it
would be sufficient to disambiguate only trans-
lated source context vectors.

3 Context Vector Disambiguation

3.1 Semantic similarity measures
A large number of WSD techniques were pro-
posed in the literature. The most widely used ones
are those that compute semantic similarity1 with
the help of WordNet. WordNet has been used in
many tasks relying on word-based similarity, in-
cluding document (Hwang et al., 2011) and im-
age (Cho et al., 2007; Choi et al., 2012) retrieval
systems. In this work, we use it to derive a se-
mantic similarity between lexical units within the
same context vector. To the best of our knowledge,
this is the first application of WordNet to bilingual
lexicon extraction from comparable corpora.

Among semantic similarity measures using
WordNet, we distinguish: (1) measures based on
path length which simply counts the distance be-
tween two words in the WordNet taxonomy, (2)
measures relying on information content in which
a semantically annotated corpus is needed to com-
pute frequencies of words to be compared and (3)
the ones using gloss overlap which are designed
to compute semantic relatedness. In this work,
we use five similarity measures and compare
their performances. These measures include three

1For consiseness, we often use “semantic similarity” to
refer collectively to both similarity and relatedness.

path-based semantic similarity measures denoted
PATH,WUP (Wu and Palmer, 1994) and LEA-
COCK (Leacock and Chodorow, 1998). PATH is
a baseline that is equal to the inverse of the short-
est path between two words. WUP finds the depth
of the least common subsumer of the words, and
scales that by the sum of the depths of individual
words. The depth of a word is its distance to the
root node. LEACOCK finds the shortest path be-
tween two words, and scales that by the maximum
path length found in the is–a hierarchy in which
they occur. Path length measures have the advan-
tage of being independent of corpus statistics, and
therefor uninfluenced by sparse data.

Since semantic relatedness is considered to be
more general than semantic similarity, we also
use two relatedness measures: LESK (Banerjee
and Pedersen, 2002) and VECTOR (Patwardhan,
2003). LESK finds overlaps between the glosses
of word pairs, as well as words’ hyponyms. VEC-
TOR creates a co-occurrence matrix for each gloss
token. Each gloss is then represented as a vector
that averages token co-occurrences.

3.2 Disambiguation process

Once translated into the target language, the con-
text vectors disambiguation process intervenes.
This process operates locally on each context vec-
tor and aims at finding the most prominent trans-
lations of polysemous words. For this purpose,
we use monosemic words as a seed set of dis-
ambiguated words to infer the polysemous word’s
translations senses. We hypothesize that a word is
monosemic if it is associated to only one entry in
the bilingual dictionary. We checked this assump-
tion by probing monosemic entries of the bilingual
dictionary against WordNet and found that 95% of
the entries are monosemic in both resources. Ac-
cording to the above-described semantic similarity
measures, a similarity value SimV alue is derived
between all the translations provided for each pol-
ysemous word by the bilingual dictionary and all
monosemic words appearing within the same con-
text vector. In practice, since a word can belong to
more than one synset2 in WordNet, the semantic
similarity between two words w1 and w2 is defined
as the maximum of SimV alue between the synset
or the synsets that include the synsets(w1) and

2a group of a synonymous words in WordNet

760



synsets(w2) according to the following equation:

SemSim(w1, w2) = max{SimV alue(s1, s2);
(s1, s2) ∈ synsets(w1)× synsets(w2)} (1)

Then, to identify the most prominent transla-
tions of each polysemous unit wp, an average sim-
ilarity is computed for each translation wjp of wp:

Ave Sim(wjp) =
1

N

NX
i=1

SemSim(wi, w
j
p) (2)

where N is the total number of monosemic words
in each context vector and SemSim is the simi-
larity value of wjp and the ith monosemic word.
Hence, according to average similarity values
Ave Sim(wjp), we obtain for each polysemous
word wp an ordered list of translations w1p . . . w

n
p .

4 Experiments and Results

4.1 Resources and Experimental Setup

We conducted our experiments on two French-
English comparable corpora specialized on the
corporate finance and the breast cancer sub-
domains. Both corpora were extracted from
Wikipedia3. We consider the domain topic in
the source language (for instance cancer du sein
[breast cancer]) as a query to Wikipedia and
extract all its sub-topics (i.e., sub-categories in
Wikipedia) to construct a domain-specific cate-
gories tree. Then we collected all articles belong-
ing to one of these categories and used inter-
language links to build the comparable corpus.
Both corpora have been normalized through the
following linguistic preprocessing steps: tokeni-
sation, part-of-speech tagging, lemmatisation and
function words removal. The resulting corpora4

sizes as well as their polysemy rate PR are given
in Table 1. The polysemy rate indicates how much
words in the comparable corpora are associated
to more than one translation in the seed bilingual
dictionary. The dictionary consists of an in-house
bilingual dictionary which contains about 120,000
entries belonging to the general language with an
average of 7 translations per entry.

In bilingual terminology extraction from com-
parable corpora, a reference list is required to
evaluate the performance of the alignment. Such
lists are often composed of about 100 single

3http://dumps.wikimedia.org/
4Comparable corpora will be shared publicly

Corpus French English PR
Corporate finance 402.486 756.840 41%

Breast cancer 396.524 524.805 47%

Table 1: Comparable corpora sizes in term of
words and polysemy rates (PR) associated to each
corpus

terms (Hazem and Morin, 2012; Chiao and
Zweigenbaum, 2002). Here, we created two ref-
erence lists5 for the corporate finance and the
breast cancer sub-domains. The first list is com-
posed of 125 single terms extracted from the glos-
sary of bilingual micro-finance terms6. The second
list contains 79 terms extracted from the French-
English MESH and the UMLS thesauri7. Note
that reference terms pairs appear more than five
times in each part of both comparable corpora.

Three other parameters need to be set up,
namely the window size, the association measure
and the similarity measure. We followed (Laroche
and Langlais, 2010) to define these parame-
ters. They carried out a complete study of the
influence of these parameters on the bilingual
alignment. The context vectors were defined by
computing the Discounted Log-Odds Ratio (equa-
tion 3) between words occurring in the same con-
text window of size 7.

Odds-Ratiodisc = log
(O11 +

1
2
)(O22 +

1
2
)

(O12 +
1
2
)(O21 +

1
2
)

(3)

where Oij are the cells of the 2 × 2 contingency
matrix of a token s co-occurring with the term S
within a given window size. As similarity mea-
sure, we chose to use the cosine measure.

4.2 Results of bilingual lexicon extraction
To evaluate the performance of our approach, we
used both the standard approach (SA) and the ap-
proach proposed by (Morin and Prochasson, 2011)
(henceforth MP11) as baselines. The experiments
were performed with respect to the five semantic
similarity measures described in section 3.1. Each
measure provides, for each polysemous word, a
ranked list of translations. A question that arises
here is whether we should introduce only the top-
ranked translation into the context vector or con-
sider a larger number of translations, mainly when
a translation list contains synonyms. For this

5Reference lists will be shared publicly
6http://www.microfinance.lu/en/
7http://www.nlm.nih.gov/

761



a)
C

or
po

ra
te

Fi
na

nc
e

Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7
Standard Approach (SA) 0.172

MP11 0.336
Si

ng
le

m
ea

su
re

WUP 0.241 0.284 0.301 0.275 0.258 0.215 0.224
PATH 0.250 0.284 0.301 0.284 0.258 0.215 0.215

LEACOCK 0.250 0.293 0.301 0.275 0.275 0.241 0.232
LESK 0.272 0.293 0.293 0.275 0.258 0.250 0.215

VECTOR 0.267 0.310 0.284 0.284 0.232 0.232 0.232
CONDORCETMerge 0.362 0.379 0.353 0.362 0.336 0.275 0.267

b)
B

re
as

tC
an

ce
r

Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7
Standard Approach (SA) 0.493

MP11 0.553

Si
ng

le
m

ea
su

re

WUP 0.481 0.566 0.566 0.542 0.554 0.542 0.554
PATH 0.542 0.542 0.554 0.566 0.578 0.554 0.554

LEACOCK 0.506 0.578 0.554 0.566 0.542 0.554 0.542
LESK 0.469 0.542 0.542 0.590 0.554 0.554 0.542

VECTOR 0.518 0.566 0.530 0.566 0.542 0.566 0.554
CONDORCETMerge 0.566 0.614 0.600 0.590 0.600 0.578 0.578

Table 2: F-Measure at Top20 for the two domains; MP11 = (Morin and Prochasson, 2011). In each
column, italics shows best single similarity measure, bold shows best result. Underline shows best result
overall.

reason, we take into account in our experiments
different numbers of translations, noted WN-Ti,
ranging from the pivot translation (i = 1) to the
seventh word in the translation list. This choice is
motivated by the fact that words in both corpora
have on average 7 translations in the bilingual dic-
tionary. Both baseline systems use all translations
associated to each entry in the bilingual dictionary.
The only difference is that in MP11 translations
are weighted according to their frequency in the
target corpus.

The results of different works focusing on bilin-
gual lexicon extraction from comparable corpora
are evaluated on the number of correct candidates
found in the first N first candidates output by the
alignment process (the TopN ). Here, we use the
Top20 F-measure as evaluation metric. The results
obtained for the corporate finance corpus are pre-
sented in Table 2a. The first notable observation is
that disambiguating context vectors using seman-
tic similarity measures outperforms the SA. The
highest F-measure is reported by VECTOR. Us-
ing the top two words (WN-T2) in context vec-
tors increases the F-measure from 0.172 to 0.310.
However, compared to MP11, no improvement
is achieved. Concerning the breast cancer cor-
pus, Table 2b shows improvements in most cases
over both the SA and MP11. The maximum F-

measure was obtained by LESK when for each
polysemous word up to four translations (WN-T4)
are considered in context vectors. This method
achieves an improvement of respectively +0.097
and +0.037% over SA and MP11.

Each of the tested 5 semantic similarity mea-
sures provides a different view of how to rank
the translations of a given test word. Combining
the obtained ranked lists should reinforce the con-
fidence in consensus translations, while decreas-
ing the confidence in non-consensus translations.
We have therefore tested their combination. For
this, we used a voting method, and chose one in
the Condorcet family the Condorcet data fusion
method. This method was widely used to combine
document retrieval results from information re-
trieval systems (Montague and Aslam, 2002; Nu-
ray and Can, 2006). It is a single-winner election
method that ranks the candidates in order of pref-
erence. It is a pairwise voting, i.e. it compares ev-
ery possible pair of candidates to decide the pref-
erence of them. A matrix can be used to present
the competition process. Every candidate appears
in the matrix as a row and a column as well. If
there are m candidates, then we need m2 elements
in the matrix in total. Initially 0 is written to all the
elements. If di is preferred to dj , then we add 1 to
the element at row i and column j (aij). The pro-

762



cess is repeated until all the ballots are processed.
For every element aij , if aij > m/2 , then di
beats dj ; if aij < m/2, then dj beats di; other-
wise (aij = m/2), there is a draw between di and
dj . The total score of each candidate is quantified
by summing the raw scores it obtains in all pair-
wise competitions. Finally the ranking is achiev-
able based on the total scores calculated.

Here, we view the ranking of the extraction re-
sults from different similarity measures as a spe-
cial instance of the voting problem where the
Top20 extraction results correspond to candidates
and different semantic similarity measures are the
voters. The combination method referred to as
CONDORCETMerge outperformed all the others
(see Tables 2a and 2b): (1) individual measures,
(2) SA, and (3) MP11. Even though the two cor-
pora are fairly different (subject and polysemy
rate), the optimal results are obtained when con-
sidering up to two most similar translations in con-
text vectors. This behavior shows that the fusion
method is robust to domain change. The addition
of supplementary translations, which are probably
noisy in the given domain, degrades the overall re-
sults. The F-measure gains with respect to SA are
+0.207 for corporate finance and +0.121 for the
breast cancer corpus. More interestingly, our ap-
proach outperforms MP11, showing that the role
of disambiguation is more important than that of
feature weighting.

5 Conclusion

We presented in this paper a novel method that
extends the standard approach used for bilingual
lexicon extraction. This method disambiguates
polysemous words in context vectors by selecting
only the most relevant translations. Five seman-
tic similarity and relatedness measures were used
for this purpose. Experiments conducted on two
specialized comparable corpora indicate that the
combination of similarity metrics leads to a better
performance than two state-of-the-art approaches.
This shows that the ambiguity present in special-
ized comparable corpora hampers bilingual lexi-
con extraction, and that methods such as the one
introduced here are needed. The obtained results
are very encouraging and can be improved in a
number of ways. First, we plan to mine much
larger specialized comparable corpora and focus
on their quality (Li and Gaussier, 2010). We also
plan to test our method on bilingual lexicon extrac-

tion from general-domain corpora, where ambigu-
ity is generally higher and disambiguation meth-
ods should be all the more needed.

References
Satanjeev Banerjee and Ted Pedersen. 2002. An

adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Proceedings of the Third In-
ternational Conference on Computational Linguis-
tics and Intelligent Text Processing, CICLing ’02,
pages 136–145, London, UK, UK. Springer-Verlag.

Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th international conference on Computational
linguistics - Volume 2, COLING ’02, pages 1–5. As-
sociation for Computational Linguistics.

Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The effect of a general lexicon in corpus-based iden-
tification of french-english medical word transla-
tions. In Proceedings Medical Informatics Europe,
volume 95 of Studies in Health Technology and In-
formatics, pages 397–402, Amsterdam.

Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,
and PanKoo Kim. 2007. Efficient image retrieval
using conceptualization of annotated images. Lec-
ture Notes in Computer Science, pages 426–433.
Springer.

Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-
won Hwang, and Pankoo Kim. 2012. A method for
enhancing image retrieval based on annotation using
modified wup similarity in wordnet. In Proceed-
ings of the 11th WSEAS international conference
on Artificial Intelligence, Knowledge Engineering
and Data Bases, AIKED’12, pages 83–87, Stevens
Point, Wisconsin, USA. World Scientific and Engi-
neering Academy and Society (WSEAS).

Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In Parallel Text Processing, pages
1–17. Springer.

Éric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Hervé Déjean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526–533.

Z.S. Harris. 1954. Distributional structure. Word.

Amir Hazem and Emmanuel Morin. 2012. Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora. In Proceedings, 8th interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey, May.

Myunggwon Hwang, Chang Choi, and Pankoo Kim.
2011. Automatic enrichment of semantic relation

763



network and its application to word sense disam-
biguation. IEEE Transactions on Knowledge and
Data Engineering, 23:845–858.

Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In 23rd
International Conference on Computational Lin-
guistics (Coling 2010), pages 617–625, Beijing,
China, Aug.

Claudia Leacock and Martin Chodorow, 1998. Com-
bining local context and WordNet similarity for word
sense identification, pages 305–332. In C. Fellbaum
(Ed.), MIT Press.

Bo Li and Ëric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In 23rd International Confer-
ence on Computational Linguistics (Coling 2010),
Beijing, China, Aug.

Mark Montague and Javed A. Aslam. 2002. Con-
dorcet fusion for improved retrieval. In Proceedings
of the eleventh international conference on Informa-
tion and knowledge management, CIKM ’02, pages
538–548, New York, NY, USA. ACM.

Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceed-
ings, 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), page 27–34, Portland, Ore-
gon, USA.

Rabia Nuray and Fazli Can. 2006. Automatic ranking
of information retrieval systems using data fusion.
Inf. Process. Manage., 42(3):595–614, May.

Siddharth Patwardhan. 2003. Incorporating Dictio-
nary and Corpus Information into a Context Vector
Measure of Semantic Relatedness. Master’s thesis,
University of Minnesota, Duluth, August.

Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Transla-
tion Summit (MT Summit XII), page 284–291, Ot-
tawa, Ontario, Canada.

Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ’95, pages 320–322. Association for
Computational Linguistics.

Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ’94, pages 133–138. Association
for Computational Linguistics.

764


