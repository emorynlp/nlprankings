



















































Chinese Grammatical Error Diagnosis by Conditional Random Fields


Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 7â€“14,
Beijing, China, July 31, 2015. cÂ©2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing

Chinese Grammatical Error Diagnosis by Conditional Random Fields
 

Po-Lin Chen, Shih-Hung Wu* 
Chaoyang University of Technology/ 

Wufeng, Taichung, Taiwan, ROC. 
streetcatsky@gmail.com, 

*contact author: shwu@cyut.edu.tw   

Liang-Pu Chen, Ping-Che Yang, Ren-Dar 
Yang   

IDEAS, Institute for Information Industry/ 
Taipei, Taiwan, ROC. 

{eit, maciaclark, rdyang}@iii.org.tw 

 
 

Abstract 
 

This paper reports how to build a Chinese 
Grammatical Error Diagnosis system based on 
the conditional random fields (CRF). The 
system can find four types of grammatical 
errors in learnersâ€™ essays. The four types or 
errors are redundant words, missing words, bad 
word selection, and disorder words. Our system 
presents the best false positive rate in 2015 
NLP-TEA-2 CGED shared task, and also the 
best precision rate in three diagnosis levels. 

1 Introduction 

Learning Chinese as foreign language is on the 
rising trend. Since Chinese has its own unique 
grammar, it is hard for a foreign learner to write 
a correct sentence. A computer system that can 
diagnose the grammatical errors will help the 
learners to learn Chinese fast (Yu et al., 2014; 
Wu et al., 2010;Yeh et al., 2014;Chang et al., 
2014).  

In the NLP-TEA-2 CGED shared task data set, 
there are four types of errors in the leanersâ€™ 
sentences: Redundant, Selection, Disorder, and 
Missing. The research goal is to build a system 
that can detect the errors, identify the type of the 
error, and point out the position of the error in 
the sentence.  

2 Methodology 

Our system is based on the conditional random 
field (CRF) (Lafferty, 2001). CRF has been used 
in many natural language processing applications, 
such as named entity recognition, word 
segmentation, information extraction, and 
parsing (Wu and Hsieh, 2012). For different task, 
it requires different feature set and different 
labeled training data. The CRF can be regarded 
as a sequential labeling tagger. Given a sequence 
data X, the CRF can generate the corresponding 
label sequence Y, based on the trained model. 
Each label Y is taken from a specific tag set, 

which needs to be defined in different task. How 
to define and interpret the label is a 
task-depended work for the developers. 
Mathematically, the model can be defined as: 
P(ğ‘Œğ‘Œ|ğ‘‹ğ‘‹) = 1

ğ‘ğ‘(ğ‘‹ğ‘‹)
exp(âˆ‘ ğœ†ğœ†ğ‘˜ğ‘˜ğ‘“ğ‘“ğ‘˜ğ‘˜ğ‘˜ğ‘˜ )         (1) 

where Z(X) is the normalization factor, fğ‘˜ğ‘˜ is a 
set of features, ğœ†ğœ†ğ‘˜ğ‘˜ is the corresponding weight. 
In this task, X is the input sentence, and Y is the 
corresponding error type label. We define the tag 
set as: {O, R, M, S, D}, corresponding to no 
error, redundant, missing, selection, and disorder 
respectively. Figure 1 shows a snapshot of our 
working file. The first column is the input 
sentence X, and the third column is the labeled 
tag sequence Y. Note that the second column is 
the Part-of-speech (POS) of the word in the first 
column. The combination of words and the POSs 
will be the features in our system. The POS set 
used in our system is listed in  
Table 1, which is a simplified POS set provided 
by CKIP1. 

Figure 2 (at the end of the paper) shows the 
framework of the proposed system. The system 
is built based on the CRF++, a linear-chain CRF 
model software, developed by Kudo2. 

Figure 1: A snapshot of our CRF sequential 
labeling working file 
                                                       
1 http://ckipsvr.iis.sinica.edu.tw/ 
2 http://crfpp.sourceforge.net/index.html 

å¯æ˜¯ C O 

æœ‰ Vt O 

ä¸€é» DET O 

å†· Vi O 

äº† T R 

 

ä½  N O 

çš„ T R 

éå¹´ Vi O 

å‘¢ T O 

7



Simplified 
CKIP POS 

 Corresponding CKIP 
POS 

A éè¬‚å½¢å®¹è© 

C å°ç­‰é€£æ¥è©ï¼Œå¦‚ï¼šå’Œã€è·Ÿ 
é—œè¯é€£æ¥è© 

POST 

é€£æ¥è©ï¼Œå¦‚ï¼šç­‰ç­‰ 
é€£æ¥è©ï¼Œå¦‚ï¼šçš„è©± 
å¾Œç½®æ•¸é‡å®šè© 

å¾Œç½®è© 

ADV 

æ•¸é‡å‰¯è© 
å‹•è©å‰ç¨‹åº¦å‰¯è© 
å‹•è©å¾Œç¨‹åº¦å‰¯è© 

å¥å‰¯è© 
å‰¯è© 

ASP æ™‚æ…‹æ¨™è¨˜ 

N 

æ™®é€šåè© 
å°ˆæœ‰åç¨± 
åœ°æ–¹è© 
ä½ç½®è© 
æ™‚é–“è© 
ä»£åè© 

DET 

æ•¸è©å®šè©. 
ç‰¹æŒ‡å®šè© 
æŒ‡ä»£å®šè© 
æ•¸é‡å®šè© 

M é‡è© 
Nv åç‰©åŒ–å‹•è© 

T 
æ„Ÿå˜†è© 
èªåŠ©è© 

çš„, ä¹‹, å¾—, åœ° 
P ä»‹è© 

Vi 

å‹•ä½œä¸åŠç‰©å‹•è© 
å‹•ä½œé¡åŠç‰©å‹•è© 
ç‹€æ…‹ä¸åŠç‰©å‹•è© 
ç‹€æ…‹é¡åŠç‰©å‹•è© 

Vt 

å‹•ä½œä½¿å‹•å‹•è© 
å‹•ä½œåŠç‰©å‹•è© 

å‹•ä½œæ¥åœ°æ–¹è³“èªå‹•è© 
é›™è³“å‹•è© 

å‹•ä½œå¥è³“å‹•è© 
å‹•ä½œè¬‚è³“å‹•è© 

åˆ†é¡å‹•è© 
ç‹€æ…‹ä½¿å‹•å‹•è© 
ç‹€æ…‹åŠç‰©å‹•è© 
ç‹€æ…‹å¥è³“å‹•è© 
ç‹€æ…‹è¬‚è³“å‹•è© 

æœ‰ 
æ˜¯ 

 
Table 1: Simplified CKIP POS 

2.1 Training phase 

In the training phase, a training sentence is first 
segmented into terms. Each term is labeled with 
the corresponding POS tag and error type tag. 
Then our system uses the CRF++ leaning 
algorithm to train a model. The features used in 
CRF++ can be expressed by templates. Table 12 
(at the end of the paper) shows one sentence in 
our training set.  
Table 13 (at the end of the paper) shows all the 
templates of the feature set used in our system 
and the corresponding value for the example. 
The format of each template is %X[row, col], 
where row is the number of rows in a sentence 
and column is the number of column as we 
shown in Figure 1. The feature templates used in 
our system are the combination of terms and 
POS of the input sentences. For example, the 
first feature template is â€œTerm+POSâ€, if an input 
sentence contains the same term with the same 
POS, the feature value will be 1, otherwise the 
feature value will be 0. The second feature 
template is â€œTerm+Previous Termâ€, if an input 
sentence contains the same term bi-gram, the 
feature value will be 1, otherwise the feature 
value will be 0. 

2.2 Test phase 

In the Test phase, our system use the trained 
model to detect and identify the error of an input 
sentence. Table 2, Table 3, and Table 4 show the 
labeling results of examples of sentences with 
error types Redundant, Selection, Disorder, and 
Missing respectively. 
 
Word POS tag Predict tag 
ä»– N O O 
æ˜¯ Vt O O 
çœŸ. ADV R R 
å¾ˆ ADV O O 
å¥½ Vi O O 
çš„ T O O 
äºº N O O 

 
Table 2: A tagging result sample of a sentence 

with error type Redundant 

8



 
Term POS tag Predict tag 
ä½  N O O 
åƒè¬ DET O O 
ä¸è¦ ADV O O 
åœ¨æ„ Vt O O 
é€™ DET O O 
å€‹ M S S 
äº‹æƒ… N O O 
 

Table 3: A tagging result sample of a sentence 
with error type Selection 

r 
Term POS tag Predict tag 
ä½  N O O 
ä»€éº¼ DET D D 
è¦. ADV D D 
ç© Vt D D 

 
Table 4: A tagging result sample of a sentence 

with error type Disorder 
. 

Term POS Tag Predict tag 
çœ‹ Vt O O 
é›»å½± N O O 
æ™‚å€™ N M M 
 

Table 5: A tagging result sample of a sentence 
with error type Missing example 

 
If all the system predict tags in the fourth column 
are the same as the tags in the third column, then 
the system labels the sentence correctly. In the 
formal run, accuracy, precision, recall (Clevereon, 
1972), and F-score (Rijsbergen,1979) are 
considered. The measure metrics are defined as 
follows. The notation is listed in  
Table 6. 

 
 

System predict tag 
A B 

Known tag 
A tpA eAB 

B eBA tpB 
 

Table 6: The confusion matrix. 
 
Precision A = ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡

ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡+ğ‘’ğ‘’ğ‘’ğ‘’ğ‘¡ğ‘¡
 

 
Recall A = ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡

ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡+ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘’
 

 
F1-Score A =2 Ã— Precision ğ‘¡ğ‘¡Ã—Recall ğ‘¡ğ‘¡

Precision ğ‘¡ğ‘¡+Recall ğ‘¡ğ‘¡
 

Accuracy =ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡+ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘’ğ‘’
ğ‘¡ğ‘¡ğ´ğ´ğ´ğ´ ğ·ğ·ğ·ğ·ğ‘¡ğ‘¡ğ·ğ·

 
 

3 Experiments 

3.1 Data set 

Our training data consists of data from 
NLP-TEA1(Chang et al.,2012)Training Data, 
Test Data, and the Training Data from 
NLP-TEA2. Figure 3  (at the end of the 
paper)shows the format of the data set. Table 7  
shows the number of sentences in our training 
set. 
 
size NLP-TEA1 NLP-TEA2 
Redundant 1830 434 
Correct 874 0 
Selection 827 849 
Disorder 724 306 
Missing 225 622 
 

Table 7: Training set size 

3.2 Experiments result 

In the formal run of NLP-TEA-2 CGED shared 
task, there are 6 participants and each team 
submits 3 runs. Table 8 shows the false positive 
rate. Our system has the lowest false positive rate 
0.082, which is much lower than the average.  
Table 9, Table 10, and Table 11 show the formal 
run result of our system compared to the average 
in Detection level, Identification level, and 
Position level respectively. Our system achieved 
the highest precision in all the three levels, but 
the accuracy of our system is fare. However, the 
recall of our system is relatively low. The 
numbers in boldface are the best performance 
amount 18 runs in the formal run this year.  
 

Submission False Positive Rate 

CYUT-Run1 0.096 
CYUT-Run2 0.082 
CYUT-Run3 0.132 

Average of all 18 
runs 0.538 

 
Table 8: The false positive rate. 

 

9



 
Detection Level 

Accuracy Precision Recall F1 

CYUT-Run1 0.584 0.7333 0.264 0.3882 
CYUT-Run2 0.579 0.7453 0.24 0.3631 
CYUT-Run3 0.579 0.6872 0.29 0.4079 
Average of 
all 18 runs 0.534 0.560 0.607 0.533 

 
Table 9: Performance evaluation in Detection 

Level. 
 

 
Identification Level 

Accuracy Precision Recall F1 

CYUT-Run1 0.522 0.5932 0.14 0.2265 
CYUT-Run2 0.525 0.6168 0.132 0.2175 
CYUT-Run3 0.505 0.5182 0.142 0.2229 
Average of 
all 18 runs 0.335 0.329 0.208 0.233 

Table 10: Performance evaluation in 
Identification Level. 

 

 Position Level 
Accuracy Precision Recall F1 

CYUT-Run1 0.504 0.52 0.104 0.1733 
CYUT-Run2 0.505 0.5287 0.092 0.1567 
CYUT-Run3 0.488 0.45 0.108 0.1742 
Average of 
all 18 runs 0.263 0.166 0.064 0.085 

 
Table 11: Performance evaluation in Position 

Level. 
 

4 Error analysis on the official test 
result 

There are 1000 sentences in the official test set of 
the 2015 CGED shared task. Our system labeled 
them according to the CRF model that we trained 
based on the official training set and the 
available data set from last year. 

The number of tag O dominates the number 
of other tags in the training set for sentences with 
or without an error. For example, sentence no. 
B1-0436, a sentence without error: 
{ä¸Šæ¬¡æˆ‘åäº† MRT å»äº†åœ“å±±ç«™åƒè§€å¯ºå»Ÿäº†ï¼Œ
O(ä¸Š)ï¼ŒO(æ¬¡)ï¼ŒO(æˆ‘)ï¼ŒO(å)ï¼ŒR(äº†)ï¼ŒO(MRT)ï¼Œ
O(å»)ï¼ŒO(äº†)ï¼ŒO(åœ“å±±)ï¼ŒO(ç«™)ï¼ŒO(åƒè§€)ï¼ŒO(å¯º
å»Ÿ)ï¼ŒO(äº†)} 

And, sentence no. A2-0322, a sentence with an 
error: 
{ä»–å€‘å¾å…¬è»Šç«™èµ°è·¯èµ°äºŒååˆ†é˜æ‰åˆ°é›»å½±é™¢äº†ï¼Œ
O(ä»–å€‘)ï¼ŒO(å¾)ï¼ŒO(å…¬è»Šç«™)ï¼ŒO(èµ°è·¯)ï¼ŒO(èµ°)ï¼Œ
O(äºŒå)ï¼ŒO(åˆ†é˜) ï¼ŒO(æ‰) ï¼ŒO(åˆ°) ï¼ŒO(é›»å½±
é™¢) ï¼ŒR(äº†)} 
   Therefore, our system tends to label words 
with tag O and it is part of the reason that our 
system gives the lowest false positive rate this 
year. Our system also has high accuracy and 
precision rate, but the Recall rate is lower than 
other systems. We will analyze the causes and 
discuss how to improve the fallbacks. 

We find that there are 11 major mistake types 
of our system result. 
1. Give two error tags in one sentence. 
2. Fail to label the Missing tag 
3. Fail to label the Disorder tag 
4. Fail to label the Redundant tag  
5. Fail to label the Selection tag 
6. Label a correct sentence with Missing tag 
7. Label a correct sentence with Redundant tag 
8. Label a correct sentence with Disorder tag 
9. Label a correct sentence with Selection tag 
10. Label a Selection type with Redundant tag 
11. Label a Disorder type with Missing tag 
 
Analysis of the error cases: 
1. Give two error tags in one sentence: In the 

official training set and test set, a sentence 
has at most one error type. However, our 
method might label more than one error tags 
in one sentence. For example, a system 
output: {ä»–æ˜¯å¾ˆè°æ˜å­¸ç”Ÿï¼ŒO(ä»–)ï¼ŒR(æ˜¯)ï¼Œ
O(å¾ˆ)ï¼ŒO(è°æ˜)ï¼ŒM(å­¸ç”Ÿ)}. Currently, we do 
not rule out the possibility that a sentence 
might contain more than one errors. We 
believe that in the real application, there 
might be a need for such situation. However, 
our system might compare the confidence 
value of each tag and retain only one error 
tag in one sentence. 
 

2. Fail to label the Missing tag: The missing 
words might be recovered by rules. For 
example, a system output: {éœ€è¦ä¸€äº›æ±è¥¿ä¿®
ç†å¥½ï¼ŒO(éœ€è¦)ï¼ŒO(ä¸€äº›)ï¼ŒO(æ±è¥¿)ï¼ŒO(ä¿®
ç†å¥½)} should be {éœ€è¦ä¸€äº›æ±è¥¿ä¿®ç†å¥½ï¼Œ
O(éœ€è¦)ï¼ŒM(ä¸€äº›)ï¼ŒO(æ±è¥¿)ï¼ŒO(ä¿®ç†å¥½)} 
and the missing word should be â€è¢«â€ or â€œæŠŠâ€. 
A set of rule for â€è¢«â€ or â€œæŠŠâ€ can be helpful. 

 
3. Fail to label the Disorder tag: The disorder 

10



error is also hard for CRF model, since the 
named entity (NE) is not recognized first. 
For example, a system output: {é›¢å°åŒ—è»Šç«™
æ·¡æ°´ä¸å¤ªè¿‘ï¼ŒO(é›¢)ï¼ŒO(å°åŒ—)ï¼ŒO(è»Šç«™)ï¼Œ
O(æ·¡æ°´)ï¼ŒO(ä¸)ï¼ŒO(å¤ª)ï¼ŒO(è¿‘)} should be 
{é›¢å°åŒ—è»Šç«™æ·¡æ°´ä¸å¤ªè¿‘ï¼ŒD(é›¢)ï¼ŒD(å°åŒ—)ï¼Œ
D(è»Šç«™)ï¼ŒD(æ·¡æ°´)ï¼ŒO(ä¸)ï¼ŒO(å¤ª)ï¼ŒO(è¿‘)}. 
The disorder error can only be recognized 
once the named entities â€œå°åŒ—è»Šç«™â€ and â€æ·¡
æ°´â€ are recognized and then the grammar 
rule â€ NE1+é›¢+NE2+è¿‘â€ can be applied. 

 
4. Fail to label the Redundant tag: Some 

adjacent words are regarded as redundant 
due to the semantics. Two adjacent words 
with almost the same meaning can be 
reduced to one. For example: a system 
output: {é‚£å…¬åœ’æ˜¯åœ¨å°åŒ—åŒ—éƒ¨æœ€è¿‘æ–°æœ‰çš„ï¼Œ
O(é‚£)ï¼ŒO(å…¬åœ’)ï¼ŒO(æ˜¯)ï¼ŒO(åœ¨)ï¼ŒO(å°åŒ—)ï¼Œ
O(åŒ—éƒ¨)ï¼ŒO(æœ€è¿‘)ï¼ŒO(æ–°)ï¼ŒO(æœ‰çš„)} fail to 
recognize the redundant word R(å°åŒ—) or 
R(åŒ—éƒ¨). In this case, â€œæ–°æœ‰çš„â€ is also bad 
Chinese, it should be â€œæ–°å»ºçš„â€. However, 
the word segmentation result makes our 
system hard to detect the error. 
 

5. Fail to label the Selection tag: We believe 
that it required more knowledge to recognize 
the selection error than limited training set. 
For example, a system output: {é€™æ˜¯ä¸€å€‹å¾ˆ
å¥½çš„æ–°èï¼ŒO(é€™)ï¼ŒO(æ˜¯)ï¼ŒO(ä¸€)ï¼ŒO(å€‹)ï¼Œ
O(å¾ˆ)ï¼ŒO(å¥½)ï¼ŒO(çš„)ï¼ŒO(æ–°è)} fail to 
recognize the classifiers (also called measure 
words) forâ€æ–°èâ€ should not be â€å€‹â€, the 
most common Mandarin classifier. It should 
be â€œå‰‡â€. A list of the noun to classifier table 
is necessary to recognize this kind of errors. 

 
6. Label a correct sentence with Missing tag: 

This case is relative rare in our system. For 
example, a system output: {ä¸€å€‹å°æ™‚ä»¥å‰æˆ‘
æ±ºå®šä¼‘æ¯ä¸€ä¸‹ï¼ŒM(ä¸€)ï¼ŒO(å€‹)ï¼ŒO(å°æ™‚)ï¼Œ
O(ä»¥å‰)ï¼ŒM(æˆ‘) ï¼ŒO(æ±ºå®š)ï¼ŒO(ä¼‘æ¯) ï¼Œ
O( ä¸€ ä¸‹ )} accurately contains no error. 
However our system regard a single â€œä¸€â€ 
should be a missing error according to the 
trained model.  

 
7. Label a correct sentence with Redundant tag: 

There are cases that we think our system 
perform well. For example, our system 
output: {å¹³å¸¸ä¸‹äº†èª²ä»¥å¾Œä»–é¦¬ä¸Šå›å®¶ï¼ŒO(å¹³

å¸¸)ï¼ŒO(ä¸‹)ï¼ŒR(äº†)ï¼ŒO(èª²)ï¼ŒO(ä»¥å¾Œ) ï¼ŒO(ä»–)ï¼Œ
O(é¦¬ä¸Š)ï¼ŒO(å›å®¶) }. Where â€œäº†â€ can be 
regarded as redundant in some similar cases. 

 
8. Label a correct sentence with Disorder tag: 

This is a rare case in our system. For 
example, a system output: {ä»¥å¾Œæ…¢æ…¢çŸ¥é“ä»–
é€™ç¨®æ–¹å¼å…¶å¯¦æ˜¯å¾ˆæ™®é€šçš„äº¤æœ‹å‹çš„æ–¹å¼ï¼Œ

D(ä»¥å¾Œ)ï¼ŒD(æ…¢æ…¢)ï¼ŒD(çŸ¥é“)ï¼ŒD(ä»–)ï¼ŒD(é€™) ï¼Œ
D(ç¨®)ï¼ŒO(æ–¹å¼)ï¼ŒO(å…¶å¯¦)ï¼ŒO (æ˜¯) ï¼ŒO (å¾ˆ)ï¼Œ
O (æ™®é€š)ï¼ŒO (çš„)ï¼ŒO (äº¤) ï¼ŒO (æœ‹å‹)ï¼ŒO (çš„)ï¼Œ
O (æ–¹å¼) }. It is a sentence that cannot be 
judged alone without enough contexts.  

 
9. Label a correct sentence with Selection tag: 

In one case, our system output: {ä»Šå¤©æ˜¯å€‹å¾ˆ
é‡è¦çš„ä¸€å¤©ï¼ŒO(ä»Šå¤©)ï¼ŒO(æ˜¯)ï¼ŒS(å€‹)ï¼ŒO(å¾ˆ)ï¼Œ
R(é‡è¦) ï¼ŒO(çš„)ï¼ŒO(ä¸€)ï¼ŒO(å¤©) }, where â€
å€‹â€ is also not a good measure word. 

 
10. Label a Selection type with Redundant tag: 

Sometimes there are more than one way to 
improve a sentence. For example, a system 
output: {ä¸‹äº†èª²ç‹å¤§è¡›æœ¬ä¾†é¦¬ä¸Šå›å®¶ï¼Œ
O(ä¸‹)ï¼ŒR(äº†)ï¼ŒO(èª²)ï¼ŒO(ç‹å¤§è¡›)ï¼ŒO(æœ¬ä¾†) ï¼Œ
O(é¦¬ä¸Š)ï¼ŒO(å›å®¶) }, which is no better than 
{ä¸‹äº†èª²ç‹å¤§è¡›æœ¬ä¾†é¦¬ä¸Šå›å®¶ï¼ŒO(ä¸‹)ï¼Œ
O(äº†)ï¼ŒO(èª²)ï¼ŒO(ç‹å¤§è¡›)ï¼ŒS(æœ¬ä¾†) ï¼ŒO(é¦¬
ä¸Š)ï¼ŒO(å›å®¶) }. Where â€œæœ¬ä¾†â€ should be 
â€œå°±â€. However, in a different context, it 
could be â€œæœ¬ä¾†æƒ³â€+â€ä½†æ˜¯â€¦â€.  

 
11. Label a Disorder type with Missing tag: 

Since a Disorder error might involve more 
than two words, comparing to other types, it 
is hard to train a good model. For example, a 
system output: {ä¸­åœ‹æ–°å¹´åˆ°äº†çš„æ™‚å€™ï¼ŒO(ä¸­
åœ‹)ï¼ŒO(æ–°å¹´)ï¼ŒO(åˆ°)ï¼ŒO(äº†)ï¼ŒM(çš„) ï¼Œ
O(æ™‚å€™) } should be {ä¸­åœ‹æ–°å¹´åˆ°äº†çš„æ™‚å€™ï¼Œ
O(ä¸­åœ‹)ï¼ŒD(æ–°å¹´)ï¼ŒD(åˆ°)ï¼ŒD(äº†)ï¼ŒO(çš„) ï¼Œ
O(æ™‚å€™) }, and the correct sentence should 
be â€œåˆ°äº†ä¸­åœ‹æ–°å¹´çš„æ™‚å€™â€. A grammar rule 
such as â€œåˆ°äº†â€+Event+â€çš„æ™‚å€™â€ might be 
help. 

 
5 Conclusion and Future work 

This paper reports our approach to the 
NLP-TEA-2 CGED Shared Task evaluation. 
Based on the CRF model, we built a system that 
can achieve the lowest false positive rate and the 
highest precision at the official run. The 

11



approach uniformly dealt with the four error 
types: Redundant, Missing, Selection, and 
Disorder. 
  According to our error analysis, the difficult 
cases suggest that to build a better system 
requires more features and more training data. 
The system can be improved by integrating rule 
based system in the future. 

Due to the limitation of time and resource, our 
system is not tested under different experimental 
settings. In the future, we will test our system 
with more feature combination on both POS 
labeling and sentence parsing. 

Acknowledgments 

This study is conducted under the "Online and 
Offline integrated Smart Commerce 
Platform(2/4)" of the Institute for Information 
Industry which is subsidized by the Ministry of 
Economic Affairs of the Republic of China . 

Reference 

Lafferty, A. McCallum, and F. Pereira. (2001) 
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In Intl. Conf. 
on Machine Learning. 
 
C. W. Clevereon, (1972), On the inverse relationship 
of recall and precision, Workshop on Machine 
Learning for Information Extraction, pp.195-201. 
 
C. van Rijsbergen, (1979),Information Retrieval, 

Butterworths. 
 
Ru-Yng Chang, Chung-Hsien Wu, and Philips Kokoh 
Prasetyo. (2012). Error Diagnosis of Chinese 
Sentences Using Inductive Learning Algorithm and 
Decomposition-Based Testing Mechanism. ACM 
Transactions on Asian Language Information 
Processing, 11(1), article 3, March. 
 
Chung-Hsien Wu, Chao-Hong Liu, Matthew Harris, 
and Liang-Chih Yu (2010). Sentence Correction 
Incorporating Relative Position and Parse Template 
Language Models. IEEE Transactions on Audio, 
Speech, and Language Processing, 18(6), 1170-1181. 
 
Shih-Hung Wu, Hsien-You Hsieh. (2012). Sentence 
Parsing with Double Sequential Labeling in 
Traditional Chinese Parsing Task. Second 
CIPS-SIGHAN Joint Conference on Chinese 
Language Processing, pages 222â€“230. 
 
Jui-Feng Yeh, Yun-Yun Lu, Chen-Hsien Lee, 
Yu-Hsiang Yu, Yong-Ting Chen. (2014). Detecting 
Grammatical Error in Chinese Sentence for Foreign. 
 
Tao-Hsing Chang, Yao-Ting Sung , Jia-Fei Hong, 
Jen-I CHANG. (2014). KNGED: a Tool for 
Grammatical Error Diagnosis of Chinese Sentences. 

 
Yu, L.-C., Lee, L.-H., & Chang, L.-P. (2014). 
Overview of grammatical error diagnosis for learning 
Chinese as a foreign language. In Proceedings of the 
1stWorkshop on Natural Language Processing 
Techniques for Educational Applications, 42-47.

 
Figure 2: The framework of the proposed system. 

 

Training 
Sentence 

Preprocessing 

Set Tag 

CRF tool 

Feature set 

model 

Test 
Sentence 

 

Preprocessing 

Tagged output 

12



 
Figure 3: An example of the source data. 

 

 
Table 12: A sample training sentence. 

 

Template Meaning Template Feature rule 

Term+POS %x[0,0]/%x[0,1] çœŸ/ADV 

Term+Previous Term %x[0,0]/%x[-1,0] çœŸ/æ˜¯ 

Term+Previous POS %x[0,0]/%x[-1,1] çœŸ/ Vt 

POS+Previous Term %x[0,1]/%x[-1,0] ADV/æ˜¯ 

POS+Previous POS %x[0,1]/%x[-1,1] ADV/ Vt 
Term+Previous Term+Previous 
POS %x[0,0]/%x[-1,0]/%x[-1,1] çœŸ/æ˜¯/ Vt 

POS+Previous Term+Previous 
POS %x[0,1]/%x[-1,0]/%x[-1,1] ADV/æ˜¯/ Vt 

Term+Second Previous Term %x[0,0]/%x[-2,0] çœŸ/ä»– 

Term+Second Previous POS %x[0,0]/%x[-2,1] çœŸ/N 

 col0 col1 col2 
r-2 ä»– N O 
r-1 æ˜¯ Vt O 
r0 (ç›®å‰ Token) çœŸ ADV R 
r1 å¾ˆ ADV O 
r2 å¥½ Vi O 
r3 çš„ T O 
r4 äºº N O 

<root> 

<ESSAY title="ä¸èƒ½åƒåŠ æœ‹å‹

æ‰¾åˆ°å·¥ä½œçš„æ…¶ç¥æœƒ"> 

<TEXT> 

<SENTENCE id="A2-0003-1">

æˆ‘ä»¥å‰çŸ¥é“å¦³åˆå¾ˆè°æ˜åˆç”¨åŠŸ

</SENTENCE> 

</TEXT> 

<MISTAKE id="A2-0003-1"> 

<TYPE>Redundant</TYPE> 

<CORRECTION>æˆ‘ä»¥å‰çŸ¥é“å¦³åˆ

è°æ˜åˆç”¨åŠŸ</CORRECTION> 

</MISTAKE> 

</ESSAY> 

13



POS+Second Previous Term %x[0,1]/%x[-2,0] ADV/ä»– 

POS+Second Previous POS %x[0,1]/%x[-2,1] ADV/N 
Term+Second Previous 
Term+Second Previous POS %x[0,0]/%x[-2,0]/%x[-2,1] çœŸ/ä»–/N 

POS+Second Previous 
Term+Second Previous POS %x[0,1]/%x[-2,0]/%x[-2,1] ADV/ä»–/N 

Term+Next Term %x[0,0]/%x[1,0] çœŸ/å¾ˆ 

Term+Next POS %x[0,0]/%x[1,1] çœŸ/ADV 

POS+Next Term %x[0,1]/%x[1,0] ADV/å¾ˆ 

POS+Next POS %x[0,1]/%x[1,1] ADV/ADV 

Term+Next Term+Next POS %x[0,0]/%x[1,0]/%x[1,1] çœŸ/å¾ˆ/ADV 

POS+Next Term+Next POS %x[0,1]/%x[1,0]/%x[1,1] ADV/å¾ˆ/ADV 

Term+Second Next Term %x[0,0]/%x[2,0] çœŸ/å¥½ 

Term+Second Next POS %x[0,0]/%x[2,1] çœŸ/ Vi 

POS+Second Next Term %x[0,1]/%x[2,0] ADV/å¥½ 

POS+Second Next POS %x[0,1]/%x[2,1] ADV/ Vi 
Term+Second Next Term+Second 
Next POS %x[0,0]/%x[2,0]/%x[2,1] çœŸ/å¥½/Vi  

POS+Second Next Term+Second 
Next POS %x[0,1]/%x[2,0]/%x[2,1] ADV/å¥½/ Vi 

 
Table 13: All the templates and the corresponding value for the sample sentence. 

 

14


