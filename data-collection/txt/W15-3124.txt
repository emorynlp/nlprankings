



















































Topic-Based Chinese Message Polarity Classification System at SIGHAN8-Task2


Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 158â€“163,
Beijing, China, July 30-31, 2015. cÂ©2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing

Topic-Based Chinese Message Polarity Classification 
System at SIGHAN8-Task2 

 
 

Chun Liao, Chong Feng, Sen Yang, Heyan Huang 
School of Computer Science 

and Technology, Beijing  
Institute of Technology 

{cliao, fengchong, syang, hhy63}@bit.edu.cn 
 

  
 

Abstract 

This paper describes the topic-based Chi-
nese message polarity classification sys-
tem submitted by LCYS_TEAM at 
SIGHAN8-Task2. The system mainly in-
cludes two parts: 1) a graph-based rank-
ing model integrating local and global in-
formation is adopted to represent the 
classification ability of words towards 
different topics. In construction of graph 
model, a new weighting approach and a 
PMI-based random jumping probability 
selection method is proposed. 2) For sen-
timental features, word embedding is 
employed for acquiring expanded topical 
words and syntactic dependency is 
adopted for getting topic-related senti-
mental words. Experiment results 
demonstrate the effectiveness of our sys-
tem. 

1 Introduction 
Sentiment analysis, which is to identify or de-
termine the implied emotional orientation, atti-
tude and opinion when people express something, 
is becoming more and more important for net-
work monitoring with its application on mi-
croblog. In the traditional sentiment analysisï¼Œ 
unsupervised methods were adopted in Ku(2005), 
Shen(2009), Vasileios(2000) and Turney(2002), 
and the limitation of such approaches based on 
semantic dictionary mainly is unable to solve the 
problem of Out-of-Vocabulary words. Super-
vised methods were employed with model of 
machine learning, such as Naive Bayes, Max 
Entropy, Support Vector Machine in Pang(2002), 
Dasgupta(2009), and Li(2011). 

Hashtags, in the form of â€œï¼ƒ topicï¼ƒ â€, are 
widely used as topics in Chinese microblogs. For 
the topic-related work, Wang(2011) and 
Jakob(2010) made research on hashtag-level sen-
timent classification in twitter. In the traditional 
sentiment analysis, the object people express 
sentiment on is not taken into consideration. And 
these methods are mostly topic-ignored and can-
not perform the accurate sentiment analysis in 
many topic-related messages. We summarize 
such kind of difficult cases into two categories. 

1) Microblogs with multiple candidate topics 
For example, â€œ# ä¸‰ æ˜Ÿ galaxy s6## å ä¸º

P8##mate8#â€ä¸‰æ˜Ÿ galaxy s6 çœŸæ²¡ä»€ä¹ˆäº®ç‚¹ï¼Œå
ä¸º P8 å°±å¯ä»¥ç§’å®ƒäº†ï¼Œæ›´ä¸ç”¨è¯´ mate8[æ‹œæ‹œ]â€. 
This sentence conveys negative sentiment to-
wards topic of â€œä¸‰æ˜Ÿ galaxy s6â€, but positive 
sentiment towards topic of â€œåä¸º P8â€ and â€œma-
te8â€. 

2) Microblogs with topic specific sentimental 
words 

For example, â€œ#è‚¡ç¥¨#å‰å¤©åˆšå…¥æ‰‹ä¸€æ”¯è‚¡ç¥¨ï¼Œ
ä¸€ç›´åœ¨å‡ï¼Œè‚¡ä»·è¶Šæ¥è¶Šé«˜â€ and â€œ#ä¸‰æ˜Ÿ#ä¸‰æ˜Ÿæ‰‹
æœºç”µé‡æ˜æ˜¾ä¸å¤Ÿç”¨ï¼Œè€—èƒ½é«˜â€. The word â€œé«˜â€ 
carrys positive sentiment orientation in the first 
sentence towards topic â€œè‚¡ç¥¨â€ and negative sen-
timent orientation in the latter towards topic â€œä¸‰
æ˜Ÿâ€. 

Considering the importance of topical infor-
mation in microblogs, this paper studied topic-
based Chinese message polarity classification. 
Given a message from Chinese Weibo Platform 
(Such as Sina, Tencent, NetEase etc. ) and a top-
ic, classify whether the message is of positive, 
negative, or neutral sentiment towards the given 
topic. For messages conveying both a positive 
and negative sentiment towards the topic, which-
ever is the stronger sentiment should be chosen.  

158



The rest of this paper is organized as follows. 
In Section 2, we briefly present the topic-based 
Chinese message polarity classification system 
from two aspects of graph-based ranking feature 
and topic-related sentimental feature. Evaluation 
results are presented in Section 3. Finally, the 
last section summarizes this paper and describes 
our future work. 

2 System Architecture 
In topic-based Chinese message polarity classifi-
cation, our system is mainly composed by two 
parts: topic-related keyword feature selection and 
topic-related sentimental feature selection. In 
detail, topic-related keyword feature is acquired 
by a novel graph-based ranking algorithm of LT-
IGT, and topic-related sentimental feature is ob-
tained by topical words expansion based on word 
embedding and syntactic parsing according to the 
expanded topical words. The architecture of our 
system is illustrated in Figure 1. 

Raw test sentences

PreparationsPreparations

TextRank topical words

LTIGT

globallocal

Expanded topical words

Word 
Embedding

Parser

Topic-related 
sentimental words

Common 
features

Support Vector Machine

Output Result

Topic-related 
keyword feature

Topic-related 
sentimental feature

 
Figure 1. System architecture 

2.1 Preparations 
To evaluate the performance of method proposed 
in this paper for topic-based Chinese microblog 
polarity classification, we carry out experiments 
on dataset offered by SIGHAN8-Task 2 called 
Topic-Based Chinese Message Polarity Classifi-
cation. This dataset is obtained from Chinese 
Weibo Platform, such as Sina, Tencent, NetEase 
etc. It contains 5*1000 manually annotated mi-
croblogs which cover 5 topics, such as â€œä¸‰æ˜Ÿ

S6â€, â€œå¤®è¡Œé™æ¯â€, etc. In experiments, we ran-
domly select 800 microblogs of each topic for 
training and 200 for testing, and finally get train-
ing set of 4000 microblogs and testing set of 
1000 microblogs to perform classification. 

Considering the non-standard feature of mi-
croblog, the corpus is firstly normalized by fol-
lowing three rules. 

Rule 1: Turn over the microblog with â€œ//â€ to 
ensure the forwarding relationship and guarantee 
the latter sentence is analyzed based on the front 
sentence. 

Rule 2: Delete structures like â€œ@+usernameâ€, 
â€œhttp://xxxâ€ to reduce noises caused by username 
and website. 

Rule 3: Replace the consecutive punctuations 
with the first one to normalize the structure of 
expression. 

Through filtration by these rules, this paper 
conducts experiments on the preprocessed da-
taset and accesses them with traditional Preci-
sion(P), Recall(R) and F-measure(F) under Mi-
cro-average and Macro-average. 

2.2 Selection of topic-related keyword fea-
ture 

Inspired by TF-IDF(Salton et al., 1975,1983), 
words own higher local importance and lower 
global importance are more significant for classi-
fication. But for topic-based Chinese message 
polarity classification, it is obviously insufficient 
to extract keywords based on frequency infor-
mation merely. For example, in the sentence of 
â€œGALAXY S6ä¸€æ”¹ä¸‰æ˜Ÿæ­¤å‰â€œä¸‡å¹´å¤§å¡‘æ–™â€
çš„å½¢è±¡ï¼Œé‡‡ç”¨äº†å‰åç»ç’ƒé¢æ¿å’Œé‡‘å±æ¡†ç»„åˆ

çš„æœºèº«è®¾è®¡ï¼Œä¸ºäº†æ”¯æ’‘æ›´çº¤è–„çš„æœºèº«ï¼Œä¸æƒœ

ç‰ºç‰²microSDå¡æ§½å’Œç”µæ± æ›´æ¢ï¼Œå³ä½¿å¦‚æ­¤ï¼Œä»
ç„¶æ— æ³•ä¸æ‹¥æœ‰å®Œç¾å¤–è§‚çš„iphoneåª²ç¾ã€‚â€, the 
conventional TF-IDF method tends to extract â€œä¸‰
æ˜Ÿã€GALAXY S6ã€iphoneã€æœºèº«ã€å¡æ§½ã€ç”µ
æ± ã€å¤–è§‚â€ as keywords, but in this topic-based 
task, topic-related words such as â€œä¸‰ æ˜Ÿ ã€
GALAXY S6ã€å¡æ§½ã€ç”µæ± ã€å¤–è§‚â€ are ex-
pected to be selected as the keywords feature for 
the topic â€œä¸‰æ˜Ÿâ€. To better solve the problem of 
microblogs with multiple candidate topics intro-
duced in section 1, this paper proposes a novel 
LT-IGT(illustrated in Figure 2) algorithm which 
integrates topic, position and co-occurrence in-
formation, its function is designed as follows. 
ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ = ğ¿ğ¿ğ¿ğ¿ Ã—  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ = ğ¿ğ¿ğ‘‡ğ‘‡ğ‘™ğ‘™ğ‘™ğ‘™(ğ‘£ğ‘£ğ‘–ğ‘–)  Ã—  

1
ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘”ğ‘”ğ‘”ğ‘”(ğ‘£ğ‘£ğ‘–ğ‘–)

 (1) 

159



where ğ¿ğ¿ğ‘‡ğ‘‡ğ‘™ğ‘™ğ‘™ğ‘™(ğ‘£ğ‘£ğ‘–ğ‘–) and ğ¿ğ¿ğ‘‡ğ‘‡ğ‘”ğ‘”ğ‘™ğ‘™(ğ‘£ğ‘£ğ‘–ğ‘–) represent for rank-
ing score of vertex ğ‘£ğ‘£ğ‘–ğ‘– under local and global Tex-
tRank.  

Microblog 2

Microblog 1

â€¦â€¦

Locâ€¦l TextRâ€¦nk

â€¦â€¦

Globâ€¦l TextRâ€¦nk

Topic 1

Topic 2

 

Figure 2. Graph Model of LT-IGT 

The idea of TextRank(Mihalce,2004) derives 
from PageRank, which is achieved by dividing 
the text into several units to build graph model 
and exploiting voting mechanism for ranking. 
This method can model the relationship between 
the current word and contextual information, and 
the contextual related words can be recommend-
ed reciprocally. Considering the importance of a 
word is related to both itself and its relevant 
words, TextRank overcomes the independence of 
words in traditional â€œbag-of-wordsâ€ model and 
characterizes the importance of a word more ac-
curately. 
ï¬ CST: A novel weighting method of graph-

based ranking model 
For each vertex in the graph, its importance 

ranking score benefits from adjacent nodes, and 
on the other hand, its own ranking score can also 
be transferred to the neighboring vertexes. Ac-
cording to the above assumptions, the indicator 
of vertex importance can be divided into follow-
ing three parts: Coverage Importance, Semantic 
Similarity Importance and Topic-Related Im-
portance. For two vertexes ğ‘£ğ‘£ğ‘–ğ‘–  and ğ‘£ğ‘£ğ‘—ğ‘— , the influ-
ence of ğ‘£ğ‘£ğ‘–ğ‘– to ğ‘£ğ‘£ğ‘—ğ‘— can be transferred by the directed 
edge e =< viï¼Œvj >. In this paper, we assign ğ‘¤ğ‘¤ğ‘–ğ‘–ğ‘—ğ‘— 
as the weight between ğ‘£ğ‘£ğ‘–ğ‘–  and ğ‘£ğ‘£ğ‘—ğ‘— , Î±, Î², Î³ as the 
proportions of these three indicators. Conse-
quently, the weight value between two vertexes 
can be defined as follows: 

ğ‘¤ğ‘¤ğ‘–ğ‘–ğ‘—ğ‘— = ğ›¼ğ›¼ğ‘¤ğ‘¤ğ‘ğ‘ğ‘ğ‘ğ‘£ğ‘£ï¿½ğ‘£ğ‘£ğ‘–ğ‘– , ğ‘£ğ‘£ğ‘—ğ‘—ï¿½ + ğ›½ğ›½ğ‘¤ğ‘¤ğ‘ ğ‘ ğ‘ ğ‘ ï¿½ğ‘£ğ‘£ğ‘–ğ‘– ,ğ‘£ğ‘£ğ‘—ğ‘—ï¿½ +
         ğ›¾ğ›¾ğ‘¤ğ‘¤ğ‘™ğ‘™ğ‘¡ğ‘¡(ğ‘£ğ‘£ğ‘–ğ‘– ,ğ‘£ğ‘£ğ‘—ğ‘—)              (2) 

Where Î±+Î²+Î³=1. 
a) ğ‘¤ğ‘¤ğ‘ğ‘ğ‘ğ‘ğ‘£ğ‘£(ğ‘£ğ‘£ğ‘–ğ‘– ,ğ‘£ğ‘£ğ‘—ğ‘—)  represents for coverage im-

portance of ğ‘£ğ‘£ğ‘–ğ‘–, it can be calculated by 

ğ‘¤ğ‘¤ğ‘ğ‘ğ‘ğ‘v(ğ‘£ğ‘£ğ‘–ğ‘– ,ğ‘£ğ‘£ğ‘—ğ‘—) =
1

|Out(ğ‘£ğ‘£ğ‘–ğ‘–)|
              (3) 

Where |Out(ğ‘£ğ‘£ğ‘–ğ‘–)| is the out-degree of vertex ğ‘£ğ‘£ğ‘–ğ‘– . 
This formula expresses the coverage importance 
of ğ‘£ğ‘£ğ‘–ğ‘–  can be transmitted to its neighboring ver-
texes uniformly. 
b) ğ‘¤ğ‘¤ğ‘ ğ‘ ğ‘ ğ‘ (ğ‘£ğ‘£ğ‘–ğ‘– ,ğ‘£ğ‘£ğ‘—ğ‘—) is regarded as semantic similari-

ty importance from ğ‘£ğ‘£ğ‘–ğ‘–  to ğ‘£ğ‘£ğ‘—ğ‘— . It can be ex-
pressed as 

ğ‘¤ğ‘¤ğ‘ ğ‘ ğ‘ ğ‘ (ğ‘£ğ‘£ğ‘–ğ‘– ,ğ‘£ğ‘£ğ‘—ğ‘—) =
PMIï¿½ğ‘£ğ‘£ğ‘–ğ‘–,ğ‘£ğ‘£ğ‘—ğ‘—ï¿½

âˆ‘ PMI(ğ‘£ğ‘£ğ‘–ğ‘–,ğ‘£ğ‘£ğ‘”ğ‘”)ğ‘£ğ‘£ğ‘”ğ‘”âˆˆOutï¿½ğ‘£ğ‘£ğ‘–ğ‘–ï¿½
      (4) 

PMIï¿½ğ‘£ğ‘£ğ‘–ğ‘– ,ğ‘£ğ‘£ğ‘—ğ‘—ï¿½ï¼ log (
ğ‘ğ‘(ğ‘£ğ‘£ğ‘–ğ‘– & ğ‘£ğ‘£ğ‘—ğ‘—)
ğ‘ğ‘(ğ‘£ğ‘£ğ‘–ğ‘–)ğ‘ğ‘(ğ‘£ğ‘£ğ‘—ğ‘—)

)        (5) 

Where PMIï¿½ğ‘£ğ‘£ğ‘–ğ‘– ,ğ‘£ğ‘£ğ‘—ğ‘—ï¿½ is the point mutual infor-
mation between ğ‘£ğ‘£ğ‘–ğ‘–  and ğ‘£ğ‘£ğ‘—ğ‘— . The larger the PMI 
value is, the higher the semantic similarity is. 
ğ‘ğ‘(ğ‘£ğ‘£ğ‘–ğ‘–  & ğ‘£ğ‘£ğ‘—ğ‘—) is the co-occurrence probability of ğ‘£ğ‘£ğ‘–ğ‘– 
and ğ‘£ğ‘£ğ‘—ğ‘— in sentences. p(ğ‘£ğ‘£ğ‘–ğ‘–  ) and pï¿½ğ‘£ğ‘£ğ‘—ğ‘—  ï¿½ respective-
ly represent for the independent occurrence 
probability of ğ‘£ğ‘£ğ‘–ğ‘–  and ğ‘£ğ‘£ğ‘—ğ‘— . This function suggests 
that words with higher mutual information can 
substantially influence each other mutually.   
c) ğ‘¤ğ‘¤ğ‘™ğ‘™ğ‘¡ğ‘¡(ğ‘£ğ‘£ğ‘–ğ‘– , ğ‘£ğ‘£ğ‘—ğ‘—)  shows the topic-related im-

portance value of ğ‘£ğ‘£ğ‘–ğ‘–. It can be computed 
by 

ğ‘¤ğ‘¤ğ‘™ğ‘™ğ‘¡ğ‘¡(ğ‘£ğ‘£ğ‘–ğ‘– , ğ‘£ğ‘£ğ‘—ğ‘—) =
Pï¿½ğ‘£ğ‘£ğ‘—ğ‘—ï¿½

âˆ‘ ğ‘ƒğ‘ƒ(ğ‘£ğ‘£ğ‘”ğ‘”)ğ‘£ğ‘£ğ‘”ğ‘”âˆˆOutï¿½ğ‘£ğ‘£ğ‘–ğ‘–ï¿½
          (6) 

Where Pï¿½ğ‘£ğ‘£ğ‘—ğ‘—ï¿½  is the position importance 
score of ğ‘£ğ‘£ğ‘—ğ‘— which can be designed with differ-
ent strategies. Considering the importance of 
topical words in measuring position im-
portance score, this paper assigns words occur-
ring in topic or existing dependency with topi-
cal words a higher score than others. If we as-
sign â€œvertex v occurring in topic or existing 
dependency with topical wordsâ€ as X, the 
function is 

P(v) = ï¿½ Î»,   v âˆˆ X1, ğ‘œğ‘œğ‘œğ‘œâ„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’                       (7) 
Where ğœ†ğœ† >1. We set ğœ†ğœ† =1.5 through investi-

gation and evaluation in experiments. 
ï¬ Selection of Random Jumping Probability 

In the traditional graph model of TextRank, 
each vertex jumps to others randomly with an 
equal probability, which is shown in the function 
of ğ‘ğ‘ğ‘¡ğ‘¡ğ‘—ğ‘—(ğ‘¤ğ‘¤ğ‘–ğ‘–)ï¼ 

1
|ğ‘‰ğ‘‰|

. But this method will bring 
about the problem of local optimization for its 
negligence of topical information. Considering 
the importance of topical words in charactering 
the main idea of an article, we assign topic-
related words with a higher random jumping 
probability to get a larger score in ranking of 
graph model. Consequently, this paper adopts 

160



PMI value between current word and topical 
word as the random jumping probability, and the 
function is as follows. 

ğ‘ğ‘ğ‘¡ğ‘¡ğ‘—ğ‘—(ğ‘¤ğ‘¤ğ‘–ğ‘–) =  
ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘£ğ‘£ğ‘–ğ‘–,topic)

âˆ‘ ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘£ğ‘£ğ‘—ğ‘—,ğ‘™ğ‘™ğ‘ğ‘ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘)
|ğ‘‰ğ‘‰|
ğ‘—ğ‘—=1

             (8) 

where ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ¿ğ¿(ğ‘£ğ‘£ğ‘–ğ‘– , topic) denotes the point mutu-
al information value between current word ğ‘£ğ‘£ğ‘–ğ‘– 
and topical word topic, |ğ‘‰ğ‘‰| is the number of ver-
texes in graph model. Moreover, the calculation 
of co-occurrence probability for PMI is per-
formed in unit of sentence in global TextRank, 
but in unit of window in local TextRank. The size 
of the window is assigned as 5 through experi-
ments. 

Consequently, in the construction of graph 
model G = (V, E) , vertexes, directions and 
weights of the links are three important points 
which should be considered. In this graph model, 
we denote the vertexes set as 
V = {ğ‘£ğ‘£1, ğ‘£ğ‘£2, ğ‘£ğ‘£3 â€¦ â€¦ ğ‘£ğ‘£ğ‘›ğ‘›}  which is combined of 
nouns and adjectives. Furthermore, the direction 
of a link between two vertexes is determined by 
a method of sliding window which adds links 
from the first word pointing to other words with-
in the window. And the size of the sliding win-
dow is assigned as 10 through experiments. And 
the weight of a link is set by method of CST pro-
posed in this paper. The basic formula of Tex-
tRank is performed for calculating the final rank-
ing scores of each vertex. Finally, we can acquire 
two ranking scores for a vertex under global and 
local TextRank separately. 

2.3 Selection of topic-related sentimental 
feature 

In recent years, the method of word embedding 
based on neural network shows its outperfor-
mance in semantic expression and has attracted 
widespread attention paid to it(Tomas, 2013). 
The task of word embedding is to represent each 
word in corpus with a real vector, and establish-
ing a mapping between discrete vocabulary and 
the feature vectors in real fields. Considering the 
semantic similarity between two words can be 
characterized by cosine value of the vectors, we 
propose a novel approach of topic-related senti-
mental word embedding which integrates syntax 
with semantics in this paper. This method ex-
pands topical words with word embedding first, 
and then performs parsing in center of these topi-
cal words to extract topical-related sentiment 
words based on the dependencies with them. Fi-
nally we cluster the topical-related sentiment 

words using K-means clustering algorithm and 
select the number of words belonging to a cate-
gory in a microblog as the dimension values to 
finish the feature selection of this part. 
ï¬ Expansion of Topical-words 
For example, â€œä¸‰æ˜ŸS6çš„å¤–è§‚ä¸é”™ï¼Œä½†ç”µæ± ä¸
è¡Œã€‚â€. Its dependency analysis result is illustrat-
ed in Figure 3 as follows. 

 Figure 3. Example of dependency analysis result 

As we can see in Figure 3, the sentimental 
words  â€œä¸é”™â€, â€œä¸è¡Œâ€ do not exist dependencies 
with topical words â€œä¸‰æ˜Ÿâ€, â€œS6â€, but exist de-
pendencies with words â€œ å¤–è§‚ â€, â€œ ç”µæ±  â€ of 
SBV(å¤–è§‚,ä¸é”™), SBV(ç”µæ± ,ä¸è¡Œ). And these 
relationships also occupy a necessary place in 
topic-based sentiment analysis of Chinese mi-
croblog. So we should obtain â€œå¤–è§‚â€, â€œç”µæ± â€ as 
the expanded topical words from topical words 
â€œä¸‰æ˜Ÿâ€, â€œS6â€. 

There are many approaches to expand the top-
ical words such as PMI(Turney, 2003), and Syn-
onyms-based method(Wang, 2009). For its better 
consideration of contextual information, we 
adopt word embedding to calculate the semantic 
similarity with topical words to expand the topi-
cal words. After getting word vectors, we calcu-
late the cosine similarity between topical words 
and nouns under each topic, and select the high-
est N words as the expansion of topical words to 
fulfill the expansion of topical words. 
ï¬ Extraction of topic-related sentimental 

words 
As we all know, people usually express emotions 
towards a specific topic or object, and the emo-
tional words often exist dependency relationship 
with topics or objects in syntactic analysis. Con-
sequently, we mainly take following three de-
pendency relations into consideration: 
1) VOB 

â€œVOBâ€ represents for the relation between 
verbs and objects. Sentimental words are verbs 
and topical words are the objects of verbs. For 
example, â€œæˆ‘å–œæ¬¢ä¸‰æ˜Ÿã€‚â€. It exits â€œVOBâ€ rela-
tion between â€œå–œæ¬¢â€ and â€œä¸‰æ˜Ÿâ€. 
2) SBV 
â€œVOBâ€ represents for the relation between sub-

jects and predicates. Sentimental words are 
predicates and topical words are the subjects 

161



of sentimental words. For example, â€œä¸‰æ˜Ÿå¾ˆ
æ¼‚äº®ã€‚â€. It exits â€œSBVâ€ relation between â€œä¸‰
æ˜Ÿâ€ and â€œæ¼‚äº®â€. 

3) ATT 
â€œATTâ€ represents for the relation of attributes. 

Sentimental words are attributes and topical 
words are the modified center of sentimental 
words. For example, â€œæ— ä¸ä¼¦æ¯”çš„ä¸‰æ˜Ÿè®¾è®¡ï¼â€. 
It exits â€œATTâ€ relation between â€œæ— ä¸ä¼¦æ¯”â€ and 
â€œä¸‰æ˜Ÿâ€. 

Therefore, we design an algorithm of topical-
related sentimental words extraction towards de-
pendency analysis result of microblogs. The pro-
cess of this algorithm is described as below. 

Algorithm1: Topical-related Sentimental Words 
Extraction 
Input: Dependency analysis result(DP), Expand-
ed Topical Words(ETW)  
Output: Topical-related Sentimental Words 
(TSW) 
for word in DP: 

if word in ETW and word.relate in â€˜SBVâ€™, 
â€˜VOBâ€™, â€˜ATTâ€™: 

   TSW+= word.parent; 
if word.parent in ETW and word.relate in 

â€˜SBVâ€™, â€˜VOBâ€™, â€˜ATTâ€™: 
   TSW+= word; 

return TSW 

3 Experiments 
In SIGHAN8-Task2, we select emoticons, basic 
sentiment lexicon, dependency relation of â€œSBVâ€, 
â€œVOBâ€, â€œATTâ€ as common features(C), LT-IGT 
Ranking score as topic-related keyword fea-
ture(TK) and dependency parsing of topical 
words with word embedding for expansion as 
topic-related sentimental feature(TS).  

Table 1 shows the evaluation results of our 
system with different groups of features. 

By attempting different groups of feature for 
topic-related Chinese microblog sentiment classi-
fication, the performance of sentiment classifica-
tion is notably improved after adding topic-
related keyword feature(TK) and topic-related 
sentimental feature(TS). This is mainly because 
these two features explore both the syntactic and 
semantic information for classification compared 
with the other features. Consequently, this exper-
iment not only demonstrates the effectiveness of 
LT-IGT algorithm, but also reveals the im-
portance of topical word expansion to topic-
related Chinese microblog sentiment classifica-
tion. 

Method Precision Recall F-measure 
C 0.6113 0.5572 0.5830 
C+TK 0.6458 0.5982 0.6211 
C+TK+TS 0.6863 0.6081 0.6448 
Table 1: results of topic-based Chinese message po-
larity classification using SVM with different groups 
of features 

4 Conclusion 
In this paper we proposed a novel method for 
topic-based Chinese microblog sentiment classi-
fication, and put forward two novel feature gen-
eration approaches of LT-IGT and topic-related 
sentimental word embedding, with other kinds of 
features together, for addition to SVM classifier 
to perform the final polarity determination. The 
experimental results demonstrated the effective-
ness of these two proposed features, which re-
minds us deep processing on syntax and seman-
tics might be helpful for traditional regarded 
shallow works. 

To further improve the performance of our 
system, we will try to extend our work in the fol-
lowing aspects: 1) Perform phrase structure anal-
ysis on microblog to excavate the relation be-
tween topical and sentimental words; 2) Investi-
gate the impact on other classifiers other than 
SVM classifier. 

Acknowledgments 
We would like to thank SIGHAN8 for offering 
the dataset. We would also acknowledge the help 
of HIT-IR-Lab for proving the Chinese depend-
ency parser(Che, 2010). 

References 
Lun-wei Ku and Tung-ho Wu and Li-ying Lee and 

Hsin-hsi Chen. 2005. Construction of an Evaluation 
Corpus for Opinion Extraction. In Journal of 
NTCIR, pages 513--520, Taipei, Taiwan. 

Yang Shen, Shuchen Li, Ling Zheng, Xiaodong Ren 
and Xiaolong Cheng. 2009. Emotion mining re-
search on microblog. In Proceedings of Computer 
Science & Education (ICCSE), pages 477-480, 
LanZhou, China.  

Vasileios Hatzivassiloglou and Janyce M. Wiebe. 
2000. Effects of Adjective Orientation and Grada-
bility on Sentence Subjectivity. In Proceedings of 
the the 18th conference on Computational linguis-
tics - Volume 1. Association for Computational 
Linguistics, pages 299-305, New York.  

Turney P D. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Journal of Proc An-

162



nual Meeting of the Association for Computational 
Linguistics,pages 417--424. 

Xiaolong Wang Y and Furu Wei Z. 2011. Topic Sen-
timent Analysis in Twitter: A Graph-based Hashtag 
Sentiment Classification Approach. In Internation-
al Conference on Information & Knowledge Man-
agement Proceedings(2011). 

Pang B, Lee L, Vaithyanathan S. 2002. Thumbs up? 
Sentiment Classification using Machine Learning 
Techniques. Proceedings of Emnlp, pages: 79-86.  

Dasgupta, S., & Ng, V. 2009. Mine the Easy, Classify 
the Hard: A Semi-Supervised Approach to Auto-
matic Sentiment Classification. In Meeting of the 
Association for Computational Linguistics, pages 
701-709. 

Li F, Liu N, Jin H, et al. 2011. Incorporating Review-
er and Product Information for Review Rating Pre-
diction. In Proceedings of the twenty-second inter-
national joint conference on artificial intelligence, 
pages 1820-1825. 

Jakob N, Darmstadt T U, Gurevych I. 2010. Extract-
ing opinion targets in a single and cross-domain 
setting with conditional random fields. In Proceed-
ings of the 2010 Conference on Empirical Methods 
in Natural Language Processing, pages 427. 

Salton G, Yu C T. 1975. On the construction of effec-
tive vocabularies for information retrieval. In Pro-
ceedings of Acm Sigplan Notices, pages 48-60. 

Salton G, Fox E. 1983. Extended Boolean information 
retrieval. In Journal of Communications of the Acm, 
26(11), pages:1022-1036. 

Mihalcea R, Tarau P. 2004. TextRank: Bringing Or-
der into Texts In Proceedings of Unt Scholarly 
Works. 

Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey 
Dean. 2013. Efficient estimation of word represen-
tations in vector space. 

Turney P D, Littman M L. 2003. Measuring Praise 
and Criticism: Inference of Semantic Orientation 
from Association. In Journal of Acm Transactions 
on Information Systems , 21(4), pages:315-346. 

Wang S G, De-Yu L I, Wei Y J, et al. 2009. A Syno-
nyms Based Word Sentiment Orientation Discrimi-
nating. In Journal of Chinese Information Pro-
cessing, pages:68-74. 

Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. 
LTP: A Chinese Language Technology Platform. In 
Proceedings of the Coling 2010: Demonstration 
Volume, pages 13-16, Beijing, China. 

 

163


