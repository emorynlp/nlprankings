



















































Interpreting Pointing Gestures and Spoken Requests - A Probabilistic, Salience-based Approach


Coling 2010: Poster Volume, pages 1558–1566,
Beijing, August 2010

Interpreting Pointing Gestures and Spoken Requests – A Probabilistic,
Salience-based Approach

Ingrid Zukerman and Gideon Kowadlo and Patrick Ye
Faculty of Information Technology

Monash University
Ingrid.Zukerman@monash.edu,gkowadlo@gmail.com,ye.patrick@gmail.com

Abstract

We present a probabilistic, salience-based
approach to the interpretation of pointing
gestures together with spoken utterances.
Our mechanism models dependencies be-
tween spatial and temporal aspects of ges-
tures and features of utterances. For our
evaluation, we collected a corpus of re-
quests which optionally included point-
ing. Our results show that pointing infor-
mation improves interpretation accuracy.

1 Introduction

DORIS (Dialogue Oriented Roaming Interactive
System) is a spoken dialogue system designed for
a household robot. In (Zukerman et al., 2008),
we described Scusi? — a spoken language in-
terpretation module which considers multiple sub-
interpretations at different levels of the interpreta-
tion process, and estimates the probability of each
sub-interpretation at each level (Section 2). This
formalism is required for requests such as “Get
me the blue cup” in the context of the scene de-
picted in Figure 1, where possible candidates are
the three white cups, and the blue and purple tum-
blers, but it is unclear which is the intended object,
as none of the alternatives match the request per-
fectly.

In this paper, we integrate pointing gestures
into Scusi?’s probabilistic formalism. We adopt
a salience-based approach, where we take into ac-
count spatial and temporal information to estimate
the probability that a pointing gesture refers to an

Figure 1: Experimental Setup

object. To evaluate our formalism, we collected a
corpus of requests where people were allowed to
point (Section 4). Our results show that when peo-
ple point, our mechanism yields significant im-
provements in interpretation accuracy; and when
pointing was artificially added to utterances where
the people did not point, its effect on interpretation
accuracy was reduced.

This paper is organized as follows. Section 2
outlines the interpretation of a spoken request and
the estimation of the probability of an interpre-
tation. Section 3 describes how pointing affects
this probability. Our evaluation is detailed in Sec-
tion 4. Related research is discussed in Section 5,
followed by concluding remarks.

2 Interpreting Spoken Requests

Here we summarize our previous work on the in-
terpretation of single-sentence requests (Makalic
et al., 2008; Zukerman et al., 2008).

1558



Scusi? processes spoken input in three stages:
speech recognition, parsing and semantic inter-
pretation. First, Automatic Speech Recognition
(ASR) software (Microsoft Speech SDK 5.3) gen-
erates candidate hypotheses (texts) from a speech
signal. The ASR produces up to 50 texts for a
spoken utterance, where each text is associated
with a probability. In the parsing stage, the texts
are considered in descending order of probability.
Charniak’s probabilistic parser (ftp://ftp.cs.
brown.edu/pub/nlparser/) is applied to each
text, yielding up to 50 parse trees — each asso-
ciated with a probability.

During semantic interpretation, parse trees are
successively mapped into two representations
based on Concept Graphs (Sowa, 1984). First
Uninstantiated Concept Graphs (UCGs), and then
Instantiated Concept Graphs (ICGs). UCGs are
obtained from parse trees deterministically — one
parse tree generates one UCG. A UCG represents
syntactic information, where the concepts corre-
spond to the words in the parent parse tree, and
the relations are derived from syntactic informa-
tion in the parse tree and prepositions. Each UCG
can generate many ICGs. This is done by nomi-
nating different instantiated concepts and relations
from the system’s knowledge base as potential re-
alizations for each concept and relation in a UCG.
Instantiated concepts are objects and actions in the
domain (e.g., mug01, mug02 and cup01 are pos-
sible instantiations of the uninstantiated concept
“mug”), and instantiated relations are similar to
semantic role labels (Gildea and Jurafsky, 2002).
The interpretation process continues until a pre-
set number of sub-interpretations (including texts,
parse trees, UCGs and ICGs) has been generated
or all options have been exhausted.

Figure 2 illustrates a UCG and an ICG for the
request “get the large red folder on the table”. The
intrinsic features of an object (lexical item, colour
and size) are stored in the UCG node for this ob-
ject. Structural features, which involve two ob-
jects (e.g., “folder-on-table”), are represented as
sub-graphs of the UCG (and the ICG).

2.1 Estimating the probability of an ICG

Scusi? ranks candidate ICGs according to their
probability of being the intended meaning of a

Patient

get01

On

folder02

table01

get

object

SIZE          LARGE

LEX folder

 COLOUR  RED

on

LEX table

Utterance:

UCG

Get the large red folder on the table

ICG

Figure 2: UCG and ICG for a sample utterance

spoken utterance. Given a speech signal W and a
context C, the probability of an ICG I , Pr(I|W, C),
is proportional to
∑

Λ

Pr(T |W )·Pr(P |T )·Pr(U |P )·Pr(I|U, C) (1)

where T , P and U denote text, parse tree and
UCG respectively. The summation is taken over
all possible paths Λ = {P,U} from the speech
wave to the ICG, because a UCG and an ICG
can have more than one ancestor. As mentioned
above, the ASR and the parser return an esti-
mate of Pr(T |W ) and Pr(P |T ) respectively; and
Pr(U |P ) = 1, since the process of generating a
UCG from a parse tree is deterministic. The es-
timation of Pr(I|U, C) is described in (Zukerman
et al., 2008). Here we present the final equation
obtained for Pr(I|U, C), and outline the ideas in-
volved in its calculation.

Pr(I|U, C)≈
∏

k∈I
Pr(u|k, C) Pr(k|kp, kgp) Pr(k|C)

(2)
where u is a node in UCG U , k is the correspond-
ing instantiated node in ICG I , kp is k’s parent
node, and kgp is k’s grandparent node. For exam-
ple, On is the parent of table01, and folder02
the grandparent in the ICG in Figure 2.

• Pr(u|k) is the “match probability” between the
specifications for node u in UCG U and the in-
trinsic features of the corresponding node k in
ICG I , i.e., the probability that a speaker who
intended a particular object k gave the specifi-
cations in u.

1559



• Pr(k|kp, kgp) represents the structural proba-
bility of ICG I , where structural information
is simplified to node trigrams, e.g., whether
folder02 is On table01.

• Pr(k|C) is the probability of a concept in light
of the context, which includes information
about domain objects, actions and relations.

Scusi? handles three intrinsic features: lexical
item, colour and size; and two structural features:
ownership and several locative relations (e.g., on,
under, near). The match probability Pr(u|k) and
the structural probability Pr(k|kp, kgp) are esti-
mated using a distance function between the re-
quirements specified by the user and what is found
in reality — the closer the distance between the
specifications and reality, the higher the probabil-
ity (for details see (Makalic et al., 2008)).

3 Incorporating Pointing Gestures

Pointing affects the salience of objects and the
language used to refer to objects: objects in the
temporal and spatial vicinity of a pointing gesture
are more salient than objects that are farther away,
and pointing is often associated with demonstra-
tive determiners. Thus, the incorporation of point-
ing into Scusi? affects the following elements of
Equation 2 (Section 2.1).

• Pr(k|C) – the context-based probability of an
object (i.e., its salience) is affected by the time
of a pointing gesture and the space it encom-
passes. For instance, if the user says “Get the
cup” in the context of the scene in Figure 1,
pointing around the time s/he said “cup”, the
gesture most likely refers to an object that may
be called “cup”. Further, among the candidate
cups in Figure 1, those closer to the “pointing
vector” have a higher probability.1

• Pr(u|k, C) – when pointing, people often use
demonstrative determiners, e.g., “get me that
cup”. Also, people often use generic identifiers
in conjunction with demonstrative determiners

1At present, we assume that an utterance is associated
with at most one pointing gesture, and that pointing pertains
to objects. This assumption is supported by our user study
(Section 4.1).

to refer to unfamiliar objects, e.g., “that thing”
to refer to a vacuum tube (Figure 1).

These probabilities are estimated in Sec-
tions 3.1 and 3.2. Our calculations are based on
information returned by the gesture recognition
system described in (Li and Jarvis, 2009): gesture
type, time, probability and relevant parameters
(e.g., a vector for a pointing gesture). Since we fo-
cus on pointing gestures, we convert the probabil-
ities expected from Li and Jarvis’s system into the
probability of Pointing and that of Not Pointing,
which comprises all other gestures and no gesture
(these hypotheses are returned at the same time).2

3.1 Calculating salience from pointing
When pointing is taken into account, the probabil-
ity of object k is expressed as follows.

Pr(k|C) = Pr(k|P, C) · Pr(P|C) + (3)
Pr(k|¬P, C) · Pr(¬P|C)

where P designates Pointing, Pr(P|C) and its
complement are returned by the gesture recog-
nition system, and Pr(k|¬P, C) = 1N (N is the
number of objects in the room, i.e., in the absence
of pointing, we assume that all the objects in the
room are equiprobable3).

As indicated above, we posit that pointing is
spatially correlated with an intended object, and
temporally correlated with a word referring to the
intended object. Hence, we separate Pointing into
two components: spatial (s) and temporal (t), ob-
taining 〈Ps,Pt〉. Thus

Pr(k|P, C) = Pr(k,Pt,Ps, C)
Pr(P, C) (4)

=
Pr(Pt|k,Ps, C) · Pr(k|Ps, C) · Pr(Ps|C)

Pr(P|C)
We assume that given k, Pt is conditionally in-

dependent fromPs; and that Pr(Ps|C) = Pr(P|C),
i.e., the spatial probability of a pointing gesture is
the probability returned by the gesture system for
the entire pointing hypothesis (time and space).
This yields

Pr(k|P, C) = Pr(Pt|k, C) · Pr(k|Ps, C) (5)
2Owing to timing limitations of the gesture recognition

system (Section 4), we simulate its output.
3At present, we do not consider dialogue salience.

1560



v
e
c
to
r

p
o
in
ti
n
g

k
j

pd(k,PLine)

d
(k
,P
L
in
e
)

PLine

u
s
e
r

(a) Pointing

p
o
in
ti
n
g

v
e
c
to
r

O
Line

k
j

pd(j,O
Line)

d(j,OLine)

u
s
e
r

(b) Occlusion

Figure 3: Spatial pointing and occlusion

where Pr(k|Ps, C) and Pr(Pt|k, C) are estimated
as described in Section 3.1.1 and 3.1.2 respec-
tively. This equation is smoothed as follows (and
incorporated into Equation 3) to take into account
objects that are (spatially or temporally) excluded
from the pointing gesture.

Pr′(k|P, C) = Pr(k|P, C) +
1
N

1 +
∑N

j=1 Pr(kj |P, C)
(6)

3.1.1 Estimating Pr(k|Ps, C)
Pr(k|Ps, C), the probability that the user in-

tended object k when pointing to a location, is es-
timated using a conic Gaussian density function
around PLine, the Pointing Line created by ex-
tending the pointing vector returned by the gesture
identification system (Figure 3(a)).4

Pr(k|Ps, C) =
αθk√

2πσPs(pd)
e
− d(k,PLine)

2

2σ2
Ps

(pd) (7)

where α is a normalizing constant; σPs(pd) is the
standard deviation of the Gaussian cone as a func-
tion of pd(k,PLine), the projected distance be-
tween the user’s pointing hand and the projection
of object k on PLine; d(k,PLine) is the shortest
distance between the center of object k and PLine;
and θk is a factor that reduces the probability of
object k if it is (partially) occluded (Figure 3(b)).

The projected distance pd takes into account
the imprecision of pointing actions — a problem
that is exacerbated by the uncertainty associated
with sensing a pointing vector. A small angular

4Since this is a continuous density function, it does not
directly yield a point probability. Hence, it is normalized on
the basis of the largest possible returned value.

error in the detected pointing vector yields a dis-
crepancy in the distance between the pointing line
and candidate objects. This discrepancy increases
as pd(k,PLine) increases. To compensate for this
situation, we increase the variance of the Gaussian
distribution linearly with the projected distance
from the user’s hand (we start with a small stan-
dard deviation ofσ0 = 5 mm at the user’s fingers,
attributed to sensor error). This allows farther ob-
jects with a relatively high displacement from the
pointing vector to be encompassed in a pointing
gesture (e.g., the larger mug in Figure 3(a)), while
closer objects with the same displacement are ex-
cluded (e.g., the smaller mug). This yields the fol-
lowing equation for the variance.

σ2Ps(pd) = σ
2
0 +K · pd(k,PLine)

where K = 2.5 mm is an empirically determined
increase rate.

The occlusion factor θk reduces the probability
of objects as they become more occluded. We ap-
proximate θk by considering the objects that are
closer to the user than k, and estimating the extent
to which these objects occlude k (Figure 3(b)).
This estimate is a function of the position of these
objects and their size — the larger an intervening
object, the lower the probability that the user is
pointing at k. These factors are taken into account
as follows.

Pr(j occl k)=
γ√

2πσθ(pd)
e
− (d(j,OLine)−

1
2 dimmin(j))

2

2σ2
θ
(pd)

(8)
where γ is a normalizing constant; the numera-
tor of the exponent represents the maximum dis-
tance from the edge of object j to the line between
the user’s hand and object k, denoted Object Line
(OLine); and

σ2θ(pd) =
1

2

(
σ20 +K · pd(j,OLine)

)

represents the variance of a cone from the user’s
hand to object k as a function of distance. In order
to represent the idea that object j must be close
to the Object Line to occlude object k, we use
half the variance of that used for the “pointing
cone”, which yields a thinner “occlusion cone”
(Figure 3(b)). θk is then estimated as 1 minus the

1561



maximum occlusion caused by the objects that are
closer to the user than k.

θk=1− max∀j d(j,hand)<d(k,hand) {Pr(j occl k)} (9)

3.1.2 Estimating Pr(Pt|k, C)
Pr(Pt|k, C) is obtained as follows.

Pr(Pt|k, C) =
n∑

i=1

Pr(Pt, k,Wi, C)
Pr(k, C) (10)

=
n∑

i=1

Pr(k|Pt, wi, C)·Pr(T (wi)|Pt, C)·Pr(Pt|C)
Pr(k|C)

where n is the number of nouns in the user’s utter-
ance, and Wi = 〈wi, T (wi)〉 is a tuple comprising
the ith noun and the mid point of the time when it
was uttered.

We make the following assumptions.

• Pr(Pt|C) = 1, as all the gesture hypotheses
are returned at the same time;

• given Pt, the timing of a word T (wi) is condi-
tionally independent of C; and

• given wi, k is conditionally independent of
the timing of the pointing gesture Pt, i.e.,
Pr(k|Pt, wi, C) = Pr(k|wi, C).
This probability is represented as

Pr(k|wi, C) =
Pr(wi|k) · Pr(k|C)∑N

j=1 {Pr(wi|kj) · Pr(kj |C)}

where N is the number of objects.

These assumptions yield

Pr(Pt|k, C)=
n∑

i=1

Pr(wi|k) · Pr(T (wi)|Pt)∑N
j=1 {Pr(wi|kj)·Pr(kj |C)}

(11)
where Pr(T (wi)|Pt), the probability of the time
of word wi given the time of the pointing gesture,
is obtained from the following Gaussian time dis-
tribution for pointing.

Pr(T (wi)|Pt) =
β√

2πσPt
e
− (T(wi)−PTime)

2

2σ2
Pt (12)

where β is a normalizing constant, PTime is the
time of the gesture, and σPt is the standard de-
viation of the Gaussian density function, which is
currently set to 650 msec (based on our corpus).

As in our previous work (Makalic et al.,
2008), we estimate Pr(wi|k) using the Leacock
and Chodorow (1998) WordNet similarity metric.
This metric also yields a match probability be-
tween most objects and generic words like “ob-
ject, thing, here, there”, enabling us to handle re-
quests such as “Get that thing over there”.

3.2 Calculating the probability of a referring
expression

As mentioned in Section 2, the intrinsic features
previously considered in Scusi? are lexical item,
colour and size (Makalic et al., 2008). Pointing
affects referring expressions in that people may
point instead of generating complex descriptions,
they may employ demonstrative determiners to-
gether with generic terms such as “thing” (espe-
cially when they are unfamiliar with the name of
an object), and they may use demonstrative pro-
nouns. The first two behaviours were exhibited in
our user study (Section 4), but none of our trial
participants used demonstrative pronouns.

To incorporate pointing into the calculation of
Pr(u|k, C), we add determiners to Scusi?’s for-
malism for intrinsic features, which yields

Pr(u|k, C) = Pr(ulex, udet, ucolr, usize|k, C)
After adding weights for the intrinsic features

(inspired by (Dale and Reiter, 1995)), and making
some simplifying assumptions, we obtain

Pr(u|k, C) = (13)
Pr(ulex|k, C)wlex · Pr(udet|k, C)wdet ·
Pr(ucolr|k)wcolr · Pr(usize|ulex, k)wsize

The estimation of Pr(ulex|k, C), Pr(ucolr|k) and
Pr(usize|ulex, k) is described in (Makalic et al.,
2008). Here we focus on Pr(udet|k, C).
3.2.1 Estimating Pr(udet|k, C)

Pr(udet|k, C) is estimated as follows.

Pr(udet|k, C) =
Pr(k|udet, C) · Pr(udet|C)

Pr(k|C) (14)

=
Pr(k|udet, C)

Pr(k|C)

[
Pr(udet|P, C) · Pr(P|C)+
Pr(udet|¬P, C) · Pr(¬P|C)

]

1562



where det = {def article, indef article, de-
monstr this, demonstr that}; Pr(P|C) and
Pr(¬P|C) are returned by the gesture system;
Pr(udet|P, C) and Pr(udet|¬P, C) are obtained
from our corpus; and for now we assume that
Pr(k|udet, C) = Pr(k|C).5 This yields

Pr(udet|k, C) = Pr(udet|P, C) · Pr(P|C) + (15)
Pr(udet|¬P, C) · Pr(¬P|C)

4 Evaluation

To obtain a corpus, we conducted a user study
whereby we set up a room with labeled objects
(Figure 1), and asked trial participants to request
12 selected items from DORIS (the room included
33 items in total, including distractors, and one of
the authors pretended to be DORIS). The objects
were selected and laid out in the room to reflect
a variety of conditions, e.g., common and rare
objects (e.g., vacuum tube); unique, non-unique
and similar objects (e.g., white cups); and objects
placed near each other and far from each other.

We divided our corpus of requests into two
parts: with and without pointing. Scusi?’s perfor-
mance was tested on input obtained from the ASR
and on textual input (perfect ASR). We consid-
ered two scenarios for each sub-corpus: Pointing,
where our pointing mechanism was activated on
the basis of a simulated pointing gesture,6 and No-
Pointing, where no pointing gesture was detected.
This was done in order to test two hypotheses:
(1) when people point, pointing information im-
proves interpretation performance; and (2) when
they do not point, even perfect pointing has little
effect on interpretation performance.

Scusi? was set to generate at most 300 sub-
interpretations in total (including texts, parse
trees, UCGs and ICGs) for each spoken request,
and at most 200 sub-interpretations for each tex-
tual request. On average, Scusi? takes 10 seconds
to go from texts to ICGs. An interpretation was

5In the future, we will incorporate distance from the user
to refine the probabilities of determiners.

6At present, we assume accurate pointing and gesture de-
tection, and precise information regarding the position of ob-
jects. In the near future, we will study the sensitivity of our
mechanism to pointing inaccuracies, and to errors in gesture
detection and scene analysis.

deemed successful if it correctly represented the
speaker’s intention, which was encoded in one or
more Gold ICGs. These ICGs were manually con-
structed on the basis of the requested objects and
the participants’ utterances. Multiple Gold ICGs
were allowed if there were several suitable actions
and objects.

4.1 The Corpus

19 people participated in the trial, generating a to-
tal of 276 requests, of which 136 involved point-
ing gestures (3 participants were asked to repeat
the experiment after it became clear that they were
refraining from pointing, as they erroneously as-
sumed they were not allowed to gesture). We fil-
tered out 64 requests, which included concepts
our system cannot yet handle, specifically “the
end of the table”, projective modifiers (e.g., “be-
hind/left”), ordinals (“first/second”), references to
groups of things (e.g., “six blue pens”), and zero-
and one-anaphora. This yielded 212 requests, of
which 105 involved pointing gestures.

In addition, the software we used has the fol-
lowing limitations: the gesture recognition sys-
tem (Li and Jarvis, 2009) requires users to hold
a gesture for 2 seconds, and the ASR system is
speaker dependent and cannot recognize certain
words (e.g., “mug”, “bowl” and “pen”). To cir-
cumvent these problems, each pointing gesture
was manually encoded into a time-stamped vec-
tor; and one of the authors read slightly sanitized
versions of participants’ utterances into the ASR:
“can you”, “please” and “DORIS” were omitted;
long prepositional phrases were shortened (e.g.,
“the thing with wires sticking out of it”); and
words that were problematic for the ASR were re-
placed (e.g., “pencil” was used instead of “pen”).

There was some difference in the length of re-
quests with and without pointing, but it wasn’t as
pronounced as reported in (Johnston et al., 2002):
requests with/without pointing had 5.84/6.27
words on average. ASR performance was worse
for the requests that had pointing, with the top
ASR interpretation being correct for only 46%
of these requests, compared to 57.5% for the re-
quests without pointing. This difference may be
attributed to the ASR having trouble with sen-
tence constructs associated with pointing. Overall

1563



% Gold ICGs in Avg adj rank % Not Avg adj rank % Not
top 1 top 3 (rank) found (rank) 20 found 20

Sub-corpus without pointing
Text, Scusi?-NoPointing 89.7 93.5 4.39 (0.78) 0.9 1.18 (0.13) 4.7
Text, Scusi?-Pointing 86.9 87.9 3.28 (1.89) 0.9 0.39 (0.35) 4.7
ASR, Scusi?-NoPointing 81.3 85.0 4.67 (0.83) 7.5 1.24 (0.17) 12.1
ASR, Scusi?-Pointing 79.4 81.3 5.00 (2.62) 5.6 0.46 (0.40) 12.1
Sub-corpus with pointing
Text, Scusi?-NoPointing 84.8 89.5 3.54 (0.59) 4.8 1.48 (0.20) 9.5
Text, Scusi?-Pointing 82.9 86.7 4.19 (1.63) 1.9 0.41 (0.29) 7.6
ASR, Scusi?-NoPointing 76.2 82.9 7.93 (0.95) 10.5 1.79 (0.27) 15.2
ASR, Scusi?-Pointing 73.3 81.0 8.65 (2.76) 8.6 0.68 (0.40) 14.3

Table 1: Scusi?’s interpretation performance

the ASR returned the correct interpretation, at any
rank, for 88% of the requests.

4.2 Results

Table 1 summarizes our results. Column 1 dis-
plays the test condition (sub-corpus with/without
pointing, text/ASR, and with/without Scusi?’s
pointing mechanism). Columns 2-3 show the per-
centage of utterances that had Gold ICGs whose
probability was among the top 1 and top 3, e.g.,
in the sub-corpus with pointing, when Scusi?-
Pointing was run on text, 82.9% of the utter-
ances had Gold ICGs with the highest probability
(top 1). The average adjusted rank (AR) and av-
erage rank of the Gold ICG appear in Column 4.
The rank of an ICG I is its position in a list sorted
in descending order of probability (starting from
position 0), such that all equiprobable ICGs are
deemed to have the same position. The adjusted
rank of an ICG I is the mean of the positions of
all ICGs that have the same probability as I . For
example, if we have 4 equiprobable ICGs in po-
sitions 0-3, each has a rank of 0, but an adjusted
rank of rbest+rworst2 = 1.5. Column 5 shows the
percentage of utterances that didn’t yield a Gold
ICG. Column 6 shows the average AR for inter-
pretations with AR < 20 (and their average rank),
and Column 7 shows the percentage of utterances
that had AR ≥ 20 or were not found. We dis-
tinguish between Gold ICGs with ARs 0 to 19
and total Gold ICGs that were found, because a
dialogue manager is likely to inspect the promis-

ing options, i.e., those with AR < K (we assume
K = 20). In addition, there is normally a trade-
off between the number of Not Found Gold ICGs
and average AR. ICGs that are not found by one
approach but are found by another approach typi-
cally have a high (bad) rank when they are even-
tually found (Zukerman et al., 2008). Thus, an ap-
proach that fails to find such “difficult” ICGs usu-
ally yields a lower average AR than an approach
that finds these ICGs. Capping the ARs of the
found Gold ICGs at 20 clarifies the trade-off be-
tween average AR and Not Found.

Our results show that, as expected, the main
role of pointing is in referent disambiguation.
This is evident from the significant reduction in
average AR-20 (Column 6) for the pointing and
no-pointing sub-corpora, under the text/ASR in-
put conditions. All the differences are statistically
significant with p < 0.01.7 Nonetheless, the im-
provements in average AR-20 obtained by artifi-
cially introduced pointing in the no-pointing sub-
corpus are smaller for both text and ASR than the
improvements obtained with actual pointing. We
posit that this smaller impact is due to the fact that
utterances without pointing are more descriptive
than those with pointing, hence benefitting less
from the disambiguating effect of pointing.

The Pointing condition has a seemingly adverse
effect on the number of interpretations with top
ranks (Columns 2-3). This is explained by the fact

7The differences were calculated using a paired t-test for
all the Gold ICGs that were found in both configurations.

1564



that all equiprobable interpretations have the same
rank, which happens more often under the No-
Pointing condition than under the Pointing con-
dition (as pointing has a disambiguating effect).

Finally, under all conditions, the rank of the re-
quest at the 75%-ile is 0, which indicates cred-
itable performance. The larger number of Not
Found Gold ICGs for the ASR condition is ex-
pected, as the ASR failed to find 12% of the cor-
rect texts on average, performing worse for the
pointing sub-corpus. The other Not Found Gold
ICGs were mainly due to parsing preferences, and
multiple parses for some utterances that had the
word “thing” (which matched all objects).

5 Related Research

Gesture recognition systems endeavour to detect
the gesture being made. Common approaches in-
clude Hidden Markov Models, e.g., (Nickel and
Stiefelhagen, 2003), and Finite State Machines,
e.g., (Li and Jarvis, 2009). Systems that focus
on pointing also identify the target object, with-
out recognizing the type of this object (Nickel and
Stiefelhagen, 2003; Li and Jarvis, 2009).

Most of the research in gesture and speech in-
tegration focuses on pointing gestures, employ-
ing speech as the main input modality, and us-
ing semantic fusion to combine spoken input with
gesture. Different approaches are used for ges-
ture detection, e.g., vision (Stiefelhagen et al.,
2004; Brooks and Breazeal, 2006) and sensor
glove (Corradini et al., 2002); and for language
interpretation, e.g., dedicated grammars (Stiefel-
hagen et al., 2004; Brooks and Breazeal, 2006)
and keywords (Einstein and Christoudias, 2004).
Fusion is variously implemented using heuristics
based on temporal overlap (Bolt, 1980; Johnston
et al., 2002), querying a gesture-sensing module
when ambiguous referents are identified (Fransen
et al., 2007), or unification to determine which
elements can be merged (Corradini et al., 2002;
Stiefelhagen et al., 2004). These are some-
times combined with search techniques coupled
with penalties (Einstein and Christoudias, 2004;
Brooks and Breazeal, 2006). With the exception
of Bolt’s system, these systems were tested on ut-
terances that were quite short and constrained.

Our approach integrates spatial and temporal

aspects of gesture into our probabilistic formal-
ism (Zukerman et al., 2008), focusing on the ef-
fect of pointing on object salience. Other salience-
based approaches are described in (Einstein and
Christoudias, 2004; Huls et al., 1995). How-
ever, they are not directly comparable with our
approach, as they use salience to weigh the im-
portance of factors pertaining to gesture-speech
alignment, but there is no uncertainty associated
with the visual salience resulting from pointing.

Our use of a probabilistic parser enables us
to handle more complex utterances than those
considered by most speech-gesture systems (Sec-
tion 2). At the same time, we do not yet handle
speech disfluencies, which are currently handled
by (Einstein and Christoudias, 2004; Stiefelhagen
et al., 2004). Also, at present we do not consider
the challenges pertaining to the real-time synchro-
nization of the output of a gesture-sensing and
a speech-recognition system (Stiefelhagen et al.,
2004; Brooks and Breazeal, 2006).

6 Conclusion and Future Work

We have extended Scusi?, our spoken language in-
terpretation system, to incorporate pointing ges-
tures. Specifically, we have offered a formalism
that takes into account relationships between as-
pects of gesture and spoken language to integrate
information about pointing gestures into the es-
timation of the probability of candidate interpre-
tations of an utterance. Our empirical evaluation
shows that our formalism significantly improves
interpretation accuracy.

In the future, we propose to refine our model
of demonstrative determiners. We also intend to
perform sensitivity analysis regarding the accu-
racy of the vision system, and that of the gesture
recognition system. In addition, we will conduct
user studies to gain insights with respect to con-
ditions that influence the probability of pointing,
e.g., type of object and its position relative to the
speaker.

Acknowledgments

This research was supported in part by ARC grant
DP0878195. The authors thank R. Jarvis and D.
Li for their help with the gesture system.

1565



References
Bolt, R.A. 1980. “Put-that-there”: voice and ges-

ture at the graphics interface. In Proceedings of
the 7th Annual Conference on Computer Graphics
and Interactive Techniques, pages 262–270, Seattle,
Washington.

Brooks, A.G. and C. Breazeal. 2006. Working with
robots and objects: Revisiting deictic reference for
achieving spatial common ground. In Proceedings
of the 1st ACM SIGCHI/SIGART Conference on
Human-robot Interaction, pages 297–304, Salt Lake
City, Utah.

Corradini, A., R.M. Wesson, and P.R. Cohen. 2002.
A Map-Based system using speech and 3D gestures
for pervasive computing. In ICMI’02 – Proceedings
of the 4th International Conference on Multimodal
Interfaces, pages 191–196, Pittsburgh, Pennsylva-
nia.

Dale, R. and E. Reiter. 1995. Computational in-
terpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18(2):233–263.

Einstein, J. and C.M. Christoudias. 2004. A salience-
based approach to gesture-speech alignment. In
Proceedings of the Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 25–32, Boston, Mas-
sachusetts.

Fransen, B., V. Morariu, E. Martinson, S. Blis-
ard, M. Marge, S. Thomas, A. Schultz, and
D. Perzanowski. 2007. Using vision, acoustics, and
natural language for disambiguation. In Proceed-
ings of the ACM/IEEE International Conference on
Human-robot Interaction, pages 73–80, Washing-
ton, DC.

Gildea, D. and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245–288.

Huls, C., W. Claassen, and E. Bos. 1995. Automatic
referent resolution of deictic and anaphoric expres-
sions. Computational Linguistics, 21(1):59–79.

Johnston, M., S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Maloor.
2002. MATCH: an architecture for multimodal di-
alogue systems. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 376–383, Philadelphia, Pennsylvania.

Leacock, C. and M. Chodorow. 1998. Combining lo-
cal context and WordNet similarity forword sense
identification. In Fellbaum, C., editor, WordNet: An
Electronic Lexical Database, pages 265–285. MIT
Press.

Li, Z. and R. Jarvis. 2009. Real time hand gesture
recognition using a range camera. In Proceedings
of the Australasian Conference on Robotics and Au-
tomation, Sydney, Australia.

Makalic, E., I. Zukerman, M. Niemann, and
D. Schmidt. 2008. A probabilistic model for under-
standing composite spoken descriptions. In PRICAI
2008 – Proceedings of the 10th Pacific Rim Interna-
tional Conference on Artificial Intelligence, pages
750–759, Hanoi, Vietnam.

Nickel, K. and R. Stiefelhagen. 2003. Pointing
gesture recognition based on 3D-tracking of face,
hands and head orientation. In ICMI’03 – Pro-
ceedings of the 5th International Conference on
Multimodal Interfaces, pages 140–146, Vancouver,
British Columbia.

Sowa, J.F. 1984. Conceptual Structures: Information
Processing in Mind and Machine. Addison-Wesley,
Reading, MA.

Stiefelhagen, R., C. Fugen, R. Gieselmann,
H. Holzapfel, K. Nickel, and A. Waibel. 2004.
Natural human-robot interaction using speech, head
pose and gestures. In IROS 2004 – Proceedings
of the IEEE/RSJ International Conference on
Intelligent Robots and Systems, volume 3, pages
2422–2427, Sendai, Japan.

Zukerman, I., E. Makalic, M. Niemann, and S. George.
2008. A probabilistic approach to the interpreta-
tion of spoken utterances. In PRICAI 2008 – Pro-
ceedings of the 10th Pacific Rim International Con-
ference on Artificial Intelligence, pages 581–592,
Hanoi, Vietnam.

1566


