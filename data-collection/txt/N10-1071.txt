










































Learning Dense Models of Query Similarity from User Click Logs


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474–482,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Learning Dense Models of Query Similarity from User Click Logs

Fabio De Bona∗
Friedrich Miescher Laboratory

of the Max Planck Society
Tübingen, Germany

fabio@tuebingen.mpg.de

Stefan Riezler
Google Research

Zürich, Switzerland
riezler@google.com

Keith Hall
Google Research

Zürich, Switzerland
kbhall@google.com

Massimiliano Ciaramita
Google Research

Zürich, Switzerland
massi@google.com

Amaç Herdaǧdelen∗
University of Trento

Rovereto, Italy
amac@herdagdelen.com

Maria Holmqvist∗
Linkopings University
Linkopings, Sweden
marho@ida.liu.se

Abstract

The goal of this work is to integrate query
similarity metrics as features into a dense
model that can be trained on large amounts
of query log data, in order to rank query
rewrites. We propose features that incorpo-
rate various notions of syntactic and semantic
similarity in a generalized edit distance frame-
work. We use the implicit feedback of user
clicks on search results as weak labels in train-
ing linear ranking models on large data sets.
We optimize different ranking objectives in a
stochastic gradient descent framework. Our
experiments show that a pairwise SVM ranker
trained on multipartite rank levels outperforms
other pairwise and listwise ranking methods
under a variety of evaluation metrics.

1 Introduction

Measures of query similarity are used for a wide
range of web search applications, including query
expansion, query suggestions, or listings of related
queries. Several recent approaches deploy user
query logs to learn query similarities. One set of ap-
proaches focuses on user reformulations of queries
that differ only in one phrase, e.g., Jones et al.
(2006). Such phrases are then identified as candi-
date expansion terms, and filtered by various signals
such as co-occurrence in similar sessions, or log-
likelihood ratio of original and expansion phrase.
Other approaches focus on the relation of queries
and search results, either by clustering queries based

∗The work presented in this paper was done while the au-
thors were visiting Google Research, Zürich.

on their search results, e.g., Beeferman and Berger
(2000), or by deploying the graph of queries and re-
sults to find related queries, e.g., Sahami and Heil-
man (2006).

The approach closest to ours is that of Jones et al.
(2006). Similar to their approach, we create a train-
ing set of candidate query rewrites from user query
logs, and use it to train learners. While the dataset
used in Jones et al. (2006) is in the order of a few
thousand query-rewrite pairs, our dataset comprises
around 1 billion query-rewrite pairs. Clearly, man-
ual labeling of rewrite quality is not feasible for our
dataset, and perhaps not even desirable. Instead, our
intent is to learn from large amounts of user query
log data. Such data permit to learn smooth mod-
els because of the effectiveness of large data sets to
capture even rare aspects of language, and they also
are available as in the wild, i.e., they reflect the ac-
tual input-output behaviour that we seek to automate
(Halevy et al., 2009). We propose a technique to au-
tomatically create weak labels from co-click infor-
mation in user query logs of search engines. The
central idea is that two queries are related if they
lead to user clicks on the same documents for a large
amount of documents. A manual evaluation of a
small subset showed that a determination of positive
versus negative rewrites by thresholding the number
of co-clicks correlates well with human judgements
of similarity, thus justifying our method of eliciting
labels from co-clicks.

Similar to Jones et al. (2006), the features of our
models are not based on word identities, but instead
on general string similarity metrics. This leads to
dense rather than sparse feature spaces. The dif-

474



ference of our approach to Jones et al. (2006) lies
in our particular choice of string similarity metrics.
While Jones et al. (2006) deploy “syntactic” fea-
tures such as Levenshtein distance, and “semantic”
features such as log-likelihood ratio or mutual in-
formation, we combine syntactic and semantic as-
pects into generalized edit-distance features where
the cost of each edit operation is weighted by vari-
ous term probability models.

Lastly, the learners used in our approach are appli-
cable to very large datasets by an integration of lin-
ear ranking models into a stochastic gradient descent
framework for optimization. We compare several
linear ranking models, including a log-linear prob-
ability model for bipartite ranking, and pairwise and
listwise SVM rankers. We show in an experimen-
tal evaluation that a pairwise SVM ranker trained on
multipartite rank levels outperforms state-of-the-art
pairwise and listwise ranking methods under a vari-
ety of evaluation metrics.

2 Query Similarity Measures

2.1 Semantic measures

In several of the similarity measures we describe be-
low, we employ pointwise mutual information (PMI)
as a measure of the association between two terms or
queries. Let wi and wj be two strings that we want
to measure the amount of association between. Let
p(wi) and p(wj) be the probability of observing wi
and wj in a given model; e.g., relative frequencies
estimated from occurrence counts in a corpus. We
also define p(wi, wj) as the joint probability of wi
and wj ; i.e., the probability of the two strings occur-
ring together. We define PMI as follows:

PMI(wi, wj) = log
p(wi, wj)

p(wi)p(wj)
. (1)

PMI has been introduced by Church and Hanks
(1990) as word assosiatio ratio, and since then
been used extensively to model semantic similar-
ity. Among several desirable properties, it correlates
well with human judgments (Recchia and Jones,
2009).

2.2 Taxonomic normalizations

As pointed out in earlier work, query transitions tend
to correlate with taxonomic relations such as gener-

alization and specialization (Lau and Horvitz, 1999;
Rieh and Xie, 2006). Boldi et al. (2009) show how
knowledge of transition types can positively impact
query reformulation. We would like to exploit this
information as well. However, rather than building a
dedicated supervised classifier for this task we try to
capture it directly at the source. First, we notice how
string features; e.g., length, and edit distance already
model this phenomenon to some extent, and in fact
are part of the features used in Boldi et al. (2009).
However, these measures are not always accurate
and it is easy to find counterexamples both at the
term level (e.g., “camping” to “outdoor activities” is
a generalization) and character level (“animal pic-
tures” to “cat pictures” is a specialization). Sec-
ondly, we propose that by manipulating PMI we can
directly model taxonomic relations to some extent.

Rather than using raw PMI values we re-
normalize them. Notice that it is not obvious in our
context how to interpret the relation between strings
co-occurring less frequently than random. Such
noisy events will yield negative PMI values since
p(wi, wj) < p(wi)p(wj). We enforce zero PMI val-
ues for such cases. If PMI is thus constrained to
non-negative values, normalization will bound PMI
to the range between 0 and 1.

The first type of normalization, called joint nor-
malization, uses the negative log joint probability
and is defined as

PMI(J)(wi, wj) = PMI(wi, wj)/−log(p(wi, wj)).

The jointly normalized PMI(J) is a symmetric
measure between wi and wj in the sense that
PMI(J)(wi, wj) = PMI(J)(wj , wi). Intuitively it
is a measure of the amount of shared information
between the two strings relative to the sum of indi-
vidual strings information. The advantages of the
joint normalization of PMI have been noticed be-
fore (Bouma, 2009).

To capture asymmetries in the relation between
two strings, we introduce two non-symmetric nor-
malizations which also bound the measure between
0 and 1. The second normalization is called special-
ization normalization and is defined as

PMI(S)(wi, wj) = PMI(wi, wj)/− log(p(wi)).

The reason we call it specialization is that PMI(S)
favors pairs where the second string is a specializa-

475



tion of the first one. For instance, PMI(S) is at its
maximum when p(wi, wj) = p(wj) and that means
the conditional probability p(wi|wj) is 1 which is an
indication of a specialization relation.

The last normalization is called the generalization
normalization and is defined in the reverse direction
as

PMI(G)(wi, wj) = PMI(wi, wj)/− log(p(wj)).

Again, PMI(G) is a measure between 0 and 1 and is
at its maximum value when p(wj |wi) is 1.

The three normalizations provide a richer rep-
resentation of the association between two strings.
Furthermore, jointly, they model in an information-
theoretic sense the generalization-specialization di-
mension directly. As an example, for the query
transition “apple” to “mac os” PMI(G)=0.2917 and
PMI(S)=0.3686; i.e., there is more evidence for a
specialization. Conversely for the query transition
“ferrari models” to “ferrari” we get PMI(G)=1 and
PMI(S)=0.5558; i.e., the target is a “perfect” gener-
alization of the source1.

2.3 Syntactic measures
Let V be a finite vocabulary and ξ be the null
symbol. An edit operation: insertion, deletion or
substitution, is a pair (a, b) ∈ {V ∪ {ξ} × V ∪
{ξ}} \ {(ξ, ξ)}. An alignment between two se-
quences wi and wj is a sequence of edit oper-
ations ω = (a1, b1), ..., (an, bn). Given a non-
negative cost function c, the cost of an alignment is
c(ω) =

∑n
i=1 c(ωi). The Levenshtein distance, or

edit distance, defined over V , dV (wi, wj) between
two sequences is the cost of the least expensive se-
quence of edit operations which transforms wi into
wj (Levenshtein, 1966). The distance computation
can be performed via dynamic programming in time
O(|wi||wj |). Similarity at the string, i.e., character
or term, level is an indicator of semantic similar-
ity. Edit distance captures the amount of overlap be-
tween the queries as sequences of symbols and has
been previously used in information retrieval (Boldi
et al., 2009; Jones et al., 2006).

We use two basic Levenshtein distance models.
The first, called Edit1 (E1), employs a unit cost func-
tion for each of the three operations. That is, given

1The values are computed from Web counts.

a finite vocabulary T containing all terms occurring
in queries:

∀a, b ∈ T, cE1(a, b) = 1 if(a 6= b), 0 else.

The second, called Edit2 (E2), uses unit costs for
insertion and deletion, but computes the character-
based edit distance between two terms to decide on
the substitution cost. If two terms are very similar
at the character level, then the cost of substitution is
lower. Given a finite vocabulary T of terms and a
finite vocabulary A of characters, the cost function
is defined as:

∀a, b ∈ T, cE2(a, b) = dA(a, b) ifa ∧ b 6= ξ, 1 else.

where dA(a, b) is linearly scaled between 0 and 1
dividing by max(|a|, |b|).

We also investigate a variant of the edit distance
algorithm in which the terms in the input sequences
are sorted, alphabetically, before the distance com-
putation. The motivation behind this variant is the
observation that linear order in queries is not always
meaningful. For example, it seems reasonable to as-
sume that “brooklyn pizza” and “pizza brooklyn”
denote roughly the same user intent. However, the
pair has an edit distance of two (delete-insert), while
the distance between “brooklyn pizza” and the less
relevant “brooklyn college” is only one (substitute).
The sorted variant relaxes the ordering constraint.

2.4 Generalized measures

In this section we extend the edit distance frame-
work introduced in Section 2.3 with the semantic
similarity measures described in Section 2.1, using
the taxonomic normalizations defined in Section 2.2.

Extending the Levenshtein distance framework
to take into account semantic similarities between
terms is conceptually simple. As in the Edit2 model
above we use a modified cost function. We introduce
a cost matrix encoding individual costs for term sub-
stitution operations; the cost is defined in terms of
the normalized PMI measures of Section 2.2, recall
that these measures range between 0 and 1. Given a
normalized similarity measure f , an entry in a cost
matrix S for a term pair (wi, wj) is defined as:

s(wi, wj) = 2− 2f(wi, wj) + �

476



We call these models SEdit (SE), where S specifies
the cost matrix used. Given a finite term vocabulary
T and cost matrix S, the cost function is defined as:

∀a, b ∈ T, cSE(a, b) = s(a, b) ifa ∧ b 6= ξ, 1 else.

The cost function has the following properties.
Since insertion and deletion have unit cost, a term
is substituted only if a substitution is “cheaper” than
deleting and inserting another term, namely, if the
similarity between the terms is not zero. The �
correction, coupled with unit insertion and deletion
cost, guarantees that for an unrelated term pair a
combination of insertion and deletion will always be
less costly then a substitution. Thus in the compu-
tation of the optimal alignment, each operation cost
ranges between 0 and 2.

As a remark on efficiency, we notice that here the
semantic similarities are computed between terms,
rather than full queries. At the term level, caching
techniques can be applied more effectively to speed
up feature computation. The cost function is imple-
mented as a pre-calculated matrix, in the next sec-
tion we describe how the matrix is estimated.

2.5 Cost matrix estimation

In our experiments we evaluated two different
sources to obtain the PMI-based cost matrices. In
both cases, we assumed that the cost of the substitu-
tion of a term with itself (i.e. identity substitution)
is always 0. The first technique uses a probabilis-
tic clustering model trained on queries and clicked
documents from user query logs. The second model
estimates cost matrices directly from user session
logs, consisting of approximately 1.3 billion U.S.
English queries. A session is defined as a sequence
of queries from the same user within a controlled
time interval. Let qs and qt be a query pair observed
in the session data where qt is issued immediately
after qs in the same session. Let q′s = qs \ qt and
q′t = qt \ qs, where \ is the set difference opera-
tor. The co-occurrence count of two terms wi and
wj from a query pair qs, qt is denoted by ni,j(qs, qt)
and is defined as:

ni,j(qs, qt) =


1 if wi = wj ∧ wi ∈ qs ∧ wj ∈ qt
1/(|q′s| |q′t|) if wi ∈ q′s ∧ wj ∈ q′t
0 else.

In other words, if a term occurs in both queries,
it has a co-occurrence count of 1. For all other term
pairs, a normalized co-occurrence count is computed
in order to make sure the sum of co-occurrence
counts for a term wi ∈ qs sums to 1 for a given
query pair. The normalization is an attempt to avoid
the under representation of terms occurring in both
queries.

The final co-occurrence count of two arbitrary
terms wi and wj is denoted by Ni,j and it is defined
as the sum over all query pairs in the session logs,
Ni,j =

∑
qs,qt

ni,j(qs, qt). Let N =
∑

wi,wj
Ni,j be

the sum of co-occurrence counts over all term pairs.
Then we define a joint probability for a term pair as
p(wi, wj) =

Ni,j
N . Similarly, we define the single-

occurrence counts and probabilities of the terms
by computing the marginalized sums over all term
pairs. Namely, the probability of a termwi occurring
in the source query is p(i, ·) =

∑
wj
Ni,j/N and

similarly the probability of a term wj occurring in
the target query is p(·, j) =

∑
wi
Ni,j/N . Plugging

in these values in Eq. (1), we get the PMI(wi, wj)
for term pair wi and wj , which are further normal-
ized as described in Section 2.2.

More explanation and evaluation of the features
described in this section can be found in Ciaramita
et al. (2010).

3 Learning to Rank from Co-Click Data

3.1 Extracting Weak Labels from Co-Clicks

Several studies have shown that implicit feedback
from clickstream data is a weaker signal than human
relevance judgements. Joachims (2002) or Agrawal
et al. (2009) presented techniques to convert clicks
into labels that can be used for machine learning.
Our goal is not to elicit relevance judgments from
user clicks, but rather to relate queries by pivoting on
commonly clicked search results. The hypothesis is
that two queries are related if they lead to user clicks
on the same documents for a large amount of docu-
ments. This approach is similar to the method pro-
posed by Fitzpatrick and Dent (1997) who attempt
to measure the relatedness between two queries by
using the normalized intersection of the top 200 re-
trieval results. We add click information to this
setup, thus strengthening the preference for preci-
sion over recall in the extraction of related queries.

477



Table 1: Statistics of co-click data sets.
train dev test

number of queries 250,000 2,500 100
average number of
rewrites per query 4,500 4,500 30

percentage of rewrites
with ≥ 10 coclicks 0.2 0.2 43

In our experiments we created two ground-truth
ranking scenarios from the co-click signals. In a first
scenario, called bipartite ranking, we extract a set
of positive and a set of negative query-rewrite pairs
from the user logs data. We define positive pairs as
queries that have been co-clicked with at least 10 dif-
ferent results, and negative pairs as query pairs with
fewer than 10 co-clicks. In a second scenario, called
multipartite ranking, we define a hierarchy of levels
of “goodness”, by combining rewrites with the same
number of co-clicks at the same level, with increas-
ing ranks for higher number of co-clicks. Statistics
on the co-click data prepared for our experiments are
given in Table 1.

For training and development, we collected
query-rewrite pairs from user query logs that con-
tained at least one positive rewrite. The training set
consists of about 1 billion of query-rewrite pairs; the
development set contains 10 million query-rewrite
pairs. The average number of rewrites per query is
around 4,500 for the training and development set,
with a very small amount of 0.2% positive rewrites
per query. In order to confirm the validity of our co-
click hypothesis, and for final evaluation, we held
out another sample of query-rewrite pairs for man-
ual evaluation. This dataset contains 100 queries for
each of which we sampled 30 rewrites in descending
order of co-clicks, resulting in a high percentage of
43% positive rewrites per query. The query-rewrite
pairs were annotated by 3 raters as follows: First the
raters were asked to rank the rewrites in descend-
ing order of relevance using a graphical user inter-
face. Second the raters assigned rank labels and bi-
nary relevance scores to the ranked list of rewrites.
This labeling strategy is similar to the labeling strat-
egy for synonymy judgements proposed by Ruben-
stein and Goodenough (1965). Inter-rater agree-
ments on binary relevance judgements, and agree-
ment between rounded averaged human relevance

scores and assignments of positive/negative labels
by the co-click threshold of 10 produced a Kappa
value of 0.65 (Siegel and Castellan, 1988).

3.2 Learning-to-Rank Query Rewrites
3.2.1 Notation

Let S = {(xq, yq)}nq=1 be a training sample
of queries, each represented by a set of rewrites
xq = {xq1, . . . , xq,n(q)}, and set of rank labels
yq = {yq1, . . . , yq,n(q)}, where n(q) is the num-
ber of rewrites for query q. For full rankings of
all rewrites for a query, a total order on rewrites is
assumed, with rank labels taking on values yqi ∈
{1, . . . , n(q)}. Rewrites of equivalent rank can be
specified by assuming a partial order on rewrites,
where a multipartite ranking involves r < n(q) rele-
vance levels such that yqi ∈ {1, . . . , r} , and a bipar-
tite ranking involves two rank values yqi ∈ {1, 2}
with relevant rewrites at rank 1 and non-relevant
rewrites at rank 2.

Let the rewrites in xq be identified by the integers
{1, 2, . . . , n(q)}, and let a permutation πq on xq be
defined as a bijection from {1, 2, . . . , n(q)} onto it-
self. Let Πq denote the set of all possible permuta-
tions on xq, and let πqi denote the rank position of
xqi. Furthermore, let (i, j) denote a pair of rewrites
in xq and let Pq be the set of all pairs in xq.

We associate a feature function φ(xqi) with each
rewrite i = 1, . . . , n(q) for each query q. Further-
more, a partial-order feature map as used in Yue et
al. (2007) is created for each rewrite set as follows:

φ(xq, πq) =
1
|Pq|

∑
(i,j)∈Pq

φ(xqi)−φ(xqj)sgn(
1
πqi
− 1
πqj

).

The goal of learning a ranking over the rewrites
xq for a query q can be achieved either by sorting the
rewrites according to the rewrite-level ranking func-
tion f(xqi) = 〈w, φ(xqi)〉, or by finding the permu-
tation that scores highest according to a query-level
ranking function f(xq, πq) = 〈w, φ(xq, πq)〉.

In the following, we will describe a variety
of well-known ranking objectives, and extensions
thereof, that are used in our experiments. Optimiza-
tion is done in a stochastic gradient descent (SGD)
framework. We minimize an empirical loss objec-
tive

min
w

∑
xq ,yq

`(w)

478



by stochastic updating

wt+1 = wt − ηtgt

where ηt is a learning rate, and gt is the gradient

gt = ∇`(w)

where

∇`(w) =
〈

∂

∂w1
`(w),

∂

∂w2
`(w), . . . ,

∂

∂wn
`(w)

〉
.

3.2.2 Listwise Hinge Loss
Standard ranking evaluation metrics such as

(Mean) Average Precision (Manning et al., 2008)
are defined on permutations of whole lists and are
not decomposable over instances. Joachims (2005),
Yue et al. (2007), or Chakrabarti et al. (2008) have
proposed multivariate SVM models to optimize such
listwise evaluation metrics. The central idea is to
formalize the evaluation metric as a prediction loss
function L, and incorporate L via margin rescal-
ing into the hinge loss function, such that an up-
per bound on the prediction loss is achieved (see
Tsochantaridis et al. (2004), Proposition 2).

The loss function is given by the following list-
wise hinge loss:

`lh(w) = (L(yq, π∗q )−
〈
w, φ(xq, yq)− φ(xq, π∗q )

〉
)+

where π∗q is the maximizer of the
maxπq∈Πq\yq L(yq, π

∗
q ) +

〈
w, φ(xq, π∗q )

〉
ex-

pression, (z)+ = max{0, z} and L(yq, πq) ∈ [0, 1]
denotes a prediction loss of a predicted ranking πq
compared to the ground-truth ranking yq.2

In this paper, we use Average Precision (AP) as
prediction loss function s.t.

LAP (yq, πq) = 1−AP (yq, πq)

where AP is defined as follows:

AP (yq, πq) =

∑n(q)
j=1 Prec(j) · (|yqj − 2|)∑n(q)

j=1 (|yqj − 2|)
,

P rec(j) =

∑
k:πqk≤πqj (|yqk − 2|)

πqj
.

2We slightly abuse the notation yq to denote the permutation
on xq that is induced by the rank labels. In case of full rankings,
the permutation πq corresponding to ranking yq is unique. For
multipartite and bipartite rankings, there is more than one pos-
sible permutation for a given ranking, so that we let πq denote
a permutation that is consistent with ranking yq .

Note that the ranking scenario is in this case bipartite
with yqi ∈ {1, 2}.

The derivatives for `lh are as follows:

∂

∂wk
`lh =


0 if

(〈
w, φ(xq, yq)− φ(xq, π∗q )

〉)
> L(yq, π∗q ),

−(φk(xq, yq)− φk(xq, π∗q )) else.

SGD optimization involves computing π∗q for each
feature and each query, which can be done effi-
ciently using the greedy algorithm proposed by Yue
et al. (2007). We will refer to this method as the
SVM-MAP model.

3.2.3 Pairwise Hinge Loss for Bipartite and
Multipartite Ranking

Joachims (2002) proposed an SVM method that
defines the ranking problem as a pairwise classifi-
cation problem. Cortes et al. (2007) extended this
method to a magnitude-preserving version by penal-
izing a pairwise misranking by the magnitude of the
difference in preference labels. A position-sensitive
penalty for pairwise ranking SVMs was proposed
by Riezler and De Bona (2009) and Chapelle and
Keerthi (2010), and earlier for perceptrons by Shen
and Joshi (2005). In the latter approaches, the mag-
nitude of the difference in inverted ranks is accrued
for each misranked pair. The idea is to impose an
increased penalty for misrankings at the top of the
list, and for misrankings that involve a difference of
several rank levels.

Similar to the listwise case, we can view the
penalty as a prediction loss function, and incor-
porate it into the hinge loss function by rescaling
the margin by a pairwise prediction loss function
L(yqi, yqj). In our experiments we used a position-
sensitive prediction loss function

L(yqi, yqj) = |
1
yqi
− 1
yqj
|

defined on the difference of inverted ranks. The
margin-rescaled pairwise hinge loss is then defined
as follows:

`ph(w) =
∑

(i,j)∈Pq

(L(yqi, yqj)−

〈w, φ(xqi)− φ(xqj)〉 sgn(
1
yqi
− 1
yqj

))+

479



Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,
SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.

MAP NDCG@10 AUC Prec@1 Prec@3 Prec@5
Random 51.8 48.7 50.4 45.6 45.6 46.6
Best-feature 71.9 70.2 74.5 70.2 68.1 68.7
SVM-bipart. 73.7 73.7 74.7 79.4 70.1 70.1
SVM-MAP 74.3 75.2 75.3 76.3 71.8 72.0
Log-linear 74.7 75.1 75.7 75.3 72.2 71.3
SVM-pos.-sens. 75.7 76.0 76.6 82.5 72.9 73.0
SVM-multipart. 76.5 77.3 77.2 83.5 74.2 73.6

The derivative of `ph is calculated as follows:

∂

∂wk
`lp =


0 if (〈w, φ(xqi)− φ(xqj)〉

sgn( 1yqi −
1
yqj

)) > L(yqi, yqj),
−(φk(xqi)− φk(xqj))sgn( 1yqi −

1
yqj

)
else.

Note that the effect of inducing a position-
sensitive penalty on pairwise misrankings applies
only in case of full rankings on n(q) rank levels,
or in case of multipartite rankings involving 2 <
r < n(q) rank levels. Henceforth we will refer to
margin-rescaled pairwise hinge loss for multipartite
rankings as the SVM-pos.-sens. method.

Bipartite ranking is a special case where
L(yqi, yqj) is constant so that margin rescaling does
not have the effect of inducing position-sensitivity.
This method will be referred to as the SVM-bipartite
model.

Also note that for full ranking or multipartite
ranking, predicting a low ranking for an instance
that is ranked high in the ground truth has a domino
effect of accruing an additional penalty at each
rank level. This effect is independent of margin-
rescaling. The method of pairwise hinge loss
for multipartite ranking with constant margin will
henceforth be referred to as the SVM-multipartite
model.

Computation in SGD optimization is dominated
by the number of pairwise comparisons |Pq| for
each query. For full ranking, a comparison of
|Pq| =

(
n(q)

2

)
pairs has to be done. In the case

of multipartite ranking at r rank levels, each in-
cluding |li| rewrites, pairwise comparisons between
rewrites at the same rank level can be ignored.
This reduces the number of comparisons to |Pq| =

∑r−1
i=1

∑r
j=i+1 |li||lj |. For bipartite ranking of p

positive and n negative instances, |Pq| = p · n com-
parisons are necessary.

3.2.4 Log-linear Models for Bipartite Ranking
A probabilistic model for bipartite ranking can be

defined as the conditional probability of the set of
relevant rewrites, i.e., rewrites at rank level 1, given
all rewrites at rank levels 1 and 2. A formalization in
the family of log-linear models yields the following
logistic loss function `llm that was used for discrim-
inative estimation from sets of partially labeled data
in Riezler et al. (2002):

`llm(w) = − log
∑

xqi∈xq |yqi=1 e
〈w,φ(xqi)〉∑

xqi∈xq e
〈w,φ(xqi)〉

.

The gradient of `llm is calculated as a difference be-
tween two expectations:

∂

∂wk
`llm = −pw [φk|xq; yqi = 1] + pw [φk|xq] .

The SGD computation for the log-linear model is
dominated by the computation of expectations for
each query. The logistic loss for bipartite ranking is
henceforth referred to as the log-linear model.

4 Experimental Results

In the experiments reported in this paper, we trained
linear ranking models on 1 billion query-rewrite
pairs using 60 dense features, combined of the build-
ing blocks of syntactic and semantic similarity met-
rics under different estimations of cost matrices. De-
velopment testing was done on a data set that was
held-out from the training set. Final testing was car-
ried out on the manually labeled dataset. Data statis-
tics for all sets are given in Table 1.

480



Table 3: P-values computed by approximate randomization test for 15 pairwise comparisons of result differences.

Best-feature SVM-bipart. SVM-MAP Log-linear SVM-pos.-sens. SVM-multipart.
Best-feature - < 0.005 < 0.005 < 0.005 < 0.005 < 0.005
SVM-bipart. - - 0.324 < 0.005 < 0.005 < 0.005
SVM-MAP - - - 0.374 < 0.005 < 0.005
Log-linear - - - - 0.053 < 0.005
SVM-pos.-sens. - - - - - < 0.005
SVM-multipart. - - - - - -

Model selection was performed by adjusting
meta-parameters on the development set. We
trained each model at constant learning rates η ∈
{1, 0.5, 0.1, 0.01, 0.001}, and evaluated each variant
after every fifth out of 100 passes over the training
set. The variant with the highest MAP score on the
development set was chosen and evaluated on the
test set. This early stopping routine also served for
regularization.

Evaluation results for the systems are reported in
Table 2. We evaluate all models according to the fol-
lowing evaluation metrics: Mean Average Precision
(MAP), Normalized Discounted Cumulative Gain
with a cutoff at rank 10 (NDCG@10), Area-under-
the-ROC-curve (AUC), Precision@n3. As baselines
we report a random permutation of rewrites (ran-
dom), and the single dense feature that performed
best on the development set (best-feature). The latter
is the log-probability assigned to the query-rewrite
pair by the probabilistic clustering model used for
cost matrix estimation (see Section 2.5). P-values
are reported in Table 3 for all pairwise compar-
isons of systems (except the random baseline) us-
ing an Approximate Randomization test where strat-
ified shuffling is applied to results on the query level
(see Noreen (1989)). The rows in Tables 2 and 3
are ranked according to MAP values of the systems.
SVM-multipartite outperforms all other ranking sys-
tems under all evaluation metrics at a significance
level ≥ 0.995. For all other pairwise comparisons
of result differences, we find result differences of
systems ranked next to each other to be not statis-
tically significant. All systems outperform the ran-
dom and best-feature baselines with statistically sig-
nificant result differences. The distinctive advantage
of the SVM-multipartite models lies in the possibil-

3For a definition of these metrics see Manning et al. (2008)

ity to rank rewrites with very high co-click num-
bers even higher than rewrites with reasonable num-
bers of co-clicks. This preference for ranking the
top co-clicked rewrites high seems the best avenue
for transferring co-click information to the human
judgements encoded in the manually labeled test set.
Position-sensitive margin rescaling does not seem to
help, but rather seems to hurt.

5 Discussion

We presented an approach to learn rankings of query
rewrites from large amounts of user query log data.
We showed how to use the implicit co-click feed-
back about rewrite quality in user log data to train
ranking models that perform well on ranking query
rewrites according to human quality standards. We
presented large-scale experiments using SGD opti-
mization for linear ranking models. Our experimen-
tal results show that an SVM model for multipartite
ranking outperforms other linear ranking models un-
der several evaluation metrics. In future work, we
would like to extend our approach to other models,
e.g., sparse combinations of lexicalized features.

References

R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra,
and P. Tsaparas. 2009. Generating labels from clicks.
In Proceedings of the 2nd ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain.

Doug Beeferman and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD’00), Boston, MA.

P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009.
From ’Dango’ to ’Japanese cakes’: Query reformula-

481



tion models and patterns. In Proceedings of Web Intel-
ligence. IEEE Cs Press.

G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Proceedings of
GSCL.

Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and
Chiru Bhattacharayya. 2008. Structured learning for
non-smooth ranking losses. In Proceedings of the 14th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD’08), Las Vegas, NV.

Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.

Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22–29.

Massimiliano Ciaramita, Amaç Herdaǧdelen, Daniel
Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler,
and Enrique Alfonseca. 2010. Generalized syntactic
and semantic models of query reformulation. In Pro-
ceedings of the 33rd ACM SIGIR Conference, Geneva,
Switzerland.

Corinna Cortes, Mehryar Mohri, and Asish Rastogi.
2007. Magnitude-preserving ranking algorithms. In
Proceedings of the 24th International Conference on
Machine Learning (ICML’07), Corvallis, OR.

Larry Fitzpatrick and Mei Dent. 1997. Automatic feed-
back using past queries: Social searching? In Pro-
ceedings of the 20th Annual International ACM SIGIR
Conference, Philadelphia, PA.

Alon Halevy, Peter Norvig, and Fernando Pereira. 2009.
The unreasonable effectiveness of data. IEEE Intelli-
gent Systems, 24:8–12.

Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD’08), New York, NY.

Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd International Conference on Machine Learn-
ing (ICML’05), Bonn, Germany.

Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th International World Wide Web
conference (WWW’06), Edinburgh, Scotland.

T. Lau and E. Horvitz. 1999. Patterns of search: analyz-
ing and modeling web query refinement. In Proceed-
ings of the seventh international conference on User
modeling, pages 119–128. Springer-Verlag New York,
Inc.

V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707–710.

Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schütze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.

Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.

G. Recchia and M.N. Jones. 2009. More data trumps
smarter algorithms: comparing pointwise mutual in-
formation with latent semantic analysis. Behavioral
Research Methods, 41(3):647–656.

S.Y. Rieh and H. Xie. 2006. Analysis of multiple query
reformulations on the web: the interactive information
retrieval context. Inf. Process. Manage., 42(3):751–
768.

Stefan Riezler and Fabio De Bona. 2009. Simple risk
bounds for position-sensitive max-margin ranking al-
gorithms. In Proceedings of the Workshop on Ad-
vances in Ranking at the 23rd Annual Conference
on Neural Information Processing Systems (NIPS’09),
Whistler, Canada.

Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL’02), Philadelphia, PA.

Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 10(3):627–633.

Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th Inter-
national World Wide Web conference (WWW’06), Ed-
inburgh, Scotland.

Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73–96.

Sidney Siegel and John Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. Second Edition.
MacGraw-Hill, Boston, MA.

Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning (ICML’04), Banff,
Canada.

Yisong Yue, Thomas Finley, Filip Radlinski, and
Thorsten Joachims. 2007. A support vector method
for optimizing average precision. In Proceedings of
the 30th Annual International ACM SIGIR Confer-
ence, Amsterdam, The Netherlands.

482


