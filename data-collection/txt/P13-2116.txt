



















































Understanding Tables in Context Using Standard NLP Toolkits


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 658–664,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Understanding Tables in Context Using Standard NLP Toolkits

Vidhya Govindaraju Ce Zhang Christopher Ré
University of Wisconsin-Madison

{vidhya, czhang, chrisre}@cs.wisc.edu

Abstract

Tabular information in text documents
contains a wealth of information, and
so tables are a natural candidate for in-
formation extraction. There are many
cues buried in both a table and its sur-
rounding text that allow us to under-
stand the meaning of the data in a ta-
ble. We study how natural-language
tools, such as part-of-speech tagging,
dependency paths, and named-entity
recognition, can be used to improve the
quality of relation extraction from ta-
bles. In three domains we show that (1)
a model that performs joint probabilis-
tic inference across tabular and natural
language features achieves an F1 score
that is twice as high as either a pure-
table or pure-text system, and (2) us-
ing only shallower features or non-joint
inference results in lower quality.

1 Introduction

Tabular data is ubiquitous and often contains
high-quality, structured relational data. Re-
cent studies found billions of high-quality re-
lations on the web in HTML (Cafarella et
al., 2008). In financial applications, a huge
amount of data is buried in the tables of cor-
porate filings and earnings reports; in science,
millions of journal articles contain billions of
scientific facts in tables. Although tables de-
scribe precise, structured relations, tables are
rarely written in a way that is self-describing,
e.g., tables may contain abbreviations or only
informal schema information; in turn, the con-
tents of tables are often ambiguously specified,
which makes extracting the relations implicit
in tabular data difficult.

Tables are, however, not written in isola-
tion. The text surrounding a table in a jour-

nal article explains its contents to its intended
audience, a human reader. For example, in
a simple study, we demonstrate that humans
can achieve more than 60% higher recall by
jointly reading the text and tables in a journal
article than by only looking at the tables. The
conclusion of this experiment is not surprising,
but it raises a question: How should a system
combine tabular and natural-language features
to understand tables in text?

The literature provides a broad spectrum of
answers to this question. Most previous ap-
proaches use textual or tabular features sepa-
rately, e.g., tabular approaches that do not use
text features (Dalvi et al., 2012; Wu and Lee,
2006; Pinto et al., 2003) or textual approaches
that do not use tabular features (Mintz et al.,
2009; Wu and Weld, 2010; Poon and Domin-
gos, 2007). In a prescient study, Liu et al.
(2007) proposed to learn the target relation in-
dependently from both table and surface tex-
tual features, and then combine the result us-
ing a linear combination of the predictions.

In a similar spirit, we propose to use both
types of features in our approach of relation
extraction. Our proposed approach differs
from prior approaches in two ways: (1) We
use deeper–but standard–NLP features than
prior approaches for table extraction. In con-
trast to the shallow, lexical features that prior
approaches have used, we use standard NLP
features, such as dependency paths, parts of
speech, etc. Our hypothesis is that a deeper
understanding of the text in which a table is
embedded will lead to higher quality table ex-
traction. (2) Our probabilistic model jointly
uses both tabular and textual features. One
advantage of a joint approach is that one can
predict portions of the complicated predicate
that is buried in a table. For example, in a ge-
ology journal article, we may read a measure-

658



Table	  

although some fractional crystallization must have occurred during
the formation of these rocks, crystal fractionation alone cannot
account for the adakitic signature of the Gangdese rocks. Considering
the presence of a ~1500 km long belt of adakitic rocks, a fractionation
model would require the existence of an extremely large parent
magma body, the evidence for which is lacking. In fact, there is a
complete absence of coeval andesitic and basaltic magmatisms in this
adakite belt. Recent investigation on the crystallization history of a
hydrous primitive andesite composition shows that garnet is stable in
andesitic and basaltic bulk compositions only after large degrees of
crystallization lead to a decrease of the Mg-number to less than 0.5,
and that high Mg-number primitive melts are not garnet saturated at
high pressures (Müentener et al., 2001). However, all of the adakitic
porphyries from southern Tibet including those with lower Mg-
number show high Dy/Yb ratios and La/Yb ratios (Gao et al., 2007a).
The high Sr/Y, Dy/Yb and La/Yb ratios, low heavy REE and Y
concentrations of the Gangdese adakites require an adakitic signature
in the primary melt source.

Overall, the adakitic rocks of different ages in the Gangdese belt
display same differentiation trends (Figs. 2 and 3). In both the pre-
collision and post-collision groups, the abundances of MgO (Fig. 3a),
TiO2 (Fig. 3c) and CaO (Fig. 3d) decrease with increasing SiO2,

whereas with few exceptions, most samples have nearly constant
Al2O3 contents (Fig. 3b). Whereas the MgO and SiO2 contents of the
pre-collision adakite show a wide range, most of the post-collision
adakites have high SiO2 and low MgO contents, and plot in the high
SiO2 adakite field (Fig. 2a). In the two types of adakites, the total
alkaline contents (K2O+Na2O wt.%), K2O abundances and K2O/Na2O
ratios show a positive correlation with SiO2, displaying the typical
differentiation trend of calc-alkaline arc magmas (Fig. 2). However,
some of the post-collision adakitic rocks have unusually high K2O
contents, yielding abnormally high total alkaline contents and K2O/
Na2O ratios. Consequently, these samples significantly depart from
the overall trends (Fig. 2). This suggests that the unusual K2O
enrichment was not simply a result of magmatic differentiation.

The two generations of adakitic rock in the Gangdese belt show
many similarities in terms of distribution of trace elements with
typical incompatible trace element fractionation patterns of subduc-
tion-related magmas (Fig. 4). Overall, the adakites of different ages
display significant positive Pb and Sr anomalies, and negative Nb, Ta
and Ti anomalies (Fig. 4), correlating with typical features of adakitic
magmas (Martin et al., 2005). Despite their similar trace element
patterns, the geochemical signatures of the rocks in different regions
show some distinction. Some of the post-collision adakitic rocks from

Table 1
Major and trace elements of the post-collision adakitic rocks from the Gangdese belt, southern Tibet.

Location Zhunuo Puridazong

Sample ZM-1 ZM-2 ZM-3 ZM-4 ZM-5 ZM-6 ZM-7 ZM-8 ZM-9 ZM-10 ZM-12 ZM-13 PRDZ1 PRDZ2 PRDZ3 PRDZ4 PRDZ5

SiO2 66.69 65.96 65.37 67.00 69.09 65.10 63.86 71.56 69.73 69.37 67.27 65.99 65.61 65.97 65.73 65.02 65.29
TiO2 0.64 0.5 0.60 0.57 0.46 0.60 0.59 0.31 0.40 0.45 0.47 0.5 0.6 0.64 0.65 0.63 0.64
Al2O3 15.28 15.28 15.35 14.56 14.67 15.44 16.01 14.2 14.35 14.14 17.12 15.27 15.45 15.4 15.39 15.04 15.18
Fe2O3 4.17 3.07 4.27 4.06 3.18 4.31 3.83 2.39 3.04 3.32 2.25 3.05 3.46 3.69 3.68 3.54 3.62
MnO 0.02 0.06 0.08 0.08 0.04 0.09 0.08 0.03 0.04 0.04 0.03 0.06 0.06 0.06 0.06 0.06 0.06
MgO 1.75 1.54 1.84 1.60 1.20 1.94 2.35 0.83 1.08 1.34 0.97 1.54 1.63 1.75 1.76 1.72 1.7
CaO 2.20 2.61 3.70 2.88 2.06 3.69 3 1.81 2.38 2.52 2.43 2.59 3.27 3.22 3.33 3.16 3.24
Na2O 4.16 4.19 4.02 3.92 3.81 4.00 4.27 4.32 4.34 3.98 5.03 4.13 3.93 3.83 3.78 3.82 3.77
K2O 3.57 3.71 3.25 3.49 4.20 3.19 3.41 4.00 3.85 3.65 3.75 3.71 3.82 3.84 3.79 3.85 3.8
P2O5 0.21 0.18 0.21 0.19 0.17 0.22 0.27 0.11 0.15 0.16 0.24 0.19 0.26 0.27 0.27 0.26 0.26
LOI 0.90 2.66 0.94 1.36 0.86 1.08 1.98 0.22 0.36 0.80 0.12 2.76 1.46 1.62 1.5 1.56 1.4
Sum 99.6 99.8 99.6 99.7 99.7 99.7 99.7 99.8 99.7 99.8 99.7 99.8 99.6 100.3 99.9 98.7 99.0
Mg# 49.4 53.9 50.1 47.9 46.8 51.2 58.8 44.7 45.3 48.5 50.1 54.0 52.3 52.5 52.7 53.1 52.2
Sc 8.31 5.2 8.55 7.36 5.72 8.65 5.79 3.6 4.52 5.62 2.54 8.96 7.27 7.57 7.28 7.43 8.03
V 78.1 66 79.9 71.5 56.3 81.5 74.3 36.2 44.9 52.1 48.8 107 84.8 87.1 83.0 75.6 91.6
Cr 23.1 118 22.6 22.6 21.8 24.5 82.6 13.3 18.2 22.2 112 33.4 24.1 25.5 23.1 377.8 28.7
Co 11 14.4 12.2 10.9 7.27 13 11.5 5.21 6.7 5.24 5.99 13.1 10.6 10.8 10.6 13.4 11.4
Ni 15 86.8 13.9 12.7 10.5 15.9 17.3 6.41 8.65 14.2 6.85 17.1 11.4 11.6 11.5 74.2 12.9
Rb 232 202 141 158 218 142 153 227 207 191 193 40.3 149 143 134 149 155
Sr 681 633 884 752 550 878 807 567 664 623 824 825 1025 987.5 994 950 995
Y 12.2 9.21 17.5 11.0 11.0 11.2 9.57 6.02 7.64 8.98 5.06 8.69 12.0 12.1 11.2 11.6 12.9
Zr 47.2 96.6 114 78.6 26.8 64.7 67.5 26.5 46 77.7 18.3 166 127 136 127 139 149
Nb 9 9.3 9.72 9.14 9.83 9.23 8.98 8.6 8.41 9.77 7.3 6.45 8.83 8.77 8.07 8.65 9.28
Cs 19.1 8.47 6.42 6.97 11.1 8.28 6.26 10.8 8.35 14.0 8.42 2.07 3.66 3.79 3.49 3.60 3.92
Ba 880 669 985 912 848 964 741 861 857 838 652 1043 1287 1234 1129 1205 1225
La 37.4 31.5 33.36 37.8 38.4 38.7 33.6 28 37.2 34.8 25.4 20.6 42.2 50.8 41.5 46.7 49.7
Ce 65.7 63.8 63.6 68.3 65.9 69.6 66.9 47.4 62.7 61.5 49.7 41 83.4 94.0 81.0 88.7 95.5
Pr 8.35 7.81 8.32 8.52 8.2 8.65 8.15 5.41 7.45 7.55 6.65 5.09 9.83 10.42 9.21 9.93 10.70
Nd 31.3 28.3 31.68 30.8 28.4 31.9 29.2 19.2 26.5 27.8 25.1 19.8 37.5 38.4 34.7 37.2 39.8
Sm 5.57 4.68 6.32 5.22 4.79 5.48 4.96 3.02 4.27 4.69 4.19 3.45 6.19 6.25 5.53 6.05 6.55
Eu 1.36 1.17 1.1 1.23 1.08 1.36 1.30 0.76 1.00 1.07 1.04 1.29 1.60 1.57 1.44 1.48 1.60
Gd 4.46 3.34 4.02 3.98 3.58 4.02 3.54 2.17 3.09 3.36 2.69 2.92 4.64 4.50 4.17 4.38 4.63
Tb 0.54 0.43 0.520 0.47 0.47 0.5 0.45 0.25 0.35 0.38 0.32 0.41 0.390 0.397 0.359 0.374 0.414
Dy 2.7 1.99 2.72 2.46 2.3 2.5 2.03 1.22 1.64 1.98 1.22 1.89 2.84 2.79 2.63 2.74 2.91
Ho 0.49 0.34 0.487 0.43 0.41 0.44 0.38 0.23 0.29 0.35 0.19 0.33 0.504 0.511 0.471 0.491 0.530
Er 1.29 0.92 1.26 1.13 1.13 1.19 0.97 0.61 0.77 0.92 0.48 0.96 1.40 1.40 1.30 1.34 1.50
Tm 0.17 0.13 0.16 0.15 0.14 0.15 0.13 0.09 0.11 0.12 0.06 0.13 0.186 0.185 0.167 0.179 0.188
Yb 0.99 0.82 1 0.96 0.94 0.91 0.83 0.6 0.67 0.74 0.38 0.87 1.032 1.004 0.957 1.004 1.096
Lu 0.13 0.12 0.13 0.14 0.13 0.12 0.11 0.09 0.1 0.1 0.04 0.12 0.166 0.166 0.157 0.172 0.173
Hf 1.44 4.05 3.57 2.76 0.94 2.35 2.74 1.11 1.82 2.7 1.34 4.88 3.58 3.84 3.65 3.89 4.15
Ta 0.69 0.79 0.77 0.73 0.92 0.69 0.69 0.82 0.76 0.82 0.62 0.4 0.582 0.567 0.534 0.573 0.592
Pb 21 60.1 34.4 43.9 42.2 34.9 42.4 28 29.7 32.1 23 10.6 32.1 30.8 29.9 31.8 31.9
Th 18 27.2 24.6 24.5 30.1 21.7 24.6 22.3 26 28.4 15 2.95 16.5 17.5 16.0 17.6 17.9
U 2.97 6.64 5.66 5.91 4.48 5.16 5.39 5.11 4.84 8.33 2.83 0.92 2.58 2.43 2.35 2.46 2.57

Mg#=Mg/(Mg+0.85⁎TFe2+).

654 Y. Gao et al. / Lithos 119 (2010) 651–663

SampleID	   Loca-on	  
ZM-­‐1	   Zhunuo	  
…	   …	  

Loca-on	   RockType	  
Zhunuo	   Granodi-­‐	  

orite	  
…	   …	  

Loca7on_RockType	  

SampleID	   RockType	  
ZM-­‐1	   Granodi-­‐

orite	  
…	   …	  

Sample_RockType	  

Zhunuo	  (ZN),	  which	  
lithologically	  corresp-­‐
ond	  to	  granodiori7c	  
and	  grani7c…	  

Context	  Text	  

Sample_Loca7on	  
Input	  Data	   Joint	  Inference	  Results	  

Figure 1: An example of joint inference be-
tween a table and its context.

1	   7	  … 8	   14	  … 15	   21	  … 

1	   7	  … 8	   14	  … 15	   21	  … 

1	   7	  … 8	   14	  … 15	   21	  … 

Whole doc. 

Table-only 

Text-only 

Geoscien/st	  1	   Geoscien/st	  2	   Geoscien/st	  3	  

Figure 2: Job assignments for the human
study.

ment in a table that tells us the type of rock
and its weight—but data such as the location
where this rock was unearthed and in what ge-
ological time interval this rock appeared may
not be specified in the table.

We consider tasks in three domains:
Petrology, Finance, and Geology. For
each domain, we build a system to extract re-
lations from text, tables, or both. We found
that a joint inference system that uses non-
shallow, but standard NLP features can sig-
nificantly improve the quality of the extracted
relations, and that this result holds consistently
across all three domains. For example, in our
Petrology application to extract a knowledge
base, called PetDB1, by using information
extracted from both text and tables, we can
achieve twice as high F1 compared to either a
pure-table or pure-text system.

2 Motivating Human Study

We describe a simple human study that mo-
tivated our approach to jointly combine both
tabular features and natural language features
to extract relations from tables. The hypoth-

1http://www.earthchem.org/petdb

0	  

0.2	  

0.4	  

0.6	  

0.8	  

1	  

Precision	   Recall	   F-­‐1	  

Text-­‐only	   Table-­‐only	   Whole	  document	  

Figure 3: Human quality to extract Sample-
Rocktype relations in PetDB.

Task	   Text Table Joint 

NER	   POS	  tags	  
Stanford	  NER	  
Regular	  Expression	  
Dic9onary	  

pd;otable	  
NER	  of	  neighbor	  cells	  
Regular	  expression	  
Dic9onary	  
#	  columns	  

Whether	  a	  men9on	  	  
in	  table	  also	  appears	  	  
in	  the	  text.	  

EL	   POS	  tags	  
Bing	  query	  results	  
Freebase	  
Stanford	  Parser	  

Pd;otable	  
Bing	  query	  results	  
Freebase	  

Subjec9ve	  men9ons	  	  
in	  the	  sentence	  near	  	  
a	  table	  

RE	   Dependency	  path	  
Term	  proximity	  
Word	  sequence	  

Table	  headers	  
Table	  subheaders	  
RE	  of	  neighbor	  rows	  

Join	  between	  
rela9ons	  (See	  Figure	  
1	  for	  an	  example)	  

Figure 4: List of features we used in Text,
Table, and Joint approaches. NER, EL,
and RE refer to named-entity recognition, en-
tity linking, and relation extraction, respec-
tively.

esis that we want to validate is that the text
surrounding a table could provide valuable in-
formation even for a human reader, and there-
fore, an ideal machine reading system should
also try to capture similar information.

We asked three geoscientists to manually
read journal articles and extract relations
for the Petrology domain. We report
our results for the target relation, Sample-
RockType, which associates a rock type with
a rock sample (see Figure 1 for an example).
We randomly sampled 21 journal articles. For
each journal article, we produced three vari-
ants: (1) the original document; (2) table-
only, which is the set of tables in the docu-
ment (without the text); (3) text-only, which
is the text of the document with the tables
removed from the document. Each geoscien-
tist was asked to read and extract the relations
from one of the three variants. We then judged
the precision and recall of their extraction, as
shown in Figure 2.

659



As shown in Figure 3, human readers not
surprisingly achieve perfect precision on each
of the variants, but lower recall on both
the table-only and text-only variants. How-
ever, summing the recall of table-only (60%)
and text-only (20%) variants together would
achieve only 80% recall; this implies that in
the best case more than 20% of the extrac-
tions require that the human reader read the
table and its surrounding text jointly. Figure 1
shows one representative example.

This motivates our approach, which uses a
joint inference system to model features from
a table and its surrounding text. We also pro-
pose to use deep linguistic features instead of
shallower features to get as close as possible to
the ability of human readers in understanding
the surrounding text of a table.

3 Empirical Study & Experiments

We describe our experiments to test the hy-
pothesis that (1) deeper linguistic features can
help to extract higher quality relations from
tables, and (2) joint inference across tables and
text improves extraction quality compared to
approaches that use pure-table, pure-text, and
non-joint ways of combining these two. We
briefly describe some experiments for a dataset
that we call Geology (Zhang et al., 2013).
The detailed experimental results in all three
domains are in the technical report version of
this paper.

3.1 Experimental Setup

We consider the task of constructing a geol-
ogy knowledge base. Specifically, our goal is
to extract a Rock-TotalOrganicCarbon
relation that maps rock formations (e.g., “Bar-
nett Formation”) to their total organic carbon
(e.g., “6%”). Such data is important for es-
timating stored energy and for global climate
research.

Dataset. We selected 100 geology journal
articles.2 We asked three geoscientists to an-
notate these journal articles manually to ex-
tract the Rock-TotalOrganicCarbon re-
lation (1.5K tuples). We processed each doc-
ument using Stanford CoreNLP (de Marneffe
et al., 2006; Toutanova and Manning, 2000),

2We choose a set of documents that (1) are in En-
glish, and (2) contain at least one table.

PDFtoHTML3, and pdf2table (Yildiz, 2004).
We then extracted features following state-of-
the-art practices (see Figure 4).

Approaches. To validate our hypothesis,
we implement four systems, each of which has
access to different types of data:

(1) Table. This approach follows Pinto et
al. (2003) and Dalvi et al. (2012) and only uses
the tables in a document.

(2) Text. This approach only has access to
the text in a document and contains all the fea-
tures mentioned in Wu and Weld (2010) and
Mintz et al. (2009).

The features used in (1) and (2) are shown in
Figure 4. In both Table and Text, we use a
conditional random field (Lafferty et al., 2001)
model for the Rock-TotalOrganicCarbon
relation.

(3) Merge. Using Table and Text, we
extract all facts and their associated probabil-
ity. Following Duin (2002), we combine these
two probabilities using a linear combination.
Merge is a baseline approach that uses infor-
mation from both tables and text.

(4) Joint. We build a joint approach that
uses information from both tables and text.
This approach is a large factor graph in which
we embed the CRFs developed in Table and
Text. Additionally, we allow Joint to pre-
dict projections of each relation, as shown in
Figure 4. Recall that a key advantage of a joint
approach is that we do not need to predict all
arguments of the relation (if such a prediction
is unwarranted from the data). The inference
is done by Gibbs sampling using our inference
engine Elementary (Zhang and Ré, 2013).
We describe the Joint system in more detail
in the technical report version of this paper.

3.2 End-to-End Quality

We were able to validate that Joint achieves
higher quality than the other three approaches
we considered. Figure 5 shows the P/R curve
of different approaches on three domains. We
analyzed the domain Geology.
Joint dominates all other approaches. At

a recall of 10%, Joint achieves 3x higher pre-
cision than all other approaches. In our error
analysis, we saw that tables in geology articles
often contain ambiguous words; for example,

3http://pdftohtml.sourceforge.net/

660



0	  
0.2	  
0.4	  
0.6	  
0.8	  
1	  

0	   0.2	   0.4	  
Recall	  

Table	  Text	   Merge	  

Joint	  

(b)	  Petrology	  Domain	  	  

0	  
0.2	  
0.4	  
0.6	  
0.8	  
1	  

0	   0.1	   0.2	   0.3	  

Pr
ec
is
io
n	  

Recall	  

Joint	  

Merge	  
Text	  

Table	  

(a)	  Geology	  Domain	  	  

0	  
0.2	  
0.4	  
0.6	  
0.8	  
1	  

0	   0.1	   0.2	  
Recall	  

Joint	  

Table	  
Merge	  

Text	  

(c)	  Finance	  Domain	  	  

Figure 5: End-to-end extraction quality on Petrology, Finance, and GeoDeepDive. The
recall is limited by the quality of state-of-the-art table recognition software on PDFs.

the word “Barnett” in a table may refer to ei-
ther a location or a rock formation. By using
features extracted from text, Joint achieves
higher precision. For recall in the range of 0–
10%, Merge outperforms both Text and Ta-
ble, with 3%–90% improvement in precision.

In Geology, Merge has precision that is
similar to Text and Table for the higher re-
call range (>10%). In this domain, we found
that relations that appeared in the text often
repeated relations described in the table. In
other domains, such as Petrology, where
the relations in text and tables have lower de-
grees of overlap, Merge significantly improves
over Text and Table (Figure 5(b)).

We conducted a statistical significance test
to check whether the improvement of Joint
over the three other approaches is statistically
significant. For each of the three probability
thresholds, t ∈ {.99, .90, .50}, we created the
set of predictions that Joint assigns probabil-
ity greater than t. Figure 6 shows the results
of the statistical significance test in which the
null hypothesis is that the F1 scores of two ap-
proaches are the same. With p = 0.01, Joint
has statistically significant improvement of F1
score over all three other approaches with each
probability threshold.

3.3 Shallow vs. Linguistic Features

We validate the hypothesis that using
linguistic features, e.g., part-of-speech
tags (Toutanova and Manning, 2000),
named-entity tags (Finkel et al., 2005), and
dependency trees (de Marneffe et al., 2006),
helps improve the quality of our approach,
called Joint. There are different ways to
use shallow and linguistic features; we select

Approaches \ Prob. .99 .90 .50
Text + + +
Table + + +
Merge + + +

Figure 6: Approximate randomization test
from Chinchor (1992) of F1 score with p =
0.01 on the impact of joint inference compared
with pure-table or pure-text approaches for
different probability thresholds. A + sign in-
dicates that the F1 score of joint approach in-
creased significantly.

Type	   Features	  
Shallow	   Regular	  Expressions	  (Dalvi	  et	  al.,	  2012)	  	  

Term	  proximity	  (Matsuo	  et	  al.,	  2003)	  
DicConary	  and	  Freebase	  (Mintz	  et	  al.,	  2009)	  

LinguisCc	   POS	  tags	  (Wu	  et	  al.,	  2010)	  
Stanford	  NER	  tags	  (Mintz	  et	  al.,	  2009)	  
Dependence	  trees	  (Mintz	  et	  al.,	  2009)	  

Figure 7: Types of Features.

state-of-the-art approaches from the literature
(see Figure 7).

We created the following variants of Joint.
Joint(-parse) removes features generated by
the dependency parser and syntax parser.
Similarly, Joint(-ner) (Joint(-pos)) removes
all features related to NER (resp. POS).
Joint(-pos) also removes NER and parser fea-
tures because the latter two are dependent on
POS features.

Figure 8 shows the P/R curve for all
these variants on Geology, and Figure 9
shows the results of statistical significance
test. For probability threshold .90, Joint
outperforms Joint(-pos) significantly. The
difference between Joint, Joint(-parse),

661



0	  
0.2	  
0.4	  
0.6	  
0.8	  
1	  

0	   0.1	   0.2	   0.3	  

Pr
ec
is
io
n	  

Recall	  

Joint(-­‐pos)	  

Joint	  
Joint(-­‐parse)	  Joint(-­‐ner)	  

Figure 8: Lesion study of different features for
Geology.

Features \ Prob. .90 .50
Joint(-parse) → Joint 0 +
Joint(-ner) → Joint 0 +
Joint(-pos) → Joint + +

Figure 9: Approximate randomization test of
F1 score with p = 0.01 on the impact of lin-
guistic features. For x → y, a + indicates that
the F1 score of y is significantly higher than x.
0 indicates that the F1 score does not change
significantly.

and Joint(-ner) is not significant because
there are “easy-to-extract” facts in the high-
probability range. For probability threshold
.50, Joint outperforms all three other vari-
ants significantly.

4 Related Work

The intuition that context features might help
table-related tasks has existed for decades. For
example, Hurst and Nasukawa (2000) men-
tioned (as future work) that context features
could be used to further improve their relation
extraction approaches from tables. Lin et al.
(2010) use bag-of-words features and hyper-
links to recommend new columns for web ta-
bles. Liu et al. (2007) extract features, includ-
ing font size and title, from PDF documents in
which a table appears to help the table rank-
ing task. They find that these features only
contribute less than 2% to precision. In con-
trast, in our approach linguistic features are
quite useful. The above approaches use con-
text features that can be extracted without
POS tagging or linguistic parsing. One aspect
of our work is to demonstrate that traditional
NLP tools can enhance the quality of table ex-
traction.

Extracting information from tables has been
discussed by different communities in the last
decade, including NLP (Wu and Lee, 2006;
Tengli et al., 2004; Chen et al., 2000), artifi-
cial intelligence (Fang et al., 2012; Pivk, 2006),
information retrieval (Wei et al., 2006; Pinto
et al., 2003), database (Cafarella et al., 2008),
and the web (Dalvi et al., 2012). This body of
work considers only features derived from ta-
bles and does not examine richer NLP features
as we do.

While joint inference is popular, it is not
clear when a joint inference system outper-
forms a more traditional NLP pipeline. Re-
cent studies have reached a variety of conclu-
sions: in some, joint inference helps extraction
quality (McCallum, 2009; Poon and Domin-
gos, 2007; Singh et al., 2009); and in some,
joint inference hurts extraction quality (Poon
and Domingos, 2007; Eisner, 2009). Our intu-
ition is that joint inference is helpful in this ap-
plication because our joint inference approach
combines non-redundant signals (textual ver-
sus tabular).

5 Conclusion

To improve the quality of extractions of tabu-
lar data, we use standard NLP techniques to
more deeply understand the text in which a
table is embedded. We validate that deeper
NLP features combined with a joint proba-
bilistic model has a statistically significant im-
pact on quality, i.e., recall and precision. Our
ongoing work is to apply these ideas to a much
larger corpus from each of the three domains.

6 Acknowledgments

We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA) DEFT Program under Air Force
Research Laboratory (AFRL) prime contract
No. FA8750-13-2-0039, the National Science
Foundation EAGER Award under No. EAR-
1242902 and CAREER Award under No. IIS-
1054009, and the Sloan Research Fellowship.
Any opinions, findings, and conclusion or rec-
ommendations expressed in this material are
those of the authors and do not necessarily re-
flect the view of DARPA, AFRL, NSF, or the
US government. We are also grateful to Jude
W. Shavlik for his insightful comments.

662



References

Michael J. Cafarella, Alon Halevy, Daisy Zhe
Wang, Eugene Wu, and Yang Zhang. 2008.
WebTables: Exploring the power of tables on the
web. Proceedings of VLDB Endowment, 1(1).

Hsin-Hsi Chen, Shih-Chung Tsai, and Jin-He Tsai.
2000. Mining tables from large scale HTML
texts. In Proceedings of the 18th Conference on
Computational Linguistics, COLING ’00.

Nancy Chinchor. 1992. The statistical significance
of the MUC-4 results. In Proceedings of the 4th
Conference on Message Understanding, MUC4
’92.

Bhavana Bharat Dalvi, William Cohen, and Jamie
Callan. 2012. WebSets: Extracting sets of en-
tities from the web using unsupervised infor-
mation extraction. In Proceedings of the 5th
ACM International Conference on Web Search
and Data Mining, WSDM ’12.

Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources and Evalua-
tion.

Robert Duin. 2002. The combining classifier: to
train or not to train? In 16th International
Conference on Pattern Recognition.

Jason Eisner. 2009. Joint models with missing
data for semi-supervised learning. In NAACL
HLT Workshop on Semi-supervised Learning for
Natural Language Processing.

Jing Fang, Prasenjit Mitra, Zhi Tang, and C. Lee
Giles. 2012. Table header detection and classi-
fication. In Proceedings of the 26th AAAI Con-
ference on Artificial Intelligence, AAAI ’12.

Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local infor-
mation into information extraction systems by
Gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational
Linguistics, ACL ’05.

Matthew Hurst and Tetsuya Nasukawa. 2000.
Layout and language: Integrating spatial and
linguistic knowledge for layout understanding
tasks. In Proceedings of the 18th Conference on
Computational Linguistics, COLING ’00.

John D. Lafferty, Andrew McCallum, and Fer-
nando C. N. Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting
and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Ma-
chine Learning, ICML ’01, pages 282–289, San
Francisco, CA, USA. Morgan Kaufmann Pub-
lishers Inc.

Cindy Xide Lin, Bo Zhao, Tim Weninger, Jiawei
Han, and Bing Liu. 2010. Entity relation dis-
covery from web tables and links. In Proceedings
of the 19th International Conference on World
Wide Web, WWW ’10.

Ying Liu, Kun Bai, Prasenjit Mitra, and C. Lee
Giles. 2007. TableSeer: Automatic table meta-
data extraction and searching in digital libraries.
In Proceedings of the 7th ACM/IEEE-CS Joint
Conference on Digital Libraries, JCDL ’07.

Andrew McCallum. 2009. Joint inference for nat-
ural language processing. In Proceedings of the
13th Conference on Computational Natural Lan-
guage Learning, CoNLL ’09.

Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings
of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP, ACL ’09.

David Pinto, Andrew McCallum, Xing Wei, and
W. Bruce Croft. 2003. Table extraction us-
ing conditional random fields. In Proceedings
of the 26th Annual International ACM SIGIR
Conference on Research and Development in In-
formaion Retrieval, SIGIR ’03.

Aleksander Pivk. 2006. Automatic ontology gen-
eration from web tabular structures. AI Com-
munication, 19(1).

Hoifung Poon and Pedro Domingos. 2007. Joint
inference in information extraction. In Proceed-
ings of the 22nd National Conference on Artifi-
cial intelligence, AAAI’07.

Sameer Singh, Karl Schultz, and Andrew Mc-
Callum. 2009. Bi-directional joint inference
for entity resolution and segmentation using
imperatively-defined factor graphs. In Pro-
ceedings of the European Conference on Ma-
chine Learning and Knowledge Discovery in
Databases, ECML PKDD ’09.

Ashwin Tengli, Yiming Yang, and Nian Li Ma.
2004. Learning table extraction from examples.
In Proceedings of the 20th International Con-
ference on Computational Linguistics, COLING
’04.

Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in
a maximum entropy part-of-speech tagger. In
Proceedings of the 2000 Joint SIGDAT Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’00.

Xing Wei, Bruce Croft, and Andrew McCallum.
2006. Table extraction for answer retrieval. In-
formation Retrieval, 9(5).

663



Dekai Wu and Ken Wing Kuen Lee. 2006. A gram-
matical approach to understanding textual ta-
bles using two-dimensional scfgs. In Proceedings
of the COLING/ACL, COLING-ACL ’06.

Fei Wu and Daniel S. Weld. 2010. Open informa-
tion extraction using Wikipedia. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ’10.

Burcu Yildiz. 2004. Information extraction – uti-
lizing table patterns. Master’s thesis, Institutfür
Softwaretechnik und Interaktive Systeme.

Ce Zhang and Christopher Ré. 2013. Towards
high-throughput Gibbs sampling at scale: A
study across storage managers. SIGMOD ’13.

Ce Zhang, Vidhya Govindaraju, Jackson Bor-
chardt, Tim Foltz, Christopher Ré, and Shanan
Peters. 2013. GeoDeepDive: Statistical infer-
ence using familiar data-processing languages.
SIGMOD ’13.

664


