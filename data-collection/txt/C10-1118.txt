Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1047–1055,

Beijing, August 2010

1047

Discriminative Induction of Sub-Tree Alignment                                 

using Limited Labeled Data 

 
        Jun Sun1,2                             Min Zhang1                      Chew Lim Tan2 

1Institute for Infocomm Research      2School of Computing, National University of Singapore     
        sunjun@comp.nus.edu.sg     mzhang@i2r.a-star.edu.sg     tancl@comp.nus.edu.sg  

 

Abstract 

We  employ  Maximum  Entropy  model  to  con-
duct  sub-tree  alignment  between  bilingual 
phrasal  structure  trees.  Various  lexical  and 
structural knowledge is explored to measure the 
syntactic  similarity  across  Chinese-English  bi-
lingual  tree  pairs.  In  the  experiment,  we  evalu-
ate  the  sub-tree  alignment  using  both  gold 
standard tree bank and the automatically parsed 
corpus  with  manually  annotated  sub-tree  align-
ment.  Compared  with  a  heuristic  similarity 
based method, the proposed method significant-
ly  improves  the  performance  with  only  limited 
sub-tree  aligned  data.  To  examine  its  effective-
ness for multilingual applications, we further at-
tempt different approaches to apply the sub-tree 
alignment in both phrase and syntax based SMT 
systems.  We  then  compare  the  performance 
with  that  of  the  widely  used  word  alignment. 
Experimental  results  on  benchmark  data  show 
that sub-tree alignment benefits both systems by 
relaxing the constraint of the word alignment. 

1 

Introduction 

Recent research in Statistical Machine Translation 
(SMT)  tends  to  incorporate  more  linguistically 
grammatical information into the translation mod-
el known as linguistically motivated syntax-based 
models.  To  develop  such  models,  the  phrasal 
structure parse tree is usually adopted as the repre-
sentation  of  bilingual  sentence  pairs  either  on  the 
source  side  (Huang  et  al.,  2006;  Liu  et  al.,  2006) 
or on the target side (Galley et al., 2006; Marcu et 
al.,  2006),  or  even  on  both  sides  (Graehl  and 
Knight,  2004;  Zhang  et  al.,  2007).  Most  of  the 
above  models  either  construct  a  pipeline  to  trans-
form from/to tree structure, or synchronously gen-
erate  two  trees  in  parallel  (i.e.,  synchronous  pars-
ing).  Both  cases  require  syntactically  rich  transla-
tional equivalences to handle non-local reordering. 
However, most current works obtain the syntactic 
translational  equivalences  by  initially  conducting 
alignment  on  the  word  level.  To  employ  word 

alignment  as  a  hard  constraint  for  rule  extraction 
has difficulty in capturing such non-local phenom-
ena  and  will  fully  propagate  the  word  alignment 
error to the later stage of rule extraction. 

Alternatively,  some  initial  attempts  have  been 
made 
to  directly  conduct  syntactic  structure 
alignment.  As  mentioned  in  Tinsley  et  al.  (2007), 
the  early  work  usually  constructs  the  structure 
alignment by hand, which is time-consuming. Re-
cent research tries to automatically align the bilin-
gual  syntactic  sub-trees.  However,  most  of  these 
works suffer from the following problems. Firstly, 
the  alignment  is  conducted  based  on  heuristic 
rules, which  may lose extensibility and generality 
in  spite  of  accommodating  some  common  cases 
(Groves et al., 2004). Secondly, various similarity 
computation  methods  are  used  based  merely  on 
lexical  translation  probabilities  (Tinsley  et  al., 
2007; Imamura, 2001) regardless of structural fea-
tures.  We  believe  the  structure  information  is  an 
important issue  to capture the non-local structural 
divergence  of  languages  by  modeling  beyond  the 
plain text.  

To address the above issues, we present a statis-
tical  framework  based  on  Maximum  Entropy 
(MaxEnt)  model.  Specifically,  we  consider  sub-
tree  alignment  as  a  binary  classification  problem 
and use Maximum Entropy model to classify each 
instance  as  aligned  or  unaligned.  Then,  we  per-
form  a  greedy  search  within  the  reduced  search 
space to conduct sub-tree alignment links based on 
the alignment probabilities obtained from the clas-
sifier. 

Unlike  the  previous  approaches  that  can  only 
measure  the  structural  divergence  via  lexical  fea-
tures,  our  approach  can  incorporate  both  lexical 
and  structural  features.  Additionally,  instead  of 
explicitly describing the instances of sub-tree pairs 
as factorized sub-structures, we frame most of our 
features  as  score  based  feature  functions,  which 
helps  solve  the  problem  using  limited  sub-tree 
alignment  annotated  data.  To  train  the  model  and 
evaluate  the  alignment  performance,  we  adopt 

1048

3  Model  

We solve the problem as binary classification and 
employ MaxEnt model with a greedy search.  

Given  a  bilingual  tree  pair      and    ,      
{                  } is  the  source  tree  consisting  of   
sub-trees，where   is also the number of nodes in 
the  source  tree          {                  } is  the  tar-
get  tree  consisting  of   sub-trees,  where   is  also 
the number of nodes in the target tree   . 

For each sub-tree pair          in the given bilin-
gual  parse  trees          ,  the  sub-tree  alignment 
probability is given by: 

      ( |        )  

where 

   [∑

 
   

    

(         )]

∑    [∑

  

 
   

    

(          )]

     (1) 

                   {

 

           (      )            
                               

                     (2) 

Feature  functions  are  defined  in  a  quadruple 
(            ).   is an additional variable to incorpo-
rate  new  dependencies  other  than  the  sub-tree 
pairs.  For  each  feature  function   (            ),  a 
weight    is applied to tailor the distribution. 

After classifying the candidate sub-tree pairs as 
aligned or unaligned, we perform a greedy search 
within  the  reduced  search  space  to  conduct  sure 
links  based  on 
the  conditional  probability 
  ( |        )  obtained  from  the  classifier.  The 
alignment probability is independently normalized 
for  each  sub-tree  pair  and  hence  suitable  as  a 
searching metric. 

The  greedy  search  algorithm  can  be  described 
as  an  automaton.  A  state  in  the  search  space  is  a 
partial  alignment  with  respect  to  the  given  bilin-
gual tree pair. A transition is to add one more link 
of node pairs to the current state. The  initial state 
has no link. The terminal state is a state where no 
more  links  can  be  added  according  to  the  defini-
tion in Section 2. We use greedy search to gener-
ate the best-links at the early stage. There are cas-
es  that  the  correctly-aligned  tree  pairs  have  very 
few  links,  while  we  have  a  bunch  of  candidates 
with  lower  alignment  probabilities.  However,  the 
sum of the lower probabilities is larger than that of 
the  correct  links’,  since  the  number  of  correct 
links  is  much  fewer.  This  makes  the  alignment 
results  biased  to  be  with  more  links.  The  greedy 
search helps avoid this asymmetric problem.  

4  Feature Functions 

In  this  section,  we  introduce  a  variety  of  feature 
functions  to  capture  the  semantically  equivalent 

Figure 1: Sub-tree alignment as referred to  

 

Node alignment 

HIT  Chinese-English  parallel  tree  bank  for  gold 
standard evaluation. To explore its effectiveness in 
SMT systems, we also manually annotate sub-tree 
alignment  on  automatically  parsed  tree  pairs  and 
perform  the  noisy  data  evaluation.  Experimental 
results  show  that  by  only  using  limited  sub-tree 
aligned  data  of  both  corpora,  the  proposed  ap-
proach  significantly  outperforms 
the  baseline 
method  (Tinsley  et  al.,  2007).  The  proposed  fea-
tures  are  very  effective  in  modeling  the  bilingual 
structural similarity. We further apply the sub-tree 
alignment  to  relax  the  constraint  of  word  align-
ment  for  both  phrase  and  syntax  based  SMT  sys-
tems and gain an improvement in BLEU. 

2  Problem definition  

A  sub-tree  alignment  process  pairs  up  the  sub-
trees  across  bilingual  parse  trees,  whose  lexical 
leaf nodes covered are translational equivalent, i.e., 
sharing  the  same  semantics.    Grammatically,  the 
task conducts links between syntactic constituents 
with  the  maximum  tree  structures  generated  over 
their word sequences in bilingual tree pairs.  

In general, sub-tree alignment can also be inter-
preted as conducting multiple links across internal 
nodes  between  sentence-aligned  tree  pairs  as 
shown in Fig. 1. The aligned sub-tree pairs usually 
maintain a non-isomorphic relation with each oth-
er especially for higher layers. We adapt the same 
criteria as Tinsley et al. (2007) in our study: 

a node can only be linked once; 

(i) 
(ii)  descendants  of  a  source  linked  node  may 
only  link  to  descendants  of  its  target 
linked counterpart; 

(iii)  ancestors of a source linked node may on-
ly  link  to  ancestors  of  its  target  linked 
counterpart. 

where  the  term  “node”  refers  to  root  of  a  sub-

tree, which can be used to represent the sub-tree. 

S

VBA

VO

Ts:

P

把

(NULL)

NG

钢笔
(pen)

VG

给

R

我

(give)

(me)

WJ

。
(.)

Give

the

pen

to

me

.

Tt:

VBP DT

NN TO PRP PUNC.

NP PP

VP

S

1049

counterparts  and  structural  divergence  across  lan-
guages.  For  the  semantic  equivalence,  we  define 
lexical  and  word  alignment  feature  functions. 
Since  those  feature  functions  are  directional,  we  
describe  most  of  these  functions  as  conditional 
feature  functions  based  on  the  conditional  lexical 
probabilities. We also introduce the tree structural 
features  to  deal  with  the  structural  divergence  of 
bilingual  parse  trees.  Inspired  by  Burkett  and 
Klein (2008), we introduce the feature functions in 
an internal-external manner based on the fact that 
the feature scores for an aligned sub-tree pair tend 
to be high inside both sub-trees, while they tend to 
be low inside one sub-tree and outside the other. 

4.1 

Internal Lexical Features  

We  use  this  feature  to  measure  the  degree  of  se-
mantic equivalence of the sub-tree pair. According 
to the definition of sub-tree alignment in Section 2, 
the  word  sequence  covered  by  the  sub-tree  pair 
should be translational equivalence. Therefore, the 
lexicons  within  the  two  corresponding  sub-spans 
should  be  highly  related  in  semantics.  We  define 
the internal lexical features as follows: 

 

 (  |  )   (∏

        

∑

        

      
)

   (  )   

 

 (  |  )   (∏

        

∑

        

      
)

   (  )   

where          refers  to  the  lexical  translation 
probability  from  the  source  word   to  the  target 
word     within  the  sub-tree  spans,  while         
refers to that from target to source;        refers to 
the  word  set  for  the  internal  span  of  the  source 
sub-tree   , while   (  ) refers to that of the target 
sub-tree   . 

4.2 

Internal-External Lexical Features  

Intuitively, lexical translation probabilities tend to 
be high within the translational equivalence, while 
low  within  the  non-equivalent  counterparts.  Ac-
cording to this, we define the internal-external lex-
ical feature functions as follows: 

 

∑

    (  )

 (  |  )  

         (  ){(             )

∑

    (  )

   

      (  )

 
 }
{(             )

|  (  )|

        

 
 }

  

  

 (  |   )  

 

where         refers  to  the  word  set  for  the  ex-
ternal span of the source sub-tree   , while         
refers to that of the target sub-tree   . We choose a 
representation  different  from  the  internal  lexical 
feature  scores,  since  for  cases  with  small  inner 

span  and  large  outer  span,  the  sum  of  internal-
external scores may be overestimated. As a result, 
we  change  the  sum  operation  into  max,  which  is 
easy to be normalized. 

4.3 

Internal Word Alignment Features  

Although  the  word  alignment  information  within 
bilingual sentence pairs is to some extent not reli-
able,  the  links  of  word  alignment  account  much 
for  the  co-occurrence  of  the  aligned  terms.  We 
define the internal word alignment features as fol-
lows: 

∑

    (  )

∑

    (  )

 
 
       (             )

 
 
(         |  (  )|)

  

 (      )  

where 

                    {

                              
 
                                 

 

The  binary  function           is  introduced  to 
trigger the computation only when a word aligned 
link exists for the two words        within the sub-
tree span. 

4.4 

Internal-External  Word  Alignment  Fea-
tures  

Similar  to  lexical  features,  we  also  introduce  in-
ternal-external word alignment features as follows: 

∑

    (  )

∑

         

 
 
          (               )

 
 
(            |  (  )|)

∑

     (  )

∑

        

 
 
          (               )

 
 
(           |   (  )|)

 

 

 (     )  

 

 (     )  

where 

                    {

                              
 
                                  

4.5  Tree Structural Features 

In addition to the lexical correspondence, we also 
capture  the  structural  divergence  by  introducing 
the tree structural features as follows: 

Span  difference:  Translational  equivalent  sub-
tree  pairs  tend  to  share  similar  length  of  spans. 
Thus  the  model  will  penalize  the  candidate  sub-
tree pairs with largely different length of spans. 

 

 

 (      )   |

        

                  

 

   (  ) 

|  

                   

Number  of  Descendants:  Similarly,  the  num-
ber  of  the  root’s  descendants  of  the  aligned  sub-
trees should also correspond. 

 

 (      )   |

       

                 

 

  (  ) 

                 

|  

1050

where       refers  to  the  descendant  set  of  the 

root to an individual sub-tree. 

Tree Depth difference: Intuitively, translation-
al  equivalent  sub-tree  pairs  tend  to  have  similar 
depth from the root node of the parse tree. We can 
further  allow  the  model  to  penalize  the  candidate 
sub-tree pairs with different distance from the root 
node. 

 (      )   |

         
      (  )

 

     (  )
      (  )

|  

4.6  Binary Grammatical Features 

In  the  previous  sections,  we  design  some  score 
based  feature  functions  to  describe  syntactic  tree 
structural  similarities,  rather  than  directly  using 
the substructures. This is because for limited anno-
tated tree alignment data, features like tokens and 
grammar  rules  are  rather  sparse.  In  spite  of  this, 
we  still  have  a  closed  set  of  grammatical  tags 
which  can  be  covered  by a  small  amount  of  data. 
Therefore,  we  use  the  combination  of  root  gram-
mar tags of the sub-tree pairs as binary features. 

5  Training 

We  train  the  sub-tree  alignment  model  in  two 
steps:  

Firstly,  we  learn  the  various  feature  functions. 
On one hand, GIZA++ is offline trained on a large 
amount of bilingual sentences to compute the lexi-
cal  and  word  alignment  features.  On  the  other 
hand,  the  tree  structural  features,  similar  to  word 
and  phrase  penalty  features  in  phrase  based  SMT 
models, are computed online for both training and 
testing. 

Secondly, we train the MaxEnt model in Eq. 1, 
using  the  training  corpus  which  consists  of  the 
bilingual parse tree pairs with manually annotated 
sub-tree alignment. We apply the widely used GIS 
(Generalized Iterative Scaling) algorithm (Darroch 
 . In practice, we 
and Ratcliff, 1972) to optimize   
modify Och’s implementation YASMET. 

Since we consider each sub-tree pair as an indi-
vidual  instance,  it  is  easy  to  see  that  the  negative 
samples heavily overwhelm the positive ones. For 
GIS  training,  such  a  skewed  distribution  easily 
drives the parameters to facilitate the negative  in-
stances.  We  address  this  problem  by  giving  more 
weight to the positive training instances.  

6  Experiments on Sub-Tree Alignments 

We  utilize  two  different  corpora  to  evaluate  the 
proposed sub-tree alignment method and its capa-
bility to plug in the related applications respective-

ly. One is HIT English Chinese parallel tree bank 
with  both  tree  structure  and  sub-tree  alignment 
manually annotated. The other is the automatically 
parsed bilingual tree pairs (allowing minor parsing 
errors)  with  manually  annotated  sub-tree  align-
ment. The latter benefits MT task, since most lin-
guistically motivated syntax SMT systems require 
a held-out automatic parser to achieve rule induc-
tion. 

6.1  Data preparation 

For the gold standard corpus based experiment, we 
use  HIT 1  Chinese-English  parallel  tree  bank, 
which  is  collected  from  English  learning  text 
books  in  China  as  well  as  example  sentences  in 
dictionaries.  It  consists  of  16131  gold  standard 
parse  tree  pairs  with  manually  annotated  sub-tree 
alignments.  The  annotation  strictly  preserves  the 
semantic  equivalence,  i.e.,  it  only  conducts  sure 
links  in  the  internal  node  level,  while  ignoring 
possible links adopted in word alignment. In con-
trast, in the POS level, n-to-n links are allowed in 
annotation. In order to be consistent with the defi-
nition in Section 2, we delete those n-to-n links in 
POS  level.  The  word  segmentation,  tokenization 
and  parse-tree  in  the  corpus  are  manually  con-
structed or checked. The Chinese parse tree in HIT 
tree  bank  adopts  a  different  annotation  criterion 
from  the  Penn  TreeBank  annotation,  which  is  de-
signed by the HIT research team. The new criteri-
on can better facilitate the description of some rare 
structural  phenomena  in  Chinese.  The  English 
parse  tree  still  uses  Penn  TreeBank  annotation. 
The statistics of HIT corpus is shown in Table 1. 

 

 
# of Sentence pair 
Avg. Sentence Length 
Avg. # of sub-tree 
Avg. # of alignment 

Chinese  English 

16131 

13.06 
21.60 

13.00 
23.74 

11.71 

 

Table 1. Statistics for HIT gold standard Tree bank  

 

Since the induction of sub-tree alignment is de-

signed to benefit the machine translation modeling, 
it  is  preferable  to  conduct  the  sub-tree  alignment 
experiment  on  the  corpus  for  MT  evaluation. 
However, most syntax based SMT systems use an 
automatic parser to facilitate training and decoding, 
which  introduces  parsing  errors.  Additionally,  the 
gold standard HIT corpus is not applicable for MT 

                                                 
1  HIT  corpus  is  designed  and  constructed  by  HIT  mitlab. 
http://mitlab.hit.edu.cn/index.php/resources.html  .    We  li-
censed the corpus from them for research usage. 

1051

experiment due to problems of domain divergence, 
annotation discrepancy (Chinese parse tree adopts 
a  different  grammar  from  Penn  Treebank  annota-
tions) and degree of tolerance for parsing errors. 

Due to the above issues, we annotate a new data 
set  to  apply  the  sub-tree  alignment  in  machine 
translation. We randomly select 300 bilingual sen-
tence pairs from the Chinese-English FBIS corpus 
with the length      in both the source and target 
sides. The selected plain sentence pairs are further 
parsed  by  Stanford  parser  (Klein  and  Manning, 
2003) on both  the English and Chinese sides. We 
manually  annotate  the  sub-tree  alignment  for  the 
automatically  parsed  tree  pairs  according  to  the 
definition in Section 2. To be fully consistent with 
the  definition,  we  strictly  preserve  the  semantic 
equivalence  for  the  aligned  sub-trees  to  keep  a 
high precision. In other words, we do not conduct 
any  doubtful  links.  The  corpus  is  further  divided 
into 200 aligned tree pairs for training and 100 for 
testing.  Some  initial  statistic  of  the  automatically 
parsed corpus is shown in Table 2. 

 
 

 
# of Sentence pair 

Train  Avg. Sentence Length 

 
 

Test 

 
 
 

Avg. # of sub-tree 
Avg. # of alignment 
# of Sentence pair 
Avg. Sentence Length 
Avg. # of sub-tree 
Avg. # of alignment 

Chinese  English 

200 

17 

28.87 

20.84 
34.54 

17.07 
100 

16.84 
29.18 

20.75 
34.1 

17.75 

Table 2. FBIS selected Corpus Statistics 

 

6.2  Baseline approach 

We implement the work in Tinsley et al. (2007) as 
our baseline methodology. 

Given  a  tree  pair           ,  the  baseline  ap-
proach first takes all the links between the sub-tree 
pairs  as  alignment  hypotheses,  i.e.,  the  Cartesian 
product of the two sub-tree sets: 

{                  }   {                  } 

 By  using  the  lexical  translation  probabilities, 
each  hypothesis  is  assigned  an  alignment  score. 
All  hypotheses  with  zero  score  are  pruned  out. 
Then  the  algorithm  iteratively  selects  the  link  of 
the  sub-tree  pairs  with  the  maximum  score  as  a 
sure link, and blocks all hypotheses that contradict 
with  this  link  and  itself,  until  no  non-blocked  hy-
potheses remain. 

The  baseline  system  uses  many  heuristics  in 
searching  the  optimal  solutions  with  alternative 
score  functions.  Heuristic  skip1  skips  the  tied  hy-

potheses  with  the  same  score,  until  it  finds  the 
highest-scoring hypothesis with no competitors of 
the  same  score.  Heuristic  skip2  deals  with  the 
same  problem.  Initially,  it  skips  over  the  tied  hy-
potheses.  When  a  hypothesis  sub-tree  pair           
without any competitor of the same score is found, 
where  neither    nor    has  been  skipped  over,  the 
hypothesis  is  chosen  as  a  sure  link.  Heuristic 
span1  postpones  the  selection  of  the  hypotheses 
on  the  POS  level.  Since  the  highest-scoring  hy-
potheses  tend  to  appear  on  the  leaf  nodes,  it  may 
introduce  ambiguity  when  conducting  the  align-
ment  for  a  POS  node  whose  child  word  appears 
twice in a sentence. 

The  baseline  method  proposes  two  score  func-
tions  based  on  the  lexical  translation  probability. 
They also compute the score function by splitting 
the tree into the internal and external components. 
Tinsley  et  al.  (2007)  adopt  the  lexical  transla-
tion  probabilities  dumped  by  GIZA++  (Och  and 
Ney,  2003)  to  compute  the  span  based  scores  for 
each pair of sub-trees. Although all of their heuris-
tics combinations are re-implemented in our study, 
we  only  present  the  best  result  among  them  with 
the  highest  Recall  and  F-value  as  our  baseline, 
denoted as skip2_s1_span12. 

6.3  Experimental settings 

 To  examine  the  effectiveness  of  the  proposed 
features, we  
    (1) learn the word alignment using the combina-
tion of the 14k of HIT tree bank and FBIS (240k) 
corpus  for  both  our  approach  and  the  baseline 
method,  and  divide  the  remaining  HIT  corpus  as 
1k for training and 1k for testing. 
    (2) learn the word alignment on the entire FBIS 
training  corpus  (240k)  for  both  our  approach  and 
the  baseline  method.  We  then  train  and  test  on 
FBIS corpus of 200 and 100 respectively as stated 
in Table 2. 
 In our task, annotating large amount of sub-tree 
alignment corpus is time consuming and more dif-
ficult compared with the tasks like sequence label-
ing. One of the important issues we are concerned 
about  is  whether  we  can  achieve  an  acceptable 
performance with limited training data. We  
    (3)  adopt  the  entire  FBIS  data  (240k)  to  learn 
the  word  alignment  and  various  amount  of  HIT 
gold  standard  corpus  to  train  the  MaxEnt  model. 
Then  we  test  the  alignment  performance  on  the 
same HIT test set (1k) as (1). 

                                                 
2 s1 denotes score function 1 in Tinsley et al. (2007) 

1052

Features 
   In Lexical 
+ InOut Lexical 
+ In word align 
+ InOut word align 
+ Tree Structure  
+ Binary Feature 
 Baseline [Tinsley 2007] 

Precision  Recall 
48.11 
53.84 
60.59 
62.25 
63.11 
85.11 
66.99 

50.96 
55.26 
56.16 
55.80 
57.64 
73.14 
64.14 

 

F-value 
49.49 
54.54 
58.29 
58.85 
60.25 
78.67 
65.53 

Features 
   In Lexical 
+ InOut Lexical 
+ In word align 
+ InOut word align 
+ Tree Structure  
+ Binary Feature 
  Baseline [Tinsley 2007] 

Precision  Recall  F-value 
58.88 
64.81 
73.30 
75.89 
76.23 
80.42 
74.36 

63.53 
66.00 
70.89 
72.05 
72.03 
76.08 
70.48 

54.87 
63.66 
75.88 
80.16 
80.95 
85.29 
78.70 

 

Table 3. Sub-tree alignment of different feature  

combination for HIT gold standard test set 

Table 4. Sub-tree alignment of different  

feature combination for FBIS test set 

 We  further  test  the  robustness  of  our  method 
under different amount of data to learn the lexical 
and  word  alignment  feature  functions.  We  gradu-
ally change the amount of FBIS corpus to train the 
word alignment. Then we  
    (4)  use  the  same  training  (1k)  and  testing  data 
(1k) with (1);  
    (5) use FBIS corpus 200 to train MaxEnt model 
and 100 for testing similar to (2). 

6.4  Experimental results 

We  use  Precision,  Recall  and  F-score  to  measure 
the  alignment  performance  and  obtain  the  results 
as follows: 
 In Table 3 and 4 for Exp (1) and (2) respectively, 
we  show  that  by  incrementally  adding  new  fea-
tures  in  a  certain  order,  the  F-value  consistently 
increases and both outperform the baseline method. 
From  both  tables,  we  find  that  the  Binary  fea-
tures, with the combination of root grammar tags 
of  the  sub-tree  pairs,  significantly  improve  the 
alignment  performance.  We  also  try  the  different 
combinations of the parent, child or even  siblings 
to  the  root  nodes.  However,  all  these  derivative 
configurations  decrease  the  performance.  We  at-
tribute the ineffectiveness to data sparseness. Fur-
ther exploration suggests that the binary feature in 
HIT  gold  standard  corpus  exhibits  a  substantially 
larger  improvement  against  other  features  than 
FBIS corpus (Table 3 against Table 4). The reason 
could be  that  the  grammar  tags  in  the gold  stand-
ard corpus are accurate, while FBIS corpus suffers 
from  parsing  errors.  Apart  from  that,  the  lexi-
cal/word-alignment features in Table 3 do not per-
form  well,  since  the  word  alignment  is  trained 
mainly  on  the  cross  domain  FBIS  corpus.  This  is 
also  an  important  reason why  there  is a  large  gap 
in performance between Table 3 and 4, where the 
automatic  parsed  FBIS  corpus  performs  better 
than HIT gold standard tree bank in all configura-
tions as well as the baseline. 

 In  Fig.  2(a)  for  Exp  (3),  we  examine  perfor-
mance  under  different  amount  of  training  data 
from 1k to 15k. The results change very little with 
over the amount of 1k. Even with only 0.25k train-
ing  data,  we  are  able  to  gain  a  result  close  to  the 
best  performance.  This  suggests  that  by  utilizing 
only  a  small  amount  of  sub-tree  aligned  corpus, 
we can still achieve a satisfactory alignment result. 
The  benefits  come  from  the  usage  of  the  score 
based  feature  functions  by  avoiding  using  sub-
structures  as  binary  features,  which  suffers  from 
the data sparseness problem.  
 In  Fig.  2(b-e)  for  Exp  (4&5),  we  find  that  in-
creasing  the  amount  of  corpus  to  train  GIZA++ 
does  not  improve  much  for  the  proposed  method 
on  both  HIT  gold  standard  corpus  (Fig.  2:  b,  c) 
and the automatic parsed data (Fig. 2: d, e). This is 
due to the various kinds of features utilized by the 
MaxEnt  model,  which  does  not  bet  on  the  lexical 
and  word  alignment  feature  too  much.  As  for  the 
baseline  method,  we  can  only  detect  a  relatively 
large improvement in the initial increment of cor-
pus,  while  later  additions  do  not  help.  This  result 
suggests that the baseline method is relatively less 
extensible since it works completely on the lexical 
similarities  which  can  be  only  learned  from  the 
word alignment corpus.  

7  Experiments on Machine Translation 

In  addition  to  the  alignment  evaluation,  we  con-
duct MT evaluation as well. We explore the effec-
tiveness of sub-tree alignment for both phrase and 
linguistically motivated syntax based systems. 

7.1  Experimental configuration 

In the experiments, we train the translation model 
on FBIS corpus (7.2M (Chinese) + 9.2M (English) 
words  in  240,000  sentence  pairs)  and  train  a  4-
gram language model on the Xinhua portion of the 
English Gigaword corpus (181M words) using the 
SRILM  Toolkits  (Stolcke,  2002).  We  use  these 

1053

                                    a                                                                                        b                                                                                     c 

 

 

 

 

 

 

 

        d 

 

 

 

 

    e 

 

Figure 2: a. Precision/Recall/F-score for various amount of training data (k).  

b~e. Various amount of data to train word alignment 

b. Precision/Recall for HIT test set. c. F-score for HIT test set.  
d. Precision/Recall for FBIS test set. e. F-score for FBIS test set. 

sentences  with  less  than  50  characters  from  the 
NIST MT-2002 test set as the development set (to 
speed  up  tuning  for  syntax  based  system)  and  the 
NIST MT-2005 test set as our test set. We use the 
Stanford  parser  (Klein  and  Manning,  2003)  to 
parse  bilingual  sentences  on  the  training  set  and 
Chinese sentences on the development and test set. 
The evaluation metric is case-sensitive BLEU-4. 

For  the  phrase  based  system,  we  use  Moses 
(Koehn  et  al,  2007)  with  its  default  settings.  For 
the syntax based system, since sub-tree alignment 
can  directly  benefit  Tree-2-Tree  based  systems, 
we apply the sub-tree alignment in an SMT system 
based on Synchronous Tree Substitution Grammar 
(STSG)  (Zhang  et  al.,  2007).  The  STSG  based 
decoder  uses  a  pair  of  elementary  tree  as  a  basic 
translation unit. Recent research on tree based sys-
tems  shows  that  relaxing  the  restriction  from  tree 
structure  to  tree  sequence  structure  (Synchronous 
Tree  Sequence  Substitution  Grammar:  STSSG) 
significantly improves the translation performance 
(Zhang  et  al.,  2008).  We 
the 
STSG/STSSG based model in Pisces decoder with 
the same features and settings in Sun et al. (2009). 
The  STSSG  based  decoder  translates  each  span 
iteratively  in  a  bottom  up  manner  which  guaran-
tees that when translating a source span, any of its 
sub-spans  has  already  been  translated.  The  STSG 

implement 

based  experiment  can  be  easily  achieved  by  re-
stricting  the  translation  rule  set  in  the  STSSG  de-
coder to be elementary tree pairs only. 

For  the  alignment  setting  of  the  baselines,  we 
use  the  word  alignment  trained  on  the  entire 
FBIS(240k)  corpus  by  GIZA++  with  heuristic 
grow-diag-final for Moses and the syntax systems 
and  perform  rule  extraction  constrained  on  the 
word  alignment.  As  for  the  experiments  adopting 
sub-tree  alignment,  we  use  the  above  word  align-
ment to learn lexical/word alignment features, and 
train  the  sub-tree  alignment  model  with  FBIS 
training data (200).  

7.2  Experimental results 

Utilizing  the syntactic  rules  only  has  been  argued 
to  be  ineffective  (Koehn  et  al.,  2003).  Therefore, 
instead of using the sub-tree aligned rules only, we 
try  to  improve  the  word  alignment  constrained 
rule set by sub-tree alignment as shown in Table 5.  
Firstly,  we  try  to  Directly  Concatenate  (DirC) 
the  sub-tree  alignment  constraint  rule  set 3 to  the 
original  syntax/phrase  rule  set  based  on  word 
alignment.  Then  we  re-train  the  MT  model  based 

                                                 
3 For syntax based system, it’s just the sub-tree pairs deducted 
from the sub-tree alignment; for phrase based system, it's the 
phrases with context equivalent to the aligned sub-tree pairs. 

0.9

0.8

0.7

0.6

Precision
Recall
F-value

0.5

0.25 1

3

5

10

15

1

0.9

0.8

0.7

0.6

0.5

0.4

0

Precision
Recall
Baseline-Precision
Baseline-Recall

50

100

150

200

250

1

0.9

0.8

0.7

0.6

0.5
0

F-valule
Baseline-F-value

50

100

150

200

250

1

0.9

0.8

0.7

0.6

0.5

0

Precision
Recall
Baseline-Precision
Baseline-Recall

50

100

150

200

250

1

0.9

0.8

0.7

0.6

0.5

0

50

100

150

200

250

F-valule
Baseline-F-value

1054

 

Figure 3: Comparison between Sub-tree alignment results and Word alignment results  

 

 

on  the  obtained  rule  set.  Tinsley  et  al.  (2009)  at-
tempts  different  duplication  of  sub-tree  alignment 
constraint rule set to append to the original phrase 
rule  set  and  reports  positive  results.  However,  as 
shown  in  Table  5,  we  only  achieve  very  minor 
improvement  (in  STSSG  based  model  the  score 
even drops) by direct introducing the new rules.  

Secondly, we propose a new approach to utilize 
sub-tree  alignment  by  modifying  the  rule  extrac-
tion process. We allow the bilingual phrases which 
are consistent with Either Word alignment or Sub-
tree alignment (EWoS) instead of to be consistent 
with  word  alignment  only.  The  results  in  Table  5 
show  that  EWoS  achieves  consistently  better  per-
formance than the baseline and DirC method. We 
also  find  that  sub-tree  alignment  benefits  the 
STSSG  based  model  less  compared  with  other 
systems.  This  is  probably  due  to  the  fact  that  the 
STSSG  based  system  relies  much  on  the  tree  se-
quence rules. 

To  benefit  intuitive  understanding,  we  provide 
two  alignment  snippets in the MT training  corpus 
in  Fig.  3,  where  the  red  lines  across  the  non-
terminal  nodes  are  the  sub-tree  aligned  links  con-
ducted by our model, while the purple lines across 
the  terminal  nodes  are  the  word  alignment  links 
trained by GIZA++. In the first example, the word 
Israel  is  wrongly  aligned  to  two  “以色列”s  by 
GIZA++,  where  the  wrong  link  is  denoted  by  the 
dash  line.  This  is  common,  since  in  a  compound 
sentence  in  English,  the  entities  appeared  more 
than  once  are  often  replaced  by  pronouns  at  its 
later  appearances.  Therefore,  the  syntactic  rules 
constraint  by  NR1-NNP1,  IP2-VP2  and  PP3-VP3 
respectively  cannot  be  extracted  for  syntax  sys-
tems; while for phrase systems, context around the 
first  “以色列”  cannot  be  fully  explored.  In  the 

System 
Moses 

 

Syntax 
STSG 

 

Syntax 
STSSG 

 

Rules 
BP* 
DirC  
EWoS  
STSG 
DirC  
EWoS  
STSSG 
DirC  
EWoS  

 

BLEU 
23.86 
24.12 
24.45 
24.71 
24.91 
25.21 
25.92 
25.88 
26.12 

Table 5. MT evaluation on various systems 

BP* denotes bilingual phrases.  

BP, STSG, STSSG are baseline rule sets using word 

alignment to constrain rule extraction. 

second example, the empty word  “了” is wrongly 
aligned,  which  usually  occurs  in  Chinese-English 
word  alignment.  As  shown  in  Fig.  3,  both  cases 
can  be  resolved  by  sub-tree  alignment  conducted 
by our model, indicating that sub-tree alignment is 
a decent supplement to the word alignment rule set.  

8  Conclusion 

In  this  paper,  we  propose  a  framework  for  bilin-
gual  sub-tree  alignment  using  Maximum  Entropy 
model.  We  explore  various  lexical  and  structural 
features  to  improve  the  alignment  performance. 
We also manually  annotated  the automatic parsed 
tree  pairs  for  both  alignment  evaluation  and  MT 
experiment.  Experimental  results  show  that  our 
alignment  framework  significantly  outperforms 
the baseline method and the proposed features are 
very  effective  to  capture  the  bilingual  structural 
similarity. Additionally, we find that our approach 
can  perform  well  using  only  a  small  amount  of 
sub-tree  aligned  training  corpus.  Further  experi-
ment shows that our approach benefits both phrase 
and syntax based MT systems. 

PP3

P

IP2

CP

VP

NP

VP

VP

DEC AD

VV

NR

NN VV

AS

VV

NR1

S1:

T1:

为
(to)

反对

(oppose)

以色列
(Israel)

的
(`s)

非法
(illegal)

占领

...

(occupation)

To oppose Israel

`s

illegal occupation

NNP1

POS

以色列
(Israel)

S2:

T2:

巴拉克
(Barak)

政府

选择

了

(government)

(choose)

(NULL)

the

Barak Government

chose

NP

JJ

NN

DT NNP

NNP

NNP

VB

NP

NP

TO

2

VP

VP3

1055

for  Statistical  Machine  Translation.  In  Proceedings 
of ACL-IJCNLP-09. 914-922. 

John  Tinsley,  Ventsislav  Zhechev,  Mary  Hearne,  and 
Andy Way. 2007. Robust language pair-independent 
sub-tree  alignment.  In  Proceedings  of  Machine 
Translation Summit-XI-07. 

John  Tinsley,  Mary  Hearne,  and  Andy  Way.  2009. 
Parallel  treebanks  in  phrase-based  statistical  ma-
chine translation. In Proceedings of CICLING-09. 

Min  Zhang,  Hongfei  Jiang,  AiTi  Aw,  Jun  Sun,  Sheng 
Li  and  Chew  Lim  Tan.  2007.  A  tree-to-tree  align-
ment-based model for statistical machine translation. 
In Proceedings of MT Summit-XI -07. 535-542. 

Min  Zhang,  Hongfei  Jiang,  AiTi  Aw,  Haizhou  Li, 
Chew Lim Tan and Sheng Li. 2008. A tree sequence 
alignment-based  tree-to-tree  translation  model.  In 
Proceedings of ACL-08. 559-567. 

References  

David  Burkett  and  Dan  Klein.  2008.  Two  languages 
are  better  than  one  (for  syntactic  parsing).  In  Pro-
ceedings of EMNLP-08. 877-886. 

Michel Galley,  Jonathan Graehl, Kevin Knight, Daniel 
Marcu,  Steve  DeNeefe,  Wei  Wang  and  Ignacio 
Thayer.  2006.  Scalable  Inference  and  training  of 
context-rich  syntactic  translation  models.  In  Pro-
ceedings of COLING-ACL-06. 961-968. 

Jonathan Graehl and Kevin Knight. 2004. Training tree 
transducers.  In  Proceedings  of  HLT-NAACL-2004. 
105-112. 

Declan  Groves,  Mary  Hearne,  and  Andy  Way.  2004. 
Robust  sub-sentential  alignment  of  phrase-structure 
trees.  In  Proceedings  of  COLING-04,  pages  1072-
1078. 

Liang  Huang,  Kevin  Knight  and  Aravind  Joshi.  2006. 
Statistical  syntax-directed  translation  with  extended 
domain of Locality. In Proceedings of AMTA-06. 

Kenji  Imamura.  2001.  Hierarchical  Phrase  Alignment 
Harmonized with Parsing. In Proceedings of NLPRS. 
377-384. 

Dan  Klein  and  Christopher  D.  Manning.  2003.  Accu-
rate Unlexicalized Parsing. In Proceedings of ACL-
03. 423-430. 

Philipp  Koehn,  Hieu  Hoang,  Alexandra  Birch,  Chris 
Callison-Burch,  Marcello  Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin and Evan Herbst. 2007. Moses: Open Source 
Toolkit  for  Statistical  Machine  Translation.  In  Pro-
ceedings of ACL-07. 177-180. 

Philipp  Koehn,  Franz  Josef  Och  and  Daniel  Marcu. 
2003.  Statistical  Phrase-based  Translation.  In  Pro-
ceedings of HLT-NAACL-2003. 48-54. 

Yang  Liu,  Qun  Liu  and  Shouxun  Lin.  2006.  Tree-to-
String  alignment  template  for  statistical  machine 
translation. In Proceedings of ACL-06, 609-616. 

Daniel  Marcu,  Wei  Wang,  Abdessamad  Echihabi  and 
Kevin  Knight.  2006.  SPMT:  statistical  machine 
translation  with 
language 
phrases. In Proceedings of EMNLP-06. 44-52. 

syntactified 

target 

Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison  of  various  statistical  alignment  models. 
Computational Linguistics, 29(1):19-51, March. 

Andreas  Stolcke.  2002.  SRILM  -  an  extensible  lan-
guage  modeling  toolkit.  In  Proceedings  of  ICSLP-
02. 901-904. 

Jun Sun, Min Zhang and Chew Lim Tan. 2009. A non-
contiguous  Tree  Sequence  Alignment-based  Model 

