



















































Unsupervised Semantic Role Induction with Graph Partitioning


Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1320–1331,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics

Unsupervised Semantic Role Induction with Graph Partitioning

Joel Lang and Mirella Lapata
Institute for Language, Cognition and Computation

School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK

J.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.uk

Abstract

In this paper we present a method for unsuper-
vised semantic role induction which we for-
malize as a graph partitioning problem. Ar-
gument instances of a verb are represented as
vertices in a graph whose edge weights quan-
tify their role-semantic similarity. Graph par-
titioning is realized with an algorithm that it-
eratively assigns vertices to clusters based on
the cluster assignments of neighboring ver-
tices. Our method is algorithmically and con-
ceptually simple, especially with respect to
how problem-specific knowledge is incorpo-
rated into the model. Experimental results on
the CoNLL 2008 benchmark dataset demon-
strate that our model is competitive with other
unsupervised approaches in terms of F1 whilst
attaining significantly higher cluster purity.

1 Introduction

Recent years have seen increased interest in the shal-
low semantic analysis of natural language text. The
term is most commonly used to describe the au-
tomatic identification and labeling of the seman-
tic roles conveyed by sentential constituents (Gildea
and Jurafsky, 2002). Semantic roles describe the se-
mantic relations that hold between a predicate and
its arguments (e.g., “who” did “what” to “whom”,
“when”, “where”, and “how”) abstracting over sur-
face syntactic configurations.

In the example sentences below, window occu-
pies different syntactic positions — it is the object of
broke in sentences (1a,b), and the subject in (1c) —
while bearing the same semantic role, i.e., the phys-

ical object affected by the breaking event. Analo-
gously, ball is the instrument of break both when
realized as a prepositional phrase in (1a) and as a
subject in (1b).

(1) a. [Jim]A0 broke the [window]A1 with a
[ball]A2.

b. The [ball]A2 broke the [window]A1.
c. The [window]A1 broke [last night]TMP.

The semantic roles in the examples are labeled in
the style of PropBank (Palmer et al., 2005), a broad-
coverage human-annotated corpus of semantic roles
and their syntactic realizations. Under the Prop-
Bank annotation framework (which we will assume
throughout this paper) each predicate is associated
with a set of core roles (named A0, A1, A2, and so
on) whose interpretations are specific to that predi-
cate1 and a set of adjunct roles such as location or
time whose interpretation is common across predi-
cates (e.g., last night in sentence (1c)).

The availability of PropBank and related re-
sources (e.g., FrameNet; Ruppenhofer et al. (2006))
has sparked the development of great many seman-
tic role labeling systems most of which conceptu-
alize the task as a supervised learning problem and
rely on role-annotated data for model training. Most
of these systems implement a two-stage architec-
ture consisting of argument identification (determin-
ing the arguments of the verbal predicate) and ar-
gument classification (labeling these arguments with
semantic roles). Despite being relatively shallow, se-

1More precisely, A0 and A1 have a common interpretation
across predicates as proto-agent and proto-patient in the sense
of Dowty (1991).

1320



mantic role analysis has the potential of benefiting a
wide spectrum of applications ranging from infor-
mation extraction (Surdeanu et al., 2003) and ques-
tion answering (Shen and Lapata, 2007), to machine
translation (Wu and Fung, 2009) and summarization
(Melli et al., 2005).

Current approaches have high performance — a
system will recall around 81% of the arguments cor-
rectly and 95% of those will be assigned a cor-
rect semantic role (see Màrquez et al. (2008) for
details), however only on languages and domains
for which large amounts of role-annotated training
data are available. For instance, systems trained on
PropBank demonstrate a marked decrease in per-
formance (approximately by 10%) when tested on
out-of-domain data (Pradhan et al., 2008). Unfortu-
nately, the reliance on role-annotated data which is
expensive and time-consuming to produce for every
language and domain, presents a major bottleneck to
the widespread application of semantic role labeling.

In this paper we argue that unsupervised meth-
ods offer a promising yet challenging alternative. If
successful, such methods could lead to significant
savings in terms of annotation effort and ultimately
yield more portable semantic role labelers that re-
quire overall less engineering effort. Our approach
formalizes semantic role induction as a graph parti-
tioning problem. Given a verbal predicate, it con-
structs a weighted graph whose vertices correspond
to argument instances of the verb and whose edge
weights quantify the similarity between these in-
stances. The graph is partitioned into vertex clus-
ters representing semantic roles using a variant of
Chinese Whispers, a graph-clustering algorithm pro-
posed by Biemann (2006). The algorithm iteratively
assigns cluster labels to graph vertices by greedily
choosing the most common label amongst the neigh-
bors of the vertex being updated. Beyond extend-
ing Chinese Whispers to the semantic role induc-
tion task, we also show how it can be understood
as a type of Gibbs sampling when our graph is inter-
preted as a Markov random field.

Experimental results on the CoNLL 2008 bench-
mark dataset demonstrate that our method, de-
spite its simplicity, improves upon competitive ap-
proaches in terms of F1 and achieves significantly
higher cluster purity.

2 Related Work

Although the bulk of previous work on semantic role
labeling has primarily focused on supervised meth-
ods (Màrquez et al., 2008), a few semi-supervised
and unsupervised approaches have been proposed
in the literature. The majority of semi-supervised
models have been developed within a framework
known as annotation projection. The idea is to com-
bine labeled and unlabeled data by projecting an-
notations from a labeled source sentence onto an
unlabeled target sentence within the same language
(Fürstenau and Lapata, 2009) or across different lan-
guages (Padó and Lapata, 2009). Outwith annota-
tion projection, Gordon and Swanson (2007) pro-
pose to increase the coverage of PropBank to un-
seen verbs by finding syntactically similar (labeled)
verbs and using their annotations as surrogate train-
ing data.

Swier and Stevenson (2004) were the first to intro-
duce an unsupervised semantic role labeling system.
Their algorithm induces role labels following a boot-
strapping scheme where the set of labeled instances
is iteratively expanded using a classifier trained on
previously labeled instances. Their method starts
with a dataset containing no role annotations at all,
but crucially relies on VerbNet (Kipper et al., 2000)
for identifying the arguments of predicates and mak-
ing initial role assignments. VerbNet is a manually
constructed lexicon of verb classes each of which is
explicitly associated with argument realization and
semantic role specifications.

Subsequent work has focused on unsupervised
methods for argument identification and classifica-
tion. Abend et al. (2009) recognize the arguments of
predicates by relying solely on part of speech anno-
tations whereas Abend and Rappoport (2010) distin-
guish between core and adjunct roles, using an unsu-
pervised parser and part-of-speech tagger. Grenager
and Manning (2006) address the role induction prob-
lem and propose a directed graphical model which
relates a verb, its semantic roles, and their possible
syntactic realizations. Latent variables represent the
semantic roles of arguments and role induction cor-
responds to inferring the state of these latent vari-
ables.

Following up on this work, Lang and Lapata
(2010) formulate role induction as the process of de-

1321



tecting alternations and finding a canonical syntactic
form for them. Verbal arguments are then assigned
roles, according to their position in this canonical
form, since each position references a specific role.
Their model extends the logistic classifier with hid-
den variables and is trained in a manner that takes
advantage of the close relationship between syntac-
tic functions and semantic roles. More recently,
Lang and Lapata (2011) propose a clustering algo-
rithm which first splits the argument instances of
a verb into fine-grained clusters based on syntac-
tic cues and then executes a series of merge steps
(mainly) based on lexical cues. The split phase cre-
ates a large number of small clusters with high purity
but low collocation, i.e., while the instances in a par-
ticular cluster typically belong to the same role the
instances for a particular role are commonly scat-
tered amongst many clusters. The subsequent merge
phase conflates clusters with the same role in order
to increase collocation.

Like Grenager and Manning (2006) and Lang
and Lapata (2010; 2011), this paper describes an
unsupervised method for semantic role induction,
i.e., one that does not require any role annotated data
or additional semantic resources for training. Con-
trary to these previous approaches, we conceptualize
role induction in a novel way, as a graph partitioning
problem. Our method is simple, computationally ef-
ficient, and does not rely on hidden variables. More-
over, the graph-based representation for verbs and
their arguments affords greater modeling flexibility.
A wide range of methods exist for finding partitions
in graphs (Schaeffer, 2007), besides Chinese Whis-
pers (Biemann, 2006), which could be easily applied
to the semantic role induction problem. However,
we leave this to future work.

Graph-based methods are popular in natural lan-
guage processing, especially with unsupervised
learning problems (Chen and Ji, 2010). The Chinese
Whispers algorithm itself (Biemann, 2006) has been
previously applied to several tasks including word
sense induction (Klapaftis and M., 2010) and unsu-
pervised part-of-speech tagging (Christodoulopou-
los et al., 2010). The same algorithm is also de-
scribed in Abney (2007, pp. 146-147) under the
name “clustering by propagation”. The term makes
explicit the algorithm’s connection to label propa-

gation, a general framework2 for semi-supervised
learning (Zhu et al., 2003) with applications to
machine translation (Alexandrescu and Kirchhoff,
2009), information extraction (Talukdar and Pereira,
2010) and structured part-of-speech tagging (Sub-
ramanya et al., 2010). The basic idea behind la-
bel propagation is to represent labeled and unlabeled
instances as vertices in an undirected graph with
edges whose weights express similarity (and possi-
bly dissimilarity) between the instances. Label in-
formation is then propagated between the vertices
in such a way that similar instances tend to be as-
signed the same label. Analogously, Chinese Whis-
pers works by propagating cluster membership in-
formation along the edges of a graph, even though
the graph does not contain any human-labeled in-
stance vertices.

3 Problem Setting

We adopt the standard architecture of supervised se-
mantic role labeling systems where argument identi-
fication and argument classification are treated sep-
arately. Our role labeler is fully unsupervised with
respect to both tasks — it does not rely on any role
annotated data or semantic resources. However, our
system does not learn from raw text. In common
with most semantic role labeling research, we as-
sume that the input is syntactically analyzed in the
form of dependency trees.

We view argument identification as a syntactic
processing step that can be largely undertaken deter-
ministically through structural analysis of the depen-
dency tree. We therefore use a small set of rules to
detect arguments with high precision and recall (see
Section 4). Argument classification is more chal-
lenging and must take into account syntactic as well
as lexical-semantic information. Both types of in-
formation are incorporated into our model through
a similarity function that assigns similarity scores
to pairs of argument instances. Following previous
work (Lang and Lapata, 2010; Grenager and Man-
ning, 2006), our system outputs verb-specific roles
by grouping argument instances into clusters and la-
beling each argument instance with an identifier cor-

2For example, Haffari and Sarkar (2007) use label propa-
gation to analyze other semi-supervised algorithms such as the
Yarowsky (1995) algorithm.

1322



responding to the cluster it has been assigned to.
Such identifiers are similar to PropBank-style core
labels (e.g., A0, A1).

4 Argument Identification

Supervised semantic role labelers often employ a
classifier in order to decide for each node in the
parse tree whether or not it represents a semantic
argument. Nodes classified as arguments are then
assigned a semantic role. In the unsupervised set-
ting, we slightly reformulate argument identification
as the task of discarding as many non-semantic ar-
guments as possible. This means that the argument
identification component does not make a final posi-
tive decision for any of the argument candidates; in-
stead, a final decision is only made in the subsequent
argument classification stage.

We discard or select argument candidates us-
ing the set of rules developed in Lang and Lap-
ata (2011). These are mainly based on the parts
of speech and syntactic relations encountered when
traversing a dependency tree from the predicate
node to the argument node. For each candidate,
rules are considered in a prespecified order and the
first matching rule is applied. When evaluated on
its own, the argument identification component ob-
tained 88.1% precision (percentage of semantic ar-
guments out of those identified) and 87.9% recall
(percentage of identified arguments out of all gold
arguments).

5 Argument Classification

After identifying likely arguments for each verb,
the next step is to infer a label for each argument
instance. Since we aim to induce verb-specific
roles (see Section 3), we construct an undirected,
weighted graph for each verb. Vertices corre-
spond to verb argument instances and edge weights
quantify the similarities between them. This
argument-instance graph is then partitioned into
clusters of vertices representing semantic roles and
each argument instance is assigned a label that indi-
cates the cluster it belongs to. In what follows we
first describe how the graph is constructed and then
provide the details of our graph partitioning algo-
rithm.

CA

E D

B

0.4
0.1

0.8 −1

1

0.3

0.2 0.7

Figure 1: Simplified example of an argument-instance
graph. All pairs of vertices with non-zero similarity are
connected through edges that are weighted with a simi-
larity score φ(vi,v j). Upon updating the label for a vertex
all neighboring vertices propagate their label to the vertex
being updated. The score for each label is determined by
summing together the weighted votes for that label and
the label with the maximal score is chosen.

5.1 Graph Construction

For each verb we construct an undirected, weighted
graph G = (V,E,φ) with vertices V , edges E, and
edge weight function φ as follows. Each argu-
ment instance in the corpus that belongs to the
verb is added as a vertex. Then, for each possi-
ble pair of vertices (vi,v j) we compute a weight
φ(vi,v j) ∈ R according to the function φ. If the
weight is non-zero, an undirected edge e = (vi,v j)
with weight φ(vi,v j) is added to the graph. The func-
tion φ quantifies the similarity or dissimilarity be-
tween instances; positive values indicate that roles
are likely to be the same, negative values indicate
that roles are likely to differ, and zero values indicate
that there is no evidence for either case. Our simi-
larity function is symmetric, i.e., φ(vi,v j) = φ(v j,vi)
and permits negative values (see Section 5.4 for a
detailed description).

Figure 1 shows an example of a graph for a verb
with five argument instances (vertices A–E). Edges
are drawn between pairs of vertices with non-zero
similarity values. For instance, vertex D is con-
nected to vertex A with weight 0.2, to vertex E
with 1, and vertex C with−1. Since edges are drawn
between all pairs of vertices with non-zero simi-
larity, the resulting graphs tend to be densely con-
nected, which for large datasets may be prohibitively

1323



inefficient. A solution would be to sample a subset
from all possible pairs, but we did not make use of
any kind of edge pruning in our experiments.

5.2 Graph Partitioning
Graph partitioning is realized with a variant of Chi-
nese Whispers (Biemann, 2006) whose details are
given below. In addition, we discuss how our algo-
rithm relates to other graph-based models in order to
help provide a better theoretical understanding.

We assume each vertex vi is assigned a label
li ∈ {1 . . .L} indicating the cluster it belongs to. Ini-
tially, each vertex belongs to its own cluster, i.e., we
let the number of clusters L = |V | and set li← i.
Given this initial vertex labeling, the algorithm pro-
ceeds by iteratively updating the label for each ver-
tex. The update is based on the labels of neighbor-
ing vertices and reflects their similarity to the vertex
being updated. Intuitively, each neighboring vertex
votes for the cluster it is currently assigned to, where
the strength of the vote is determined by the similar-
ity (i.e., edge weight) to the vertex being updated.
The label li of vertex vi is thus updated according to
the following equation:

li← arg max
l∈{1...L} ∑v j∈Ni(l)

φ(vi,v j) (2)

where Ni(l) = {v j|(vi,v j) ∈ E ∧ l = l j} denotes the
set of vi’s neighbors with label l. In other words,
for each label we compute a score by summing to-
gether the weights of edges to neighboring vertices
with that label and select the label with the maximal
score. Note that negative edges decrease the score
for a particular label, thus demoting the label.

Consider again Figure 1. Assume we wish to up-
date vertex A. In addition, assume that B and E are
currently assigned the same label (i.e., they belong
to the same cluster) whereas C and D are each in
different clusters. The score for cluster {B,E} is
0.4+0.8 = 1.2, the score for cluster {C} is 0.3 and
the score for cluster {D} is 0.2. We would thus as-
sign A to cluster {B,E} as it has the highest score.

The algorithm is run for several iterations. At
each iteration it passes over all vertices, and the up-
date order of the vertices is chosen randomly. As
the updates proceed, labels can disappear from the
graph, whereby the number of clusters decreases.
Empirically, we observe that for sufficiently many

iterations the algorithm converges to a fixed labeling
or oscillates between labelings that differ only in a
few vertices. The result of the algorithm is a hard
partitioning of the given graph, where the number of
clusters is determined automatically.

5.3 Propagation Prioritization
We make one important modification to the basic al-
gorithm described so far based on the intuition that
higher scores for a label indicate more reliable prop-
agations. More precisely, when updating vertex vi to
label l we define the confidence of the update as the
average similarity to neighbors with label l:

con f (li← l) =
1

|Ni(l)| ∑v j∈Ni(l)
φ(vi,v j) (3)

We can then prioritize high-confidence updates by
setting a threshold θ and allowing only updates with
confidence greater or equal to θ. The threshold is
initially set to 1 (i.e., the maximal possible confi-
dence) and then lowered by some small constant ∆
after each iteration until it reaches a minimum θmin,
at which point the algorithm terminates. This im-
proves the resulting clustering, since it promotes
reliable updates in earlier phases of the algorithm
which in turn has a positive effect on successive up-
dates.

5.4 Argument-Instance Similarity
As described earlier, the edge weights in our graph
are similarity scores, with positive values indicating
similarity and negative values indicating dissimilar-
ity. Determining the similarity function φ without
access to labeled training data poses a major diffi-
culty which we resolve by relying on prior linguis-
tic knowledge. Specifically, we measure the sim-
ilarity of argument instances based on three sim-
ple and intuitive criteria: (1) whether the instances
are lexically similar; (2) whether the instances oc-
cur in the same syntactic position; and (3) whether
the instances occur in the same frame (i.e., are argu-
ments in the same clause). The same criteria were
used in (Lang and Lapata, 2011) and shown effec-
tive in quantifying role-semantic similarity between
clusters of argument instances. Lexical and syntac-
tic similarity are scored through functions lex(vi,v j)
and syn(vi,v j) with range [−1,1], whereas the third
criterion enters the scoring function directly:

1324



φ(vi,v j)=

{
−∞ if vi and v j are in same frame (4)
αlex(vi,v j)+(1−α)syn(vi,v j) otherwise.

The first case in the function expresses a com-
mon linguistic assumption, i.e., that two argument
instances vi and v j occurring in the same frame can-
not have the same semantic role. The function im-
plements this constraint by returning−∞.3 The syn-
tactic similarity function s(vi,v j) indicates whether
two argument instances occur in a similar syntactic
position. We define syntactic positions through four
cues: the relation of the argument head word to its
governor, verb voice (active/passive), the linear po-
sition of the argument relative to the verb (left/right)
and the preposition used for realizing the argument
(if any). The score is S4 where S is the number of cues
which agree, i.e., have the same value. The syntac-
tic score is set to zero when the governor relation
of the arguments is not the same. Lexical similar-
ity l(vi,v j) is measured in terms of the cosine of the
angle between vectors hi and h j representing the ar-
gument head words:

lex(vi,v j) = cos(hi,h j) =
hi·h j
‖hi‖‖h j‖

(5)

We obtain hi and h j from a simple semantic space
model (Turney and Pantel, 2010) which requires no
supervision (Section 6 describes the details of the
model used in our experiments).

Our similarity function weights the contribution
of syntax vs. semantics equally, i.e., α is set to 0.5.
This reflects the linguistic intuition that lexical and
syntactic information are roughly of equal impor-
tance.

5.5 Relation to Other Models
This section briefly points out some connections to
related models. The averaging procedure used for
updating the graph vertices (Equation 2) appears in
some form in most label propagation algorithms (see
Talukdar (2010) for details). Label propagation al-
gorithms are commonly interpreted as random walks

3Formally, φ has range ran(φ) = [−1,1] ∪ {−∞} and for
x ∈ ran(φ) we define x+(−∞) =−∞. This means that the over-
all score computed for a label (Equation 2) is −∞ if one of the
summands is −∞.

2?

3 3

1

Figure 2: The update rule (Equation 2) can be under-
stood as choosing a minimal edge-cut, thereby greedily
maximizing intra-cluster similarity and minimizing inter-
cluster similarity. Assuming equal weight for all edges
above, label 3 is chosen for the vertex being updated such
that the sum of weights of edges crossing the cut is mini-
mal.

on graphs. In our case such an interpretation is
not directly possible due to the presence of negative
edge weights. This could be changed by transform-
ing the edge weights onto a non-negative scale, but
we find the current setup more expedient for model-
ing dissimilarity.

Our model could be also transformed into a prob-
abilistic graphical model that specifies a distribution
over vertex labels. In the transformed model each
vertex corresponds to a random variable over labels
and edges are associated with binary potential func-
tions over vertex-pairs. Let 1(vi = v j) denote an in-
dicator function which takes value 1 iff. li = l j and
value 0, otherwise. Then pairwise potentials can be
defined in terms of the original edge weights4 as
ψ(vi,v j) = exp(1(vi = v j)φ(vi,v j)). A Gibbs sam-
pler used to sample from the distribution of the
resulting pairwise Markov random field (Bishop,
2006; Wainwright and Jordan, 2008) would employ
almost the same update procedure as in Equation 2,
the difference being that labels would be sampled
according to their probabilities, rather than chosen
deterministically based on scores.

A third way of understanding the update rule
is as a heuristic for maximizing intra-cluster sim-
ilarity and minimizing inter-cluster similarity. By

4Including weights with value zero and thus connecting all
vertex pairs.

1325



assigning the label with maximal score to vi, we
greedily maximize the sum of intra-cluster edge
weights while minimizing the sum of inter-cluster
edge weights, i.e., the weight of the edge-cut. This
is illustrated in Figure 2. Cut-based methods are
a common method in graph clustering (Schaeffer,
2007) and are also used for inference in pairwise
Markov random fields like the one described in the
previous paragraph (Boykov et al., 2001).

Note that while it would be possible to transform
our model into a model with a formal probabilistic
interpretation (either as a graph random walk or as a
probabilistic graphical model) this would not change
the non-empirical nature of the similarity function,
which is unavoidable in the unsupervised setting and
is also common in the semi-supervised methods dis-
cussed in Section 2.

6 Experimental Setup

In this section we describe how we assessed the
performance of our model. We discuss the dataset
on which our experiments were carried out, explain
how our system’s output was evaluated and present
the methods used for comparison with our approach.

Data We compared the output of our model
against the PropBank gold standard annotations con-
tained in the CoNLL 2008 shared task dataset (Sur-
deanu et al., 2008). The latter was taken from the
Wall Street Journal portion of the Penn Treebank
and converted into a dependency format (Surdeanu
et al., 2008). In addition to gold standard depen-
dency parses, the dataset also contains automatic
parses obtained from the MaltParser (Nivre et al.,
2007). The dataset provides annotations for ver-
bal and nominal predicate-argument constructions,
but we only considered the former, following previ-
ous work on semantic role labeling (Màrquez et al.,
2008). All the experiments described in this paper
use the CoNLL 2008 training dataset.

Evaluation Metrics For each verb, we determine
the extent to which argument instances in the clus-
ters share the same gold standard role (purity) and
the extent to which a particular gold standard role is
assigned to a single cluster (collocation).

More formally, for each group of verb-specific
clusters we measure the purity of the clusters as the

percentage of instances belonging to the majority
gold class in their respective cluster. Let N denote
the total number of instances, G j the set of instances
belonging to the j-th gold class and Ci the set of in-
stances belonging to the i-th cluster. Purity can be
then written as:

PU =
1
N ∑i

max
j
|G j ∩Ci| (6)

Collocation is defined as follows. For each gold
role, we determine the cluster with the largest num-
ber of instances for that role (the role’s primary clus-
ter) and then compute the percentage of instances
that belong to the primary cluster for each gold role:

CO =
1
N ∑j

max
i
|G j ∩Ci| (7)

Per-verb scores are aggregated into an overall
score by averaging over all verbs. We use the
micro-average obtained by weighting the scores for
individual verbs proportionately to the number of in-
stances for that verb.

Finally, we use the harmonic mean of purity and
collocation as a single measure of clustering quality:

F1 =
2·CO·PU
CO+PU

(8)

Model Parameters Recall that our algorithm pri-
oritizes updates with confidence higher than a
threshold θ. Initially, θ is set to 1 and its value
decreases at each iteration by a small constant ∆
which we set to 0.0025. The algorithm terminates
when a minimum confidence θmin is reached. While
choosing a value for ∆ is straightforward — it sim-
ply has to be a small fraction of the maximal pos-
sible confidence — specifying θmin on the basis of
objective prior knowledge is less so. And although
a human judge could determine the optimal termina-
tion point based on several criteria such as clustering
quality or the number of clusters, we used a develop-
ment set instead for the sake of reproducibility and
comparability. Specifically, we optimized θmin on
the CoNLL test set and obtained best results with
θmin = 13 . This value was used for all our experi-
ments and was also kept fixed for all verbs. Impor-
tantly, the development set was not used for any kind
of supervised training.

1326



Syntactic Function Latent Logistic Split-Merge Graph Partitioning
PU CO F1 PU CO F1 PU CO F1 PU CO F1

auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2 82.5 68.8 75.0
gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9 84.0 73.5 78.4
auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3 87.4 65.9 75.2
gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1 88.6 70.7 78.6

Table 1: Evaluation of the output of our graph partitioning algorithm compared to our previous models and a baseline
that assigns arguments to clusters based on their syntactic function.

0 10 20 30 40 50 60 70
Average number of clusters per verb

60

70

80

90

100

C
lu

st
er

 p
ur

ity
 (

%
)

Figure 3: Purity (vertical axis) against average number
of clusters per verb (horizontal axis) on the auto/auto
dataset.

Recall that one of the components in our simi-
larity function is lexical similarity which we mea-
sure using a vector-based model (see Section 5.4).
We created such a model from the Google N-Grams
corpus (Brants and Franz, 2006) using a context
window of two words on both sides of the target
word and co-occurrence frequencies as vector com-
ponents (no weighting was applied). The large size
of this corpus allows us to use bigram frequencies,
rather than frequencies of individual words and to
distinguish between left and right bigrams. We used
randomized algorithms (Ravichandran et al., 2005)
to build the semantic space efficiently.

Comparison Models We compared our graph par-
titioning algorithm against three competitive ap-
proaches. The first one assigns argument instances
to clusters according to their syntactic function
(e.g., subject, object) as determined by a parser. This
baseline has been previously used as a point of com-
parison by other unsupervised semantic role induc-
tion systems (Grenager and Manning, 2006; Lang
and Lapata, 2010) and shown difficult to outperform.

0 100 200 300 400 500
Number of iterations

0

20

40

60

80

F1
 s

co
re

 (
%

)

Syntactic Function
Graph Partitioning

Figure 4: F1 (vertical axis) against number of iterations
(horizontal axis) on the auto/auto dataset.

Our implementation allocates up to N = 21 clusters5

for each verb, one for each of the 20 most frequent
syntactic functions and a default cluster for all other
functions. We also compared our approach to Lang
and Lapata (2010) using the same model settings
(with 10 latent variables) and feature set proposed
in that paper. Finally, our third comparison model
is Lang and Lapata’s (2011) split-merge clustering
algorithm. Again we used the same parameters and
number of clusters (on average 10 per verb). Our
graph partitioning method uses identical cues for as-
sessing role-semantic similarity as the method de-
scribed in Lang and Lapata (2011).

7 Results

Our results are summarized in Table 1. We report
cluster purity (PU), collocation (CO) and their har-
monic mean (F1) for the baseline (Syntactic Func-
tion), our two previous models (the Latent Logistic
classifier and Split-Merge) and the graph partition-
ing algorithm on four datasets. These result from the
combination of automatic parses with automatically
identified arguments (auto/auto), gold parses with

5This is the number of gold standard roles.

1327



Syntactic Function
PU 91.4 68.6 45.1 59.7 62.4 61.9 63.5 75.9 76.7 69.6 63.1 53.7
CO 91.3 71.9 56.0 68.4 72.7 76.8 65.6 79.7 76.0 63.8 73.4 58.9
F1 91.4 70.2 49.9 63.7 67.1 68.6 64.5 77.7 76.3 66.6 67.9 56.2

Graph Partitioning
PU 95.6 83.5 72.3 75.4 83.3 84.4 74.8 84.8 89.5 83.0 73.2 66.3
CO 89.1 62.7 42.1 64.2 56.2 66.3 57.2 73.2 64.1 54.3 66.0 57.7
F1 92.2 71.6 53.2 69.4 67.1 74.3 64.8 78.5 74.7 65.7 69.4 61.7
Verb say make go increase know tell consider acquire meet send open break
Freq 15238 4250 2109 1392 983 911 753 704 574 506 482 246

Table 2: Clustering results for individual verbs on the auto/auto dataset with our graph partitioning algorithm and the
syntactic function baseline; the scores were taken from a single run.

automatic arguments (gold/auto), automatic parses
with gold arguments (auto/gold) and gold parses
with gold arguments (gold/gold). Table 1 reports
averages across multiple runs. This was necessary
in order to ensure that the results of our randomized
graph partitioning algorithm are stable.6 The argu-
ments for the auto/auto and gold/auto datasets were
identified using the rules described in Lang and Lap-
ata (2011) (see Section 4). Bold-face is used to high-
light the best performing system under each measure
(PU, CO, or F1) on each dataset.

Compared to the Syntactic Function baseline,
the Graph Partitioning algorithm has higher F1 on
the auto/auto and auto/gold datasets but lags be-
hind by 0.5 points on the gold/auto dataset and
by 0.9 points on the gold/gold dataset. It attains
highest purity on all datasets except for gold/gold,
where it is 0.1 points below Split-Merge. When con-
sidering F1 in conjunction with purity and colloca-
tion, we observe that Graph Partitioning can attain
higher purity than the comparison models by trading
off collocation. If we were to hand label the clusters
output by our system, purity would correspond to the
quality of the resulting labeling, while collocation
would determine the labeling effort. The relation-
ship is illustrated more explicitly in Figure 3, which
plots purity against the average number of clusters
per verb on the auto/auto dataset. As the algorithm

6For example, on the auto/auto dataset and over 10 runs,
the standard deviation in F1 was 0.11 points in collocation 0.16
points and in purity 0.08 points. The worst F1 was 0.20 points
below the average, the worst collocation was 0.32 points be-
low the average and the worst purity was 0.17 points below the
average.

proceeds the number of clusters is reduced which
results in a decrease of purity. The latter decreases
more rapidly once the number of 20 clusters per verb
is reached. This is accompanied by a decreasing
tradeoff ratio between collocation and purity: at this
stage decreasing purity by one point increases collo-
cation by roughly one point, whereas in earlier itera-
tions a decrease of purity by one point goes together
with several points increase in collocation. This is
most likely due to the fact that the number of gold
standard classes is around 20.

Figure 4 shows the complete learning curve of our
graph partitioning method on the auto/auto dataset
(F1 is plotted against the number of iterations).
The algorithm naturally terminates at iteration 266
(when θmin = 1/3), but we have also plotted itera-
tions beyond that point. Since lower values of θ per-
mit unreliable propagations, F1 eventually falls be-
low the baseline (see Section 5.2). The importance
of our propagation prioritization mechanism is fur-
ther underlined by the fact that when it is not em-
ployed (i.e., when using the vanilla Chinese Whis-
pers algorithm without any modifications), it per-
forms substantially worse than the comparison mod-
els. On the auto/auto dataset, F1 converges to 59.1
(purity is 55.5 and collocation 63.2) within 10 itera-
tions.

Finally, Table 2 shows how performance varies
across verbs. We report results for the Syntac-
tic Function baseline and Graph Partitioning on the
auto/auto dataset for 12 verbs. These were selected
so as to exhibit varied occurrence frequencies and
alternation patterns. As can be seen, the macro-

1328



scopic result — increase in F1 and purity — also
holds across verbs.

8 Conclusions

In this paper we described an unsupervised method
for semantic role induction, in which argument-
instance graphs are partitioned into clusters repre-
senting semantic roles. The approach is conceptu-
ally and algorithmically simple and novel in its for-
malization of role induction as a graph partitioning
problem. We believe this constitutes an interesting
alternative for two reasons. Firstly, eliciting and
encoding problem-specific knowledge in the form
of instance-wise similarity judgments can be easier
than encoding it into model structure e.g., by mak-
ing statistical independence assumptions or assump-
tions about latent structure. Secondly, the approach
is general and amenable to other graph partitioning
algorithms and relates to well-known graph-based
semi-supervised learning methods.

The similarity function in this paper is by neces-
sity rudimentary, since it cannot be estimated from
data. Nevertheless, the resulting system attains com-
petitive F1 and notably higher purity than the com-
parison models. Arguably, performance could be
improved by developing a better similarity function.
Therefore, in the future we intend to investigate how
our system performs in a weakly supervised setting,
where the similarity function is estimated from a
small amount of labeled instances, since this would
allow us to incorporate richer syntactic features and
result in more precise similarity scores.

Acknowledgments We are grateful to Charles
Sutton for his valuable feedback on this work. The
authors acknowledge the support of EPSRC (grant
GR/T04540/01).

References
O. Abend and A. Rappoport. 2010. Fully unsupervised

core-adjunct argument classification. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 226–236, Uppsala,
Sweden.

O. Abend, R. Reichart, and A. Rappoport. 2009. Un-
supervised Argument Identification for Semantic Role
Labeling. In Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics

and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing, pages 28–36, Singapore.

S. Abney. 2007. Semisupervised Learning for Computa-
tional Linguistics. Chapman & Hall/CRC.

A. Alexandrescu and K. Kirchhoff. 2009. Graph-based
learning for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
119–127, Boulder, Colorado.

C. Biemann. 2006. Chinese Whispers: an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the First Workshop on Graph Based
Methods for Natural Language Processing, pages 73–
80, New York City.

C. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.

Y. Boykov, O. Veksler, and R. Zabih. 2001. Fast Ap-
proximate Energy Minimization via Graph Cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 23(11):1222–1239.

T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1.
Linguistic Data Consortium, Philadelphia.

Z. Chen and H. Ji. 2010. Graph-based clustering for
computational linguistics: A survey. In Proceedings of
TextGraphs-5 - 2010 Workshop on Graph-based Meth-
ods for Natural Language Processing, pages 1–9, Up-
psala, Sweden.

C. Christodoulopoulos, S. Goldwater, and M. Steedman.
2010. Two decades of unsupervised POS induction:
How far have we come? In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 575–584, Cambridge, MA.

D. Dowty. 1991. Thematic Proto Roles and Argument
Selection. Language, 67(3):547–619.

H. Fürstenau and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 11–20, Singa-
pore.

D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28(3):245–288.

A. Gordon and R. Swanson. 2007. Generalizing Se-
mantic Role Annotations Across Syntactically Similar
Verbs. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
192–199, Prague, Czech Republic.

T. Grenager and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 1–8, Sydney, Australia.

1329



G. Haffari and A. Sarkar. 2007. Analysis of Semi-
Supervised Learning with the Yarowsky Algorithm. In
Proceedings of the 23rd Conference on Uncertainty in
Artificial Intelligence, Vancouver, BC.

K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of the 17th AAAI Conference on Artificial Intelligence,
pages 691–696. AAAI Press / The MIT Press.

I. Klapaftis and Suresh M. 2010. Word sense induction
& disambiguation using hierarchical random graphs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 745–
755, Cambridge, MA.

J. Lang and M. Lapata. 2010. Unsupervised Induction
of Semantic Roles. In Proceedings of the 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 939–
947, Los Angeles, California.

J. Lang and M. Lapata. 2011. Unsupervised Semantic
Role Induction via Split-Merge Clustering. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics, Portland, Oregon.
To appear in.

L. Màrquez, X. Carreras, K. Litkowski, and S. Stevenson.
2008. Semantic Role Labeling: an Introduction to the
Special Issue. Computational Linguistics, 34(2):145–
159.

G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Description
of SQUASH, the SFU Question Answering Summary
Handler for the DUC-2005 Summarization Task. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing Document Understand-
ing Workshop, Vancouver, Canada.

J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. Kübler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering, 13(2):95–135.

S. Padó and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research, 36:307–340.

M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71–106.

S. Pradhan, W. Ward, and J. Martin. 2008. Towards Ro-
bust Semantic Role Labeling. Computational Linguis-
tics, 34(2):289–310.

D. Ravichandran, P. Pantel, and E. Hovy. 2005. Ran-
domized Algorithms and NLP: Using Locality Sensi-
tive Hash Function for High Speed Noun Clustering.

In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, page 622629,
Ann Arbor, Michigan.

J. Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,
and J. Scheffczyk. 2006. FrameNet II: Extended The-
ory and Practice, version 1.3. Technical report, In-
ternational Computer Science Institute, Berkeley, CA,
USA.

S. Schaeffer. 2007. Graph clustering. Computer Science
Review, 1(1):27–64.

D. Shen and M. Lapata. 2007. Using Semantic Roles
to Improve Question Answering. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing and the Conference on Com-
putational Natural Language Learning, pages 12–21,
Prague, Czech Republic.

A. Subramanya, S. Petrov, and F. Pereira. 2010. Effi-
cient graph-based semi-supervised learning of struc-
tured tagging models. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 167–176, Cambridge, MA.

M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 8–15, Sapporo, Japan.

M. Surdeanu, R. Johansson, A. Meyers, and L. Màrquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL, pages 159–177, Manchester,
England.

R. Swier and S. Stevenson. 2004. Unsupervised Seman-
tic Role Labelling. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 95–102, Barcelona, Spain.

P. Talukdar and F. Pereira. 2010. Experiments in graph-
based semi-supervised learning methods for class-
instance acquisition. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1473–1481, Uppsala, Sweden.

P. Talukdar. 2010. Graph-Based Weakly Supervised
Methods for Information Extraction & Integration.
Ph.D. thesis, CIS Department, University of Pennsyl-
vania.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141–
188.

M. Wainwright and M. Jordan. 2008. Graphical Mod-
els, Exponential Families, and Variational Inference.
Foundations and Trends in Machine Learning, 1(1-
2):1–305.

D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of North

1330



American Annual Meeting of the Association for Com-
putational Linguistics HLT 2009: Short Papers, pages
13–16, Boulder, Colorado.

D. Yarowsky. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189–196, Cam-
bridge, MA.

X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
Supervised Learning Using Gaussian Fields and Har-
monic Functions. In Proceedings of the 20th Interna-
tional Conference on Machine Learning, Washington,
DC.

1331


