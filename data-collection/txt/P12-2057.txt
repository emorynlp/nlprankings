










































Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation


Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 291–295,
Jeju, Republic of Korea, 8-14 July 2012. c©2012 Association for Computational Linguistics

Translation Model Size Reduction for
Hierarchical Phrase-based Statistical Machine Translation

Seung-Wook Lee† Dongdong Zhang‡ Mu Li‡ Ming Zhou‡ Hae-Chang Rim†
† Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea

{swlee,rim}@nlp.korea.ac.kr
‡ Microsoft Research Asia, Beijing, China
{dozhang,muli,mingzhou}@microsoft.com

Abstract

In this paper, we propose a novel method of
reducing the size of translation model for hier-
archical phrase-based machine translation sys-
tems. Previous approaches try to prune in-
frequent entries or unreliable entries based on
statistics, but cause a problem of reducing the
translation coverage. On the contrary, the pro-
posed method try to prune only ineffective
entries based on the estimation of the infor-
mation redundancy encoded in phrase pairs
and hierarchical rules, and thus preserve the
search space of SMT decoders as much as
possible. Experimental results on Chinese-to-
English machine translation tasks show that
our method is able to reduce almost the half
size of the translation model with very tiny
degradation of translation performance.

1 Introduction

Statistical Machine Translation (SMT) has gained
considerable attention during last decades. From a
bilingual corpus, all translation knowledge can be
acquired automatically in SMT framework. Phrase-
based model (Koehn et al., 2003) and hierarchical
phrase-based model (Chiang, 2005; Chiang, 2007)
show state-of-the-art performance in various lan-
guage pairs. This achievement is mainly benefit
from huge size of translational knowledge extracted
from sufficient parallel corpus. However, the errors
of automatic word alignment and non-parallelized
bilingual sentence pairs sometimes have caused the
unreliable and unnecessary translation rule acquisi-
tion. According to Bloodgood and Callison-Burch

(2010) and our own preliminary experiments, the
size of phrase table and hierarchical rule table con-
sistently increases linearly with the growth of train-
ing size, while the translation performance tends to
gain minor improvement after a certain point. Con-
sequently, the model size reduction is necessary and
meaningful for SMT systems if it can be performed
without significant performance degradation. The
smaller the model size is, the faster the SMT de-
coding speed is, because there are fewer hypotheses
to be investigated during decoding. Especially, in a
limited environment, such as mobile device, and for
a time-urgent task, such as speech-to-speech transla-
tion, the compact size of translation rules is required.
In this case, the model reduction would be the one
of the main techniques we have to consider.

Previous methods of reducing the size of SMT
model try to identify infrequent entries (Zollmann
et al., 2008; Huang and Xiang, 2010). Several sta-
tistical significance testing methods are also exam-
ined to detect unreliable noisy entries (Tomeh et al.,
2009; Johnson et al., 2007; Yang and Zheng, 2009).
These methods could harm the translation perfor-
mance due to their side effect of algorithms; simi-
lar multiple entries can be pruned at the same time
deteriorating potential coverage of translation. The
proposed method, on the other hand, tries to mea-
sure the redundancy of phrase pairs and hierarchi-
cal rules. In this work, redundancy of an entry is
defined as its translational ineffectiveness, and esti-
mated by comparing scores of entries and scores of
their substituents. Suppose that the source phrase
s1s2 is always translated into t1t2 with phrase en-
try <s1s2→t1t2> where si and ti are correspond-

291



ing translations. Similarly, source phrases s1 and
s2 are always translated into t1 and t2, with phrase
entries, <s1→t1> and <s2→t2>, respectively. In
this case, it is intuitive that <s1s2→t1t2> could be
unnecessary and redundant since its substituent al-
ways produces the same result. This paper presents
statistical analysis of this redundancy measurement.
The redundancy-based reduction can be performed
to prune the phrase table, the hierarchical rule table,
and both. Since the similar translation knowledge
is accumulated at both of tables during the train-
ing stage, our reduction method performs effectively
and safely. Unlike previous studies solely focus on
either phrase table or hierarchical rule table, this
work is the first attempt to reduce phrases and hi-
erarchical rules simultaneously.

2 Proposed Model

Given an original translation model, TM , our goal
is to find the optimally reduced translation model,
TM∗, which minimizes the degradation of trans-
lation performance. To measure the performance
degradation, we introduce a new metric named con-
sistency:

C(TM,TM∗) =

BLEU(D(s;TM),D(s;TM∗)) (1)

where the function D produces the target sentence
of the source sentence s, given the translation model
TM . Consistency measures the similarity between
the two groups of decoded target sentences produced
by two different translation models. There are num-
ber of similarity metrics such as Dices coefficient
(Kondrak et al., 2003), and Jaccard similarity coef-
ficient. Instead, we use BLEU scores (Papineni et
al., 2002) since it is one of the primary metrics for
machine translation evaluation. Note that our con-
sistency does not require the reference set while the
original BLEU does. This means that only (abun-
dant) source-side monolingual corpus is needed to
predict performance degradation. Now, our goal can
be rewritten with this metric; among all the possible
reduced models, we want to find the set which can
maximize the consistency:

TM∗ = argmax
TM ′⊂TM

C(TM,TM ′) (2)

In minimum error rate training (MERT) stages,
a development set, which consists of bilingual sen-
tences, is used to find out the best weights of fea-
tures (Och, 2003). One characteristic of our method
is that it isolates feature weights of the transla-
tion model from SMT log-linear model, trying to
minimize the impact of search path during decod-
ing. The reduction procedure consists of three
stages: translation scoring, redundancy estimation,
and redundancy-based reduction.

Our reduction method starts with measuring the
translation scores of the individual phrase and the
hierarchical rule. Similar to the decoder, the scoring
scheme is based on the log-linear framework:

PS(p) =
∑

i

λihi(p) (3)

where h is a feature function and λ is its weight.
As the conventional hierarchical phrase-based SMT
model, our features are composed of P (e|f ), P (f |e),
Plex(e|f ), Plex(f |e), and the number of phrases,
where e and f denote a source phrase and a target
phrase, respectively. Plex is the lexicalized proba-
bility. In a similar manner, the translation scores of
hierarchical rules are calculated as follows:

HS(r) =
∑

i

λihi(r) (4)

The features are as same as those that are used for
phrase scoring, except the last feature. Instead of the
phrase number penalty, the hierarchical rule num-
ber penalty is used. The weight for each feature is
shared from the results of MERT. With this scoring
scheme, our model is able to measure how important
the individual entry is during decoding.

Once translation scores for all entries are es-
timated, our method retrieves substituent candi-
dates with their combination scores. The combina-
tion score is calculated by accumulating translation
scores of every member as follows:

CS(p1...n) =

n∑

i=1

PS(pi) (5)

This scoring scheme follows the same manner
what the conventional decoder does, finding the best
phrase combination during translation. By compar-
ing the original translation score with combination

292



scores of its substituents, the redundancy scores are
estimated, as follows:

Red(p) = min
p1...n∈Sub(p)

PS(p)−CS(p1...n) (6)

where Sub is the function that retrieves all possi-
ble substituents (the combinations of sub-phrases,
and/or sub-rules that exactly produce the same tar-
get phrase, given the source phrase p). If the com-
bination score of the best substituent is same as the
translation score of p, the redundancy score becomes
zero. In this case, the decoder always produces the
same translation results without p. When the redun-
dancy score is negative, the best substituent is more
likely to be chosen instead of p. This implies that
there is no risk to prune p; the search space is not
changed, and the search path is not changed as well.

Our method can be varied according to the desig-
nation of Sub function. If both of the phrase table
and the hierarchical rule table are allowed, cross re-
duction can be possible; the phrase table is reduced
based on the hierarchical rule table and vice versa.
With extensions of combination scoring and redun-
dancy scoring schemes like following equations, our
model is able to perform cross reduction.

CS(p1...n, h1...m) =
n∑

i=1

PS(pi) +

m∑

i=1

HS(hi) (7)

Red(p) = min
<p1...n,h1...m>∈Sub(p)

PS(p)− CS(p1...n, h1...m) (8)

The proposed method has some restrictions for
reduction. First of all, it does not try to prune the
phrase that has no substituents, such as unigram
phrases; the phrase whose source part is composed
of a single word. This restriction guarantees that
the translational coverage of the reduced model is
as high as those of the original translation model.
In addition, our model does not prune the phrases
and the hierarchical rules that have reordering within
it to prevent information loss of reordering. For
instance, if we prune phrase, <s1s2s3→t3t1t2>,
phrases, <s1s2→t1t2> and <s3→t3> are not able
to produce the same target words without appropri-
ate reordering.

Once the redundancy scores for all entries have
been estimated, the next step is to select the best
N entries to prune to satisfy a desired model size.
We can simply prune the first N from the list of en-
tries sorted by increasing order of redundancy score.
However, this method may not result in the opti-
mal reduction, since each redundancy scores are es-
timated based on the assumption of the existence of
all the other entries. In other words, there are depen-
dency relationships among entries. We examine two
methods to deal with this problem. The first is to
ignore dependency, which is the more efficient man-
ner. The other is to prune independent entries first.
After all independent entries are pruned, the depen-
dent entries are started to be pruned. We present the
effectiveness of each method in the next section.

Since our goal is to reduce the size of all transla-
tion models, the reduction is needed to be performed
for both the phrase table and the hierarchical rule
table simultaneously, namely joint reduction. Sim-
ilar to phrase reduction and hierarchical rule reduc-
tion, it selects the best N entries of the mixture of
phrase and hierarchical rules. This method results
in safer pruning; once a phrase is determined to be
pruned, the hierarchical rules, which are related to
this phrase, are likely to be kept, and vice versa.

3 Experiment

We investigate the effectiveness of our reduction
method by conducting Chinese-to-English transla-
tion task. The training data, as same as Cui et
al. (2010), consists of about 500K parallel sentence
pairs which is a mixture of several datasets pub-
lished by LDC. NIST 2003 set is used as a devel-
opment set. NIST 2004, 2005, 2006, and 2008 sets
are used for evaluation purpose. For word align-
ment, we use GIZA++1, an implementation of IBM
models (Brown et al., 1993). We have implemented
a hierarchical phrase-based SMT model similar to
Chiang (2005). The trigram target language model
is trained from the Xinhua portion of English Gi-
gaword corpus (Graff and Cieri, 2003). Sampled
10,000 sentences from Chinese Gigaword corpus
(Graff, 2007) was used for source-side development
dataset to measure consistency. Our main met-
ric for translation performance evaluation is case-

1http://www.statmt.org/moses/giza/GIZA++.html

293



 0.60

 0.70

 0.80

 0.90

 1.00
C

on
si

st
en

cy

Freq-Cutoff
NoDep

Dep
CrossNoDep

CrossDep

0.286

0.290

0.294

0.298

0% 10% 20% 30% 40% 50% 60%

B
LE

U

Phrase Reduction Ratio

0% 10% 20% 30% 40% 50% 60%

Hierarchical Rule Reduction Ratio

0% 10% 20% 30% 40% 50% 60%

Joint Reduction Ratio

Figure 1: Performance comparison. BLEU scores and consistency scores are averaged over four evaluation sets.

insensitive BLEU-4 scores (Papineni et al., 2002).

As a baseline system, we chose the frequency-
based cutoff method, which is one of the most
widely used filtering methods. As shown in Fig-
ure 1, almost half of the phrases and hierarchical
rules are pruned when cutoff=2, while the BLEU
score is also deteriorated significantly. We intro-
duced two methods for selecting the N pruning
entries considering dependency relationships. The
non-dependency method does not consider depen-
dency relationships, while the dependency method
prunes independent entries first. Each method can be
combined with cross reduction. The performance is
measured in three different reduction tasks: phrase
reduction, hierarchical rule reduction, and joint re-
duction. As the reduction ratio becomes higher,
the model size, i.e., the number of entries, is re-
duced while BLEU scores and coverage are de-
creased. The results show that the translation per-
formance is highly co-related with the consistency.
The co-relation scores measured between them on
the phrase reduction and the hierarchical rule reduc-
tion tasks are 0.99 and 0.95, respectively, which in-
dicates very strong positive relationship.

For the phrase reduction task, the dependency
method outperforms the non-dependency method in
terms of BLEU score. When the cross reduction
technique was used for the phrase reduction task,

BLEU score is not deteriorated even when more than
half of phrase entries are pruned. This result implies
that there is much redundant information stored in
the hierarchical rule table. On the other hand, for the
hierarchical rule reduction task, the non-dependency
method shows the better performance. The depen-
dency method sometimes performs worse than the
baseline method. We expect that this is caused by
the unreliable estimation of dependency among hi-
erarchical rules since the most of them are automat-
ically generated from the phrases. The excessive de-
pendency of these rules would cause overestimation
of hierarchical rule redundancy score.

4 Conclusion

We present a novel method of reducing the size of
translation model for SMT. The contributions of the
proposed method are as follows: 1) our method is
the first attempt to reduce the phrase table and the hi-
erarchical rule table simultaneously. 2) our method
is a safe reduction method since it considers the re-
dundancy, which is the practical ineffectiveness of
individual entry. 3) our method shows that almost
the half size of the translation model can be reduced
without significant performance degradation. It may
be appropriate for the applications running on lim-
ited environment, e.g., mobile devices.

294



Acknowledgement

The first author performed this research during an
internship at Microsoft Research Asia. This research
was supported by the MKE(The Ministry of Knowl-
edge Economy), Korea and Microsoft Research, un-
der IT/SW Creative research program supervised by
the NIPA(National IT Industry Promotion Agency).
(NIPA-2010-C1810-1002-0025)

References

Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the Trend: Large-Scale Cost-Focused Active
Learning for Statistical Machine Translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 854–864.

Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19:263–311, June.

David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of the 43th Annual Meeting on Association
for Computational Linguistics, pages 263–270.

David Chiang. 2007. Hierarchical Phrase-based Transla-
tion. Computational Linguistics, 33:201–228, June.

Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. Hybrid Decoding: Decoding with
Partial Hypotheses Combination Over Multiple SMT
Systems. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10, pages 214–222, Stroudsburg, PA, USA.
Association for Computational Linguistics.

David Graff and Christopher Cieri. 2003. English Giga-
word. In Linguistic Data Consortium, Philadelphia.

David Graff. 2007. Chinese Gigaword Third Edition. In
Linguistic Data Consortium, Philadelphia.

Fei Huang and Bing Xiang. 2010. Feature-Rich Discrim-
inative Phrase Rescoring for SMT. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ’10, pages 492–500, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving Translation Quality by Dis-
carding Most of the Phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967–
975.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 48–54, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can Improve Statistical Translation
Models. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy: companion volume of the Proceedings of HLT-
NAACL 2003–short papers - Volume 2, NAACL-Short
’03, pages 46–48, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.

Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based Phrase-Table Filtering for
Statistical Machine Translation.

Mei Yang and Jing Zheng. 2009. Toward Smaller, Faster,
and Better Hierarchical Phrase-based SMT. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, ACLShort ’09, pages 237–240, Stroudsburg,
PA, USA. Association for Computational Linguistics.

Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A Systematic Comparison of Phrase-
based, Hierarchical and Syntax-Augmented Statistical
MT. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 1145–1152, troudsburg, PA, USA. Association
for Computational Linguistics.

295


