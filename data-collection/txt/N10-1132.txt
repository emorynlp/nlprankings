










































Interpretation and Transformation for Abstracting Conversations


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 894–902,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Interpretation and Transformation for Abstracting Conversations

Gabriel Murray
gabrielm@cs.ubc.ca

Giuseppe Carenini
carenini@cs.ubc.ca

Department of Computer Science, University of British Columbia
Vancouver, Canada

Raymond Ng
rng@cs.ubc.ca

Abstract

We address the challenge of automatically ab-
stracting conversations such as face-to-face
meetings and emails. We focus here on
the stages of interpretation, where sentences
are mapped to a conversation ontology, and
transformation, where the summary content
is selected. Our approach is fully developed
and tested on meeting speech, and we subse-
quently explore its application to email con-
versations.

1 Introduction

The dominant approach to the challenge of auto-
matic summarization has been extraction, where in-
formative sentences in a document are identified and
concatenated to form a condensed version of the
original document. Extractive summarization has
been popular at least in part because it is a binary
classification task that lends itself well to machine
learning techniques, and does not require a natural
language generation (NLG) component. There is ev-
idence that human abstractors at times use sentences
from the source documents nearly verbatim in their
own summaries, justifying this approach to some ex-
tent (Kupiec et al., 1995). Extrinsic evaluations have
also shown that, while extractive summaries may be
less coherent than human abstracts, users still find
them to be valuable tools for browsing documents
(He et al., 1999; McKeown et al., 2005; Murray et
al., 2008).

However, these same evaluations also indicate
that concise abstracts are generally preferred by
users and lead to higher objective task scores. The
limitation of a cut-and-paste summary is that the
end-user does not know why the selected sentences
are important; this can often only be discerned by
exploring the context in which each sentence origi-
nally appeared. One possible improvement is to cre-

ate structured extracts that represent an increased
level of abstraction, where selected sentences are
grouped according to phenomena such as decisions,
action items and problems, thereby giving the user
more information on why the sentences are being
highlighted. For example, the sentence Let’s go with
a simple chip represents a decision. An even higher
level of abstraction can be provided by generating
new text that synthesizes or extrapolates on the in-
formation contained in the structured summary. For
example, the sentence Sandra and Sue expressed
negative opinions about the remote control design
can be coupled with extracted sentences containing
these negative opinions, forming a hybrid summary.
Our summarization system ultimately performs both
types of abstraction, grouping sentences according
to various sentence-level phenomena, and generat-
ing novel text that describes this content at a higher
level.

In this work we describe the first two components
of our abstractive summarization system. In the in-
terpretation stage, sentences are mapped to nodes
in a conversation ontology by utilizing classifiers
relating to a variety of sentence-level phenomena
such as decisions, action items and subjective sen-
tences. These classifiers achieve high accuracy by
using a very large feature set integrating conversa-
tion structure, lexical patterns, part-of-speech (POS)
tags and character n-grams. In the transformation
stage, we select the most informative sentences by
maximizing a function based on the derived ontol-
ogy mapping and the coverage of weighted enti-
ties mentioned in the conversation. This transforma-
tion component utilizes integer linear programming
(ILP) and we compare its performance with several
greedy selection algorithms.

We do not discuss the generation compo-
nent of our summarization system in this pa-
per. The transformation component is still ex-

894



tractive in nature, but the sentences that are se-
lected in the transformation stage correspond to
objects in the ontology and the properties link-
ing them. Specifically, these are triples of the
form < participant, relation, entity > where a
participant is a person in the conversation, an
entity is an item under discussion, and a relation
such as positive opinion or action item links the two.
This intermediate output enables us to create struc-
tured extracts as described above, with the triples
also acting as input to the downstream NLG com-
ponent.

We have tested our approach in summarization
experiments on both meeting and email conversa-
tions, where the quality of a sentence is measured
by how effectively it conveys information in a model
abstract summary according to human annotators.
On meetings the ILP approach consistently outper-
forms several greedy summarization methods. A
key finding is that emails exhibit markedly varying
conversation structures, and the email threads yield-
ing the best summarization results are those that are
structured similarly to meetings. Other email con-
versation structures are less amenable to the current
treatment and require further investigation and pos-
sibly domain adaptation.

2 Related Research

The view that summarization consists of stages of
interpretation, transformation and generation was
laid out by Sparck-Jones (1999). Popular ap-
proaches to text extraction essentially collapse inter-
pretation and transformation into one step, with gen-
eration either being ignored or consisting of post-
processing techniques such as sentence compres-
sion (Knight and Marcu, 2000; Clarke and Lapata,
2006) or sentence merging (Barzilay and McKeown,
2005). In contrast, in this work we clearly separate
interpretation from transformation.

The most relevant research to ours is by Klein-
bauer et al. (2007), similarly focused on meet-
ing abstraction. They create an ontology for the
AMI scenario meeting corpus (Carletta et al., 2005),
described in Section 5.1. The system uses topic
segments and topic labels, and for each topic seg-
ment in the meeting a sentence is generated that de-
scribes the most frequently mentioned content items

in that topic. Our systems differ in two major re-
spects: their summarization process uses human
gold-standard annotations of topic segments, topic
labels and content items from the ontology, while
our summarizer is fully automatic; and the ontology
used by Kleinbauer et al. is specific not just to meet-
ings but to the AMI scenario meetings, while our
ontology applies to conversations in general.

While the work by Kleinbauer et al. is among
the earliest research on abstracting multi-party dia-
logues, much attention in recent years has been paid
to extractive summarization of such conversations,
including meetings (Galley, 2006), emails (Rambow
et al., 2004; Carenini et al., 2007), telephone con-
versations (Zhu and Penn, 2006) and internet relay
chats (Zhou and Hovy, 2005).

Recent research has addressed the challenges of
detecting decisions (Hsueh et al., 2007), action items
(Purver et al., 2007; Murray and Renals, 2008) and
subjective sentences (Raaijmakers et al., 2008). In
our work we perform all of these tasks but rely on
general conversational features without recourse to
meeting-specific or email-specific features.

Our approach to transformation is an adaptation
of an ILP sentence selection algorithm described by
Xie et al. (2009). We describe both ILP approaches
in Section 4.

3 Interpretation - Ontology Mapping

Source document interpretation in our system re-
lies on a simple conversation ontology. The ontol-
ogy is written in OWL/RDF and contains two core
upper-level classes: Participant and Entity. When
additional information is available about participant
roles in a given domain, Participant subclasses such
as ProjectManager can be utilized. The ontology
also contains six properties that express relations be-
tween the participants and the entities. For example,
the following snippet of the ontology indicates that
hasActionItem is a relationship between a meeting
participant (the property domain) and a discussed
entity (the property range).

<owl:ObjectProperty rdf:ID="hasActionItem">
<rdfs:domain rdf:resource="#Participant"/>
<rdfs:range rdf:resource="#Entity"/>

</owl:ObjectProperty>

Similar properties exist for decisions, actions,
problems, positive-subjective sentences, negative-

895



subjective sentences and general extractive sen-
tences (important sentences that may not match the
other categories), all connecting conversation par-
ticipants and entities. The goal is to populate the
ontology with participant and entity instances from
a given conversation and determine their relation-
ships. This involves identifying the important en-
tities and classifying the sentences in which they
occur as being decision sentences, action item sen-
tences, etc.

Our current definition of entity is simple. The en-
tities in a conversation are noun phrases with mid-
range document frequency. This is similar to the
definition of concept as defined by Xie et al. (Xie
et al., 2009), where n-grams are weighted by tf.idf
scores, except that we use noun phrases rather than
any n-grams because we want to refer to the enti-
ties in the generated text. We use mid-range doc-
ument frequency instead of idf (Church and Gale,
1995), where the entities occur in between 10% and
90% of the documents in the collection. In Section 4
we describe how we use the entity’s term frequency
to detect the most informative entities. We do not
currently attempt coreference resolution for entities;
recent work has investigated coreference resolution
for multi-party dialogues (Muller, 2007; Gupta et
al., 2007), but the challenge of resolution on such
noisy data is highlighted by low accuracy (e.g. F-
measure of 21.21) compared with using well-formed
text (e.g. monologues).

We map sentences to our ontology’s object prop-
erties by building numerous supervised classifiers
trained on labeled decision sentences, action sen-
tences, etc. A general extractive classifier is also
trained on sentences simply labeled as important.
After predicting these sentence-level properties, we
consider a participant to be linked to an entity if
the participant mentioned the entity in a sentence in
which one of these properties is predicted. We give a
specific example of the ontology mapping using this
excerpt from the AMI corpus:

1. A: And you two are going to work together on
a prototype using modelling clay.

2. A: You’ll get specific instructions from your
personal coach.

3. C: Cool.
4. A: Um did we decide on a chip?
5. A: Let’s go with a simple chip.

Example entities are italicized. Sentences 1 and
2 are classified as action items. Sentence 3 is clas-
sified as positive-subjective, but because it contains
no entities, no < participant, relation, entity >
triple can be added to the ontology. Sentence
4 is classified as a decision sentence, and Sen-
tence 5 is both a decision sentence and a positive-
subjective sentence (because the participant is advo-
cating a particular position). The ontology is pop-
ulated by adding all of the sentence entities as in-
stances of the Entity class, all of the participants
as instances of the Participant class, and adding
< participant, relation, entity > triples for Sen-
tences 1, 2, 4 and 5. For example, Sentence 5 results
in the following two triples being added to the on-
tology:

<ProjectManager rdf:ID="participant-A">
<hasDecision rdf:resource="#simple-chip"/>
</ProjectManager>

<ProjectManager rdf:ID="participant-A">
<hasPos rdf:resource="#simple-chip"/>
</ProjectManager>

Elements in the ontology are associated with lin-
guistic annotations used by the generation compo-
nent of our system; since we do not discuss the gen-
eration task here, we presently skip the details of this
aspect of the ontology. In the following section we
describe the features used for the ontology mapping.

3.1 Feature Set

The interpretation component uses general features
that are applicable to any conversation domain. The
first set of features we use for ontology mapping are
features relating to conversational structure. These
are listed and briefly described in Table 1. The
Sprob and Tprob features measure how terms clus-
ter between conversation participants and conver-
sation turns. There are simple features measur-
ing sentence length (SLEN, SLEN2) and position
(TLOC, CLOC). Pause-style features indicate how
much time transpires between the previous turn, the
current turn and the subsequent turn (PPAU, SPAU).
For email conversations, pause features are based on
the timestamps between consecutive emails. Lexical
features capture cohesion (CWS) and cosine sim-
ilarity between the sentence and the conversation
(CENT1, CENT2). All structural features are nor-
malized by document length.

896



Feature ID Description

MXS max Sprob score
MNS mean Sprob score
SMS sum of Sprob scores
MXT max Tprob score
MNT mean Tprob score
SMT sum of Tprob scores
TLOC position in turn
CLOC position in conv.
SLEN word count, globally normalized
SLEN2 word count, locally normalized
TPOS1 time from beg. of conv. to turn
TPOS2 time from turn to end of conv.
DOM participant dominance in words
COS1 cos. of conv. splits, w/ Sprob
COS2 cos. of conv. splits, w/ Tprob
PENT entro. of conv. up to sentence
SENT entro. of conv. after the sentence
THISENT entropy of current sentence
PPAU time btwn. current and prior turn
SPAU time btwn. current and next turn
BEGAUTH is first participant (0/1)
CWS rough ClueWordScore
CENT1 cos. of sentence & conv., w/ Sprob
CENT2 cos. of sentence & conv., w/ Tprob

Table 1: Features Key

While these features have been found to work
well for generic extractive summarization, we use
additional features for capturing the more specific
sentence-level phenomena of this research.

• Character trigrams We derive all of the char-
acter trigrams in the collected corpora and in-
clude features indicating the presence or ab-
sence of each trigram in a given sentence.

• Word bigrams We similarly derive all of the
word bigrams in the collected corpora.

• POS bigrams We similarly derive all of the
POS-tag bigrams in the collected corpora.

• Word pairs We consider w1, w2 to be a word
pair if they occur in the same sentence and w1
precedes w2. We derive all of the word pairs
in the collected corpora and includes features
indicating the presence or absence of each word
pair in the given sentence. This is essentially a
skip bigram where any amount of intervening
material is allowed as long as the words occur
in the same sentence.

• POS pairs We calculate POS pairs in the same
manner as word pairs, above. These are essen-
tially skip bigrams for POS tags.

• Varying instantiation ngrams We derive a
simplified set of VIN features for these exper-

iments. For each word bigram w1, w2, we fur-
ther represent the bigram as p1, w2 and w1, p2
so that each pattern consists of a word and a
POS tag. We include a feature indicating the
presence or absence of each of these varying
instantiation bigrams.

After removing features that occur fewer than five
times, we end up with 218,957 total features.

4 Transformation - ILP Content Selection

In the previous section we described how we
identify sentences that link participants and enti-
ties through a variety of sentence-level phenom-
ena. Having populated our ontology with these
triples to form a source representation, we now turn
to the task of transforming the source representa-
tion to a summary representation, identifying the <
participant, relation, entity > triples for which
we want to generate text. We adapt a method pro-
posed by Xie et al. (2009) for extractive sentence
selection. They propose an ILP approach that cre-
ates a summary by maximizing a global objective
function:

maximize (1− λ) ∗
∑

i

wici + λ ∗
∑

j

ujsj (1)

subject to
∑

j

ljsj < L (2)

where wi is the tf.idf score for concept i, uj is the
weight for sentence j using the cosine similarity to
the entire document, ci is a binary variable indicat-
ing whether concept i is selected (with the concept
represented by a unique weighted n-gram), sj is a
binary variable indicating whether sentence j is se-
lected, lj is the length of sentence j and L is the
desired summary length. The λ term is used to bal-
ance concept and sentence weights. This method se-
lects sentences that are weighted strongly and which
cover as many important concepts as possible. As
described by Gillick et al. (2009), concepts and
sentences are tied together by two additional con-
straints:

∑

j

sjoij ≥ ci ∀i (3)

sjoij ≤ ci ∀i,j (4)

897



where oij is the occurence of concept i in sentence
j. These constraints state that a concept can only be
selected if it occurs in a sentence that is selected,
and that a sentence can only be selected if all of its
concepts have been selected.

We adapt their method in several ways. As men-
tioned in the previous section, we use weighted noun
phrases as our entities instead of n-grams. In our
version of Equation 1, wi is the tf score of en-
tity i (the idf was already used to identify entities
as described previously). More importantly, our
sentence weight uj is the sum of all the posterior
probabilities for sentence j derived from the various
sentence-level classifiers. In other words, sentences
are weighted highly if they correspond to multiple
object properties in the ontology. To continue the
example from Section 3, the sentence Let’s go with
the simple chip may be selected because it represents
both a decision and a positive-subjective opinion, as
well as containing the entity simple chip which is
mentioned frequently in the conversation.

We include constraint 3 but not 4; it is possi-
ble for a sentence to be extracted even if not all
of its entities are. We know that all the sentences
under consideration will contain at least one en-
tity because sentences with no entities would not
have been mapped to the ontology in the form of
< participant, relation, entity > triples in the
first place. To begin with, we set the λ term at 0.75
as we are mostly concerned with identifying impor-
tant sentences containing multiple links to the on-
tology. In our case L is 20% of the total document
word count.

5 Experimental Setup

In this section we describe our conversation cor-
pora, the statistical classifiers used, and the evalu-
ation metrics employed.

5.1 Corpora

These experiments are conducted on both meeting
and email conversations, which we describe in turn.

5.1.1 The AMI Meetings Corpus

For our meeting summarization experiments, we
use the scenario portion of the AMI corpus (Carletta
et al., 2005), where groups of four participants take
part in a series of four meetings and play roles within

a fictitious company. There are 140 of these meet-
ings in total, including a 20 meeting test set contain-
ing multiple human summary annotations per meet-
ing (the others are annotated by a single individual).
We report results on both manual and ASR tran-
scripts. The word error rate for the ASR transcripts
is 38.9%.

For the summary annotation, annotators wrote ab-
stract summaries of each meeting and extracted sen-
tences that best conveyed or supported the informa-
tion in the abstracts. The human-authored abstracts
each contain a general abstract summary and three
subsections for “decisions,” “actions” and “prob-
lems” from the meeting. A many-to-many mapping
between transcript sentences and sentences from the
human abstract was obtained for each annotator. Ap-
proximately 13% of the total transcript sentences are
ultimately labeled as extracted sentences. A sen-
tence is considered a decision item if it is linked to
the decision portion of the abstract, and action and
problem sentences are derived similarly.

For the subjectivity annotation, we use annota-
tions of positive-subjective and negative-subjective
utterances on a subset of 20 AMI meetings (Wil-
son, 2008). Such subjective utterances involve
the expression of a private state, such as a pos-
itive/negative opinion, positive/negative argument,
and agreement/disagreement. Of the roughly 20,000
total sentences in the 20 AMI meetings, nearly 4000
are labeled as positive-subjective and nearly 1300 as
negative-subjective.

5.1.2 The BC3 Email Corpus

While our main experiments focus on the AMI
meeting corpus, we follow these up with an inves-
tigation into applying our abstractive techniques to
email data. The BC3 corpus (Ulrich et al., 2008)
contains email threads from the World Wide Web
Consortium (W3C) mailing list. The threads fea-
ture a variety of topics such as web accessibility and
planning face-to-face meetings. The annotated por-
tion of the mailing list consists of 40 threads. The
threads are annotated in the same manner as the AMI
corpus, with three human annotators per thread first
authoring abstracts and then linking email thread
sentences to the abstract sentences. The corpus also
contains speech act annotations. Unlike the AMI
corpus, however, there are no annotations for deci-

898



sions, actions and problems, an issue addressed later.

5.2 Classifiers

For these experiments we use a maximum entropy
classifier using the liblinear toolkit1 (Fan et al.,
2008). For each of the AMI and BC3 corpora, we
perform 10-fold cross-validation on the data. In all
experiments we apply a 20% compression rate in
terms of the total document word count.

5.3 Evaluation

We evaluate the various classifiers described in Sec-
tion 3 using the ROC curve and the area under the
curve (AUROC), where a baseline AUROC is 0.5
and an ideal classifier approaches 1.

To evaluate the content selection in the transfor-
mation stage, we use weighted recall.This evaluation
metric is based on the links between extracted sen-
tences and the human gold-standard abstracts, with
the underlying motivation being that sentences with
more links to the human abstract are generally more
informative, as they provide the content on which an
effective abstract summary should be built. If M is
the number of sentences selected in the transforma-
tion step, O is the total number of sentences in the
document, and N is the number of annotators, then
Weighted Recall is given by

recall =

∑M
i=1

∑N
j=1 L(si, aj)∑O

i=1

∑N
j=1 L(si, aj)

where L(si, aj) is the number of links for a sen-
tence si according to annotator aj . We can com-
pare machine performance with human performance
in the following way. For each annotator, we rank
their sentences from most-linked to least-linked and
select the best sentences until we reach the same
word count as our selections. We then calculate their
weighted recall score by using the other N-1 annota-
tions, and then average over all N annotators to get
an average human performance. We report all trans-
formation scores normalized by human performance
for that dataset.

6 Results

In this section we present results for our interpreta-
tion and transformation components.

1http://www.csie.ntu.edu.tw/ cjlin/liblinear/

6.1 Interpretation: Meetings

Figure 1 shows the ROC curves for the sentence-
level classifiers applied to manual transcripts. On
both manual and ASR transcripts, the classifiers
with the largest AUROCs are the action item and
general extractive classifiers. Action item sentences
can be detected very well with this feature set, with
the classifier having an AUROC of 0.92 on man-
ual transcripts and 0.93 on ASR, a result compa-
rable to previous findings of 0.91 and 0.93 (Mur-
ray and Renals, 2008) obtained using a speech-
specific feature set. General extractive classification
is also similar to other state-of-the-art extraction ap-
proaches on spoken data using speech features (Zhu
and Penn, 2006)2 with an AUROC of 0.87 on man-
ual and 0.85 on ASR. Decision sentences can also
be detected quite well, with AUROCs of 0.81 and
0.77. Positive-subjective, negative-subjective and
problem sentences are the most difficult to detect,
but the classifiers still give credible performance
with AUROCs of approximately 0.76 for manual
and 0.70-0.72 for ASR.

 0

 0.2

 0.4

 0.6

 0.8

 1

 0  0.2  0.4  0.6  0.8  1

T
P

FP

actions
decisions
problems

positive-subjective
negative-subjective

extractive
random

Figure 1: ROC Curves for Ontology Mapping Classifiers
(Manual Transcripts)

6.2 Transformation: Meetings

In this section we present the weighted recall scores
for the sentences selected using the ILP method de-
scribed in Section 4. Remember, weighted recall
measures how useful these sentences would be in
generating sentences for an abstract summary. We
also assess the performance of three baseline sum-
marizers operating at the same compression level.

2Based on visual inspection of their reported best ROC curve

899



The simplest baseline (GREEDY) selects sentences
by ranking the posterior probabilites output by the
general extractive classifier. The second baseline
(CLASS COMBO) averages the posterior proba-
bilites output by all the classifiers and ranks sen-
tences from best to worst. The third baseline (RE-
TRAIN) uses the posterior probability outputs of all
the classifiers (except for the extractive classifier) as
new feature inputs for the general extractive classi-
fier.

 0.5

 0.6

 0.7

 0.8

 0.9

 1

Greedy

Class Com
bo

Retrain

ILP

W
ei

gh
te

d 
R

ec
al

l, 
N

or
m

al
iz

ed

manual
ASR

Figure 2: Weighted Recall Scores for AMI Meetings

Figure 2 shows the weighted recall scores, nor-
malized by human performance, for all approaches
on both manual and ASR transcripts. On man-
ual transcripts, the ILP approach (0.76) is better
than GREEDY (0.71) with a marginally significant
difference (p=0.07) and is significantly better than
CLASS COMBO and RETRAIN (both 0.68) ac-
cording to t-test (p < 0.05) . For ASR transcripts,
the ILP approach is significantly better than all other
approaches (p < 0.05). Xie et al. (2009) reported
ROUGE-1 F-measures on a different meeting cor-
pus, and our ROUGE-1 scores are in the same range
of 0.64-0.69 (they used 18% compression ratio).

6.3 Interpretation: Emails

We applied the same summarization method to the
40 BC3 email threads, with contrasting results. Be-
cause the BC3 corpus does not currently contain an-
notations for decisions, actions and problems, we
simply ran the AMI-trained models over the data
for those three phenomena. We can assess the
performance of the extractive, positive-subjective
and negative-subjective classifiers by examining the

ROC curves displayed in Figure 3. Both the general
extractive and negative-subjective classifiers have
AUROCs of around 0.75. The positive-subjective
classifier initially has the worst performance with
an AUROC of 0.66, but we found that positive-
subjective performance increased dramatically to an
AUROC of 0.77 when we used only conversational
features and not word bigrams, character trigrams or
POS tags.

 0

 0.2

 0.4

 0.6

 0.8

 1

 0  0.2  0.4  0.6  0.8  1

T
P

FP

positive-subjective
negative-subjective

extractive
random

Figure 3: ROC Curves for Ontology Mapping Classifiers
(BC3 Corpus)

6.4 Transformation: Emails

If we examine the weighted recall scores in Fig-
ure 4 we see that the ILP approach is worse than
the greedy summarizers on the BC3 dataset. How-
ever, the differences are not significant between ILP
and COMBO CLASS (p=0.15) and only marginally
significant compared with RETRAIN and GREEDY
(both p=0.08). The performance of the ILP approach
varies greatly across email threads. The top 15
threads (out of 40) yield ILP weighted recall scores
that are on par with human performance, while the
worst 15 are half that.

6.4.1 Email Corpus Analysis

Due to the large discrepancy in performance on
BC3 emails, we conducted additional experiments
for error analysis. We first explored whether we
could build a classifier that could discriminate the
best 15 emails from the worst 15 emails in terms of
weighted recall scores with the ILP approach, to de-
termine whether there are certain features that cor-
relate with good performance. Using the same fea-

900



 0.5

 0.6

 0.7

 0.8

 0.9

 1

Greedy

Class Com
bo

Retrain

ILP

W
ei

gh
te

d 
R

ec
al

l, 
N

or
m

al
iz

ed

Figure 4: Weighted Recall Scores for BC3 Threads

tures described in Section 3.1, we built a logistic re-
gression classifier on the two classes and found that
they can be discriminated quite well (80% accuracy
on an approximately balanced dataset) and that the
conversation structure features are the most useful
for discerning them. Table 2 shows the weighted
recall scores and several conversation features that
were weighted most highly by the logistic regres-
sion model. In particular, we found that the email
threads that yielded good performance tended to fea-
ture more active participants (# Participants), were
not dominated by a single individual (BEGAUTH),
and featured a higher number of turns (# Turns)
that followed each other in quick succession without
long pauses (PPAU, pause as percentage of conver-
sation length). In other words, these emails were
structured more similarly to meetings. Note that
since we normalize weighted recall by human per-
formance, it is possible to have a weighted recall
score higher than 1. On the 15 best threads, our sys-
tem achieves human-level performance. Because we
used AMI-trained models for detecting decisions,
actions and problems in the BC3 data, it is not sur-
prising that performance was better on those emails
structured similarly to meetings. All of this indicates
that there are many different types of emails and that
we will have to focus on improving performance on
emails that differ markedly in structure.

7 Conclusion

We have presented two components of an abstractive
conversation summarization system. The interpreta-
tion component is used to populate a simple conver-

Metric Worst 15 Best 15
Weighted Recall 0.49 1.05
# Turns 6.27 6.73
# Participants 4.67 5.4
PPAU 0.18 0.12
BEGAUTH 0.31 0.18

Table 2: Selected Email Features, Averaged

sation ontology where conversation participants and
entities are linked by object properties such as deci-
sions, actions and subjective opinions. For this step
we show that highly accurate classifiers can be built
using a large set of features not specific to any con-
versation modality.

In the transformation step, a summary is cre-
ated by maximizing a function relating sentence
weights and entity weights, with the sentence
weights determined by the sentence-ontology map-
ping. Our evaluation shows that the sentences we
select are highly informative to generate abstract
summaries, and that our content selection method
outperforms several greedy selection approaches.
The system described thus far may appear extrac-
tive in nature, as the transformation step is iden-
tifying informative sentences in the conversation.
However, these selected sentences correspond to
< participant, relation, entity > triples in the
ontology, for which we can subsequently gener-
ate novel text by creating linguistic annotations of
the conversation ontology (Galanis and Androut-
sopolous, 2007). Even without the generation step,
the approach described above allows us to create
structured extracts by grouping sentences according
to specific phenomena such as action items and de-
cisions. The knowledge represented by the ontology
enables us to significantly improve sentence selec-
tion according to intrinsic measures and to generate
structured output that we hypothesize will be more
useful to an end user compared with a generic un-
structured extract.

Future work will focus on the generation compo-
nent and on applying the summarization system to
conversations in other modalities such as blogs and
instant messages. Based on the email error analysis,
we plan to pursue domain adaptation techniques to
improve performance on different types of emails.

901



References

R. Barzilay and K. McKeown. 2005. Sentence fusion for
multidocument news summarization. Computational
Linguistics, 31(3):297–328.

G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proc. of ACM
WWW 07, Banff, Canada.

J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Well-
ner. 2005. The AMI meeting corpus: A pre-
announcement. In Proc. of MLMI 2005, Edinburgh,
UK, pages 28–39.

K. Church and W. Gale. 1995. Inverse document fre-
quency IDF: A measure of deviation from poisson. In
Proc. of the Third Workshop on Very Large Corpora,
pages 121–130.

J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proc. of COLING/ACL 2006, pages 144–
151.

R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. Liblinear: A library for large linear
classification. Journal of Machine Learning Research,
9:1871–1874.

D. Galanis and I. Androutsopolous. 2007. Generating
multilingual descriptions from linguistically annotated
owl ontologies: the naturalowl system. In Proc. of
ENLG 2007, Schloss Dagstuhl, Germany.

M. Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proc. of EMNLP 2006, Sydney, Australia, pages 364–
372.

D. Gillick, K. Riedhammer, B. Favre, and D. Hakkani-
Tür. 2009. A global optimization framework for meet-
ing summarization. In Proc. of ICASSP 2009, Taipei,
Taiwan.

S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky. 2007.
Resolving ”You” in multi-party dialog. In Proc. of
SIGdial 2007, Antwerp, Belgium.

L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-
summarization of audio-video presentations. In Proc.
of ACM MULTIMEDIA ’99, Orlando, FL, USA, pages
489–498.

P-Y. Hsueh, J. Kilgour, J. Carletta, J. Moore, and S. Re-
nals. 2007. Automatic decision detection in meeting
speech. In Proc. of MLMI 2007, Brno, Czech Repub-
lic.

K. Spärck Jones. 1999. Automatic summarizing: Factors
and directions. In I. Mani and M. Maybury, editors,
Advances in Automatic Text Summarization, pages 1–
12. MITP.

T. Kleinbauer, S. Becker, and T. Becker. 2007. Com-
bining multiple information layers for the automatic
generation of indicative meeting abstracts. In Proc. of
ENLG 2007, Dagstuhl, Germany.

K. Knight and D. Marcu. 2000. Statistics-based summa-
rization - step one: Sentence compression. In Proc. of
AAAI 2000, Austin, Texas, USA, pages 703–710.

J. Kupiec, J. Pederson, and F. Chen. 1995. A trainable
document summarizer. In Proc. of the 18th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval. Seattle, Wash-
ington, USA, pages 68–73.

K. McKeown, J. Hirschberg, M. Galley, and S. Maskey.
2005. From text to speech summarization. In Proc. of
ICASSP 2005, Philadelphia, USA, pages 997–1000.

C. Muller. 2007. Resolving It, This and That in un-
restricted multi-party dialog. In Proc. of ACL 2007,
Prague, Czech Republic.

G. Murray and S. Renals. 2008. Detecting action items
in meetings. In Proc. of MLMI 2008, Utrecht, the
Netherlands.

G. Murray, T. Kleinbauer, P. Poller, S. Renals, T. Becker,
and J. Kilgour. 2008. Extrinsic summarization evalu-
ation: A decision audit task. In Proc. of MLMI 2008,
Utrecht, the Netherlands.

M. Purver, J. Dowding, J. Niekrasz, P. Ehlen, and
S. Noorbaloochi. 2007. Detecting and summariz-
ing action items in multi-party dialogue. In Proc. of
the 9th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.

S. Raaijmakers, K. Truong, and T. Wilson. 2008. Multi-
modal subjectivity analysis of multiparty conversation.
In Proc. of EMNLP 2008, Honolulu, HI, USA.

O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proc. of HLT-
NAACL 2004, Boston, USA.

J. Ulrich, G. Murray, and G. Carenini. 2008. A publicly
available annotated corpus for supervised email sum-
marization. In Proc. of AAAI EMAIL-2008 Workshop,
Chicago, USA.

T. Wilson. 2008. Annotating subjective content in meet-
ings. In Proc. of LREC 2008, Marrakech, Morocco.

S. Xie, B. Favre, D. Hakkani-Tür, and Y. Liu. 2009.
Leveraging sentence weights in a concept-based op-
timization framework for extractive meeting summa-
rization. In Proc. of Interspeech 2009, Brighton, Eng-
land.

L. Zhou and E. Hovy. 2005. Digesting virtual ”geek”
culture: The summarization of technical internet relay
chats. In Proc. of ACL 2005, Ann Arbor, MI, USA.

X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proc. of Interspeech 2006,
Pittsburgh, USA, pages 1531–1534.

902


