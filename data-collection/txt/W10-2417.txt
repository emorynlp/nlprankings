










































Simplified Feature Set for Arabic Named Entity Recognition


Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110–115,
Uppsala, Sweden, 16 July 2010. c©2010 Association for Computational Linguistics

Simplified Feature Set for Arabic Named Entity Recognition 

Ahmed Abdul-Hamid, Kareem Darwish 

Cairo Microsoft Innovation Center 

Cairo, Egypt 

{ahmedab,kareemd}@microsoft.com  

  

Abstract 

This paper introduces simplified yet effective 

features that can robustly identify named enti-

ties in Arabic text without the need for mor-

phological or syntactic analysis or gazetteers. 

A CRF sequence labeling model is trained on 

features that primarily use character n-gram of 

leading and trailing letters in words and word 

n-grams.  The proposed features help over-

come some of the morphological and ortho-

graphic complexities of Arabic.  In comparing 

to results in the literature using Arabic specific 

features such POS tags on the same dataset 

and same CRF implementation, the results in 

this paper are lower by 2 F-measure points for 

locations, but are better by 8 points for organi-

zations and 9 points for persons.     

1 Introduction 

Named entity recognition (NER) continues to be 

an important part of many NLP applications such 

as information extraction, machine translation, 

and question answering (Benajiba et al., 2008).  

NER is concerned with identifying sequences of 

words referring to named entities (NE’s) such as 

persons, locations, and organizations.  For exam-

ple, in the word sequence “Alan Mulally, CEO of 

Detroit based Ford Motor Company,” Alan Mu-

lally, Detroit, and Ford Motor Company would 

be identified as a person, a location, and an or-

ganization respectively.   

Arabic is a Semitic language that present inter-

esting morphological and orthographic challeng-

es that may complicate NER.  Some of these 

challenges include: 

 Coordinating conjunctions, prepositions, 
possessive pronouns, and determiners are 

typically attached to words as prefixes or 

suffixes.   

 Proper names are often common language 
words.  For example, the proper name 

“Iman” also means faith. 

 Lack capitalization of proper nouns.  

The paper introduces a simplified set of features 

that can robustly identify NER for Arabic with-

out the need for morphological or syntactic anal-

ysis.  The proposed features include: word lead-

ing and trailing character n-gram features that 

help handle prefix and suffix attachment; word 

n-gram probability based features that attempt to 

capture the distribution of NE’s in text; word 

sequence features; and word length.   

The contributions of this paper are as follows: 

1. Identifying simplified features that work well 
for Arabic without gazetteers and without 

morphological and syntactic features, leading 

to improvements over previously reported re-

sults. 

2. Using leading and trailing character n-grams 
in words, which help capture valuable mor-

phological and orthographic clues that would 

indicate or counter-indicate the presence of 

NE’s. 

3. Incorporating word language modeling based 
features to capture word associations and rela-

tive distribution of named entities in text. 

Conditional Random Fields (CRF) sequence la-

beling was used in identifying NE’s, and the ex-

periments were performed on two standard Ara-

bic NER datasets.   

The rest of the paper is organized as follows:  

Section 2 surveys prior work on Arabic NER; 

Section 3 introduces the proposed features and 

motivates their use; Section 4 describes experi-

mental setup and evaluation sets; Section 5 re-

ports on experimental results; and Section 6 con-

cludes the paper. 

2 Background  

Much work has been done on NER with multiple 

evaluation forums dedicated to information ex-

traction in general and to NER in specific.  

Nadeau and Sekine (2009) surveyed lots of work 

on NER for a variety of languages and using a 

myriad of techniques.  Significant work has been 

conducted by Benajiba and colleagues on Arabic 

NER (Benajiba and Rosso, 2008; Benajiba et al., 

2008; Benajiba and Rosso, 2007; Benajiba et al., 

110



2007).  Benajiba et al. (2007) used a maximum 

entropy based classification trained on a feature 

set that include the use of gazetteers and a stop-

word list, appearance of a NE in the training set, 

leading and trailing word bigrams, and the tag of 

the previous word.  They reported 80%, 37%, 

and 47% F-measure for locations, organizations, 

and persons respectively.  Benajiba and Rosso 

(2007) improved their system by incorporating 

POS tags to improve NE boundary detection.  

They reported 87%, 46%, and 52% F-measure 

for locations, organizations, and persons respec-

tively.  Benajiba and Rosso (2008) used CRF 

sequence labeling and incorporated many lan-

guage specific features, namely POS tagging, 

base-phrase chunking, Arabic tokenization, and 

adjectives indicating nationality.  They reported 

that tokenization generally improved recall.  Us-

ing POS tagging generally improved recall at the 

expense of precision, leading to overall im-

provement in F-measure.  Using all their sug-

gested features they reported 90%, 66%, and 

73% F-measure for location, organization, and 

persons respectively.   In Benajiba et al. (2008), 

they examined the same feature set on the Auto-

matic Content Extraction (ACE) datasets using 

CRF sequence labeling and Support Vector Ma-

chine (SVM) classifier.  They did not report per 

category F-measure, but they reported overall 

81%, 75%, and 78% macro-average F-measure 

for broadcast news and newswire on the ACE 

2003, 2004, and 2005 datasets respectively.  

Huang (2005) used an HMM based NE recog-

nizer for Arabic and reported 77% F-measure on 

the ACE 2003 dataset.  Farber et al. (2008) used 

POS tags obtained from an Arabic morphological 

analyzer to enhance NER.  They reported 70% F-

measure on the ACE 2005 dataset.  Shaalan and 

Raza (2007) reported on a rule-based system that 

uses hand crafted grammars and regular expres-

sions in conjunction with gazetteers.  They re-

ported upwards of 93% F-measure, but they con-

ducted their experiments on non-standard da-

tasets, making comparison difficult. 

McNamee and Mayfield (2002) explored the 

training of an SVM classifier using many lan-

guage independent binary features such as lead-

ing and trailing letters in a word, word length, 

presence of digits in a word, and capitalization.  

They reported promising results for Spanish and 

Dutch.  In follow on work, Mayfield et al. (2003) 

used thousands of language independent features 

such character n-grams, capitalization, word 

length, and position in a sentence, along with 

language dependent features such as POS tags 

and BP chunking.  For English, they reported 

89%, 79%, and 91% F-measure for location, or-

ganization, and persons respectively. 

The use of CRF sequence labeling has been 

increasing over the past few years (McCallum 

and Li, 2003; Nadeau and Sekine, 2009) with 

good success (Benajiba and Rosso, 2008).  

Though, CRF’s are not guaranteed to be better 

than SVM’s (Benajiba et al., 2008). 

3 NER Features 

For this work, a CRF sequence labeling was 

used.  The advantage of using CRF is that they 

combine HMM-like generative power with clas-

sifier-like discrimination (Lafferty et al., 2001; 

Sha and Pereira, 2003).  When a CRF makes a 

decision on the label to assign to a word, it also 

accounts for the previous and succeeding words.  

The CRF was trained on a large set of surface 

features to minimize the use of Arabic morpho-

logical and syntactic features.  Apart from stem-

ming two coordinating conjunctions, no other 

Arabic specific features were used.   

The features used were as follows: 

 Leading and trailing character bigrams (6bi).  
For a given word composed of the letter se-

quence   
 , where    and    are a start and 

end word markers respectively, the first three 

bigrams (   
 ,   

 , and   
 ) and last three bi-

grams (    
   ,      

   , and     
 ) were used as 

features.  Using leading and trailing charac-

ter bigrams of a word was an attempt to ac-

count for morphological and orthographic 

complexities of Arabic and to capture sur-

face clues that would indicate the presence of 

a NE or not.  For example, plural forms of 

common words in Arabic are often obtained 

by attaching the suffixes wn
1
 (ين) or yn (ون) 

for masculine nouns and At (ات) for feminine 

nouns.  Presence of such plural form markers 

would generally indicate a plural noun, but 

would counter-indicate a NE.  Also, verbs in 

present tense start with the letters A (ا), t (ت), 

y (ي), and n (ن). These would contribute to 

concluding that a word may not be a NE.  

Further, coordinate conjunctions, such as f 

 and prepositions, such as b ,(و) and w (ف)

-composed of single let ,(ل) and l ,(ك) k ,(ب)

ters are often attached as prefixes to words.  

Accounting for them may help overcome 

some of the problems associated with not 

                                                 
1
 Arabic letters are presented using the Buckwalter 

transliteration scheme 

111



stemming.  Further, the determiner Al (ال) 

may be a good indicator for proper nouns 

particularly in the case of organizations.  

This would be captured by the second bi-

gram from the head of the word.  If the de-

terminer is preceded by a coordinating con-

junction, the third bigram from the head of 

the word would be able to capture this fea-

ture. 

 Leading and trailing character trigrams 
(6tri).  For a given word composed of the 

letter sequence   
 , where    and    are a start 

and end word markers respectively, the first 

three trigrams (  
 ,   

 , and   
 ) and last three 

trigrams (    
   ,      

   , and     
 ) were used as 

features.  The rationale for using these fea-

tures is very similar to that of using character 

bigrams.  The added value of using character 

trigrams, is that they would allow for the 

capture of combinations of prefixes and suf-

fixes.  For example, a word may begin with 

the prefixes w+Al (و+ال), which are a coordi-

nating conjunction and determiner respec-

tively. 

 Leading and trailing character 4-grams 
(6quad).  For a given word composed of the 

letter sequence   
 , where    and    are a start 

and end word markers respectively, the first 

three 4 grams (  
 ,   

 , and   
 ) and last three 4 

grams (    
   ,      

   , and     
 ) were used as 

features.  Similar to leading and trailing tri-

grams, these features can capture combina-

tions of prefixes and suffixes. 

 Word position (WP).  The feature captures 
the relative position of a word in a sentence 

as follows: 

   
                 

               
 

Typically, Arabic is a VSO language.  Thus, 

NE’s in specific and nouns in general do not 

start sentences. 

 Word length (WL).  The feature captures the 
length of named entities, as some NE’s, par-

ticularly transliterated NE’s, may be longer 

than regular words. 

 Word unigram probability (1gP).  This is 
simply the unigram probability of word.  Ac-

counting for unigram probability would help 

exclude common words.  Also, named enti-

ties are often out-of-vocabulary words. 

 Word with previous and word with succeed-
ing word-unigram ratio (1gPr).  Given a 

word wi, these two features are computed as: 

      
 (  )

 (    )
 

  

      
 (    )

 (  )
 

This feature would potentially capture major 

shifts between word probabilities.  For ex-

ample, a named entity is likely to have much 

lower probability compared to the word be-

fore it and the word after it. 

 Features that account for dependence be-
tween words in a named entity.  Popular 

NE’s are likely collocations, and words that 

make up named entities don’t occur next to 

each other by chance.  These features are as 

follows: 

o Word with previous and word with succeed-
ing word bigram (2gP).  For a given word wi, 

the two bigram probabilities are p(wi-1wi) and 

p(wiwi+1).  Words composing named entities 

are likely conditionally dependent. 

o t-test between a word and the word that pre-
cedes and succeeds it (T).  Given a word se-

quence wi and wi+1: 

   
 ̅   

√ 
 

 

 

Wher ̅     (       ),   (  )   (    ) , 
    ̅, and N is the number of words in the 
corpus (Manning and Schutze, 1999). 

o Mutual information between a word and the 
word that precedes and succeeds it (MI).  

Given a word sequence wi and wi+1: 

         [
 ̅

 
] , where  ̅ and   are identical 

to those in the t-test. 

 Character n-gram probability (3gCLM).  
Given character trigram language models for 

locations, persons, organizations, and non-

NE’s, the four features are just the character 

language model probabilities using the four 

different language models.  The motivation 

for these features stem from the likelihood 

that NE’s may have a different distribution 

of characters particularly for person names.  

This stems from the fact that many NE’s are 

transliterated names. 

4 Experimental Setup 

4.1 Datasets 

For this work, the NE’s of interest were persons, 

locations, and organizations only.  Two datasets 

were used for the work in this paper.  The first 

112



was a NE tagged dataset developed by Binajiba 

et al. (2007).  The Binajiba dataset is composed 

of newswire articles totaling more than 150,000 

words.  The number of different NE’s in the col-

lection are: 

Locations (LOC)  878 

Organizations (ORG)  342 

Persons (PER)   689 

The second was the Arabic Automatic Content 

Extraction (ACE) 2005 dataset.  The ACE da-

taset is composed of newswire, broadcast news, 

and weblogs.  For experiments in this work, the 

weblogs portion of the ACE collection was ex-

cluded, because weblogs often include colloquial 

Arabic that does not conform to modern standard 

Arabic.  Also, ACE tags contain many sub-

categories.  For example, locations are tagged as 

regions, bodies of water, states, etc.  All sub-tags 

were ignored and were conflated to the base tags 

(LOC, ORG, PER).  Further, out of the 40 sub-

entity types, entities belonging to the following 

13 ACE sub-entity types were excluded because 

they require anaphora resolution or they refer to 

non-specific NE’s: nominal, pronominal, kind of 

entity (as opposed to a specific entity), negative-

ly quantified entity, underspecified entity, ad-

dress, boundary (eg. border), celestial object 

(comet), entertainment venue (eg. movie theater), 

sport (eg. football), indeterminate (eg. human), 

vehicle, and weapon.  The total number of words 

in the collection is 98,530 words (66,590 from 

newswire and 31,940 from broadcast news).  The 

number of NE’s is as follows: 

Locations (LOC)  867 

Organizations (ORG)  269 

Persons (PER)   524 

Since both collections do not follow the same 

tagging conventions, training and testing were 

conducted separately for each collection.  Each 

collection was 80/20 split for training and test-

ing. 

4.2 Data Processing and Sequence Labeling 

Training and testing were done using CRF++ 

which is a CRF sequence label toolkit.  The fol-

lowing processing steps of Arabic were per-

formed: 

 The coordinating conjunctions w (و) and f 
-which always appear as the first prefix ,(ف)

es in a word, were optionally stemmed. w 

and f were stemmed using an in-house Ara-

bic stemmer that is a reimplementation of the 

stemmer proposed by Lee et al. (2003).  

However, stemming w or f could have been 

done by stemming the w or f and searching 

for the stemmed word in a large Arabic cor-

pus.  If the stemmed word appears more than 

a certain count, then stemming was appropri-

ate. 

 The different forms of alef (A (أ) < ,(آ) | ,(ا), 
and < (إ)) were normalized to A (ا), y (ي) and 

Y (ى) were normalized to y (ي), and p (ة) was 

mapped to h (هـ). 

4.3 Evaluation  

The figures of merit for evaluation were preci-

sion, recall, and F-measure ( = 1), with evalua-
tion being conducted at the phrase level.  Report-

ing experiments with all the different combina-

tions of features would adversely affect the read-

ability of the paper.  Thus, to ascertain the con-

tribution of the different features, a set of 15 ex-

periments are being reported for both datasets.  

The experiments were conducted using raw Ara-

bic words (3w) and stems (3s).  Using the short 

names of features (bolded after feature names in 

section 3), the experiments were as follows: 

 3w 

 3w_6bi 

 3w_6bi_6tri 

 3w_6bi_6tri_6quad 

 3w_6bi_6tri_6quad_WL 

 3w_6bi_6tri_6quad_WP 

 3s 

 3s_6bi_6tri_6quad 

 3s_6bi_6tri_6quad_1gP 

 3s_6bi_6tri_6quad_1gPr_1gP 

 3s_6bi_6tri_6quad_2gP 

 3s_6bi_6tri_6quad_3gCLM 

 3s_6bi_6tri_6quad_MI 

 3s_6bi_6tri_6quad_T 

 3s_6bi_6tri_6quad_T_MI 

5 Experimental Results 

Table 1 lists the results for the Benajiba and 

ACE datasets respectively.  Tables 2 and 3 report 

the best obtained results for both datasets.  The 

results include precision (P), recall (R), and F-

measure (F) for NE’s of types location (LOC), 

organization (ORG), and person (PER).  The best 

results for P, R, and F are bolded in the tables.  

In comparing the base experiments 3w and 3s in 

which the only the surface forms and the stems 

were used respectively, both produced the high-

est precision.  However, 3s improved recall over 

3w by 7, 13, and 14 points for LOC, ORG, and 

PER respectively on the Benajiba dataset.  

Though using 3s led to a drop in P for ORG 

113



compared to 3w, it actually led to improvement 

in P for PER.  Similar results were observed for 

the ACE dataset, but the differences were less 

pronounced with 1% to 2% improvements in re-

call.  However, when including the 6bi, 6tri, and 

6quad features the difference between using 

words or stems dropped to about 1 point in recall 

and nearly no difference in precision.  This 

would indicate the effectiveness of using leading 

and trailing character n-grams in overcoming 

morphological and orthographic complexities.  

 
  Benajiba ACE 

Run Name Type P R F P R F 

3w 

LOC 96 59 73 88 59 71 

ORG 92 36 51 87 50 63 

PER 90 32 48 94 47 63 

3w_6bi 

LOC 92 75 82 85 72 78 

ORG 83 57 67 76 54 63 

PER 87 68 76 89 70 78 

3w_6bi_6tri 

LOC 93 79 86 87 77 82 

ORG 82 61 70 77 56 65 

PER 89 72 80 89 73 80 

3w_6bi_6tri

_6quad 

LOC 93 83 87 87 77 81 

ORG 84 64 72 77 55 65 

PER 90 73 81 92 71 80 

3w_6bi_6tri

_6quad_WL 

LOC 93 82 87 87 78 82 

ORG 83 64 73 79 56 65 

PER 89 73 80 93 71 81 

3w_6bi_6tri

_6quad_WP 

LOC 91 82 86 88 77 82 

ORG 83 62 71 77 59 67 

PER 89 74 81 91 70 79 

3s 

LOC 96 66 78 89 60 72 

ORG 88 49 63 86 52 65 

PER 93 46 61 92 49 64 

3s_6bi_6tri_

6quad 

LOC 93 83 88 87 77 82 

ORG 84 63 72 78 58 67 

PER 90 74 81 91 70 80 

3s_6bi_6tri_

6quad_1gP 

LOC 93 83 88 87 77 82 

ORG 84 64 73 79 57 66 

PER 90 75 82 93 70 80 

3s_6bi_6tri_

6quad_1gPr_

1gP 

LOC 93 81 87 87 77 81 

ORG 85 60 70 82 55 66 

PER 91 72 81 93 69 79 

3s_6bi_6tri_

6quad_2gP 

LOC 93 81 87 88 77 82 

ORG 85 61 71 82 56 67 

PER 89 74 81 90 69 78 

3s_6bi_6tri_

6quad_3gCL

M 

LOC 93 82 87 87 76 81 

ORG 84 65 74 78 56 66 

PER 90 74 81 93 71 81 

3s_6bi_6tri_

6quad_MI 

LOC 93 81 86 87 77 82 

ORG 84 59 69 82 56 66 

PER 90 72 80 93 70 80 

3s_6bi_6tri_

6quad_T 

LOC 93 81 87 87 76 81 

ORG 85 61 71 82 55 66 

PER 90 72 80 93 69 79 

3s_6bi_6tri_

6quad_T_MI 

LOC 93 80 86 87 76 81 

ORG 85 57 68 82 54 65 

PER 91 71 80 93 67 78 

Table 1: NER results for the Benajiba and 

ACE datasets 

 P R F 

LOC 93 83 88 

ORG 84 64 73 

PERS 90 75 82 

Avg. 89 74 81 

Table 2:  Best results on Benajiba dataset  

(Run name: 3s_6bi_6tri_6quad_1gP) 

 

 P R F 

LOC 87 77 82 

ORG 79 56 65 

PERS 93 71 81 

Avg. 88 70 76 

Table 3:  Best results on ACE dataset 

(Run name: 3w_6bi_6tri_6quad_WL) 

 

 P R F 

LOC 93 87 90 

ORG 84 54 66 

PERS 80 67 73 

Avg. 86 69 76 

Table 4:  The results in (Benajiba and Rosso, 

2008) on Benajiba dataset 

 

The 3s_6bi_6tri_6quad run produced nearly the 

best F-measure for both datasets, with extra fea-

tures improving overall F-measure by at most 1 

point. 

Using t-test T and mutual information MI did 

not yield any improvement in either recall or 

precision, and often hurt overall F-measure.  As 

highlighted in the results, the 1gP, 2gP, WL, WP, 

and 3gCLM typically improved recall slightly, 

often leading to 1 point improvement in overall 

F-measure. 

To compare to results in the literature, Table 4 

reports the results obtained by Benajiba and Ros-

so (2008) on the Benajiba dataset using the 

CRF++ implementation of CRF sequence label-

ing trained on a variety of Arabic language spe-

cific features.  The comparison was not done on 

their results on the ACE 2005 dataset due to po-

tential difference in tags.  The averages in Tables 

2, 3, and 4 are macro-averages as opposed to mi-

cro-averages reported by Benajiba and Rosso 

(2008).  In comparing Tables 2 and 4, the fea-

tures suggested in this paper reduced F-measure 

for locations by 2 points, but improved F-

measure for organizations and persons by 8 

points and 9 points respectively, due to im-

provements in both precision and recall. 

114



The notable part of this work is that using a sim-

plified feature set outperforms linguistic features.  

As explained in Section 3, using leading and 

trailing character n-grams implicitly capture 

morphological and syntactic features that typical-

ly used for Arabic lemmatization and POS tag-

ging (Diab, 2009).  The improvement over using 

linguistic features could possibly be attributed to 

the following reasons:  not all prefixes and suf-

fixes types equally help in identifying named 

entities (ex. appearance of a definite article or 

not); not all prefixes and suffix surface forms 

equally help (ex. appearance of the coordinating 

conjunction w “و” vs. f “ف”); and mistakes in 

stemming and POS tagging.  The lag in recall for 

locations behind the work of Benajiba and Rosso 

(2008) could be due to the absence of location 

gazetteers.  

6 Conclusion and Future Work 

This paper presented a set of simplified yet effec-

tive features for named entity recognition in Ar-

abic.  The features helped overcome some of the 

morphological and orthographic complexities of 

Arabic.  The features included the leading and 

trailing character n-grams in words, word associ-

ation features such as t-test, mutual information, 

and word n-grams, and surface features such 

word length and relative word position in a sen-

tence.  The most important features were leading 

and trailing character n-grams in words.  The 

proposed feature set yielded improved results 

over those in the literature with as much as 9 

point F-measure improvement for recognizing 

persons. 

For future work, the authors would like to exam-

ine the effectiveness of the proposed feature set 

on other morphologically complex languages, 

particularly Semitic languages.  Also, it is worth 

examining the combination of the proposed fea-

tures with morphological features. 

References  

Y. Benajiba, M. Diab, and P. Rosso. 2008.  Arabic 

Named Entity Recognition using Optimized Fea-

ture Sets.  Proceedings of the 2008 Conference on 

Empirical Methods in Natural Language Pro-

cessing, pages 284–293, Honolulu, October 2008. 

Y. Benajiba and P. Rosso. 2008. Arabic Named Entity 

Recognition using Conditional Random Fields. In 

Proc. of Workshop on HLT & NLP within the Ar-

abic World, LREC’08. 

Y. Benajiba, P. Rosso and J. M. Benedí.  2007. AN-

ERsys: An Arabic Named Entity Recognition sys-

tem based on Maximum Entropy. In Proc. of CI-

CLing-2007, Springer-Verlag, LNCS(4394), pp. 

143-153. 

Y. Benajiba and P. Rosso. 2007. ANERsys 2.0: Con-

quering the NER task for the Arabic language by 

combining the Maximum Entropy with POS-tag in-

formation. In Proc. of Workshop on Natural Lan-

guage-Independent Engineering, IICAI-2007. 

M. Diab. 2009. Second Generation Tools (AMIRA 

2.0): Fast and Robust Tokenization, POS tagging, 

and Base Phrase Chunking. Proceedings of the Se-

cond International Conference on Arabic Language 

Resources and Tools, 2009. 

B. Farber, D. Freitag, N. Habash, and O. Rambow. 

2008. Improving NER in Arabic Using a Morpho-

logical Tagger. In Proc. of LREC’08. 

F. Huang. 2005. Multilingual Named Entity Extrac-

tion and Translation from Text and Speech. Ph.D. 

Thesis. Pittsburgh: Carnegie Mellon University. 

J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-

ditional random fields: Probabilistic models for 

segmenting and labeling sequence data, In Proc. of 

ICML, pp.282-289, 2001.  

Young-Suk Lee, Kishore Papineni, Salim Roukos, 

Ossama Emam, Hany Hassan. 2003. Language 

Model Based Arabic Word Segmentation. ACL 

2003: 399-406 

C. Manning and H. Schutze. 1999. Foundations of 

Statistical Natural Language Processing. Cam-

bridge, Massachusetts: The MIT Press. 

J. Mayfield, P. McNamee, and C. Piatko.  2003.  

Named Entity Recognition using Hundreds of 

Thousands of Features.  HLT-NAACL 2003-

Volume 4, 2003. 

A. McCallum and W. Li. 2003. Early Results for 

Named Entity Recognition with Conditional Ran-

dom Fields, Features Induction and Web-

Enhanced Lexicons.  In Proc. Conference on Com-

putational Natural Language Learning. 

P. McNamee and J. Mayfield. 2002.  Entity extraction 

without language-specific. Proceedings of CoNLL, 

2002. 

D. Nadeau and S. Sekine. 2009.  A survey of named 

entity recognition and classification.  Named enti-

ties: recognition, classification and use, ed. S. 

Sekine and E. Ranchhod, John Benjamins Publish-

ing Company. 

F. Sha and F. Pereira. 2003. Shallow parsing with 

conditional random fields, In Proc. of 

HLT/NAACL-2003. 

K. Shaalan and H. Raza. 2007.  Person Name Entity 

Recognition for Arabic.  Proceedings of the 5th 

Workshop on Important Unresolved Matters, pages 

17–24, Prague, Czech Republic, June 2007. 

115


