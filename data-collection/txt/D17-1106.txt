



















































Mapping Instructions and Visual Observations to Actions with Reinforcement Learning


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1004–1015
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Mapping Instructions and Visual Observations to Actions
with Reinforcement Learning

Dipendra Misra†, John Langford‡, and Yoav Artzi†

† Dept. of Computer Science and Cornell Tech, Cornell University, New York, NY 10044
{dkm, yoav}@cs.cornell.edu

‡ Microsoft Research, New York, NY 10011
jcl@microsoft.com

Abstract

We propose to directly map raw visual ob-
servations and text input to actions for in-
struction execution. While existing ap-
proaches assume access to structured envi-
ronment representations or use a pipeline
of separately trained models, we learn a
single model to jointly reason about lin-
guistic and visual input. We use reinforce-
ment learning in a contextual bandit set-
ting to train a neural network agent. To
guide the agent’s exploration, we use re-
ward shaping with different forms of su-
pervision. Our approach does not re-
quire intermediate representations, plan-
ning procedures, or training different mod-
els. We evaluate in a simulated environ-
ment, and show significant improvements
over supervised learning and common re-
inforcement learning variants.

1 Introduction
An agent executing natural language instructions
requires robust understanding of language and its
environment. Existing approaches addressing this
problem assume structured environment represen-
tations (e.g.,. Chen and Mooney, 2011; Mei et al.,
2016), or combine separately trained models (e.g.,
Matuszek et al., 2010; Tellex et al., 2011), includ-
ing for language understanding and visual reason-
ing. We propose to directly map text and raw im-
age input to actions with a single learned model.
This approach offers multiple benefits, such as
not requiring intermediate representations, plan-
ning procedures, or training multiple models.

Figure 1 illustrates the problem in the Blocks
environment (Bisk et al., 2016). The agent ob-
serves the environment as an RGB image using a
camera sensor. Given the RGB input, the agent

North

South

EastWest

Put the Toyota block in the same row as the SRI block, in
the first open space to the right of the SRI block
Move Toyota to the immediate right of SRI, evenly aligned
and slightly separated
Move the Toyota block around the pile and place it just to
the right of the SRI block
Place Toyota block just to the right of The SRI Block
Toyota, right side of SRI

Figure 1: Instructions in the Blocks environment. The
instructions all describe the same task. Given the ob-
served RGB image of the start state (large image), our
goal is to execute such instructions. In this task, the
direct-line path to the target position is blocked, and
the agent must plan and move the Toyota block around.
The small image marks the target and an example path,
which includes 34 steps.

must recognize the blocks and their layout. To un-
derstand the instruction, the agent must identify
the block to move (Toyota block) and the destina-
tion (just right of the SRI block). This requires
solving semantic and grounding problems. For
example, consider the topmost instruction in the
figure. The agent needs to identify the phrase re-
ferring to the block to move, Toyota block, and
ground it. It must resolve and ground the phrase
SRI block as a reference position, which is then
modified by the spatial meaning recovered from
the same row as or first open space to the right
of, to identify the goal position. Finally, the agent
needs to generate actions, for example moving the
Toyota block around obstructing blocks.

To address these challenges with a single model,

1004



we design a neural network agent. The agent exe-
cutes instructions by generating a sequence of ac-
tions. At each step, the agent takes as input the
instruction text, observes the world as an RGB im-
age, and selects the next action. Action execution
changes the state of the world. Given an obser-
vation of the new world state, the agent selects
the next action. This process continues until the
agent indicates execution completion. When se-
lecting actions, the agent jointly reasons about its
observations and the instruction text. This enables
decisions based on close interaction between ob-
servations and linguistic input.

We train the agent with different levels of su-
pervision, including complete demonstrations of
the desired behavior and annotations of the goal
state only. While the learning problem can be eas-
ily cast as a supervised learning problem, learning
only from the states observed in the training data
results in poor generalization and failure to recover
from test errors. We use reinforcement learn-
ing (Sutton and Barto, 1998) to observe a broader
set of states through exploration. Following recent
work in robotics (e.g., Levine et al., 2016; Rusu
et al., 2016), we assume the training environment,
in contrast to the test environment, is instrumented
and provides access to the state. This enables a
simple problem reward function that uses the state
and provides positive reward on task completion
only. This type of reward offers two important ad-
vantages: (a) it is a simple way to express the ideal
agent behavior we wish to achieve, and (b) it cre-
ates a platform to add training data information.

We use reward shaping (Ng et al., 1999) to ex-
ploit the training data and add to the reward ad-
ditional information. The modularity of shap-
ing allows varying the amount of supervision, for
example by using complete demonstrations for
only a fraction of the training examples. Shap-
ing also naturally associates actions with imme-
diate reward. This enables learning in a contex-
tual bandit setting (Auer et al., 2002; Langford
and Zhang, 2007), where optimizing the immedi-
ate reward is sufficient and has better sample com-
plexity than unconstrained reinforcement learn-
ing (Agarwal et al., 2014).

We evaluate with the block world environment
and data of Bisk et al. (2016), where each instruc-
tion moves one block (Figure 1). While the orig-
inal task focused on source and target prediction
only, we build an interactive simulator and formu-

late the task of predicting the complete sequence
of actions. At each step, the agent must select be-
tween 81 actions with 15.4 steps required to com-
plete a task on average, significantly more than
existing environments (e.g., Chen and Mooney,
2011). Our experiments demonstrate that our re-
inforcement learning approach effectively reduces
execution error by 24% over standard supervised
learning and 34-39% over common reinforcement
learning techniques. Our simulator, code, models,
and execution videos are available at: https:
//github.com/clic-lab/blocks.

2 Technical Overview
Task Let X be the set of all instructions, S
the set of all world states, and A the set of all
actions. An instruction x̄ ∈ X is a sequence
〈x1, . . . , xn〉, where each xi is a token. The agent
executes instructions by generating a sequence of
actions, and indicates execution completion with
the special action STOP. Action execution mod-
ifies the world state following a transition func-
tion T : S × A → S. The execution ē of an
instruction x̄ starting from s1 is an m-length se-
quence 〈(s1, a1), . . . , (sm, am)〉, where sj ∈ S,
aj ∈ A, T (sj , aj) = sj+1 and am = STOP. In
Blocks (Figure 1), a state specifies the positions
of all blocks. For each action, the agent moves
a single block on the plane in one of four direc-
tions (north, south, east, or west). There are 20
blocks, and 81 possible actions at each step, in-
cluding STOP. For example, to correctly execute
the instructions in the figure, the agent’s likely first
action is TOYOTA-WEST, which moves the Toyota
block one step west. Blocks can not move over or
through other blocks.
Model The agent observes the world state via
a visual sensor (i.e., a camera). Given a world
state s, the agent observes an RGB image I gen-
erated by the function IMG(s). We distinguish be-
tween the world state s and the agent context1 s̃,
which includes the instruction, the observed image
IMG(s), images of previous states, and the pre-
vious action. To map instructions to actions, the
agent reasons about the agent context s̃ to generate
a sequence of actions. At each step, the agent gen-
erates a single action. We model the agent with a

1We use the term context similar to how it is used in the
contextual bandit literature to refer to the information avail-
able for decision making. While agent contexts capture in-
formation about the world state, they do not include physical
information, except as captured by observed images.

1005



neural network policy. At each step j, the network
takes as input the current agent context s̃j , and pre-
dicts the next action to execute aj . We formally
define the agent context and model in Section 4.
Learning We assume access to training data
with N examples {(x̄(i), s(i)1 , ē(i))}Ni=1, where x̄(i)
is an instruction, s(i)1 is a start state, and ē

(i) is
an execution demonstration of x̄(i) starting at s(i)1 .
We use policy gradient (Section 5) with reward
shaping derived from the training data to increase
learning speed and exploration effectiveness (Sec-
tion 6). Following work in robotics (e.g., Levine
et al., 2016), we assume an instrumented environ-
ment with access to the world state to compute the
reward during training only. We define our ap-
proach in general terms with demonstrations, but
also experiment with training using goal states.
Evaluation We evaluate task completion error
on a test set {(x̄(i), s(i)1 , s(i)g )}Mi=1, where x̄(i) is an
instruction, s(i)1 is a start state, and s

(i)
g is the goal

state. We measure execution error as the distance
between the final execution state and s(i)g .

3 Related Work
Learning to follow instructions was studied ex-
tensively with structured environment represen-
tations, including with semantic parsing (Chen
and Mooney, 2011; Kim and Mooney, 2012,
2013; Artzi and Zettlemoyer, 2013; Artzi et al.,
2014a,b; Misra et al., 2015, 2016), alignment
models (Andreas and Klein, 2015), reinforcement
learning (Branavan et al., 2009, 2010; Vogel and
Jurafsky, 2010), and neural network models (Mei
et al., 2016). In contrast, we study the problem of
an agent that takes as input instructions and raw vi-
sual input. Instruction following with visual input
was studied with pipeline approaches that use sep-
arately learned models for visual reasoning (Ma-
tuszek et al., 2010, 2012; Tellex et al., 2011; Paul
et al., 2016). Rather than decomposing the prob-
lem, we adopt a single-model approach and learn
from instructions paired with demonstrations or
goal states. Our work is related to Sung et al.
(2015). While they use sensory input to select and
adjust a trajectory observed during training, we
are not restricted to training sequences. Executing
instructions in non-learning settings has also re-
ceived significant attention (e.g., Winograd, 1972;
Webber et al., 1995; MacMahon et al., 2006).

Our work is related to a growing interest in
problems that combine language and vision, in-

cluding visual question answering (e.g., Antol
et al., 2015; Andreas et al., 2016b,a), caption gen-
eration (e.g., Chen et al., 2015, 2016; Xu et al.,
2015), and visual reasoning (Johnson et al., 2016;
Suhr et al., 2017). We address the prediction of the
next action given a world image and an instruction.

Reinforcement learning with neural networks
has been used for various NLP tasks, including
text-based games (Narasimhan et al., 2015; He
et al., 2016), information extraction (Narasimhan
et al., 2016), co-reference resolution (Clark and
Manning, 2016), and dialog (Li et al., 2016).

Neural network reinforcement learning tech-
niques have been recently studied for behavior
learning tasks, including playing games (Mnih
et al., 2013, 2015, 2016; Silver et al., 2016) and
solving memory puzzles (Oh et al., 2016). In con-
trast to this line of work, our data is limited. Ob-
serving new states in a computer game simply re-
quires playing it. However, our agent also consid-
ers natural language instructions. As the set of in-
structions is limited to the training data, the set of
agent contexts seen during learning is constrained.
We address the data efficiency problem by learn-
ing in a contextual bandit setting, which is known
to be more tractable (Agarwal et al., 2014), and us-
ing reward shaping to increase exploration effec-
tiveness. Zhu et al. (2017) address generalization
of reinforcement learning to new target goals in vi-
sual search by providing the agent an image of the
goal state. We address a related problem. How-
ever, we provide natural language and the agent
must learn to recognize the goal state.

Reinforcement learning is extensively used in
robotics (Kober et al., 2013). Similar to recent
work on learning neural network policies for robot
control (Levine et al., 2016; Schulman et al., 2015;
Rusu et al., 2016), we assume an instrumented
training environment and use the state to compute
rewards during learning. Our approach adds the
ability to specify tasks using natural language.

4 Model
We model the agent policy π with a neural net-
work. The agent observes the instruction and an
RGB image of the world. Given a world state
s, the image I is generated using the function
IMG(s). The instruction execution is generated
one step at a time. At each step j, the agent
observes an image Ij of the current world state
sj and the instruction x̄, predicts the action aj ,
and executes it to transition to the next state sj+1.

1006



Place the Toyota east of SRIx̄ :

h1

SOUTH

Visual State v10

LSTM

hdhb

TOYOTA

SoftMax Layers

Task Specific

TOYOTA-SOUTH Action a10

TOYOTA-SOUTH

CNN

l1 l2 l3 l4 l5 l6

Instruction Representation x̄

I10

I8 I9

Previous Action a9

Agent Context s̃10

Figure 2: Illustration of the policy architecture showing the 10th step in the execution of the instruction Place the
Toyota east of SRI in the state from Figure 1. The network takes as input the instruction x̄, image of the current
state I10, images of previous states I8 and I9 (with K = 2), and the previous action a9. The text and images are
embedded with LSTM and CNN. The actions are selected with the task specific multi-layer perceptron.

This process continues until STOP is predicted and
the agent stops, indicating instruction completion.
The agent also has access to K images of previ-
ous states and the previous action to distinguish
between different stages of the execution (Mnih
et al., 2015). Figure 2 illustrates our architecture.

Formally,2 at step j, the agent consid-
ers an agent context s̃j , which is a tuple
(x̄, Ij , Ij−1, . . . , Ij−K , aj−1), where x̄ is the natu-
ral language instruction, Ij is an image of the cur-
rent world state, the images Ij−1, . . . , Ij−K repre-
sent K previous states, and aj−1 is the previous
action. The agent context includes information
about the current state and the execution. Consid-
ering the previous action aj−1 allows the agent to
avoid repeating failed actions, for example when
trying to move in the direction of an obstacle. In
Figure 2, the agent is given the instruction Place
the Toyota east of SRI, is at the 10-th execution
step, and considers K = 2 previous images.

We generate continuous vector representations
for all inputs, and jointly reason about both text
and image modalities to select the next action.
We use a recurrent neural network (RNN; Elman,
1990) with a long short-term memory (LSTM;
Hochreiter and Schmidhuber, 1997) recurrence
to map the instruction x̄ = 〈x1, . . . , xn〉 to
a vector representation x̄. Each token xi is
mapped to a fixed dimensional vector with the
learned embedding function ψ(xi). The instruc-
tion representation x̄ is computed by applying the
LSTM recurrence to generate a sequence of hid-
den states li = LSTM(ψ(xi), li−1), and comput-
ing the mean x̄ = 1n

∑n
i=1 li (Narasimhan et al.,

2015). The current image Ij and previous im-
ages Ij−1,. . . ,Ij−K are concatenated along the
channel dimension and embedded with a convolu-
tional neural network (CNN) to generate the vi-

2We use bold-face capital letters for matrices and bold-
face lowercase letters for vectors. Computed input and state
representations use bold versions of the symbols. For exam-
ple, x̄ is the computed representation of an instruction x̄.

sual state v (Mnih et al., 2013). The last ac-
tion aj−1 is embedded with the functionψa(aj−1).
The vectors vj , x̄, and ψa(aj−1) are concatenated
to create the agent context vector representation
s̃j = [vj , x̄, ψa(aj−1)].

To compute the action to execute, we use a feed-
forward perceptron that decomposes according to
the domain actions. This computation selects the
next action conditioned on the instruction text and
observations from both the current world state and
recent history. In the block world domain, where
actions decompose to selecting the block to move
and the direction, the network computes block and
direction probabilities. Formally, we decompose
an action a to direction aD and block aB . We com-
pute the feedforward network:

h1 = max(W(1)s̃j + b
(1), 0)

hD = W(D)h1 + b(D)

hB = W(B)h1 + b(B) ,

and the action probability is a product of the com-
ponent probabilities:

P (aDj = d | x̄, sj , aj−1) ∝ exp(hDd )
P (aBj = b | x̄, sj , aj−1) ∝ exp(hBb ) .

At the beginning of execution, the first action a0
is set to the special value NONE, and previous im-
ages are zero matrices. The embedding function ψ
is a learned matrix. The function ψa concatenates
the embeddings of aDj−1 and a

B
j−1, which are ob-

tained from learned matrices, to compute the em-
bedding of aj−1. The model parameters θ include
W(1), b(1), W(D), b(D), W(B), b(B), the param-
eters of the LSTM recurrence, the parameters of
the convolutional network CNN, and the embed-
ding matrices. In our experiments (Section 7), all
parameters are learned without external resources.

5 Learning
We use policy gradient for reinforcement learn-
ing (Williams, 1992) to estimate the parameters
θ of the agent policy. We assume access to a

1007



training set of N examples {(x̄(i), s(i)1 , ē(i))}Ni=1,
where x̄(i) is an instruction, s(i)1 is a start state, and
ē(i) is an execution demonstration starting from
s
(i)
1 of instruction x̄

(i). The main learning chal-
lenge is learning how to execute instructions given
raw visual input from relatively limited data. We
learn in a contextual bandit setting, which provides
theoretical advantages over general reinforcement
learning. In Section 8, we verify this empirically.
Reward Function The instruction execution
problem defines a simple problem reward to mea-
sure task completion. The agent receives a posi-
tive reward when the task is completed, a negative
reward for incorrect completion (i.e., STOP in the
wrong state) and actions that fail to execute (e.g.,
when the direction is blocked), and a small penalty
otherwise, which induces a preference for shorter
trajectories. To compute the reward, we assume
access to the world state. This learning setup is
inspired by work in robotics, where it is achieved
by instrumenting the training environment (Sec-
tion 3). The agent, on the other hand, only uses
the agent context (Section 4). When deployed, the
system relies on visual observations and natural
language instructions only. The reward function
R(i) : S ×A → R is defined for each training ex-
ample (x̄(i), s(i)1 , ē

(i)), i = 1 . . . N :

R(i)(s, a) =


1.0 if s = sm(i) and a = STOP
−1.0 s 6= sm(i) and a = STOP
−1.0 a fails to execute
−δ else

,

where m(i) is the length of ē(i).
The reward function does not provide interme-

diate positive feedback to the agent for actions that
bring it closer to its goal. When the agent explores
randomly early during learning, it is unlikely to
encounter the goal state due to the large number
of steps required to execute tasks. As a result, the
agent does not observe positive reward and fails
to learn. In Section 6, we describe how reward
shaping, a method to augment the reward with ad-
ditional information, is used to take advantage of
the training data and address this challenge.
Policy Gradient Objective We adapt the policy
gradient objective defined by Sutton et al. (1999)
to multiple starting states and reward functions:

J = 1
N

N∑
i=1

V (i)π (s
(i)
1 ) ,

where V (i)π (s
(i)
1 ) is the value given by R

(i) start-
ing from s(i)1 under the policy π. The summation
expresses the goal of learning a behavior parame-

terized by natural language instructions.
Contextual Bandit Setting In contrast to most
policy gradient approaches, we apply the objec-
tive to a contextual bandit setting where immedi-
ate reward is optimized rather than total expected
reward. The primary theoretical advantage of con-
textual bandits is much tighter sample complexity
bounds when comparing upper bounds for contex-
tual bandits (Langford and Zhang, 2007) even with
an adversarial sequence of contexts (Auer et al.,
2002) to lower bounds (Krishnamurthy et al.,
2016) or upper bounds (Kearns et al., 1999) for
total reward maximization. This property is par-
ticularly suitable for the few-sample regime com-
mon in natural language problems. While re-
inforcement learning with neural network poli-
cies is known to require large amounts of train-
ing data (Mnih et al., 2015), the limited number
of training sentences constrains the diversity and
volume of agent contexts we can observe during
training. Empirically, this translates to poor results
when optimizing the total reward (REINFORCE
baseline in Section 8). To derive the approximate
gradient, we use the likelihood ratio method:

∇θJ = 1
N

N∑
i=1

E[∇θ log π(s̃, a)R(i)(s, a)] ,

where reward is computed from the world state but
policy is learned on the agent context. We approx-
imate the gradient using sampling.

This training regime, where immediate reward
optimization is sufficient to optimize policy pa-
rameters θ, is enabled by the shaped reward we
introduce in Section 6. While the objective is de-
signed to work best with the shaped reward, the al-
gorithm remains the same for any choice of reward
definition including the original problem reward or
several possibilities formed by reward shaping.
Entropy Penalty We observe that early in train-
ing, the agent is overwhelmed with negative re-
ward and rarely completes the task. This results in
the policy π rapidly converging towards a subopti-
mal deterministic policy with an entropy of 0. To
delay premature convergence we add an entropy
term to the objective (Williams and Peng, 1991;
Mnih et al., 2016). The entropy term encourages a
uniform distribution policy, and in practice stimu-
lates exploration early during training. The regu-
larized gradient is:
∇θJ =

1

N

N∑
i=1

E[∇θ log π(s̃, a)R(i)(s, a) + λ∇θH(π(s̃, ·))] ,

1008



Algorithm 1 Policy gradient learning

Input: Training set {(x̄(i), s(i)1 , ē(i))}Ni=1, learning rate µ,
epochs T , horizon J , and entropy regularization term λ.

Definitions: IMG(s) is a camera sensor that reports an RGB
image of state s. π is a probabilistic neural network
policy parameterized by θ, as described in Section 4.
EXECUTE(s, a) executes the action a at the state s, and
returns the new state. R(i) is the reward function for
example i. ADAM(∆) applies a per-feature learning rate
to the gradient ∆ (Kingma and Ba, 2014).

Output: Policy parameters θ.
1: » Iterate over the training data.
2: for t = 1 to T , i = 1 to N do
3: I1−K , . . . , I0 = ~0
4: a0 = NONE, s1 = s

(i)
1

5: j = 1
6: » Rollout up to episode limit.
7: while j ≤ J and aj 6= STOP do
8: » Observe world and construct agent context.
9: Ij = IMG(sj)

10: s̃j = (x̄(i), Ij , Ij−1, . . . , Ij−K , adj−1)
11: » Sample an action from the policy.
12: aj ∼ π(s̃j , a)
13: sj+1 = EXECUTE(sj , aj)
14: » Compute the approximate gradient.
15: ∆j ← ∇θ log π(s̃j , aj)R(i)(sj , aj)

+λ∇θH(π(s̃j , ·))
16: j+ = 1
17: θ ← θ + µADAM( 1

j

∑j
j′=1 ∆j′)

18: return θ

where H(π(s̃, ·)) is the entropy of π given the
agent context s̃, λ is a hyperparameter that con-
trols the strength of the regularization. While
the entropy term delays premature convergence, it
does not eliminate it. Similar issues are observed
for vanilla policy gradient (Mnih et al., 2016).
Algorithm Algorithm 1 shows our learning al-
gorithm. We iterate over the data T times. In each
epoch, for each training example (x̄(i), s(i)1 , ē

(i)),
i = 1 . . . N , we perform a rollout using our policy
to generate an execution (lines 7 - 16). The length
of the rollout is bound by J , but may be shorter if
the agent selected the STOP action. At each step
j, the agent updates the agent context s̃j (lines 9 -
10), samples an action from the policy π (line 12),
and executes it to generate the new world state
sj+1 (line 13). The gradient is approximated us-
ing the sampled action with the computed reward
R(i)(sj , aj) (line 15). Following each rollout, we
update the parameters θ with the mean of the gra-
dients using ADAM (Kingma and Ba, 2014).

6 Reward Shaping
Reward shaping is a method for transforming a
reward function by adding a shaping term to the

Low

High

Figure 3: Visualization of the shaping potentials for
two tasks. We show demonstrations (blue arrows), but
omit instructions. To visualize the potentials intensity,
we assume only the target block can be moved, while
rewards and potentials are computed for any block
movement. We illustrate the sparse problem reward
(left column) as a potential function and consider only
its positive component, which is focused on the goal.
The middle column adds the distance-based potential.
The right adds both potentials.

problem reward. The goal is to generate more in-
formative updates by adding information to the re-
ward. We use this method to leverage the train-
ing demonstrations, a common form of supervi-
sion for training systems that map language to ac-
tions. Reward shaping allows us to fully use this
type of supervision in a reinforcement learning
framework, and effectively combine learning from
demonstrations and exploration.

Adding an arbitrary shaping term can change
the optimality of policies and modify the orig-
inal problem, for example by making bad poli-
cies according to the problem reward optimal ac-
cording to the shaped function.3 Ng et al. (1999)
and Wiewiora et al. (2003) outline potential-based
terms that realize sufficient conditions for safe
shaping.4 Adding a shaping term is safe if the
order of policies according to the shaped reward
is identical to the order according to the original
problem reward. While safe shaping only applies
to optimizing the total reward, we show empiri-
cally the effectiveness of the safe shaping terms
we design in a contextual bandit setting.

We introduce two shaping terms. The final
shaped reward is a sum of them and the problem
reward. Similar to the problem reward, we define
example-specific shaping terms. We modify the
reward function signature as required.
Distance-based Shaping (F1) The first shaping
term measures if the agent moved closer to the
goal state. We design it to be a safe potential-based

3For example, adding a shaping term F = −R will result
in a shaped reward that is always 0, and any policy will be
trivially optimal with respect to it.

4For convenience, we briefly overview the theorems of Ng
et al. (1999) and Wiewiora et al. (2003) in Appendix A.

1009



term (Ng et al., 1999):
F

(i)
1 (sj , aj , sj+1) = φ

(i)
1 (sj+1)− φ(i)1 (sj) .

The potential φ(i)1 (s) is proportional to the nega-
tive distance from the goal state s(i)g . Formally,
φ

(i)
1 (s) = −η‖s− s(i)g ‖, where η is a constant

scaling factor, and ‖.‖ is a distance metric. In the
block world, the distance between two states is the
sum of the Euclidean distances between the posi-
tions of each block in the two states, and η is the
inverse of block width. The middle column in Fig-
ure 3 visualizes the potential φ(i)1 .
Trajectory-based Shaping (F2) Distance-
based shaping may lead the agent to sub-optimal
states, for example when an obstacle blocks the
direct path to the goal state, and the agent must
temporarily increase its distance from the goal to
bypass it. We incorporate complete trajectories
by using a simplification of the shaping term
introduced by Brys et al. (2015). Unlike F1, it
requires access to the previous state and action.
It is based on the look-back advice shaping
term of Wiewiora et al. (2003), who introduced
safe potential-based shaping that considers the
previous state and action. The second term is:
F

(i)
2 (sj−1, aj−1, sj , aj) = φ

(i)
2 (sj , aj)−φ(i)2 (sj−1, aj−1) .

Given ē(i) = 〈(s1, a1), . . . , (sm, am)〉, to com-
pute the potential φ(i)2 (s, a), we identify the closest
state sj in ē(i) to s. If η‖sj − s‖ < 1 and aj = a,
φ

(i)
2 (s, a) = 1.0, else φ

(i)
2 (s, a) = −δf , where δf

is a penalty parameter. We use the same distance
computation and parameter η as in F1. When the
agent is in a state close to a demonstration state,
this term encourages taking the action taken in the
related demonstration state. The right column in
Figure 3 visualizes the effect of the potential φ(i)2 .

7 Experimental Setup
Environment We use the environment of Bisk
et al. (2016). The original task required predicting
the source and target positions for a single block
given an instruction. In contrast, we address the
task of moving blocks on the plane to execute in-
structions given visual input. This requires gen-
erating the complete sequence of actions needed
to complete the instruction. The environment con-
tains up to 20 blocks marked with logos or digits.
Each block can be moved in four directions. In-
cluding the STOP action, in each step, the agent
selects between 81 actions. The set of actions is
constant and is not limited to the blocks present.

The transition function is deterministic. The size
of each block step is 0.04 of the board size. The
agent observes the board from above. We adopt
a relatively challenging setup with a large action
space. While a simpler setup, for example decom-
posing the problem to source and target prediction
and using a planner, is likely to perform better, we
aim to minimize task-specific assumptions and en-
gineering of separate modules. However, to better
understand the problem, we also report results for
the decomposed task with a planner.

Data Bisk et al. (2016) collected a corpus of in-
structions paired with start and goal states. Fig-
ure 1 shows example instructions. The original
data includes instructions for moving one block or
multiple blocks. Single-block instructions are rel-
atively similar to navigation instructions and re-
ferring expressions. While they present much of
the complexity of natural language understanding
and grounding, they rarely display the planning
complexity of multi-block instructions, which are
beyond the scope of this paper. Furthermore,
the original data does not include demonstrations.
While generating demonstrations for moving a
single block is straightforward, disambiguating
action ordering when multiple blocks are moved is
challenging. Therefore, we focus on instructions
where a single block changes its position between
the start and goal states, and restrict demonstra-
tion generation to move the changed block. The
remaining data, and the complexity it introduces,
provide an important direction for future work.

To create demonstrations, we compute the
shortest paths. While this process may introduce
noise for instructions that specify specific trajecto-
ries (e.g., move SRI two steps north and . . . ) rather
than only describing the goal state, analysis of the
data shows this issue is limited. Out of 100 sam-
pled instructions, 92 describe the goal state rather
than the trajectory. A secondary source of noise is
due to discretization of the state space. As a re-
sult, the agent often can not reach the exact target
position. The demonstrations error illustrates this
problem (Table 3). To provide task completion re-
ward during learning, we relax the state compari-
son, and consider states to be equal if the sum of
block distances is under the size of one block.

The corpus includes 11,871/1,719/3,177 in-
structions for training/development/testing. Ta-
ble 1 shows corpus statistic compared to the com-
monly used SAIL navigation corpus (MacMahon

1010



SAIL Blocks
Number of instructions 3,237 16,767
Mean instruction length 7.96 15.27

Vocabulary 563 1,426
Mean trajectory length 3.12 15.4

Table 1: Corpus statistics for the block environment we
use and the SAIL navigation domain.

et al., 2006; Chen and Mooney, 2011). While the
SAIL agent only observes its immediate surround-
ings, overall the blocks domain provides more
complex instructions. Furthermore, the SAIL en-
vironment includes only 400 states, which is in-
sufficient for generalization with vision input. We
compare to other data sets in Appendix D.
Evaluation We evaluate task completion error
as the sum of Euclidean distances for each block
between its position at the end of the execution
and in the gold goal state. We divide distances
by block size to normalize for the image size. In
contrast, Bisk et al. (2016) evaluate the selection
of the source and target positions independently.
Systems We report performance of ablations,
the upper bound of following the demonstrations
(Demonstrations), and five baselines: (a) STOP:
the agent immediately stops, (b) RANDOM: the
agent takes random actions, (c) SUPERVISED: su-
pervised learning with maximum-likelihood es-
timate using demonstration state-action pairs,
(d) DQN: deep Q-learning with both shaping
terms (Mnih et al., 2015), and (e) REINFORCE:
policy gradient with cumulative episodic reward
with both shaping terms (Sutton et al., 1999). Full
system details are given in Appendix B.
Parameters and Initialization Full details are
in Appendix C. We consider K = 4 previous im-
ages, and horizon length J = 40. We initialize our
model with the SUPERVISED model.

8 Results
Table 2 shows development results. We run each
experiment three times and report the best result.
The RANDOM and STOP baselines illustrate the
task complexity of the task. Our approach, includ-
ing both shaping terms in a contextual bandit set-
ting, significantly outperforms the other methods.
SUPERVISED learning demonstrates lower perfor-
mance. A likely explanation is test-time execution
errors leading to unfamiliar states with poor later
performance (Kakade and Langford, 2002), a form
of the covariate shift problem. The low perfor-
mance of REINFORCE and DQN illustrates the
challenge of general reinforcement learning with
limited data due to relatively high sample com-

Algorithm Distance Error Min. DistanceMean Med. Mean Med.
Demonstrations 0.35 0.30 0.35 0.30
Baselines
STOP 5.95 5.71 5.95 5.71
RANDOM 15.3 15.70 5.92 5.70
SUPERVISED 4.65 4.45 3.72 3.26
REINFORCE 5.57 5.29 4.50 4.25
DQN 6.04 5.78 5.63 5.49
Our Approach 3.60 3.09 2.72 2.21

w/o Sup. Init 3.78 3.13 2.79 2.21
w/o Prev. Action 3.95 3.44 3.20 2.56
w/o F1 4.33 3.74 3.29 2.64
w/o F2 3.74 3.11 3.13 2.49
w/ Distance 8.36 7.82 5.91 5.70

Reward
Ensembles
SUPERVISED 4.64 4.27 3.69 3.22
REINFORCE 5.28 5.23 4.75 4.67
DQN 5.85 5.59 5.60 5.46
Our Approach 3.59 3.03 2.63 2.15

Table 2: Mean and median (Med.) development results.

Algorithm Distance Error Min. DistanceMean Med. Mean Med.
Demonstrations 0.37 0.31 0.37 0.31
STOP 6.23 6.12 6.23 6.12
RANDOM 15.11 15.35 6.21 6.09
Ensembles
SUPERVISED 4.95 4.53 3.82 3.33
REINFORCE 5.69 5.57 5.11 4.99
DQN 6.15 5.97 5.86 5.77
Our Approach 3.78 3.14 2.83 2.07

Table 3: Mean and median (Med.) test results.

plexity (Kearns et al., 1999; Krishnamurthy et al.,
2016). We also report results using ensembles of
the three models.

We ablate different parts of our approach. Ab-
lations of supervised initialization (our approach
w/o sup. init) or the previous action (our ap-
proach w/o prev. action) result in increase in er-
ror. While the contribution of initialization is mod-
est, it provides faster learning. On average, af-
ter two epochs, we observe an error of 3.94 with
initialization and 6.01 without. We hypothesize
that the F2 shaping term, which uses full demon-
strations, helps to narrow the gap at the end of
learning. Without supervised initialization and F2,
the error increases to 5.45 (the 0% point in Fig-
ure 4). We observe the contribution of each shap-
ing term and their combination. To study the bene-
fit of potential-based shaping, we experiment with
a negative distance-to-goal reward. This reward
replaces the problem reward and encourages get-
ting closer to the goal (our approach w/distance
reward). With this reward, learning fails to con-
verge, leading to a relatively high error.

Figure 4 shows our approach with varying

1011



0 10 20 30 40 50 60 70 80 90 100
3.5

4

4.5

5

5.5

% Demonstrations

M
ea

n
E

rr
or

Figure 4: Mean distance error as a function of the ratio
of training examples that include complete trajectories.
The rest of the data includes the goal state only.

amount of supervision. We remove demonstra-
tions from both supervised initialization and the
F2 shaping term. For example, when only 25%
are available, only 25% of the data is available for
initialization and the F2 term is only present for
this part of the data. While some demonstrations
are necessary for effective learning, we get most
of the benefit with only 12.5%.

Table 3 provides test results, using the ensem-
bles to decrease the risk of overfitting the develop-
ment. We observe similar trends to development
result with our approach outperforming all base-
lines. The remaining gap to the demonstrations
upper bound illustrates the need for future work.

To understand performance better, we measure
minimal distance (min. distance in Tables 2 and
3), the closest the agent got to the goal. We ob-
serve a strong trend: the agent often gets close to
the goal and fails to stop. This behavior is also
reflected in the number of steps the agent takes.
While the mean number of steps in development
demonstrations is 15.2, the agent generates on av-
erage 28.7 steps, and 55.2% of the time it takes
the maximum number of allowed steps (40). Test-
ing on the training data shows an average 21.75
steps and exhausts the number of steps 29.3% of
the time. The mean number of steps in training
demonstrations is 15.5. This illustrates the chal-
lenge of learning how to be behave at an absorbing
state, which is observed relatively rarely during
training. This behavior also shows in our video.5

We also evaluate a supervised learning variant
that assumes a perfect planner.6 This setup is sim-
ilar to Bisk et al. (2016), except using raw image
input. It allows us to roughly understand how well
the agent generates actions. We observe a mean
error of 2.78 on the development set, an improve-
ment of almost two points over supervised learn-
ing with our approach. This illustrates the com-

5https://github.com/clic-lab/blocks
6As there is no sequence of decisions, our reinforcement

approach is not appropriate for the planner experiment. The
architecture details are described in Appendix B.

plexity of the complete problem.
We conduct a shallow linguistic analysis to un-

derstand the agent behavior with regard to dif-
ferences in the language input. As expected, the
agent is sensitive to unknown words. For instruc-
tions without unknown words, the mean develop-
ment error is 3.49. It increases to 3.97 for instruc-
tions with a single unknown word, and to 4.19 for
two.7 We also study the agent behavior when ob-
serving new phrases composed of known words by
looking at instructions with new n-grams and no
unknown words. We observe no significant corre-
lation between performance and new bi-grams and
tri-grams. We also see no meaningful correlation
between instruction length and performance. Al-
though counterintuitive given the linguistic com-
plexities of longer instructions, it aligns with re-
sults in machine translation (Luong et al., 2015).

9 Conclusions
We study the problem of learning to execute in-
structions in a situated environment given only
raw visual observations. Supervised approaches
do not explore adequately to handle test time er-
rors, and reinforcement learning approaches re-
quire a large number of samples for good conver-
gence. Our solution provides an effective combi-
nation of both approaches: reward shaping to cre-
ate relatively stable optimization in a contextual
bandit setting, which takes advantage of a signal
similar to supervised learning, with a reinforce-
ment basis that admits substantial exploration and
easy avenues for smart initialization. This com-
bination is designed for a few-samples regime, as
we address. When the number of samples is un-
bounded, the drawbacks observed in this scenario
for optimizing longer term reward do not hold.

Acknowledgments
This research was supported by a Google Fac-
ulty Award, an Amazon Web Services Research
Grant, and a Schmidt Sciences Research Award.
We thank Alane Suhr, Luke Zettlemoyer, and the
anonymous reviewers for their helpful feedback,
and Claudia Yan for technical help. We also
thank the Cornell NLP group and the Microsoft
Research Machine Learning NYC group for their
support and insightful comments.

7This trend continues, although the number of instructions
is too low (< 20) to be reliable.

1012



References
Alekh Agarwal, Daniel J. Hsu, Satyen Kale, John

Langford, Lihong Li, and Robert E. Schapire. 2014.
Taming the monster: A fast and simple algorithm
for contextual bandits. In Proceedings of the Inter-
national Conference on Machine Learning.

Jacob Andreas and Dan Klein. 2015. Alignment-
based compositional semantics for instruction fol-
lowing. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing. https://doi.org/10.18653/v1/D15-1138.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016a. Learning to compose neu-
ral networks for question answering. In Proceed-
ings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
https://doi.org/10.18653/v1/N16-1181.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016b. Neural module networks. In
Conference on Computer Vision and Pattern Recog-
nition.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual question an-
swering. In International Journal of Computer Vi-
sion.

Yoav Artzi, Dipanjan Das, and Slav Petrov. 2014a.
Learning compact lexicons for CCG semantic pars-
ing. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing.
https://doi.org/10.3115/v1/D14-1134.

Yoav Artzi, Maxwell Forbes, Kenton Lee, and Maya
Cakmak. 2014b. Programming by demonstration
with situated semantic parsing. In AAAI Fall Sym-
posium Series.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly
supervised learning of semantic parsers for map-
ping instructions to actions. Transactions of the
Association of Computational Linguistics 1:49–62.
http://aclweb.org/anthology/Q13-1005.

Peter Auer, Nicolò Cesa-Bianchi, Yoav Freund, and
Robert E. Schapire. 2002. The nonstochastic multi-
armed bandit problem. SIAM J. Comput. 32(1):48–
77.

Yonatan Bisk, Deniz Yuret, and Daniel Marcu. 2016.
Natural language communication with robots. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
https://doi.org/10.18653/v1/N16-1089.

S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of

the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP.
http://aclweb.org/anthology/P09-1010.

S.R.K. Branavan, Luke Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines:
Learning to map high-level instructions to com-
mands. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics. http://aclweb.org/anthology/P10-1129.

Tim Brys, Anna Harutyunyan, Halit Bener Suay, Sonia
Chernova, Matthew E. Taylor, and Ann Nowé. 2015.
Reinforcement learning from demonstration through
shaping. In Proceedings of the International Joint
Conference on Artificial Intelligence.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artificial Intelligence.

Wenhu Chen, Aurélien Lucchi, and Thomas Hofmann.
2016. Bootstrap, review, decode: Using out-of-
domain textual data to improve image captioning.
CoRR abs/1611.05321.

Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C. Lawrence Zitnick. 2015. Microsoft COCO cap-
tions: Data collection and evaluation server. CoRR
abs/1504.00325.

Kevin Clark and D. Christopher Manning. 2016. Deep
reinforcement learning for mention-ranking coref-
erence models. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing. http://aclweb.org/anthology/D16-1245.

Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science 14:179–211.

Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao,
Lihong Li, Li Deng, and Mari Ostendorf. 2016.
Deep reinforcement learning with a natural lan-
guage action space. In Proceedings of the 54th
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).
https://doi.org/10.18653/v1/P16-1153.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C. Lawrence Zitnick, and
Ross B. Girshick. 2016. CLEVR: A diagnostic
dataset for compositional language and elementary
visual reasoning. CoRR abs/1612.06890.

Sham Kakade and John Langford. 2002. Approxi-
mately optimal approximate reinforcement learning.
In Machine Learning, Proceedings of the Nineteenth
International Conference (ICML 2002), University
of New South Wales, Sydney, Australia, July 8-12,
2002.

1013



Michael Kearns, Yishay Mansour, and Andrew Y. Ng.
1999. A sparse sampling algorithm for near-optimal
planning in large markov decision processes. In
Proeceediings of the International Joint Conference
on Artificial Intelligence.

Joohyun Kim and Raymond Mooney. 2012. Unsu-
pervised PCFG induction for grounded language
learning with highly ambiguous supervision. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning.
http://aclweb.org/anthology/D12-1040.

Joohyun Kim and Raymond Mooney. 2013. Adapt-
ing discriminative reranking to grounded lan-
guage learning. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).
http://aclweb.org/anthology/P13-1022.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Jens Kober, J. Andrew Bagnell, and Jan Peters. 2013.
Reinforcement learning in robotics: A survey. In-
ternational Journal of Robotics Research 32:1238–
1274.

Akshay Krishnamurthy, Alekh Agarwal, and John
Langford. 2016. PAC reinforcement learning with
rich observations. In Advances in Neural Informa-
tion Processing Systems.

John Langford and Tong Zhang. 2007. The epoch-
greedy algorithm for multi-armed bandits with side
information. In Advances in Neural Information
Processing Systems 20, Proceedings of the Twenty-
First Annual Conference on Neural Information
Processing Systems, Vancouver, British Columbia,
Canada, December 3-6, 2007.

Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter
Abbeel. 2016. End-to-end training of deep visuo-
motor policies. Journal of Machine Learning Re-
search 17.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep re-
inforcement learning for dialogue generation. In
Proceedings of the 2016 Conference on Empir-
ical Methods in Natural Language Processing.
http://aclweb.org/anthology/D16-1127.

Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to
attention-based neural machine translation. In
Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
http://aclweb.org/anthology/D15-1166.

Matthew MacMahon, Brian Stankiewics, and Ben-
jamin Kuipers. 2006. Walk the talk: Connecting

language, knowledge, action in route instructions.
In Proceedings of the National Conference on Ar-
tificial Intelligence.

Cynthia Matuszek, Dieter Fox, and Karl Koscher.
2010. Following directions using statistical machine
translation. In Proceedings of the international con-
ference on Human-robot interaction.

Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer,
and Dieter Fox. 2012. Learning to parse natural lan-
guage commands to a robot control system. In Pro-
ceedings of the International Symposium on Experi-
mental Robotics.

Hongyuan Mei, Mohit Bansal, and R. Matthew Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-fine alignment. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
https://doi.org/10.18653/v1/N16-1086.

Dipendra K. Misra, Jaeyong Sung, Kevin Lee, and
Ashutosh Saxena. 2016. Tell me dave: Context-
sensitive grounding of natural language to manip-
ulation instructions. The International Journal of
Robotics Research 35(1-3):281–300.

Kumar Dipendra Misra, Kejia Tao, Percy Liang, and
Ashutosh Saxena. 2015. Environment-driven lexi-
con induction for high-level instructions. In Pro-
ceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the
7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers).
https://doi.org/10.3115/v1/P15-1096.

Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi
Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu.
2016. Asynchronous methods for deep reinforce-
ment learning. In Proceedings of the International
Conference on Machine Learning.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin A. Riedmiller. 2013. Playing atari with
deep reinforcement learning. In Advances in Neural
Information Processing Systems.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, and Georg Ostrovski. 2015. Human-level con-
trol through deep reinforcement learning. Nature
518(7540).

Karthik Narasimhan, Tejas Kulkarni, and Regina
Barzilay. 2015. Language understanding for text-
based games using deep reinforcement learning.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing.
https://doi.org/10.18653/v1/D15-1001.

1014



Karthik Narasimhan, Adam Yala, and Regina Barzi-
lay. 2016. Improving information extraction by ac-
quiring external evidence with reinforcement learn-
ing. In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Processing.
http://aclweb.org/anthology/D16-1261.

Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.
1999. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In
Proceedings of the International Conference on Ma-
chine Learning.

Junhyuk Oh, Valliappa Chockalingam, Satinder P.
Singh, and Honglak Lee. 2016. Control of mem-
ory, active perception, and action in minecraft. In
Proceedings of the International Conference on Ma-
chine Learning.

Rohan Paul, Jacob Arkin, Nicholas Roy, and
Thomas M. Howard. 2016. Efficient grounding of
abstract spatial concepts for natural language inter-
action with robot manipulators. In Robotics: Sci-
ence and Systems.

Andrei A. Rusu, Matej Vecerik, Thomas Rothörl, Nico-
las Heess, Razvan Pascanu, and Raia Hadsell. 2016.
Sim-to-real robot learning from pixels with progres-
sive nets. CoRR .

John Schulman, Sergey Levine, Philipp Moritz,
Michael I. Jordan, and Pieter Abbeel. 2015. Trust
region policy optimization .

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George van den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, Sander Dieleman, Do-
minik Grewe, John Nham, Nal Kalchbrenner, Ilya
Sutskever, Timothy Lillicrap, Madeleine Leach, Ko-
ray Kavukcuoglu, Thore Graepel, and Demis Hass-
abis. 2016. Mastering the game of go with deep neu-
ral networks and tree search. Nature 529 7587:484–
9.

Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.
2017. A corpus of compositional language for vi-
sual reasoning. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.

Jaeyong Sung, Seok Hyun Jin, and Ashutosh Sax-
ena. 2015. Robobarista: Object part based trans-
fer of manipulation trajectories from crowd-sourcing
in 3d pointclouds. In International Symposium on
Robotics Research.

Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement learning: An introduction. IEEE Trans.
Neural Networks 9:1054–1054.

Richard S. Sutton, David A. McAllester, Satinder P.
Singh, and Yishay Mansour. 1999. Policy gradi-
ent methods for reinforcement learning with func-
tion approximation. In Advances in Neural Infor-
mation Processing Systems.

Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew Walter, Ashis G. Banerjee, Seth Teller, and
Nicholas Roy. 2011. Understanding natural lan-
guage commands for robotic navigation and mobile
manipulation. In Proceedings of the National Con-
ference on Artificial Intelligence.

Adam Vogel and Daniel Jurafsky. 2010. Learn-
ing to follow navigational directions. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics.
http://aclweb.org/anthology/P10-1083.

Bonnie Webber, Norman Badler, Barbara Di Euge-
nio, Christopher Geib, Libby Levison, and Michael
Moore. 1995. Instructions, intentions and expecta-
tions. Artificial Intelligence 73(1):253–269.

Eric Wiewiora, Garrison W. Cottrell, and Charles
Elkan. 2003. Principled methods for advising re-
inforcement learning agents. In Proceedings of the
International Conference on Machine Learning.

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning 8.

Ronald J Williams and Jing Peng. 1991. Function opti-
mization using connectionist reinforcement learning
algorithms. Connection Science 3(3):241–268.

Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive Psychology 3(1):1–191.

Kelvin Xu, Jimmy Ba, Jamie Ryan Kiros, Kyunghyun
Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation
with visual attention. In Proceedings of the Inter-
national Conference on Machine Learning.

Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J.
Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi.
2017. Target-driven visual navigation in indoor
scenes using deep reinforcement learning.

1015


