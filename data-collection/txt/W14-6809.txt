



















































Segment-based Fine-grained Emotion Detection for Chinese Text


Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 52–60,
Wuhan, China, 20-21 October 2014

Segment-based Fine-grained Emotion Detection for Chinese Text

Odbal
Dept. Of Automation, University of
Science and Technology of China
Institute of Intelligent Machines,
Chinese Academy of Sciences

wdbl@iim.ac.cn

Zengfu Wang
Dept. Of Automation, University of
Science and Technology of China
Institute of Intelligent Machines,
Chinese Academy of Sciences
zfwang@ustc.edu.cn

Abstract

Emotion detection has been extensively
studied in recent years. Current base-
line methods often use token-based fea-
tures which cannot properly capture more
complex linguistic phenomena and emo-
tional composition in fine grained emotion
detection. A novel supervised learning
approach―segment-based fine-grained e-
motion detection model for Chinese text
has been proposed in this paper. Differ-
ent from most existing methods, the pro-
posed model applies the hierarchical struc-
ture of sentence (e.g., dependency rela-
tionship) and exploits segment-based fea-
tures. Furthermore, the emotional compo-
sition in short text is addressed by using
the log linear model. We perform emotion
detection on our dataset: news contents,
fairly tales, and blog dataset, and compare
our proposed method to representative ex-
isting approaches. The experimental re-
sults demonstrate the effectiveness of the
proposed segment-based model.

1 Introduction

Emotion detection aims to identify fine-grained
emotion categories (e.g., happy, angry, disgust,
fear, sadness and surprise) of a given text, and it
is a challenging and difficult problem with appli-
cations throughout natural language processing.

Currently, the most widely used probability
models for emotion classification are supervised
based machine learning algorithms, such as Naive
Bayes (NB) and Support Vector Machine (SVM)
etc,. Researchers have trained the classifier de-
pends on corpus-based features, mainly unigrams,
combined with lexical features (Alm et al, 2005;
Aman and Szpakowicz, 2007; Katz, et al, 2007).
Nevertheless, these methods used in the emotion

classification system concentrate on token based
features and do not include any linguistic or con-
textual information, which often yields poor per-
formance. Therefore, recent studies have investi-
gated the approach using contextual information
around emotional words to identify fine grained e-
motion classes. (Das and Bandyopadhyay, 2010)
observe that the emotion word, POS, intensifier
and direct dependency features play an importan-
t role in extracting emotional expressions as well
as tagging sentences with emotions and intensi-
ties. (Diman Ghazi et al., 2012) propose an ap-
proach which takes the contextual emotion of a
word and the syntactic structure of the sentence in-
to account to classify sentences by emotion class-
es. However, these works still use token-based
features, which cannot address the problem of the
emotional composition, especially those that are
the expression-level representations.

There has been previous work using composi-
tion rules and statistical methods to handle senti-
ment composition. (Moilanen and Pulman, 2007)
propose a theoretical composition model, and e-
valuate a lexical dependency parsing post-process
implementation, which treat both negation and
intensifier via three models: sentiment propaga-
tion, polarity conflict resolution and polarity re-
versal. (Choi and Cardie, 2008) incorporate struc-
tural inference motivated by compositional seman-
tics into the learning procedure for subsentential
sentiment analysis. (Socher et al., 2011, 2012)
present matrix-vector representations with a recur-
sive neural network. The model is built on a parse
tree where the nodes are associated to a vector.
The matrix captures how each constituent modi-
fies its neighbor. (Baptiste Chardon et al,. 2013)
propose a computational model that accounts for
the effects of negation and modality on opinion ex-
pressions. However, it is not as clear how to use a
compositional treatment to classify fine grained e-
motion classes. Sentiment composition combines

52



individual positive and negative words or phrases,
and the final polarity of a sentence is positive or
negative. Nevertheless, it is more challenging and
difficult to make categorization into distinct emo-
tion classes for the higher level of classification in
emotion recognition task. In order to facilitate our
discussion, consider the following examples:

1.不过在教堂里,站在讲台上的牧师却是大叫
大嚷,非常生气. (But inside the church the pas-
tor stood in the pulpit, and spoke very loudly and
angrily.)[anger]

2.迷信使她的血一会儿变冷,一会儿变
热.(Superstition made her alternately shudder with
cold or burn with the heat of fever.)[fear]

3.骑在桦木条上的那个蜡人忽然变得又
高又大了.他像一阵旋风似地扑向纸花那儿
去,说:”居然把这样的怪想头灌进一个孩子的脑
子里去!全是些没有道理的幻想!”这蜡人跟那
位戴宽帽子的枢密顾问官一模一样,而且他的
那副面孔也是跟顾问官一样发黄和生气.可是
那些纸花在他的瘦腿上打了一下,于是他缩做
一团,又变成了一个渺小的蜡人.(All at once the
wax doll which rode on the carnival rod seemed to
grow larger and taller, and it turned round and said
to the paper flowers, ”How can you put such things
in a child’s head? they are all foolish fancies;” and
then the doll was exactly like the lawyer with the
broad brimmed hat, and looked as yellow and as
cross as he did; but the paper dolls struck him on
his thin legs, and he shrunk up again and became
quite a little wax doll.)[anger]

In the first example, we can use the key word-
s ”大叫”,”大嚷”(spoke very loudly), and ”生
气”(anger), to easily identify the emotion class-
es of the sentence. However, in the second ex-
ample, we cannot use the words ”血”(blood), ”变
冷”(make cold), ”变热”(make burn) or the phrase
”血变冷” and ”血变热” to easily detect the final
emotion category of the sentence. ”血” and ”变
冷” carry ”fear” category, and the words ”血” and
”变热” can be classified as ”joy”, but the final e-
motion label of the sentence is ”fear”. In the last
example, there are four types of emotion classes
for sub-sentential segments, for example, ”蜡人变
得又高又大”( the wax doll seemed to grow larger
and taller)[joy], ”怪想头”(such things) [surprise],
”没有道理的幻想!”(foolish fancies)[anger], ”生
气”(anger) [anger], and ”一个渺小的蜡人”( a lit-
tle wax doll)[sad], but the overall emotion of the
short text is ”anger”.

These examples demonstrate that a sentence

or short text exists several expression-level emo-
tion labels, and the words or constituents inter-
act with each other to yield the overall emotion
label, which cannot be easily resolved by token-
based methods. To solve this problem, we present
segment-based supervised learning approach to in-
vestigate how to recognize the overall emotion tag
of a sentence or short text. Closer to our current
purposes is the work of (Nakagawa et al, 2010).
It employs a conditional random field (CRF) for
sentiment classification of Japanese and English
subjective sentences using dependency tree-based
method. In their method, the sentiment polarity of
each dependency subtree, which is not observable
in training data, is represented by a hidden vari-
able. The polarity of the whole sentence is calcu-
lated in consideration of interactions between the
hidden variables. However, this research doesn’t
work on the fine grained emotion recognition and
it is unable to deal with multiple consecutive to-
kens (e.g., a phrase).

In this paper, we employ semi-Markov con-
ditional random fields (semi-CRFs) for segment-
based emotion detection. Semi-CRFs (Sarawagi
and Cohen, 2004) are more powerful than CRFs
in that they can assign labels to segments instead
of tokens; hence, features can be defined at the
segment level. To our knowledge, segment-based
fine-grained emotion recognition for Chinese text
has not been attempted. Our learning framework
can be determined in a three-step process: (1) seg-
ment the input sentence or short text into some de-
pendency subtrees and then (2) employ the semi-
CRFs with various context informed features to
assess the emotion classes of the constituents of
the segment, and (3) exploit a composition learn-
ing model to combine the segment level emotion
labels. We evaluate the proposed model on our
construction dataset, which consists of news con-
tent, fairy tales and blog dataset, and the experi-
mental results show that segment-based learning
algorithm works well in our experimental data.

2 Related Work

Supervised learning method has been well studied
and used in fine-grained emotion detection with
promising results. (Alm et al., 2005) explores the
text-based emotion prediction problem empirical-
ly, using supervised machine learning. (Das and
Bandyopadhyay, 2010) deals with the extraction
of emotional expressions and tagging of English

53



blog sentences with Ekman’s six basic emotion
tags and any of the three intensities: low, medi-
um and high. Baseline system is developed based
on WordNet Affect lists and dependency relation-
s. SVM based supervised framework is employed
by incorporating different word and context level
features. (Chaffar and Inkpen, 2011) adopts a su-
pervised machine learning approach to recognize
six basic emotions (anger, disgust, fear, happi-
ness, sadness and surprise) using a heterogeneous
emotion-annotated dataset which combines news
headlines, fairy tales and blogs. (Saif Mohammad,
2012) uses word-level affect lexicons to provide
significant improvements in sentence-level emo-
tion classification. (Purver and Battersby, 2012)
describe a set of experiments using automatical-
ly labeled data to train supervised classifiers for
multi-class emotion detection in Twitter messages
with no manual intervention. (Diman Ghazi et al.,
2012) present a method which enables us to take
the contextual emotion of a word and the syntactic
structure of the sentence into account to classify
sentences by emotion classes.

Other related studies on this task are emotion
resource construction. (Xu et al., 2010) adopts a
graph-based algorithm to build Chinese emotion
lexicons for public use. (Patra et al., 2013) us-
es the Potts model for constructing emotion lex-
icon annotated with Ekman’s six basic emotion
classes. There are also studies that analyzed the
deeper level information, such as color-concept-
emotion associations (Volkova et al., 2012); e-
motion causes detection (Chen et al., 2010); and
learning hashtags to improve emotion classifica-
tion performance (Qadir and Riloff, 2013). In sen-
timent composition, the presence of modalities is
generally used to combine the individual positive
and negative word (Moilanen and Pulman, 2007;
Choi and Cardie, 2008; Nakagawa, 2010; Socher
et al., 2011, 2012; Chardon et al,.2013). There is
a few works on the higher level of composition in
emotion recognition task.

Different from above approaches, we use a
segment-based method for the fine-grained emo-
tion detection. To use the strengths of segment-
based features, we propose to employ the semi-
Markov Conditional Random Field, which was
previously used in information extraction to tag
continuous segments of input sequences and out-
performed conventional CRFs in the task of named
entity recognition and opinion extraction (Sarawa-

gi and Cohen, 2004; Okanohara et al., 2006; An-
drew, 2006; Yang and Cardie, 2012). We describe
this model in the following section.

3 Segment-based Emotion Detection
using semi-CRF

In this section, we first introduce the semi-Markov
conditional random field and then elaborate the
proposed segment-based emotion detection mod-
el.

3.1 Semi-CRF
In this subsection we briefly review the semi-
Markov conditional random field. We follow the
definitions in (Sarawagi and Cohen, 2004). Let
s = sm1 =< s1, · · · , sm > denote a segmenta-
tion of an observed sequence x. To represent al-
l the information associated with each segmenta-
tion, we define si as si =< ti, ui, yi >, which
consisting of three components: a start position
ti, an end position ui, and a label yi. We as-
sume that segments have a positive length bound-
ed above by the pre-defined upper bound L (1 ≤
ui − ti + 1 ≤ |x|) and completely cover the
sequence x without overlapping, that is, s satis-
fies t1 = 1, um = |x|, and ti + 1 = ui + 1
for i = 1, ..., m-1. For emotion detection, a
valid segmentation of the sentence ” 善良的姑
娘细心地照顾这只弱小的猫” might be s =<
(1, 3, happy), (4, 6, happy), (7, 11, sad) >, cor-
responding to the label sequence y =<
happy, happy, sad >.

Then, Semi-CRF defines a conditional proba-
bility of a state sequence y given an observed se-
quence x by:

p(y, s|x) = 1
Z(x)

exp(
m∑

i=1

|s|∑
t=1

λifi(x, s, y)) (1)

where fi(x, s, y) = fi(yj−1, yj , x, sj) is a fea-
ture function and Z(x) is the normalization factor
as defined for CRF. The model parameters are a set
of real-valued weights λ = {λj}, each of which
represents the weight of a feature.

Z(x) =
∑
s′

exp(
m∑

i=1

|s|∑
t=1

λifi(x, s′, y)) (2)

The inference problem for semi-CRF can be
solved by using a semi-Markov analog of the usu-
al Viterbi algorithm. An implementation of semi-
CRF is available at http://crf.sourceforge.net.

54



3.2 Segment-based Emotion Detection Model

In this subsection, we will describe our segment-
based emotion detection model (see Figure 1).

Assume that we are given a sequence of obser-
vations x = xJ1 =< x1, · · · , xJ > and we would
like to infer a corresponding label yt, where yt ∈ y
is one of the Ekman’s six basic emotion type-
s such as happiness, sadness, fear, surprise, anger
and disgust. Every emotion class is regarded as
a possible emotion tag for the input sentence or
short text with a posterior probability p(y|x).

Our proposed segment-based approach can
be determined in a three-step process: at first, a
sentence or short text is divided into non-fixed
length segments. We construct segment units
from the dependency parse tree of each sentence,
and then build up possible segment candidates
based on those units. More specifically, the
dependency subtrees that contain the path from
the root node (e.g., core verb 照顾(take care
of)) to leaf node are selected for the candidate
segmentation. For instance, let us consider the
subjective sentence ”善良的姑娘细心地照顾
这只弱小的猫”(Good girl carefully take care
of the small cat). The dependency parse tree of
this sentence is illustrated in Figure 2. We can
select four dependency subtrees (善良的姑娘,照
顾) ( good girl, take care of), (细心地,照顾) (
carefully, take care of), (照顾,这只,猫) ( take
care of, the cat), and (照顾,弱小的猫) ( take care
of, the small cat) as the candidate segmentations.
The reason that the dependency representations
are chosen as the segment unit is, compared
with phrase-structure tree, it can describe more
complicated structure information of a sentence
(such as the long distance dependency relation).
Then, we use the segmentation strings as ob-
servations and supply various context-informed
features as inputs to the semi-CRF to assess the
emotion classes of the segment. That is, instead of
determining y directly from x, we introduce hid-
den variables z = (z1, · · · , zm) as intermediate
decision variables, where zi = (si, yi) and yi ∈ {
happiness, sadness, fear, surprise, anger, disgust,
none }, so that yi represents whether si is a phrase
with happiness, sadness, fear, surprise, anger,
or disgust, or none of the above. In the above
example, we can obtain the emotion label of each
segment y =< happy, happy, happy, sad >. At
last, once we determine the intermediate decision
variables, we use a probabilistic model based

on log linear model to combine expression-level
emotion categories. For simplicity, we decompose
the probability by introducing two probability
distribution models: expression-level emotion
detection model and emotion tag distribution
model. Specifically, for the segment-based emo-
tion detection problem, the discriminate function
can be defined as follows:

p(yt|x) = ∑
s,y

p(sK1 |x) · p(yK1 |sK1 , x) · p(yt|yK1 , sK1 , x)

=
∑
s,y

K∏
k=1

p(yK1 , s
K
1 |x) · p(yt|yK1 )

(3)
There are two probability distributions:
- Expression-level emotion detection model:

p(y, s|x). This model describes the distribution
of the sequence of segmentation si(1 : k) and its
corresponding emotion tag yi(1 : k). This distri-
bution can be calculated directly by the semi-CRF
model.

- Emotion tag distribution model: p(yt|yK1 ).
This model describes the probability distribution
of the emotion classes. Where yK1 is expression-
level emotion tag and yt indicates the overall e-
motion tag. This distribution can be calculated by
similar n-gram model.

In this study, we use the maximum a posteriori
estimation with Gaussian priors for parameter es-
timation. The inference problem can be solved by
the Viterbi algorithm.

Figure 1: Graphical presentation for semi-CRF
segment based model

3.3 Feature Design
We reused features in the original token-based
model based on unigram, POS tags, emotion word
lists and context-informed dependency relations.

Bag-of-words: Surface forms of word unigram-
s and bigrams in the sentence are used as features.

55



Figure 2: A dependency parse tree example. There
are four segment units in the sentence

Part-of-speech: The part-of-speech (POS) of
the current word and the surrounding words are
used as a feature for emotion classification.

Content bag-of-words: N (noun), V (verb), JJ
(adjective) words by POS is used as features.

Emotion word lists: This set of features is
based on the emotion-word itself. The emotion
class of a word can be assigned as the word’s pri-
or emotion tag according to the Chinese emotion
lexicon, which is a translation and extension ver-
sion of WordNet-Affect lexicon and its construc-
tion details described as section 4.1.

Dependency relations: This set of features is
binary indicators of whether the leaf phrase in the
dependency parse tree belongs to one of the emo-
tion classes. The dependencies are all binary re-
lations: a grammatical relation holds between a
governor (head) and a dependent (modifier). De-
pendency arcs are stored as 3-tuples of the form
< w1, r, w2 >, denoting occurrences of words w1
and word w2 related by the syntactic dependency
r.

After parsing the sentence and getting the de-
pendencies, we count the following dependency-
tree boolean features for the emotional word, if
this sentence have the emotional words:

- Whether the word is in a ”neg” dependency
(negation modifier): true when there is a negation
word which modifies the emotional word.

- Whether the word is in an ”amod” dependency
(adjectival modifier): true if the emotional word
is (i) a noun modified by an adjective or (ii) an
adjective modifying a noun.

- Whether the word is in an ”advmod” depen-
dency (adverbial modifier): true if the emotion-
al word (i) is a non-clausal adverb or adverbial
phrase which serves to modify the meaning of a

word, or (ii) has been modified by an adverb.
If the sentence has not any emotional word, we

will consider the adjective words and its around
words.

4 Experiments and results

4.1 Data Construction

In this subsection, we explain the dataset and lex-
icon used in our experiments. Table 1 shows the
details of the construction dataset, and Figure 3
displays the distribution of the six emotion classes
(happy, fear, sad, surprised, angry, and disgust) in
the corpora. The various corpora and lexicon have
the following origins:

(1) Chinese emotion lexicon. Currently, there
is not any open and free existing Chinese emotion
lexicon with fine-grained emotion classes. There-
fore, the first resource we need to construct is an
emotional lexicon of Chinese with various emo-
tion categories. The English WordNet Affect lists
(Strapparava et al., 2004) based on Ekman’s six
basic emotion types have adequate number of e-
motion word entries. These English words lists
can be used to convert to Chinese words using En-
glish to Chinese bilingual dictionary or thesaurus.
Our final lexicon contains 1810 entries.

(2) News dataset. This news domain corpus is
created manually by two annotators. The annota-
tion process proceeds as follows: they have been
trained separately and work independently in or-
der to avoid any annotation bias and get a true
understanding of the task difficulty. Each anno-
tator marks the sentence level or short text with
one of six primary emotions (Ekman, 1992), and
then calculate the kappa value to assess such reli-
ability regarding emotion categories with a value
of 0.7 or above it indicating complete agreement.
Disagreements can be annotated by the third one,
then calculate the kappa value.

(3) Alm’s translation dataset. This data set is
based on Alm’s dataset (Alm et al., 2005), which
include annotated sentences from fairy tales, and
five emotion tags (happy, fearful, sad, surprised
and angry-disgusted) from the Ekman’s list of ba-
sic emotions were used for sentences or short tex-
t annotations. The construction process of this
dataset proceeds as follows: firstly, we collec-
t English-Chinese parallel corpora of fairy tales,
and split the text into individual sentences. Sec-
ondly, select Chinese sentences which correspond-
ing translation appeared in Alm’s Dataset accord-

56



Table 1: The dataset entries used in our experiment

Chinese emotion lexicon Alm’s translation dataset News dataset Blog dataset unlabeled corpora

1810 1223 1135 1000 115M

Figure 3: The distribution of the six emotions (happy, fear, sad, surprised, angry, and disgust)in the
corpora

ing to sentence alignment strategy. Lastly, anno-
tate angry and disgusted sentences by manually.
Since Alm’s dataset doesn’t separate the angry and
disgusted categories.

(4) Blog dataset. This dataset consists of
emotion-rich sentences or short text collected
from blogs. These sentences or short text are la-
beled with six emotion tags by two annotators.
The annotation process is the same as that of news
dataset.

(5) Unlabeled corpora. We downloaded addi-
tional 15M Chinese version of children’s story
from Andersen’s and Green’s fairy tales and 100M
Chinese news dataset to use as the unlabeled set.
We haven’t select blog corpora, because it is noisy.
This allows us to check the performance of each
system on the same kinds of data, and the unla-
beled set and the test set are in the same domain
and have similar underlying feature distributions.

4.2 Preprocessing

Given a labeled or an unlabeled data, we first car-
ry out segmentation and part-of-speech (POS) tag-
ging on each sentence or short text using the Stan-
ford toolkit, and then apply a simple word filter
based on POS tags to select content words (noun-
s, verbs, and adjectives). In next step, we create
dependency parse tree produced by the Stanford
dependency parser, and construct dependency sub-
trees. As we all know, the performance of Chinese
dependency parser is not very satisfactory. Hence,

we modify several wrong results manually. We
just want to testify our idea of that the fragments
based on dependency grammar are better than to-
kens.

4.3 Experimental Results

In this subsection, we report experimental results
on our dataset which contains news dataset, Alm’s
translation dataset and blogs dataset. The entries
of our dataset are short text or sentence. The news
dataset consists of 1135 entries and its average
length is 27.09. The Alm’s translation dataset con-
sist of 1223 entries and its average length 34.76.
The blog dataset contains 1000 entries and its av-
erage length is 30.73. The tasks on the Alm’s
translation dataset may be difficult because the
syntactic structures of the sentences are less re-
stricted and highly variable.

Table 2, Table 3 and Table 4 respectively
shows the accuracy result of our segment-based
method compared to two token-based approach
using SVM and MaxEnt, and a segment-based
method using CRF models (similar to the work
of (Nakagawa et al., 2010)), which employ five
kinds of feature sets (BOW, contentBOW, part-of-
speech, emotion words and dependency relation-
s) and their combination features, setting 10-fold
cross validation as a testing option.

As shown in Table 2-4, we can obtain below
conclusions:

(1) We can see that our approach based on the

57



Table 2: Experimental results on news dataset %

Feature SVM MaxEnt CRF Our approach

BOW 46.84 46.1 53.29 53.33
contentBOW 48.59 47.8 55 55.74
contentBOW+POS 48.64 47.67 56.78 56.69
contentBOW+Emotion 51.46 50.7 57.41 58.55
contentBOW+Emotion+POS 50.2 47.1 58.53 61.53
contentBOW+Emotion+POS+Dependency 54.45 54.32 59.06 65.12

Table 3: Experimental results on Alm’s translation dataset %

Feature SVM MaxEnt CRF Our approach

BOW 39.59 40.30 35.79 40.09
contentBOW 39.98 40.59 35.87 42.11
contentBOW+POS 40.19 38.82 36.05 43.95
contentBOW+Emotion 45.86 42.26 39.44 46.49
contentBOW+Emotion+POS 46.15 40.98 41.89 48.95
contentBOW+Emotion+POS+Dependency 48.23 45.05 45.68 50.81

Table 4: Experimental results on blog dataset %

Feature SVM MaxEnt CRF Our approach

BOW 46.09 45.81 44.24 45.62
contentBOW 46.34 46.06 46.33 46.19
contentBOW+POS 46.56 45.93 46.92 47.01
contentBOW+Emotion 47.92 46.03 47.23 47.63
contentBOW+Emotion+POS 48.38 45.77 47.98 48.63
contentBOW+Emotion+POS+Dependency 50.05 48.12 49.61 53.23

segment-based semi-CRF model has the highest
accuracy rate for each dataset using the combina-
tion features of contentBOW + Emotion + POS +
Dependency. Segment-based approach performed
better than token-based approach for the news
dataset, but without expected results for the Alm’s
translation and blogs dataset. This result, on the
one hand, demonstrates that Semi-CRF is more
powerful than CRF, and on the other hand, our e-
motion tag distribution model gives effective re-
sults. For token-based method, SVM gives a bet-
ter result than MaxEnt for all three of our Chinese
corpora.

(2) The accuracy rate of SVM has slightly less
than our model, but the results of MaxEnt and CR-
F is unbalanced. As we notice from table 2 to ta-
ble 4, CRF gives better results on the news dataset
than on the Alm’s translation dataset, but the re-
sults of MaxEnt on all dataset is worst. The rea-
sons for this result may be due to the bias problem

of MaxEnt.

(3) We can observe that using the combination
features of contentBOW + Emotion + POS + De-
pendency has the highest accuracy rate for each
dataset and each classifier. There are two types of
features achieve significantly improvements: emo-
tion words and the dependency relations, for ex-
ample, on news dataset, SVM with contentBOW
has the accuracy rate of 48.59% and adding e-
motion words has the accuracy rate of 51.46%,
showing the improvements of 2.87%. This is not
surprising result since emotion words has key in-
fluence to detection of the emotion category of a
sentence. However, the words or constituents in-
teract with each other to yield the overall emo-
tion label, there exists expression level emotion.
Dependency relationship features can solve this
problem and improve the performance of the sys-
tem,like in the example above,adding Dependen-
cy relationship features has the accuracy rate of

58



54.45%, showing the improvements of 5.86%.
When the baseline system use the content-

BOW features, the POS, Emotion and Dependen-
cy representation improve the accuracy rates of the
SVM, CRF and our classifier for each dataset, but
the use of POS representation for the MaxEnt clas-
sifier decreased the accuracy rate compared to the
Emotion and Dependency representations. One
reason lead to this problem might be the quality
of the data we use in this experiment.

(4) Overall performances on the news dataset
are better than on the Alm’s translation dataset
and blogs dataset. The reason perhaps is that the
syntactic structures of the sentences from Alm’s
translation dataset are less restricted and highly
variable, and the sentences from blogs dataset are
noisy, and there exist some linguistic or spelling
error.

5 Conclusion

In this paper, we present a segment-based learning
approach for fine-grained emotion detection. In
this method, the emotion label of each dependen-
cy subtree of a subjective sentence or short text is
represented by a hidden variable. The values of the
hidden variables are calculated in consideration of
interactions between variables whose nodes have
head-modifier relation in the dependency tree. D-
iffer from the existing token-based approach, the
segment-based emotion detection model can si-
multaneously exploit both the linguistic structure
and the expression-level emotion relation embed-
ded in sentences or short text. Three different
dataset, which contains news content, fairly tales,
and blogs data, is constructed to test our proposed
model, and the experimental results show that our
approach performed the best on three emotion cor-
pora and make a statistically significant improve-
ment over other classification algorithms, reflect-
ing its potential usage in the emotion detection
task.

References
P. Ekman. 1992. An argument for basic emotions Cog-

nition and Emotion, 6(3):169–200.

C. Alm, D. Roth, and R. Sproat. 2005. Emotions
from text: Machine learning for text-based emotion
prediction, In Proceedings of HLT–EMNLP, 579–
586.

Saima Aman and Stan Szpakowicz. 2007. Identifying

Expressions of Emotion in Text, TSD 2007, 196–
205.

Phil Katz, Matthew Singleton, Richard Wicentows-
ki. 2007. SWAT-MP: The SemEval-2007 System-
s for Task 5 and Task 14, Proceedings of the 4th
International Workshop on Semantic Evaluations
(SemEval-2007). Prague,308–313

Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition, In Proceedings of Recent Advances in
Natural Language Processing.

Yejin Choi and Claire Cardie. 2008. Learning
with Compositional Semantics as Structural Infer-
ence for Subsentential Sentiment Analysis, EMNLP
2008,793–801.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions, EMNLP 2011,151–
161.

Richard Socher, Brody Huval, Christopher D. Man-
ning, Andrew Y. Ng. 2012. Semantic Composi-
tionality through Recursive Matrix-Vector Spaces,
EMNLP-CoNLL 2012,1201–1211.

Baptiste Chardon, Farah Benamara, Yannick Math-
ieu,Vladimir Popescu, Nicholas Asher. 2013. Senti-
ment Composition Using a Parabolic Model, Pro-
ceedings of the 10th International Conference on
Computational Semantics (IWCS 2013) .

Tetsuji Nakagawa, Kentaro Inui, Sadao Kurohashi.
2010. Dependency Tree-based Sentiment Classifi-
cation using CRFs with Hidden Variables, HLT-
NAACL 2010,786–794.

S. Chaffar and D. Inkpen. 2011. Using a heteroge-
neous dataset for emotion analysis in text, In Cana-
dian Conference on AI,62–67.

S. M. Mohammad. 2012. Portable Features for
Classifying Emotional Text, 2012 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies,587–591.

Matthew Purver and Stuart Battersby. 2012. Experi-
menting with Distant Supervision for Emotion Clas-
sification, Proceedings of the 13th Conference of the
European Chapter of the Association for Computa-
tional Linguistics,482–491.

Ge Xu, Xinfan Meng, Houfeng Wang. 2010. Build
Chinese Emotion Lexicons Using A Graph-based Al-
gorithm and Multiple Resources, In Proceeding of
COLING-10,1209–1217.

Braja Gopal Patra, Hiroya Takamura, Dipankar
Das, Manabu Okumura and Sivaji Bandyopadhyay.
2013. Construction of Emotional Lexicon Using
Potts Model, International Joint Conference on Nat-
ural Language Processing,674–679.

59



Svitlana Volkova, William B. Dolan, Theresa Wil-
son. 2012. CLex: A Lexicon for Exploring Col-
or, Concept and Emotion Associations in Language,
Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics,306–314.

Ying Chen, Sophia Yat Mei Lee, Shoushan Li, Chu-
Ren Huang. 2010. Emotion Cause Detection
with Linguistic Constructions, Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010),179–187.

Ashequl Qadir, Ellen Riloff. 2013. Bootstrapped
Learning of Emotion Hashtags hashtags4you, Pro-
ceedings of the 4th Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis,2–11.

Dipankar Das and Sivaji Bandyopadhyay. 2010. Iden-
tifying Emotional Expressions, Intensities and Sen-
tence level Emotion Tags using a Supervised Frame-
work, PACLIC 2010,95–104.

Diman Ghazi, Diana Inkpen, Stan Szpakowicz. 2012.
Prior versus Contextual Emotion of a Word in a Sen-
tence, Proceedings of the 3rd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis,70–78.

Sunita Sarawagi, William W. Cohen. 2004. Semi-
Markov Conditional Random Fields for Information
Extraction, NIPS 2004.

Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, Jun’ichi Tsujii. 2006. Improving the Scal-
ability of Semi-Markov Conditional Random Fields
for Named Entity Recognition, ACL 2006,465–472.

Galen Andrew. 2006. A Hybrid Markov/Semi-Markov
Conditional Random Field for Sequence Segmenta-
tion, EMNLP 2006,465–472.

Bishan Yang, Claire Cardie. 2012. Extracting Opinion
Expressions with semi-Markov Conditional Random
Fields, EMNLP-CoNLL 2012,1335–1345.

C Strapparava, A Valitutti. 2004. WordNet-Affect: an
Affective Extension of WordNet, LREC 2004,1083–
1086.

60


