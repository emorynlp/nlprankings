



















































Recurrent Positional Embedding for Neural Machine Translation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1361â€“1367,
Hong Kong, China, November 3â€“7, 2019. cÂ©2019 Association for Computational Linguistics

1361

Recurrent Positional Embedding for Neural Machine Translation

Kehai Chen, Rui Wangâˆ—, Masao Utiyama, and Eiichiro Sumita
National Institute of Information and Communications Technology (NICT), Kyoto, Japan
{khchen, wangrui, mutiyama, eiichiro.sumita}@nict.go.jp

Abstract

In the Transformer network architecture,
positional embeddings are used to encode
order dependencies into the input representa-
tion. However, this input representation only
involves static order dependencies based on
discrete numerical information, that is, are
independent of word content. To address
this issue, this work proposes a recurrent
positional embedding approach based on
word vector. In this approach, these
recurrent positional embeddings are learned
by a recurrent neural network, encoding
word content-based order dependencies into
the input representation. They are then
integrated into the existing multi-head self-
attention model as independent heads or part
of each head. The experimental results
revealed that the proposed approach improved
translation performance over that of the state-
of-the-art Transformer baseline in WMTâ€™14
English-to-German and NIST Chinese-to-
English translation tasks.

1 Introduction

Transformer translation systems (Vaswani et al.,
2017), without recurrent and convolutional neural
networks, rely on a positional embedding (PE)
approach to encode order information into the
input representation. PE is typically learned
based on the position index of each word and
is added to corresponding word embedding.
This allows the Transformer to encode order
dependencies between words in addition to the
words themselves. Finally, the Transformer uses
these combined vectors as the input to self-
attention networks (SANs), achieving state-of-the-
art translation performance with several language
pairs (Vaswani et al., 2017; Dou et al., 2018;
Zhang et al., 2018a; Marie et al., 2018, 2019).

âˆ—Corresponding author

In spite of their success, the input representation
only involves static order dependencies based on
discrete numerical information. That is, any word
in the entire vocabulary has the same PE on the
same position index. As a result, the dependencies
encoded by the original PEs are independent
of word content, which may further hinder the
improvement of translation capacity. Recently,
Chen et al. (2018) and Hao et al. (2019) introduced
the additional source representation learned by an
RNN-based encoder into Transformer to alleviate
this issue, and reported improvements on the
WMTâ€™14 English-to-German translation task.

Inspired by their works (Chen et al., 2018; Hao
et al., 2019), we propose an simple and efficient
recurrent positional embedding approach to
capture order dependencies based on word content
in a sentence, thus learning a more effective
sentence representation for the Transformer. In
addition, we designed two simple multi-head
self-attentions to introduce these learned RPEs
and original input representation into the existing
Transformer model for enhancing the sentence
representation of Transformer. Experimental
results on WMTâ€™14 English-to-German and NIST
Chinese-to-English translation tasks show that
our models significantly improved translation
performance over a strong Transformer baseline.

2 Background

2.1 Input Representation

In the Transformer network architecture (Vaswani
et al., 2017), given a sentence of length J ,
the positional embedding of each word is firstly
computed based on its position:

pe(j,2i) = sin(j/10000
2i/dmodel),

pe(j,2i+1) = cos(j/10000
2i/dmodel),

(1)



1362

where j is the wordâ€™s numerical position index in
the sentence and i is the dimension of position
index. The word embedding xj is then added with
pej to get a combined embedding vj :

vj = xj + pej . (2)

As a result, there is a sequence of vectors {v1, Â· Â· Â· ,
vJ} as the input to the encoder or decoder of the
Transformer to learn the source or target sentence
representations.

2.2 Self-Attention Mechanism
Following the input layer, the self-attention layer
is used to learn the sentence representation for
the Transformer (Vaswani et al., 2017). These
combined vectors {v1, Â· Â· Â· , vJ} are then packed
into a query matrix Q, their keys and values
matrices K and V. The output of the self-attention
layer can be computed by Eq.(3):

Att(Q,K,V) = softmax(
QKTâˆš
dmodel

)V. (3)

Moreover, self-attention can be further refined
as multi-head self-attention to jointly attend to
the information from different representation sub-
spaces at different positions. Specifically, Q,
K, and V are linearly projected H times with
different, learned linear projections to dk and
dv dimensions, respectively. On each of these
projected versions of queries, keys, and values,
the attention function is performed in parallel,
yielding dv-dimensional output values. Take a
single head as an example, the output of the h-th
head Oh is computed by Eq.(4):

Oh = Att(QW
Q
h ,KW

K
h ,VW

V
h ), (4)

where the parameter matrices are
WQh âˆˆR

dmodelÃ—dk , WKh âˆˆRdmodelÃ—dk , and
WVh âˆˆRdmodelÃ—dv . For example, if there are
H=8 heads and dmodel is 512, dk=dv=512/8=64.
Finally, the outputs of H heads are concatenated
to serve as the sentence representation S:

S = Concat(O1, Â· Â· Â· ,OHâˆ’1,OH)WO, (5)

where WO âˆˆ RHdvÃ—dmodel is a parameter matrix.

3 Recurrent Positional Embedding

We propose a recurrent positional embedding
approach based on part of word embedding

instead of numerical indices of words to capture
order dependencies between words in a sentence.
Specifically, the embedding of each word xj
is divided into two parts xpj and x

r
j , and

their dimensions are dp and dr (dmodel=dp+dr),
respectively. As a result, the sequence of word
vectors {x1, Â· Â· Â· , xJ} are spited into {xp1, Â· Â· Â· ,
xpJ} and {xr1, Â· Â· Â· , xrJ}. An RNN with an non-
linear projection layer is then designed to learn its
recurrent state rj for each word over {xr1, Â· Â· Â· , xrJ}:

rj = Tanh(fRNN(xrj , rjâˆ’1)W
r + br), (6)

where Wr âˆˆ RdrÃ—dr is a parameter matrix and br
âˆˆ Rdr is a bias item.1 Note that the xrj is derived
from part of the word embedding xj . Finally, there
is a sequence R ={r1, Â· Â· Â· , rJ}, called as recurrent
positional embeddings (RPEs). In this work, a
bidirectional RNN and a forward RNN (Bahdanau
et al., 2015) are used to learn source RPEs and
target RPEs, respectively. Noth that the RNN is
also replaced by other neural networks for learning
order dependency information, such as GRU (Cho
et al., 2014), and SRU (Li et al., 2018a).

In addition, other sub-sequence {xp1, Â· Â· Â· , x
p
J}

is used to gain the reduced dimension input
representation P={p1, Â· Â· Â· , pJ} according to the
Section 2.1. Both of R and P will be together
as the input to the encoder (or decoder) to learn
a more effective source (or target) representation
for the Transformer.

4 Neural Machine Translation with RPE

To make use of these learned RPEs, we propose
two simple methods: RPE head (RPEHead) self-
attention and mixed positional representation head
(MPRHead) self-attention. Both of RPEHead
and MPRHead can utilize RPEs to learn sentence
representation for the Transformer.

4.1 RPEHead Self-Attention

For the RPEHead self-Attention, these learned
RPEs are integrated into multi-head self-attention
(Fig 1(a)) as several independent heads to learn the
sentence representation, as shown in Fig 1(b).

First, we concatenate P={p1, Â· Â· Â· , pJ} and
R ={r1, Â· Â· Â· , rJ} as a new combined input
representation T . To perform the attention
function in Eq.(3) over the combined input
representation T , dr and dp are set to t*dmodel/H

1The r0 is set as a zero vector.



1363

Linear Linear

ğ“šğ‘´

Linear

ğ“¢ğ‘´

ğ“ ğ‘´ ğ“¥ğ‘´

Scaled Dot-Product Attention

RPE

WE

WE+PE

(c)

Linear Linear

ğ“šğ“£

Linear

ğ“¢ğ“£

ğ“ ğ“£ ğ“¥ğ“£

Scaled Dot-Product Attention

(b)

Linear Linear

ğ‘²

Linear

ğ‘º

ğ‘¸ ğ‘½

Scaled Dot-Product Attention

(a)

Linear

Concat

Linear

Concat

Linear

Concat

Figure 1: (a) Multi-Head Self-Attention; (b) RPEHead Self-Attention; (c) MPRHead Self-Attention.

and dmodel-dr in the process of learning RPEs,
respectively. This guarantees that there are two
types of heads: one type of head only contains
RPE and other type of head only contains the
original reduced dimension input representation.
Second, the T is mapped to a new query matrix
QT , and their keys and values matrices KT and
VT . According to Eq.(4), the output of each head
is computed by Eq.(7):

OTh = Att(QT WQh ,K
T WKh ,VT WVh ). (7)

Therefore, the final sentence representation ST is
formally represented as:

ST = Concat(OT1 , Â· Â· Â· ,OTHâˆ’1,OTH)WO. (8)

4.2 MPRHead Self-Attention
Compared with the RPEHead model, the
MPRHead model applies the multi-head
mechanism to the RPEs. In other words,
RPEs are encoded into the sentence representation
from different vector sub-spaces, as shown in
Fig 1(c).

To this end, each vector of P is divided into the
H heads {p1j , Â· Â· Â· , pHj }. Similarly, each vector
of R is divided into the H heads {r1j , Â· Â· Â· , rHj }.
The corresponding heads for pj and rj are then
concatenated as a combined sequence prj , in turn:

prj = {p1j : r1j , Â· Â· Â· ,pHj : rHj }, (9)

where â€œ:â€ is the concatenation operation. All
heads in prj are further concatenated as a mixed
embedding mj âˆˆ Rdmodel in turn. As a result,
there is a new sequence of mixed embeddings:

M = {m1, Â· Â· Â· ,mJ}. (10)

The M is mapped to a query matrix QM, and
their keys and values matrices KM and VM.

According to Eq.(4) and Eq.(5), the final sentence
representation SM is represented as:

OMh = Att(QMWQh ,K
MWKh ,VMWVh ),

SM = Concat(OM1 , Â· Â· Â· ,OMHâˆ’1,OMH)WO. (11)

Note that for both RPEHead and MPRHead
models, the RPEs will be jointly learned with the
existing Transformer architecture.

5 Experiments

5.1 Experimental Setup

The proposed methods were evaluated on the
WMTâ€™14 English to German (EN-DE) and
NIST Chinese-to-English (ZH-EN) translation
tasks. The ZH-EN training set includes
1.28 million bilingual sentence pairs from the
LDC corpora, where the NIST06 and the
NIST02/NIST03/NIST04 data sets were used as
the development and test sets, respectively. The
EN-DE training set includes 4.43 million bilingual
sentence pairs of the WMTâ€™14 corpora, where
the newstest2013 and newstest2014 data sets were
used as the development and test sets, respectively.

The BPE (Sennrich et al., 2016) was adopted
and the vocabulary size was set as 32K. The
dimension of all input and output layers was set
to 512, and that of the inner feedforward neural
network layer was set to 2048. The total heads
of all multi-head modules were set to 8 in both
encoder and decoder layers. In each training
batch, there was a set of sentence pairs containing
approximately 4096*4 source tokens and 4096*4
target tokens. For the other setting not mentioned,
we followed the setting in Vaswani et al. (2017).

Baseline systems included a vanilla Trans-
former (Vaswani et al., 2017), Relative PEs (Shaw
et al., 2018), and directional SAN (DiSAN) (Shen



1364

System Architecture newstest2014 #Param
Existing NMT systems

Vaswani et al. (2017)
Transformer (base) 27.3 65M
Transformer (big) 28.4 213M

Chen et al. (2018) RNMT+SAN 28.49 378.9M

Hao et al. (2019)
Transformer (base)+BiARN 28.21 97.4M
Transformer (big)+BiARN 28.98 323.5M

Our NMT systems

This work

Transformer (base) 27.25 97.35M
+Relative PE 27.60 97.42M
+DiSAN 27.66 97.39M
+RPEHead 28.11* 97.84M
+MPRHead 28.35* 97.72M

Transformer (big) 28.22 272.6M
+MPRHead 29.11* 289.1M

Table 1: Results for EN-DE translation task. The mark â€œ*â€ after scores indicates that the model was significantly
better than the baseline Transformer (base or big) at the significance level p <0.01 (Collins et al., 2005).

0 64 128 192 256 320 384 448 512

27.5

28

dr

B
L

E
U

Transformer (base) +RPEHead +MPRHead

Figure 2: The BLEU scores on the different dr.

et al., 2018). Besides, we reported results of
the existing works (Vaswani et al., 2017; Chen
et al., 2018; Shaw et al., 2018; Hao et al., 2019;
Liu et al., 2019; Chen et al., 2019). We re-
implemented the baseline Transformer, Relative
PEs, and DiSAN models on the OpenNMT
toolkit (Klein et al., 2017). All the models were
trained for 200k batches and evaluated on a single
V100 GPU. The multi-bleu.perl was used as the
evaluation metric to obtain the case-sensitive 4-
gram BLEU score of EN-DE and ZH-EN tasks.

5.2 Effect of RPEs

In this work, we extracted dr dimensions of each
word vector to learn recurrent embeddings. To
explore the relation between dr and translation
performance, Figure 2 shows the translation
performance on the different dr. For +RPEHead
(or +MPRHead), with the increasing in the
dimension of dr, BLEU scores are gradually
increasing, but BLEU scores begin to decrease
when dr is more than 320 (or 256). In
particular, +RPEHead and +MPRHead achieve

the highest BLEU scores at dr=320 and dr=256,
respectively. This means that the original partial
input representation and our RPE can complement
each other to improve translation performance.

5.3 Main Results

According to the results of Fig 2, dr of RPEHead
is set to 320 and dr of MPRHead is set to 256. The
main translation results are shown in Table 1.

1) For the proposed methods, both RPEHead
(base) and MPRHead (base) outperformed Trans-
former (base), especially are better than +RPE and
+DiSAN. This indicates that the learned RPEs are
beneficial for the Transformer system.

2) Moreover, +MPRHead (base) performed
better than RPEHead/RPE (base). The reason
may be that adding RPEs into variant heads can
encode order dependencies based word vectors
from variant vector sub-spaces, which is one
of the advantages of the multi-head mechanism.
In particular, +MPRHead (base/big) is slightly
better than Transformer (base/big)+BiARN. This
denotes that the RPEs can more effective improve
the performance of Transformer.

3) MPRHead (big) was superior to Transformer
(big) significantly. MPRHead (base) achieved
a comparable performance compared to Trans-
former (big) which contains approximately three
times parameters, indicating that the proposed
RPE is efficient.

In addition, Table 2 shows that the proposed
models also gave similar improvements over the
baseline system and the compared methods on the
NIST ZH-EN task. These results indicate that our



1365

System Architecture NIST02 NIST03 NIST04 #Param
Existing NMT systems

Liu et al. (2019) Share-private Embeddings 43.73 41.99 44.53 N/A
Chen et al. (2019) Reordering Embeddings 47.54 46.56 47.27 78.3M

Our NMT systems

This work

Transformer (base) 46.62 45.90 45.95 77.94M
+Relative PEs (Shaw et al., 2018) 46.91 46.47 45.96 77.01M
+RPEHead 47.06 46.21 46.32 78.04M
+MPRHead 47.69* 47.27* 47.31* 78.04M

Transformer (big) 47.88 47.58 47.37 243.7M
+MPRHead 48.73* 48.61* 48.21* 245.5M

Table 2: Results for ZH-EN translation Task. The mark â€œ*â€ after scores indicates that the model was significantly
better than the baseline Transformer (base or big) at the significance level p <0.01 (Collins et al., 2005).

approach is a universal method for improving the
translation of other language pairs.

5.4 Ablation Experiments
To further explore the effect of position informa-
tion, PEs and RPEs were applied on the encoder
(Enc) and decoder (Dec) side, respectively.
Table 3 shows the results of ablation experiments
on the EN-DE newstest2014 test set:

newstest2014 #Speed
Transformer (base) 27.25 11.7K

- PE of Dec 26.73 11.7K
- PE of Enc 10.71 11.7K

RPEHead (base) 28.11 10.5K
- RPE&PE of Dec 27.54 11.2K
- RPE&PE of Enc 11.16 10.7K

MPRHead (base) 28.35 9.9K
- RPE&PE of Dec 27.74 11.2K
- RPE/PE of Enc 10.89 10.7K
- RPE&PE of Enc&Dec 10.43 11.7K

Table 3: Ablation experiments of position information.
â€œ#Speedâ€ denotes the training speed (tokens/second).

1) For Enc/Dec/Enc&Dec, the proposed RPE-
Head/MPRHead outperformed the Transformer in
corresponding ablation settings, indicating that
our methods worked well on both the encoder and
decoder.

2) For PE/RPEs, performance became slightly
lower when PE was removed from the decoder.
However, when PE was removed from the
encoder, the translation performance drastically
decreased. We think that the source sentence
representation is more sensitive to position infor-
mation than the target sentence representation. It
is possible that each hidden state in the decoder
takes the previous hidden state into consideration,
which would be somewhat similar to the proposed

RPEs. In contrast, the encoder would not be
expected to contain this mechanism. Therefore,
the position information would be more important
in the encoder than the decoder.

3) The training speeds of the proposed
RPEHead (base) and MPRHead (base) were
just slightly slower than those of the vanilla
Transformer (base), because we nearly did not
introduce additional (less than 2%) parameters.

6 Conclusion and Future Work

In this paper, we presented a recurrent embedding
to capture the order dependencies in a sentence.
Empirical results show that this method can
improve the performance of NMT efficiently.
In future work, we plan to extend this method
to unsupervised NMT (Sun et al., 2019) and
other natural language processing tasks, such as
dependency parsing (Li et al., 2018b) and reading
comprehension (Zhang et al., 2018b).

Acknowledgments

We are grateful to the anonymous reviewers and
the area chair for their insightful comments and
suggestions. This work was partially conducted
under the program â€œResearch and Development
of Enhanced Multilingual and Multipurpose
Speech Translation Systemsâ€ of the Ministry
of Internal Affairs and Communications (MIC),
Japan. Masao Utiyama is partly supported by
JSPS KAKENHI Grant Number 19H05660. Rui
Wang was partially supported by JSPS grant-
in-aid for early-career scientists (19K20354):
â€œUnsupervised Neural Machine Translation in
Universal Scenariosâ€ and NICT tenure-track re-
searcher startup fund â€œToward Intelligent Machine
Translationâ€.



1366

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings
of the 3rd International Conference on Learning
Representations, San Diego, CA.

Kehai Chen, Rui Wang, Masao Utiyama, and
Eiichiro Sumita. 2019. Neural machine translation
with reordering embeddings. In Proceedings
of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 1787â€“1799,
Florence, Italy. Association for Computational
Linguistics.

Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin
Johnson, Wolfgang Macherey, George Foster, Llion
Jones, Mike Schuster, Noam Shazeer, Niki Parmar,
Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,
Zhifeng Chen, Yonghui Wu, and Macduff Hughes.
2018. The best of both worlds: Combining
recent advances in neural machine translation. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 76â€“86, Melbourne, Australia.
Association for Computational Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoderâ€“decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724â€“
1734, Doha, Qatar. Association for Computational
Linguistics.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACLâ€™05), pages 531â€“540, Ann Arbor,
Michigan. Association for Computational Linguis-
tics.

Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming
Shi, and Tong Zhang. 2018. Exploiting deep
representations for neural machine translation. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
4253â€“4262, Brussels, Belgium. Association for
Computational Linguistics.

Jie Hao, Xing Wang, Baosong Yang, Longyue Wang,
Jinfeng Zhang, and Zhaopeng Tu. 2019. Modeling
recurrence for transformer. In Proceedings of the
2019 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
and Short Papers), pages 1198â€“1207, Minneapolis,
Minnesota. Association for Computational Linguis-
tics.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. Opennmt:

Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67â€“72, Vancouver, Canada. Association
for Computational Linguistics.

Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu,
and Tong Zhang. 2018a. Multi-head attention with
disagreement regularization. In Proceedings of the
2018 Conferenctemporale on Empirical Methods in
Natural Language Processing, pages 2897â€“2903,
Brussels, Belgium. Association for Computational
Linguistics.

Zuchao Li, Jiaxun Cai, Shexia He, and Hai Zhao.
2018b. Seq2seq dependency parsing. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 3203â€“3214, Santa
Fe, New Mexico, USA.

Xuebo Liu, Derek F. Wong, Yang Liu, Lidia S. Chao,
Tong Xiao, and Jingbo Zhu. 2019. Shared-private
bilingual word embeddings for neural machine
translation. In Proceedings of the 57th Annual
Meeting of the Association for Computational
Linguistics, pages 3613â€“3622, Florence, Italy.
Association for Computational Linguistics.

Benjamin Marie, Haipeng Sun, Rui Wang, Kehai
Chen, Atsushi Fujita, Masao Utiyama, and Eiichiro
Sumita. 2019. NICTâ€™s unsupervised neural and
statistical machine translation systems for the
WMT19 news translation task. In Proceedings
of the Fourth Conference on Machine Translation
(Volume 2: Shared Task Papers, Day 1), pages 294â€“
301, Florence, Italy. Association for Computational
Linguistics.

Benjamin Marie, Rui Wang, Atsushi Fujita, Masao
Utiyama, and Eiichiro Sumita. 2018. NICTâ€™s
neural and statistical machine translation systems
for the WMT18 news translation task. In
Proceedings of the Third Conference on Machine
Translation: Shared Task Papers, pages 449â€“455,
Belgium, Brussels. Association for Computational
Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages
1715â€“1725, Berlin, Germany. Association for
Computational Linguistics.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
2018. Self-attention with relative position repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers), pages 464â€“
468, New Orleans, Louisiana. Association for
Computational Linguistics.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan:

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
https://www.aclweb.org/anthology/P19-1174
https://www.aclweb.org/anthology/P19-1174
http://aclweb.org/anthology/P18-1008
http://aclweb.org/anthology/P18-1008
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://aclweb.org/anthology/P05-1066
http://aclweb.org/anthology/P05-1066
http://aclweb.org/anthology/D18-1457
http://aclweb.org/anthology/D18-1457
https://doi.org/10.18653/v1/N19-1122
https://doi.org/10.18653/v1/N19-1122
http://aclweb.org/anthology/P17-4012
http://aclweb.org/anthology/P17-4012
http://aclweb.org/anthology/D18-1317
http://aclweb.org/anthology/D18-1317
https://www.aclweb.org/anthology/C18-1271
https://www.aclweb.org/anthology/P19-1352
https://www.aclweb.org/anthology/P19-1352
https://www.aclweb.org/anthology/P19-1352
https://www.aclweb.org/anthology/W19-5330
https://www.aclweb.org/anthology/W19-5330
https://www.aclweb.org/anthology/W19-5330
https://www.aclweb.org/anthology/W18-6419
https://www.aclweb.org/anthology/W18-6419
https://www.aclweb.org/anthology/W18-6419
http://www.aclweb.org/anthology/P16-1162
http://www.aclweb.org/anthology/P16-1162
https://doi.org/10.18653/v1/N18-2074
https://doi.org/10.18653/v1/N18-2074
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099


1367

Directional self-attention network for rnn/cnn-free
language understanding. In AAAI Conference on
Artificial Intelligence.

Haipeng Sun, Rui Wang, Kehai Chen, Masao
Utiyama, Eiichiro Sumita, and Tiejun Zhao. 2019.
Unsupervised bilingual word embedding agreement
for unsupervised neural machine translation. In
Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics,
pages 1235â€“1245, Florence, Italy. Association for
Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Å ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 30, pages 5998â€“6008. Curran
Associates, Inc.

Biao Zhang, Deyi Xiong, and Jinsong Su. 2018a.
Accelerating neural transformer via an average
attention network. In Proceedings of the 56th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1789â€“1798, Melbourne, Australia. Association for
Computational Linguistics.

Zhuosheng Zhang, Yafang Huang, and Hai Zhao.
2018b. Subword-augmented embedding for cloze
reading comprehension. In Proceedings of the
27th International Conference on Computational
Linguistics, pages 1802â€“1814, Santa Fe, New
Mexico, USA. Association for Computational
Linguistics.

https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16126/16099
https://www.aclweb.org/anthology/P19-1119
https://www.aclweb.org/anthology/P19-1119
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://aclweb.org/anthology/P18-1166
http://aclweb.org/anthology/P18-1166
https://www.aclweb.org/anthology/C18-1153
https://www.aclweb.org/anthology/C18-1153

