



















































Joint Embedding of Query and Ad by Leveraging Implicit Feedback


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 482–491,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Joint Embedding of Query and Ad by Leveraging Implicit Feedback

Sungjin Lee and Yifan Hu
Yahoo Labs

229 West 43rd Street, New York, NY 10036, USA
{junion, yifanhu}@yahoo-inc.com

Abstract
Sponsored search is at the center of a multibil-
lion dollar market established by search tech-
nology. Accurate ad click prediction is a key
component for this market to function since
the pricing mechanism heavily relies on the
estimation of click probabilities. Lexical fea-
tures derived from the text of both the query
and ads play a significant role, complementing
features based on historical click information.
The purpose of this paper is to explore the use
of word embedding techniques to generate ef-
fective text features that can capture not only
lexical similarity between query and ads but
also the latent user intents. We identify several
potential weaknesses of the plain application
of conventional word embedding methodolo-
gies for ad click prediction. These observa-
tions motivated us to propose a set of novel
joint word embedding methods by leveraging
implicit click feedback. We verify the effec-
tiveness of these new word embedding models
by adding features derived from the new mod-
els to the click prediction system of a com-
mercial search engine. Our evaluation results
clearly demonstrate the effectiveness of the
proposed methods. To the best of our knowl-
edge this work is the first successful applica-
tion of word embedding techniques for the task
of click prediction in sponsored search.

1 Introduction
Sponsored search is a multibillion dollar market
(Easley and Kleinberg, 2010) that makes most search
engine revenue and is one of the most successful ways
for advertisers to reach their intended audiences. When
search engines deliver results to a user, sponsored ad-
vertisement impressions (ads) are shown alongside the
organic search results (Figure 1). Typically the adver-
tiser pays the search engine based on the pay-per-click
model. In this model the advertiser pays only if the im-
pression that accompanies the search results is clicked.
The price is usually set by a generalized second-price
(GSP) auction (Edelman et al., 2005) that encourages
advertisers to bid truthfully. An advertiser wins if the
expected revenue for this advertiser, which is the bid

Figure 1: Sponsored ads when “pizza” was searched at
Yahoo! (www.yahoo.com).

price times the expected click probability (also know
as click through rate, or CTR), is ranked the highest.
The price the advertiser pays, known as cost-per-click
(CPC), is the bid price for the second ranked advertiser
times the ratio of the expected CTR between the sec-
ond and first ranked advertisers. From this discussion
it should be clear that CTR plays a key role in deciding
both the ranking and the pricing of the ads. Therefore
it is very important to predict CTR accurately.

The state of the art search engine typically uses a
machine learning model to predict CTR by exploiting
various features that have been found useful in prac-
tice. These include historical click performance fea-
tures such as historical click probability for the query,
the ad, the user, and a combination of these; contextual
features such as temporal and geographical informa-
tion; and text-based features such as query keywords or
ad title and description. Among these, historical click
performance features often have the most predictive
power for queries, ads and users that have registered
many impressions. For queries, ads and users that have
not registered many impressions, however, historical
CTR may have too high a variance to be useful. Hillard
et al. (2011) observed that the number of impressions
and clicks recorded on query-ad pairs have a very long
tail: only 61% of queries has greater than three clicks.
They also reported a drastic drop in the accuracy of the
click prediction model when fewer historical observa-
tions are available. Furthermore, fine-grained historical
CTR information takes a huge amount of space, which

482



makes it costly to maintain. On the other hand, text
features are always readily available, and thus are par-
ticularly useful for those cases for which there is insuf-
ficient historical information.

Multiple researchers, for example (Richardson,
2007; Cheng and Cantú-Paz, 2010), reported the us-
age of text features including simple lexical similarity
scores between the query and ads, word or phrase over-
laps and the number of overlapping words and charac-
ters. Such features rely on the assumption that query-ad
overlap is correlated with perceived relevance. While
this is true to a certain extent, the use of simple lexical
similarity cannot capture semantic information such as
synonyms, entities of the same type and strong rela-
tionships between entities (e.g. CEO-company, brand-
model, part-of). Recently a host of studies on word
embedding have been conducted; all map words into
a vector space such that semantically relevant words
are placed near each other in the space (Mikolov et al.,
2013a; Pennington et al., 2014; Baroni et al., 2014).
The use of continuous word vectors has been shown
to be helpful for a wide range of NLP tasks by better
capturing both syntactic and semantic information than
simple lexical features (Socher et al., 2012a).

No previous research on sponsored search has suc-
cessfully used word embeddings to generate text fea-
tures. In this paper, we explore the use of word em-
beddings for click prediction. However, it is clear that
conventional word embeddings (which solely rely on
word co-occurrence in a context window) can only of-
fer limited discriminative power because queries and
ad text are typically very short. In addition, conven-
tional word embeddings cannot capture user intents,
preferences and desires. Wang et al. (2013) showed
that specific frequently occurring lexical patterns, e.g.,
x% off, guaranteed return in x days and official site, are
effective in triggering users desires, and thus lead to
significant differences in CTR. Conventional word em-
beddings cannot capture these phenomena since they
do not incorporate the implicit feedback users provide
through clicks and non-clicks. These observations nat-
urally lead us to leverage click feedback to infuse users’
intentions and desires into the vector space.

The simplest way to harness click feedback is to
train conventional word embedding models on a cor-
pus that only includes clicked impressions, where each
“sentence” is constructed by mixing the query and ad
text. Having trained a word embedding model, we sim-
ply take the average of word vectors of the query and
ads respectively to obtain sentence (or paragraph) vec-
tors, which in turn are used to compute the similarity
scores between the query and ads. Our experiments
show that this method does improve click prediction
performance. However, this method has several po-
tential weaknesses. First, the use of only clicked im-
pressions ignores the large amount of negative signals
contained in the non-clicked ad impressions. Second,
the use of indirect signals (word co-occurrences) can

be noisy or even harmful to our ultimate goal (accurate
click prediction) when it is combined with direct sig-
nals (impressions with click feedback). Third, without
explicit consideration about the averaging step in the
training process of word embedding models, a simple
averaging scheme across word vectors may be a subop-
timal. We therefore propose several joint word embed-
ding models; all of these aim to put query vectors close
to relevant ad vectors by explicitly utilizing both posi-
tive and negative click feedback. We evaluate all these
models against a large sponsored search data set from
a commercial search engine, and demonstrate that our
proposed models significantly improve click prediction
performance.

The rest of this paper is organized as follows. In Sec-
tion 2 we present a brief summary of related work. In
Section 3 we give some background information on ad
click prediction in sponsored search. In Section 4 we
describe our methods. In Section 5 we discuss our ex-
periments. We finish with some conclusions and future
directions in Section 6.

2 Related Work

Text features for predicting click probability There
have been many studies on the use of text features
for click prediction. For example, Dembczynski et
al. (2008) used a decision rule-based approach involv-
ing such lexical features as the number of words in
ad title and description, the number of segments and
length of the ad URL, and individual words and terms
in ad title and description. Cheng et al. (2010) used
a logistic regression model that used both historical
click performance features and simple lexical features
such as word or phrase overlap between query and ad
title and description. Trofimov et al. (2012) used a
variant of boosted decision trees with similar features.
Richardson et al. (2007) specifically considered new
ads (which lack historical click prediction data) and
proposed to use the CTR for ad terms, the frequency
of certain unigrams (e.g., dollar signs) and general En-
glish usage patterns, and simple lexical distance be-
tween the query and ads. In all this previous work,
text features consisted only of surface-level text fea-
tures. To the best of our knowledge, there is no previ-
ous work adopting semantic-level text features for the
purpose of click prediction, in particular word embed-
dings to measure query-ad relevance. In a similar vein
of research, Grbovic et al. (2015) adopted word embed-
dings to the task of query rewriting for a better match
between queries and keywords that advertisers entered
into an auction. Using the embeddings, semantically
similar queries are mapped into vectors close in the em-
bedding space, which allows expansion of a query via
K-nearest neighbor search.
Word embeddings for language processing Recently
many NLP systems have obtained improved perfor-
mance with less human engineering by adopting dis-
tributed word representations (Socher et al., 2012a).

483



In particular, neural word embedding techniques are
now known to be effective in capturing syntactic and
semantic relationships, and more computationally ef-
ficient than many other competitors (Mikolov et al.,
2013a; Pennington et al., 2014; Baroni et al., 2014). On
top of word embeddings, a new stream of research on
sentence and paragraph embeddings has also emerged
to tackle higher level tasks such as sentiment analy-
sis, machine translation and semantic relatedness tasks.
Recursive Neural Networks (RNNs) have been used for
syntactic parsing (Socher et al., 2012b; Socher et al.,
2013; Socher et al., 2014; Irsoy and Cardie, 2014).
Long Short-Term Memory (LSTM) networks have been
applied to machine translation (Bahdanau et al., 2014;
Sutskever et al., 2014) and semantic processing (Tai
et al., 2015). Interestingly Convolutional Neural Net-
works (CNNs), widely used for image processing, have
recently emerged as a strong class of models for NLP
tasks (Kim, 2014; Blunsom et al., 2014). As apposed
to the models above, PhraseVector (Le and Mikolov,
2014) takes a less structured but unsupervised approach
by treating a piece of text as a token and performing
word embedding-like training with an unlimited con-
text window. None of this previous work exactly fits
the click prediction task. Since queries and ads are
much less structured than usual text, it is not attractive
to use models with complex structures, such as RNNs,
at the cost of speed and scalability. PhraseVector is less
structured but it does not support compositionality, suf-
fering from sparseness or requiring to train new vectors
for each unseen query and ad. Interestingly, as reported
in (Tai et al., 2015), a simple averaging scheme (mean
vector) was found to be very competitive to more com-
plex models for high level semantic tasks despite its
simplicity. These observations lead us to one of our
models that aims to improve the mean vector method
by directly optimizing mean vectors instead of word
vectors.
Joint embedding to bridge multiple views. Multi-
ple studies have explored the task of bringing multiple
views into the same vector space. For example, there
is now a large body of research on joint modeling of
text and image information (Frome et al., 2013; Karpa-
thy and Fei-Fei, 2014; Socher et al., 2014). The mul-
timodal embedding space helps find appropriate align-
ments between image regions and corresponding pieces
of text description. Joint embedding has also been ap-
plied to question answering (Wang et al., 2014) and se-
mantic understanding (Yang et al., 2014). In contrast
to the tasks above, there is no natural component-wise
correspondence between queries and ads; instead the
relationship is more implicit and pragmatic. Because of
this, our methods rely on global rather than component-
level signals for model training.

3 Baseline Click Prediction Model
We first present a high-level description of sponsored
search. The process consists of several stages. First,

given a user query, a list of candidate ads are retrieved,
either by exactly matching query terms to the bid terms
of the advertiser, or by first using query term expansion
to obtain a longer list of matched ads. Some candi-
date ads may be filtered out based on metrics such as
ad quality. Then, a click prediction model scores the
candidate ads to estimate how likely it is that each will
be clicked. This click probability serves a crucial role
both in the user experience and in the revenue for the
search engine. The ads with the highest click probabil-
ities are placed in the search results page. The price-
per-click for each ad shown is determined based on the
click probabilities and the GSP auction.

Our baseline click prediction model is formulated as
a supervised learning problem. Specifically we use Lo-
gistic Regression (LR) since LR is well suited for prob-
ability estimation. Given a variety of features, the prob-
ability of a click is expressed as:

p(c|q, a, u) = 1
1 + exp

{∑
i wifi(q, a, u)

} , (1)
where c ∈ {1, 0} is the label (1: click or 0: non-click),
fi(q, a, u) is the ith feature derived for query-ad-user
triple (q, a, u) and wi is the associated weight. The
model is trained using a stochastic gradient descent al-
gorithm on a per impression basis with l1 regularization
to avoid overfitting.

An accurate LR model relies greatly on the effective-
ness of its features. Our baseline model is furnished
with a rich set of features that are typically used in
commercial search engines. The first feature type is
based on the historical CTR of user, query, ad triples
(if there is enough historical information on this). We
use two groups of features of this type: COEC based
features and user factor features. The second feature
type is based on query and ad text.

Due to the significant decrease of CTR depending on
the ad position, it has become common practice to use
position-normalized CTR (a.k.a. Clicks Over Expected
Clicks):

COEC =

∑
p cp∑

p ip ∗ CTRp
, (2)

where the numerator is the total number of clicks re-
ceived by the configuration of interest; the denomi-
nator is the expected clicks (ECs) that an average ad
would receive after ip times impressions at position
p, and CTRp is the average CTR at position p, cal-
culated over all queries, ads and users. We use user-
independent features derived from COEC statistics for
specific query-ad pairs. However, many impressions
are needed for these statistics to be reliable and there-
fore data for specific query-ad pairs can be sparse and
noisy (only around 70% of queries and about 50% of
query-URL, query-bid term pairs have historical CTR).
To alleviate this problem, additional COEC statistics
over aggregations of queries or ads are also used. The
exact description of these aggregations is beyond the

484



scope of this paper, but briefly we exploit the catego-
rization of the ads in ad groups, campaigns, and ac-
counts defined by advertisers.

Since it is well known that personalization features
are crucial to obtain accurate click prediction mod-
els (Cheng and Cantú-Paz, 2010), we also use features
that measure user factors relating to CTR. The user
click feedback features capture the inclination of indi-
vidual users to click on ads in general. The user-query
click feedback features indicate the propensity of users
to click for certain queries or groups of queries. Fi-
nally user-ad features dictate the user preferences on
certain ads or advertisers. However the data sparseness
problem becomes even more serious when it comes to
user-specific features (we use a threshold of 100 for
statistical confidence). For example, only about 5%
of user-URL pairs and 1% of user, query, URL triples
have historical CTR information. Therefore a set of
segment-level features can be extracted as back-off fea-
tures where users, queries and ads are clustered into
groups, and group level historical CTRs are collected.

Our second major feature type involves the lexical
similarity between query and ad text. These text fea-
tures assume that users are more likely to click on ads
that seem to be relevant to the query, and that per-
ceived relevance is correlated with the degree of query-
ad overlap. These features include the number of over-
lapping words and characters in query-ad URL, query-
ad title, and query-ad description, and the number of
words and characters in the query. The discrimination
power of simple lexical features is relatively limited be-
cause query and ad text are typically very short.

Finally, we use other contextual features that are
helpful in predicting the click probabilities.; for exam-
ple, time of day, day of week, and geographic infor-
mation. To model interactions among features, some
features are selected by domain knowledge to be con-
joined. All together, our baseline model utilize a com-
prehensive set of historical CTR, lexical, and contex-
tual features (over a hundred features in total). The fact
that this baseline model is highly optimized makes the
subsequent performance improvement from our pro-
posed algorithm meaningful. This baseline model is
used in production in part of a major search platform.

4 Joint Embedding for Click Prediction

We now describe several methods that jointly embed
words in both the query and the ads into the same vec-
tor space. In our experiments, we incorporate these
methods as features in our click prediction model. We
start by defining the notation used in this section. A
sponsored search dataset D is a set of tuples for each
ad impression (q, t, d, y) where q ≡ {qj} is a multi-
word query string, t ≡ {tk}, d ≡ {dl} are multiword
ad title and description strings, and y is a binary in-
dicator for whether the ad is clicked. We have two
choices in defining the vocabulary V from which words
are drawn: we can use a unified vocabulary for both

query and ads or define a separate vocabulary for each
– V ≡ Vq ⊕ Va. In our initial experiments the uni-
fied vocabulary constantly yielded better performances,
thus we always use the unified vocabulary here. We use
bold letters qj , tk,dl to denote the corresponding em-
bedding representations of {qj , tk, dl}. Finally we use
W to represent the vocabulary matrix; in W each col-
umn is a word vector.

4.1 Exploiting word2vec embedding
Typically word embeddings are learned from a given
text corpus through implicit supervision of predicting
the current word given a window of its surrounding text
or predicting each word in the window of the current
word. The former approach is known as continuous
bag-of-words (CBOW) and the latter Skip-gram. For
simplicity’s sake we use negative-sampling for training
word embedding models (Mikolov et al., 2013b). More
formally we define the binary conditional probability
for a pair of words (v, w): 1

p(v,w) =
1

1 + exp(−vT w) , (3)

The CBOW algorithm learns word embeddings by
minimizing the following logloss of each impression i
with regard to W :

CBi(W ) =− log p(wi,µC)
−

∑
v∈N(wi)

log (1− p(v,µC)) , (4)

where the context C of a word wi comes from a win-
dow of size k around the word in a sentence of n words
w1,. . .,wn: C = wi−k,. . .,wi−1,wi+1,. . .,wi+k. µC is
the averaged context vector of wi; µC =

1
|C|
∑

v∈C v.
N(wi) is the set of negative examples which is drawn
according to the unigram distribution of the corpus
raised to the 3/4th power. Similarly to (Mikolov et al.,
2013b), we adopt a dynamic window size – for each
word the actual window size is sampled uniformly from
1, . . . , k.

Similarly the Skip-gram algorithm minimizes the
logloss of each impression i with regard to W :

SKi(W ) =−
∑
v∈C

log p(wi,v)

−
∑

v∈N(wi)
log (1− p(wi,v)) .

(5)

In our first word embedding model that incorporates
click feedback, we construct a corpus by taking only
clicked impressions from D and then mixing (q, t, d)
of each impression into a sentence. Then we simply
train CBOW and Skip-gram models on the corpus.

1We use only a single vector for a word unlike (Mikolov
et al., 2013b) where two vectors (“input” and “output”) for a
word are used. This halves the required space to store vectors
without performance loss.

485



4.2 Joint word embedding using click feedback

Although the CBOW and Skip-gram models trained on
a specially constructed corpus can capture signals from
both click feedback and word co-occurrence, they have
a couple of drawbacks. First, by ingesting only clicked
impressions we “waste” the large amount of negative
signals contained in the non-clicked impressions. Sec-
ond, the incorporation of indirect signals such as word
co-occurrences can be rather harmful for achieving ac-
curate click prediction as these are very noisy com-
pared to direct signals such as click feedback.

For our second word embedding model that incorpo-
rates click feedback, we define a joint word embedding
model that minimizes the following weighted logloss:

JWi(W ) = η(yi)
{
l(yi, qi, ti) + l(yi, qi, di)

}
, (6)

where the component loss function l(·, ·, ·) is defined
as follows:

l(y, a, b) =
|a|∑
k

|b|∑
l

− y log p(ak,bl)

− (1− y) log (1− p(ak,bl)) .
(7)

Here η(yi) is a function that returns a small weight η
only to negative examples:

η(yi) =

{
η, if yi = 0,
1, otherwise.

(8)

The fact that the user did not click on an ad does
not necessarily mean that the ad is not what the user
wanted; it often means that the ad is just less favored
than the clicked ads (Rendle et al., 2009). Since the
scope of this work is restricted to estimating click prob-
ability on a per-impression basis, we adopt a weighting
scheme rather than optimizing rank-based loss over a
set of related impressions.

4.3 Joint mean vector optimization

The joint word embedding models defined in the pre-
vious sections do not define how to aggregate a vari-
able length sequence of word vectors into a sentence
(or paragraph) vector to facilitate the computation of
sentence-level similarity scores. One approach to this
aggregation task is mean vector: simply average the
word-level embeddings across the sentence or para-
graph. As noted by (Tai et al., 2015), this approach
is a strong competitor to more complex models such
as RNNs or LSTMs despite its simple composition
method. However, this method may generate subop-
timal sentence vectors. With weight logloss, we aim
to optimize sentence vectors instead of individual word
vectors:

JMi(W ) = η(yi)
{
l̄(yi, qi, ti) + l̄(yi, qi, di)

}
, (9)

where the component loss function l̄(·, ·, ·) is defined
as follows:

l̄(y, a, b) =− y log p(µa,µb)
− (1− y) log (1− p(µa,µb)) ,

(10)

where µs returns the average vector for the multiword
string s, i.e. µs =

1
|s|
∑|s|

k sk.

5 Experiments
Data The data used in our experiments were collected
from a random bucket of the Yahoo! sponsored search
traffic logs for a period of 4 weeks in October 2014.
In the data there are approximately 65 million unique
users, 150 million unique queries and 12 million unique
ads. There are approximately 985 million ad impres-
sion events in total. We split the data into 3 partitions
with respect to time: the first 14 days’ data are used
for training word embedding models, the next 7 days’
data for training click prediction models, and the last 7
days’ data for testing. Table 1 presents more detailed
statistics for the data.
Models We are interested in evaluating the usefulness
of different word embedding models as features for
click prediction, and already have a very good baseline
system for this task. Consequently, in all experiments
below, we used the same personalized historical CTRs,
contextual and lexical features as the baseline system
described in Section 3. We tested models with the fol-
lowing additional features derived from word embed-
dings:

1. cosine similarity between the mean vectors of the
query and ad title

2. cosine similarity between the mean vectors of the
query and ad description

3. sum of 1 and 2

4. sigmoid function value for the dot product of the
mean vectors of the query and ad title

5. sigmoid function value for the dot product of the
query and ad description

6. sum of 4 and 5

All these continuous features are quantized into 50
bins. We compared five different word embedding al-
gorithms:

1. Skip-gram trained on Wikipedia (SK-WIKI)

2. Skip-gram trained on clicked impressions (SK-
CI), see Section 4.1

3. CBOW trained on clicked impressions (CB-CI),
see Section 4.1

4. Joint individual word vector embedding (JIWV),
see Section 4.2

486



Embedding Train Test Total
Number of impressions 489M 252M 244M 985M
Number of unique queries 82M 46M 45M 150M
Number of unique ads 8M 6M 6M 12M
Number of unique users 40M 25M 25M 65M
Number of unique words in query 6757K 6039K 5774K 16028K
Number of unique words in ad title 3371K 2586K 2557K 4598K
Number of unique words in ad description 1993K 1583K 1589K 2628K
Number of words in query 1600M 830M 800M 3230M
Number of words in ad title 3260M 1690M 1635M 6585M
Number of words in ad description 4311M 2206M 2131M 8650M
Average number of words in query per impression 3.275 3.284 3.289 3.281
Average number of words in ad title per impression 6.667 6.708 6.706 6.687
Average number of words in ad description per impression 8.818 8.759 8.741 8.784

Table 1: Data description

5. Joint mean vector embedding (JMV), see Sec-
tion 4.3

We used the skip-gram mode of word2vec 2 with a win-
dow size of 5 and negative sampling to train the SK-
WIKI model. For all other models we used in-house
implementation which employs AdagradRDA (Duchi
et al., 2011) to minimize the loss functions introduced
in Section 4. The dimension of a word vector is set
to 100 for all algorithms3. We removed a set of pre-
fixed stop-words and all words occurring fewer than
100 times; the resulting vocabulary comprised 126K
unique words. To process our web-scale data, we im-
plemented a multithread program where each thread
randomly traverses over a partition of the data D to
compute gradients and update the matrix W stored in a
shared memory. For computational efficiency, the hog-
wild lock-free approach (Recht et al., 2011) is used.
We set η in Eq. 8 and Eq. 9 to 0.2 through grid search
based on two-fold cross-validation. This small value
indeed verifies the idea of weak negative feedback for
unclicked impressions.

In order to circumvent the severe influence of ad po-
sition on the click prediction model, only the impres-
sions that are placed at the top position were used for
training the click prediction model. Note that CTR
at other positions can be derived from that of the ad
at the top position through scaling. Dembczynski et
al. (2008) showed that CTR can be decomposed as a
product of the probability of an ad getting clicked given
its being seen and the probability of an ad being seen at
a particular position.

Results We ordered query-ad pairs by the predicted
score to compute AucLoss (i.e. 1 - AUC where AUC
is the area under the Receiver Operating Characteris-
tic (ROC) curve). The ROC AUC is known to have a
correlation with the quality of ranking by the predicted
score (Fawcett, 2006); thus is one of the most important

2Available at https://code.google.com/p/word2vec/
3Higher vector dimensions such as 200 were also tried but

did not give a significant improvement.

metrics for click prediction (McMahan et al., 2013).

AucLoss Reduction (%)
Baseline + SK-WIKI 0.530
Baseline + SK-CI 0.595
Baseline + CB-CI 0.656
Baseline + JIWV 2.276
Baseline + JMV 4.114

Table 2: Comparative evaluation results in AucLoss re-
duction from the baseline system

Our experimental results (Table 2) show that the use
of features derived from our proposed word embed-
ding models significantly reduce AucLoss by up to
4.1%. For commercial search engines which have a
very strong baseline AucLoss, a reduction of 1% can be
considered large (McMahan et al., 2013). Moreover, as
expected, the more issues (as identified in Section 4) an
algorithm addresses, the better performance it achieves.
From the comparison between the SK-WIKI model and
the rest, we can recognize the importance of word em-
bedding models specialized to domain text and super-
vision signals. Also the difference between the CB-CI
(SK-CI) model and the JIWV model indicates the nois-
iness of indirect signals such as word co-occurrence
compared to direct signals like click feedback. Finally
the gap between the JIWV and JWV models highlights
the significance of considering compositionality in the
word embedding training process.

Eyeballing the most similar words to several queries
in the vector space is often helpful for getting some
sense about how different methods influence the result-
ing vector spaces. Table 3 lists the top 20 most similar
words to the query “metal watch.” The top words for
the SK-WIKI model are not semantically interesting in
terms of capturing the intent or desires. This clearly
shows the limitation of word embedding methods that
only rely on indirect signals (i.e. word co-occurrences)
from a generic text corpus (e.g. Wikipedia) for spon-
sored search click prediction. Noticeably the methods

487



SK-WIKI CB-CI JIWV JMV
watch (0.733) metal (0.718) wwhl (0.711) tacticalwatch.com (0.701)
grind (0.687) previews (0.652) cbs (0.704) watchrepairsusa.com (0.695)
grease (0.682) yidio.com (0.648) station (0.668) omegas (0.689)
kites (0.676) episodes (0.633) putlocker.com (0.662) watchco.com (0.682)

hammer (0.675) steel (0.626) iwatch (0.659) wach (0.670)
spinning (0.672) whatch (0.615) kidizoom (0.658) station (0.656)
flashing (0.671) bobcometal.com (0.586) freesports360.com (0.658) shockwarehouse.com (0.628)

trash (0.670) ridiculousness (0.582) tedtalks (0.655) freesports360.com (0.618)
flame (0.669) www.abc.com (0.574) 11/10c (0.650) akribos (0.616)

flaming (0.665) a&e (0.573) movie2k (0.640) 18mm (0.616)
cigar (0.664) utube (0.570) nfl.com/now (0.637) narutoget.com (0.614)

home-made (0.663) instantly (0.565) criminalsgonewilddvd (0.631) watchstation.com (0.611)
glow (0.662) khoobsurat (0.562) espnnfllive.com (0.631) authenticwatches.com (0.610)

bouncing (0.662) outnumbered (0.561) foxsports1 (0.623) interrupted (0.603)
filler (0.662) itv (0.559) viooz (0.623) criminalsgonewilddvd (0.601)

smoke (0.660) premiere (0.556) bubble (0.623) whatch (0.600)
shoot (0.658) films (0.555) wewood (0.621) $109.99 (0.597)
scoop (0.653) stainless (0.554) westclox (0.617) tirebuyer.com (0.596)
noises (0.652) fabricators (0.550) potlocker (0.613) skagen.com (0.588)

rocking (0.651) fabrication (0.550) nickelodeon (0.613) wewood (0.584)

Table 3: Top 20 most similar words to “metal watch”

SK-WIKI CB-CI JIWV JMV
costumes (0.762) princess (0.833) costumes (0.831) new-costumes.com (0.815)

bride (0.723) costumed (0.814) wonder (0.784) coustume (0.808)
princesses (0.722) customes (0.808) sweetiegames.com (0.782) namefully.com (0.800)

serene (0.708) costume (0.806) cleopatra (0.753) $35.90 (0.789)
highness (0.676) costomes (0.806) new-costumes.com (0.752) 2-days (0.781)

bess (0.674) costimes (0.803) girls.simple (0.749) $36.90 (0.776)
princess]] (0.671) m.buycostumes.com (0.800) werewolf (0.749) costume (0.766)

attire (0.670) custumes (0.796) yoshi (0.747) $28.90 (0.764)
princess (0.662) officialprincesscostumes.com (0.792) leia (0.747) princess (0.758)
dresses (0.662) custume (0.789) merida (0.742) cistumes (0.756)

highness (0.658) coustome (0.789) $49.90 (0.735) spider-woman (0.756)
jewels (0.652) cosyumes (0.787) low-budget (0.727) the-wristband-factory.com (0.750)

wedding (0.651) coustume (0.786) babies (0.727) $17.90 (0.750)
robes (0.648) coustums (0.786) fembot (0.726) sugarsmascotcostumesċom (0.748)
prince (0.644) buycostumes.com (0.785) costums (0.722) cotumes (0.748)
clothes (0.644) codtume (0.784) $3.90 (0.721) coneheads (0.747)
dancing (0.644) leia (0.784) hermione (0.718) $23.90 (0.739)
sophie (0.640) coustumes (0.784) supergirl (0.715) $7.90 (0.739)

consorts (0.639) costums (0.783) toothless (0.713) costomes (0.738)
glamorous (0.639) coatumes (0.782) starlord (0.709) $19.90 (0.737)

Table 4: Top 20 most similar words to “princess costumes”

that incorporate click feedback find more words related
to products, services or websites instead of just con-
ceptuatlly related words. Given that real products or
services can be regarded as the best possible surrogates
to user intents and desires, this demonstrates the effec-
tiveness of our methods. This tendency gets stronger as
a method takes into account both positive and negative
click feedback.

Another very interesting observation comes from the
fact that none of the methods except JMV successfully
captures the composite meaning of “metal watch”; they
tend to either find related words separately for each
query word (e.g. “watch” is strongly associated to the
sense of watching something like movie or other types
of video) or find totally unrelated words (particularly
SK-WIKI). This demonstrates that it is crucial to ad-
dress compositionality in the very process of learning

word vectors.
Table 4 shows the top 20 most similar words to the

query “princess costumes.” In this example we can spot
another surprising result. The JMV model pushes a lot
of price related expressions to the top4. This may im-
ply that many parents search for lower cost costumes,
clearly showing a clear psychological desire in the fi-
nancial dimension. This observation confirms the find-
ings in (Wang et al., 2013) about the significant role of
certain ad expressions in triggering users’ psychologi-
cal desires. We also note that the CB-CI model returns
a lot of misspells for “costume(s)”, which would not
be possible with simple lexical features of the baseline
system. A close look at this example generally con-
firms the observations we made for the previous ex-

4We have not tried any normalization for numbers but it
might be worth doing given the important role they play.

488



SK-WIKI CB-CI JIWV JMV
kids (0.724) game (0.629) kids (0.795) program (0.764)

pac-man (0.708) gams (0.615) kidsfootlocker.com (0.788) scorekeepers (0.757)
games (0.703) games (0.613) flight: (0.754) gamingchairs.hayneedle.com (0.754)
pinball (0.687) petrasplanet.com (0.601) edit: (0.744) teepees (0.752)

boy (0.680) for (0.590) raz (0.737) nn2 (0.750)
adventure (0.664) gamse (0.584) program (0.733) game (0.749)

roleplaying (0.664) ganes (0.574) abercrombiekids.com (0.729) girls (0.746)
quests (0.658) collectors (0.574) sumdog (0.721) cranium (0.746)
d&d (0.658) awsome (0.551) take20 (0.702) ggg (0.738)
genie (0.655) kid (0.550) freegamesdownload.com (0.699) pac-man (0.732)

solitaire (0.654) game.com (0.537) program; (0.695) kidsfootlocker.com (0.730)
gamers (0.653) g2u.com (0.536) gromsocial.com (0.694) equestriagirls.hasbro.com (0.721)

games=== (0.651) bouncing (0.533) softschools (0.693) kids (0.718)
game; (0.649) for.kids (0.532) gapkids (0.691) mahjong (0.718)

consoles]] (0.645) catching (0.528) downnload (0.682) tvo (0.717)
in-game (0.645) gamesfreak (0.519) edheads (0.681) sumdog (0.701)

rpg (0.644) minecraft (0.516) ruum.com (0.674) cpm.wargaming.net (0.699)
wargames (0.644) firetruck (0.514) gamestop (0.672) osgood-schlatter (0.698)

game]] (0.643) games: (0.513) a&f (0.668) gamestop (0.697)
fast-paced (0.641) todlers (0.512) pac-man (0.663) $4.01 (0.695)

Table 5: Top 20 most similar words to “game for kids”

ample. Finally Table 5 shows top the 20 most similar
words for the query “game for kids.” Once again we
found the same analysis holds for this case.

6 Conclusions

In this paper we explored the use of word embedding
techniques to overcome the shortcomings of traditional
lexical features for ad click prediction in sponsored
search. We identified several potential weaknesses of
the plain application of conventional word embedding
methodologies: the lack of the right machinery to har-
ness both positive and negative click feedback, the lim-
ited utility of pure word co-occurrence signals, and no
consideration of vector composition in the word em-
bedding training process. We proposed a set of new
implicit feedback-based joint word embedding meth-
ods to address those issues. We evaluated the new word
embedding methods in the context of a very good base-
line click prediction system, on a large scale data set
collected from Yahoo! search engine logs. Our exper-
imental results clearly demonstrate the effectiveness of
the proposed methods. We also presented several ex-
amples for qualitative analysis to advance our under-
standing on how each algorithm really contributes to
the improved performance. To the best of our knowl-
edge this work is the first successful application of
word embedding techniques for the sponsored search
task.

There are multiple interesting research directions for
future work. One of these directions is to extend the
vocabulary by identifying significant phrases (as well
as words) before training word vectors. Hillard et
al. (2011) employed Conditional Random Fields to di-
vide queries with multiple words into segments and
collected historical CTR on the segment level. We also
like to investigate more structured embedding methods
such as RNNs (probably for ad descriptions). In case

the computational cost of such methods are too high to
be practical for sponsored search, we can employ them
only for a small fraction of ads filtered by faster meth-
ods.

It may be possible to deal with the implicit nega-
tive feedback of unclicked ad impressions in a more
principled way by adopting ranking-based loss func-
tions. However, this is only possible with the extra cost
of identifying and aggregating related ads into a single
transaction.

Though not directly related to NLP, yet another
promising direction is to jointly embed not only text
data but also a variety of user activities (e.g., organic
search results, mobile app usages, other daily activities)
all together in the same vector space. Since many of the
different sources contain their own unique information,
we might be able to obtain a much better understand-
ing about the user state and intent through this rich joint
embedding space. Joint embedding with rich informa-
tion can also help us to perform automatic clustering of
users, eventually leading to powerful smoothing meth-
ods for personalized historical CTR statistics.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Dont count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics, volume 1, pages
238–247.

Phil Blunsom, Edward Grefenstette, Nal Kalchbrenner,

489



et al. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics. Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics.

Haibin Cheng and Erick Cantú-Paz. 2010. Person-
alized click prediction in sponsored search. In Pro-
ceedings of the Third ACM International Conference
on Web Search and Data Mining, WSDM ’10, pages
351–360, New York, NY, USA. ACM.

K. Dembczynski, W.Kotlowski, and D.Weiss. 2008.
Predicting ads click-through rate with decision rules.
In Proceedings of WWW 08.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

David Easley and Jon Kleinberg. 2010. Networks,
Crowds, and Markets: Reasoning about a Highly
Connected World. Cambridge University Press.

Benjamin Edelman, Michael Ostrovsky, Michael
Schwarz, Thank Drew Fudenberg, Louis Kaplow,
Robin Lee, Paul Milgrom, Muriel Niederle, and
Ariel Pakes. 2005. Internet advertising and the
generalized second price auction: Selling billions of
dollars worth of keywords. American Economic Re-
view, 97.

Tom Fawcett. 2006. An introduction to ROC analysis.
Pattern Recognition Letters, 27(8):861–874, June.

Andrea Frome, Greg S Corrado, Jon Shlens, Samy
Bengio, Jeff Dean, Tomas Mikolov, et al. 2013.
Devise: A deep visual-semantic embedding model.
In Advances in Neural Information Processing Sys-
tems, pages 2121–2129.

Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavl-
jevic, Fabrizio Silvestri, and Narayan Bhamidipati.
2015. Context- and content-aware embeddings for
query rewriting in sponsored search. In Proceedings
of the 38th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
SIGIR ’15, pages 383–392, New York, NY, USA.
ACM.

Dustin Hillard, Eren Manavoglu, Hema Raghavan,
Chris Leggetter, Erick Cantú-Paz, and Rukmini Iyer.
2011. The sum of its parts: reducing sparsity in
click estimation with query segments. Inf. Retr.,
14(3):315–336.

Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.

Andrej Karpathy and Li Fei-Fei. 2014. Deep visual-
semantic alignments for generating image descrip-
tions. arXiv preprint arXiv:1412.2306.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.

H. Brendan McMahan, Daniel Golovin, Sharat
Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar
Hrafnkelsson, Tom Boulos, Jeremy Kubica, Gary
Holt, D. Sculley, Michael Young, Dietmar Ebner,
Julian Grady, Lan Nie, Todd Phillips, and Eugene
Davydov. 2013. Ad Click Prediction: a View
from the Trenches. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining - KDD ’13, page
1222, New York, New York, USA, August. ACM
Press.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.

Benjamin Recht, Christopher Re, Stephen Wright, and
Feng Niu. 2011. Hogwild: A Lock-Free Approach
to Parallelizing Stochastic Gradient Descent. In Ad-
vances in Neural Information Processing Systems,
pages 693–701.

Steffen Rendle, Christoph Freudenthaler, Zeno Gant-
ner, and Lars Schmidt-Thieme. 2009. Bpr:
Bayesian personalized ranking from implicit feed-
back. In Proceedings of the Twenty-Fifth Conference
on Uncertainty in Artificial Intelligence, UAI ’09,
pages 452–461, Arlington, Virginia, United States.
AUAI Press.

Matthew Richardson. 2007. Predicting clicks: Esti-
mating the click-through rate for new ads. In In Pro-
ceedings of the 16th International World Wide Web
Conference (WWW-07, pages 521–530. ACM Press.

Richard Socher, Yoshua Bengio, and Christopher D.
Manning. 2012a. Deep learning for nlp (without
magic). In Tutorial Abstracts of ACL 2012, ACL
’12, pages 5–5, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012b. Semantic composition-
ality through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical

490



Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201–
1211. Association for Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the conference on empirical
methods in natural language processing (EMNLP),
volume 1631, page 1642.

Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.

Ilya Trofimov, Anna Kornetova, and Valery Topinskiy.
2012. Using boosted trees for click-through rate pre-
diction for sponsored search. In Proceedings of the
Sixth International Workshop on Data Mining for
Online Advertising and Internet Economy, ADKDD
’12, pages 2:1–2:6, New York, NY, USA. ACM.

Taifeng Wang, Jiang Bian, Shusen Liu, Yuyu Zhang,
and Tie-Yan Liu. 2013. Psychological advertis-
ing: exploring user psychology for click prediction
in sponsored search. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 563–571.
ACM.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph and text jointly em-
bedding. In Proceedings of the Empiricial Methods
in Natural Language Processing (EMNLP 2014).

Min-Chul Yang, Nan Duan, Ming Zhou, and Hae-
Chang Rim. 2014. Joint relational embeddings for
knowledge-based question answering. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
645–650.

491


