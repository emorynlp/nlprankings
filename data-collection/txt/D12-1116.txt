










































Entity based QA Retrieval


Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1266–1277, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics

Entity based Q&A retrieval

Amit Singh
IBM Research

Bangalore, India
amising3@in.ibm.com

Abstract

Bridging the lexical gap between the user’s
question and the question-answer pairs in the
Q&A archives has been a major challenge for
Q&A retrieval. State-of-the-art approaches
address this issue by implicitly expanding the
queries with additional words using statistical
translation models. While useful, the effec-
tiveness of these models is highly dependant
on the availability of quality corpus in the ab-
sence of which they are troubled by noise is-
sues. Moreover these models perform word
based expansion in a context agnostic manner
resulting in translation that might be mixed
and fairly general. This results in degraded
retrieval performance. In this work we ad-
dress the above issues by extending the lex-
ical word based translation model to incor-
porate semantic concepts (entities). We ex-
plore strategies to learn the translation proba-
bilities between words and the concepts using
the Q&A archives and a popular entity cata-
log. Experiments conducted on a large scale
real data show that the proposed techniques
are promising.

1 Introduction

Over the past few years community-based ques-
tion answering (CQA) portals like Naver, Ya-
hoo! Answers, Baidu Zhidao and WikiAnswers
have attracted great attention from both academia
and industry (Adamic et al., 2008; Singh and
Visweswariah, 2011). These portals foster collab-
orative creation of content by allowing the users to
both submit questions to be answered and answer

questions asked by other users. These portals aim
to provide highly focused access to this information
by directly returning pertinent question and answer
(Q&A) pairs to the users questions, instead of a long
list of ranked URLs. This is in noted contrast to the
usual search paradigm, where the question is used to
search the database of potential answers, in this case
the question is used to search the database of pre-
vious questions, which in turn are associated with
answers. This involves addressing the word mis-
match problem between the users question and the
question-answer pairs in the archive. This is the ma-
jor challenge for Q&A retrieval.

Researchers have proposed the use of translation
models (Berger and Lafferty, 1999; Jeon et al., 2005;
Xue et al., 2008) to solve this problem. As a princi-
pled approach to capturing semantic word relations,
statistical translation language models are built by
using the IBM model 1 (Brown et al., 1993) and
have been shown to outperform traditional docu-
ment language models on Q&A retrieval task. The
basic idea is to estimate the likelihood of translat-
ing a document1 to a query by exploiting the depen-
dencies that exists between query words and doc-
ument words. For example the document contain-
ing the word Wheezing may well answer the ques-
tion containing the term Asthma. They learn the
these dependencies (encoded as translation proba-
bilities) between words using parallel mono-lingual
corpora created from the Q&A pairs. While useful,
the effectiveness of these models is highly depen-
dant on the availability of quality corpus (Lee et al.,

1we will use (Q&A, document), (word, term) and (user
query, question) interchangeably

1266



Figure 1: Need for entity based expansions

2008). Also these models only capture shallow se-
mantics between words via the co-occurrence statis-
tics, while some of the more explicit relationships
between words and entities is freely available exter-
nally. Being context agnostic (Zhou et al., 2007) is
another very common criticism hailed on translation
models as it results in noisy and generic translations.

Example shown in Figure 1 captures these prob-
lems. Specifically, the word Blizzard can
refer to an American game development com-
pany that develops World of Warcraft game or
it could refer to a severe snowstorm. Expand-
ing query without taking the gaming context es-
tablished by the word WOW (acronym for World
of Warcraft) into account would lead to topic
drift. Also it would be difficult to learn relation-
ships between World of Warcraft Burning
Crusade and Blizzard from the Q&A corpus
alone due to the sparsity of co-occurance counts as
these can be expressed in several lexical forms, some
of which are multi word phrases.

In this paper we argue that solution to all the
above problems lies in a unified model in which en-
tities are a primary citizen. The guiding hypothesis
being, an entity based representation provides a less
ambiguous representation of the users question and
provides for a more semantically accurate expansion
if the relationship between entities and words can be
estimated more reliably. Our main contributions are

1. We propose Entity based Translation Language

Model (ETLM) for Q&A retrieval that accom-
modates semantic information associated be-
tween entities and words. Being closely re-
lated to the general source-channel framework
(Berger and Lafferty, 1999), the model enjoys
its benefits, while mitigating some of its short-
comings. Specifically it provides for context
aware expansions of the query by exploiting
entity annotations on both, the document and
the query side. Entity annotations also provide
a means to handle the “many-to-one” (Moore,
2004) translation limitation in the IBM model,
due to which each word in the target document
can be generated by at most one word in the
question2. For the same reasons, it also al-
leviates another related limitation by enabling
translation between contiguous words across
the query and documents (Moore, 2004).

2. We learn relationships between entities and
terms by proposing new ways of organiz-
ing monolingual parallel corpus and simul-
taneously leveraging external resources like
Wikipedia from which one can derive these re-
lationships reliably. This helps alleviate the
noise problem associated with learning transla-
tion models on Q&A archive described above.
An important point to note is that, our tech-
nique has merits independent to the choice
of the entity catalog. In this work we use

2entity mentions can be of more than unit word length

1267



D original Q&A collection
E set of all entities in catalog
d(e) description of entity e
C D annotated with e ∈ E
quser users question
q quser annotated with e ∈ E
t token span

tq, td token span in q and d
V word vocabulary

Table 1: Notation.

Wikipedia, as it is a popular choice due to its
large and ever expanding coverage and its abil-
ity to keep up with world events on a timely
basis.

3. We provide detailed evaluation of impact of
modelling assumptions and model components
on retrieval performance on a large scale real
data from Yahoo Answers comprising ∼5 mil-
lion Q&A pairs.

Rest of the paper is organized as follows: In the
next section, we define ETLM and outline its de-
tails. This is followed by Section 3 which gives
the details of entity annotators and its performance.
Section 4 describes our experiments on the retrieval
method used Q&A retrieval. In Section 5 we com-
pare and contrast related literature. Finally, we con-
clude in Section 6.

2 Our Approach

Problem Definition: Let D = d1, d2, ..., dn denote
the Q&A collection. Here di refers to the i-th Q&A
data consisting of a question qi and its answer ai.
Given the user question quser, the task of Q&A
retrieval is to rank di according to score(quser, di).
Figure 2 outlines the approach to compute this score
in the ETLM framework.

Offline processing: Using the entity catalog E, we
learn the entity annotation models EAoffline and
EAonline for annotation of entities in the Q&A cor-
pus and the query respectively. Refer Section 3 for
details. For each di ∈ D, we then annotate refer-
ences to entities in Wikipedia using EAoffline re-

sulting in annotated Q&A corpus C. We then com-
pute relationships between entities and words using
C and E. These relationships are used to learn our
ETLM model.
Online processing: At runtime, annotate the user
query quser with entities using EAonlineto create an
enriched question q. Issue this query over the an-
notated corpus C and rank the candidates as per the
ETLM model described below.

2.1 ETLM Model
Let the annotated query q (and similarly annotated
Q&A pair d) be composed of sequence of token
spans Tq (and Td). Each token span tq (similarly
td) corresponds to sequence of contiguous words oc-
curring in the running text. These tq’s can corre-
spond to entity mentions, phrases or words. Let eq
denote the tokens spans that are annotated and neq
that are not (Tq = eq ∪ neq). For example, in the
query , What︸ ︷︷ ︸

neq

is︸︷︷︸
neq

a︸︷︷︸
neq

Quadratic Formula︸ ︷︷ ︸
eq

?,

token span Quadratic Formula is linked to an
entity corresponding to Quadratic Equation3, while
all other token spans are marked as neq .

For the sake of simplicity, in this work we do
not identify phrases i.e. neq is always of unit word
length4. In the ETLM framework, the similarity be-
tween a query q and a document dwithin a collection
C is given by the probability

score(q, d) ∼ P (q|d) =
∏
tq∈q

tq=eq∪neq

P (tq|d)

P (tq|d) = (1− λ)Pml(tq|d) + λPml(tq|C)
Pml(tq|d) =

∑
td∈d

T (tq|td)Pml(td|d) (1)

Intuitively this indicates a generative process for cre-
ating q from d. Ideally both q and d are “only” com-
posed of e i.e. ∀tq ∈ q; tq ∈ EU , where EU is the
universal set of entities 5 (similarly for all td). This
is because when the document was created, each and
every td ∈ d had a sense attached to it. however in
reality, for various reasons, set of target entities are
clearly a subset of EU (for e.g. E: set of all entities

3http://en.wikipedia.org/wiki/Quadratic equation
4its not a restriction as the model is valid for neq consisting

of more than one word.
5language for creating q and d

1268



Figure 2: ETLM Architecture (gray and brown arrows indicate offline and online processes respectively

in the catalog) also not all of them may be recog-
nized by the annotation system.
T (tq|td) in Equation 1 denotes the probability

that a token span tq is the translation of token span
td. This induces the desired query expansion effect.
The key task is to estimate Pml(tq|C), T (tq|td) and
Pml(td|d); tq ∈ eq ∪ neq and td ∈ ed ∪ ned

2.2 Estimating Model Parameters
We adopt 2 different approach for estimating
T (tq|td), leading to 2 different configurations of
ETLM system . As the name suggests, ETLMqa is
estimated from Q&A data (C andD) while we lever-
age the entity catalog (in our case it is Wikipedia) for
ETLMwiki.

2.3 ETLMqa: Estimate from parallel corpus
Following (Xue et al., 2008) we pool the question
and answers from D to create a master parallel cor-
pus P = (q1, a1), .., (qn, an) ∪ (a1, q1), ., (an, qn).
This is used for learning T (ne|ne′)6. Similarly we
create P ∗ from C. We then derive 2 different paral-
lel corpora from P and P ∗ as follows

Pentity We remove all non linked tokens ne from
P ∗ thereby reducing it to parallel corpus over e.
This is used for learning T (e|e′) i.e. translation
probabilities between two entities e and e′ inE.

Phybrid This is hybrid of Pentity and P where in
one part of Q&A pair consists on only newhile
other consists of only e. This is used for learn-
ing T (ne|e) and T (e|ne).

To handle entities e, we introduce special id’s in the
ne space. Thus our universal token span set is given

6subscript of q and d has been dropped as translation proba-
bility learnt agnostic to it, due to pooling.

by V ∪ E. This is done so that T (tq|td) is learnt
from P , Pentity and Phybrid, w/o any modification
to the corresponding translation algorithm (Brown
et al., 1993). Lets call this approach ETLMqa′ .

We explored another intuitive approach ETLMqa,
to learn T (e|e′), T (e|ne), T (ne|e) and T (ne|ne′)
directly by using only P ∗ as our parallel corpus.
We do so by redistributing the probability mass
i.e. when calculating T (e|e′), we redistribute prob-
ability mass spread over all the ne to e given by
Equation 2 and 3. Similar process is followed for
T (e|ne), T (ne|e) and T (ne|ne′).

S(e|e′) = T (e|e
′)∑

t∈V T (t|e′)
(2)

T (e|e′) = S(e|e
′)∑

t∈E T (t|e′)
(3)

Remaining model components are calculated using
Equation 4 and 5. Here d refers to question part of
the Q&A pair.

Pml(tq|C) =
tftq ,C + 1∑

t′∈C tft′,C + |C|
(4)

Pml(tq|d) =
tftq ,d∑
t′∈d tft′,d

(5)

2.4 ETLMwiki: Estimating from Wikipedia

Number of symmetric measures have been pro-
posed (Medelyan et al., 2009) to measure seman-
tic relationships between entities and words using
Wikipedia. For our problem we need an asym-
metric measure. We use co-citation information
in Wikipedia to detect relatedness between enti-
ties (T (e|e′)) and co-occurrence counts to estimate

1269



T (ne|ne′) as follows: .

T (e|e′) = co(e, e
′)∑

e′′ co(e
′′, e′)

(6)

T (ne|ne′) = cf(ne, ne
′)∑

ne′′ cf(ne
′′, ne′)

(7)

T (ne|e) =
tfne,D(e) + 1

|D(e)|+ |V |
(8)

T (e|ne) =
tfne,D(e) + 1∑

e′∈E tfne,D(e′) + |E|
(9)

Here d(e) represents the page corresponding to
entity e. D(e) represents concatenation of d(e)
and all context of size 5 surrounding anchor text in
Wikipedia that link to e. cf(ne, ne′) is the number
context windows of fixed size containing both
ne and ne′ in Wikipedia. In our case, we set the
window size at 10 (because this size turned out to be
reasonable in our pilot experiments). tft,d(e) is the
frequency of t in d(e); co(e, e′) indicates number
of entities in Wikipedia that have a hyperlink to
both e and e′. As links from pages with a small
number of outgoing links are generally considered
to be more valuable than links from pages with a
high outgoing link degree, we tried with weighted
version of 6 where the co-citations are weighted
by the outdegree of Wikipedia page corresponding
to entity s that link to e and e′. Lets denote the
weighted version by ETLMwiki and unweighted
version by ETLMwiki′ . Pml(tq|C) and Pml(tq|d)
are estimated as per Equation 4 and 5 respectively.

2.5 Self translation probability
To make sure self translation probability is not un-
derestimated i.e. T (t|t) ≥ T (t′|t) always holds
true, we introduce new parameter γ as T (t|t′) =
γ + (1− γ)T (t|t′); γ = 0 when t 6= t′ and γ > 0.5
otherwise.

2.6 ETLMcombo: Combining ETLMqa and
ETLMwiki

Often, combining language models yields better re-
sults than any of the individual language models
themselves. Linear interpolation is often the tech-
nique of choice in language modelling for combin-
ing models to exploit complementary features of the
component models. It involves taking a weighted

sum of the probabilities given by the component lan-
guage models. An advantage of the linear interpola-
tion is that it is simple and fast to calculate. If the
inputs are probability estimates, also the output is a
probability estimate. The mixture translation model
Tcombo(e|e′) over M component models is given by
Equation 10.

Tcombo(t|t′) =
M∑
j=1

αjTj(t|t′)

t ∈ E ∪ V ;
M∑
j=1

αj = 1; αj ≥ 0

(10)

One can immediately notice that Tcombo(t|t′) has
one global weight for each of the M component
models which might not be ideal. With access to
large training data one could employ more power-
ful context dependent interpolation techniques (Liu
et al., 2008). In our case we have 2 components
Tqa and Twiki and four classes for each ; α

(e,e′)
wiki

7,
α

(e,ne)
wiki , α

(ne,ne′)
wiki and α

(ne,e)
wiki ), one corresponding to

each class of T (t|t′). respectively.

3 Entity Annotation

In this section we describe our entity annotation
system. Recently there has been lot of work ad-
dressing the problem of annotating text with links
to Wikipedia entities (Mihalcea and Csomai, 2007;
Bunescu and Pasca, 2006; Milne and Witten, 2008;
Kulkarni et al., 2009; Ratinov et al., 2011; Ferrag-
ina and Scaiella, 2010). We adopt a similar ap-
proach, wherein we first find the best disambigua-
tion (BESTDISAMBIGUATION) for a given mention
and then decide to prune it (PRUNE), via the dummy
mapping NA (similar to “no assignment” (Kulkarni
et al., 2009)).

3.1 BESTDISAMBIGUATION
As defined earlier, e ∈ E represent an entity cor-
responding to URN of a Wikipedia article. Let
Em = {em,1, em,2, · · · , em,|Em|} em,i ∈ E repre-
sent the set of possible disambiguations for a men-
tion m (m is an index over all mentions in the cor-
pus). Given a mention m, task is to find best disam-
biguation e from Wikipedia. Without loss of gener-

7α
(e,e′)
qa = 1− α(e,e

′)
wiki

1270



ality, we consider em,∗ ∈ Em as the correct answer.
Let φ(m, em,j) represent the mapping onto features
between an entity mention m and the Wikipedia en-
tity em,j and −→ω be the corresponding weight vector
and D(em,j) = −→ω φ(m, em,j) represent the disam-
biguation score. The task is to learn −→ω such that
argmax
em,j

D(em,j) gives the best disambiguation for

the mention m.
We pose this as a ranking problem and solve

it using max-margin technique (Joachims, 2002;
Joachims, 2006) as follows

minimize
−→ω ,
−→
ξ

1

2
−→ω · −→ω + C

∑
ξi,j

subject to

∀m,∀em,j ∈ Em − em,∗ :

−→ω φ(m, em,∗) > −→ω φ(m, em,j) + ξi,j
∀i,∀j : ξi,j ≥ 0

(11)

where
∑
ξi,j is the total training error that upper

bounds the number of pair preferences violations.
This is controlled by adjusting the parameter C. Note
that Equation 11 means pairwise comparison be-
tween the correct disambiguation em,∗ and other dis-
ambiguation candidates em,j such that j 6= index
corresponding to *.

3.2 PRUNE
The disambiguation phase produces one candidate
disambiguation per mention. To discard any un-
meaningful annotations a simple strategy similar to
LOCAL (Kulkarni et al., 2009) is followed where the
D(em,∗) is compared against a predefined threshold
ρna, so that if D(em,∗) < ρna then that annotation
for menton m is discarded by linking m to NA. The
parameter ρna allows the algorithm to back-off when
short of evidence.

3.3 FEATUREMAP φ(m, em,j)
Sense probability prior (SP): It represents the prior
probability that a mention name s points to a specific
entity in Wikipedia. For example, without any other
information, mention name “tree” will more likely
refer to the entity woody plant8, rather than the less

8en.wikipedia.org/wiki/Tree

popular notion related to graphs 9.
Entity Probability prior (EP): It captures the pop-
ularity knowledge as a distribution of entities, i.e.,
the EP (ei) should be larger than EP (ej) if ei is
more popular than ej . This score is independent of
the mention name.
Context specific features: It captures the tex-
tual similarity between weighted word vectors cor-
responding to the context of the mention (window
around the mention) and textual description associ-
ated with the entity (Wikipedia page).

Let EAonline and EAoffline represent config-
urations for annotating user question and corpus
respectively. For EAonline, user question repre-
sents the document from which context specific
features are computed. For EAoffline, question
and the answer(best) is concatenated to represents
the document. Based on the “one sense per dis-
course” assumption, one additional heuristic is used
in EAoffline where, for the same Q&A pair, if same
name mention is repeated multiple times across the
question and the answer then one with the maximum
D(em,∗) > ρna is annotated for all instances.

3.4 Annotation Experiments

We used 2010 version of Wikipedia as our knowl-
edge base. It contains more than 2.5 million en-
tities. Annotations were done by volunteers fluent
in english. Volunteers were told to be as exhaus-
tive as possible and tag all possible name mentions,
even if to mark them as ”NA”. Inter-annotator agree-
ment=92.1%; Kappa coefficient = 0.72. As our cor-
pus, we collected 8.3K manual annotations spanning
1315 Q&A pairs. 2.8K of the annotations were as-
signed to NA. 2.1K annotations (out of 8.3K) were
made in the question of which 551 were assigned to
NA. We use Precision, Recall and F1 score micro-
averaged across documents as the evaluation mea-
sures. We do a linear scan of data to identify en-
tity mentions by first tokenizing and then identify-
ing token sequences that maximally match an en-
tity ID in the entity name dictionary (constructed
using Wikipedia anchor text, redirect pages). Fig-
ure 3 outlines the performance of EAoffline and
EAonline. We measured EAoffline in 3 test data
configurations; (1) EAoffline: measured over entire

9en.wikipedia.org/wiki/Tree (data structure)

1271



Figure 3: Precision v/s Recall

annotation set; 2) EAoffline′ is measured only on
annotations made in question. this is done to com-
pare it with EAonline; 3) EAoffline∗ is similar to
(2), only difference is that for (2) entire Q&A pair
is the context, while here only question part is the
context. This is done to check if separate annotators
are required for online and online phase. As seen
in Figure 3, this indeed is necessary as EAoffline∗
performs worse than EAonline. Closer look at the
feature weights revealed that in EAoffline context
specific features have much more weightage when
compared to its weight in EAonline, on the contrary
EAonline weighs SP significantly higher.

4 Evaluation

We now describe the empirical evaluation where
we compare our techniques against the baseline
techniques. We use several standard measures (R-
Precision, MAP, MRR, Precision@k) in evaluation.
We first describe the dataset used followed by de-
scribing an exhaustive set of results across tech-
niques and performance measures.

4.1 Dataset

We crawled a dataset of ∼5 million questions and
answers from Yahoo! Answers spanning all the leaf
level categories. Tokenization and stop word re-
moval were the only preprocessing steps performed.
We have used a stoplist10 having a vocabulary of 429
common words to remove the stopwords.

In our retrieval experiments we used 339 queries
(average length 5.6 words). We employed pool-
ing technique used in the TREC conference series.

10http://truereader.com/manuals/onix/stopwords1.html

We pooled the top 25 Q&A pairs from retrieval re-
sults generated by varying the retrieval algorithms
and the search field. Relevance judgments were
marked by human annotators without disclosing the
identity of method used for retrieval. The annota-
tors were asked to label candidate as “relevant” or
“irrelevant” based on semantic similarity with the
query. Answer quality/correctness was not a crite-
ria. In case of disagreement between two volunteers,
authors made the final judgment. Inter-annotator
agreement was 87.9% and Kappa coefficient = 0.68.
Over all we had collected more than 12K relevance
judgements corresponding to these queries, of which
>2.3K were marked as relevant.

4.2 Baselines

To evaluate the effectiveness of our models we
compared them against the following baselines

Traditional models: VSM (Zobel and Moffat,
2006) and OKAPI BM25 (Robertson et al., 1996)
(k1, b, and k3 are parameters that are set to 1.2, 0.75
and∞ respectively).
Translation based language models: TLM (Jeon
et al., 2005), TransLM (using answers) (Xue et al.,
2008) and CTM (Lee et al., 2008).

For our experiments we used a set of 50 queries to
select the model parameters. Translation based lan-
guage model use 2 parameters; smoothing parameter
λ in the Language Model and β to control the self-
translation impact in the TransLM. Final values of
parameters used in our experiments were λ = 0.2
(Zhai and Lafferty, 2004) and β = 0.75 (Xue et
al., 2008). For CTM, we used tf-idf based weigh-
ing scheme (Lee et al., 2008) to remove words from
the (Q‖A) corpus P . Word elimination threshold of
20% was selected based on the above 50 queries. Fi-
nal values of ETLM parameters used in our experi-
ments were λ = 0.18 and γ = 0.65.

4.3 Result Analysis

Table 2 presents the performance of the various tech-
niques. Under each measure, we highlight the best
performing technique. Performance of all the trans-
lation based models is better than VSM and OKAPI
thereby confirming the importance of addressing the
lexical gap. Using high confidence annotations for

1272



MAP %chg MRR %chg R-Prec %chg Prec@5 %chg Prec@10 %chg

VSM 0.221 0.421 0.21 0.202 0.15
OKAPI 0.298 0.532 0.271 0.264 0.214

TLM 0.337 0.583 0.318 0.297 0.239
TransLM 0.352 0.612 0.347 0.332 0.261
CTM 0.361 0.641 0.351 0.341 0.279

ETLMqa 0.390† 8.03 0.699† 9.05 0.379† 7.98 0.367† 7.62 0.302† 8.24
ETLMwiki 0.413† 14.40 0.719† 12.17 0.399† 13.68 0.391† 14.66 0.323† 15.77
ETLMcombo 0.427† 18.28 0.726† 13.26 0.413† 17.66 0.407† 19.35 0.331† 18.64

Table 2: Comparisons of retrieval models. † indicate a statistically significant improvement over the CTM using paired
t-test with p-value < 0.05. %chg indicates change over CTM as it is the most competitive baseline

query expansion in ETLM, leads to an improved
performance as compared to the all the baseline
methods that do not consider this signal. This is
validated by the fact that ETLMqa and ETLMwiki
can achieve statistically significant improvements in
terms of all the measures. The reason for this im-
provement is the context sensitive computation of
T (t|t′) leading to reduced spurious expansions and
improved top expansions, this is made possible be-
cause of entity disambiguation. This computation in
baselines happens on word by word basis without
exploiting contextual information. ETLMqa per-
forms worse than ETLMwiki. On close inspec-
tion of failure cases and translation probability ta-
bles we found that T (e|e′) for ETLMqa was much
worse than ETLMwiki. This is because for getting
good estimates of T (e|e′), we need enough instances
where both e and e′ need to be correctly annotated
in the same Q&A pair. Failure in this leads to sparse
counts thereby reducing the gap in T (e|e′) scores
for related and unrelated e. Figure 4 shows the
impact of choices made for learning the translation
probabilities T (t|t′). We found that ETLMwiki per-
forms slightly better than ETLMwiki′ , indicating the
utility of weighted co-citation measure for comput-
ing T (e|e′). We believe that embedding other mea-
sures that are better in capturing relationships from
Wikipedia, should improve the performance. Simi-
larly ETLMqa also performs better than ETLMqa′ .
This is because for creating Pentity all ne are re-
moved. This leads to count sparsity problem dis-

Figure 4: Performance of ETLM configurations

cussed above, but slightly worse. Due to absence
of ne, in ETLMqa′ e′ in d are thought be being
generated “only” from e in q. On the contrary in
ETLMqa, e′ had an option of mapping to ne in q.
An interesting observation is that while the perfor-
mances of different configurations vary, all of them
perform better than CTM which is the best baseline.

4.4 Impact of Annotations on retrieval

Since entities are central in our model, impact of
entity annotation on quser is one of the most im-
portant aspect to be studied. Figure 5 shows the
plot of retrieval measures obtained by varying ρna
in EAonline. CTM is shown by horizontal lines. As
explained in Section 3, value of ρna is inversely pro-
portional to aggressiveness of annotation. Which
implies for high values, EAonline will annotate only
those mentions in query that its highly confident
about. Beyond a value no annotations are made.

1273



Figure 5: Impact of query annotation on retrieval. x-axis
represents value of ρna used to control annotation

This is represented on the extreme right in Figure 5.
One important observation is that, even with no an-
notations made in query, performance of ETLMqa
and ETLMwiki is competitive to CTM. This is ev-
idence for addressing the noise related issue for
which CTM is designed. For large range of values,
all ETLM configurations are above CTM. As we de-
crease ρna performance increases smoothly and then
after a certain point (ρna = 5) is starts decreasing.

4.5 ETLMcombo
We believe that T (t|t′) learnt from one source would
encode word association characteristics which might
not be exactly the same across sources.ETLMcombo
tries to address this by combining the two models.
Values for mixing parameters are : α(e,e

′)
wiki = 0.95

11,
α

(e,ne)
wiki = 0.75, α

(ne,ne′)
wiki = 0.7 and α

(ne,e)
wiki = 0.75).

The interpolation weights were obtained by optimiz-
ing the retrieval performance by doing a using grid
search over the parameter space. Same 50 queries
were used for tuning. As seen entity relationships
obtained from Wikipedia are far superior to one
from Q&A corpus. As seen in Table 2 combining
the two models improves the performance.

5 Related Work

Recently Q&A retrieval has been garnering lot of at-
tention. Translation model (TLM) (Jeon et al., 2005)
has been extensively employed in question search
and has been shown to outperform the traditional IR
methods significantly (VSM, BM25, LM). Existing

11α
(e,e′)
qa = (1− 0.95)

work can be broadly grouped under the following
topics:
(a) Improved training of translation models by ex-
ploiting answer content/inter-word co-occurrence
relations and restriction to reliable parallel cor-
pora: Translation-based language model (TRLM)
(Xue et al., 2008) improved stability of TLM by
providing better probability estimates and also ex-
ploited answers for question retrieval. It further im-
proved the retrieval results and obtained the state-of-
the-art performance. Another line of work on trans-
lation models focused on providing suitable parallel
data to learn the translation probabilities. Compact
translation models (CTM) (Lee et al., 2008) tried to
further improve the translation probabilities based
on question-answer pairs by selecting the most im-
portant terms to build compact translation models.
We show that such special-purpose models to con-
trol noisy translations may not be necessary because
models learnt using entity annotations are robust to
noise in Q&A data.

Instead of using noisy Q&A data, new approach
(Bernhard and Gurevych, 2009) to build parallel cor-
pus from reliable sources has showed improvements.
They proposed to use as a parallel training data com-
prising of set the definitions and glosses provided
for the same term by different lexical semantic re-
sources. We move beyond terms and capture rela-
tionships between entities and terms using the page
contents and link structure in Wikipedia.

Apart from translation models there are other
approaches (Gao et al., 2004) that try to extend the
existing language models for adhoc retrieval by
incorporating term relationships or dependencies.
Some expand queries using word relationships
derived from co-occurrence thesaurus (Bai et al.,
2005; Qiu and Frei, 1993), hand-crafted thesaurus
(Liu et al., 2004; Voorhees, 1994) and combination
of both (Cao et al., 2005).

(b) Incorporation of query context information in
translation models using latent factor modeling
and smoothing approaches: All these existing
approaches mentioned above are considered to be
context independent, in that they do not take into
account any contextual information in modeling
word word relationships. Topic signature model
(Zhou et al., 2007) exploited contextual information

1274



by decomposing a document into a set of weighted
topic signatures and use it for model smoothing.
This model turns out to be inefficient when con-
fronted with ambiguous words and phrases because
it is unable to disambiguate the sense of topic
signatures. Others (Liu and Croft, 2004) perform
semantic smoothing by means of clustering. Re-
cently (Tu et al., 2010; Cai et al., 2011; Zhou
et al., 2011) showed improved performance by
performing sense based smoothing for document
retrieval, latent topic mining and phase based
retrieval respectively. Contrary to these approaches
we used entity disambiguation to capture contextual
information for improving Q&A retrieval.

(c) Complementary ideas for improving retrieval
performance that can be used alongside translation
models: Other work on question retrieval include
the use of category information available (Cao et
al., 2010), learning-to-rank techniques (Bian et al.,
2008; Surdeanu et al., 2008; Bunescu and Huang,
2010), proposed a syntactic tree matching ((Wang et
al., 2009) or question structure for important phrase
matching (Duan et al., 2008)). These methods seem
orthogonal to ours, in some cases complementary
and can be leveraged to get an even better perfor-
mance

There also exists work where exploiting entity
based representation has been found helpful in in-
formation retrieval (Singh et al., 2009; Egozi et al.,
2011; Meij et al., 2008; Grootjen and van der Weide,
2006). In our work we use entity annotations in
Q&A retrieval context. There is also some work on
using Wikipedia in general web search (Xu et al.,
2009).

6 Conclusion

In this work we extend word based model to in-
corporate semantic concepts for addressing the lex-
ical gap issue in retrieval models for large online
Q&A collections. Compared to the existing trans-
lation based model, our model is more robust and
effective in that it can perform context aware expan-
sions. We proposed ways to embed rich information
freely available in Wikipedia into our models and
combine it one learnt from Q&A corpus. Experi-
ments performed on a large real Q&A data demon-

strate that all configurations of ETLM significantly
outperforms existing models for Q&A retrieval.

Acknowledgments

Thanks to Srujana Merugu for helpful discussions.
Thanks to the anonymous reviewers for helping us
improve the presentation.

References

Lada A. Adamic, Jun Zhang, Eytan Bakshy, and Mark S.
Ackerman. 2008. Knowledge sharing and yahoo an-
swers: everyone knows something. In Proceedings of
the 17th international conference on World Wide Web,
WWW ’08, pages 665–674, New York, NY, USA.
ACM.

Jing Bai, Dawei Song, Peter Bruza, Jian-Yun Nie, and
Guihong Cao. 2005. Query expansion using term
relationships in language models for information re-
trieval. In Proceedings of the 14th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ’05, pages 688–695, New York, NY,
USA. ACM.

Adam Berger and John Lafferty. 1999. Information
retrieval as statistical translation. In Proceedings of
the 22nd annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’99, pages 222–229, New York, NY, USA.
ACM.

Delphine Bernhard and Iryna Gurevych. 2009. Combin-
ing lexical semantic resources with question & answer
archives for translation-based answer finding. In ACL-
IJCNLP ’09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 2, pages 728–736, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.

Jiang Bian, Yandong Liu, Eugene Agichtein, and
Hongyuan Zha. 2008. Finding the right facts in the
crowd: factoid question answering over social media.
In WWW ’08: Proceeding of the 17th international
conference on World Wide Web, pages 467–476, New
York, NY, USA. ACM.

Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263–311, June.

Razvan Bunescu and Yunfeng Huang. 2010. Learning
the relative usefulness of questions in community qa.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP

1275



’10, pages 97–107, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

R. Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of EACL, volume 6, pages 9–16.

Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao. 2011.
Learning the latent topics for question retrieval in com-
munity qa. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
273–281, Chiang Mai, Thailand, November. Asian
Federation of Natural Language Processing.

Guihong Cao, Jian-Yun Nie, and Jing Bai. 2005. Inte-
grating word relationships into language models. In
Proceedings of the 28th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, SIGIR ’05, pages 298–305, New
York, NY, USA. ACM.

Xin Cao, Gao Cong, Bin Cui, and Christian S. Jensen.
2010. A generalized framework of exploring category
information for question retrieval in community ques-
tion answer archives. In WWW ’10: Proceedings of
the 19th international conference on World wide web,
pages 201–210, New York, NY, USA. ACM.

Huizhong Duan, Yunbo Cao, Chin yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In In Proceedings of
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Tchnologies
(ACL:HLT).

Ofer Egozi, Shaul Markovitch, and Evgeniy Gabrilovich.
2011. Concept-based information retrieval using ex-
plicit semantic analysis. ACM Trans. Inf. Syst.,
29(2):8:1–8:34, April.

Paolo Ferragina and Ugo Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM interna-
tional conference on Information and knowledge man-
agement, CIKM ’10, pages 1625–1628, New York,
NY, USA. ACM.

Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Gui-
hong Cao. 2004. Dependence language model for
information retrieval. In Proceedings of the 27th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’04, pages 170–177, New York, NY, USA. ACM.

F. A. Grootjen and Th. P. van der Weide. 2006. Concep-
tual query expansion. Data Knowl. Eng., 56(2):174–
193, February.

Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, CIKM ’05, pages 84–90, New York, NY,
USA. ACM.

Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’02, pages 133–
142, New York, NY, USA. ACM.

Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ’06, pages 217–226, New York,
NY, USA. ACM.

S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages
457–466. ACM.

Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and Hae-
Chang Rim. 2008. Bridging lexical gaps between
queries and questions on large online q&#38;a collec-
tions with compact translation models. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’08, pages 410–418,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-based
retrieval using language models. In Proceedings of
the 27th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’04, pages 186–193, New York, NY, USA.
ACM.

Shuang Liu, Fang Liu, Clement Yu, and Weiyi Meng.
2004. An effective approach to document retrieval
via utilizing wordnet and recognizing phrases. In Pro-
ceedings of the 27th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, SIGIR ’04, pages 266–272, New York,
NY, USA. ACM.

Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.
2008. Context dependent language model adaptation.
In INTERSPEECH 2008, 9th Annual Conference of
the International Speech Communication Association,
Brisbane, Australia, September 22-26, 2008, pages
837–840. ISCA.

Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from wikipedia.
Int. J. Hum.-Comput. Stud., 67:716–754, September.

Edgar Meij, Dolf Trieschnigg, Maarten de Rijke, and
Wessel Kraaij. 2008. Parsimonious concept model-
ing. In Proceedings of the 31st annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ’08, pages 815–816,
New York, NY, USA. ACM.

R. Mihalcea and A. Csomai. 2007. Wikify!: linking

1276



documents to encyclopedic knowledge. In CIKM, vol-
ume 7, pages 233–242.

D. Milne and I.H. Witten. 2008. Learning to link with
wikipedia. In Proceeding of the 17th ACM conference
on Information and knowledge management, pages
509–518. ACM.

Robert C. Moore. 2004. Improving ibm word-alignment
model 1. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics, ACL
’04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Yonggang Qiu and Hans-Peter Frei. 1993. Concept
based query expansion. In Proceedings of the 16th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’93, pages 160–169, New York, NY, USA. ACM.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ’11, pages 1375–1384, Stroudsburg, PA, USA.
Association for Computational Linguistics.

S.E. Robertson, S. Walker, S. Jones, M.M. Hancock-
Beaulieu, and M. Gatford. 1996. Okapi at trec-3.
pages 109–126.

Amit Singh and Karthik Visweswariah. 2011. CQC:
classifying questions in cqa websites. In Proceedings
of the 20th ACM international conference on Informa-
tion and knowledge management, CIKM ’11, pages
2033–2036, New York, NY, USA. ACM.

Amit Singh, Sayali Kulkarni, Somnath Banerjee, Ganesh
Ramakrishnan, and Soumen Chakrabarti. 2009. Cu-
rating and searching the annotated web. In In SIGKDD
Conference, 2009. ACM.

Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online qa collections. In In Proceedings of the 46th
Annual Meeting for the Association for Computational
Linguistics: Human Language Technologies (ACL-08:
HLT, pages 719–727.

Xinhui Tu, Tingting He, Long Chen, Jing Luo, and
Maoyuan Zhang. 2010. Wikipedia-based semantic
smoothing for the language modeling approach to in-
formation retrieval. In Proceedings of the 32nd Eu-
ropean conference on Advances in Information Re-
trieval, ECIR’2010, pages 370–381, Berlin, Heidel-
berg. Springer-Verlag.

Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the 17th
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ’94, pages 61–69, New York, NY, USA. Springer-
Verlag New York, Inc.

Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009.
A syntactic tree matching approach to finding similar
questions in community-based qa services. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ’09, pages 187–194, New York, NY,
USA. ACM.

Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009.
Query dependent pseudo-relevance feedback based on
wikipedia. In Proceedings of the 32nd international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ’09, pages 59–66, New
York, NY, USA. ACM.

Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008.
Retrieval models for question and answer archives. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, SIGIR ’08, pages 475–482, New
York, NY, USA. ACM.

Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Trans. Inf. Syst., 22(2):179–
214, April.

Xiaohua Zhou, Xiaohua Hu, and Xiaodan Zhang. 2007.
Topic signature language models for ad hoc retrieval.
IEEE Trans. on Knowl. and Data Eng., 19(9):1276–
1287, September.

Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011.
Phrase-based translation model for question retrieval
in community question answer archives. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ’11, pages 653–662,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Justin Zobel and Alistair Moffat. 2006. Inverted files for
text search engines. ACM Comput. Surv., 38, July.

1277


