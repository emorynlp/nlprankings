










































Parser lexicalisation through self-learning


Proceedings of NAACL-HLT 2013, pages 391–400,
Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics

Parser lexicalisation through self-learning

Marek Rei
Computer Labratory

University of Cambridge
United Kingdom

Marek.Rei@cl.cam.ac.uk

Ted Briscoe
Computer Laboratory

University of Cambridge
United Kingdom

Ted.Briscoe@cl.cam.ac.uk

Abstract

We describe a new self-learning framework
for parser lexicalisation that requires only a
plain-text corpus of in-domain text. The
method first creates augmented versions of de-
pendency graphs by applying a series of mod-
ifications designed to directly capture higher-
order lexical path dependencies. Scores are
assigned to each edge in the graph using statis-
tics from an automatically parsed background
corpus. As bilexical dependencies are sparse,
a novel directed distributional word similar-
ity measure is used to smooth edge score es-
timates. Edge scores are then combined into
graph scores and used for reranking the top-
n analyses found by the unlexicalised parser.
The approach achieves significant improve-
ments on WSJ and biomedical text over the
unlexicalised baseline parser, which is origi-
nally trained on a subset of the Brown corpus.

1 Introduction

Most parsers exploit supervised machine learning
methods and a syntactically annotated dataset (i.e.
treebank), incorporating a wide range of features in
the training process to deliver competitive perfor-
mance. The use of lexically-conditioned features,
such as relations between lemmas or word forms,
is often critical when choosing the correct syntac-
tic analysis in ambiguous contexts. However, util-
ising such features leads the parser to learn infor-
mation that is often specific to the domain and/or
genre of the training data. Several experiments have
demonstrated that many lexical features learnt in

one domain provide little if any benefit when pars-
ing text from different domains and genres (Sekine,
1997; Gildea, 2001). Furthermore, manual creation
of in-domain treebanks is an expensive and time-
consuming process, which can only be performed by
experts with sufficient linguistic and domain knowl-
edge.

In contrast, unlexicalised parsers avoid using lex-
ical information and select a syntactic analysis us-
ing only more general features, such as POS tags.
While they cannot be expected to achieve optimal
performance when trained and tested in a single do-
main, unlexicalised parsers can be surprisingly com-
petitive with their lexicalised counterparts (Klein
and Manning, 2003; Petrov et al., 2006). In this
work, instead of trying to adapt a lexicalised parser
to new domains, we explore how bilexical features
can be integrated effectively with any unlexicalised
parser. As our novel self-learning framework re-
quires only a large unannotated corpus, lexical fea-
tures can be easily tuned to a specific domain or
genre by selecting a suitable dataset. In addition,
we describe a graph expansion process that captures
selected bilexical relations which improve perfor-
mance but would otherwise require sparse higher-
order dependency path feature types in most ap-
proaches to dependency parsing. As many bilex-
ical features will still be sparse, we also develop
an approach to estimating confidence scores for de-
pendency relations using a directional distributional
word similarity measure. The final framework in-
tegrates easily with any unlexicalised (and therefore
potentially less domain/genre-biased) parser capable
of returning ranked dependency analyses.

391



2 Background

We hypothesise that a large corpus will often contain
examples of dependency relations in non-ambiguous
contexts, and these will mostly be correctly parsed
by an unlexicalised parser. Lexical statistics derived
from the corpus can then be used to select the cor-
rect parse in a more difficult context. For example,
consider the following sentences:

(1) a. Government projects interest researchers

b. Government raises interest rates

c. Government projects receive funding

d. Interest rates are increasing

Noun-verb ambiguities over projects and interest
might erroneously result in the unlexicalised parser
returning similar dependency graphs for both a and
b. However, sentences c and d contain less ambigu-
ous instances of the same phrases and can provide
clues to correctly parsing the first two examples. In
a large in-domain corpus we are likely to find more
cases of researchers being the object for interest and
fewer cases where it is the object of project. In con-
trast, rates is more likely to have interest as a mod-
ifier than as a head in an object relation. Exploiting
this lexical information, we can assign the correct
derivation to each of the more ambiguous sentences.

Similar intuitions have been used to motivate the
acquisition of bilexical features from background
corpora for improving parser accuracy. However,
previous work has focused on including these statis-
tics as auxiliary features during supervised training.
For example, van Noord (2007) incorporated bilex-
ical preferences as features via self-training to im-
prove the Alpino parser for Dutch. Plank and van
Noord (2008) investigated the application of aux-
iliary distributions for domain adaptation. They
incorporated information from both in-domain and
out-of-domain sources into their maximum entropy
model and found that the out-of-domain auxiliary
distributions did not contribute to parsing accuracy
in the target domain. Zhou et al. (2011) extracted n-
gram counts from Google queries and a large corpus
to improve the MSTParser. In contrast to previous
work, we refer to our approach as self-learning be-
cause it differs from self-training by utilising statis-
tics found using an initial parse ranking model to

create a separate unsupervised reranking compo-
nent, without retraining the baseline unlexicalised
model.

We formulate our self-learning framework as a
reranking process that assigns new scores to the top-
n ranked analyses found by the original parser. Parse
reranking has been successfully used in previous
work as a method of including a wider range of fea-
tures to rescore a smaller selection of highly-ranked
candidate parses. Collins (2000) was one of the first
to propose supervised reranking as an additional step
to increase parser accuracy and achieved 1.55% ac-
curacy improvement for his parser. Charniak and
Johnson (2005) utilise a discriminative reranker and
show a 1.3% improvement for the Charniak parser.
McClosky et al. (2006) extend their work by adding
new features and further increase the performance
by 0.3%. Ng et al. (2010) implemented a dis-
criminative maximum entropy reranker for the C&C
parser and showed a 0.23% improvement over the
baseline. Bansal and Klein (2011) discriminatively
rerank derivations from the Berkeley unlexicalised
parser (Petrov et al., 2006) demonstrating that lex-
ical features derived from the Google n-gram cor-
pus improve accuracy even when used in conjunc-
tion with other reranking features. They have all
treated reranking as a supervised task and trained a
discriminative classifier using parse tree features and
annotated in-domain data. In contrast, our reranker
only uses statistics from an unlabelled source and
requires no manual annotation or training of the
reranking component. As we utilise an unlexicalised
parser, our baseline performance on WSJ text is
lower compared to some fully-lexicalised parsers.
However, an unlexicalised parser is also likely to be
less biased to domains or genres manifested in the
text used to train its original ranking model. This
may allow the reranker to adapt it to a new domain
and/or genre more effectively.

3 Reordering dependency graphs

For our experiments, we make use of the unlexi-
calised RASP parser (Briscoe et al., 2006) as the
baseline system. For every sentence s the parser
returns a list of dependency graphs Gs, ranked by
the log probability of the associated derivation in the
structural ranking model. Our goal is to reorder this

392



list to improve ranking accuracy and, most impor-
tantly, to improve the quality of the highest-ranked
dependency graph. This is done by assigning a con-
fidence score to every graph gs,r ∈ Gs where r is the
rank of gs for sentence s. The method treats each
sentence independently, therefore we can omit the
sentence identifiers and refer to gs,r as gr.

We first calculate confidence scores for all the in-
dividual edges and then combine them into an over-
all score for the dependency graph. In the following
sections, we describe a series of graph modifications
that incorporates selected higher-order dependency
path relations, without introducing unwanted noise
or complexity into the reranker. Next, we outline
different approaches for calculating and smoothing
the confidence scores for bilexical relations. Finally,
we describe methods for combining together these
scores and calculating an overall score for a depen-
dency graph. We make publically available all the
code developed for performing these steps in the
parse reranking system.1

3.1 Graph modifications

For every dependency graph gr the graph expan-
sion procedure creates a modified representation g′r
which contains a wider range of bilexical relations.
The motivation for this graph expansion step is sim-
ilar to that motivating the move from first-order to
higher-order dependency path feature types (e.g.,
Carreras (2007)). However, compared to using all
nth-order paths, these rules are chosen to maximise
the utility and minimise the sparsity of the result-
ing bilexical features. In addition, the cascading na-
ture of the expansion steps means in some cases the
expansion captures useful 3rd and 4th order depen-
dencies. Similar approaches to graph modifications
have been successfully used for several NLP tasks
(van Noord, 2007; Arora et al., 2010).

For any edge e we also use notation (rel, w1, w2),
referring to an edge from w1 to w2 with the label
rel. We perform the following modifications on ev-
ery dependency graph:

1. Normalising lemmas. All lemmas are converted
to lowercase. Numerical lemmas are replaced
with more generic tags to reduce sparsity.

1www.marekrei.com/projects/lexicalisation

2. Bypassing conjunctions. For every edge pair
(rel1, w1, w2) and (rel2, w2, w3) where w2 is
tagged as a conjunction, we create an additional
edge (rel1, w1, w3). This bypasses the conjunc-
tion node and creates direct edges between the
head and dependents of the conjunctive lemma.

3. Bypassing prepositions. For every edge pair
(rel1, w1, w2) and (rel2, w2, w3) where w2 is
tagged as a preposition, we create an additional
edge (rel3, w1, w3). rel3 = rel1 +‘ prep’, where
‘ prep’ is added as a marker to indicate that the
relation originally contained a preposition.

4. Bypassing verbs. For every edge pair
(rel1, w1, w2) and (rel2, w1, w3) where w1 is
tagged as a verb, w2 and w3 are both tagged
as open-class lemmas, rel1 starts with a subject
relation, and rel2 starts with an object relation,
we create an additional edge (rel3, w2, w3) where
rel3 = rel1 + ‘-’ + rel2. This creates an additional
edge between the subject and the object, with the
new edge label containing both of the original la-
bels.

5. Duplicating nodes. For every existing node in
the graph, containing the lemma and POS for
each token (lemma pos), we create a parallel node
without the POS information (lemma). Then, for
each existing edge, we create three correspond-
ing edges, interconnecting the parallel nodes to
each other and the original graph. This allows the
reranker to exploit both specific and more generic
instantiations of each lemma.

Figure 1 illustrates the graph modification pro-
cess. It is important to note that each of these mod-
ifications gets applied in the order that they are de-
scribed above. For example, when creating edges for
bypassing verbs, the new edges for prepositions and
conjunctions have already been created and also par-
ticipate in this step. We performed ablation tests on
the development data and verified that each of these
modifications contributes positively to the final per-
formance.

3.2 Edge scoring methods

We start the scoring process by assigning individual
confidence scores to every bilexical relation in the

393



italian pm meet with cabinet member and senior official
JJ NP1 VVZ IW NN1 NN2 CC JJ NN2

ncmod ncsubj iobj

dobj

ncmod conj

conj

ncmod

ncsubj-iobj prepncsubj-iobj prep

iobj prep
iobj prepiobj prep

dobjdobj

Figure 1: Modified graph for the sentence ‘Italian PM meets with Cabinet members and senior officials’ after steps
1-4. Edges above the text are created by the parser, edges below the text are automatically created using the operations
described in Section 3.1. The 5th step will create 9 new nodes and 45 additional edges (not shown).

modified graph. In this section we give an overview
of some possible strategies for performing this task.

The parser returns a ranked list of graphs and this
can be used to derive an edge score without requir-
ing any additional information. We estimate that the
likelihood of a parse being the best possible parse for
a given sentence is roughly inversely proportional
to the rank that it is assigned by the parser. These
values can be summed for all graphs that contain a
specific edge, normalised to approximate a proba-
bility. We then calculate the score for edge e as the
Reciprocal Edge Score (RES) – the probability of e
belonging to the best possible parse:

RES(e) =
∑R

r=1[
1
r × contains(g

′
r, e)]∑R

r=1
1
r

whereR is the total number of parses for a sentence,
and contains(g′r, e) returns 1 if graph g

′
r contains

edge e, and 0 otherwise. The value is normalised,
so that an edge which is found in all parses will have
a score of 1.0, but occurrences at higher ranks will
have a considerably larger contribution.

The score of an edge can also be assigned by es-
timating the probability of that edge using a parsed
reference corpus. van Noord (2007) improved over-
all parsing performance in a supervised self-training
framework using feature weights based on pointwise
mutual information:

I(e) = log
P(rel, w1, w2)

P(rel, w1, ∗)× P(∗, ∗, w2)

where P(rel, w1, w2) is the probability of seeing an
edge from w1 to w2 with label rel, P(rel, w1, ∗) is

the probability of seeing an edge from w1 to any
node with label rel, and P(∗, ∗, w2) is the prob-
ability of seeing any type of edge linking to w2.
Plank and van Noord (2008) used the same approach
for semi-supervised domain adaptation but were not
able to achieve similar performance benefits. In our
implementation we omit the logarithm in the equa-
tion, as this improves performance and avoids prob-
lems with log(0) for unseen edges.

I(e) compares the probability of the complete
edge to the probabilities of partially specified edges,
but it assumes that w2 will have an incoming rela-
tion, and that w1 will have an outgoing relation of
type rel to some unknown node. These assumptions
may or may not be true – given the input sentence,
we have observed w1 and w2 but do not know what
relations they are involved in. Therefore, we create
a more general version of the measure that compares
the probability of the complete edge to the individual
probabilities of the two lemmas – the Conditional
Edge Score (CES1):

CES1(e) =
P(rel, w1, w2)
P(w1)× P(w2)

where P(w1) is the probability of seeing w1 in text,
estimated from a background corpus using maxi-
mum likelihood.

Finally, we know that w1 and w2 are in a sen-
tence together but cannot assume that there is a de-
pendency relation between them. However, we can
choose to think of each sentence as a fully connected
graph, with an edge going from every lemma to ev-
ery other lemma in the same sentence. If there exists

394



ECES1(rel, w1, w2) =
1

2
× (

∑
c1∈C1 sim(c1, w1)×

P(rel,c1,w2)
P(c1)×P(w2)∑

c1∈C1 sim(c1, w1)
+

∑
c2∈C2 sim(c2, w2)×

P(rel,w1,c2)
P(w1)×P(c2)∑

c2∈C2 sim(c2, w2)
)

ECES2(rel, w1, w2) =
1

2
× (

∑
c1∈C1 sim(c1, w1)×

P(rel,c1,w2)
P(∗,c1,w2)∑

c1∈C1 sim(c1, w1)
+

∑
c2∈C2 sim(c2, w2)×

P(rel,w1,c2)
P(∗,w1,c2)∑

c2∈C2 sim(c2, w2)
)

Figure 2: Expanded edge score calculation methods using the list of distributionally similar lemmas

no genuine relation between the lemmas, the edge is
simply considered a null edge. We can then find the
conditional probability of the relation type given the
two lemmas:

CES2(e) =
P(rel, w1, w2)
P(∗, w1, w2)

where P(rel, w1, w2) is the probability of the fully-
specified relation, and P(∗, w1, w2) is the probability
of there being an edge of any type fromw1 tow2, in-
cluding a null edge. Using fully connected graphs,
the latter is equivalent to the probability of w1 and
w2 appearing in a sentence together, which again can
be calculated from the background corpus.

3.3 Smoothing edge scores

Apart from RES, all the scoring methods from
the previous section rely on correctly estimat-
ing the probability of the fully-specified edge,
P(rel, w1, w2). Even in a large background corpus
these triples will be very sparse, and it can be useful
to find approximate methods for estimating the edge
scores.

Using smoothing techniques derived from work
on language modelling, we could back-off to a more
general version of the relation. For example, if
(dobj, read, publication) is not frequent enough, the
value could be approximated using the probabilities
of (dobj, read, *) and (dobj, *, publication). How-
ever, this can lead to unexpected results due to com-
positionality – while (dobj, read, *) and (dobj, *,
rugby) can be fairly common, (dobj, read, rugby) is
an unlikely relation.

Instead, we can consider looking at other lemmas
which are similar to the rare lemmas in the relation.
If (dobj, read, publication) is infrequent in the data,
the system might predict that book is a reasonable
substitute for publication and use (dobj, read, book)

to estimate the original probability.
Given that we have a reliable way of finding likely

substitutes for a given lemma, we can create ex-
panded versions of CES1 and CES2, as shown in
Figure 2. C1 is the list of substitute lemmas for w1,
and sim(c1, w1) is a measure showing how similar
c1 is to w1. The methods iterate over the list of sub-
stitutes and calculate the CES score for each of the
modified relations. The values are then combined by
using the similarity score as a weight – more similar
lemmas will have a higher contribution to the final
result. This is done for both the head and the depen-
dent in the original relation, and the scores are then
normalised and averaged.

Experiments with a wide variety of distributional
word similarity measures revealed that WeightedCo-
sine (Rei, 2013), a directional similarity measure
designed to better capture hyponymy relations, per-
formed best. Hyponyms are more specific versions
of a word and normally include the general proper-
ties of the hypernym, making them well-suited for
lexical substitution. The WeightedCosine measure
incorporates an additional directional weight into
the standard cosine similarity, assigning different
importance to individual features for the hyponymy
relation. We retain the 10 most distributionally simi-
lar putative hyponyms for each lemma and substitute
them in the relation. The original lemma is also in-
cluded with similarity 1.0, thereby assigning it the
highest weight. The lemma vectors are built from
the same vector space model that is used for cal-
culating edge probabilities, which includes all the
graph modifications described in Section 3.1.

3.4 Combining edge scores
While the CES and ECES measures calculate con-
fidence scores for bilexical relations using statistics
from a large background corpus, they do not include
any knowledge about grammar, syntax, or the con-

395



CMB1(e) = 3
√

RES(e) ∗ CES1(e) ∗ CES2(e) CMB2(e) = 3
√

RES(e) ∗ ECES1(e) ∗ ECES2(e)

Figure 3: Edge score combination methods

text in a specific sentence. In contrast, the RES score
implicitly includes some of this information, as it is
calculated based on the original parser ranking. In
order to take advantage of both information sources,
we combine these scores into CMB1 and CMB2, as
shown in Figure 3.

3.5 Graph scoring
Every edge in graph g′r is assigned a score indicat-
ing the reranker’s confidence in that edge belonging
to the best parse. We investigated different strate-
gies for combining these values together into a con-
fidence score for the whole graph. The simplest so-
lution is to sum together individual edge scores, but
this would lead to always preferring graphs that have
a larger number of edges. Interestingly, averaging
the edge scores does not produce good results either
because it is biased towards smaller graph fragments
containing only highly-confident edges.

We created a new scoring method which prefers
graphs that cover all the nodes, but does not create
bias for a higher number of edges. For every node
in the graph, it finds the average score of all edges
which have that node as a dependent. These scores
are then averaged again over all nodes:

NScore(n) =

∑
e∈Eg EdgeScore(e)× isDep(e, n)∑

e∈Eg isDep(e, n)

GraphScore(g) =

∑
n∈Ng NScore(n)

|Ng|

where g is the graph being scored, n ∈ Ng is a
node in graph g, e ∈ Eg is an edge in graph g,
isDep(e, n) is a function returning 1.0 if n is the de-
pendent in edge e, and 0.0 otherwise. NScore(n) is
set to 0 if the node does not appear as a dependent in
any edges. We found this metric performs well, as
it prefers graphs that connect together many nodes
without simply rewarding a larger number of edges.

While the score calculation is done using the
modified graph g′r, the resulting score is directly as-
signed to the corresponding original graph gr, and

the reordering of the original dependency graphs is
used for evaluation.

4 Experiments

4.1 Evaluation methods

In order to evaluate how much the reranker improves
the highest-ranked dependency graph, we calculate
the microaveraged precision, recall and F-score over
all dependencies from the top-ranking parses for
the test set. Following the official RASP evalua-
tion (Briscoe et al., 2006) we employ the hierarchi-
cal edge matching scheme which aggregates counts
up the dependency relation subsumption hierarchy
and thus rewards the parser for making more fine-
grained distinctions.2 Statistical significance of the
change in F-score is calculated by using the Approx-
imate Randomisation Test (Noreen, 1989; Cohen,
1995) with 106 iterations.

We also wish to measure how well the reranker
does at the overall task of ordering dependency
graphs. For this we make use of an oracle that cre-
ates the perfect ranking for a set of graphs by calcu-
lating their individual F-scores; this ideal ranking is
then compared to the output of our system. Spear-
man’s rank correlation coefficient between the two
rankings is calculated for each sentence and then av-
eraged over all sentences. If the scores for all of the
returned analyses are equal, this coefficient cannot
be calculated and is set to 0.

4.2 DepBank

We evaluated our self-learning framework using
the DepBank/GR reannotation (Briscoe and Carroll,
2006) of the PARC 700 Dependency Bank (King
et al., 2003). The dataset is provided with the
open-source RASP distribution3 and has been used
for evaluating different parsers, including RASP
(Briscoe and Carroll, 2006; Watson et al., 2007) and

2Slight changes in the performance of the baseline parser
compared to previous publications are due to using a more re-
cent version of the parser and minor corrections to the gold stan-
dard annotation.

3ilexir.co.uk/2012/open-source-rasp-3-1/

396



C&C (Clark and Curran, 2007). It contains 700 sen-
tences, randomly chosen from section 23 of the WSJ
Penn Treebank (Marcus et al., 1993), divided into
development (140 sentences) and test data (560 sen-
tences). We made use of the development data to
experiment with a wider selection of edge and graph
scoring methods, and report the final results on the
test data.

For reranking we collect up to 1000 top-ranked
analyses for each sentence. The actual number of
analyses that the RASP parser outputs depends on
the sentence and can be smaller. As the parser first
constructs parse trees and converts them to depen-
dency graphs, several parse trees may result in iden-
tical graphs; we remove any duplicates to obtain a
ranking of unique dependency graphs.

Our approach relies on a large unannotated corpus
of in-domain text, and for this we used the BLLIP
corpus containing 50M words of in-domain WSJ ar-
ticles. Our version of this corpus excludes texts that
are found in the Penn Treebank, thereby also exclud-
ing the section that we use for evaluation.

The baseline system is the unlexicalised RASP
parser with default settings. In order to construct
the upper bound, we use an oracle to calculate the F-
score for each dependency graph individually, and
then create the best possible ranking using these
scores.

Table 1 contains evaluation results on the Dep-
Bank/GR test set. The baseline system achieves
76.41% F-score on the test data, with 32.70% av-
erage correlation. I and RES scoring methods give
comparable results, with RES improving correlation
by 9.56%. The CES and ECES scores all make use
of corpus-based statistics and all significantly im-
prove over the baseline system, with absolute in-
creases in F-score of more than 2% for the fully-
connected edge score variants.

Finally, we combine the RES score with the
corpus-based methods and the fully-connected
CMB2 variant again delivers the best overall results.
The final F-score is 79.21%, an absolute improve-
ment of 2.8%, corresponding to 33.65% relative er-
ror reduction with respect to the upper bound. Cor-
relation is also increased by 16.32%; this means the
methods not only improve the chances of finding the
best dependency graph, but also manage to create
a better overall ranking. The F-scores for all the

corpus-based scoring methods are statistically sig-
nificant when compared to the baseline (p < 0.05).

By using our self-learning framework, we were
able to significantly improve the original unlexi-
calised parser. To put the overall result in a wider
perspective, Clark and Curran (2007) achieve an
F-score of 81.86% on the DepBank/GR test sen-
tences using the C&C lexicalised parser, trained
on 40,000 manually-treebanked sentences from the
WSJ. The unlexicalised RASP parser, using a
manually-developed grammar and a parse ranking
component trained on 4,000 partially-bracketed un-
labelled sentences from a domain/genre balanced
subset of Brown (Watson et al., 2007), achieves an
F-score of 76.41% on the same test set. The method
introduced here improves this to 79.21% F-score
without using any further manually-annotated data,
closing more than half of the gap between the perfor-
mance of a fully-supervised in-domain parser and a
more weakly-supervised more domain-neutral one.

We also performed an additional detailed analysis
of the results and found that, with the exception of
the auxiliary dependency relation, the reranking pro-
cess was able to improve the F-score of all other in-
dividual dependency types. Complements and mod-
ifiers are attached with much higher accuracy, result-
ing in 3.34% and 3.15% increase in the correspond-
ing F-scores. The non-clausal modifier relation (nc-
mod), which is the most frequent label in the dataset,
increases by 3.16%.

4.3 Genia
One advantage of our reranking framework is that
it does not rely on any domain-dependent manually
annotated resources. Therefore, we are interested in
seeing how it performs on text from a completely
different domain and genre.

The GENIA-GR dataset (Tateisi et al., 2008) is
a collection of 492 sentences taken from biomedi-
cal research papers in the GENIA corpus (Kim et
al., 2003). The sentences have been manually anno-
tated with dependency-based grammatical relations
identical to those output by the RASP parser. How-
ever, it does not contain dependencies for all tokens
and many multi-word phrases are treated as single
units. For example, the tokens ‘intracellular redox
status’ are annotated as one node with label intra-
cellular redox status. We retain this annotation and

397



DepBank/GR GENIA-GR
Prec Rec F ρ Prec Rec F ρ

Baseline 77.91 74.97 76.41 32.70 79.91 78.86 79.38 36.54
Upper Bound 86.74 82.82 84.73 75.36 86.33 84.71 85.51 78.66
I 77.77 75.00 76.36 33.32 77.18 76.21 76.69 30.23
RES 78.13 74.94 76.50 42.26 80.06 78.89 79.47 47.52
CES1 79.68 76.40 78.01 41.95 78.64 77.50 78.07 36.06
CES2 80.48 77.28 78.85 48.43 79.92 78.92 79.42 43.09
ECES1 79.96 76.68 78.29 42.41 79.09 78.11 78.60 38.02
ECES2 80.71 77.52 79.08 49.05 79.84 78.95 79.39 43.64
CMB1 80.64 77.31 78.94 48.25 80.60 79.51 80.05 44.96
CMB2 80.88 77.60 79.21 49.02 80.69 79.64 80.16 46.24

Table 1: Performance of different edge scoring methods on the test data. For each measure we report precision,
recall, F-score, and average Spearman’s correlation (ρ). The highest results for each measure are marked in bold. The
underlined F-scores are significantly better compared to the baseline.

allow the unlexicalised parser to treat these nodes as
atomic unseen words during POS tagging and pars-
ing. However, we use the last lemma in each multi-
word phrase for calculating the edge score statistics.

In order to initialise our parse reranking frame-
work, we also need a background corpus that closely
matches the evaluation domain. The annotated sen-
tences in GENIA-GR were chosen from abstracts
that are labelled with the MeSH term ‘NF-kappa B’.
Following this method, we created our background
corpus by extracting 7,100 full-text articles (1.6M
sentences) from the PubMed Central Open Access
collection, containing any of the following terms
with any capitalisation: ‘nf-kappa b’, ‘nf-kappab’,
‘nf kappa b’, ‘nf-kappa b’, ‘nf-kb’, ‘nf-κb’. Since
we retain all texts from matching documents, this
keyword search acts as a broad indicator that the sen-
tences contain topics which correspond to the evalu-
ation dataset. This focussed corpus was then parsed
with the unlexicalised parser and used to create a
statistical model for the reranking system, following
the same methods as described in Sections 3 and 4.2.

Table 1 also contains the results for experiments
in the biomedical domain. The first thing to notice
is that while the upper bound for the unlexicalised
parser is similar to that for the DepBank experiments
in Section 4.2, the baseline results are considerably
higher. This is largely due to the nature of the dataset
– since many complicated multi-word phrases are
treated as single nodes, the parser is not evaluated on
edges within these nodes. In addition, treating these

nodes as unseen words eliminates many incorrect
derivations that would otherwise split the phrases.
This results in a naturally higher baseline of 79.38%,
and also makes it more difficult to further improve
the performance.

The edge scoring methods I, CES1 and ECES1
deliver F-scores lower than the baseline in this ex-
periment. RES, CES2 and ECES2 yield a modest
improvement in both F-score and Spearman’s cor-
relation. Finally, the combination methods again
give the best performance, with CMB2 delivering an
F-score of 80.16%, an absolute increase of 0.78%,
which is statistically significant (p < 0.05). The
experiment shows that our self-learning framework
works on very different domains, and it can be used
to significantly increase the accuracy of an unlexi-
calised parser without requiring any annotated data.

5 Conclusion

We developed a new self-learning framework for de-
pendency graph reranking that requires only a plain-
text corpus from a suitable domain. We automati-
cally parse this corpus and use the highest ranked
analyses to estimate maximum likelihood probabili-
ties for bilexical relations. Every dependency graph
is first modified to incorporate additional edges that
model selected higher-order dependency path rela-
tionships. Each edge in the graph is then assigned a
confidence score based on statistics from the back-
ground corpus and ranking preferences from the un-

398



lexicalised parser. We also described a novel method
for smoothing these scores using directional dis-
tributional similarity measures. Finally, the edge
scores are combined into an overall graph score by
first averaging them over individual nodes.

As the method requires no annotated data, it can
be easily adapted to different domains and genres.
Our experiments showed that the reranking process
significantly improved performance on both WSJ
and biomedical data.

References

Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Rosé,
and Eric Nyberg. 2010. Sentiment Classification us-
ing Automatically Extracted Subgraph Features. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text.

Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics, pages 693–702.

Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the COLING/ACL
on Main conference poster sessions, number July,
pages 41–48, Morristown, NJ, USA. Association for
Computational Linguistics.

Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, number July, pages 77–80, Sydney, Aus-
tralia. Association for Computational Linguistics.

Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL, vol-
ume 7, pages 957–961.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics - ACL ’05,
1(June):173–180.

Stephen Clark and James R. Curran. 2007. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, vol-
ume 45, pages 248–255.

Paul R Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.

Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In The 17th International Con-
ference on Machine Learning (ICML).

Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
pages 167–202.

Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(1):180–182.

Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the EACL03:
4th International Workshop on Linguistically Inter-
preted Corpora (LINC-03), pages 1–8.

Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, number July, pages 423–430. Association for
Computational Linguistics Morristown, NJ, USA.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, pages 1–22.

David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, number June, pages 152–
159, Morristown, NJ, USA. Association for Computa-
tional Linguistics.

Dominick Ng, Matthew Honnibal, and James R. Curran.
2010. Reranking a wide-coverage CCG parser. In
Australasian Language Technology Association Work-
shop 2010, page 90.

Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley, New
York.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL (ACL ’06),
pages 433–440, Morristown, NJ, USA. Association for
Computational Linguistics.

Barbara Plank and Gertjan van Noord. 2008. Explor-
ing an auxiliary distribution based approach to domain
adaptation of a syntactic disambiguation model. In
Coling 2008: Proceedings of the Workshop on Cross-
Framework and Cross- Domain Parser Evaluation,
pages 9–16, Manchester, UK. Association for Com-
putational Linguistics.

399



Marek Rei. 2013. Minimally supervised dependency-
based methods for natural language processing. Ph.D.
thesis, University of Cambridge.

Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proceedings of the fifth conference on Applied
natural language processing, volume 1, pages 96–102,
Morristown, NJ, USA. Association for Computational
Linguistics.

Yuka Tateisi, Yusuke Miyao, Kenji Sagae, and Jun’ichi
Tsujii. 2008. GENIA-GR: a Grammatical Relation
Corpus for Parser Evaluation in the Biomedical Do-
main. In Proceedings of LREC, pages 1942–1948.

Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the 10th International Conference on
Parsing Technologies, number June, pages 1–10, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.

Rebecca Watson, Ted Briscoe, and John Carroll. 2007.
Semi-supervised training of a statistical parser from
unlabeled partially-bracketed data. Proceedings of the
10th International Conference on Parsing Technolo-
gies - IWPT ’07, (June):23–32.

Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting Web-Derived Selectional Preference to Im-
prove Statistical Dependency Parsing. In 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1556–1565.

400


