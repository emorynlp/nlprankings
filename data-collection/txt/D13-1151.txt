










































Automatically Identifying Pseudepigraphic Texts


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1449–1454,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Automatically Identifying Pseudepigraphic Texts 

Moshe Koppel 

Bar Ilan University 

Ramat-Gan, 52900, Israel  

moishk@gmail.com 

Shachar Seidman 

Bar Ilan University 

Ramat-Gan, 52900, Israel 

shachar9@gmail.com 

 

 

Abstract 

The identification of pseudepigraphic texts – 

texts not written by the authors to which they 

are attributed – has important historical, fo-

rensic and commercial applications. We in-

troduce an unsupervised technique for identi-

fying pseudepigrapha. The idea is to identify 

textual outliers in a corpus based on the pair-

wise similarities of all documents in the cor-

pus. The crucial point is that document simi-

larity not be measured in any of the standard 

ways but rather be based on the output of a re-

cently introduced algorithm for authorship ve-

rification. The proposed method strongly 

outperforms existing techniques in systematic 

experiments on a blog corpus. 

1 Introduction 

The Shakespeare attribution problem is centuries 

old and shows no signs of abating. Some scholars 

argue that some, or even all, of Shakespeare’s 

works were not actually written by him. The most 

mainstream theory – and the one that interests us 

here – is that most of the works were written by 

Shakespeare, but that several of them were not. 

Could modern methods of computational author-

ship attribution be used to detect which, if any, of 

the works attributed to Shakespeare were not writ-

ten by him? 

More generally, this paper deals with the unsu-

pervised problem of detecting pseudepigrapha: 

documents in a supposedly single-author corpus 

that were not actually written by the corpus’s pre-

sumed author. Studies as early as Mendenhall 

(1887), have observed that texts by a single author 

tend to be somewhat homogeneous in style. If this 

is indeed the case, we would expect that pseudepi-

grapha would be detectable as outliers.  

Identifying such outlier texts is, of course, a 

special case of general outlier identification, one of 

the central tasks of statistics. We will thus consider 

the pseudepigrapha problem in the context of the 

more general outlier detection problem. 

Typically, research on textual outliers assumes 

that we have a corpus of known authentic docu-

ments and are asked to decide if a specified other 

document is authentic or not (Juola and Stamata-

tos, 2013). One crucial aspect of our problem is 

that we do not assume that any specific text in a 

corpus is known a priori to be authentic or pseude-

pigraphic; we can assume only that most of the 

documents in the corpus are authentic. 

The method we introduce in this paper builds on 

the approach of Koppel and Winter (2013) for de-

termining if two documents are by the same au-

thor. We apply that method to every pair of 

documents in a corpus and use properties of the 

resulting adjacency graph to identify outliers. In 

the following section, we briefly outline previous 

work. In Section 3 we provide a framework for 

outlier detection and in Section 4 we describe our 

method. In Section 5 we describe the experimental 

setting and give results and in Section 6 we present 

results for the plays of Shakespeare. 

2 Related Work 

Identifying outlier texts consists of two main stag-

es: first, representing each text as a numerical vec-

tor representing relevant linguistic features of the 

text and second, using generic methods to identify 

outlier vectors.  

There is a vast literature on generic methods for 

outlier detection, summarized in Hodge & Austin 

(2004) and Chandola et al. (2009). Since our prob-

1449



lem setup does not entail obtaining any labeled 

examples of authentic or outlier documents, super-

vised and semi-supervised methods are inapplica-

ble. The available methods are unsupervised, 

principally probabilistic or proximity-based me-

thods. A classical variant of such methods for un-

ivariate normally distributed data uses the the z-

score (Grubbs, 1969). Such simple univariate out-

lier detectors are, however, inappropriate for iden-

tifying outliers in a high-dimensional textual 

corpus. Subsequent work, such as the Stahel-

Donoho Estimator (Stahel, 1981; Donoho, 1982), 

PCout (Filzmoser et al., 2008), LOF (Breunig and 

Kriegel, 2000) and ABOD (Kriegel et al., 2008) 

have generalized univariate methods to high-

dimensional data points. 

In his comprehensive review of outlier detection 

methods in textual data, Guthrie (2008) compares a 

variety of vectorization methods along with a va-

riety of generic outlier methods. The vectorization 

methods employ a variety of lexical and syntactic 

stylistic features, while the outlier detection me-

thods use a variety of similarity/distance measures 

such as cosine and Euclidean distance. Similar me-

thods have also been used in the field of intrinsic 

plagiarism detection, which involves segmenting a 

text and then identifying outlier segments (Stama-

tatos, 2009; Stein et al., 2010). 

3 Proximity Methods 

Formally, the problem we wish to solve is defined 

as follows: Given a set of documents D = 

{d1,…,dn}, all or most of which were written by 

author A, which, if any, documents in D were not 

written by A? 

We begin by considering the kinds of proximity 

methods for textual outlier detection considered by 

Guthrie (2008) and in the work on intrinsic plagiar-

ism detection; these will serve as baseline methods 

for our approach. The idea is simple: mark as an 

outlier any document that is too far from the rest of 

the documents in the corpus.  

We briefly sketch the key steps: 

1. Represent a document as a numerical vector.  
The kinds of measurable features that can be 

used to represent a document include frequen-

cies of word unigrams, function words, parts-

of-speech and character n-grams, as well as 

complexity measures such as type/token ratio, 

sentence and word length and so on. 

2. Measure the similarity of two document vec-
tors. 

We can use either inverses of distance meas-

ures such as Euclidean distance or Manhattan 

distance, or else direct similarity measures 

such as cosine or min-max. 

3. Use an aggregation method to measure the 
similarity of a document to a set of documents. 

One approach is to simply measure the dis-

tance from a document to the centroid of all 

the other documents (centroid method). 

Another approach is to first measure the simi-

larity of a document to each other document 

and then to aggregate the results by averaging 

all the obtained values (mean method): 

 
Alternatively, we can average the values only 

for the k nearest neighbors (k-NN method): 

 
(where Dk = k nearest neighbors of di). 

Yet another method is to use median distance 

(median method).  

 
We note that the centroid method and mean 

method suffer from the masking effect (Bendre 

and Kale, 1987; Rousseeuw and Leroy, 2003): 

the presence of some outliers in the data can 

greatly distort the estimator's results regarding 

the presence of other outliers. The k-NN me-

thod and the median method are both much 

more robust. 

4. Choose some threshold beyond which a docu-
ment is marked as an outlier.  

Choosing the threshold is one of the central is-

sues in statistical approaches. For our purpos-

es, however, the choice of threshold is simply 

a parameter trading off recall and precision. 

4 Second-Order Similarity 

Our approach is to use an entirely different kind of 

similarity measure in Step 2. Rather than use a 

first-order similarity measure, as is customary, we 

employ a second-order similarity measure that is 

the output of an algorithm used for the authorship 

verification problem (Koppel et al. 2011), in which 

we need to determine if two, possibly short, docu-

ments were written by the same author.  

That algorithm, known as the “impostors me-

thod” (IM), works as follows. Given two docu-

1450



ments, d1 and d2, generate an appropriate set of 

impostor documents, p1,…,pm and represent each 

of the documents in terms of some large feature set 

(for example, the frequencies of various words or 

character n-grams in the document). For some ran-

dom subset of the feature set, measure the similari-

ty of d1 to d2 as well as to each of the documents 

p1,…,pm and note if d1 is closer to d2 than to any of 

the impostors. Repeat this k times, choosing a dif-

ferent random subset of the features in each itera-

tion. If d1 is closer to d2 than to any of the 

impostors (and likewise switching the roles of d1 

and d2) for at least θ% of iterations, then output 

that d2 and d1 are the same author. (The parameter 

θ is used to trade-off recall and precision.)  

Adapting that method for our purposes, we use 

the proportion of iterations for which d1 is closer to 

d2 than to any of the impostors as our similarity 

measure (adding a small twist to make the measure 

symmetric over d1 and d2, as can be seen in line 

2.2.2 of the algorithm). More precisely, we do the 

following:  

 
Given: Corpus D={d1,…,dn} 
1. Choose a feature set FS for representing documents, a 

first-order similarity measure sim, and an impostor set 

{p1,…,pm}. 

2. For each pair of documents <di, dj> in set D: 
2.1. Let sim2(di, dj) := 0 
2.2. Iterate K times: 

2.2.1. Randomly choose 40% of features in FS 
2.2.2. If sim(di, dj)

 2  >   

maxu∈{1,..,m}sim(di,  pu)*maxu∈{1,..,m}sim(dj, pu), 

then sim2(di, dj) ≔ sim2(di, dj) + 1/K 
3. For each document di in set D: 

3.1. Compute sim2(di, D) = agg w∈{1,..,n}[sim2(di, dw)] 
where agg is some aggregation function  

3.2. If sim2(di, D) < θ (where θ is a parameter),  
then mark di as outlier. 
 

The method for choosing the impostor set is 

corpus-dependent, but quite straightforward: we 

simply choose random impostors from the same 

genre and language as the documents in question. 

The choice of feature set FS, first-order similarity 

measure sim, and aggregation function agg can be 

varied. For FS, we simply use bag-of-words 

(BOW). As for sim and agg, we show below re-

sults of experiments comparing the effectiveness of 

various choices for these parameters.  

Using second-order similarity has several sur-

face advantages over standard first-order measures. 

First, it is decisive: for most pairs, second-order 

similarity will be close to 0 or close to 1. Second, it 

is self-normalizing: scaling doesn’t depend on the 

size of the underlying feature sets or the lengths of 

the documents. As we will see, it is also simply 

much more effective for identifying outliers. 

5 Experiments 

We begin by assembling a corpus consisting of 

3540 blog posts written by 156 different bloggers. 

The blogs are taken from the blog corpus assem-

bled by Schler et al. (2006) for use in authorship 

attribution tasks. Each of the blogs was written in 

English by a single author in 2004 and each post 

consists of 1000 words (excess is truncated). 

For our initial experiments, each trial consists of 

10 blog posts, all but p of which are by a single 

blogger. The number of pseudepigraphic docu-

ments, p, is chosen from a uniform distribution 

over the set {0,1,2,3}. Our task is to identify 

which, if any, documents in the set are not by the 

main author of the set. The pseudepigraphic docu-

ments might be written by a single author or by 

multiple authors. 

To measure the performance of a given similari-

ty measure sim, we do the following in each trial: 

1. Represent each document in the trial set D 
in terms of BOW. 

2. Measure the similarity of each pair of doc-
uments in the trial set using the similarity 

measure sim. 

3. Using some aggregation function agg, 
compute for each document di:   

sim(di, D) = agg w∈{1,..,n}[sim(di, dw)]. 

4. If sim (di, D) < θ, mark di as an outlier 
(where θ is a parameter ). 

 

Our objective is to show that results using 

second-order similarity are stronger than those us-

ing first-order similarity. Before we do this, we 

need to determine the best aggregation function to 

use in our experiments. In Figure 1, we show re-

call-precision breakeven values (for the outlier 

class) over 250 independent trials, for each of our 

four first-order similarity measures (inverse Eucli-

dean, inverse Manhattan, cosine, min-max) used in 

conjunction with each of four aggregation func-

tions (centroid, mean, k-NN mean, median). As is 

evident, k-NN is the best aggregation function in 

each case. We will give these baseline methods an 

advantage by using k-NN as our aggregation func-

tion in all our subsequent experiments. 

1451



 
Figure 1. Breakeven values on first-order similarity 

measures with various aggregation functions. 

 

 We are now ready to perform our main expe-

riment. We use BOW as our feature set and k-NN 

as our aggregation function. We use 500 random 

blog posts as our impostor set. In Figure 2, we 

show recall-precision curves for outlier documents 

over 250 independent trials, as just described, us-

ing four first-order similarity measures as well our 

second-order similarity measure using each of the 

four as a base measure. As can be seen, even the 

worst second-order similarity measure significantly 

outperforms all the standard first-order measures. 

In Figure 3, we show the breakeven values for each 

measure, pairing each first-order measure with the 

second-order measure that uses it as a base. Clear-

ly, the mere use of a second-order method im-

proves results, regardless of the base measure. 

 

 
Figure 2. Recall-precision curves for four first-order 

similarity measures and four second-order similarity 

measures, based on 250 trials of 10 documents each. 

 

 
Figure 3. Breakeven values for first-order measures and 

corresponding second-order measures. 

 

 Thus far we have considered authorial corpora 

consisting of only ten documents. In Figures 4 and 

5, we repeat the experiment described in Figures 2 

and 3 above, but with each trial consisting of 50 

documents including any number of pseudepi-

graphic documents in the range 0 to 15. The same 

phenomenon is apparent: second-order similarity 

strongly improves results over the corresponding 

first-order base similarity measure.  

 

 
Figure 4. Recall-precision curves for four first-order 

similarity measures and four second-order similarity 

measures, based on 250 trials of 50 documents each. 

 

1452



 
Figure 5. Breakeven values for first-order measures and 

corresponding second-order measures 

6 Results on Shakespeare 

We applied our methods to the texts of 42 plays by 

Shakespeare (taken from Project Gutenberg). We 

included two plays by Thomas Kyd as sanity 

checks. In addition, we included three plays occa-

sionally attributed to Shakespeare, but generally 

regarded by authorities as pseudepigrapha (A York-

shire Tragedy, The Life of Sir John Oldcastle and 

Pericles Prince of Tyre). We also included King 

Edward III and King Henry VI (Part 1), both of 

which are subjects of dispute among Shakespeare 

scholars. As impostors we used 39 works by con-

temporaries of Shakespeare, including Christopher 

Marlowe, Ben Jonson and John Fletcher.  

We found that the two plays by Thomas Kyd 

and the three pseudepigraphic plays were all 

among the seven furthest outliers, as one would 

expect. In addition, King Edward III was 9th fur-

thest. King Henry VI (Part 1) was not found to be 

an outlier at all. Curiously, however, three undis-

puted plays by Shakespeare were found to be 

greater outliers than King Edward III. These are 

The Merry Wives of Windsor, The Comedy of Er-

rors and The Tragedy of Julius Caesar. The Merry 

Wives of Windsor is a particularly distant outlier, 

even further out than Oldcastle and Pericles. We 

leave it to Shakespeare scholars to explain the rea-

sons for these anomalies. 

7 Conclusion 

In this paper we defined the problem of unsuper-

vised outlier detection in the authorship verifica-

tion domain. Our method combines standard 

outlier detection methods with a novel inter-

document similarity measure. This similarity 

measure is the output of the impostors method re-

cently developed for solving the authorship verifi-

cation problem. We have found that use of the 

kNN method for outlier detection in conjunction 

with this second-order similarity measure strongly 

outperforms methods based on any outlier detec-

tion method used in conjunction with any standard 

first-order similarity measures. This improvement 

proves to be robust, holding for various corpus siz-

es and various underlying base similarity measures 

used in the second-order similarity measure. 

The method can be used to resolve historical 

conundrums regarding the authenticity of works in 

questioned corpora, such as the Shakespeare cor-

pus briefly considered here. This is currently the 

subject of our ongoing research. 

 

References 
S. M. Bendre and B. K. Kale. 1987. Masking effect 

on tests for outliers in normal samples, Biome-

trika, 74(4):891-896.  

Markus M. Breunig,  Hans-Peter Kriegel, 

Raymond T. Ng and Jörg Sander. 2000. LOF: 

Identifying Density-Based Local Outliers, ACM 

SIGMOD Conference Proceedings. 

Varun Chandola, Arindam Banerjee and Vipin 

Kumar. 2009. Anomaly detection: a survey. 

ACM Computing Surveys 41, 3, Article 15.  

David L. Donoho. 1982. Breakdown properties of 

multivariate location estimators. Ph.D. 

qualifying paper, Harvard University. 

Peter Filzmoser, Ricardo Maronna and Mark 

Werner. 2008. Outlier identification in high 

dimensions. Computational Statistics and Data 

Analysis, 52:1694-1711. 

David Guthrie. 2008. Unsupervised Detection of 

Anomalous Text. PhD Thesis, University of 

Sheffield.  

Frank E. Grubbs. 1969. Procedures for detecting 

outlying observations in samples, 

Technometrics. 

V.J. Hodge and J. Austin. 2004. A survey of outlier 

detection methodologies. Artificial. Intelligence 

Review, 22 (2). pp. 85-126. 

1453



Patrick Juola and Efstathios Stamatatos. 2013. 

Overview of the Author Identification Task at 

PAN 2013. P. Forner, R. Navigli, and D. Tufis 

(eds) CLEF 2013 Evaluation Labs and 

Workshop –Working Notes Papers. 

Moshe Koppel and Jonathan Schler 2004. 

Authorship verification as a one-class 

classification problem. In ICML ’04: Twenty-

first International Conference on Machine 

Learning, New York, NY, USA. 

Moshe Koppel, Jonathan Schler, and Shlomo 

Argamon. 2011. Authorship attribution in the 

wild. Language Resources and Evaluation, 

45(1): 83–94. 

Moshe Koppel M. and Yaron Winter. 2013. 

Determining If Two Documents Are by the Same 

Author. J. Am. Soc. Inf. Sci. Technol. 

Frederick Mosteller and David L. Wallace. 1964. 

Inference and Disputed Authorship: The 

Federalist. Reading, Mass. Addison Wesley. 

Hans-Peter Kriegel, Matthias S. Schubert and 

Arthur Zimek. 2008. Angle-based outlier 

detection in high dimensional data. Proc. KDD. 

Thomas C. Mendenhall. 1887. The characteristic 

curves of composition, Science 9, 237-259. 

Sridhar Ramaswamy, Rajeev Rastogi and Kyuseok 

Shim. 2000. Efficient Algorithms for Mining 

Outliers from Large Data Sets. Proc. ACM 

SIDMOD Int. Conf. on Management of Data. 

Peter J. Rousseeuw. 1984. Least median of squares 

regression. Journal of the American Statistical 

Association, 79(388):87-880. 

Peter J. Rousseeuw and Annick M. Leroy. 2003. 

Robust Regression and Outlier Detection. John 

Wiley & Sons. 

J. Schler, M. Koppel, S. Argamon and J. 

Pennebaker. 2006. Effects of Age and Gender on 

Blogging. in Proceedings of 2006 AAAI Spring 

Symposium on Computational Approaches for 

Analyzing Weblogs. 

Werner A. Stahel. 1981. Breakdown of covariance 

estimators. Research Report 31, Fachgruppe f¨ur 

Statistik, Swiss Federal Institute of Technology 

(ETH), Zurich. 

Efstathios Stamatatos. 2009. Intrinsic plagiarism 

detection using character n-gram profiles. 

Proceedings of the SEPLN’09 Workshop on 

Uncovering Plagiarism, Authorship and Social 

Software Misuse. pp. 38–46. 

Benno Stein B, Nedim Lipka and Peter 

Prettenhofer. 2010. Intrinsic Plagiarism 

Analysis. Language Resources and Evaluation, 

1–20. 2010. 

Benno Stein B, Nedim Lipka and Peter 

Prettenhofer. 2010. Intrinsic Plagiarism 

Analysis. Language Resources and Evaluation, 

1–20. 2010. 

 

1454


