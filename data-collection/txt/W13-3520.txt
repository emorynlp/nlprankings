



















































Polyglot: Distributed Word Representations for Multilingual NLP


Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183–192,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

Polyglot: Distributed Word Representations for Multilingual NLP

Rami Al-Rfou’ Bryan Perozzi
Computer Science Dept. Stony Brook University Stony Brook, NY 11794
{ralrfou, bperozzi, skiena}@cs.stonybrook.edu

Steven Skiena

Abstract

Distributed word representations (word
embeddings) have recently contributed
to competitive performance in language
modeling and several NLP tasks. In
this work, we train word embeddings for
more than 100 languages using their cor-
responding Wikipedias. We quantitatively
demonstrate the utility of our word em-
beddings by using them as the sole fea-
tures for training a part of speech tagger
for a subset of these languages. We find
their performance to be competitive with
near state-of-art methods in English, Dan-
ish and Swedish. Moreover, we inves-
tigate the semantic features captured by
these embeddings through the proximity
of word groupings. We will release these
embeddings publicly to help researchers in
the development and enhancement of mul-
tilingual applications.

1 Introduction

Building multilingual processing systems is a
challenging task. Every NLP task involves dif-
ferent stages of preprocessing and calculating in-
termediate representations that will serve as fea-
tures for later stages. These stages vary in com-
plexity and requirements for each individual lan-
guage. Despite recent momentum towards devel-
oping multilingual tools (Nivre et al., 2007; Hajič
et al., 2009; Pradhan et al., 2012), most of NLP
research still focuses on rich resource languages.
Common NLP systems and tools rely heavily on
English specific features and they are infrequently
tested on multiple datasets. This makes them hard
to port to new languages and tasks (Blitzer et al.,
2006).

A serious bottleneck in the current approach
for developing multilingual systems is the require-

ment of familiarity with each language under con-
sideration. These systems are typically carefully
tuned with hand-manufactured features designed
by experts in a particular language. This approach
can yield good performance, but tends to create
complicated systems which have limited portabil-
ity to new languages, in addition to being hard to
enhance and maintain.

Recent advancements in unsupervised feature
learning present an intriguing alternative. In-
stead of relying on expert knowledge, these ap-
proaches employ automatically generated task-
independent features (or word embeddings) given
large amounts of plain text. Recent developments
have led to state-of-art performance in several
NLP tasks such as language modeling (Bengio
et al., 2006; Mikolov et al., 2010), and syntactic
tasks such as sequence tagging (Collobert et al.,
2011). These embeddings are generated as a result
of training “deep” architectures, and it has been
shown that such representations are well suited for
domain adaptation tasks (Glorot et al., 2011; Chen
et al., 2012).

We believe two problems have held back the
research community’s adoption of these methods.
The first is that learning representations of words
involves huge computational costs. The process
usually involves processing billions of words over
weeks. The second is that so far, these systems
have been built and tested mainly on English.

In this work we seek to remove these barriers
to entry by generating word embeddings for over
a hundred languages using state-of-the-art tech-
niques. Specifically, our contributions include:

• Word embeddings - We will release word
embeddings for the hundred and seventeen
languages that have more than 10,000 ar-
ticles on Wikipedia. Each language’s vo-
cabulary will contain up to 100,000 words.
The embeddings will be publicly available at

183



(www.cs.stonybrook.edu/˜dsl), for
the research community to study their charac-
teristics and build systems for new languages.
We believe our embeddings represent a valu-
able resource because they contain a minimal
amount of normalization. For example, we
do not lower case words for European lan-
guages as other studies have done for En-
glish. This preserves features of the under-
lying language.

• Quantitative analysis - We investigate
the embedding’s performance on a part-of-
speech (PoS) tagging task, and conduct qual-
itative investigation of the syntactic and se-
mantic features they capture. Our experi-
ments represent a valuable chance to evalu-
ate distributed word representations for NLP
as the experiments are conducted in a consis-
tent manner and a large number of languages
are covered. As the embeddings capture in-
teresting linguistic features, we believe the
multilingual resource we are providing gives
researchers a chance to create multilingual
comparative experiments.

• Efficient implementation - Training these
models was made possible by our contri-
butions to Theano (machine learning library
(Bergstra et al., 2010)). These optimizations
empower researchers to produce word em-
beddings under different settings or for dif-
ferent corpora than Wikipedia.

The rest of this paper is as follows. In Section
2, we give an overview of semi-supervised learn-
ing and learning representations related work. We
then describe, in Section 3, the network used to
generate the word embeddings and its characteris-
tics. Section 4 discusses the details of the corpus
collection and preparation steps we performed.
Next, in Section 5, we discuss our experimental
setup and the training progress over time. In Sec-
tion 6 we discuss the semantic features captured
by the embeddings by showing examples of the
word groupings in multiple languages. Finally,
in Section 7 we demonstrate the quality of our
learned features by training a PoS tagger on sev-
eral languages and then conclude.

2 Related Work

There is a large body of work regarding semi-
supervised techniques which integrate unsuper-

vised feature learning with discriminative learning
methods to improve the performance of NLP ap-
plications. Word clustering has been used to learn
classes of words that have similar semantic fea-
tures to improve language modeling (Brown et al.,
1992) and knowledge transfer across languages
(Täckström et al., 2012). Dependency parsing
and other NLP tasks have been shown to bene-
fit from such a large unannotated corpus (Koo et
al., 2008), and a variety of unsupervised feature
learning methods have been shown to unilaterally
improve the performance of supervised learning
tasks (Turian et al., 2010). (Klementiev et al.,
2012) induce distributed representations for a pair
of languages jointly, where a learner can be trained
on annotations present in one language and ap-
plied to test data in another.

Learning distributed word representations is a
way to learn effective and meaningful information
about words and their usages. They are usually
generated as a side effect of training parametric
language models as probabilistic neural networks.
Training these models is slow and takes a signif-
icant amount of computational resources (Bengio
et al., 2006; Dean et al., 2012). Several sugges-
tions have been proposed to speed up the training
procedure, either by changing the model architec-
ture to exploit an algorithmic speedup (Mnih and
Hinton, 2009; Morin and Bengio, 2005) or by esti-
mating the error by sampling (Bengio and Senecal,
2008).

(Collobert and Weston, 2008) shows that word
embeddings can almost substitute NLP common
features on several tasks. The system they built,
SENNA, offers part of speech tagging, chunking,
named entity recognition, semantic role labeling
and dependency parsing (Collobert, 2011). The
system is built on top of word embeddings and per-
forms competitively compared to state of art sys-
tems. In addition to pure performance, the system
has a faster execution speed than comparable NLP
pipelines (Al-Rfou’ and Skiena, 2012).

To speed up the embedding generation process,
SENNA embeddings are generated through a pro-
cedure that is different from language modeling.
The representations are acquired through a model
that distinguishes between phrases and corrupted
versions of them. In doing this, the model avoids
the need to normalize the scores across the vocab-
ulary to infer probabilities. (Chen et al., 2013)
shows that the embeddings generated by SENNA

184



Apple apple Bush bush corpora dangerous
Dell tomato Kennedy jungle notations costly
Paramount bean Roosevelt lobster digraphs chaotic
Mac onion Nixon sponge usages bizarre
Flex potato Fisher mud derivations destructive

Table 1: Words nearest neighbors as they appear in the English embeddings.

perform well in a variety of term-based evaluation
tasks. Given the training speed and prior perfor-
mance on NLP tasks in English, we generate our
multilingual embeddings using a similar network
architecture to the one SENNA used.

However, our work differs from SENNA in the
following ways. First, we do not limit our mod-
els to English, we train embeddings for a hundred
and seventeen languages. Next, we preserve lin-
guistic features by avoiding excessive normaliza-
tion to the text. For example, our English model
places “Apple” closer to IT companies and “ap-
ple” to fruits. More examples of linguistic fea-
tures preserved by our model are shown in Table
1. This gives us the chance to evaluate the embed-
dings performance over PoS tagging without the
need for manufactured features. Finally, we re-
lease the embeddings and the resources necessary
to generate them to the community to eliminate
any barriers.

Despite the progress made in creating dis-
tributed representations, combining them to pro-
duce meaning is still a challenging task. Sev-
eral approaches have been proposed to address
feature compositionality for semantic problems
such as paraphrase detection (Socher et al., 2011),
and sentiment analysis (Socher et al., 2012) using
word embeddings.

3 Distributed Word Representation

Distributed word representations (word embed-
dings) map the index of a word in a dictionary to a
feature vector in high-dimension space. Every di-
mension contributes to multiple concepts, and ev-
ery concept is expressed by a combination of sub-
set of dimensions. Such mapping is learned by
back-propagating the error of a task through the
model to update random initialized embeddings.
The task is usually chosen such that examples can
be automatically generated from unlabeled data
(i.e so it is unsupervised). In case of language
modeling, the task is to predict the last word of
a phrase that consists of n words.

In our work, we start from the example con-
struction method outlined in (Bengio et al., 2009).
They train a model by requiring it to distinguish
between the original phrase and a corrupted ver-
sion of the phrase. If it does not score the
original one higher than the corrupted one (by
a margin), the model will be penalized. More
precisely, for a given sequence of words S =
[wi−n . . . wi . . . wi+n] observed in the corpus T ,
we will construct another corrupted sequence S′

by replacing the word in the middle wi with a word
wj chosen randomly from the vocabulary. The
neural network represents a function score that
scores each phrase, the model is penalized through
the hinge loss function J(T ) as shown in 1.

J(T ) =
1

|T |
∑

i∈T
|1−score(S′)+score(S)|+ (1)

Figure 1 shows a neural network that takes a se-
quence of words with size 2n + 1 to compute a
score. First, each word is mapped through a vo-
cabulary dictionary with the size |V | to an index
that is used to index a shared matrix C with the
size |V |∗M where M is the size of the vector rep-
resenting the word. Once the vectors are retrieved,
they are concatenated into one vector called pro-
jection layer P with size (2n + 1) ∗M . The pro-
jection layer plays the role of an input to a hidden
layer with size |H|, the activations A of which are
calculated according to equation 3, where W1, b1
are the weights and bias of the hidden layer.

A = tanh(W1P + b1) (2)

To calculate the phrase score, a linear combina-
tion of the hidden layer activations A is computed
using W2 and b2.

score(P ) = W2A+ b2 (3)

Therefore, the five parameters that have to be
learned are W1, W2, b1, b2, C with a total number
of parameters (2n+ 1) ∗M ∗H +H +H + 1+
|V | ∗M ≈M ∗ (nH + |V |) .

185



C
Imagination

C
is

C
greater

C
than

C
detail

Score

Hidden Layer
H

C

M

|V
|

Projection 
Layer

Figure 1: Neural network architecture. Words are
retrieved from embeddings matrix C and concate-
nated at the projection layer as an input to com-
puter the hidden layer activation. The score is
the linear combination of the activation values of
the hidden layer. The scores of two phrases are
ranked according to hinge loss to distinguish the
corrupted phrase from the original one.

4 Corpus Preparation

We have chosen to generate our word embeddings
from Wikipedia. In addition to size, there are other
desirable properties that we wish for the source of
our language model to have:

• Size and variety of languages - As of this
writing (April, 2013), 42 languages had more
than 100,000 article pages, and 117 lan-
guages had more than 10,000 article pages.
• Well studied - Wikipedia is a prolific re-

source in the literature, and has been used
for a variety of problems. Particularly,
Wikipedia is well suited for multilingual ap-
plications (Navigli and Ponzetto, 2010).
• Quality - Wikipedians strive to write arti-

cles that are readable, accurate, and consist
of good grammar.
• Openly accessible - Wikipedia is a resource

available for free use by researchers
• Growing - As technology becomes more ac-

cessible, the size and scope of the multilin-
gual Wikipedia effort continues to expand.

To process Wikipedia markup, we first extract
the text using a modified version of the Bliki en-

gine1. Next we must tokenize the text. We rely
on an OpenNLP probabilistic tokenizer whenever
possible, and default to the Unicode text segmen-
tation2 algorithm offered by Lucene when we have
no such OpenNLP model. After tokenization, we
normalize the tokens to reduce their sparsity. We
have two main normalization rules. The first re-
places digits with the symbol #, so “1999” be-
comes ####. In the second, we remove hyphens
and brackets that appear in the middle of a token.
As an additional rule for English, we map non-
Latin characters to their unicode block groups.

In order to capture the syntactic and semantic
features of words, we must observe each word sev-
eral times in each of its valid contexts. This re-
quirement, when combined with the Zipfian dis-
tribution of words in natural language, implies that
learning a meaningful representation of a language
requires a huge amount of unstructured text. In
practice we deal with this limitation by restricting
ourselves to considering the most frequently oc-
curring tokens in each language.

Table 2 shows the size of each language corpus
in terms of tokens, number of word types and cov-
erage of text achieved by building a vocabulary out
of the most frequent 100,000 tokens, |V |. Out of
vocabulary (OOV) words are replaced with a spe-
cial token 〈UNK〉.

While Wikipedia has 284 language specific en-
cyclopedias, only five of them have more than a
million articles. The size drops dramatically, such
that the 42nd largest Wikipedia, Hindi, has slightly
above 100,000 articles and the 100th, Tatar, has
slightly over 16,000 articles3.

Significant Wikipedias in size have a word cov-
erage over 92% except for German, Russian, Ara-
bic and Czech which shows the effect of heavy us-
age of morphological forms in these languages on
the word usage distribution.

The highest word coverage we achieve is unsur-
prisingly for Chinese. This is expected given the
limited size vocabulary of the language - the num-
ber of entries in the Contemporary Chinese Dictio-
nary are estimated to be 65 thousand words (Shux-
iang, 2004).

1Java Wikipedia API (Bliki engine) - http://code.
google.com/p/gwtwiki/

2http://www.unicode.org/reports/tr29/
3http://meta.wikimedia.org/w/index.

php?title=List_of_Wikipedias&oldid=
5248228

186



Language Tokens Words Coverage∗106 ∗103
English 1,888 12,125 96.30%
German 687 9,474 91.78%
French 473 4,675 95.78%
Spanish 399 3,978 96.07%
Russian 328 5,959 90.43%
Italian 322 3,642 95.52%
Portuguese 197 2,870 95.68%
Dutch 197 3,712 93.81%
Chinese 196 423 99.67%
Swedish 101 2,707 92.36%
Czech 80 2,081 91.84%
Arabic 52 1,834 91.78%
Danish 44 1,414 93.68%
Bulgarian 39 1,114 94.35%
Slovene 30 920 94.42%
Hindi 23 702 96.25%

Table 2: Statistics of a subset of the languages pro-
cessed. The second column reports the number of
tokens found in the corpus in millions while the
third column reports the word types found in thou-
sands. The coverage indicates the percentage of
the corpus that will be matching words in a vocab-
ulary consists of the most frequent 100 thousand
words.

5 Training

For our experiments, we build a model as the one
described in Section 3 using Theano (Bergstra et
al., 2010). We choose the following parameters,
context window size 2n + 1 = 5, vocabulary
|V | = 100, 000, word embedding size M = 64,
and hidden layer size H = 32. The intuition, here,
is to maximize the relative size of the embeddings
compared to the rest of the network. This might
force the model to store the necessary information
in the embeddings matrix instead of the hidden
layer. Another benefit is that we will avoid over-
fitting on the smaller Wikipedias. Increasing the
window size or the embedding size slows down
the training speed, making it harder to converge
within a reasonable time.

The examples are generated by sweeping a win-
dow over sentences. For each sentence in the cor-
pus, all unknown words are replaced with a special
token 〈UNK〉 and sentences are padded with 〈S〉,
〈/S〉 tokens. In case the window exceeds the edges
of a sentence, the missing slots are filled with our
padding token, 〈PAD〉.

Figure 2: Training and test errors of the French
model after 23 days of training. We did not notice
any overfitting while training the model. The error
curves are smoother the larger the language corpus
is.

To train the model, we consider the data in mini-
batches of size 16. Every 16 examples, we es-
timate the gradient using stochastic gradient de-
scent (Bottou, 1991), and update the parameters
which contributed to the error using backpropaga-
tion (Rumelhart et al., 2002). Calculating an exact
gradient is prohibitive given that the dataset size is
in millions of examples. We calculate the devel-
opment error by sampling randomly 10000 mini-
batches from the development dataset.

For each language, we set the batch size to 16
examples, and the learning rate to be 0.1. Follow-
ing, (Collobert et al., 2011)’s advice, we divide
each layer by the fan in of that layer, and we con-
sider the embeddings layer to have a fan in of 1.
We divide the corpus to three sets, training, devel-
opment and testing with the following percentages
90, 5, 5 respectively.

One disadvantage of the approach used by (Col-
lobert et al., 2011) is that there is no clear stop-
ping criteria for the model training process. We
have noticed that after a few weeks of training,
the model’s performance reaches the point where
there is no significant decrease in the average loss
over the development set, and when this occurs we
manually stop the training. An interesting prop-
erty of this model is that we did not notice any
sign of overfitting for large Wikipedias. This could
be explained by the infinite amount of examples
we can generate by randomly choosing the re-

187



Word Translation Word Translation Word Word

Fr
en

ch

rouge red

Sp
an

is
h

dentista dentist

E
ng

lis
h

Mumbai Bombay
juane yellow peluquero barber Chennai Madras
rose pink ginecólog gynecologist Bangalore Shanghai
blanc white camionero truck driver Kolkata Calultta
orange orange oftalmólogo ophthalmologist Cairo Bangkok
bleu blue telegrafista telegraphist Hyderabad Hyderabad

A
ra

bi
c

A
ra

bi
c

G
er

m
an

Jkr� thanks ¤d�  two boys Eisenbahnbetrieb rail operations
¤Jkr� and thanks ��nA  two sons Fahrbetrieb driving
��yA�¨ greetings ¤d§ two boys Reisezugverkehr passenger trains
Jkr�¾ thanks + diacritic Vf®  two children Fährverkehr ferries
¤Jkr�¾ and thanks + diacritic ��ny two sons Handelsverkehr Trade
r�bA hello ��ntA  two daughters Schülerverkehr students Transport

R
us

si
an

C
hi

ne
se

Transliteration

It
al

ia
n

Ïóòèí Putin dongzhi Winter Solstice papa Pope
ßíóêîâè÷ Yanukovych chunfen Vernal Equinox Papa Pope
Òðîöêèé Trotsky xiazhi Summer solstice pontefice pontiff
Ãèòëåð Hitler qiufen Autumnal Equinox basileus basileus
Ñòàëèí Stalin ziye Midnight canridnale cardinal
Ìåäâåäåâ Medvedev chuxi New Year’s Eve frate friar

Table 3: Examples of the nearest five neighbors of every word in several languages. Translation is
retrieved from http://translate.google.com.

placement word in the corrupted phrase. Figure
2 shows a typical learning curve of the training.
As the number of examples have been seen so far
increased both the training error and the develop-
ment error go down.

6 Qualitative Analysis

In order to understand how the embeddings space
is organized, we examine the subtle information
captured by the embeddings through investigating
the proximity of word groups. This information
has the potential to help researchers develop ap-
plications that use such semantic and syntactic in-
formation. The embeddings not only capture syn-
tactic features, as we will demonstrate in Section
4, but also demonstrate the ability to capture in-
teresting semantic information. Table 3 shows dif-
ferent words in several languages. For each word
on top of each list, we rank the vocabulary accord-
ing to their Euclidean distance from that word and
show the closest five neighboring words.
• French & Spanish - Expected groupings of

colors and professions is clearly observed.
• English - The example shows how the em-

bedding space is aware of the name change
that happened to a group of Indian cities.
“Mumbai” used to be called “Bombay”,
“Chennai” used to be called “Madras and
“Kolkata” used to be called “Calcutta”. On
the other hand, “Hyderabad” stayed at a sim-
ilar distance from both names as they point to
the same conceptual meaning.
• Arabic - The first example shows the word

“Thanks”. Despite not removing the diacrit-

ics from the text, the model learned that the
two surface forms of the word mean similar
things and, therefore, grouped them together.
In Arabic, conjunction words do not get sepa-
rated from the following word. Usually, ”and
thanks” serves as a letter signature as “sin-
cerely” is used in English. The model learned
that both words {“and thanks”, “thanks” }
are similar, regardless their different forms.
The second example illustrates a specific syn-
tactic morphological feature of Arabic, where
enumeration of couples has its own form.
• German - The example demonstrates that the

compositional semantics of multi-unit words
are still preserved.
• Russian - The model learned to group Rus-

sian/Soviet leaders and other figures related
to the Soviet history together.
• Chinese - The list contains three solar terms

that are part of the traditional East Asian lu-
nisolar calendars. The remaining two terms
correspond to traditional holidays that occur
at the same dates of these solar terms.
• Italian - The model learned that the lower

and upper cases of the word has similar
meaning.

7 Sequence Tagging

Here we analyze the quality of the models we have
generated. To test the quantitative performance of
the embeddings, we use them as the sole features
for a well studied NLP task, part of speech tag-
ging.

To demonstrate the capability of the learned dis-

188



Language Source Test TnTUnknown Known All
German Tiger† (Brants et al., 2002) 89.17% 98.60% 97.85% 98.10%
Bulgarian BTB† (Simov et al., 2002) 75.74% 98.33% 96.33% 97.50%
Czech PDT 2.5 (Bejček et al., 2012) 71.98% 99.15% 97.13% 99.10%
Danish DDT† (Kromann, 2003) 73.03% 98.07% 96.45% 96.40%
Dutch Alpino† (Van der Beek et al., 2002) 73.47% 95.85% 93.86% 95.00%
English PennTreebank (Marcus et al., 1993) 75.97% 97.74% 97.18% 96.80%
Portuguese Sint(c)tica† (Afonso et al., 2002) 75.36% 97.71% 95.95% 96.80%
Slovene SDT† (Džeroski et al., 2006) 68.82% 95.17% 93.46% 94.60%
Swedish Talbanken05† (Nivre et al., 2006) 83.54% 95.77% 94.68% 94.70%

Table 4: Results of our model against several PoS datasets. The performance is measured using accuracy
over the test datasets. Third column represents the total accuracy of the tagger the former two columns
reports the accuracy over known words and OOV words (unknown). The results are compared to the
TnT tagger results reported by (Petrov et al., 2012).
†CoNLL 2006 dataset

tributed representations in extracting useful word
features, we train a PoS tagger over the subset of
languages that we were able to acquire free anno-
tated resources for. We choose our tagger for this
task to be a neural network because it has a fast
convergence rate based on our initial experiments.

The part of speech tagger has similar architec-
ture to the one used for training the embeddings.
However we have changed some of the network
parameters, specifically, we use a hidden layer of
size 300 and learning rate of 0.3. The network is
trained by minimizing the negative of the log like-
lihood of the labeled data. To tag a specific word
wi we consider a window with size 2n where n
in our experiment is equal to 2. Equation 4 shows
how we construct a feature vector F by concate-
nating (⊕) the embeddings of the words occurred
in the window, where C is the matrix that contains
the embeddings of the language vocabulary.

F =
i+2⊕

j=i−2
C[wj ] (4)

The feature vector will be fed to the network and
the error will back propagated back to the embed-
dings.

The results of this experiment are presented in
Table 4. We train and test our models on the uni-
versal tagset proposed by (Petrov et al., 2012).
This universal tagset maps each original tag in a
treebank to one out of twelve general PoS tags.
This simplifies the comparison of classifiers per-
formance across languages. We compare our re-
sults to a similar experiment conducted in their

work, where they trained a TnT tagger (Brants,
2000) on several treebanks. The TnT tagger is
based on Markov models and depends on trigram
counts observed in the labeled data. It was cho-
sen for its fast speed and (near to) state-of-the-art
accuracy, without language specific tuning.

The performance of embeddings is competitive
in general. Surprisingly, it is doing better than the
TnT tagger in English and Danish. Moreover, our
performance is so close in the case of Swedish.
This task is hard for our tagger for two reasons.
The first is that we do not add OOV words seen
during training of the tagger to our vocabulary.
The second is that all OOV words are substituted
with one representation, 〈UNK〉 and there is no
character level information used to inform the tag-
ger about the characteristic of the OOV words.

On the other hand, the performance on the
known words is strong and consistent showing the
value of the features learned about these words
from the unsupervised stage. Although the word
coverage of German and Czech are low in the orig-
inal Wikipedia corpora (See Table 2), the features
learned are achieving great accuracy on the known
words. They both achieve above 98.5% accuracy.
It is noticeable that the Slovene model performs
the worst, under both known and unknown words
categories. It achieves only 93.46% accuracy on
the test dataset. Given that the Slovene embed-
dings were trained on the least amount of data
among all other embeddings we test here, we ex-
pect the quality to go lower for the other smaller
Wikipedias not tested here.

189



In Table 5, we present how well the vocabulary
of each language’s embeddings covered the part of
speech datasets. The datasets come from a differ-
ent domain than Wikipedia, and this is reflected in
the results.

In Table 6, we present the results of training the
same neural network part of speech tagger with-
out using our embeddings as initializations. We
found that the embeddings benefited all the lan-
guages we considered, and observed the greatest
benefit in languages which had a small number of
training examples. We believe that these results
illustrate the performance

Language % Token % WordCoverage Coverage
Bulgarian 94.58 77.70
Czech 95.37 65.61
Danish 95.41 80.03
German 94.04 60.68
English 98.06 79.73
Dutch 96.25 77.76
Portuguese 94.09 72.66
Slovene 95.33 83.67
Swedish 95.87 73.92

Table 5: Coverage statistics of the embedding’s
vocabulary on the part of speech datasets after nor-
malization. Token coverage is the raw percentage
of words which were known, while the Word cov-
erage ignores repeated words.

8 Conclusion

Distributed word representations represent a valu-
able resource for any language, but particularly for
resource-scarce languages. We have demonstrated
how word embeddings can be used as off-the-shelf
solution to reach near to state-of-art performance
over a fundamental NLP task, and we believe that
our embeddings will help researchers to develop
tools in languages with which they have no exper-
tise.

Moreover, we showed several examples of in-
teresting semantic relations expressed in the em-
beddings space that we believe will lead to inter-
esting applications and improve tasks as semantic
compositionality.

While we have only considered the properties of
word embeddings as features in this work, it has
been shown that using word embeddings in con-
junction with traditional NLP features can signifi-

Language # Training AccuracyExamples Drop
Bulgarian 200,049 -2.01%
Czech 1,239,687 -0.86%
Danish 96,581 -1.77%
German 735,826 -0.89%
English 950,561 -0.25%
Dutch 208,418 -1.37%
Portuguese 212,749 -0.91%
Slovene 27,284 -2.68%
Swedish 199,509 -0.82%

Table 6: Accuracy of randomly initialized tag-
ger compared to our results. Using the embed-
dings was generally helpful, especially in lan-
guages where we did not have many training ex-
amples. The scores presented are the best we
found for each language (languages with more re-
sources could afford to train longer before overfit-
ting).

cantly improve results on NLP tasks (Turian et al.,
2010; Collobert et al., 2011). With this in mind,
we believe that the entire research community can
benefit from our release of word embeddings for
over 100 languages.

We hope that these resources will advance the
study of possible pair-wise mappings between em-
beddings of several languages and their relations.
Our future work in this area includes improving
the models by increasing the size of the context
window and their domain adaptivity through in-
corporating other sources of data. We will be
investigating better strategies for modeling OOV
words. We see improvements to OOV word han-
dling as essential to ensure robust performance of
the embeddings on real-world tasks.

Acknowledgments

This research was partially supported by NSF
Grants DBI-1060572 and IIS-1017181, with ad-
ditional support from TexelTek.

References
Susana Afonso, Eckhard Bick, Renato Haber, and Di-

ana Santos. 2002. Floresta sintá (c) tica”: a treebank
for portuguese. In Proc. of the Third Intern. Conf. on
Language Resources and Evaluation (LREC), pages
1698–1703.

Rami Al-Rfou’ and Steven Skiena. 2012. Speedread:
A fast named entity recognition pipeline. In Pro-

190



ceedings of the 24th International Conference on
Computational Linguistics (Coling 2012), pages 53–
61, Mumbai, India, December. Coling 2012 Orga-
nizing Committee.

Eduard Bejček, Jarmila Panevová, Jan Popelka, Pavel
Straňák, Magda Ševčı́ková, Jan Štěpánek, and
Zdeněk Žabokrtský. 2012. Prague Dependency
Treebank 2.5 – a revisited version of PDT 2.0.
In Proceedings of COLING 2012, pages 231–246,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.

Yoshua Bengio and J-S Senecal. 2008. Adaptive im-
portance sampling to accelerate training of a neu-
ral probabilistic language model. Neural Networks,
IEEE Transactions on, 19(4):713–722.

Y. Bengio, H. Schwenk, J.S. Senécal, F. Morin, and J.L.
Gauvain. 2006. Neural probabilistic language mod-
els. Innovations in Machine Learning, pages 137–
186.

Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In International Con-
ference on Machine Learning, ICML.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney, Aus-
tralia.

Léon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-Nı̂mes
91, Nimes, France. EC2.

Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In IN PROCEEDINGS OF THE WORK-
SHOP ON TREEBANKS AND LINGUISTIC THEO-
RIES, pages 24–41.

Thorsten Brants. 2000. Tnt: a statistical part-of-
speech tagger. In Proceedings of the sixth confer-
ence on Applied natural language processing, pages
224–231. Association for Computational Linguis-
tics.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.

Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In John Langford and

Joelle Pineau, editors, Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML-
12), ICML ’12, pages 767–774. ACM, New York,
NY, USA, July.

Yanqing Chen, Bryan Perozzi, Rami Al-Rfou’, and
Steven Skiena. 2013. The expressive power of word
embeddings. CoRR, abs/1301.3226.

R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.

Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In AISTATS.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,
Matthieu Devin, Quoc Le, Mark Mao, Marc’Aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and
Andrew Ng. 2012. Large scale distributed deep net-
works. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges,
L. Bottou, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems 25, pages
1232–1240.

Sašo Džeroski, Tomaž Erjavec, Nina Ledinek, Petr Pa-
jas, Zdenek Žabokrtsky, and Andreja Žele. 2006.
Towards a slovene dependency treebank. In Proc. of
the Fifth Intern. Conf. on Language Resources and
Evaluation (LREC).

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the Twenty-eight International Confer-
ence on Machine Learning (ICML’11), volume 27,
pages 97–110, June.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING
2012, pages 1459–1474, Mumbai, India, December.
The COLING 2012 Organizing Committee.

Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In In Proc. ACL/HLT.

191



Matthias Trautner Kromann. 2003. The danish depen-
dency treebank and the dtag treebank tool. In Pro-
ceedings of the Second Workshop on Treebanks and
Linguistic Theories (TLT), page 217.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.

T. Mikolov, M. Karafiát, L. Burget, J. Cernocky, and
S. Khudanpur. 2010. Recurrent neural network
based language model. Proceedings of Interspeech.

Andriy Mnih and Geoffrey E Hinton. 2009. A scalable
hierarchical distributed language model. Advances
in neural information processing systems, 21:1081–
1088.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246–252.

Roberto Navigli and Simone Paolo Ponzetto. 2010.
Babelnet: Building a very large multilingual seman-
tic network. In Proceedings of the 48th annual meet-
ing of the association for computational linguistics,
pages 216–225. Association for Computational Lin-
guistics.

Joakim Nivre, Jens Nilsson, and Johan Hall. 2006.
Talbanken05: A swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
the fifth International Conference on Language Re-
sources and Evaluation (LREC), pages 1392–1395.

Joakim Nivre, Johan Hall, Sandra Kübler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Mehmet Uğur Doğan, Bente Maegaard,
Joseph Mariani, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC’12), Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Sixteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2012), Jeju, Korea.

David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 2002. Learning representations by back-
propagating errors. Cognitive modeling, 1:213.

Lu Shuxiang. 2004. The Contemporary Chinese Dic-
tionary (Xiandai Hanyu Cidian). Commercial Press.

Kiril Simov, Petya Osenova, Milena Slavcheva,
Sia Kolkovska, Elisaveta Balabanova, Dimitar
Doikoff, Krassimira Ivanova, Er Simov, and Milen
Kouylekov. 2002. Building a linguistically inter-
preted corpus of bulgarian: the bultreebank. In In:
Proceedings of LREC 2002, Canary Islands.

Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems 24.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).

Oscar Täckström, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 477–487. Asso-
ciation for Computational Linguistics.

J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 384–394. Association for Com-
putational Linguistics.

Leonoor Van der Beek, Gosse Bouma, Rob Malouf,
and Gertjan Van Noord. 2002. The alpino depen-
dency treebank. Language and Computers, 45(1):8–
22.

192


