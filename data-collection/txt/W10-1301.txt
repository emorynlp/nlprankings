










































Using NLG and Sensors to Support Personal Narrative for Children with Complex Communication Needs


Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 1–9,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Using NLG and Sensors to Support Personal Narrative for 

Children with Complex Communication Needs 

 
 

Rolf Black Joe Reddington, Ehud Reiter, Nava Tintarev Annalu Waller 
School of Computing Department of Computing Science School of Computing 

University of Dundee            University of Aberdeen University of Dundee            
rolfblack@ 

computing.dundee.ac.uk 

{j.reddington, e.reiter  n.tintarev}@abdn.ac.uk awaller@ 

computing.dundee.ac.uk 

 
 

Abstract 

We are building a tool that helps children with 

Complex Communication Needs
1
 (CCN) to 

create stories about their day at school. The 

tool uses Natural Language Generation (NLG) 

technology to create a draft story based on 

sensor data of the child’s activities, which the 

child can edit. This work is still in its early 

stages, but we believe it has great potential to 

support interactive personal narrative which is 

not well supported by current Augmentative 

and Alternative Communication (AAC) tools. 

1 Introduction 

Many tools have been developed to help children and 

adults who cannot speak (or who have limited speech) 

communicate better.  However, most of these tools have 

focused on supporting communication for practical 

goals, such as “I am thirsty.” But human communica-

tion is also used for social goals; we develop friendships 

and other inter-personal relationships via social interac-

tion and communication. The bulk of conversation is 

characterized by free narrative (Cheepen 1988). One of 

the most important types of conversational narrative is 

personal narrative: someone telling a story about what 

happened to him or her. 

 People with limited or no functional speech do tell 

stories, but these tend to be in monologue form, or in a 

sequence of pre-stored utterances on voice output com-

munication aids (Waller 2006). Individuals who use 

                                                           
1 The term Complex Communication Needs (CCN) describes 

individuals who, due to motor, language, cognitive, and/or 

sensory perceptual impairments (e.g., as a result of cerebral 

palsy), do not develop speech and language skills as expected. 

This heterogeneous group typically experiences restricted 

access to the environment, limited interactions with their 

communication partners, and few opportunities for communi-

cation (Light and Drager 2007). 

Augmentative and Alternative Communication (AAC) 

tools tend to be passive, responding to questions with 

single words or short sentences (e.g. Soto, Hartmann et 

al. 2006) and if able to initiate and maintain extended 

conversations tend to relate experience word for word 

each time they tell a story, even though much of conver-

sation is reused (Clarke and Clarke 1977). This is time 

consuming and physically exhausting – typical rates 

range from 8 to 10 words per minute up to 12 to 15 per 

minute when techniques such as word prediction are 

used (Higginbotham, Shane et al. 2007), with the result 

that people seldom engage in storytelling. Despite the 

importance of narrative, little work has been done on 

specific tools to help language-impaired individuals 

engage in personal storytelling. In this paper, we de-

scribe our work in progress on building a tool that uses 

Natural Language Generation (NLG) technology to help 

children tell stories about their day at school, describing 

both the work we have done to date, and the challenges 

that we face in further developing this concept. 

2 Background 

2.1 AAC 

Technology underpins much of Augmentative and Al-

ternative Communication (AAC), a field that attempts to 

augment natural speech and provides alternative ways to 

communicate for people with limited or no speech. At 

the simplest level, people with Complex Communica-

tion Needs (CCN) can cause a pre-stored message to be 

spoken by activating a single switch. At the most so-

phisticated level, literate users can generate novel text 

using input methods ranging from a single switch to a 

full keyboard.  

Despite advances in AAC, there are still many indi-

viduals for whom communication remains problematic. 

Although some individuals with CCN become effective 

communicators, most do not – they tend to be passive 

communicators, responding mainly to questions or 

prompts at a one or two word level. Conversational 

1



skills such as initiation, elaboration and storytelling are 

seldom observed (Waller 2006). 

One reason for the reduced levels of communicative 

ability is the cognitive demands of AAC interfaces. Cur-

rent AAC technology provides the user with a purely 

physical link to speech output. The user is required to 

have sufficient cognitive abilities and physical stamina 

to translate what they want to say into the sequence of 

operations needed to produce the desired output. Mne-

monic codes and dynamic displays (Beukelman and 

Mirenda 2005) provide some help in the retrieval 

process, but users still have to master complex retrieval 

and production strategies.  

A second reason for the impoverished quality of 

conversation is the focus of AAC devices on transac-

tional communication; conversation which expresses 

needs wants and information transfer, for example, “I 

am thirsty”, “I use a straw for drinking”. Instead, inter-

active conversation is characterized by free narrative 

and phatic conversation, for example, “Guess what 

happened this morning…”, “Hello”, and “How are 

you?” Without easy access to extended interactive 

communication, it is difficult to develop the skills 

needed to initiate new topics and engage in storytelling.  

2.2 Importance of Narrative 

 Conversational narratives (oral stories told during 

interactive conversations) are crucial to social engage-

ment. Narratives provide a means for people to relate 

and share experiences, develop organizational skills, 

work through problems, develop self image, express 

personality, give form and meaning to life, and allow 

people to be interesting entertainers (Waller 2006).  

 Narrative skills develop experientially with children 

being able to engage in storytelling even before they are 

verbal (Bruner 1975). Early personal experience stories 

consist of a high point, for example, “Mummy fall!” 

with adults scaffolding the full story, eliciting the ‘who’, 

‘what’, ‘when’ and ‘where’. However, not all expe-

riences make good stories. An experience becomes a 

story if the storyteller has an emotional connection to 

the event (Labov, 1972), or if the event is unusual (Qua-

sthoff & Nikolaus, 1982). 

 Parents of typically developing children encourage 

development of narrative skills by eliciting stories from 

their children (Peterson and McCabe 1983), but the de-

velopment of narrative skills is problematic for people 

with CCN. We recall a study where disabled children 

were told different stories more often than typically 

developing peers who were read the same story night 

after night (Light, Binger et al. 1994). In doing so, the 

disabled children did not have the chance to learn the 

sequence of stories, or the structure commonly used in 

narrative such as beginning, middle and end. As such, 

initially children should use the same story template 

consistently until they are ready to progress to another 

one. 

 It is difficult to provide access to event information 

which may become a story, and few AAC systems pro-

vide support for interactive story narration. However 

NLG gives us a possibility to change the underlying 

paradigm of AAC. Instead of placing the entire cogni-

tive load on the user, AAC devices can be designed to 

support the retrieval of story events and the scaffolding 

of story narration for individuals with CCN. 

2.3 NLG, Data-to-text 

NLG systems generate texts in English (or other human 

languages) from non-linguistic data (Reiter and Dale 

2000).  Our vision is to use an NLG system to generate 

a draft story, which the child can edit.  The non-

linguistic input to our story-generator is sensor data 

about the child’s activities, including location data 

(where the child was) and interaction data (what people 

and objects the child interacted with).  We also want to 

allow teachers and school staff to enter information 

about the child’s activities (such as voice messages). 

 A number of data-to-text systems (Reiter 2007) have 

been developed in recent years, which generate English 

summaries of sensor and other numerical data.  The 

most popular application area has been weather fore-

casting (generating textual weather forecasts from the 

results of a numerical atmosphere simulation model), 

and indeed several weather forecast generators have 

been fielded and used operationally (Goldberg, Driedger 

et al. 1994; Reiter, Sripada et al. 2005). A number of 

data-to-text systems have also been developed in the 

medical community, such as BabyTalk (Gatt, Portet et 

al. 2009), which generates summaries of clinical data 

from a neonatal intensive care unit, and the commercial 

Narrative Engine (Harris 2008) which summarizes data 

acquired during a doctor/patient encounter. 

 Most previous research in data-to-text has focused 

on summarizing technical data for expert users, with the 

goal of effectively communicating key information.  In 

our work, in contrast, the focus is on summarizing data 

about everyday events, with the goal of having some-

thing interesting to talk about.  There has been consider-

able work in the computational creativity community on 

generating interesting stories (Péréz and Sharples 2004), 

but it has focused on fictional written stories, where the 

computer system can say whatever it wishes, without 

the constraint of describing real events. 

 Most previous work in NLG has focused on com-

puter systems which generate texts without human in-

put.  However, in our case we want children to be able 

to annotate (evaluate) and edit stories, as far as their 

abilities permit.  There has been some research on hu-

man post-editing of NLG texts (Sripada, Reiter et al. 

2005), but this has focused on editing at the text level.  

2



Since editing at the text level is very laborious for AAC 

users, we need a higher-level interface that lets children 

edit content and structure without needing to type 

words.  We also want children to be able to control how 

a story is narrated, perhaps in response to a listener’s 

questions or body language.  For example children may 

wish to add comments such as “It was awesome!”, or 

tell events out of sequence. 

    In short, we need to develop interfaces and interac-

tion techniques that allow our users to control the NLG 

system.  Unfortunately there has been very little pre-

vious work on this topic, indeed almost nothing is 

known about Human-Computer Interaction aspects of 

NLG systems.  Developing a better understanding of 

these aspects is one of the main research challenges we 

face from an NLG perspective. 

3 Current and ongoing work 

3.1 “How was School today…?” 

We developed an initial version of “How was School 

today…?” in 2009; see Reiter et al (2009) for more de-

tails about this system. 

 

 
 

Fig. 1: Participating pupil with support worker:  

The prototype system is mounted on the wheelchair, and 

the pupil has access to the system via head switch con-

trolled row/column scanning. 

 

 This system used Radio Frequency Identification 

(RFID), an emerging application in AAC to identify or 

give access to relevant vocabulary (Bart, Riny et al. 

2008; DeRuyter and Fried-Oken 2010). Sensors were 

used to track both location (by putting tags on doors, 

which were automatically sensed by a long-range RFID 

reader) and interaction (by asking staff to manually 

swipe RFID cards in a short-range reader when the child 

interacted with a person or object). Staff could also 

record spoken messages about interesting events during 

the day (see Fig. 1). 

The software analyzed this data to remove sensor noise, 

and then compared it to a timetable which specified 

where children were supposed to be, what they were 

supposed to be doing and which teacher was supposed 

to be taking the class throughout the day.  This allowed 

the software to both fill in missing information, and to 

identify divergences from the schedule. The result of 

this process was a series of events (which corresponded 

to classes, for example, maths class), each of which had 

a set of associated messages (interactions during the 

event, divergence from schedule, etc.). 

 After the data analysis was completed, an NLG sys-

tem identified the events most interesting (to the child), 

using a heuristic that took into consideration both how 

inherent interesting an event was (for example, lunch 

was regarded as an inherently interesting event that 

children were likely to want to talk about) and also 

whether an event was unusual or not.  The latter is based 

on the observation that most personal narratives focus 

on unexpected or unusual events.  Unusual events were 

identified by the presence of recorded voice messages 

and by divergence from the timetable, e.g. a different 

teacher present or a different location. The system se-

lected the five most ‘interesting’ events and displayed 

them to the child in a simple visual editing interface.  In 

this interface the child could delete events he/she did 

not wish to talk about, and also annotate events with 

simple opinions (evaluations), such as I liked it, using 

the evaluation buttons on the interface, generating ap-

propriate utterances according to the last narrated event 

or message.(see Fig. 2).  

     When editing was finished, the NLG system generat-

ed texts describing the events and messages, which the 

child communicated using a simple narration interface 

(which was similar to the editing interface). Emphasis 

was placed on providing quick access to messages to 

minimize the length of pauses between utterances due to 

the physical accessing difficulties of the users. The 

narrative model is based on the Labov social narrative 

model (Labov 1972) which emphasizes the highpoint 

and evaluation. The dialogue model from beginning 

through to highpoint to the end with the user being able 

to add evaluations at any point of the narration. Stories 

are initially chronological order but interactively under 

the control of the user. This control of narration differs 

significantly from current AAC interventions where 

narrative tends to be output in a monologue format.  

 From an NLG perspective, the system was fairly 

straightforward. The most challenging microplanning 

tasks were choosing connectives, time phrases, lexical 

variety in embellishments, and pronouns based on dis-

course context. Connectives and time phrases were ne-

cessary since children could narrate events in different 

orders (for example, “I went to maths.  Then I went to 

lunch” versus “I went to lunch.  Before lunch, I went to 

maths”).  Document structuring  was simple because we 

Short range RFID reader 

and microphone for voice 

message recording 

Visual interface 

Access switch in head 

rest 

Long range RFID reader 

3



assumed that the children would choose their own order 

in which to narrate events. In fact some children are not 

able to do this; such children would need to be sup-

ported by a more sophisticated document planner that 

had a model of appropriate text structures in this do-

main. 

 We asked two children to use the system for one 

week for a qualitative formative evaluation.  Research-

ers supplied ongoing support during this, primarily trial 

observing how the children used the system, and dis-

cussing it with teachers, therapists and parents.  Gener-

ally it worked well for one child, Julie
2
, who had severe 

motor impairment (no independent means of mobility 

and interacted with a computer using a head switch with 

row/column scanning, see Fig. 1). Her expressive abili-

ties were limited but her comprehension skills were 

comparable to her non-disabled peers with some deve-

lopmental delay. The other child, Jessica, had more 

cognitive impairment, and found the interface too diffi-

cult. 

 

 

 
 

Fig. 2: Example screenshot from interface 

1: Navigation: Day and date of story, maximum of five 

story events, exit; 2: Event messages, numbers vary for 

each event. Here: 2 computer-generated messages, 3 

recorded messages, 1 user added evaluation; 

3: Sequential message navigation: previous, repeat, 

next; 4: Evaluation: delete event, negative evaluation, 

positive evaluation; 

 

   

 In a second evaluation, a third child, Eric, joined and 

all three children used the system over two weeks each. 

In this evaluation, we asked teachers and other staff to 

use the system without on-site support from the re-

searchers. This highlighted many practical usability 

issues, such as delays caused by starting the system in 

the morning, and problems caused by limited battery 

life. We eliminated the long-range RFID sensor because 

of its difficult setup; instead we asked staff to swipe 

                                                           
2 The names of the children mentioned in this paper are 

changed to ensure anonymity. 

door cards when children entered rooms.  However, this 

strategy was not successful, as it was difficult for staff 

to remember to swipe both interactions and location 

changes. 

 The participants took the system home for use with 

their parents who gave positive feedback but also re-

ported issues with system usability (e.g. lack of access 

to stories from previous days) or suitability (too compli-

cated interface for Jessica).   

 Eric’s timetable was different from Julie and Jessi-

ca’s, because he visited college one morning a day, and 

we could not collect data during this period. Since some 

of the most exciting events in a school day happen out-

side the school building (sports and school trips as well 

as college), in the long term we do need to see if we can 

collect data outside as well as inside the school.  

3.2 HWST example 

     Julie used the system on her DynaVox™ Vmax™ 

Voice Output Communication Aid (VOCA) via head 

switch using row/column scanning. The above transcript 

shows an extract of a conversation Julie had with her 

Speech and Language Therapist (SLT) on day three 

about her experiences during day two. The researcher 

(RA) had been present all day for technical support. The 

conversation extract starts with Julie reporting about her 

morning break.  

     In this example Julie is able to quickly reply to con-

text related questions from her communication partner 

using the evaluations (“So what happened?” – “It was 

fun!”). Compared to conversations usually observed 

between aided and unaided partners Julie is able to con-

trol the conversation when starting a new topic after 

talking about the morning break, inviting her communi-

cation partner to prompt for more detailed information. 

Julie provides this with her next generated phrase. 

When she is asked about the event she replies with an 

evaluation the system has generated in relation to its 

previously generated message “A visitor was there.”. 

We note that the system is able to refer to the correct 

gender of the visitor. 

 

1 Julie {next} [I had break.] 

2 Julie {next} [Lesley was there.] 

3 SLT Lesley was there?  

4 Julie ((Opens mouth in agreement, then turns back 

to screen)) 

5 SLT Ok mhmh. So what happened? 

6 Julie {positive evaluation} [It was fun.] 

7 SLT Oh good! ((laughs)) I’m glad to hear it! 

8 RA We like Lesley. 

9 SLT ((nods in direction of RA)) 

10 Julie ((smiles)) 

11 Julie:  {next} [Then I went to Junior Primary in-

stead of Reading Class.] 

1 

2 

3 

4 

4



12 SLT: Right, you went to Junior Primary? I wonder 

why that was?  

13 Julie: {next} [A visitor was there.] 

14 SLT:  Oh, a visitor, right. Wonder what the visitor 

was doing?  

15 Julie: {next} [“The dental hygienist came to give a 

talk.”] 

16 SLT: Oh, dental hygienist. 

17 Julie:  {previous} [A visitor was there.] 

18 SLT: That was the visitor, okay. That’s why you 

went to junior primary, uhm, what did you 

think of the talk? 

19 Julie:  {positive evaluation} [She was nice.] 

20 SLT: She was nice, that was good! ((laughs)) 

 

Notation: 

- Switch selected button by Julie: {curly brackets} 
- Natural speech: standard text.  
- Computer generated language accessed using one 

button: [standard text in square brackets].  

- Recorded messages accessed using one button: 
[“quoted standard text in square brackets”]. 

- Paralinguistic behaviors:  
((standard text in double brackets)).  

 

3.3  “How was School Today” – in the Wild 

We have now started a new project to further develop 

our work, called “How was School today…?” – in the 

Wild (‘in the wild’ indicates that the focus is on how the 

technology works in a real school environment). The 

basic goal is to improve the system sufficiently so that it 

can be tried out over a period of several months, with 

children with varying levels and types of impairments; 

we will also work with several schools in the initial 

phase, although for practical reasons the evaluations 

may be at just one school. 

     During this project we will do some work on the 

issues described in Section 4; in particular we will try to 

make the system usable by children with different im-

pairments and ability levels (Section 4.1). This means 

having a very simple interface for children with consi-

derable cognitive impairments (such as Jessica); but 

also giving children with more cognitive abilities the 

opportunity to exert more control over the story (during 

both editing and narration), for example by supporting a 

richer range of annotations, and by making it easier to 

describe events and messages in any order. 

     Another intermediate goal is to improve the integra-

tion of voice messages entered by staff with the com-

puter-generated messages. This could be done by some 

combination of training staff to enter messages in a spe-

cific way (referring to the child in the first person); ask-

ing staff to annotate the messages so the computer 

knows something about their content; and/or using 

speech recognition to analyze the voice messages. In 

general there is a lot of interesting information that can 

only come from staff, and we need to think about the 

best way to help staff enter information in a way that is 

easy for them and useful for our system. 

 Now that a complete system is built, we are also able 

to thoroughly and formally evaluate the system. Mul-

tiple baseline single case study methodology will be 

used (Schlosser 2003) to evaluate the use and impact of 

our system. We intend to have up to four children (with 

varying ability levels) use the system for a period of 3 

months. This will give us a chance to observe the im-

pact of the system on the users and their environment 

such as the children’s interaction with the system and 

how staff at the school envisage using this new tool. 

The observations will be supported by semi-structured 

interviews with the children, their classroom teacher, 

their speech and language therapist and a parent. 

 We will look at the children’s conversations (with 

and without using our system) about interesting, staged 

events with different partners, analyzing them conversa-

tional characteristics such as narrative initiation, struc-

ture, length and evaluation. Analysis methods will 

include the Revised Edinburgh Functional Communica-

tion Profile (REFCP) (Wirz, Skinner et al. 1990). 

     However, much of our focus will be on addressing 

the practical issues that make it difficult to use our cur-

rent research prototype over a period of months.  We 

have identified many such issues, both from our pre-

vious evaluations (Section 3.1) and also from a ques-

tionnaire that was distributed to school staff during an 

in-service day. 

      Location tracking – There are problems with both 

of the techniques we have tried to date (automatically 

reading RFID tags on doors, and asking staff to swipe 

location information).  In this project we intend to try 

tracking the location of a child using Wi-Fi location 

tracking, which seems to be rapidly gaining popularity 

in the commercial world (Liu, Sen et al. 2008). 

 Data entry, 2D bar codes – We need to allow staff 

to easily enter and update information about the children 

(for example, their timetables) and sensor tags (e.g., if a 

new tag is given to a visitor).  For the latter, we want to 

investigate 2D bar codes, which could allow encoding 

of alphanumeric input data without reference to a cen-

tral database. 

 Portability, battery life – The current system runs on 

a tablet PC (8”-12” touch screen, generic or VOCA 

hardware). During the evaluation, late powering up, run-

down batteries or simply forgetting a component caused 

significant data loss and usability issues. A future proto-

type should favor an ‘always-on’ system, such as a mo-

bile phone, allowing for easy portability and extended 

battery life. 

 Story generation – The prototype system was only 

able to create a story towards the end of a day and gave 

5



only access to stories generated on that day. However, 

often the user desired to tell stories that had occurred on 

previous days, or to, say, tell a story at lunch that oc-

curred in the morning.  When data was insufficient for 

the system to create a story, the only output was an error 

message “Can’t generate story right now.” This fru-

strated users, so future systems should be able to deliver 

a story with incomplete data.  

 Voice messages – as mentioned above, we want to 

handle these in a more sophisticated way.  From a more 

practical perspective, we also want to make it easier for 

staff to listen to and change previously recorded mes-

sages.  We also want to allow parents to record messag-

es about events at home. 

4 Long-term vision and issues 

4.1 Supporting children with different levels 
and types of impairment 

A key issue in AAC is of course the diversity of AAC 

users. Children with CCN differ enormously in terms of 

cognitive ability, motor ability, and social ability. This 

was clear even in our initial evaluation where we 

worked just with two children, and discovered that our 

interface worked well for Julie but not Jessica. 

 Julie has little functional speech and severe physical 

impairments, and accesses her VOCA using a head 

switch through the slow process of scanning the inter-

face. Her VOCA interface consists of a grid of 15 to 30 

buttons per page, with more than 20 pages of vocabu-

lary. However, her cognitive skills were sufficient for 

her to master the interface on the second day. She used 

the system quite successfully, as shown in the example 

in Section 3.2.  

 Jessica also has severe physical impairments but 

does not use technology to support her communication 

(she has functional speech).  She has cognitive impair-

ments, which (amongst other things) affect her ability to 

remember and place events correctly in time. She had 

more difficulty mastering the interface than Julie. We 

simplified the interface for her (no editing, minimal 

control of narration), and then she displayed pragmatics 

known from typical language development in children, 

by telling her story with no room for interaction of her 

communication partner.  

 We also need to keep in mind that abilities are not 

static, but are likely to progress with age (see also Sec-

tion 4.2) and (hopefully) with the assistance of commu-

nication aids. For example, the WriteTalk project 

showed how pupils were both able to initiate and con-

trol communication more effectively with Talk:About 

and how their formal writing skills improved over time 

(Waller et al., 1999). 

 In summary, some children may need a very simple 

interface because of cognitive impairments, but this 

should grow with them.  For example, the best narration 

tool for Jessica at her current stage of development is 

probably a single button that advances sequentially 

through the computer-generated story. The challenge is 

to provide an interface that Jessica can initially use via 

repeatedly pressing an ‘Advance’ button, but which 

gives her the possibility of exerting more control as her 

skills and abilities develop. 

 Other children (such as Julie) may have motor diffi-

culties that restrict the way in which they can interact 

with computer systems, and thus may require simple 

controls although they have reasonable cognitive skills. 

Restricted motor skills make certain tasks, such as en-

tering an arbitrary word, quite difficult and time con-

suming; hence the interface must avoid such tasks, and 

instead endeavor to give the child as much control as 

possible with a minimum amount of data entry.  Once 

these users master a basic story telling structure, it may 

help them develop their conversation skills if they use a 

wide variety of conversation patterns.  For this purpose, 

it may be worthwhile for the system to randomly vary 

the structure and language used in the narratives. 

 Still other children, for example on the autistic spec-

trum, may have good cognitive and motor abilities, but 

not have the experience of expressive communication 

necessary to develop interactive skills. These children 

are more likely to benefit from a system that supports 

the pragmatics of language in general and personal narr-

ative in particular.  For example, children on the high 

functioning end of autism may be comfortable with ra-

ther advanced software, which can help them adapt their 

storytelling according to the intended listener.  Indeed, 

giving these children more complex controls, if done 

correctly, can make the software fun and challenging in 

a positive way. 

     In the long term, as we broaden the range of children 

we work with, there may be overlaps between our work 

and research on tools to help typically developing child-

ren create stories, such as Robertson and Good (2005), 

and also between our work and research on tools to help 

adults with CCN tell personal narratives, such as Demp-

ster (2008).  Ideally it would be very nice to combine 

these efforts and create a story telling tool that could be 

used across the age and impairment spectrum. 

4.2 Narrative across the lifespan 

We would like our tool to be able to support children 

over time, as their abilities grow and as their expe-

riences accumulate. From the perspective of changing 

abilities, the challenge is to offer children an interface 

which is not only appropriate for their current stage of 

development (Sect 4.1), but also allows and indeed en-

6



courages them to exert more control over story content, 

language, and narration as their abilities grow. 

 We would also like our tool to become a repository 

of a child’s personal stories. The ability to relate rele-

vant stories can influence the quality of life, as well as 

social development and successful transitions. The life 

stories of people who use AAC are often held by parents 

and siblings (e.g., stories relating to health care 

(Hemsley, Balandin et al. 2007)), and there is the inevit-

able concern that these stories and others are lost as 

parents age and siblings move away.  

 Technology has the potential both to support the 

acquisition of conversational skills for people who use 

AAC and to provide a repository for life stories. In the 

context of our work, it is essential that we provide ways 

of enabling children to develop their narrative skills so 

that they are more able to manage their own story repo-

sitory. In terms of development, young children will 

narrate recent stories regardless of conversational con-

text. By enabling the child to develop story structure by 

scaffolding interaction and enabling children to easily 

annotate stories, the child will begin to anticipate and 

control conversation. 

 Conversational narratives have traditionally not been 

supported by AAC tools partly due to the fact that they 

are so nebulous; they emerge during interactive conver-

sation (to date, events have to be manually input into a 

system and it is difficult to predict what events will be-

come a story); ‘new’ stories are repeated often (to date it 

is difficult to save conversation online); as stories age 

they are repeated in context (retrieval is often contextual 

e.g. topic based) and they grow longer having more em-

bellishments added to them. The technology we are de-

veloping provides an opportunity for children to access 

information about personal events over time, which they 

can communicate and narrate during a conversation. 

They can also evaluate (annotate) their stories, thereby 

embellishing and lengthening the stories.  However this 

will only be possible if the children can easily access 

previously experienced, generated and saved stories.  

 We can provide fast access to recent stories while 

anticipating the use of older stories such as for example 

those which closely match the current conversation top-

ic. In a research prototype called PROSE (Waller and 

Newell 1997), stories had to be physically tagged; there 

is now the potential to automate topic matching by re-

cognizing topic words spoken by a listener and parsing 

stored information for appropriate stories. Over a life-

time, some stories may fall into disuse, while others will 

be weighted more strongly depending on frequency of 

use and relevance. 

4.3 True dialogue in narration 

The ultimate goal of our research is to enable children to 

tell stories in the context of a social dialogue; for exam-

ple, we want children to be able to chat to their parents 

and other interested parties about what they did during 

the day.  

 Our current system incorporates a simple model of a 

conversation, where children are restricted at any point 

to choosing from a small number of options. The child 

chooses an event to talk about, and then goes through 

the sequence of messages associated with that event.  

The child has the freedom to switch to a different event, 

hence controlling the conversation, and to add annota-

tions/evaluations (for example “it was fun!”). 

     This is adequate in many cases, but in the long term 

we would like to support more complex conversations; 

for example interrupting a discussion about today’s 

events to talk about what happened yesterday, or to dis-

cuss a particular teacher instead of an event.  We would 

also like children to easily be able to add conversational 

phrases, such as “Guess what happened today at 

school”. 

 Because our children have motor and cognitive im-

pairments, we cannot present them with a large number 

of options for conversational moves. Ideally, the system 

would detect what the conversational partner wishes to 

talk about, and from this present the child with a small 

number of appropriate choices. For example, if the con-

versational partner asks the child what happened over 

the past week, our system would detect this and then 

give the child the option of talking about any individual 

weekday or the week in general. 

 One way of detecting what the conversational part-

ner intends is to use speech recognition and Natural 

Language Processing (NLP) technology to analyze what 

he or she says. Speech and NLP technology tend to 

work best when it is possible to train the system to the 

user’s voice, and also (in essence) train the user to un-

derstand what the speech/NLP system can and cannot 

do. This should be possible in our context, at least for 

people (such as parents) with whom the child regularly 

interacts. 

 Another possibility is to create a graphical user in-

terface for the conversational partner, perhaps on the 

same device that the child uses, which the partner could 

use to indicate what he/she wants to talk about.  This is 

probably technically easier, but does move away from 

the goal of having as natural a dialogue as possible. 

4.4 Pragmatics of interacting with others 

Currently, “How was School today…?” supports story-

telling between language-impaired children and adults 

who are the children’s parents, carers, teachers, and 

therapists.  But of course for normally developing child-

ren, many of their most important social interactions are 

with other children. 

 An interesting example here is the STANDUP sys-

tem, which was developed to help children who use 

7



AAC create and tell novel punning riddles. The study 

results suggested that children saved the jokes so that 

they could retell them to friends and family (Waller, 

Black et al. 2009). Whilst the evidence is anecdotal, 

there did also appear to be a marked increase in joke 

telling by participants, both amongst their peers and 

with adults in the home environment. Hence STANDUP 

succeeded in supporting interaction with other children 

as well as with adults. 

 One of the key challenges in interacting with other 

children, and indeed with adults who are not formally 

involved in the care or teaching of the child, is to adapt 

the story to the interests of the recipients. In other 

words, a child’s parents and teachers will not insist on 

stories that are interesting to them, but other conversa-

tional partners will.  These conversational partners may 

also need additional information.  For example “Jane 

came to take me to the OT room” makes more sense if 

the recipient knows that Jane is the occupational therap-

ist; parents and teachers already know this, but other 

people may need to be told this. Also if the conversa-

tional partner was present at an event, this should be 

acknowledged and indeed used in the story. For exam-

ple, "Did you really enjoy maths? I thought it was bor-

ing!” 

 In short, telling stories to peers and adults who do 

not know the child well requires adapting the story to 

the interests, knowledge, and involvement of the part-

ner; this is part of learning pragmatics. This is not some-

thing we are looking at currently, but it is something 

that we hope to look at in the future. 

4.5 Security and privacy issues 

We need to ensure that data about the children is private 

and secure.  Taken to its logical conclusion, our project 

would result in an intimate record of the child's life at 

school, home and beyond.  It is important that both the 

raw data and the generated content are under the control 

of the child and his/her guardians, with the child exer-

cising as much control as possible. This is especially 

important since children with learning difficulties are 

very vulnerable; there is potential for great harm if data 

about a child’s activities got into the hands of a mali-

cious outsider. 

 In a study on the software tool TalksBac, which 

supports personal narrative (Waller, Dennis et al. 1997), 

privacy issues were coded along with stories. This al-

lowed the NLG process to decide the appropriateness of 

telling a story to a specific communication partner. 

Children in general do not care who they tell their sto-

ries to. Only when older children learn to distinguish 

which story is appropriate for a conversation partner. 

This process could be embedded into the prediction 

algorithm that presents stories for narration. Currently 

prediction on AAC devices only support character, word 

or phrase selection.   

 Another concern is information that is embarrassing 

or otherwise puts the child in a negative light; for exam-

ple, imagine a staff member entered the voice message 

"I refused to eat my lunch today".  We believe that the 

child should be free to delete such messages; she should 

never be forced to include material in a story that she 

does not want to include. 

 A related issue is whether we should allow stories 

generated for one child to use information acquired 

about another child.  In principle this is very valuable, 

for example it allows messages such as “Jane didn’t eat 

her lunch today”. But is this acceptable from the pers-

pective of ensuring the privacy of data about Jane’s ac-

tivities? On the other hand, this is exactly the sort of 

thing that a normally developed child would say about a 

classmate. 

5 Conclusion 

In addition to having difficulty in communicating de-

sires and needs, language-impaired children also find it 

hard to participate in social linguistic interaction that 

would help create and build up friendships and other 

interpersonal relationships.  We believe that we can help 

these children participate in such interactions by giving 

them a tool that helps them tell a story about their day at 

school, by using an NLG system that has access to sen-

sor and other data about the child’s activities. We are 

still at an early stage in this work, but our initial proto-

type system has shown great potential to improve the 

quality of life of children with limited speech. Our cur-

rent work plans to explore this potential further while 

evaluating the efficacy of the system for four children 

with varying ability levels.  

Acknowledgements 

We would like to express our thanks to the children, 

their parents and staff and the special school where this 

project had its base. Without their valuable contribu-

tions and feedback this research would not have been 

possible. We would also like to thank DynaVox Sys-

tems Ltd for supplying the communication devices to 

run our system on. 

 This research was supported by the UK Engineering 

and Physical Sciences Research Council under grants 

EP/F067151/1, EP/F066880/1, EP/E011764/1, 

EP/H022376/1, and EP/H022570/1. 

8



References 

Agrawal, R. and Ramakrishnan, S. (2000) Privacy-

preserving data mining. ACM International 

Conference on Management of Data, pp. 439--450, 

Bart, H., V. Riny, et al. (2008). LinguaBytes. 

Proceedings of the 7th international conference on 

Interaction design and children. Chicago, Illinois, 

ACM: 17-20. 

Beukelman, D. R. and P. Mirenda (2005). Augmentative 

and Alternative Communication: Management of 

Severe Communication Disorders in Children and 

Adults. Baltimore, Paul H. Brookes Publishing Co. 

Bruner, J. (1975). "From communication to language: A 

psychological perspective." Cognition 3: 255-289. 

Cheepen, C. (1988). The predictability of  informal 

conversation. Oxford, Printer Publishers Ltd. 

Clarke, H. H. and E. V. Clarke (1977). Psychology and 

Language. New York, Harcourt Brace Jovanovich. 

Dempster, M. (2008). Using natural language 

generation to encourage effective communication in 

nonspeaking people. Proceedings of Young 

Researchers Consortium, ICCHP'08. 

DeRuyter and Fried-Oken. (2010). "Context-sensitive 

messaging with RFID technology."   Retrieved 2010, 
April 10, from http://aac-rerc.psu.edu/index.php/projecttypes/list 

Gatt, A., F. Portet, et al. (2009). "From Data to Text in 

the Neonatal Intensive Care Unit: Using NLG 

Technology for Decision Support and Information 

Management." AI Communications 22: 153-186. 

Goldberg, E., N. Driedger, et al. (1994). "Using natural-

language processing to produce weather forecasts." 

IEEE Expert 9(2): 45-53. 

Harris, M. (2008). Building a Large-Scale Commer-cial 

NLG System for an EMR. Proc of INLG-2008. 

Hemsley, B., S. Balandin, et al. (2007). "Family 

caregivers discuss roles and  needs in supporting 

adults with cerebral palsy and complex 

communication needs in the hospital setting." 

Journal of Developmental and Physical Disabilities 

19(2): 115-124. 

Higginbotham, D. J., H. Shane, et al. (2007). "Access to 

AAC: Present, past, and future." Augmentative & 

Alternative Communication 23(3): 243-257. 

Labov, W. (1972). Language in the inner city: Studies in 

the Black English Vernacular. Philadelphia, 

University of Pennsylvania Press. 

Light, J., C. Binger, et al. (1994). "Story Reading 

interactions between preschoolers who use AAC and 

their mothers." Augmentative and Alternative 

Communication 10: 255-268. 

Light, J. and K. Drager (2007). "AAC Technologies for 

Young Children with Complex Communication 

Needs: State of the Science and Future Research 

Directions." Augmentative and Alternative 

Communication 23(3): pp. 204 – 216. 

Liu, X., A. Sen, et al. (2008). A Software Client for Wi-

Fi Based Real-Time Location Tracking of Patients. 

Medical Imaging and Informatics. 

Berlin/Heidelberg, Springer. 4987/2008: 141-150. 

Péréz, R. P. y. and M. Sharples (2004). "Three 

Computer-Based Models of StoryTelling: BRUTUS, 

MINSTREL, and MEXICA." Knowledge-Based 

Systems 17: 15-29. 

Peterson, C. and A. McCabe (1983). Developmental 

psycholinguistics: Three ways of looking at a child’s 

narrative. New York, Plenum. 

Reiter, E. (2007). An Architecture for Data-to-Text 

Systems. ENLG-2007. 

Reiter, E. and R. Dale (2000). Building Natural-

Language Generation Systems., Cambridge 

University Press. 

Reiter, E., S. Sripada, et al. (2005). "Choosing Words in 

Computer-Generated Weather Forecasts." Artificial 

Intelligence 167: 137-169. 

Reiter, E., R. Turner, et al. (2009). Using NLG to Help 

Language-Impaired Users Tell Stories and 

Participate in Social Dialogues. ENLG2009. Athens, 

Greece, Association for Computational Linguistics. 

Robertson, J. and J. Good (2005). "Story creation in 

virtual game worlds." Communications of the ACM 

48: 61-65. 

Schlosser, R. W. (2003). The Efficacy of Augmentative 

and Alternative Communication. San Diego, 

Elsevier Science. 

Soto, G., E. Hartmann, et al. (2006). "Exploring the 

Elements of Narrative that Emerge in the 

Interactions between an 8-Year-Old Child who uses 

an AAC Device and her Teacher." Augmentative 

and Alternative Communication 22(4): pp. 231 - 

241. 

Sripada, S., E. Reiter, et al. (2005). Evaluating an NLG 

System using Post-Edit Data: Lessons Learned. 

Proceedings of ENLG-2005, 10th European 

Workshop on Natural Language Generation, 

Aberdeen, Scotland. 

Waller, A. (2006). "Communication Access to 

Conversational Narrative." Topics in Language 

Disorders 26(3): 221-239. 

Waller, A., R. Black, et al. (2009). "Evaluating the 

STANDUP Pun Generating Software with Children 

with Cerebral Palsy." ACM Trans. Access. Comput. 

1(3): 27. 

Waller, A., F. Dennis, et al. (1997). "Evaluating the use 

of TalksBac, a predictive communication device for 

non-fluent aphasic adults." International Journal of 

Language and Communication Disorders 33: 45-70. 

Waller, A. and A. F. Newell (1997). "Towards a 

narrative based communication system." European 

Journal of Disorders of Communication 32: 289-

306. 

 

9


