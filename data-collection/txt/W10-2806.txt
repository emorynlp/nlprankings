










































Semantic Composition with Quotient Algebras


Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 38–44,
Uppsala, Sweden, 16 July 2010. c©2010 Association for Computational Linguistics

Semantic Composition with Quotient Algebras

Daoud Clarke
University of Hertfordshire

Hatfield, UK
daoud@metrica.net

Rudi Lutz
University of Sussex

Brighton, UK
rudil@sussex.ac.uk

David Weir
University of Sussex

Brighton, UK
davidw@sussex.ac.uk

Abstract

We describe an algebraic approach for
computing with vector based semantics.
The tensor product has been proposed as
a method of composition, but has the un-
desirable property that strings of different
length are incomparable. We consider how
a quotient algebra of the tensor algebra can
allow such comparisons to be made, offer-
ing the possibility of data-driven models of
semantic composition.

1 Introduction

Vector based techniques have been exploited in a
wide array of natural language processing appli-
cations (Schütze, 1998; McCarthy et al., 2004;
Grefenstette, 1994; Lin, 1998; Bellegarda, 2000;
Choi et al., 2001). Techniques such as latent se-
mantic analysis and distributional similarity anal-
yse contexts in which terms occur, building up a
vector of features which incorporate aspects of the
meaning of the term. This idea has its origins in
the distributional hypothesis of Harris (1968), that
words with similar meanings will occur in similar
contexts, and vice-versa.

However, there has been limited attention paid
to extending this idea beyond individual words,
so that the distributional meaning of phrases and
whole sentences can be represented as vectors.
While these techniques work well at the word
level, for longer strings, data becomes extremely
sparse. This has led to various proposals explor-
ing methods for composing vectors, rather than de-
riving them directly from the data (Landauer and
Dumais, 1997; Foltz et al., 1998; Kintsch, 2001;
Widdows, 2008; Clark et al., 2008; Mitchell and
Lapata, 2008; Erk and Pado, 2009; Preller and
Sadrzadeh, 2009). Many of these approaches use
a pre-defined composition operation such as ad-
dition (Landauer and Dumais, 1997; Foltz et al.,

1998) or the tensor product (Smolensky, 1990;
Clark and Pulman, 2007; Widdows, 2008) which
contrasts with the data-driven definition of com-
position developed here.

2 Tensor Algebras

Following the context-theoretic semantics of
Clarke (2007), we take the meaning of strings as
being described by a multiplication on a vector
space that is bilinear with respect to the addition
of the vector space, i.e.

x(y + z) = xy + xz (x+ y)z = xz + yz

It is assumed that the multiplication is associative,
but not commutative. The resulting structure is an
associative algebra over a field — or simply an
algebra when there is no ambiguity.

One commonly used bilinear multiplication op-
erator on vector spaces is the tensor product (de-
noted ⊗), whose use as a method of combining
meaning was first proposed by Smolensky (1990),
and has been considered more recently by Clark
and Pulman (2007) and Widdows (2008), who also
looked at the direct sum (which Widdows calls
the direct product, denoted ⊕).

We give a very brief account of the tensor prod-
uct and direct sum in the finite-dimensional case;
see (Halmos, 1974) for formal and complete defi-
nitions. Roughly speaking, if u1, u2, . . . un form
an orthonormal basis for a vector space U and
v1, v2, . . . vm form an orthonormal basis for vector
space V , then the space U ⊗V has dimensionality
nm with an orthonormal basis formed by the set
of all ordered pairs (ui, vj), denoted by ui ⊗ vj ,
of the individual basis elements. For arbitrary el-
ements u =

∑n
i=1 αiui and v =

∑m
j=1 βjvj the

tensor product of u and v is then given by

u⊗ v =
n∑
i

m∑
j

αiβj ui ⊗ vj

38



For two finite dimensional vector spaces U and
V (over a field F ) of dimensionality n and m re-
spectively, the direct sum U ⊕ V is defined as the
cartesian product U × V together with the oper-
ations (u1, v1) + (u2, v2) = (u1 + u2, v1 + v2),
and a(u1, v1) = (au1, av1), for u1, u2 ∈ U ,
v1, v2 ∈ V and a ∈ F . In this case the vectors
u1, u2, . . . un, v1, v2, . . . vm form an orthonormal
set of basis vectors in U ⊕ V , which is thus of
dimensionality n + m. In this case one normally
identifies U with the set of vectors in U ⊕V of the
form (u, 0), and V with the set of vectors of the
form (0, v). This construction makes U ⊕ V iso-
morphic to V ⊕U , and thus the direct sum is often
treated as commutative, as we do in this paper.

The motivation behind using the tensor product
to combine meanings is that it is very fine-grained.
So, if, for example, red is represented by a vector
u consisting of a feature for each noun that is mod-
ified by red, and apple is represented by a vector
v consisting of a feature for each verb that occurs
with apple as a direct object, then red apple will
be represented by u ⊗ v with a non-zero compo-
nent for every pair of non-zero features (one from
u and one from v). So, there is a non-zero ele-
ment for each composite feature, something that
has been described as red, and something that has
been done with an apple, for example, sky and eat.

Both ⊕ and ⊗ are intuitively appealing as se-
mantic composition operators, since u and v are
reconstructible from each of u⊗ v and u⊕ v, and
thus no information is lost in composing u and v.
Conversely, this is not possible with ordinary vec-
tor addition, which also suffers from the fact that it
is strictly commutative (not simply up to isomor-
phism like⊕), whereas natural language composi-
tion is in general manifestly non-commutative.

We make use of a construction called the tensor
algebra on a vector space V (where V is a space
of context features), defined as:

T (V ) = R⊕ V ⊕ (V ⊗ V )⊕ (V ⊗ V ⊗ V )⊕ · · ·

Any element of T (V ) can be described as a sum of
components with each in a different tensor power
of V . Multiplication is defined as the tensor prod-
uct on these components, and extended linearly to
the whole of T (V ). We define the degree of a vec-
tor u in T (V ) to be the tensor power of its high-
est dimensional non-zero component, and denote
it deg(u); so for example, both v⊗v and u⊕(v⊗v)
have degree two, for 0 6= u, v ∈ V . We restrict
T (V ) to only contain vectors of finite degree.

A standard way to compare elements of a vector
space is to make use of an inner product, which
provides a measure of semantic distance on that
space. Assuming we have an inner product 〈·, ·〉 on
V , T (V ) can be given an inner product by defining
〈α, β〉 = αβ for α, β ∈ R, and

〈x1 ⊗ y1, x2 ⊗ y2〉 = 〈x1, x2〉〈y1, y2〉

for x1, y1, x2, y2 ∈ V , and then extending this in-
ductively (and by linearity) to the whole of T (V ).

We assume that words are associated with vec-
tors in V , and that the higher tensor powers repre-
sent strings of words. The problem with the tensor
product as a method of composition, given the in-
ner product as we have defined it, is that strings
of different lengths will have orthogonal vectors,
clearly a serious problem, since strings of different
lengths can have similar meanings. In our previous
example, the vector corresponding to the concept
red apple lives in the vector space U ⊗ V , and so
we have no way to compare it to the space V of
nouns, even though red apple should clearly be re-
lated to apple.

Previous work has not made full use of the ten-
sor product space; only tensor products are used,
not sums of tensor products, giving us the equiva-
lent of the product states of quantum mechanics.
Our approach imposes relations on the vectors of
the tensor product space that causes some product
states to become equivalent to entangled states,
containing sums of tensor products of different de-
grees. This allows strings of different lengths to
share components. We achieve this by construct-
ing a quotient algebra.

3 Quotient Algebras

An ideal I of an algebra A is a sub-vector space
of A such that xa ∈ I and ax ∈ I for all a ∈ A
and all x ∈ I . An ideal introduces a congruence
≡ on A defined by x ≡ y if and only if x− y ∈ I .
For any set of elements Λ ⊆ A there is a unique
minimal ideal IΛ containing all elements of Λ; this
is called the ideal generated by Λ. The quotient
algebra A/I is the set of all equivalence classes
defined by this congruence. Multiplication is de-
fined on A/I by the multiplication on A, since ≡
is a congruence.

By adding an element x − y to the generating
set Λ of an ideal, we are saying that we want to
set x − y to zero in the quotient algebra, which
has the effect of setting x equal to y. Thus, if we

39



have a set of pairs of vectors that we wish to make
equal in the quotient algebra, we put their differ-
ences in the generating set of the ideal. Note that
putting a single vector v in the generating set can
have knock-on effects, since all products of v with
elements of A will also end up in the ideal.

Although we have an inner product defined on
T (V ), we are not aware of any satisfactory method
for defining an inner product on T (V )/I , a con-
sequence of the fact that both T (V ) and I are
not complete. Instead, we define an inner prod-
uct on a space which contains the quotient algebra,
T (V )/I . Rather than considering all elements of
the ideal when computing the quotient, we con-
sider a sub-vector space of the ideal, limiting our-
selves to the space Gk generated from Λ by only
allowing multiplication by elements up to a certain
degree, k.

Let us denote the vector subspace generated by
linearity alone (no multiplications) from a sub-
set Λ of T (V ) by G(Λ). Also suppose B =
{e1, . . . , eN} is a basis for V . We then define
the spaces Gk as follows. Define sets Λk (k =
0, 1, 2, . . .) inductively as follows:

Λ0 = Λ

Λk = Λk−1 ∪ {(ei ⊗ Λk−1)|ei ∈ B}
∪ {(Λk−1 ⊗ ei)|ei ∈ B}

Define
Gk = G(Λk)

We note that

G0 ⊆ G1 ⊆ . . . Gk ⊆ . . . ⊆ I ⊆ T (V )

form an increasing sequence of linear vector sub-
spaces of T (V ), and that

I =

∞⋃
k=0

Gk

This means that for any x ∈ I there exists a small-
est k such that for all k′ ≥ k we have that x ∈ Gk′ .
Lemma. Let x ∈ I, x 6= 0 and let deg(x) = d.
Then for all k ≥ d − mindeg(Λ) we have that
x ∈ Gk, where mindeg(Λ) is defined to be the
minimum degree of the non-zero components oc-
curring in the elements of Λ.

Proof. We first note that for x ∈ I it must
be the case that deg(x) ≥ mindeg(Λ) since I
is generated from Λ. Therefore we know d −

mindeg(Λ) ≥ 0. We only need to show that
x ∈ Gd−mindeg(Λ). Let k′ be the smallest in-
teger such that x ∈ Gk′ . Since x 6∈ Gk′−1 it
must be the case that the highest degree term of
x comes from V ⊗ Gk′−1 ∪ Gk′−1 ⊗ V . There-
fore k′ + mindeg(Λ) ≤ d ≤ k′ + maxdeg(Λ).
From this it follows that the smallest k′ for which
x ∈ Gk′ satisfies k′ ≤ d − mindeg(Λ), and we
know x ∈ Gk for all k ≥ k′. In particular x ∈ Gk
for k ≥ d−mindeg(Λ).

We show that T (V )/Gk (for an appropriate
choice of k) captures the essential features of
T (V )/I in terms of equivalence:

Proposition. Let deg(a − b) = d and let k ≥
d − mindeg(Λ). Then a ≡ b in T (V )/Gk if and
only if a ≡ b in T (V )/I .

Proof. Since Gk ⊆ I , the equivalence class of an
element a in T (V )/I is a superset of the equiva-
lence class of a in T (V )/Gk, which gives the for-
ward implication. The reverse follows from the
lemma above.

In order to define an inner product on
T (V )/Gk, we make use of the result of Berbe-
rian (1961) that if M is a finite-dimensional
linear subspace of a pre-Hilbert space P , then
P = M ⊕ M⊥, where M⊥ is the orthogonal
complement of M in P . In our case this implies
T (V ) = Gk ⊕ G⊥k and that every element
x ∈ T (V ) has a unique decomposition as
x = y + x′k where y ∈ Gk and x′k ∈ G⊥k . This
implies that T (V )/Gk is isomorphic to G⊥k , and
that for each equivalence class [x]k in T (V )/Gk
there is a unique corresponding element x′k ∈ G⊥k
such that x′k ∈ [x]k. This element x′k can be
thought of as the canonical representation of all
elements of [x]k in T (V )/Gk, and can be found
by projecting any element in an equivalence class
onto G⊥k . This enables us to define an inner
product on T (V )/Gk by 〈[x]k, [y]k〉k = 〈x′k, y′k〉.

The idea behind working in the quotient algebra
T (V )/I rather than in T (V ) is that the elements
of the ideal capture differences that we wish to ig-
nore, or alternatively, equivalences that we wish to
impose. The equivalence classes in T (V )/I repre-
sent this imposition, and the canonical representa-
tives in I⊥ are elements which ignore the distinc-
tions between elements of the equivalence classes.

40



However, by using Gk, for some k, instead of
the full ideal I , we do not capture some of the
equivalences implied by I . We would, therefore,
like to choose k so that no equivalences of impor-
tance to the sentences we are considering are ig-
nored. While we have not precisely established a
minimal value for k that achieves this, in the dis-
cussion that follows, we set k heuristically as

k = l −mindeg(Λ)

where l is the maximum length of the sentences
currently under consideration, and Λ is the gen-
erating set for the ideal I . The intuition behind
this is that we wish all vectors occurring in Λ to
have some component in common with the vec-
tor representation of our sentences. Since com-
ponents in the ideal are generated by multipli-
cation (and linearity), in order to allow the ele-
ments of Λ containing the lowest degree compo-
nents to potentially interact with our sentences,
we will have to allow multiplication of those el-
ements (and all others) by components of degree
up to l −mindeg(Λ).

Given a finite set Λ ⊆ T (V ) of elements gen-
erating the ideal I , to compute canonical repre-
sentations, we first compute a generating set Λk
for Gk following the inductive definition given
earlier, and removing any elements that are not
linearly independent using a standard algorithm.
Using the Gram-Schmidt process (Trefethen and
Bau, 1997), we then calculate an orthonormal ba-
sis Λ′ for Gk, and, by a simple extension of Gram-
Schmidt, compute the projection of a vector u onto
G⊥k using the basis Λ

′.
We now show how Λ, the set of vectors gener-

ating the ideal, can be constructed on the basis of
a tree-bank, ensuring that the vectors for any two
strings of the same grammatical type are compa-
rable.

4 Data-driven Composition

Suppose we have a tree-bank, its associated tree-
bank grammar G, and a way of associating a con-
text vector with every occurrence of a subtree in
the tree-bank (where the vectors indicate the pres-
ence of features occurring in that particular con-
text). The context vector associated with a spe-
cific occurrence of a subtree in the tree-bank is an
individual context vector.

We assume that for every rule, there is a distin-
guished non-terminal on the right hand side which

we call the head. We also assume that for every
production π there is a linear function φπ from the
space generated by the individual context vectors
of the head to the space generated by the individ-
ual context vectors of the left hand side. When
there is no ambiguity, we simply denote this func-
tion φ.

Let X̂ be the sum over all individual vectors of
subtrees rooted withX in the tree-bank. Similarly,
for each Xj in the right-hand-side of the rule πi :
X → X1 . . . Xr(πi), where r(π) is the rank of π,
let π̂i,j be the sum over the individual vectors of
those subtrees rooted with Xj where the subtree
occurs as the jth daughter of a local tree involving
the production πi in the tree-bank.

For each rule π : X → X1 . . . Xr with headXh
we add vectors

λπ,i = φ(ei)−X̂1⊗. . .⊗X̂h−1⊗ei⊗X̂h+1⊗. . .⊗X̂r

for each basis element ei of VXh to the generating
set. The reasoning behind this is to ensure that the
meaning corresponding to a vector associated with
the head of a rule is maintained as it is mapped to
the vector space associated with the left hand side
of the rule.

It is often natural to assume that the individual
context vector of a non-terminal is the same as the
individual context vector of its head. In this case,
we can take φ to be the identity map. In particular,
for a rule of the form π : X → X1, then λπ,i is
zero.

It is important to note at this point that we have
presented only one of many ways in which a gram-
mar could be used to generate an ideal. In partic-
ular, it is possible to add more vectors to the ideal,
allowing more fine-grained distinctions, for exam-
ple through the use of a lexicalised grammar.

For each sentence w, we compute the tensor
product ŵ = â1 ⊗ â2 ⊗ · · · ⊗ ân where the string
of words a1 . . . an form w, and each âi is a vector
in V . For a sentence w we find an element ŵO of
the orthogonal complement of Gk in T (V ) such
that ŵO ∈ [ŵ], where [ŵ] denotes the equivalence
class of ŵ given the subspace Gk.

5 Example

We show how our formalism applies in a simple
example. Assume we have a corpus which
consists of the following sentences:

41



ap
pl

e

bi
g

ap
pl

e

re
d

ap
pl

e

ci
ty

bi
g

ci
ty

re
d

ci
ty

bo
ok

bi
g

bo
ok

re
d

bo
ok

apple 1.0 0.26 0.24 0.52 0.13 0.12 0.33 0.086 0.080
big apple 1.0 0.33 0.13 0.52 0.17 0.086 0.33 0.11
red apple 1.0 0.12 0.17 0.52 0.080 0.11 0.33
city 1.0 0.26 0.24 0.0 0.0 0.0
big city 1.0 0.33 0.0 0.0 0.0
red city 1.0 0.0 0.0 0.0
book 1.0 0.26 0.24
big book 1.0 0.33
red book 1.0

Figure 1: Similarities between phrases

see red apple see big city
buy apple visit big apple
read big book modernise city
throw old small red book see modern city
buy large new book

together with the following productions.

1. N′ → Adj N′

2. N′ → N

where N and Adj are terminals representing nouns
and adjectives, along with rules for the terminals.
We consider the space of adjective/noun phrases,
generated by N′, and define the individual context
of a noun to be the verb it occurs with, and the in-
dividual context of an adjective to be the noun it
modifies. For each rule, we take φ to be the iden-
tity map, so the vector spaces associated with N
and N′, and the vector space generated by indi-
vidual contexts of the nouns are all the same. In
this case, the only non-zero vectors which we add
to the ideal are those for the second rule (ignoring
the first rule, since we do not consider verbs in this
example except as contexts), which has the set of
vectors

λi = ei − Âdj⊗ ei

where i ranges over the basis vectors for contexts
of nouns: see, buy , visit , read ,modernise , and

Âdj = 2eapple + 2ebook + ecity

In order to compute canonical representations
of vectors, we take k = 1.

5.1 Discussion

Figure 1 shows the similarity between the noun
phrases in our sample corpus. Note that the vec-
tors we have put in the generating set describe only
compositionality of meaning — thus for example
the similarity of the non-compositional phrase big
apple to city is purely due to the distributional
similarity between apple and city and composition
with the adjective big.

Our preliminary investigations indicate that the
cosine similarity values are very sensitive to the
particular corpus and features chosen; we are cur-
rently investigating other ways of measuring and
computing similarity.

One interesting feature in the results is how ad-
jectives alter the similarity between nouns. For ex-
ample, red apple and red city have the same sim-
ilarity as apple and city, which is what we would
expect from a pure tensor product. This also ex-
plains why all phrases containing book are disjoint
to those containing city, since the original vector
for book is disjoint to city.

The contribution that the quotient algebra gives
is in comparing the vectors for nouns with those
for noun-adjective phrases. For example, red ap-
ple has components in common with apple, as we
would expect, which would not be the case with
just the tensor product.

6 Conclusion and Further Work

We have presented the outline of a novel approach
to semantic composition that uses quotient alge-
bras to compare vector representations of strings
of different lengths.

42



The dimensionality of the construction we use
increases exponentially in the length of the sen-
tence; this is a result of our use of the tensor prod-
uct. This causes a problem for computation us-
ing longer phrases; we hope to address this in fu-
ture work by looking at the representations we use.
For example, product states can be represented in
much lower dimensions by representing them as
products of lower dimensional vectors.

The example we have given would seem to in-
dicate that we intend putting abstract (syntactic)
information about meaning into the set of generat-
ing elements of the ideal. However, there is no rea-
son that more fine-grained aspects of meaning can-
not be incorporated, even to the extent of putting
in vectors for every pair of words. This would
automatically incorporate information about non-
compositionality of meaning. For example, by in-
cluding the vector ̂big apple − b̂ig ⊗ âpple , we
would expect to capture the fact that the term big
apple is non-compositional, and more similar to
city than we would otherwise expect.

Future work will also include establishing the
implications of varying the constant k and explor-
ing different methods for choosing the set Λ that
generates the ideal. We are currently preparing
an experimental evaluation of our approach, using
vectors obtained from large corpora.

7 Acknowledgments

We are grateful to Peter Hines, Stephen Clark, Pe-
ter Lane and Paul Hender for useful discussions.
The first author also wishes to thank Metrica for
supporting this research.

References
Jerome R. Bellegarda. 2000. Exploiting latent se-

mantic information in statistical language modeling.
Proceedings of the IEEE, 88(8):1279–1296.

Sterling K. Berberian. 1961. Introduction to Hilbert
Space. Oxford University Press.

Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text
segmentation. In Proceedings of the 2001 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 109–117.

Stephen Clark and Stephen Pulman. 2007. Combin-
ing symbolic and distributional models of meaning.
In Proceedings of the AAAI Spring Symposium on
Quantum Interaction, pages 52–55, Stanford, CA.

Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
Second Quantum Interaction Symposium (QI-2008),
pages 133–140, Oxford, UK.

Daoud Clarke. 2007. Context-theoretic Semantics
for Natural Language: an Algebraic Framework.
Ph.D. thesis, Department of Informatics, University
of Sussex.

Katrin Erk and Sebastian Pado. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proceedings of the EACL
Workshop on Geometrical Methods for Natural Lan-
guage Semantics (GEMS).

P. W. Foltz, W. Kintsch, and T. K. Landauer. 1998.
The measurement of textual coherence with latent
semantic analysis. Discourse Process, 15:285–307.

Gregory Grefenstette. 1994. Explorations in auto-
matic thesaurus discovery. Kluwer Academic Pub-
lishers, Dordrecht, NL.

Paul Halmos. 1974. Finite dimensional vector spaces.
Springer.

Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.

W. Kintsch. 2001. Predication. Cognitive Science,
25:173–202.

T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to Plato’s problem: the latent semantic analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2):211–
240.

Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics and the 17th International Conference
on Computational Linguistics (COLING-ACL ’98),
pages 768–774, Montreal.

43



Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ’04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.

Anne Preller and Mehrnoosh Sadrzadeh. 2009. Bell
states and negation in natural languages. In Pro-
ceedings of Quantum Physics and Logic.

Heinrich Schütze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.

Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1-
2):159–216, November.

Lloyd N. Trefethen and David Bau. 1997. Numerical
Linear Algebra. SIAM.

Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
Second Symposium on Quantum Interaction, Ox-
ford, UK.

44


