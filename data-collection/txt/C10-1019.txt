Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169,

Beijing, August 2010

161

Jointly Modeling WSD and SRL with Markov Logic

Wanxiang Che and Ting Liu

Research Center for Information Retrieval

MOE-Microsoft Key Laboratory of Natural Language Processing and Speech

School of Computer Science and Technology

{car, tliu}@ir.hit.edu.cn

Abstract

Semantic role labeling (SRL) and word
sense disambiguation (WSD) are two fun-
damental tasks in natural language pro-
cessing to ﬁnd a sentence-level seman-
tic representation. To date,
they have
mostly been modeled in isolation. How-
ever, this approach neglects logical con-
straints between them. We therefore ex-
ploit some pipeline systems which verify
the automatic all word sense disambigua-
tion could help the semantic role label-
ing and vice versa. We further propose a
Markov logic model that jointly labels se-
mantic roles and disambiguates all word
senses. By evaluating our model on the
OntoNotes 3.0 data, we show that this
joint approach leads to a higher perfor-
mance for word sense disambiguation and
semantic role labeling than those pipeline
approaches.

Introduction

1
Semantic role labeling (SRL) and word sense dis-
ambiguation (WSD) are two fundamental tasks in
natural language processing to ﬁnd a sentence-
level semantic representation. Semantic role la-
beling aims at identifying the relations between
predicates in a sentence and their associated ar-
guments. Word sense disambiguation is the pro-
cess of identifying the correct meaning, or sense
of a word in a given context. For example, for
the sentence in Figure 1, we can ﬁnd out that the
predicate token “hitting” at position 3 has sense
“cause to move by striking” and the sense label is
“hit.01”. The argument headed by the token “cat”
at position 1 with sense “feline mammal” (cat.01)
is referring to the player (A0), and the argument
headed by the token “ball” at position 5 with sense

Figure 1: A sample of word sense disambiguation
and semantic role labeling.

“round object that is hit in games” (ball.01) is re-
ferring to the game object (A1) being hit.

Normally, semantic role labeling and word
sense disambiguation are regarded as two inde-
pendent tasks, i.e., the word sense information
is rarely used in a semantic role labeling system
and vice versa. A few researchers have used se-
mantic roles to help the verb sense disambigua-
tion (Dang and Palmer, 2005). More people used
predicate senses in semantic role labeling (Hajiˇc
et al., 2009; Surdeanu et al., 2008). However, both
of the pipeline methods ignore possible dependen-
cies between the word senses and semantic roles,
and can result in the error propagation problem.
The same problem also appears in other natural
language processing tasks.

In order to make different natural language pro-
cessing tasks be able to help each other, jointly
modeling methods become popular recently, such
as joint Chinese word segmentation and part-of-
speech tagging (Kruengkrai et al., 2009; Zhang
and Clark, 2008; Jiang et al., 2008), joint lemma-
tization and part-of-speech prediction (Toutanova
and Cherry, 2009), joint morphological segmenta-
tion and syntactic parsing (Goldberg and Tsarfaty,
2008), joint text and aspect ratings for sentiment
summarization (Titov and McDonald, 2008), and
joint parsing and named entity recognition (Finkel
and Manning, 2009). For semantic role label-
ing, Dahlmeier et al. (2009) proposed a method
to maximize the joint probability of the seman-

162

tic role of preposition phrases and the preposition
sense.

In order to do better joint learning, a novel
statistical relational learning framework, Markov
logic (Domingos and Lowd, 2009) was intro-
duced to join semantic role labeling and predicate
senses (Meza-Ruiz and Riedel, 2009). Markov
logic combines the ﬁrst order logic and Markov
networks, to develop a joint probability model
over all related rules. Global constraints (intro-
duced by Punyakanok et al. (2008)) among se-
mantic roles can be easily added into Markov
logic. And the more important, the jointly model-
ing can be realized using Markov logic naturally.
Besides predicates and prepositions, other word
senses are also important information for recog-
nizing semantic roles. For example, if we know
“cat” is an “agent” of the predicate “hit” in a sen-
tence, we can guess that “dog” can also be an
“agent” of “hit”, though it does not appear in the
training data. Similarly, the semantic role infor-
mation can also help to disambiguate word senses.
In addition, the predicate sense and the argument
sense can also help each other.
In the sentence
“The cat is hitting a ball.”, if we know “hit” here
has a game related sense, we can guess that the
“ball” should have the sense “is a round object in
games”. In the same way, the correct “ball” sense
can help to disambiguate the sense of “hit”. The
joint probability, that they are disambiguated cor-
rectly simultaneously will be larger than other ab-
normalities.

The release of OntoNotes (Hovy et al., 2006)
provides us an opportunity to jointly model all
word senses disambiguation and semantic role la-
beling. OntoNotes is a large corpus annotated
with constituency trees (based on Penn Tree-
bank), predicate argument structures (based on
Penn PropBank), all word senses, etc. It has been
used in some natural language processing tasks,
such as joint parsing and named entity recogni-
tion (Finkel and Manning, 2009), and word sense
disambiguation (Zhong et al., 2008).

In this paper, we ﬁrst propose some pipeline
systems which exploit automatic all word sense
disambiguation into semantic role labeling task
and vice versa. Then we present a Markov logic
model which can easily express useful global con-

straints and jointly disambiguate all word senses
and label semantic roles.

Experiments on the OntoNotes 3.0 corpus show
that (1) the automatic all word sense disambigua-
tion and semantic role labeling tasks can help each
other when using pipeline approaches, and more
important, (2) the joint approach using Markov
logic leads to higher accuracy for word sense dis-
ambiguation and performance (F1) for semantic
role labeling than pipeline approaches.

2 Related Work
Joint models were often used in semantic role la-
beling community. Toutanova et al. (2008) and
Punyakanok et al. (2008) presented a re-ranking
model and an integer linear programming model
respectively to jointly learn a global optimal se-
mantic roles assignment. Besides jointly learning
semantic role assignment of different constituents
for one task (semantic role labeling), their meth-
ods have been used to jointly learn for two tasks
(semantic role labeling and syntactic parsing).
However, it is easy for the re-ranking model to
loss the optimal result, if it is not included in the
top n results. In addition, the integer linear pro-
gramming model can only use hard constraints. A
lot of engineering work is also required in both
models.

Recently, Markov logic (Domingos and Lowd,
2009) became a hot framework for joint model.
It has been successfully used in temporal relations
recognition (Yoshikawa et al., 2009), co-reference
resolution (Poon and Domingos, 2008), etc.
It
is very easy to do joint modeling using Markov
logic. The only work is to deﬁne relevant formu-
las. Meza-Ruiz and Riedel (2009) have joined se-
mantic role labeling and predicate senses disam-
biguation with Markov logic.

The above idea, that the predicate senses and
the semantic role labeling can help each other,
may be inspired by Hajiˇc et al. (2009), Surdeanu
et al. (2008), and Dang and Palmer (2005). They
have shown that semantic role features are helpful
to disambiguate verb senses and vice versa.

Besides predicate senses, Dahlmeier et al.
(2009) proposed a joint model to maximize prob-
ability of the preposition senses and the semantic
role of prepositional phrases.

163

Except for predicate and preposition senses,
Che et al. (2010) explored all word senses for se-
mantic role labeling. They showed that all word
senses can improve the semantic role labeling per-
formance signiﬁcantly. However, the golden word
senses were used in their experiments. The results
are still unknown when an automatic word sense
disambiguation system is used.

In this paper, we not only use all word senses
disambiguated by an automatic system, but also
make the semantic role labeling results to help
word sense disambiguation synchronously with a
joint model.

3 Markov Logic

Markov logic can be understood as a knowledge
representation with a weight attached to a ﬁrst-
order logic formula. Let us describe Markov
logic in the case of the semantic role labeling
task. We can model this task by ﬁrst introduc-
ing a set of logical predicates such as role(p, a, r)
and lemma(i, l), which means that the argument
at position a has the role r with respect to the
predicate at position p and token at position i has
lemma l respectively. Then we specify a set of
weighted ﬁrst order formulas that deﬁne a distri-
bution over sets of ground atoms of these predi-
cates (or so-called possible worlds).

Ideally, the distribution we deﬁne with these
weighted formulas assigns high probability to
possible worlds where semantic role labeling is
correct and a low probability to worlds where this
is not the case. For instance, for the sentence
in Figure 1, a suitable set of weighted formulas
would assign a high probability to the world:

lemma(1, cat), lemma(3, hit), lemma(5, ball)

role(3, 1, A0), role(3, 5, A1)

and low probabilities to other cases.

A Markov logic network (MLN) M is a set
of weighted formulas, i.e., a set of pairs (φ, ω),
where φ is a ﬁrst order formula and ω is the real
weight of the formula. M deﬁnes a probability
distribution over possible worlds:

p(y) =

1
Z

exp( X(φ,ω)∈M

ω Xc∈Cφ

f φ
c (y))

where each c is a binding of free variables in φ
to constants. Each f φ
c is a binary feature function
that returns 1 if the possible world y includes the
ground formula by replacing the free variables in
φ with the constants in c is true, and 0 otherwise.
Cφ is the set of all bindings for the variables in φ.
Z is a normalization constant.

4 Model

We divide our system into two stages: word sense
disambiguation and semantic role labeling. For
comparison, we can process them with pipeline
strategy, i.e., the word sense disambiguation re-
sults are used in semantic role labeling or the se-
mantic role labeling results are used in word sense
disambiguation. Of course, we can jointly process
them with Markov logic easily.

We deﬁne two hidden predicates for the two
stages respectively. For word sense disambigua-
tion, we deﬁne the predicate sense(w, s) which
indicates that the word at position w has the
sense s. For semantic role labeling, the predicate
role(p, a, r) is deﬁned as mentioned in above.

Different from Meza-Ruiz and Riedel (2009),
which only used sense number as word sense
representation, we use a triple (lemma, part-of-
speech, sense num) to represent the word sense
s. For example, (hit, v, 01) denotes that the verb
“hit” has sense number 01. Obviously, our rep-
resentation can distinguish different word senses
which have the identical sense number.
In ad-
dition, we use one argument classiﬁcation stage
with predicate role to label semantic roles as Che
et al. (2009). Similarly, no argument identiﬁca-
tion stage is used in our model. The approach can
improve the recall of the system.

In addition to the hidden predicates, we deﬁne
observable predicates to represent the information
available in the corpus. Table 1 presents these
predicates.

4.1 Local Formula
A local formula means that its groundings relate
any number of observed ground atoms to exactly
one hidden ground atom. For example

lemma(p, +l1)∧lemma(a, +l2) ⇒ role(p, a, +r)

164

Predicates
word(i, w)
pos(i, t)
lemma(i, l)
chdpos(i, t)

chddep(i, d)

f irstLemma(i, l)

lastLemma(i, l)

posF rame(i, f r)

dep(h, a, de)

isP redicate(p)
posP ath(p, a, pa)

depP ath(p, a, pa)

pathLen(p, a, le)

position(p, a, po)

f amily(p, a, f a)

wsdCand(i, t)

uniqe(r)

Description
Token i has word w
Token i has part-of-speech t
Token i has lemma l
The part-of-speech string of to-
ken i’s all children is t
The dependency relation string
of token i’s all children is t
The leftmost lemma of a sub-
tree rooted by token i is l
The rightmost lemma of a sub-
tree rooted by token i is l
f r is a part-of-speech frame at
token i
The dependency relation be-
tween an argument a and its
head h is de
Token p is a predicate
The part-of-speech path be-
tween a predicate p and an ar-
gument a is pa
The dependency relation path
between a predicate p and an ar-
gument a is pa
The path length between a pred-
icate p and an argument a is le
The relative position between a
predicate p and an argument a
is po
The family relation between a
predicate p and an argument a
is f a
Token i is a word sense disam-
biguation candidate, here t is
“v” or “n”
For a predicate, semantic role r
can only appear once

Table 1: Observable Predicates.

means that if the predicate lemma at position p
is l1 and the argument lemma at position a is l2,
then the semantic role between the predicate and
the argument is r with some possibility.

The + notation signiﬁes that Markov logic gen-
erates a separate formula and a separate weight for
each constant of the appropriate type, such as each
possible pair of lemmas (l1, l2, r). This type of
“template-based” formula generation can be per-
formed automatically by a Markov logic engine,
such as the thebeast1 system.

The local formulas are based on features em-
ployed in the state-of-the-art systems. For word
sense disambiguation, we use the basic features
mentioned by Zhong et al. (2008). The semantic
role labeling features are from Che et al. (2009),

1http://code.google.com/p/thebeast/

Features
Lemma
POS
FirstwordLemma
HeadwordLemma
HeadwordPOS
LastwordLemma
POSPath
PathLength
Position
PredicateLemma
PredicatePOS
RelationPath
DepRelation
POSUpPath
POSFrame
FamilyShip
BagOfWords
Window3OrderedWords
Window3OrderedPOSs

•
•

SRL WSD
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•

•
•
•

Table 2: Local Features.

the best system of the CoNLL 2009 shared task.
The ﬁnal features are listed in Table 2.

What follows are some simple examples in or-
der to explain how we implement each feature as
a formula (or a set of formulas).

Consider the “Position” feature. We ﬁrst intro-
duce a predicate position(p, a, po) that denotes
the relative position between predicate p and ar-
gument a is po. Then we add a formula

position(p, a, +po) ⇒ role(p, a, +r)

for all possible combinations of position and role
relations.

The “BagOfWords” feature means that

the
sense of a word w is determined by all of lemmas
in a sentence. Then, we add the following formula
set:

. . .

. . .

wsdCand(w, +tw) ∧ lemma(w, +lw) ∧ lemma(1, +l1) ⇒ sense(w, +s)
wsdCand(w, +tw) ∧ lemma(w, +lw) ∧ lemma(2, +li) ⇒ sense(w, +s)
wsdCand(w, +tw) ∧ lemma(w, +lw) ∧ lemma(n, +ln) ⇒ sense(w, +s)
where, the w is the position of current word and
tw is its part-of-speech tag, lw is its lemma.
li
is the lemma of token i. There are n tokens in a
sentence totally.

4.2 Global Formula
Global formulas relate more than one hidden
ground atoms. We use this type of formula for
two purposes:

165

1. To capture the global constraints among dif-

ferent semantic roles;

2. To reﬂect the joint relation between word
sense disambiguation and semantic role labeling.
Punyakanok et al. (2008) proposed an integer
linear programming (ILP) model to get the global
optimization for semantic role labeling, which sat-
isﬁes some constraints. This approach has been
successfully transferred into dependency parse
tree based semantic role labeling system by Che
et al. (2009). The ﬁnal results must satisfy two
constraints which can be described with Markov
logic formulas as follows:

C1: Each word should be labeled with one and

only one label.

role(p, a, r1) ∧ r1 6= r2 ⇒ ¬role(p, a, r2)
The same unique constraint also happens on the

word sense disambiguation, i.e.,

sense(w, s1) ∧ s1 6= s2 ⇒ ¬sense(w, s2)
C2: Some roles (A0∼A5) appear only once for

a predicate.

role(p, a1, r) ∧ uniqe(r) ∧ a1 6= a2 ⇒ ¬role(p, a2, r)

It is also easy to express the joint relation be-
tween word sense disambiguation and semantic
role labeling with Markov logic. What we need
to do is just adding some global formulas. The
relation between them can be shown in Figure 2.
Inspired by CoNLL 2008 (Surdeanu et al., 2008)
and 2009 (Hajiˇc et al., 2009) shared tasks, where
most of successful participant systems used pred-
icate senses for semantic role labeling, we also
model that the word sense disambiguation impli-
cates the semantic role labeling.

Here, we divide the all word sense disambigua-
tion task into two subtasks: predicate sense dis-
ambiguation and argument sense disambiguation.
The advantages of the division method approach
lie in two aspects. First, it makes us distinguish
the contributions of predicate and argument word
sense disambiguation respectively. Second, as
previous discussed, the predicate and argument
sense disambiguation can help each other. There-
fore, we can reﬂect the help with the division and
use Markov logic to represent it.

Figure 2: Global model between word sense dis-
ambiguation and semantic role labeling.

Finally, we use three global formulas to imple-
ment the three lines with direction in Figure 2.
They are:

sense(p, +s) ⇒ role(p, a, +r)
sense(a, +s) ⇒ role(p, a, +r)
sense(p, +s) ⇒ sense(a, +s)

5 Experiments
5.1 Experimental Setting
In our experiments, we use the OntoNotes
Release 3.02 corpus,
the latest version of
OntoNotes (Hovy et al., 2006). The OntoNotes
project leaders describe it as “a large, multilingual
richly-annotated corpus constructed at 90% inter-
nanotator agreement.” The corpus has been an-
notated with multiple levels of annotation, includ-
ing constituency trees, predicate argument struc-
ture, word senses, co-reference, and named enti-
ties. For this work, we focus on the constituency
trees, word senses, and predicate argument struc-
tures. The corpus has English, Chinese, and Ara-
bic portions, and we just use the English portion,
which has been split into four sections: broad-
cast conversation (bc), broadcast news (bn), mag-
azine (mz), and newswire (nw). There are several
datasets in each section, such as cnn and voa.

We will do our experiments on all of the
OntoNotes 3.0 English datasets. For each dataset,
we aimed for roughly a 60% train / 20% develop-
ment / 20% test split. See Table 3 for the detailed
statistics. Here, we use the human annotated part-
of-speech and parse trees provided by OntoNotes.
The lemma of each word is extracted using Word-
Net tool3.

2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?

catalogId=LDC2009T24

3http://wordnet.princeton.edu/

166

bc

bn

cctv
cnn
msnbc
phoenix
abc
cnn
mnb
nbc
pri
voa
sinorama

mz
nw wsj
xinhua
All

1,042
2,927
2,472
590
594
1,610
309
281
1,104
1,159
5,051
8,138
2,285

Training

(0000-0003)
(0000-0004)
(0000-0003)
(0000-0001)
(0001-0040)
(0001-0234)
(0001-0015)
(0001-0023)
(0001-0068)
(0001-0159)
(1001-1048)
(0020-1446)
(0001-0195)

27,562

Developing

328
963
1,209
240
146
835
111
128
399
315
1,262
2,549
724

(0004-0004)
(0005-0006)
(0004-0005)
(0002-0002)
(0041-0054)
(0235-0329)
(0016-0020)
(0024-0031)
(0069-0090)
(0160-0212)
(1049-1063)
(1447-1705)
(0196-0260)
9,209

333
880
1,315
322
126
1,068
114
78
366
315
1,456
3,133
670

Testing

(0005-0005)
(0007-0008)
(0006-0007)
(0003-0003)
(0057-0069)
(0330-0437)
(0021-0025)
(0032-0039)
(0091-0112)
(0213-0265)
(1064-1078)
(1730-2454)
(0261-0325)

10,176

Table 3: Training, developing and testing set sizes for the datasets in sentences. The ﬁle ranges (in
parenthesis) refer to the numbers within the names of the original OntoNotes 3.0 ﬁles. Here, we remove
4,873 sentences without semantic role labeling annotation.

Because we used semantic role labeling sys-
tem which is based on dependence syntactic trees,
we convert the constituency trees into dependence
trees with an Constituent-to-Dependency Conver-
sion Tool4.

The thebeast system is used in our experiment
as Markov logic engine. It uses cutting planes in-
ference technique (Riedel, 2008) with integer lin-
ear programming. The weights are learned with
MIRA (Crammer and Singer, 2003) online learn-
ing algorithm.

To our knowledge, this is the ﬁrst word sense
disambiguation and semantic role labeling exper-
iment on OntoNotes 3.0 corpus. In order to com-
pare our joint model with previous work, we build
several systems:

Baseline: There are two independent baseline
systems: word sense disambiguation and seman-
tic role labeling.
In each of baseline systems,
we only use the local formulas (Section 4.1) and
the global formulas which only express the global
constraints (Section 4.2).

Pipeline:

In a pipeline system, we use ad-
ditional features outputted by preceded stages.
Such as in semantic role labeling pipeline sys-
tem, we use word sense as features, i.e., we set
sense(w, s) as an observable predicate and add
sense(p, s) ⇒ role(p, a, r) and sense(a, s) ⇒
role(p, a, r) formulas into semantic role label-
ing task. As for word sense disambiguation

task, we add a set of formulas role(p, ai, r) ⇒
sense(p, s), where ai
is the ith argument of
the predicate at position p, and a formula
role(p, a, r) ⇒ sense(p, s) for the argument at
position a respectively.
Jointly: We use all global formulas mentioned
in Section 4.2. With Markov logic, we can add
global constraints and get the word sense disam-
biguation and the semantic role labeling results si-
multaneously.

5.2 Results and Discussion
The performance of these systems on test set is
shown in Table 4. All of the parameters are ﬁne
tuned on the development set.

Here, we only consider the noun and verb word
sense disambiguation, which cover most of multi-
sense words. Therefore, the word sense disam-
biguation performance means the accuracy of all
nouns and verbs in the test set. The performance
of semantic role labeling is calculated using the
semantic evaluation metric of the CoNLL 2009
shared task scorer5. It measures the precision, re-
call, and F1 score of the recovered semantic de-
pendencies. The F1 score is used as the ﬁnal per-
formance metric. A semantic dependency is cre-
ated for each predicate and its arguments. The la-
bel of such dependency is the role of the argument.
The same with the CoNLL 2009 shared task, we
assume that the predicates have been identiﬁed

4http://nlp.cs.lth.se/software/treebank converter/

5http://ufal.mff.cuni.cz/conll2009-st/eval09.pl

167

Most Frequent Sense

Baseline

Pipeline

Jointly

PS
AS

PS + AS
PS ⇒ SRL
AS ⇒ SRL
PS ⇒ AS
Fully

PS + AS ⇒ SRL

WSD
SRL
85.58 —
89.37
89.53
89.41

83.97
84.17
83.94
— 84.24
84.27
84.16

89.53
89.49
89.45 —
89.54
89.55

84.34
84.36

Table 4: The results of different systems. Here, PS
means predicate senses and AS means argument
senses.

correctly.

The ﬁrst row of Table 4 gives the word sense
disambiguation result with the most frequent
sense, i.e., the #01 sense of each candidate word
which normally is the most frequent one in a bal-
ance corpus.

The second row shows the baseline perfor-
mances. Here, we note that the 89.37 word sense
disambiguation accuracy and the 83.97 semantic
role labeling F1 we obtained are comparable to
the state-of-the-art systems, such as the 89.1 word
sense disambiguation accuracy given by Zhong et
al. (2008) and 85.48 semantic role labeling perfor-
mance given by Che et al. (2010) on OntoNotes
2.0 respectively, although the corpus used in our
experiments is upgraded version of theirs6. Ad-
ditionally,
the performance of word sense dis-
ambiguation is higher than that of the most fre-
quent sense signiﬁcantly (z-test7 with ρ < 0.01).
Therefore, the experimental results show that the
Markov logic can achieve considerable perfor-
mances for word sense disambiguation and se-
mantic role labeling on the latest OntoNotes 3.0
corpus.

There are two kinds of pipeline systems: word
sense disambiguation (WSD) based on semantic
role labeling and semantic role labeling (SRL)
based on word sense disambiguation. For the us-
ing method of word senses, we ﬁrst only exploit
predicate senses (PS) as mentioned by Surdeanu
et al. (2008) and Hajiˇc et al. (2009). Then, in or-

6Compared with OntoNotes 2.0, the version 3.0 incorpo-

rates more corpus.

calculators/ztest.html

7http://www.dimensionresearch.com/resources/

der to examine the contribution of word senses ex-
cept for predicates, we use argument senses (AS)
in isolation. Finally, all word senses (PS + AS)
were considered.

We can see that when the predicate senses (PS)
are used to label semantic role, the performance
of semantic role labeling can be improved from
83.97 to 84.17. The conclusion, that the predi-
cate sense can improve semantic role labeling per-
formance, is similar with CoNLL 2008 (Surdeanu
et al., 2008) and 2009 (Hajiˇc et al., 2009) shared
tasks. However, the improvement is not signiﬁ-
cant (χ2-test8 with ρ < 0.1). Additionally, the
semantic role labeling can improve the predicate
sense disambiguation signiﬁcantly from 89.37 to
89.53 (z-test with ρ < 0.1). The same conclusion
was obtained by Dang and Palmer (2005).

However, when we only use argument senses
(AS), both of the word sense disambiguation and
semantic role labeling performances are almost
unchanged (from 89.37 to 89.41 and from 83.97
to 83.94 respectively). For the semantic role la-
beling task, the reason is that the original lemma
and part-of-speech features have been able to de-
scribe the argument related information. This kind
of sense features is just reduplicate. On the other
hand, the argument senses cannot be determined
only by the semantic roles. For example, the
semantic role “A1” cannot predict the argument
sense of “ball” exactly. The predicates must be
considered simultaneously.

Therefore, we use the last strategy (PS + AS),
which combines the predicate sense and the ar-
gument sense together to predict semantic roles.
The results show that the performance can be
improved signiﬁcantly (χ2-test with ρ < 0.05)
from 83.97 to 84.24. Accordingly, the experi-
ment proves that automatic all word sense disam-
biguation can further improve the semantic role
labeling performance. Different from Che et al.
(2010), where the semantic role labeling can be
improved with correct word senses about F1 = 1,
our improvement is much lower. The main reason
is that the performance of our word sense disam-
biguation with the most basic features is not high
enough. Another limitation of the pipeline strat-

8http://graphpad.com/quickcalcs/chisquared1.cfm

168

egy is that it is difﬁcult to predict the combination
between predicate and argument senses. This is
an obvious shortcoming of the pipeline method.

With Markov logic, we can easily join different
tasks with global formulas. As shown in Table 4,
we use ﬁve joint strategies:

1. PS ⇒ SRL: means that we jointly disam-
biguate predicate senses and label semantic roles.
Compared with the pipeline PS system, word
sense disambiguation performance is unchanged.
However, the semantic role labeling performance
is improved from 84.17 to 84.27. Compared with
the baseline’s 83.97, the improvement is signiﬁ-
cant (χ2-test with ρ < 0.05).

2. AS ⇒ SRL: means that we jointly disam-
biguate argument senses and label semantic roles.
Compared with the pipeline AS system, both of
word sense disambiguation and semantic role la-
beling performances are improved (from 89.41 to
89.49 and from 83.94 to 84.16 respectively). Al-
though, the improvement is not signiﬁcant, it is
observed that the joint model has the capacity to
improve the performance, especially for semantic
role labeling, if we could have a more accurate
word sense disambiguation.

3.

PS ⇒ AS: means that we jointly dis-
ambiguate predicate word senses and argument
senses. This kind of joint model does not inﬂu-
ence the performance of semantic role labeling.
The word sense disambiguation outperforms the
baseline system from 89.37 to 89.45. The result
veriﬁes our assumption that the predicate and ar-
gument senses can help each other.

4. PS + AS ⇒ SRL: means that we jointly
disambiguate all word senses and label semantic
roles. Compared with the pipeline method which
uses the PS + AS strategy, the joint method can
further improve the semantic role labeling (from
84.24 to 84.34). Additionally, it can obtain the
predicate and argument senses together. The all
word sense disambiguation performance (89.54)
is higher than the baseline (89.37) signiﬁcantly (z-
test with ρ < 0.1).

5. Fully: ﬁnally, we use all of the three global
formulas together, i.e., we jointly disambiguate
predicate senses, argument senses, and label se-
mantic roles. It fully joins all of the tasks. Both of
all word sense disambiguation and semantic role

labeling performances can be further improved.
Although the improvements are not signiﬁcant
compared with the best pipeline system, they sig-
niﬁcantly (z-test with ρ < 0.1 and χ2-test with
ρ < 0.01 respectively) outperform the baseline
system. Additionally, the performance of the fully
joint system does not outperform partly joint sys-
tems signiﬁcantly. The reason seems to be that
there is some overlap among the contributions of
the three joint systems.

6 Conclusion

In this paper, we presented a Markov logic model
that jointly models all word sense disambiguation
and semantic role labeling. We got the following
conclusions:

1. The baseline systems with Markov logic is
competitive to the state-of-the-art word sense dis-
ambiguation and semantic role labeling systems
on OntoNotes 3.0 corpus.

2. The predicate sense disambiguation is ben-
eﬁcial to semantic role labeling. However, the
automatic argument sense disambiguation itself is
harmful to the task. It must be combined with the
predicate sense disambiguation.

3. The semantic role labeling not only can help
predicate sense disambiguation, but also argument
sense disambiguation (a little).
In contrast, be-
cause of the limitation of the pipeline model, it
is difﬁcult to make semantic role labeling to help
predicate and argument sense disambiguation si-
multaneously.

4.

It is easy to implement the joint model of
all word sense disambiguation and semantic role
labeling with Markov logic. More important, the
joint model can further improve the performance
of the all word sense disambiguation and semantic
role labeling than pipeline systems.

Acknowledgement

This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the “863” National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientiﬁc Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).

169

References
Che, Wanxiang, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing.
In Proceedings of CoNLL 2009: Shared Task, pages
49–54, June.

Che, Wanxiang, Ting Liu, and Yongqiang Li. 2010.
Improving semantic role labeling with word sense.
In NAACL 2010, pages 246–249, June.

Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951–991.

Kruengkrai, Canasai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of ACL-IJCNLP 2009,
pages 513–521, August.

Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic.
In Proceedings of NAACL 2009,
pages 155–163, June.

Poon, Hoifung and Pedro Domingos. 2008.

Joint
unsupervised coreference resolution with Markov
Logic. In Proceedings of EMNLP 2008, pages 650–
659, October.

Dahlmeier, Daniel, Hwee Tou Ng, and Tanja Schultz.
2009. Joint learning of preposition senses and se-
mantic roles of prepositional phrases. In Proceed-
ings of EMNLP 2009, pages 450–458, August.

Punyakanok, Vasin, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).

Dang, Hoa Trang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL 2005, pages 42–49, Morris-
town, NJ, USA.

Domingos, Pedro and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artiﬁcial Intelligence.
Synthesis Lectures on Artiﬁcial Intelligence and
Machine Learning. Morgan & Claypool Publishers.

Riedel, Sebastian. 2008. Improving the accuracy and
efﬁciency of map inference for markov logic.
In
Proceedings of UAI 2008, pages 468–475. AUAI
Press.

Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of CoNLL
2008, pages 159–177, August.

Finkel, Jenny Rose and Christopher D. Manning.
2009. Joint parsing and named entity recognition.
In Proceedings of NAACL 2009, pages 326–334,
June.

Titov, Ivan and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL 2008, pages 308–316,
June.

Goldberg, Yoav and Reut Tsarfaty. 2008. A single
generative model for joint morphological segmenta-
tion and syntactic parsing. In Proceedings of ACL
2008, pages 371–379, June.

Hajiˇc, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages.
In Proceedings of CoNLL
2009: Shared Task, pages 1–18, June.

Hovy, Eduard, Mitchell Marcus, Martha Palmer,
2006.
In Proceedings of

Lance Ramshaw, and Ralph Weischedel.
Ontonotes: The 90% solution.
NAACL 2006, pages 57–60, June.

Toutanova, Kristina and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech
prediction.
In Proceedings of ACL-IJCNLP 2009,
pages 486–494, August.

Toutanova, Kristina, Aria Haghighi, and Christo-
pher D. Manning. 2008. A global joint model for
semantic role labeling. Computational Linguistics,
34(2).

Yoshikawa, Katsumasa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly iden-
tifying temporal relations with markov logic.
In
Proceedings of ACL-IJCNLP 2009, pages 405–413,
August.

Zhang, Yue and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL 2008, pages 888–896,
June.

Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan L¨u.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL 2008, pages 897–904, June.

Zhong, Zhi, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study.
In Proceedings of EMNLP 2008,
pages 1002–1010, October.

