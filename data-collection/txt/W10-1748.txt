










































BBN System Description for WMT10 System Combination Task


Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 321–326,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

BBN System Description for WMT10 System Combination Task

Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA

{arosti,bzhang,smatsouk,schwartz}@bbn.com

Abstract

BBN submitted system combination out-
puts for Czech-English, German-English,
Spanish-English, French-English, and All-
English language pairs. All combinations
were based on confusion network decod-
ing. An incremental hypothesis alignment
algorithm with flexible matching was used
to build the networks. The bi-gram de-
coding weights for the single source lan-
guage translations were tuned directly to
maximize the BLEU score of the decod-
ing output. Approximate expected BLEU
was used as the objective function in gra-
dient based optimization of the combina-
tion weights for a 44 system multi-source
language combination (All-English). The
system combination gained around 0.4-
2.0 BLEU points over the best individual
systems on the single source conditions.
On the multi-source condition, the system
combination gained 6.6 BLEU points.

1 Introduction

The BBN submissions to the WMT10 system
combination task were based on confusion net-
work decoding. The confusion networks were
built using the incremental hypothesis alignment
algorithm with flexible matching introduced in the
BBN submission for the WMT09 system combi-
nation task (Rosti et al., 2009). This year, the
system combination weights were tuned to max-
imize the BLEU score (Papineni et al., 2002) of
the 1-best decoding output (lattice based BLEU
tuning) using downhill simplex method (Press et
al., 2007). A 44 system multi-source combina-
tion was also submitted. Since the gradient-free
optimization algorithms do not seem to be able to
handle more than 20-30 weights, a gradient ascent
to maximize an approximate expected BLEU ob-

jective was used to optimize the larger number of
weights.

The lattice based BLEU tuning may be imple-
mented using any optimization algorithm that does
not require the gradient of the objective function.
Due to the size of the lattices, the objective func-
tion evaluation may have to be distributed to mul-
tiple servers. The optimizer client accumulates the
BLEU statistics of the 1-best hypotheses from the
servers for given search weights, computes the fi-
nal BLEU score, and passes it to the optimiza-
tion algorithm which returns a new set of search
weights. The lattice based tuning explores the en-
tire search space and does not require multiple de-
coding iterations with N -best list merging to ap-
proximate the search space as in the standard min-
imum error rate training (Och, 2003). This allows
much faster turnaround in weight tuning.

Differentiable approximations of BLEU have
been proposed for consensus decoding. Tromble
et al. (2008) used a linear approximation and Pauls
et al. (2009) used a closer approximation called
CoBLEU. CoBLEU is based on the BLEU for-
mula but the n-gram counts are replaced by ex-
pected counts over a translation forest. Due to the
min-functions required in converting the n-gram
counts to matches and a non-differentiable brevity
penalty, a sub-gradient ascent must be used. In
this work, an approximate expected BLEU (Exp-
BLEU) defined over N -best lists was used as a
differentiable objective function. ExpBLEU uses
expected BLEU statistics where the min-function
is not needed as the statistics are computed off-
line and the brevity penalty is replaced by a dif-
ferentiable approximation. The ExpBLEU tun-
ing yields comparable results to direct BLEU tun-
ing using gradient-free algorithms on combina-
tions of small number of systems (fewer than 20-
30 weights). Results on a 44 system combination
show that the gradient based optimization is more
robust with larger number of weights.

321



This paper is organized as follows. Section
2 reviews the incremental hypothesis alignment
algorithm used to built the confusion networks.
Decoding weight optimization using direct lattice
1-best BLEU tuning and N -best list based Exp-
BLEU tuning are presented in Section 3. Exper-
imental results on combining single source lan-
guage to English outputs and all 44 English out-
puts are detailed in Section 4. Finally, Section 5
concludes this paper with some ideas for future
work.

2 Hypothesis Alignment

The confusion networks were built by using the
incremental hypothesis alignment algorithm with
flexible matching introduced in Rosti et al. (2009).
The algorithm is reviewed in more detail here. It
is loosely related to the alignment performed in
the calculation of the translation edit rate (TER)
(Snover et al., 2006) which estimates the edit
distance between two strings allowing shifts of
blocks of words in addition to insertions, dele-
tions, and substitutions. Calculating an exact TER
for strings longer than a few tokens1 is not compu-
tationally feasible, so the tercom2 software uses
heuristic shift constraints and pruning to find an
upper bound of TER. In this work, the hypothe-
ses were aligned incrementally with the confusion
network, thus using tokens from all previously
aligned hypotheses in computing the edit distance.
Lower substitution costs were assigned to tokens
considered equivalent and the heuristic shift con-
straints of tercom were relaxed3.

First, tokens from all hypotheses are put into
equivalence classes if they belong to the same
WordNet (Fellbaum, 1998) synonym set or have
the same stem. The 1-best hypothesis from each
system is used as the confusion network skeleton
which defines the final word order of the decod-
ing output. Second, a trivial confusion network
is generated from the skeleton hypothesis by gen-
erating a single arc for each token. The align-
ment algorithm explores shifts of blocks of words
that minimize the edit distance between the cur-
rent confusion network and an unaligned hypothe-

1Hypotheses are tokenized and lower-cased prior to align-
ment. Tokens generally refer to words and punctuation.

2http://www.cs.umd.edu/˜snover/tercom/
current version 0.7.25.

3This algorithm is not equivalent to an incremental TER-
Plus (Snover et al., 2009) due to different shift constraints and
the lack of paraphrase matching

30 1cat(1) 2sat(1) mat(1)

(a) Skeleton hypothesis.

40 1cat(1,1) 2sat(1,1) 3on(0,1)
NULL(1,0)

mat(1,1)

(b) Two hypotheses (insertion).

40 1cat(1,1,0)
NULL(0,0,1)

2sat(1,1,1) 3on(0,1,0)
NULL(1,0,1)

mat(1,1,1)

(c) Three hypotheses (deletion).

40 1cat(1,1,0,1)
NULL(0,0,1,0)

2sat(1,1,1,1) 3on(0,1,0,0)
NULL(1,0,1,1)

mat(1,1,1,0)
hat(0,0,0,1)

(d) Four hypotheses (substitution).

Figure 1: Example of incrementally aligning “cat
sat mat”, “cat sat on mat”, “sat mat”, and “cat sat
hat”.

sis. Third, the hypothesis with the lowest edit dis-
tance to the current confusion network is aligned
into the network. The heuristically selected edit
costs used in the WMT10 system were 1.0 for
insertions, deletions, and shifts, 0.2 for substitu-
tions of tokens in the same equivalence class, and
1.0001 for substitutions of non-equivalent tokens.
An insertion with respect to the network always
results in a new node and two new arcs. The first
arc contains the inserted token and the second arc
contains a NULL token representing the missing
token from all previously aligned hypotheses. A
substitution/deletion results in a new token/NULL
arc or increase in the confidence of an existing to-
ken/NULL arc. The process is repeated until all
hypotheses are aligned into the network.

For example, given the following hypotheses
from four systems: “cat sat mat”, “cat sat on mat”,
“sat mat”, and “cat sat hat”, an initial network in
Figure 1(a) is generated. The following two hy-
potheses have a distance of one edit from the initial
network, so the second can be aligned next. Figure
1(b) shows the additional node created and the two
new arcs for ‘on’ and ‘NULL’ tokens. The third
hypothesis has deleted token ‘cat’ and matches the

322



‘NULL’ token between nodes 2 and 3 as seen in
Figure 1(c). The fourth hypothesis matches all but
the final token ‘hat’ which becomes a substitution
for ‘mat’ in Figure 1(d). The binary vectors in
the parentheses following each token show which
system generated the token aligned to that arc. If
the systems generated N -best hypotheses, a frac-
tional increment could be added to these vectors
as in (Rosti et al., 2007). Given these system spe-
cific scores are normalized to sum to one over all
arcs connecting two consecutive nodes, they may
be viewed as system specific word arc posterior
estimates. Note, for 1-best hypotheses the scores
sum to one without normalization.

Given system outputs E = {E1, . . . , ENs},
an algorithm to build a set of Ns confusion
networks C = {C1, . . . , CNs} may be written
as:

for n = 1 to Ns do
Cn ⇐ Init(En) {initialize confusion net-
work from the skeleton}
E ′ ⇐ E − En {set of unaligned hypotheses}
while E ′ 6= ∅ do

Em ⇐ arg minE∈E ′ Dist(E,Cn)
{compute edit distances}
Cn ⇐ Align(Em, Cn) {align closest hy-
pothesis}
E ′ ⇐ E ′ − Em {update set of unaligned
hypotheses}

end while
end for

The set of Ns confusion networks are expanded to
separate paths with distinct bi-gram contexts and
connected in parallel into a big lattice with com-
mon start and end nodes with NULL token arcs.
A prior probability estimate is assigned to the sys-
tem specific word arc confidences connecting the
common start node and the first node in each sub-
network. A heuristic prior is estimated as:

pn =
1
Z

exp(−100 en
Nn

) (1)

where en is the total cost of aligning all hypothe-
ses when using system n as the skeleton, Nn is
the number of nodes in the confusion network be-
fore bi-gram expansion, and Z is a scaling factor
to guarantee pn sum to one. This gives a higher
prior for a network with fewer alignment errors
and longer expected decoding output.

3 Weight Optimization

Standard search algorithms may be used to find N -
best hypotheses from the final lattice. The score
for arc l is computed as:

sl = log
( Ns∑

n=1

σnsnl
)

+ λL(wl|wP (l)) + ωS(wl)

(2)
where σn are the system weights constrained to
sum to one, snl are the system specific arc pos-
teriors, λ is a language model (LM) scaling fac-
tor, L(wl|wP (l)) is the bi-gram log-probability for
the token wl on the arc l given the token wP (l)
on the arc P (l) preceding the arc l, ω is the word
insertion scaling factor, and S(wl) is zero if wl
is a NULL token and one otherwise. The path
with the highest total score under summation is
the 1-best decoding output. The decoding weights
θ = {σ1, . . . , σNs , λ, ω} are tuned to optimize two
objective functions described next.

3.1 Lattice Based BLEU Optimization

Powell’s method (Press et al., 2007) on N -best
lists was used in system combination weight tun-
ing in Rosti et al. (2007). This requires multiple
decoding iterations and merging the N -best lists
between tuning runs to approximate the full search
space as in Och (2003). To speed up the tuning
process, a distributed optimization method can be
used. The lattices are divided into multiple chunks
each of which are loaded into memory by a server.
A client runs the optimization algorithm relying
on the servers for parallelized objective function
evaluation. The client sends a new set of search
weights to the servers which decode the chunks
of lattices and return the 1-best hypothesis BLEU
statistics back to the client. The client accumulates
the BLEU statistics from all servers and computes
the final BLEU score used as the objective func-
tion by the optimization algorithm. Results similar
to Powell’s method can be obtained with fewer it-
erations by using the downhill simplex method in
multi-dimensions (Amoeba) (Press et al., 2007).
To enforce the sum to one constraint of the sys-
tem weights σn, the search weights are restricted
to [0, 1] by assigning a large penalty if any cor-
responding search weight breaches the limits and
these restricted search weights are scaled to sum
to one before the objective function evaluation.

After optimizing the bi-gram decoding weights
directly on the lattices, a 300-best list are gener-

323



ated. The 300-best hypotheses are re-scored using
a 5-gram LM and another set of re-scoring weights
are tuned on the development set using the stan-
dard N -best list based method. Multiple random
restarts may be used in both lattice and N-best list
based optimization to decrease chances of finding
a local minimum. Twenty sets of initial weights
(the weights from the previous tuning and 19 ran-
domly perturbed weights) were used in all experi-
ments.

3.2 Approximate Expected BLEU
Optimization

The gradient-free optimization algorithms like
Powell’s method and downhill simplex work well
for up to around 20-30 weights. When the number
of weights is larger, the algorithms often get stuck
in local optima even if multiple random restarts
are used. The BLEU score for a 1-best output is
defined as follows:

BLEU =
4∏

n=1

(∑
i m

n
i∑

i h
n
i

) 1
4

φ

(
1−

∑
i ri∑
i h

1
i

)
(3)

where mni is the number of n-gram matches be-
tween the hypothesis and reference for segment
i, hni is the number of n-grams in the hypothesis,
ri is the reference length (or the reference length
closest to the hypothesis if multiple references are
available), and φ(x) = min(1.0, ex) is the brevity
penalty. The first term in Equation 3 is a harmonic
mean of the n-gram precisions up to n = 4. The
selection of 1-best hypotheses is discrete and the
brevity penalty is not continuous, so the BLEU
score is not differentiable and gradient based op-
timization cannot be used. Given a posterior dis-
tribution over all possible decoding outputs could
be defined, an expected BLEU could be optimized
using gradient ascent. However, this posterior dis-
tribution can only be approximated by expensive
sampling methods.

A differentiable objective function over N -best
lists to approximate the BLEU score can be de-
fined using expected BLEU statistics and a con-
tinuous approximation of the brevity penalty. The
posterior probability for hypothesis j of segment i
is simply the normalized decoder score:

pij =
eγSij∑
k e

γSik
(4)

where γ is a posterior scaling factor and Sij is the
total score of hypothesis j of segment i. The pos-

terior scaling factor controls the shape of the pos-
terior distribution: γ > 1.0 moves the probability
mass toward the 1-best hypothesis and γ < 1.0
flattens the distribution. The BLEU statistics in
Equation 3 are replaced by the expected statistics;
for example, m̂ni =

∑
j pijmij , and the brevity

penalty φ(x) is approximated by:

ϕ(x) =
ex − 1

e1000x + 1
+ 1 (5)

ExpBLEU has a closed form solution for the gra-
dient, provided the total decoder score is differen-
tiable.

The penalty used to restrict the search weights
corresponding to the system weights σn in
gradient-free BLEU tuning is not differentiable.
For expected BLEU tuning, the search weights ςn
are unrestricted but the system weights are ob-
tained by a sigmoid transform and normalized to
sum to one:

σn =
δ(ςn)∑
m δ(ςm)

(6)

where δ(ςn) = 1/(1 + e−ςn).
The expected BLEU tuning is performed on N -

best lists in similar fashion to direct BLEU tuning.
Tuned weights from one decoding iteration are
used to generate a new N -best list, the new N -best
list is merged with the N -best list from the previ-
ous tuning run, and a new set of weights are op-
timized using limited memory Broyden-Fletcher-
Goldfarb-Shanno method (lBFGS) (Liu and No-
cedal, 1989). Since the posterior distribution is
affected by the size of the N -best list and differ-
ent decoding weights, the posterior scaling factor
can be set for each tuning run so that the perplex-
ity of the posterior distribution given the merged
N -best list is constant. A target perplexity of 5.0
was used in the experiments. Four iterations of
bi-gram decoding weight tuning were performed
using 300-best lists. The final 300-best list was re-
scored with a 5-gram and another set of re-scoring
weights was tuned on the development set.

4 Experimental Evaluation

System outputs for all language pairs with En-
glish as the target were combined. Unpruned En-
glish bi-gram and 5-gram language model com-
ponents were trained using the WMT10 corpora:
EuroParl, GigaFrEn, NewsCommentary,
and News. Additional six Gigaword v4 com-
ponents were trained: AFP, APW, XIN+CNA,

324



tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU

worst 68.99 13.85 68.45 15.07 60.86 21.02 71.17 15.00
best 56.77 22.84 57.76 25.05 51.81 30.10 53.66 28.64
syscomb 57.31 25.11 54.97 27.75 50.46 31.54 51.35 31.16

test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU

worst 68.65 14.29 67.50 15.66 60.52 21.86 68.36 16.82
best 56.13 23.56 58.12 24.34 51.45 30.56 52.16 29.79
syscomb 56.89 25.12 55.60 26.38 50.33 31.59 51.36 30.16

Table 1: Case insensitive TER and BLEU scores on syscombtune (tune) and syscombtest (test)
for combinations of outputs from four source languages.

LTW, NYT, and Headlines+Datelines. In-
terpolation weights for the ten components
were tuned so as to minimize perplexity on
the newstest2009-ref.en development set.
The LMs used modified Kneser-Ney smoothing.
On the multi-source condition (xx-en) another
LM was trained from the system outputs and in-
terpolated with the general LM using an interpola-
tion weight 0.3 for the LM trained on the system
outputs. This LM is referred to as biasLM later.
A tri-gram true casing model was trained using all
available English data. This model was used to
restore the case of the lower-case system combi-
nation output.

All six 1-best system outputs on cz-en, 16
outputs on de-en, 8 outputs on es-en, and
14 outputs on fr-en were combined. The lat-
tice based BLEU tuning was used to optimize the
bi-gram decoding weights and N-best list based
BLEU tuning was used to optimize the 5-gram re-
scoring weights. Results for these single source
language experiments are shown in Table 1. The
gains on syscombtune were similar to those on
syscombtest for all but French-English. The
tuning set contained only 455 segments but ap-
peared to be well matched with the larger (2034
segments) test set. The characteristics of the indi-
vidual system outputs were probably different for
the tuning and test sets on French-English transla-
tion. In our experience, optimizing system com-
bination weights using the ExpBLEU tuning for
a small number of systems yields similar results
to lattice based BLEU tuning. The lattice based
BLEU tuning is faster as there is no need for mul-
tiple decoding and tuning iterations. Using the bi-
asLM on the single source combinations did not

xx-en tune test
System TER BLEU TER BLEU

worst 71.17 13.85 68.65 14.29
best 51.81 30.10 51.45 30.56
lattice 43.15 35.72 43.79 35.29
expBLEU 44.07 36.91 44.35 36.62
+biasLM 43.63 37.61 44.50 37.12

Table 2: Case insensitive TER and BLEU scores
on syscombtune (tune) and syscombtest
(test) for xx-en combination. Combinations us-
ing lattice BLEU tuning, expected BLEU tuning,
and after adding the system output biased LM are
shown.

yield any gains. The output for these conditions
probably did not contain enough data for biasLM
training given the small tuning set and small num-
ber of systems.

Finally, experiments combining all 44 1-best
system outputs were performed to produce a
multi-source combination output. The first experi-
ment used the lattice based BLEU tuning and gave
a 5.6 BLEU point gain on the tuning set as seen in
Table 2. The ExpBLEU tuning gave an additional
1.2 point gain which suggests that the direct lattice
based BLEU tuning got stuck in a local optimum.
Using the system output biased LM gave an addi-
tional 0.7 point gain. The gains on the test set were
similar and the best combination gave a 6.6 point
gain over the best individual system.

5 Conclusions

The BBN submissions for WMT10 system com-
bination task were described in this paper. The
combination was based on confusion network de-

325



coding. The confusion networks were built us-
ing an incremental hypothesis alignment algo-
rithm with flexible matching. The bi-gram de-
coding weights for the single source conditions
were optimized directly to maximize the BLEU
scores of the 1-best decoding outputs and the 5-
gram re-scoring weights were tuned on 300-best
lists. The BLEU gains over the best individual
system outputs were around 1.5 points on cz-en,
2.0 points on de-en, 1.0 points on es-en, and
0.4 points on fr-en. The system combination
weights on xx-en were tuned to maximize Exp-
BLEU, and a system output biased LM was used.
The BLEU gain over the best individual system
was 6.6 points. Future work will investigate tuning
of the edit costs used in the alignment. A lattice
based ExpBLEU tuning will be investigated. Also,
weights for more complicated functions with addi-
tional features may be tuned using ExpBLEU.

Acknowledgments

This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.

References
Christiane Fellbaum, editor. 1998. WordNet: An Elec-

tronic Lexical Database. MIT Press.

Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory method for large scale optimization. Math-
ematical Programming, 45(3):503–528.

Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160–167.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311–318.

Adam Pauls, John DeNero, and DanKlein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1418–1427.

William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2007. Numerical
recipes: the art of scientific computing. Cambridge
University Press, 3rd edition.

Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of

the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312–319.

Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hy-
pothesis alignment with flexible matching for build-
ing confusion networks: BBN system description
for WMT09 system combination task. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 61–65.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223–231.

Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259–268.

Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing,
pages 620–629.

326


