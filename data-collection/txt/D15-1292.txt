



















































Navigating the Semantic Horizon using Relative Neighborhood Graphs


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2451–2460,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Navigating the Semantic Horizon
using Relative Neighborhood Graphs

Amaru Cuba Gyllensten and Magnus Sahlgren
Gavagai

Bondegatan 21
116 33 Stockholm

Sweden
{amaru|mange}@gavagai.se

Abstract

This paper introduces a novel way to nav-
igate neighborhoods in distributional se-
mantic models. The approach is based
on relative neighborhood graphs, which
uncover the topological structure of local
neighborhoods in semantic space. This
has the potential to overcome both the
problem with selecting a proper k in k-NN
search, and the problem that a ranked list
of neighbors may conflate several different
senses. We provide both qualitative and
quantitative results that support the viabil-
ity of the proposed method.

1 Introduction

Nearest neighbor search is a fundamental opera-
tion in data mining, in which we are interested in
finding the closest points to some given reference
point. Formally, if we have a reference point r
and a set of other points P in a metric space M
with some distance function d, the nearest neigh-
bor search task is to find the point p ∈ P that min-
imizes d(p, r). In k-Nearest Neighbor search (k-
NN), we want to find the k closest points to some
given reference point. Nearest neighbor search
is a well-studied task, and in particular the com-
plexity of the task (a linear search has a running
time of O(Ni) where N is the cardinality of P
and i the complexity of the distance function d)
has generated a lot of research; suggestions for re-
ducing the complexity of linear nearest neighbor
searches include using various types of space par-
titioning techniques like k-d trees (Bentley, 1975),
or various techniques for doing approximate near-
est neighbor search (Arya et al., 1998), of which
one of the most well-known is locality-sensitive
hashing (Indyk and Motwani, 1998).

The problem we are concerned with in this
paper is not the complexity of nearest neighbor

search, but the question of how to identify the in-
ternal structure of neighborhoods defined by the
nearest neighbors. The problem with a normal k-
NN is that the result — a sorted list of the k nearest
neighbors — does not say anything about the inter-
nal structure of the neighborhood. It is quite pos-
sible for two neighborhoods with widely different
internal structures to produce identical k-NN re-
sults. In the context of Distributional Semantic
Models (DSMs), which collect and represent co-
occurrence statistics in high-dimensional vector
spaces, such structural differences may carry sig-
nificant semantic information, e.g. about the dif-
ferent senses of terms. We argue that the inability
of standard k-NN to account for structural prop-
erties has been misinterpreted as a shortcoming
of the distributional representation (Erk and Padó,
2010).

We will demonstrate in this paper that this is
not a shortcoming of the distributional represen-
tation, but of the mode of querying the DSM. We
argue that information about the different usages
(i.e. senses) of a term is encoded in the structural
properties of the nearest neighborhoods, and we
propose the use of relative neighborhood graphs
for identifying these structural properties. Relative
neighborhood graphs may also be used for finding
a relevant k for a given reference point, which we
refer to as the horizon with respect to the reference
point.

2 Distributional Semantics and Nearest
Neighbor Search

Collecting and comparing co-occurrence statis-
tics for terms in language has become a stan-
dard approach for computational semantics, and
is now commonly referred to as distributional se-
mantics. There are many different types of mod-
els that can be used for this purpose, but their
common objective is to represent terms as vec-
tors that record (some function of) their distri-

2451



butional properties. The standard approach for
generating such vectors is to collect distributional
statistics in a co-occurrence matrix that records
co-occurrence counts between terms and contexts.
The co-occurrence matrix is then subject to var-
ious types of transformations, ranging from the
application of simple frequency filters or associ-
ation measures to matrix factorization or regres-
sion models. The resulting representations are re-
ferred to as distributional vectors (or word embed-
dings), which are used to compute similarity be-
tween terms.

Given a similarity — or distance — measure on
such distributional vectors, we can perform a near-
est neighbor search. This is a particularly impor-
tant operation in distributional semantics, since it
answers the question “which other terms are sim-
ilar to this one?” and this is a central question in
semantics; lexica and thesauri are built with the
main purpose of answering this question. Conse-
quently, nearest neighbor search in a DSM could
be seen as a compilation step in a distributional
lexicon.

The result of a nearest neighbor search in a
DSM is often presented as a list of (the top k)
neighbors, sorted by descending similarity with
the target term. Table 1 illustrates typical sorted
nearest neighbor lists produced with three dif-
ferent DSMs: a standard model based on Point-
wise Mutual Information (PMI)1 that has been re-
duced to 2,000 dimensions by applying a Gaus-
sian random projection; GloVe, which uses regres-
sion to find distributional vectors such that their
dot product approximates their log probability of
co-occurring (Pennington et al., 2014); and the
Skipgram model, which uses stochastic gradient
descent and hierarchical softmax combined with
negative sampling and subsampling to find dis-
tributional vectors that maximize the probability
of observed co-occurrence events (Mikolov et al.,
2013). We refer to the respective papers for de-
tails regarding the various models. The similarity
measure used is the cosine similarity: s(a, b) =

a·b
‖a‖‖b‖ .

Table 1 lists the 10 nearest neighbors to suit
in these three different DSMs using the entire
Wikipedia as data. As can be expected, there
are both similarities and dissimilarities between

1For observations a and b, PMI(a, b)= log p(a,b)
p(a)p(b)

. The
probabilities are often replaced in DSMs by co-occurrence
counts of a and b and their respective frequency counts.

Table 1: Sorted list of the nearest neighbors to
“suit” in three different distributional models.

PMI GloVe Skipgram
suits suits suits
dress lawsuit lawsuit
jacket filed countersuit
wearing case classaction
hat wearing doublebreasted
trousers laiming skintight
costume lawsuits necktie
shirt alleging wetsuit
pants alleges crossbone
lawsuit classaction lawsuits

these neighborhoods; “suits” and “lawsuit” oc-
cur among the 10 nearest neighbors to “suit” in
all three models, whereas other terms are spe-
cific for one particular model. What is com-
mon between the three models is that they all
feature neighbors that represent two different us-
ages of “suit”: the law-sense (“lawsuit”) and
the clothes-sense (“dress”, “wearing”, “double-
breasted”).2 However, these distinction are not
discernible by merely looking at the list of near-
est neighbors; the only information it provides is
the ranking of the nearest neighbors in descending
order of similarity.

It has been argued that DSMs that represent
terms by a single vector cannot adequately handle
polysemy, since they conflate several different us-
age patterns in one and the same vector (Véronis,
2004; Erk and Padó, 2010). Examples like the one
above is often cited as evidence. We argue that
this critique is unfounded and misinformed, and
that it is the mode of querying the DSM that can
be susceptible to problems with polysemy. As the
above example demonstrates, querying DSMs by
k-NN conflates different usages of terms. The rea-
son for this seems quite obvious: simply ranking
the nearest neighbors by similarity (or distance)
ignores any local structures of the neighborhood.
If “suit” has as neighbors both “dress” and “law-
suit”, which represent two distinct types of usages
of “suit”, there will be a structural distinction in
the neighborhood of “suit” between these differ-
ent neighbors, since they will be mutually unre-
lated (i.e. there is a similarity between “suit” and

2The Skipgram model also features a manga-related sense
of “suit” in the neighbor “crossbone,” which refers to the
mange series “Mobile Suit Crossbone Gundam.”

2452



“dress” and between “suit” and “lawsuit”, but not
between “dress” and “lawsuit”).

k-NN also gives rise to another problem re-
lated to polysemy in DSMs. The problem is that
the most frequent senses will populate the top
of the nearest neighbor list, while the less fre-
quent senses will not appear until further down
the list, and if we set a too restrictive k, we will
only see neighbors relating to the most frequent
sense. As an example, consider the two differ-
ent senses of “suit” above. The distributional vec-
tor for “suit” can be thought of as a sum vsuit =
fsuit|lawvsuit|law + fsuit|clothesvsuit|clothes, where
vsuit|law is an idealized notion of the true dis-
tributional vector of “suit” in the law-sense, and
fsuit|law is the relative frequency of this sense.3

From there one can easily argue that a similar-
ity such as s(vsuit, vclothes) is actually a weighted
composite of the similarities s(vsuit|law, vclothes)
and s(vsuit|clothes, vclothes).4 If “suit” occurs pre-
dominantly in the law-sense in our corpus, the k-
NN neighborhood of “suit” will be dominated by
words pertaining to its law-sense, while the less
frequent senses might not be present at all. A
misguided k may thus obscure any other, less fre-
quent, senses of a term.

3 Word-Sense Induction

Selecting a relevant k for a given term and group-
ing the neighbors according to which senses they
represent is an example of Word-Sense Induction
(WSI). DSMs are well suited for this task, and
there have been a number of different approaches
suggested in the literature. One of the earliest ap-
proaches is distributional clustering (Pereira et al.,
1993), which is based on a probabilistic decompo-
sition model that uses maximum likelihood esti-
mation to fit the model to observed data. Another
example is Clustering By Committee (CBC) (Pan-
tel and Lin, 2002), which first uses average-link
clustering to recursively cluster the nearest neigh-
bors of a term into committees, which are then
used to define clusters by iteratively adding com-
mittees whose similarity to the term exceeds a cer-
tain threshold, and that is not too similar to any
other added committee. For each added commit-
tee, its features are also removed from the distri-

3Weighting schemes muddles this notion quite a bit, but
we think the general intuition still holds.

4In the case of cosine similarity this follows nicely from
the distributive property of dot products: v = av1 + bv2,
s(v, w) = v·w‖v‖‖w‖ =

a(v1·w)+b(v2·w)
‖v‖‖w‖

butional representation of the term. This last step
ensures that the clusters do not become too similar,
and that clusters representing less frequent senses
can be discovered.

The idea of iteratively removing features from
the distributional vector when a sense cluster as
been formed is also present in Dorow and Wid-
dows (2003), who use a graph-based clustering
method. Another graph-based approach is the
HyperLex algorithm (Véronis, 2004), which con-
structs a graph connecting all pairs of terms that
co-occur in the context of an ambiguous term. The
resulting graph contains highly connected compo-
nents, which represent the different senses of the
term. Agirre et al. (2006) compare HyperLex to
PageRank (Brin and Page, 1998) and demonstrates
that the two methods perform similarly.

There have also been several attempts to use
various types of matrix factorization for WSI. The
idea is that the factorization uncovers a set of
global senses in the form of the latent factors,
and that the sense distribution for a given term
can be described as a distribution over these la-
tent factors. Examples of factorization methods
that have been used include different versions of
Latent Dirichlet Allocation ((Brody and Lapata,
2009; Séaghdha and Korhonen, 2011; Yao and
Van Durme, 2011; Lau et al., 2012) and non-
negative matrix factorization (Dinu and Lapata,
2010; Van de Cruys and Apidianaki, 2011).

Tomuro et al. (2007) argue that clustering ap-
proaches like distributional clustering or CBC may
produce clusters that are themselves polysemous,
which may not be a desirable property of a WSI
algorithm, and suggests using feature domain sim-
ilarity to solve this problem. The idea is to incor-
porate similarities between the features of items
rather than the similarity between the items them-
selves in a modified version of CBC that enables
the algorithm to utilize feature similarities, which
inhibit the formation of polysemous clusters.

Koptjevskaja Tamm and Sahlgren (2014) also
leverage on the idea of using feature similarity
as the basis of sense clustering. The approach,
called syntagmatically labeled partitioning, relies
on a DSM that encodes sequential as well as sub-
stitutable relations. The method essentially sorts
the k nearest (substitutable) neighbors according
to which sequential connections they share. The
resulting partitioning of the nearest distributional
neighbors does not only constitute a WSI, but it

2453



also provides labels for the induced senses in the
form of the sequential connections the neighbors
share.

4 Neighborhood Graphs

Many of the previous WSI approaches operate at
a global level, utilizing global structural proper-
ties of the semantic spaces, e.g. by matrix fac-
torization techniques. We believe this is as ill-
advised as setting a global k or radius for the near-
est neighbor search, since it is the local structures
that are important when analyzing nearest neigh-
bors. Other WSI approaches use various forms
of clustering techniques. However, previous stud-
ies of the intrinsic dimensionality of distributional
semantic spaces using fractal dimensions indicate
that neighborhoods in semantic space have a fila-
mentary rather than clustered structure (Karlgren
et al., 2008).

We therefore propose the use of topological
models that take the local structure of neighbor-
hoods in semantic space into account. The ap-
proach discovers different word senses from the
local structure of neighborhoods, given nothing
but similarities between points. As such it is easy
to test on widely different vector models, as long
as there exists a well behaved similarity function.
The proposed approach not only answers the ques-
tion which other terms are similar to a given term,
but also how are they similar.

Relative neighborhoods, first proposed in (Tou-
ssaint, 1980), are examples of empty region graphs
(Cardinal et al., 2009), where points are neighbors
if some region between them is empty. For Rela-
tive Neighborhood Graphs (RNG) this region be-
tween two points a and c is defined as the inter-
section of the two spheres with centers in a and c
with radius d(a, c). In other words, a point b lies
between points a and c if it is closer to both a and
c than a and c are to each other. If no such point b
exists, a and c are neighbors. Illustrations of this
can be seen in Figure 1.

Figure 1: Example of when point b is between
point a and c (left), and when it is not (right).

Such neighborhoods have been argued to better
preserve local topology (Bremer et al., 2014), and
be more robust to deformations of the data than k-
NN neighborhoods (Correa and Lindstrom, 2012)
as they in some sense contain information about
direction whereas k-NN neighborhoods only con-
tain information about distance. Going back to the
“suit” example, we can see that if “suit” in the law-
sense is more similar to the composite “suit” than
to its clothes-sense, and vice versa, then the com-
posite vsuit lies between vsuit|law and vsuit|clothes.
This in turn means that out of those two points,
both are relative neighbors to “suit”, and neither
of them lies between the other and “suit”.

Formally, the set of points between two points
a, c ∈ V can be characterized and computed in the
following way:

btw(V, a, c) = {b|b ∈ V, b is between a and c}

rng-nbh(V, a) = {c|c ∈ V, btw(V, a, c) = ∅}
Erng(V ) = {(a, b)|a ∈ V, b ∈ rng-nbh(V, a)}

where Erng is the undirected edge set of the RNG.
The function btw(V, a, c) can be straightforwardly
translated to an algorithm taking O(|V |) time,
making the rng-nbh(V, a) function take O(|V |2)
time, which in turn makes the computation of the
complete graph take O(|V |3) time.5 Clearly un-
feasible, but we have not found any alternatives
that perform better in the high-dimensional case.6

Correa and Lindstrom (2012) note that the inter-
section of the RNG and the k-NN graph is a more
feasible alternative:

k-rng-nbh(V, a) = rng-nbh(V ′, a)

where V ′ = k nearest neighbors of a.

Given a precompiled k-NN lookup, the above
takes O(k2) time, so using a heap-based
O(|V | lg k) k-NN algorithm results in an algo-
rithm taking O(k2 + |V | lg k) time.

The same idea can be used to build a tree struc-
ture rooted in a reference word a in the following
way:

rnbh-tree(V, a) = {(c, arg min
b∈Bc

d(b, c))|c ∈ V }

where Bc = {a} ∪ btw(V, a, c)
5Assuming a constant time distance function.
6It should be noted that there are more efficient algorithms

for lower-dimensional situations.

2454



which can easily be restricted to the k-nearest
neighbors of a in much the same way as above,
with the same monotonic behavior.

Computing this for a point a produces a tree
where the direct children of a are its relative neigh-
bors, and the parent of a point c further down the
tree is the point between a and c that is closest
to c. This structure, while similar to a minimum
spanning tree, differs in some crucial regards: the
rnbh-tree(V, a) is rooted in a word a. The differ-
ence between rnbh-tree(V, a) and rnbh-tree(V, b)
is often quite significant. Furthermore, the re-
stricted k-rnbh-tree is monotonic in k. That prop-
erty does not hold for a minimum spanning tree of
a local neighborhood.

5 Examples of RNGs

To get an intuition of what these neighborhoods
look like we present a few examples. The words
have been chosen either because they are com-
mon examples in similar work — e.g. “heart” and
“suit” from Pantel and Lin (2002) — or because
they represent different parts-of-speech (“above”
is a preposition, “bad” is an adjective, and “ser-
vice” is a noun) and disparate kinds of ambiguity
(“orange” can be both a fruit and a color).

Figure 2 (next page) illustrates what an RNG
looks like for the term “heart” and its 100 near-
est neighbors in the PMI model. Note that the
root “heart” (at the mid-left in the graph) only
has two relative neighbors: “cardiac” and “soul,”
arguably representing a body-sense and a soul-
sense of the term. One advantage of using this
type of structure for the neighborhood is that
it enables us to examine various depths of the
tree. Depth one includes only the direct neigh-
bors (“cardiac” and “soul”), while depth two in-
cludes all neighbors two steps away in the graph:
“disease,” “coronary,” “pulmonary,” “cardiovascu-
lar,” “ventricular,” and “failure,” which are all chil-
dren to “cardiac.” This tree structure can be used
to identify neighbors that are themselves polyse-
mous (c.f. the critique mentioned in Section 3 of
clustering-based approaches to word-sense induc-
tion that they may produce polysemous clusters ).
One example is the neighbor “disease” at depth
two, which has six children that refer to different
aspects of disease.

We argue that the RNG can be quite useful
for WSI, since the branching structure indicates
different usages, and the depth factor enables us

to calibrate the granularity of the induced word
senses. If we only consider direct neighbors
(i.e. depth one), and set k = V (i.e. we do an
exhaustive nearest neighbor search), we will ex-
tract all terms that have a direct connection to the
reference term. We refer to this neighborhood as
the semantic horizon. At the most coarse level of
analysis, this is the neighborhood that represents
the main induced senses of a term. Tables 2 and
3 provide examples of 1,000-RNG neighborhoods
of depth one.

Table 2: RNG for k = 1, 000 of the words “suit,”
“orange,” and “heart” in three different semantic
models. The numbers in parenthesis indicate the
k-NN ranks of the neighbors.

PMI GloVe Skipgram
suit

suits (1) suits (1) suits (1)
dress (2) lawsuit (2) lawsuit (2)
lawsuit (10) mobile (33)
dinosaur (53) gundam (34)
costly (60) trump (55)
option (76) zoot (133)
counterparts (99) rebid (423)
predator (107) serenaders

(458)
trump (109) hev (987)
...

orange
yellow (1) yellow (1) redorange (1)
lemon (16) ktype (12)

lemon (14)
citrus (17)
jersey (21)
cherry (24)
county (26)
peel (42)
jumpsuits (57)
...

heart
cardiac (1) my (1) congestive (1)
soul (22) blood (2) hearts (2)
hearts (183) throbs (3)
ashtray(641) suffering (4)
rags(771) brain (6)

cardiac (8)
hearts (11)
throb (17)
lungs (22)
...

These examples demonstrate some interesting
similarities and differences between the three
models. First of all, there are some direct neigh-
bors that are present in all three models: “suit”
has “suits” and “lawsuit” as direct neighbors in
all three models, “heart” has “hearts,” “service”

2455



Figure 2: RNG for “heart” in the PMI model, restricted to the 100 nearest neighbors.

has “services,” and “above” has “below”. Plu-
ral forms are of course reasonable neighbors of
their singular counterparts in a semantic model,
but their usefulness for WSI can perhaps be ques-
tioned. Taking “suits” to indicate the clothes-sense
of “suit,” all three models produce both a clothes-
sense and a law-sense. For “orange,” the Skipgram
model only represents the color-sense, while the
PMI and GloVe models also feature a fruit-sense.
For “heart,” all three models have a disease-sense
(represented by the neighbors “cardiac” in the PMI
and GloVe models, and the neighbor “congestive”
in the Skipgram model), and an organ-sense (rep-
resented by the plural form “hearts”). “Service”
is a comparably vague term that has a number of
different senses in the PMI and GloVe models,
but only one in the Skipgram model. “Bad” pro-
duces both a negativity-sense and a German spa
town-sense in all three models, but only the GloVe
and Skipgram models have a separate antonym-
sense (“good” is not a direct neighbor in the PMI
model). “Above” has both the antonym and direct
neighbors relating to measurements in all three
models.

It is interesting to note that GloVe produces a

significant amount of sequential relations; “mo-
bile suit gundam”, “cheap suit serenaders”, “or-
ange peel”, and “orange jumpsuit” are just some
of many examples of sequential relations found in
the relative neighborhood of terms in the GloVe
model.

The PMI and GloVe models produce the struc-
turally most similar RNGs in these examples, with
on average a handful of direct neighbors, of which
some can be very distant. The Skipgram model
on the other hand produces very few direct neigh-
bors. This led us to look further into the struc-
tural properties of neighborhoods in the Skipgram
model. An interesting observation — and possi-
ble complication — is that the neighborhoods in
the Skipgram model are highly asymmetric: the
first neighbor of “information” is “informations”,
whereas “information” is the 1,829th neighbor of
“informations.” While such asymmetry occurs in
all models, it seems much more prevalent in the
Skipgram model. Figure 3 confirms this suspi-
cion: each point corresponds to a random word
pair (a, b) with x corresponding to where b is in
the ordered list of a’s neighbor, and y to where a
is in the ordered list of b’s neighbors. The figure

2456



●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

● ●

●

●

●

●

●

●

●

●

●

●●
●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●
●

●

●

●

●

●

●

●

●

●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●

●

●

●

●

●

● ●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

PMI GloVe skipgram

0

50000

100000

150000

0 50000 100000 150000 0 50000 100000 150000 0 50000 100000 150000

Figure 3: Neighborhood reciprocity in the different models; PMI to the left, GloVe in the middle, and
Skipgram to the right.

Table 3: k-RNG for k = 1, 000 of the words “ser-
vice,” “bad,” and “above” in three different seman-
tic models. The numbers in parenthesis indicate
the k-NN ranks of the neighbors.

PMI GloVe Skipgram
service

services (1) services (1) services (1)
network (2) operated (3)
operates (8) serving (6)
launched (18) military (17)
served (22) duty (20)
intercity(34) passenger (21)

dialaride (644)
aftersales (759)
limitedstop
(802)

bad
terrible (1) good (1) nauheim (1)
that (2) kissingen (2) good (2)
luck (39) ugly (45) dreadful (5)
unfortunate (70) nasty (48)
stalling (276) dirty (106)
donnersbergkreis omen (328)
(860)
rancid (980) conkers (360)

karma (952)
above

below (1) below (1) below (1)
around (2) level (2) 500ft (2)
feet (5) height (3)
measuring (29) just (4)
beneath (36) stands (10)
columns (62) lower (11)
atop (102) beneath (12)

rise (21)
sea (30)
...

shows that the local densities vary much more in
the Skipgram model than in the others. This is not
in itself undesirable, but wild differences in neigh-
borhood reciprocity complicates the choice of k in
the k-RNG algorithm, as observed by the particu-
larly sparse neighborhoods of the Skipgram model
above.

6 WSI Evaluation

The standard way to evaluate WSI algorithms is to
use one the SemEval WSI test collections (Agirre
and Soroa, 2007; Manandhar et al., 2010; Nav-
igli and Vannella, 2013; Jurgens and Klapaftis,
2013), which are all designed similarly: systems
are expected to first perform WSI and then to as-
sign texts to the induced senses (i.e. in effect do-
ing a word-sense disambiguation step). We con-
sider this type of evaluation to be a less useful
for our purposes, since the required disambigua-
tion step is a highly non-trivial task in itself. The
RNG method proposed in this paper is a pure
WSI algorithm, and as such does not offer a solu-
tion to the disambiguation problem. We therefore
opted to focus solely on the hypothesis that rel-
ative neighborhoods cover senses that k-NNs do
not. In essence, we investigate whether k-RNG
retrieval does a better job at covering different
senses than k-NN retrieval. This was done using
pseudowords.

Pseudowords are artificially ambiguous words,
created by regarding different words as identi-
cal. We can, for example, say that the pseu-

2457



PMI GloVe Skipgram

●

●

● ● ●

●

●

●

●
●

●

●
●

●

●

●
●

●

●

●
● ●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●●

●
●

●
●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●
●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●
●

●
●

●
●

●
●

●
●●

●

●● ●
●

●

●

●●
●

●

●

●

●

●

●
●

●

●

●●

●

●

●

● ●●

●
●●

●●

●

●

●

●

●●●

●

●
●●

●

●

●

●

●

●

●

● ●●
●●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●●

●

●●

●

●

●

●

●
●● ●

●

●

●●

●
●●

●

●

●

●

●

●

●

●
●

●

●

●
●

●●

●

●

●

●

●

●

●

●
●

●

●●

●
●

●

●

●

●
●

●

●●●

●
●

●

●

●

●

●

●●
●●

●

●

●

●

●

●
●●

●
●●

● ●
●

●

●

●

●

●
●

●

●
● ●

●
●

●●

●
●

●

●●
●

●

●●

●

●
●

●
●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●●●

●
●●

●

●

●

●

●

●

●●
●

●

●

●
●

●

●

●

●

●●

●

● ●

●

●

●

●

●
●●

●

●

● ●

●

●●

●
●

●

●

●●
●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

● ●
●

●

●●

●

●

●

●

●● ●
●

●

●

●

●
●

●

●
●

●

●

●
●

●
●●

●

●

●

●

●

●

●

●
●

● ●

●

●
●

●

●

●

●

●

●
●

●
●

●

●

●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●

● ●
●

●●●●● ●

●

●

●

●
●●

●●
●

●

●

●
●

●
●

●

●
●

●

●

●

●●●●

●

●
●

●

● ●●●

●●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●●

●

●

●

● ●

● ●

●
●

●

●

●

●●

●

●

●
●

●

●

●●

●
●

●

●●
●

●
●

●

●

●

● ●●

●

●

●

●

●

●

●

●

●●
●

●

●

●
●

●

●

●

●●

●

●

●

●

●
●

●

●

●

●

●

●

●

●
●

●

●

●

●
●

●

●●
●

●●

●

●

●

●

●

●

●

●
●

●

●

●

●●

●

●

●
●

●
●●

●

●
●

●

●

●

●

●

●
●

● ●

●

●

●
●

●●

●

● ●
●

●

●

●

●

● ●
●

●

●

●

●
●

●

●

●

●

● ●

●

●

●

●

●●

●

●

●●

●

●

●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

● ●

●

●●
●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●●

●●●

●

●

●

●

●

●

●

●

●

●
●

●

● ●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●
●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●
●

●

●

●

●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●●●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

● ●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

● ●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

● ●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

● ●

●

●●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●

●

●

●

●

●

●
●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

0.00

0.25

0.50

0.75

0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75
KNN10

R
N

G
10

00

Figure 4: Comparison of minmax pseudosense score for k-RNGs and k-NNs for k = 1, 000 and k = 10
respectively; PMI to the left, GloVe in the middle, and Skipgram to the right.

doword <deadeye> is a composite of the two
words marksman and loudspeaker. A corpus with
the artificially ambiguous word <deadeye> in it
can then be created by replacing all occurrences
of the words marksman and loudspeaker with
<deadeye>.

Using the pseudowords provided by Pilehvar
and Navigli (2013) a corpus with 689 non-
overlapping pseudowords was created, based on
the BNC corpus.7 Two models were then trained,
one on the altered corpus, and one on the unal-
tered one. To check whether the neighborhood of a
pseudoword contains information about its under-
lying senses we compared each underlying sense
to the words in the neighborhood, taking the mini-
mum of all senses’ maximum similarity as a score,
as demonstrated in Table 4. The similarities were
calculated using the model trained on the unaltered
corpus, as the one based on the altered corpus will
not contain the underlying senses of pseudowords.

Working through the example in Table 4, the
neighborhood of the pseudoword <deadeye >
consists of the three words shooter, stereo, and
sport. The pseudoword in itself is made up of
the two underlying senses marksman and loud-
speaker. The similarities between the words in
the neighborhood of the model trained on the unal-

7www.natcorp.ox.ac.uk

tered data and the words of the underlying senses
are as presented in Table 4. The closest word to
marksman is shooter, with a similarity score of
0.7. The closest word to loudspeaker is stereo,
with a score of 0.3. So the scoring would, in total,
be 0.3. It should be noted that the upper bound for
this score is oftentimes significantly lower than 1:
The neighborhood could not possibly contain the
words marksman or loudspeaker, as those words
are not present in the corpus. This means that the
scores are bounded by the similarity of the least
similar closest neighbor to the underlying senses.

Table 4: Example scoring of a neighborhood of
the word <deadeye>.

<deadeye> shooter stereo sport max
marksman 0.7 0.04 0.4 0.7
loudspeaker 0.01 0.3 0.05 0.3

min: 0.3

This score was chosen because of its simplic-
ity and intuitive interpretation: a low score im-
plies that at least one word sense was not repre-
sented in the neighborhood whereas a high score
means that all senses are represented in the neigh-
borhood. One can then plot these scores for both
relative neighborhoods and k-NN neighborhoods
for each pseudoword as is done in Figure 4. Each

2458



point (x, y) represents a pseudoword, with x and
y being the score of the k-NN neighborhood and
the k-RNG neighborhood respectively.

Figure 5 shows an aggregate of Figure 4, plot-
ting the distribution of y−x, i.e. the difference be-
tween the scores achieved by the k-RNG and the
k-NN. As seen in Figure 4, a lot of points lie on
the line y = x, meaning both methods achieved
the same score. However, when this is not the
case, there is a clear bias for the k-RNG to out-
perform the k-NN, as demonstrated in Figure 5.
Here, using the BNC instead of Wikipedia as train-
ing data, the GloVe and Skipgram models yielded
sparse relative neighborhoods — both with an av-
erage of about 8 neighbors — but the PMI model
produced quite dense neighborhoods averaging 63
neighbors. Since the scoring function does not pe-
nalize neighborhood size there is good reason to
be skeptical of its viability, and specifically the
performance of the PMI-model based on these fig-
ures.

●

●

●
●

●

●
●
●
●
●●●

●

●●

●●

●
●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●●
●
●

●

●

●

●

●

●

●●

●●

●
●
●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●
●

●●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●
●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

−0.25

0.00

0.25

0.50

0.75

PMI GloVe Skipgram
id

R
N

G
10

00
 −

 K
N

N
10

id

PMI

GloVe

Skipgram

Figure 5: Distribution of difference between
scores for k-RNGs and k-NNs. Positive scores
means that the k-RNG scored higher than the k-
NN

7 Conclusions

This paper has discussed the question how to
query semantic models, which is a question that
has been long neglected in research on computa-
tional semantics. Nearest neighbor search (or k-
NN) is often treated as the only available option,
which leads to misunderstandings regarding how

semantic models represent and handle vagueness
and polysemy. We have argued that the structure
— or topology — of the local neighborhoods in se-
mantic models carry useful semantic information
regarding the different usages — or senses — of
a term, and that such topological properties there-
fore can be used to analyze polysemy and do WSI.

We have introduced relative neighborhood
graphs (RNG) as an alternative to standard k-NN,
and we have exemplified k-RNG in three differ-
ent well-known semantic models. The examples
demonstrate that k-RNG manages to retrieve dis-
parate and relevant neighbors in all three models,
yet the kind of neighbors returned and the nature
of the neighborhoods differ. Quantitatively, The k-
RNG method consistently outperformed k-NN on
underlying sense retrieval.

We have also illustrated how k-RNG can be
used as a tool to gain insight into the topological
properties of different models. The GloVe model,
for example, makes no difference between sequen-
tial and substitutable relations, leading to neigh-
borhoods that contain n-grams instead of senses.
This can clearly be seen in for example Table 2.
Skipgram uses more sophisticated tokenization,
which alleviates this issue.

Another interesting result of the paper is that the
RNG uncovers otherwise unseen differences be-
tween the models, which manifest not as scoring
differences but as properties of the word represen-
tations themselves. One example is the differences
in neighborhood reciprocity observed between the
different models.

References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007

task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of SemEval,
pages 7–12.

Eneko Agirre, David Martı́nez, Oier López de Lacalle,
and Aitor Soroa. 2006. Two graph-based algo-
rithms for state-of-the-art WSD. In Proceedings of
EMNLP, pages 585–593.

Sunil Arya, David M. Mount, Nathan S. Netanyahu,
Ruth Silverman, and Angela Y. Wu. 1998. An
optimal algorithm for approximate nearest neighbor
searching fixed dimensions. Journal of the ACM,
45(6):891–923.

Jon Louis Bentley. 1975. Multidimensional binary
search trees used for associative searching. Com-
munications of the ACM, 18(9):509–517.

2459



Peer-Timo Bremer, Ingrid Hotz, Valerio Pascucci, and
Ronald Peikert. 2014. Topological Methods in Data
Analysis and Visualization III. Springer.

Sergey Brin and Larry Page. 1998. The anatomy of a
large-scale hypertextual web search engine. In Pro-
ceedings of WWW, pages 107–117.

Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of EACL,
pages 103–111.

Jean Cardinal, Sébastien Collette, and Stefan Langer-
man. 2009. Empty region graphs. Computational
geometry, 42(3):183–195.

Carlos D Correa and Peter Lindstrom. 2012.
Locally-scaled spectral clustering using empty re-
gion graphs. In Proceedings of KDD, pages 1330–
1338.

Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP, pages 1162–1172.

Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpus-specific word senses. In Proceedings of
EACL, pages 79–82.

Katrin Erk and Sebastian Padó. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL, pages 92–97.

Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC, pages 604–
613.

David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of SemEval,
pages 290–299.

Jussi Karlgren, Anders Holst, and Magnus Sahlgren.
2008. Filaments of meaning in word space. In Pro-
ceedings of ECIR, pages 531–538.

Maria Koptjevskaja Tamm and Magnus Sahlgren.
2014. Temperature in word space. In Benedikt
Szmrecsanyi and Bernhard Wälchli, editors, Aggre-
gating dialectology, typology, and register analysis,
pages 231–267. De Gruyter.

Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings of
EACL, pages 591–601.

Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of SemEval, pages 63–68.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.

Roberto Navigli and Daniele Vannella. 2013.
Semeval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of SemEval, pages 193–201.

Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of KDD,
pages 613–619.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors
for word representation. In Proceedings of EMNLP,
pages 1532–1543.

Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of ACL, pages 183–190.

Mohammad Taher Pilehvar and Roberto Navigli. 2013.
Paving the way to a large-scale pseudosense-
annotated dataset. In Proceedings of NAACL-HLT,
pages 1100–1109.

Diarmuid Ó Séaghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of EMNLP, pages 1047–1057.

Noriko Tomuro, Steven L. Lytinen, Kyoko Kanzaki,
and Hitoshi Isahara. 2007. Clustering using fea-
ture domain similarity to discover word senses for
adjectives. In Proceedings of ICSC, pages 370–377.

Godfried T. Toussaint. 1980. The relative neighbour-
hood graph of a finite planar set. Pattern Recogni-
tion, 12(4):261 – 268.

Tim Van de Cruys and Marianna Apidianaki. 2011.
Latent semantic word sense induction and disam-
biguation. In Proceedings of HLT, pages 1476–
1485.

Jean Véronis. 2004. HyperLex: lexical cartography
for information retrieval. Computer Speech & Lan-
guage, 18(3):223–252.

Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs, pages 10–14.

2460


