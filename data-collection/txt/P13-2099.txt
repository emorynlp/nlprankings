



















































Evolutionary Hierarchical Dirichlet Process for Timeline Summarization


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 556–560,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Evolutionary Hierarchical Dirichlet Process for Timeline Summarization

Jiwei Li
School of Computer Science

Cornell University
Ithaca, NY, 14853

jl3226@cornell.edu

Sujian Li
Laboratory of Computational Linguistics

Peking University
Bejing, P.R.China, 150001
lisujian@pku.edu.cn

Abstract
Timeline summarization aims at generat-
ing concise summaries and giving read-
ers a faster and better access to under-
stand the evolution of news. It is a new
challenge which combines salience rank-
ing problem with novelty detection. Pre-
vious researches in this field seldom ex-
plore the evolutionary pattern of topics
such as birth, splitting, merging, develop-
ing and death. In this paper, we develop
a novel model called Evolutionary Hier-
archical Dirichlet Process(EHDP) to cap-
ture the topic evolution pattern in time-
line summarization. In EHDP, time vary-
ing information is formulated as a series
of HDPs by considering time-dependent
information. Experiments on 6 different
datasets which contain 3156 documents
demonstrates the good performance of our
system with regard to ROUGE scores.

1 Introduction

Faced with thousands of news articles, people usu-
ally try to ask the general aspects such as the
beginning, the evolutionary pattern and the end.
General search engines simply return the top rank-
ing articles according to query relevance and fail
to trace how a specific event goes. Timeline sum-
marization, which aims at generating a series of
concise summaries for news collection published
at different epochs can give readers a faster and
better access to understand the evolution of news.

The key of timeline summarization is how to
select sentences which can tell readers the evolu-
tionary pattern of topics in the event. It is very
common that the themes of a corpus evolve over
time, and topics of adjacent epochs usually exhibit
strong correlations. Thus, it is important to model
topics across different documents and over differ-
ent time periods to detect how the events evolve.

The task of timelime summarization is firstly
proposed by Allan et al.(2001) by extracting clus-
ters of noun phases and name entities. Chieu et
al.(2004) built a similar system in unit of sentences
with interest and burstiness. However, these meth-
ods seldom explored the evolutionary character-
istics of news. Recently, Yan et al.(2011) ex-
tended the graph based sentence ranking algorithm
used in traditional multi-document summarization
(MDS) to timeline generation by projecting sen-
tences from different time into one plane. They
further explored the timeline task from the opti-
mization of a function considering the combina-
tion of different respects such as relevance, cover-
age, coherence and diversity (Yan et al., 2011b).
However, their approaches just treat timeline gen-
eration as a sentence ranking or optimization prob-
lem and seldom explore the topic information lied
in the corpus.

Recently, topic models have been widely used
for capturing the dynamics of topics via time.
Many dynamic approaches based on LDA model
(Blei et al., 2003) or Hierarchical Dirichelt Pro-
cesses(HDP) (Teh et al., 2006) have been pro-
posed to discover the evolving patterns in the cor-
pus as well as the snapshot clusters at each time
epoch (Blei and Lafferty, 2006; Chakrabarti et al.,
2006; Wang and McCallum, 2007; Caron et al.,
2007; Ren et al., 2008; Ahmed and Xing, 2008;
Zhang et al., 2010).

In this paper, we propose EHDP: a evolution-
ary hierarchical Dirichlet process (HDP) model
for timeline summarization. In EHDP, each HDP
is built for multiple corpora at each time epoch,
and the time dependencies are incorporated into
epochs under the Markovian assumptions. Topic
popularity and topic-word distribution can be in-
ferred from a Chinese Restaurant Process (CRP).
Sentences are selected into timelines by consider-
ing different aspects such as topic relevance, cov-
erage and coherence. We built the evaluation sys-

556



tems which contain 6 real datasets and perfor-
mance of different models is evaluated accord-
ing to the ROUGE metrics. Experimental results
demonstrate the effectiveness of our model .

2 EHDP for Timeline Summarization

2.1 Problem Formulation
Given a general query Q = {wqi}i=Qni=1 , we firstly
obtain a set of query related documents. We no-
tate different corpus as C = {Ct}t=Tt=1 according
to their published time where Ct = {Dti}i=Nti=1 de-
notes the document collection published at epoch
t. Document Dti is formulated as a collection of
sentences {stij}j=Ntij=1 . Each sentence is presented
with a series of words stij = {wtijl}

l=Ntij
l=1 and as-

sociated with a topic θtij . V denotes the vocabu-
lary size. The output of the algorithm is a series
of timelines summarization I = {It}t=Tt=1 where
It ⊂ Ct

2.2 EHDP
Our EHDP model is illustrated in Figure 2. Specif-
ically, each corpus Ct is modeled as a HDP. These
HDP shares an identical base measure G0, which
serves as an overall bookkeeping of overall mea-
sures. We use Gt0 to denote the base measure at
each epoch and draw the local measureGti for each
document at time t from Gt0. In EHDP, each sen-
tence is assigned to an aspect θtij with the consid-
eration of words within current sentence.

To consider time dependency information in
EHDP, we link all time specific base measures Gt0
with a temporal Dirichlet mixture model as fol-
lows:

Gt0 ∼ DP (γt,
1

K
G0+

1

K

∆∑

δ=0

F (v, δ)·Gt−δ0 ) (1)

where F (v, δ) = exp(−δ/v) denotes the expo-
nential kernel function that controls the influence
of neighboring corpus. K denotes the normaliza-
tion factor where K = 1 +

∑∆
δ=0 F (v, δ). ∆ is

the time width and λ is the decay factor. In Chi-
nese Restaurant Process (CRP), each document is
referred to a restaurant and sentences are com-
pared to customers. Customers in the restaurant
sit around different tables and each table btin is as-
sociated with a dish (topic) Ψtin according to the
dish menu. Let mtk denote the number of ta-
bles enjoying dish k in all restaurants at epoch t,
mtk =

∑Nt
i=1

∑Ntib
n=1 1(Ψ

t
in = k). We redefine

for each epoch t ∈ [1, T ]
1. draw global measure
Gt0 ∼ DP (α, 1KG0 + 1K

∑∆
δ=0 F (v, δ)G

t−δ
0 )

2. for each document Dti at epoch t,
2.1 draw local measure Gti ∼ DP (γ,Gt0)
2.2 for each sentence stij in D

t
i

draw aspect θtij ∼ Gti
for w ∈ stij draw w ∼ f(w)|θtij

Figure 1: Generation process for EHDP

another parameter Mtk to incorporate time depen-
dency into EHDP.

Mtk =
∆∑

δ=0

F (v, δ) ·mt−δ,k (2)

Let ntib denote the number of sentences sitting
around table b, in document i at epoch t. In CRP
for EHDP, when a new customer stij comes in,
he can sit on the existing table with probability
ntib/(n

t
i−1+γ), sharing the dish (topic) Ψtib served

at that table or picking a new table with probabil-
ity γ/(nti − 1 + γ). The customer has to select
a dish from the global dish menu if he chooses a
new table. A dish that has already been shared in
the global menu would be chosen with probability
M tk/(

∑
kM

t
k+α) and a new dish with probability

α/(
∑

kM
t
k + α).

θtij |θti1, ..., θtij−1, α ∼
∑

φtb=θij

ntib
nti − 1 + γ

δφjb +
γ

nti − 1 + γ
δφnewjb

φnewti |φ, α ∼
∑

k

Mtk∑
iMti + α

δφk +
α∑

iMti + α
G0

(3)

We can see that EHDP degenerates into a series of
independent HDPs when ∆ = 0 and one global
HDP when ∆ = T and v = ∞, as discussed in
Amred and Xings work (2008).

2.3 Sentence Selection Strategy
The task of timeline summarization aims to pro-
duce a summary for each time and the generated
summary should meet criteria such as relevance ,
coverage and coherence (Li et al., 2009). To care
for these three criteria, we propose a topic scoring
algorithm based on Kullback-Leibler(KL) diver-
gence. We introduce the decreasing logistic func-
tion ζ(x) = 1/(1 + ex) to map the distance into
interval (0,1).

557



Figure 2: Graphical model of EHDP.

Relevance: the summary should be related with
the proposed query Q.

FR(I
t) = ζ(KL(It||Q))

Coverage: the summary should highly generalize
important topics mentioned in document collec-
tion at epoch t.

FCv(I
t) = ζ(KL(It||Ct))

Coherence: News evolves over time and a good
component summary is coherent with neighboring
corpus so that a timeline tracks the gradual evolu-
tion trajectory for multiple correlative news.

FCh(I
t) =

∑δ=∆/2
δ=−∆/2 F (v, δ) · ζ(KL(It||Ct−δ))

∑δ=∆/2
δ=−∆/2 F (v, δ)

Let Score(It) denote the score of the summary
and it is calculated in Equ.(4).

Score(It) = λ1FR(I
t)+λ2FCv(I

t)+λ3FCh(I
t)

(4)∑
i λi = 1. Sentences with higher score are se-

lected into timeline. To avoid aspect redundancy,
MMR strategy (Goldstein et al., 1999) is adopted
in the process of sentence selection.

3 Experiments

3.1 Experiments set-up
We downloaded 3156 news articles from selected
sources such as BBC, New York Times and CNN
with various time spans and built the evaluation
systems which contains 6 real datasets. The news
belongs to different categories of Rule of Interpre-
tation (ROI) (Kumaran and Allan, 2004). Detailed
statistics are shown in Table 1. Dataset 2(Deep-
water Horizon oil spill), 3(Haiti Earthquake) and
5(Hurricane Sandy) are used as training data and

New Source Nation News Source Nation
BBC UK New York Times US
Guardian UK Washington Post US
CNN US Fox News US
ABC US MSNBC US

Table 1: New sources of datasets
News Subjects (Query) #docs #epoch
1.Michael Jackson Death 744 162
2.Deepwater Horizon oil spill 642 127
3.Haiti Earthquake 247 83
4.American Presidential Election 1246 286
5.Hurricane Sandy 317 58
6.Jerry Sandusky Sexual Abuse 320 74

Table 2: Detailed information for datasets

the rest are used as test data. Summary at each
epoch is truncated to the same length of 50 words.

Summaries produced by baseline systems and
ours are automatically evaluated through ROUGE
evaluation metrics (Lin and Hovy, 2003). For
the space limit, we only report three ROUGE
ROUGE-2-F and ROUGE-W-F score. Reference
timeline in ROUGE evaluation is manually gener-
ated by using Amazon Mechanical Turk1. Work-
ers were asked to generate reference timeline for
news at each epoch in less than 50 words and we
collect 790 timelines in total.

3.2 Parameter Tuning
To tune the parameters λ(i = 1, 2, 3) and v in our
system, we adopt a gradient search strategy. We
firstly fix λi to 1/3. Then we perform experiments
on with setting different values of v/#epoch in
the range from 0.02 to 0.2 at the interval of 0.02.
We find that the Rouge score reaches its peak at
round 0.1 and drops afterwards in the experiments.
Next, we set the value of v is set to 0.1 · #epoch
and gradually change the value of λ1 from 0 to 1
with interval of 0.05, with simultaneously fixing
λ2 and λ3 to the same value of (1 − λ1)/2. The
performance gets better as λ1 increases from 0 to
0.25 and then declines. Then we set the value of
λ1 to 0.25 and change the value of λ2 from 0 to
0.75 with interval of 0.05. And the value of λ2 is
set to 0.4, and λ3 is set to 0.35 correspondingly.

3.3 Comparison with other topic models
In this subsection, we compare our model with
4 topic model baselines on the test data. Stand-
HDP(1): A topic approach that models different
time epochs as a series of independent HDPs with-
out considering time dependency. Stand-HDP(2):

1http://mturk.com

558



M.J. Death US Election S. Sexual Abuse
System R2 RW R2 RW R2 RW
EHDP 0.089 0.130 0.081 0.154 0.086 0.152
Stand-HDP(1) 0.080 0.127 0.075 0.134 0.072 0.138
Stand-HDP(2) 0.077 0.124 0.072 0.127 0.071 0.131
Dyn-LDA 0.080 0.129 0.073 0.130 0.077 0.134
Stan-LDA 0.072 0.117 0.065 0.122 0.071 0.121

Table 3: Comparison with topic models

M.J. Death US Election S. Sexual Abuse
System R2 RW R2 RW R2 RW
EHDP 0.089 0.130 0.081 0.154 0.086 0.152
Centroid 0.057 0.101 0.054 0.098 0.060 0.132
Manifold 0.053 0.108 0.060 0.111 0.069 0.128
ETS 0.078 0.120 0.073 0.130 0.075 0.135
Chieu 0.064 0.107 0.064 0.122 0.071 0.131

Table 4: Comparison with other baselines

A global HDP which models the whole time span
as a restaurant. The third baseline, Dynamic-
LDA is based on Blei and Laffery(2007)‘s work
and Stan-LDA is based on standard LDA model.
In LDA based models, aspect number is prede-
fined as 80 2. Experimental results of different
models are shown in Table 2. As we can see,
EHDP achieves better results than the two stan-
dard HDP baselines where time information is not
adequately considered. We also find an interesting
result that Stan-HDP performs better than Stan-
LDA. This is partly because new aspects can be
automatically detected in HDP. As we know, how
to determine topic number in the LDA-based mod-
els is still an open problem.

3.4 Comparison with other baselines
We implement several baselines used in tradi-
tional summarization or timeline summarization
for comparison. (1) Centroid applies the MEAD
algorithm (Radev et al., 2004) according to the
features including centroid value, position and
first-sentence overlap. (2) Manifold is a graph
based unsupervised method for summarization,
and the score of each sentence is got from the
propagation through the graph (Wan et al., 2007).
(3) ETS is the timeline summarization approach
developed by Yan et al., (2011a), which is a graph
based approach with optimized global and local
biased summarization. (4) Chieu is the time-
line system provided by (Chieu and Lee, 2004)
utilizing interest and bursty ranking but neglect-
ing trans-temporal news evolution. As we can
see from Table 3, Centroid and Manifold get
the worst results. This is probably because meth-
ods in multi-document summarization only care

2In our experiments, the aspect number is set as 50, 80,
100 and 120 respectively and we select the best performed
result with the aspect number as 80

about sentence selection and neglect the novelty
detection task. We can also see that EHDP under
our proposed framework outputs existing timeline
summarization approaches ETS and chieu. Our
approach outputs Yan et al.,(2011a)s model by
6.9% and 9.3% respectively with regard to the av-
erage score of ROUGE-2-F and ROUGE-W-F.

4 Conclusion

In this paper we present an evolutionary HDP
model for timeline summarization. Our EHDP ex-
tends original HDP by incorporating time depen-
dencies and background information. We also de-
velop an effective sentence selection strategy for
candidate in the summaries. Experimental results
on real multi-time news demonstrate the effective-
ness of our topic model.

Oct. 3, 2012
S1: The first debate between President Obama and Mitt Rom-
ney, so long anticipated, quickly sunk into an unenlightening
recitation of tired talking points and mendacity. S2. Mr. Rom-
ney wants to restore the Bush-era tax cut that expires at the end
of this year and largely benefits the wealthy
Oct. 11, 2012
S1: The vice presidential debate took place on Thursday, Oc-
tober 11 at Kentucky’sCentre College, and was moderated by
Martha Raddatz. S2: The first and only debate between Vice
President Joe Biden and Congressman Paul Ryan focused on
domestic and foreign policy. The domestic policy segments in-
cluded questions on health care, abortion
Oct. 16, 2012
S1. President Obama fights back in his second debate with Mitt
Romney, banishing some of the doubts he raised in their first
showdown. S2: The second debate dealt primarily with domes-
tic affairs and include some segues into foreign policy. includ-
ing taxes, unemployment, job creation, the national debt, energy
and women’s rights, both legal and

Table 5: Selected timeline summarization gener-
ated by EHDP for American Presidential Election

5 Acknowledgement

This research has been supported by NSFC grants
(No.61273278), National Key Technology RD
Program (No:2011BAH1B0403), National 863
Program (No.2012AA011101) and National So-
cial Science Foundation (No.12ZD227).

References

Amr Ahmed and Eric Xing. Dynamic non-parametric
mixture models and the recurrent chinese restaurant
process. 2008. In SDM.

559



James Allan, Rahul Gupta and Vikas Khandelwal.
Temporal summaries of new topics. 2001. In Pro-
ceedings of the 24th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval

David Blei, Andrew Ng and Micheal Jordan. 2003.
Latent dirichlet allocation. In Journal of Machine
Learning Research.

David Blei and John Lafferty. Dynamic topic models.
2006. In Proceedings of the 23rd international con-
ference on Machine learning.

Francois Carol, Manuel Davy and Arnaud Doucet.
Generalized poly urn for time-varying dirichlet pro-
cess mixtures. 2007. In Proceedings of the Interna-
tional Conference on Uncertainty in Artificial Intel-
ligence.

Deepayan Chakrabarti, Ravi Kumar and Andrew
Tomkins. Evolutionary Clustering. InProceedings of
the 12th ACM SIGKDD international conference
Knowledge discoveryand data mining.

Hai-Leong Chieu and Yoong-Keok Lee. Query based
event extraction along a timeline. In Proceedings of
the 27th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval

Giridhar Kumaran and James Allan. 2004. Text classifi-
cation and named entities for new event detection. In
Proceedings of the 27th annual international ACM
SIGIR04.

Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha
and Yong Yu. Enhancing diversity, coverage and bal-
ance for summarization through structure learning.
In Proceedings of the 18th international conference
on World wide web.

Chin-Yew Lin and Eduard Hovy. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the Human Language Technology
Conference of the NAACL. 2003.

Dragomar Radev, Hongyan. Jing, and Malgorzata Stys.
2004. Centroid-based summarization of multiple
documents. In Information Processing and Manage-
ment.

Lu Ren, David Dunson and Lawrence Carin. The dy-
namic hierarchical Dirichlet process. 2008. In Pro-
ceedings of the 25th international conference on
Machine Learning.

Xiaojun Wan, Jianwu Yang and Jianguo Xiao.
2007. Manifold-ranking based topic-focused multi-
document summarization. In Proceedings of Inter-
national Joint Conference on Artificial Intelligence.

Xuerui Wang and Andrew MaCallum. Topics over
time: a non-Markov continuous-time model of topi-
cal trends. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining.

Yee Whye Teh, Michael Jordan, Matthew Beal and
David Blei. Hierarchical Dirichlet Processes. In
American Statistical Association.

Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li and Yan Zhang. 2011a. Evolutionary
Timeline Summarization: a Balanced Optimization
Framework via Iterative Substitution. In Proceed-
ings of the 34th international ACM SIGIR confer-
ence on Research and development in Information
Retrieval.

Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Jahna Otterbacher, Xiaoming Li and Yan Zhang.
Timeline Generation Evolutionary Trans-Temporal
Summarization. 2011b. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.

Jianwen Zhang, Yangqiu Song, Changshui Zhang and
Shixia Liu. 2010. Evolutionary Hierarchical Dirich-
let Processes for Multiple Correlated Time-varying
Corpora. In Proceedings of the 16th ACM SIGKDD
international conference on Knowledge discovery
and data mining.

560


