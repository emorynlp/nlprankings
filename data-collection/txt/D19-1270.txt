



















































Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2682–2691,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2682

Modeling Event Background for If-Then Commonsense Reasoning Using
Context-aware Variational Autoencoder

Li Du, Xiao Ding, Ting Liu∗ and Zhongyang Li
Research Center for Social Computing and Information Retrieval

Harbin Institute of Technology, China
{ldu, xding, tliu, zyli}@ir.hit.edu.cn

Abstract
Understanding event and event-centered com-
monsense reasoning are crucial for natural lan-
guage processing (NLP). Given an observed
event, it is trivial for human to infer its in-
tents and effects, while this type of If-Then
reasoning still remains challenging for NLP
systems. To facilitate this, a If-Then common-
sense reasoning dataset Atomic is proposed,
together with an RNN-based Seq2Seq model
to conduct such reasoning. However, two fun-
damental problems still need to be addressed:
first, the intents of an event may be multiple,
while the generations of RNN-based Seq2Seq
models are always semantically close; sec-
ond, external knowledge of the event back-
ground may be necessary for understanding
events and conducting the If-Then reasoning.
To address these issues, we propose a novel
context-aware variational autoencoder effec-
tively learning event background information
to guide the If-Then reasoning. Experimental
results show that our approach improves the
accuracy and diversity of inferences compared
with state-of-the-art baseline methods.

1 Introduction

Recently, event-centered commonsense knowl-
edge has attracted much attention (Chambers and
Jurafsky, 2008; Segers et al., 2016; Wang et al.,
2017; Li et al., 2018), because of understanding
events is an important component of NLP. Given
a daily-life event, human can easily understand it
and reason about its causes, effects, and so on.
However, it still remains a challenging task for
NLP systems. This is partly due to most of them
are trained for task-specific datasets or objectives,
which results in models that are adapt at finding
task-specific underlying correlation patterns but
have limited capability in simple and explainable
commonsense reasoning (Sap et al., 2018).

∗Corresponding author

Figure 1: A illustration of two challenging problems in If-
Then reasoning. (a) Given an observed event, the feelings
about this event could be multiple. (b) Background knowl-
edge is need for generating reasonable inferences, which is
absent in the dataset (marked by dashed lines).

To facilitate this, Rashkin et al. (2018) build the
Event2Mind dataset and Sap et al. (2018) present
the Atomic dataset, mainly focus on nine If-Then
reasoning types to describe causes, effects, intents
and participant characteristic about events. To-
gether with these datasets, a simple RNN-based
encoder-decoder framework is proposed to con-
duct the If-Then reasoning.

However, there still remains two challenging
problems. First, as illustrated in Figure 1, given
an event “PersonX finds a new job”, the plausi-
ble feeling of PersonX about that event could be
multiple (such as “needy/stressed out” and “re-
lieved/joyful”). Previous work showed that for the
one-to-many problem, conventional RNN-based
encoder-decoder models tend to generate generic
responses, rather than meaningful and specific an-
swers (Li et al., 2016; Serban et al., 2016).

Second, as a commonsense reasoning problem,
rich background knowledge is necessary for gen-
erating reasonable inferences. For example, as
shown in Figure 1, the feeling of PersonX upon the
event “PersonX finds a new job” could be multi-
ple. However, after given a context “PersonX was
fired”, the plausible inferences would be narrowed
down to “needy” or “stressed out”.

To better solve these problems, we propose a
context-aware variational autoencoder (CWVAE)
together with a two-stage training procedure. Vari-



2683

ational Autoencoder (VAE) based models have
shown great potential in modeling the one-to-
many problem and generate diversified inferences
(Bowman et al., 2015; Zhao et al., 2017).

In addition to the traditional VAE structure, we
introduces an extra context-aware latent variable
in CWVAE to learn the event background knowl-
edge. In the pretrain stage, CWVAE is trained
on an auxiliary dataset (consists of three narra-
tive story corpora and contains rich event back-
ground knowledge), to learn the event background
information by using the context-aware latent vari-
able. Subsequently, in the finetune stage, CWVAE
is trained on the task-specific dataset to adapt the
event background information to each specific as-
pect of If-Then inferential target (e.g., intents, re-
actions, etc.).

Experiments on the Event2Mind and Atomic
dataset show that our proposed approach outper-
forms baseline methods in both the accuracy and
diversity of inferences. The code is released at
https://github.com/sjcfr/CWVAE.

2 Background

Before specifically describing two dataset —-
Event2Mind and Atomic used in this paper as well
as the If-Then reasoning task, for clarity, we define
the following terminologies:

Base event: the prerequisite event in If-Then
reasoning, organized as a verb phrase with a predi-
cate and its arguments, such as the event “PersonX
finds a new job” shown in Figure 1.

Inference dimension: a particular If-Then rea-
soning type, e.g., intents, effects of the base event.
Details are shown in Table 1 and Table 2.

Target: the inferential results. For example,
as shown in Figure 1, given a base event “Per-
sonX finds a new job” and one inference dimen-
sion “xReact”, the targets could be “relieved” or
“needy”. Notice that each inference dimension can
have multiple targets.
Event2Mind Dataset contains 25K base events
and 300K targets, annotated through crowdsourc-
ing. Event2Mind is organized in a hierarchical
form: each base event has three types of inference
dimensions, and given a base event, under one of
inference dimensions, several targets may simul-
taneously exist. Table 1 shows the (base event-
inference dimension-target) hierarchical structure
through an example from Event2Mind.
Atomic Dataset Inspired by Event2Mind, the

Base event Inference
Dim.

Target

PersonX
writes PersonY
a letter

xIntent to send a message,express themself

xReact nervous,thoughtful

oReact indifferent,receptive

Table 1: Hierarchical structure of Event2Mind dataset. For
specific inference dimensions, “x” and “o” refers to PersonX
and others respectively.

Base event Inference
Dim.

Target

PersonX
adopts a child

xIntent to help another person,to have a child

xNeed to visit adoption agency,to be approved for adoption

xAttr compassionate,generous

xEffect becomes a parent,gains love and companionship

xWant take child home,buy child clothes

xReact happy,caring

oReact has a parent,receives love and affection

oWant try on new clothes,to have a family

oEffect has a parent,Receives love and affection

Table 2: Hierarchical structure of Atomic dataset. For spe-
cific inference dimensions, “x” and “o” refers to PersonX and
others respectively.

Atomic dataset shares the same hierarchical struc-
ture as Event2Mind, while scales up the size
of dataset and expands the scope to nine types
of inference dimensions. Table 2 shows the
(base event-inference dimension-target) hierarchi-
cal structure through an example from Atomic.
Though Atomic covers the inference dimensions
of Event2Mind, the base event collection of
Event2Mind is nonidentical to that of Atomic.
Problem Definition The If-Then reasoning task
could be formally defined as a conditional one-to-
many generation problem: given a base event x
and one inference dimension d, the model is re-
quired to generate targets y = f(x, d) as close
to the ground truths as possible. Both x and y
consist of sequence of words: x = {x1, . . . , xm},
and y = {y1, . . . , yn}, where m and n denotes the
length of x and y, respectively.
Conditional Variational Autoencoder The vari-
ational autoencoder (VAE) defines a generative
framework suited for one-to-many generation
problem (Kingma and Welling, 2014). While con-

https://github.com/sjcfr/CWVAE


2684

Figure 2: Illustration of inference and generation process of
CVAE in a directed graph. Dashed lines represent the infer-
ence of z. Solid lines represent the generation process.

ditional variational autoencoder (CVAE) (Sohn
et al., 2015) is an extension of VAE on the con-
ditional generation problem. As shown in Fig-
ure 2 (a), CVAE characterizes the conditional one-
to-many generation problem using three random
variables: event x, target y and a latent variable
z, which is used for modeling the latent distri-
bution of semantic over targets given an event.
Hence, under a certain inference dimension, with
regard to the latent semantic variable z, the condi-
tional generation problem could be expressed as
p(y|x) =

∫
p(y|x, z)p(z|x)dz. CVAE models

p(y|x, z) and p(z|x) using deep neural networks
(parameterized by θ) pθ(y|x, z) and pθ(z|x). Then
as illustrated in Figure 2 (b), y could be generated
from x and z.

CVAE is trained to maximize the conditional
likelihood p(y|x), which involves an intractable
marginalization over the latent variable z. In-
stead, following Kingma and Welling (2014), a
practical way is to introduce another deep network
(parameterized by φ) qφ(z|x, y) to approximate
the true posterior distribution p(z|x, y) and maxi-
mize the evidence lower bound (ELBO) of the log-
likelihood function:

LELBO(θ, φ) =Eqφ(z|x,y)log(pθ(y|x, z))−
KL(qφ(z|x, y)||pθ(z|x))
≤ logp(y|x)

(1)

Therefore, CVAE is composed of three neural
networks in general. We refer to pθ(z|x) as a prior
network, qφ(z|x, y) as a recognition network, and
pθ(y|x, z) as a neural decoder.

3 Context-aware Variational
Autoencoder

Traditional CVAE can model the event-target
relation. In other words, given an observed
event, CVAE can generate its corresponding tar-
gets. While in this paper we model the If-Then
reasoning as a [(background), event]-target
process. It means that in addition to the observed

Figure 3: Illustration of pretrain, finetune and generation
process of CWVAE in a directed graph. Dashed lines rep-
resent the inference of z, zc and zc′ . Solid lines represent
the generation process. Red circle denotes the context-aware
latent variable.

event, we also want to involve the event back-
ground knowledge (which can be learned from
event contexts) to generate the reasonable targets.

To this end, we propose a context-aware varia-
tional autoencoder (CWVAE), with two additional
latent variables: a context-acquiring latent vari-
able zc to directly acquire context information,
and a context-aware latent variable zc′ to learn
background knowledge from zc, as shown in Fig-
ure 3 (a). However, the event context information
is absent in the Event2Mind and Atomic dataset.
To learn from the external event context informa-
tion, we design the following two-stage training
procedure for CWVAE.

Pretrain: Learning Event Background Knowl-
edge from Auxiliary Dataset In the pretrain
stage, CWVAE is trained on three narrative story
corpora with rich event context information. As
shown in Figure 3 (a), context-acquiring latent
variable zc is directly conditioned on the context c.
Hence, zc could be employed for acquiring back-
ground knowledge from event contexts. Then, we
minimize the distance between zc and the context-
aware latent variable zc′ , by which the event back-
ground knowledge is transferred from zc to zc′ .

Finetune: Adapt Event Background Knowl-
edge to Each Inference Dimension In the fine-
tune stage, as shown in Figure 3 (b), CWVAE
is trained on the Event2Mind and Atomic dataset
without the event context information. Pretrained
CWVAE is finetuned to learn the specific inferen-
tial knowledge of each inference dimension. After
the training procedure, as shown in Figure 3 (c),
samples of z is generated based on x and samples
of zc′ , where zc′ contains rich event background
knowledge helpful for If-Then reasoning.



2685

Figure 4: Architecture of CWVAE. We mark Neural encoder
in green, prior network in blue, recognition network in brown
and neural decoder in orange, respectively.

3.1 Architecture of CWVAE

As shown in Figure 4, CWVAE is mainly com-
posed of four parts: a neural encoder that provides
distributed representations of base events/targets,
a recognition network for inferring qφ(z|x, y),
qφ(zc|x, c) and qφ(z|zc′ , x), a prior network for
modeling pθ(zc′ |x) and pθ(z|x, zc′), and a neural
decoder that integrates the information from z and
zc′ to generate targets.
Neural Encoder We employ a bidirectional GRU
as neural encoder, which encodes context c, event
x and target y into distributed representations
hc = {hc1, . . . , hclc}, h

x = {hx1 , . . . , hxlx} and
hy = {hy1, . . . , h

y
ly
}, where lc, lx and ly is the

length of c, x and y, respectively.
Recognition Network The recognition network
models qφ(z|x, y), qφ(zc|x, c), qφ(z|zc′ , x) based
on hx, hy and hc.

Following traditional VAE, the above-
mentioned three distributions are assumed to
be multivariate Gaussian distribution with a
diagonal covariance structure:

qφ(zc|x, c) ∼ N(µzc(x, c), σzc(x, c)I)
qφ(zc′ |x, y) ∼ N(µzc′ (x, y), σzc′ (x, y)I)
qφ(z|x, y) ∼ N(µz(x, y), σz(x, y)I)

(2)

where µ denotes the mean of the distribution, σ
denotes the standard deviation of the distribution,
and I denotes the identity matrix.

Given hx, hy and hc, we propose a novel
attention-based inferer (ABI) module to estimate
the mean and standard deviation of qφ(zc|x, c),
qφ(zc′ |x, y) and qφ(z|x, y):

µzc , σz = ABIc(h
c, hx)

µzc′ , σz = ABIc′(h
y, hx)

µz, σz = ABIz(zc′ , h
x)

(3)

Briefly, through the attention mechanism, ABI
can capture the semantic interaction between input
sequences, and estimate the parameters of distri-
butions based on it. We will introduce the specific
structure of ABI in below.
Prior Network Prior Network models pθ(zc′ |x)
and pθ(z|x, zc′) based on hx. The distribution of
pθ(zc′ |x) and pθ(z|x, zc′) are still assumed to be
multivariate Gaussian, whereas the parameters are
different:

pθ(z|x) ∼ N(µ
′
z(x), σ

′
z(x)I)

pθ(zc′ |x) ∼ N(µ
′
zc′

(x), σ
′
zc′

(x)I)
(4)

where µ
′

denotes the mean of the distribution, σ
′

denotes the standard deviation of the distribution
and I denotes the identity matrix.

Then the attention-based inferer module is still
employed to estimate parameters of distributions:

µ
′
zc′
, σ
′
z = ABI

′
c′(h

y, hx)

µ
′
z, σ

′
z = ABI

′
z(zc′ , h

x)
(5)

Neural Decoder Given the base event x, the se-
mantic latent variable z, and the context-aware la-
tent variable zc′ , the neural decoder defines the
generation probability of y as following:

p(y|x, z, zc′) =
n∏
j=1

p(yj |y < j, z, zc′ , x) (6)

where p(yj |y < j, z, zc′ , x) = g(yj−1, sj−1, ej),
g(·) is an attention-based feed forward model,
ej =

∑
i αjih

x
i is the context vector and sj−1

is the hidden state of the decoder. We obtain
g(·) and ej the same way as Bahdanau et al.
(2014). Whereas our decoder differs from Bah-
danau et al. (2014) in that our model integrates
the context-aware latent variable zc′ and seman-
tic latent variable z in the computation of sj =
GRU([Eyj ; sj−1, z, zj−1]), where Eyj is the word
embeddings of target words.

Note that through concatenating z and zc′ with
Eyj and sj−1, sj could be affected by context-
aware latent variable zc′ and semantic latent vari-
able z. This allows model to directly access to the
event background knowledge from zc′ . In addi-
tion, the randomness of z and zc′ would increase
the diversity of model generation.
Attention-based Inferer Attention mechanism
has shown strong ability in capturing semantic in-
teractions (Gong et al., 2017). Inspired by the co-
attention mechanism (Parikh et al., 2016), we pro-
pose an attention-based inferer (ABI) to estimate



2686

the mean and standard deviation of a distribution
belongs to pθ(·) or qφ(·) by capturing semantic in-
teractions of input sequences.

Specifically, given two input sequences (e.g.,
representations of contexts and events) a =
{a1, . . . , ala} and b = {b1, . . . , blb} with length
la and lb, we first obtain the attention scores from
each side through:

γat =
exp[(Wah

a
t )
T (Wbh

b
i )]∑

i exp[(Wah
a
t )
T (Wbhbi )]

γbt =
exp[(Wah

a
i )
T (Wbh

b
t)]∑

i exp[(Wah
a
i )
T (Wbhbt)]

(7)

where Wa ∈ Rd×da and Wb ∈ Rd×db are parame-
ter weights.

With these attention scores, the context vectors
of both sequences are given by:

cat = aγ
a
t

cbt = bγ
b
t

(8)

Then we perform a mean pooling operation on
context vectors of both sequences:

c̄a =
1

la

la∑
t=1

cat

c̄b =
1

lb

lb∑
t=1

cbt

(9)

To obtain the mean and standard deviation, the
pooled context vectors c̄a and c̄b which carry se-
mantic interaction between two sequences, are
concatenated and projected into a latent semantic
space through a nonlinear transformation:

hz = tanh(W [c̄
a; c̄b] + bz) (10)

Finally the mean and standard deviation are
generated through a nonlinear transformation over
hz:

µ = Wµhz + bµ

σ = softplus(Wσhz + bσ)
(11)

3.2 Optimizing

With the incorporation of zc′ , the original loglike-
lihood could be decomposed as:

logp(y|x) =
∫∫

p(y|z, zc′ , x)dzdzc′ (12)

Then following traditional CVAE, the ELBO of
CWVAE is defined as follows:

LELBO(θ, φ) =

Eqφ(z|x,zc′ )qφ(zc′ |x,y)logpθ(y|x, z, zc′)︸ ︷︷ ︸
Reconstruction Loss

−
∫
qφ(z|x, y)KL(qφ(zc′ |x, y)||pθ(zc′ |x))dz︸ ︷︷ ︸

KL term

−
∫
qφ(zc′ |x, y)KL(qφ(z|x, zc′)||pθ(z|x, zc′))dzc′︸ ︷︷ ︸

KL term

(13)

which is the objective function at the finetune
stage.

While in the pretrain stage, as we aim to learn
background knowledge through minimizing the
distance between zc and zc′ , in addition toLELBO,
a context-aware regulation term is introduced:

LPT (θ, φ) =LELBO(θ, φ)

−λ
∫

KL(qφ(zc|x, c)||qφ(zc′ |x, y))dzc′︸ ︷︷ ︸
context-aware regulation

(14)

where the context aware regularization term is the
KL distance between z and zc′ . Through mini-
mizing the context aware regularization term, we
aim to pass event context knowledge from zc to the
context aware latent variable zc′ .

3.3 Training Details
To test the performance of CWVAE, we split the
Event2Mind and Atomic dataset into training, de-
velopment and test sets (80%, 10%, 10%) in the
same way as Rashkin et al. (2018) and Sap et al.
(2018), respectively.

We initialize the embedding layer from 300d
GloVe word embeddings. The neural encoder is
chosen to be biGRU with 300 hidden units. For
the ABI module, size of Wa and Wb is set to be
100 × da and 100 × db respectively. The dimen-
sion of zc, zc′ and z is all set as 40. The neural
decoder is set to be GRU with 300d hidden state.
Regulation coefficient λ of context-aware regula-
tion term is set to be 0.1. Models are trained using
an Adam optimizer (Kinga and Adam, 2015) with
a learning rate of 0.001.

4 Experiments

4.1 Auxiliary Dataset
The auxiliary dataset is built upon three human-
written story corpora: ROCStories (Mostafazadeh



2687

Context Event Inference Target
1© jason had been really stressed out at work.
2© he decided he needed a different kind of job.
3© jason applied for a job in a different field.

4© he got the job . 5© jason was much happier at his new job .

Table 3: An example for the construction of auxiliary dataset. For a five-sentence-paragraph, the first three sentences are taken
as event context, while the fourth and fifth sentence is taken as base event and target respectively.

et al., 2016), VIST (Huang et al., 2016) and Writ-
ingPrompts (Fan et al., 2018). ROCStories and
VIST are composed of short stories with five sen-
tences. We filter out stories of more than 1,000
words in WritingPrompts, and cut the remaining
stories into five-sentence-paragraphs.

For each five-sentence-paragraph, we define the
first three sentences as contexts of the base event,
the fourth sentence as the base event, and the fifth
sentence as the inference target. For example, as
shown in Table 3, the first three sentences describe
a context that Jason was unsatisfied about his job
and applied for a new job. Hence, after happening
the event “he got the job”, a plausible react about
the event could be “jason was much happier at his
new job”. In total, the auxiliary dataset contains
192,316 (context, event, target) triples.

4.2 Baselines
We compared our proposed model with the follow-
ing four baseline methods:

• RNN-based Seq2Seq proposed by Sap et al.
(2018) for the If-Then reasoning on Atomic.
• Variational Seq2Seq combines a latent vari-

able with the encoder-decoder structure through
converting the last hidden state of RNN en-
coder into a Gaussian distributed latent variable
(Bowman et al., 2015).
• VRNMT Propose by Su et al. (2018), VRNMT

combines CVAE with attention-based encoder-
decoder framework through introduces a latent
variable to model the semantic distribution of
targets.
• CWVAE-Unpretrained refers to the CWVAE

model without the pretrain stage.

Note that, for each baseline method, we train dis-
tinct models for each distinct inference dimension,
respectively.

4.3 Evaluation Metrics
Automatic Evaluation We first compare the
perplexity of CWVAE with baseline methods. Per-
plexity measures the probability of model to re-
generate the exact targets, which is particular suit-

Metric Methods xIntent xReact oReact

PPL

RNN-based Seq2Seq 44.12 29.18 14.08
Variational Seq2Seq 42.06 28.22 12.62
VRNMT 33.45 25.54 11.93
CWVAE-Unpretrained 31.32 24.07 11.37
CWVAE 29.23 23.17 11.04

BLEU

RNN-based Seq2Seq 2.75 2.11 5.18
Variational Seq2Seq 2.84 2.43 2.08
VRNMT 3.94 4.81 6.61
CWVAE-Unpretrained 5.52 7.36 5.33
CWVAE 5.65 12.98 6.97

Table 4: Average perplexity and BLEU score (reported in
percentages) for the top 10 generations under each inference
dimension of Event2Mind. The the best result for each di-
mension is emboldened.

Metric Methods xIntent xReact oReact

dist-1

RNN-based Seq2Seq 0.0002 0.0002 0.0001
Variational Seq2Seq 0.0006 0.0003 0.0001
VRNMT 0.0002 0.0002 0.0003
CWVAE-Unpretrained 0.0023 0.0017 0.0004
CWVAE 0.0052 0.0033 0.0025

dist-2

RNN-based Seq2Seq 0.0005 0.0002 0.0002
Variational Seq2Seq 0.0014 0.0002 0.0001
VRNMT 0.0005 0.0003 0.0001
CWVAE-Unpretrained 0.0061 0.0040 0.0013
CWVAE 0.0146 0.0099 0.0063

Table 5: Distinct-1 and distinct-2 scores for the top 10 gen-
erations under each inference dimension of Event2Mind. The
the best result for each dimension is emboldened.

able for evaluating the model performance on one-
to-many problem (Serban et al., 2017). Further,
we employ BLEU score to evaluate the accuracy
of generations (Papineni et al., 2002), and the
number of distinct n-gram to evaluate the diver-
sity of generations (Li et al., 2016). The distinct is
normalized to [0, 1] by dividing the total number
of generated tokens.

Human Evaluation Since automatic evaluation
of generations is still a challenging task (Liu et al.,
2016), we also conduct human evaluations on the
model performance. Five human experts are em-
ployed to evaluate the coherence, diversity and flu-
ency of generated targets. Experts are asked to
vote for if a generation is fluent or coherent for
each generated target, and give a 1-5 score for the
diversity of generations. For both Event2Mind and
Atomic datasets, 100 events are randomly selected
from the test set. For each method, top 10 gener-
ated targets of each base event are used for eval-



2688

Metric Methods xIntent xNeed xAttr xEffect xReact xWant oWant oReact oEffect

PPL

RNN-based Seq2Seq 22.54 24.69 33.54 65.13 29.52 26.63 16.76 14.99 35.17
Variational Seq2Seq 26.48 28.31 33.00 68.62 29.93 29.50 16.98 14.25 34.20

VRNMT 21.04 24.28 24.87 61.05 26.62 28.57 14.45 14.86 30.12
CWVAE-Unpretrained 20.73 23.72 25.80 60.62 25.75 26.71 15.93 12.82 32.00

CWVAE 15.93 20.32 23.85 50.74 21.39 24.02 14.02 11.70 29.13

BLEU

RNN-based Seq2Seq 8.17 12.35 2.96 5.26 3.43 13.44 7.08 4.09 6.42
Variational Seq2Seq 8.31 12.05 2.13 6.07 2.52 11.71 7.40 4.08 6.38

VRNMT 9.52 13.35 4.87 4.42 7.64 9.80 10.79 5.28 13.71
CWVAE-Unpretrained 11.37 14.64 4.07 14.11 7.86 12.70 12.09 8.16 14.93

CWVAE 12.12 15.67 5.63 14.64 8.13 15.01 13.83 8.58 11.63
Table 6: Average perplexity and BLEU scores (reported in percentages) for the top 10 generations under each inference
dimension of Atomic. The the best result for each dimension is emboldened.

Metric Methods xIntent xNeed xAttr xEffect xReact xWant oWant oReact oEffect

dist-1

RNN-based Seq2Seq 0.0012 0.0029 0.0004 0.0019 0.0001 0.0022 0.0006 0.0001 0.0006
Variational Seq2Seq 0.0006 0.0018 0.0002 0.0002 0.0001 0.0013 0.0007 0.0001 0.0002

VRNMT 0.0002 0.0001 0.0053 0.0005 0.0018 0.0022 0.0005 0.0001 0.0004
CWVAE-Unpretrained 0.0019 0.0036 0.0119 0.0046 0.0021 0.0013 0.0018 0.0005 0.0006

CWVAE 0.0055 0.0045 0.0142 0.0028 0.0043 0.0040 0.0021 0.0030 0.0033

dist-2

RNN-based Seq2Seq 0.0036 0.0081 0.0002 0.0018 0.0002 0.0006 0.0013 0.0001 0.0011
Variational Seq2Seq 0.0013 0.0042 0.0001 0.0003 0.0002 0.0026 0.0002 0.0003 0.0006

VRNMT 0.0002 0.0011 0.0002 0.0005 0.0001 0.0034 0.0005 0.0001 0.0004
CWVAE-Unpretrained 0.0060 0.0088 0.0136 0.0113 0.0043 0.0029 0.0041 0.0011 0.0009

CWVAE 0.0162 0.0112 0.0146 0.0072 0.0013 0.0107 0.0044 0.0068 0.0093
Table 7: Distinct-1 and distinct-2 scores for the top 10 generations under each inference dimension of Atomic. The the best
result for each dimension is emboldened.

uation. Finally we report three overall averaged
scores of coherence, diversity and fluency on both
datasets, respectively.

4.4 Overall Results
We list the perplexity and BLEU score of CWVAE
and baseline methods on Event2Mind and Atomic
in Table 4 and Table 6, respectively, and show the
distinct-1 and distinct-2 score on Event2Mind and
Atomic in Table 5 and Table 7, respectively. We
find that:

(1) As shown in Table 5 and Table 7, compari-
son between RNN-based Seq2Seq and variational-
based methods, including Variational Seq2Seq,
VRNMT, CWVAE-unpretrained and CWVAE
shows that, variational-based methods could in-
crease the diversity of generations. This con-
firms one of our motivations that variational-based
methods could capture the latent semantic distri-
bution within targets and increase the diversity of
If-Then reasoning.

(2) Comparing CWVAE-unpretrained with
other baseline methods shows that, in general CW-
VAE improves the accuracy and diversity on both
dataset. These results indicate the efficiency of
CWVAE in capturing the latent semantic distribu-
tion of targets, and generate more reasonable in-
ferential results.

(3) Comparison between CWVAE and
CWVAE-unpretrained shows that the pre-

Methods Coherence Diversity Fluency
RNN-based Seq2Seq 0.28 2.03 0.73
Variational Seq2Seq 0.33 1.67 0.92
VRNMT 0.32 2.60 0.83
CWVAE-Unpretrained 0.36 2.10 0.92
CWVAE 0.43 2.85 0.96

Table 8: Human evaluation results on Event2Mind.

Methods Coherence Diversity Fluency
RNN-based Seq2Seq 0.21 2.66 0.78
Variational Seq2Seq 0.22 2.70 0.90
VRNMT 0.24 2.61 0.78
CWVAE-Unpretrained 0.25 2.72 0.83
CWVAE 0.32 3.03 0.90

Table 9: Human evaluation results on Atomic.

train stage could enhance the performance of
CWVAE in both the accuracy and diversity.
This is mainly because event knowledge could
offer the guidance for If-Then reasoning. In the
pretrain stage, CWVAE could capture the event
background knowledge through context-aware
latent variable, and such knowledge could be be
adapted to our task through the fintune stage.

To further evaluate the effectiveness of our pro-
posed approach, we also conduct human eval-
uations, the results of which are shown in Ta-
ble 8 and Table 9. On both datasets, CWVAE-
based methods achieve consistent better coher-
ence, diversity and fluency performances. While
comparing with CWVAE-Unpretrained, the pre-
train procedure could improves the performance



2689

Base event Inferencedim.
Generations Ground truthCWVAE RNN-based Seq2Seq

PersonX works
tirelessly xIntent

be productive and hardworking
finish his work soon
earn more money and accomplish goal
finish his work and make money
finish his work and accomplish goal
be productive and successful

get the job done
get the job done on time
get the job done in the way
get the job done for his life
make money for his own future
make money for his own money

be productive
finish the project as soon as possible
reach goal

Table 10: An example of inferences made by CWVAE and RNN-based Seq2Seq model under inference dimension “xIntent”.

on coherence and fluency. The main reasons are
twofold: first, the CWVAE has advantage in cap-
turing the semantic distribution of targets; second,
event background learned from the pretrain stage
is helpful for the If-Then reasoning.

4.5 Case Study

Table 10 provides an example of model genera-
tions given the base event “PersonX works tire-
lessly” and the inference dimension “xIntent”.
The generations under CWVAE mainly contain
four kinds of semantics: (1) be productive, (2) fin-
ish his work soon, (3) accomplish goal, (4) earn
more money. While the semantics of generations
using baseline RNN-based Seq2Seq model is rel-
atively limited. Furthermore, the first three kinds
of semantic overlap the three ground truth targets,
and the fourth kind of semantic is in accordance
with daily-life commonsense. Compared to RNN-
based Seq2Seq model, our approach can increase
the diversity and rationality of generations, mean-
while keep the accuracy.

5 Related Work

5.1 Event-Centered Commonsense
Reasoning

Understanding events and constructing event-
centered commonsense knowledge are crucial to
many NLP applications, such as intention recog-
nition (Goldwasser and Zhang, 2016) and dialog
generation (Wen et al., 2017). Recently a growing
number of studies focus on event-centered com-
monsense reasoning, which mainly concentrates
on two areas, script event prediction and story end-
ing generation/choosing.

Script event prediction concerns with the
temporal relationships between script events
(Granroth-Wilding and Clark, 2016), which re-
quires models to choose a correct subsequent
triple-organized event among the candidates
(Wang et al., 2017). Prior work mainly focused
on modeling event pairs (Granroth-Wilding and
Clark, 2016), event chains (Wang et al., 2017) and
event graph (Li et al., 2018) to predict the subse-

quent event. Story ending generation focuses on
generating plausible story endings (Mostafazadeh
et al., 2016), which requires models to understand
the story context, and keep generated endings log-
ically consistent with it (Peng et al., 2017; Guan
et al., 2019). The above tasks mainly investi-
gate the logical orders of events, whereas the If-
Then reasoning task focuses on inferring the men-
tal state of event participants.

5.2 Variational AutoEncoder-Decoder Based
Natural Language Generation

VAE (Kingma and Welling, 2014) has been widely
applied in various of text generation tasks, such as
dialogue and machine translation.

In dialogue generation, Zhao et al. (2017)
adapts VAE with encoder-decoder framework to
model the latent semantic distribution of answers,
which can increase the diversity of generations.
For the task of machine translation, Su et al.
(2018) and Zhang et al. (2016) employ a latent
variable to capture the semantic interaction be-
tween the source and target sentence, and regard
the latent variable as a supplementation of atten-
tion mechanism. While Wang et al. (2019) use the
latent variable to model topic distributions in text
generation. In this paper, we introduce an addi-
tional context-aware latent variable to effectively
learn background knowledge and conduct If-Then
reasoning on the guidance of it.

6 Conclusion

In this paper, we propose a novel context-aware
VAE (CWVAE) framework with two training
stages for If-Then commonsense reasoning. By in-
troducing an additional context-aware latent vari-
able, CWVAE is able to learn external background
knowledge, and conduct If-Then reasoning under
its guidance. In the pretrain stage, CWVAE learns
event background knowledge, then in the finetune
stage CWVAE adapts such knowledge to each in-
ference dimension. Experimental results demon-
strate that CWVAE outperforms baseline methods
in both the accuracy and diversity of generations.



2690

7 Acknowledgments

We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Key Re-
search and Development Program of China
(SQ2018AAA010010), the National Key Re-
search and Development Program of China
(2018YFB1005103), the National Natural Science
Foundation of China (NSFC) via Grant 61702137.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. Proc. of
ACL-08: HLT, pages 789–797.

Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 889–898.

Dan Goldwasser and Xiao Zhang. 2016. Understand-
ing satirical articles using common-sense. Transac-
tions of the Association for Computational Linguis-
tics, 4:537–549.

Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natu-
ral language inference over interaction space. arXiv
preprint arXiv:1709.04348.

Mark Granroth-Wilding and Stephen Clark. 2016.
What happens next? event prediction using a com-
positional neural network model. In AAAI.

Jian Guan, Yansen Wang, and Minlie Huang. 2019.
Story ending generation with incremental encoding
and commonsense knowledge.

Ting-Hao Kenneth Huang, Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Ja-
cob Devlin, Ross Girshick, Xiaodong He, Pushmeet
Kohli, Dhruv Batra, et al. 2016. Visual storytelling.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1233–1239.

D Kinga and J Ba Adam. 2015. A method for stochas-
tic optimization. In ICLR, volume 5.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. stat, 1050:10.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting objec-
tive function for neural conversation models. In Pro-
ceedings of NAACL-HLT, pages 110–119.

Zhongyang Li, Xiao Ding, and Ting Liu. 2018. Con-
structing narrative event evolutionary graph for
script event prediction. In Proceedings of the 27th
International Joint Conference on Artificial Intelli-
gence, pages 4201–4207. AAAI Press.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An em-
pirical study of unsupervised evaluation metrics for
dialogue response generation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing, pages 2122–2132.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of the 2016
Conference of the NAACL: Human Language Tech-
nologies, pages 839–849.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
ACL, pages 311–318.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2249–2255.

Haoruo Peng, Snigdha Chaturvedi, and Dan Roth.
2017. A joint model for semantic sequences:
Frames, entities, sentiments. In Proceedings of
the 21st Conference on Computational Natural Lan-
guage Learning (CoNLL 2017), pages 173–183.

Hannah Rashkin, Maarten Sap, Emily Allaway,
Noah A Smith, and Yejin Choi. 2018. Event2mind:
Commonsense inference on events, intents, and re-
actions. In Proc. of the 56th ACL, volume 1, pages
463–473.

Maarten Sap, Ronan LeBras, Emily Allaway, Chan-
dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,
Brendan Roof, Noah A Smith, and Yejin Choi. 2018.
Atomic: An atlas of machine commonsense for if-
then reasoning. arXiv preprint arXiv:1811.00146.

Roxane Segers, Marco Rospocher, Piek Vossen, Egoitz
Laparra, German Rigau, and Anne-Lyse Minard.
2016. The event and implied situation ontol-
ogy (eso): Application and evaluation. In Pro-
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016),
Paris, France. European Language Resources Asso-
ciation (ELRA).



2691

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Thirtieth AAAI
Conference on Artificial Intelligence.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville, and
Yoshua Bengio. 2017. A hierarchical latent variable
encoder-decoder model for generating dialogues. In
AAAI.

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output representation using
deep conditional generative models. In Advances in
neural information processing systems, pages 3483–
3491.

Jinsong Su, Shan Wu, Deyi Xiong, Yaojie Lu, Xianpei
Han, and Biao Zhang. 2018. Variational recurrent
neural machine translation. In AAAI.

Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang,
and Lawrence Carin. 2019. Topic-guided variational
autoencoders for text generation.

Zhongqing Wang, Yue Zhang, and Ching-Yun Chang.
2017. Integrating order information and event rela-
tion for script event prediction. In Proc. of the 2017
EMNLP, pages 57–67.

Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
Steve Young. 2017. Latent intention dialogue mod-
els. In Proc. of the 34th ICML-Volume 70, pages
3732–3741. JMLR. org.

Biao Zhang, Deyi Xiong, Hong Duan, Min Zhang,
et al. 2016. Variational neural machine translation.
In Proc. of the 2016 EMNLP, pages 521–530.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 654–664.


