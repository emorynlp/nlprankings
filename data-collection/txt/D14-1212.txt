



















































Online topic model for Twitter considering dynamics of user interests and topic trends


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1977–1985,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Online Topic Model for Twitter Considering Dynamics of User Interests
and Topic Trends

Kentaro Sasaki, Tomohiro Yoshikawa, Takeshi Furuhashi
Graduate School of Engineering Nagoya University
sasaki@cmplx.cse.nagoya-u.ac.jp

yoshikawa, furuhashi@cse.nagoya-u.ac.jp

Abstract

Latent Dirichlet allocation (LDA) is a
topic model that has been applied to var-
ious fields, including user profiling and
event summarization on Twitter. When
LDA is applied to tweet collections, it gen-
erally treats all aggregated tweets of a user
as a single document. Twitter-LDA, which
assumes a single tweet consists of a single
topic, has been proposed and has shown
that it is superior in topic semantic coher-
ence. However, Twitter-LDA is not capa-
ble of online inference. In this study, we
extend Twitter-LDA in the following two
ways. First, we model the generation pro-
cess of tweets more accurately by estimat-
ing the ratio between topic words and gen-
eral words for each user. Second, we en-
able it to estimate the dynamics of user in-
terests and topic trends online based on the
topic tracking model (TTM), which mod-
els consumer purchase behaviors.

1 Introduction

Microblogs such as Twitter, have prevailed rapidly
in our society recently. Twitter users post a mes-
sage using 140 characters, which is called a tweet.
The characters limit allows users to post tweets
easily about not only personal interest or real life
but also public events such as traffic accidents
or earthquakes. There have been many studies
on how to extract and utilize such information
on tweets (Diao et al., 2012; Pennacchiotti and
Popescu, 2011; Sakaki et al., 2010; Weng et al.,
2010).

Topic models, such as latent Dirichlet alloca-
tion (LDA) (Blei et al., 2003) are widely used to
identify latent topic structure in large collections
of documents. Recently, some studies have ap-
plied LDA to Twitter for user classification (Pen-

nacchiotti and Popescu, 2011), detection of influ-
ential users (Weng et al., 2010), and so on. LDA
is a generative document model, which assumes
that each document is represented as a probabil-
ity distribution over some topics, and that each
word has a latent topic. When we apply LDA
to tweets, each tweet is treated as a single docu-
ment. This direct application does not work well
because a tweet is very short compared with tradi-
tional media such as newspapers. To deal with the
shortness of a tweet, some studies aggregated all
the tweets of a user as a single document (Hong
and Davison, 2010; Pennacchiotti and Popescu,
2011; Weng et al., 2010). On the other hand, Zhao
et al. (2011) proposed “Twitter-LDA,” which is
a model that considers the shortness of a tweet.
Twitter-LDA assumes that a single tweet consists
of a single topic, and that tweets consist of topic
and background words. Zhao et al. (2011) show
that it works well at the point of semantic coher-
ence of topics compared with LDA. However, as
with the case of LDA, Twitter-LDA cannot con-
sider a sequence of tweets because it assumes that
samples are exchangeable. In Twitter, user inter-
ests and topic trends are dynamically changing.
In addition, when new data comes along, a new
model must be generated again with all the data
in Twitter-LDA because it does not assume online
inference. Therefore, it cannot efficiently analyze
the large number of tweets generated everyday. To
overcome these difficulties, a model that considers
the time sequence and has the capability of online
inference is required.

In this study, we first propose an improved
model based on Twitter-LDA, which assumes that
the ratio between topic and background words dif-
fers for each user. This study evaluates the pro-
posed method based on perplexity and shows the
efficacy of the new assumption in the improved
model. Second, we propose a new topic model
called “Twitter-TTM” by extending the improved

1977



model based on the topic tracking model (TTM)
(Iwata et al., 2009), which models the purchase
behavior of consumers and is capable of online
inference. Finally, we demonstrate that Twitter-
TTM can effectively capture the dynamics of user
interests and topic trends in Twitter.

2 Improvement of Twitter-LDA

2.1 Improved-Model

Figure 1(a) shows the graphical representation of
Twitter-LDA based on the following assumptions.
There are K topics in Twitter and each topic is rep-
resented by a topic word distribution. Each user
has his/her topic interests ϕu represented by a dis-
tribution over K topics. Topic k is assigned to
each tweet of user u depending on the topic inter-
ests ϕu. Each word in the tweet assigned by topic
k is generated from a background word distribu-
tion θB or a topic word distribution θk. Whether
the word is a background word or a topic word
is determined by a latent value y. When y = 0,
the word is generated from the background word
distribution θB , and from the topic word distribu-
tion θk when y = 1. The latent value y is chosen
according to a distribution π. In other words, the
ratio between background and topic words is de-
termined by π.

In Twitter-LDA, π is common for all users,
meaning that the rate between background and
topic words is the same for each user. However,
this assumption could be incorrect, and the rate
could differ for each user. Thus, we develop an
improved model based on Twitter-LDA, which as-
sumes that π is different for each user, as shown
in Figure 1(b). In the improved model, the rate
between background and topic words for user u is
determined by a user-specific distribution πu. The
improved model is expected to infer the generative
process of tweets more efficiently.

2.2 Experiment for Improved Model

We performed an experiment to compare the pre-
dictive performances of LDA, TTM, and the im-
proved model shown in Section 2.1. In this ex-
periment, LDA was applied as the method to ag-
gregate all tweets of a user as a single document.
The original Twitter data set contains 14,305 users
and 292,105 tweets collected on October 18, 2013.
We then removed words that occurred less than
20 times and stop words. Retweets1 were treated

1Republishing a tweet written by another Twitter user.

as the same as other general tweets because they
reflected the user’s interests. After the above
preprocessing, we obtained the final dataset with
14,139 users, 252,842 tweets, and 7,763 vocab-
ularies. Each model was inferred with collapsed
Gibbs sampling (Griffiths and Steyvers, 2004) and
the iteration was set at 500. For a fair comparison,
the hyper parameters in these models were opti-
mized in each Gibbs sampling iteration by max-
imizing likelihood using fixed iterations (Minka,
2000).

This study employs perplexity as the evaluation
index, which is the standard metric in information
retrieval literature. The perplexity of a held-out
test set is defined as

perplexity = exp
(
− 1

N

∑
u

log p(wu)
)

(1)

where wu represents words are contained in the
tweets of user u and N is the number of words in
the test set. A lower perplexity means higher pre-
dictive performance. We set the number of topics
K at 50, 100, 150, 200, and 250 and evaluated the
perplexity for each model in each K via a 10-fold
cross-validation.

The results are shown in Table 1, which shows
that the improved model performs better than the
other models for any K. Therefore, the new as-
sumption of the improved model, that the rate be-
tween background and topic words is different for
each user, could be more appropriate. LDA per-
formance worsens with an increase in K because
the aggregated tweets of a single user neglect the
topic of each tweet.

Table 2 shows examples of the tweets of users
with high and low rates of background words. The
users with a high background words rate tend to
use basic words that are often used in any top-
ics, such as “like,” “about,” and “people,” and they
tend to tweet about their personal lives. On the
other hand, for users with a low background words
rate, topical words are often used such as “Arse-
nal,” “Justin,” and “Google”. They tend to tweet
about their interests, including music, sports, and
movies.

3 Twitter-TTM

3.1 Model Extension based on Topic
Tracking Model

We extend the improved model shown in Section
2.1 considering the time sequence and capabil-

1978



w 

θk 

K 

Nu,s 

Φ 

z 

U 

Nu 

y π θB 

β 

α 

λ γ 

(a) Twitter-LDA

w 

θk 

K 

Nu,s 

Φ 

z 

U 

Nu 

y 

π 

θB 

β 

α 

λ 

γ 

(b) Improved-model

Figure 1: Graphical representation of Twitter-LDA and Improved-model

Table 1: Perplexity of each model in 10 runs
Number of topic K LDA Twitter-LDA Improved-model

50 1586.7 (14.4) 2191.0 (28.4) 1555.3 (36.7)
100 1612.7 (11.9) 1933.9 (23.6) 1471.7 (22.3)
150 1635.3 (11.2) 1760.1 (15.7) 1372.3 (20.0)
200 1655.2 (13.0) 1635.4 (22.1) 1289.5 (13.3)
250 1672.7 (17.2) 1542.8 (12.5) 1231.1 (11.9)

Table 2: Example of tweets of users with high and low rate of background words
High rate of background words Low rate of background words

I hope today goes quickly Team Arsenal v will Ozil be
I want to work in a cake Making Justin smile and laugh as he is working on music
All need your support please Google nexus briefly appears in Google play store

ity of online inference based on TTM (Iwata et
al., 2009). TTM is a probabilistic consumer pur-
chase behavior model based on LDA for track-
ing the interests of each user and the trends in
each topic. Other topic models considering the dy-
namics of topics include the dynamic topic model
(DTM) (Blei and Lafferty, 2006) and topic over
time (ToT) (Wang and McCallum, 2006). DTM is
a model for analyzing the time evolution of top-
ics in time-ordered document collections. It does
not track the interests of each user as shown in
Figure 2(a) because it assumes that a user (doc-
ument) has only one time stamp. ToT requires all
the data over time for inference, thus, it is not ap-

propriate for application to continuously generated
data such as Twitter. We consider a model must be
capable of online inference and track the dynam-
ics of user interests and topic trends for modeling
tweets. Since TTM has these abilities, we adapt it
to the improved model described in Section 2.

Figure 2(b) shows the graphical representation
of TTM. TTM assumes that the mean of user in-
terests at the current time is the same as that at the
previous time, unless new data is observed. For-
mally, the current interest ϕt,u are drawn from the
following Dirichlet distribution in which the mean
is the previous interest ϕ̂t−1,u and the precision is

1979



w 

α 

θk 

K 

Φ 

z 

U 

Nu 

w 

θk
 

K 

Φ 

z 

U 

Nu 

α α 

t-1 t 

(a) DTM

w 

α 

θk 

K 

Φ 

z 

U 

Nu 

w 

θk
 

K 

Φ 

z 

U 

Nu 

α α 

β β 

t-1 t 

(b) TTM

Figure 2: Graphical representation of DTM and TTM

αt,u

p(ϕt,u|ϕ̂t−1,u, αt,u) ∝
∏
k

ϕ
αt,uϕ̂t−1,u,k−1
t,u,k (2)

where ϕt,u,k represents the probability that user u
is interested in topic k at time t. t is a discrete
variable and can be arbitrarily set as the unit time
interval, e.g., at one day or one week. The preci-
sion αt,u represents the interest persistence of how
consistently user u maintains his/her interests at
time t compared with the previous time t−1. αt,u
is estimated for each time period and each user
because interest persistence depends on both time
and users. As mentioned above, the current topic
trend θt,k is drawn from the following Dirichlet
distribution with the previous trend θ̂t−1,k

p(θt,k|θ̂t−1,k, βt,k) ∝
∏
v

θ
βt,k θ̂t−1,k,v−1
t,k,v (3)

where θt,k,v represents the probability that word v
is chosen in topic k at time t.

Here our proposed Twitter-TTM adapts the
above TTM assumptions to the improved model.
That is, we extend the improved model whereby
user interest ϕt,u and topic trend θt,k depend on
previous states. Time dependency is not consid-
ered on θB and πu because they can be regarded
as being independent of time.

Figures 3 and 4 show the generative process
and a graphical representation of Twitter-TTM, re-
spectively. Twitter-TTM can capture the dynam-
ics of user interests and topic trends in Twitter
considering the features of tweets online. More-
over, Twitter-TTM can be extended to capture
long-term dependences, as described in Iwata et
al. (2009).

3.2 Model Inference

We use a stochastic expectation-maximization al-
gorithm for Twitter-TTM inference, as described
in Wallach (2006) in which Gibbs sampling of la-
tent values and maximum joint likelihood estima-
tion of parameters are alternately iterated. At time
t, we estimate user interests Φt = {ϕ̂t,u}Uu=1,
topic trends Θt = {θ̂t,k}Kk=1, background word
distribution θt,B , word usage rate distribution πt,u,
interest persistence parameters αt = {αt,u}Uu=1,
and trend persistence parameters βt = {βt,k}Kk=1
using the previous time interests Φ̂t−1 and trends
Θ̂t−1.

We employ collapsed Gibbs sampling to infer
the latent variables. Let Dt be a set of tweets and
Zt, Yt be a set of latent variables z, y at time t. We
can integrate the parameters in the joint distribu-

1980



tion as follows:

p(Dt, Yt, Zt|Φ̂t−1, Θ̂t−1, αt,βt, λ, γ)
=

(Γ(2γ)
Γ(γ)2

)U ∏
u

Γ(γ + nt,u,B)Γ(γ + nt,u,K)
Γ(2γ + nt,u)

×Γ(V λ)
Γ(λ)V

∏
v
Γ(nt,B,v + λ)

Γ(nt,B + V λ)

×
∏
k

Γ(βt,k)
Γ(nt,k + βt,k)

∏
v

Γ(nt,k,v + βt,kθ̂t−1,k,v)
Γ(βt,kθ̂t−1,k,v)

×
∏
u

Γ(αt,u)
Γ(ct,u + αt,u)

∏
k

Γ(ct,u,k + αt,uϕ̂t−1,u,k)
Γ(αt,uϕ̂t−1,u,k)

,

(4)

where nt,u,B and nt,u,K are the number of back-
ground and topic words of user u at time t, nt,B,v
is the number of times that word v is assigned as
a background word at time t, nt,k,v is the num-
ber of times that word v is assigned to topic k
at time t, ct,u,k is the number of tweets assigned
to topic k for user u at time t. In addition,
nt,u = nt,u,B + nt,uK , nt,B =

∑
v nt,B,v, nt,K =∑

k nt,k =
∑

k

∑
v nt,k,v, nt,u =

∑
k nt,u,k, and

ct,u =
∑

k ct,u,k.
Given the assignment of all other latent vari-

ables, we derive the following formula calculated
from eq.(4) to infer a latent topic,

p(zi = k|Dt, Yt, Zt\i, Φ̂t−1, Θ̂t−1, αt, βt)

∝ ct,u,k\i + αt,uϕ̂t−1,u,k
ct,u\i + αt,u

Γ(nt,k\i + βt,k)
Γ(nt,k + βt,k)

×
∏
v

Γ(nt,k,v + βt,kθ̂t−1,k,v)
Γ(nt,k,v\i + βt,kθ̂t−1,k,v)

, (5)

where i = (t, u, s), thus zi represents a topic as-
signed to the s-th tweet of user u at time t, and \i
represents a count excluding the i-th tweet.

Then, when zi = k is given, we derive the fol-
lowing formula to infer a latent variable yj ,

p(yj = 0|Dt, Yt\j , Zt, λ, γ)

∝ nt,B,v\j + λ
nt,B\j + V λ

nt,u,B\j + γ
nt,u\j + 2γ

, (6)

p(yj = 1|Dt, Yt\j , Zt, Θ̂t−1, βt, γ)

∝ nt,k,v\j + βt,kθ̂t−1,k,v
nt,k\j + βt,k

nt,u,K\j + γ
nt,u\j + 2γ

, (7)

where j = (t, u, s, n), thus yj represents a latent
variable assigned to the n-th word in the s-th tweet

of user u at time t, and \j represents a count ex-
cluding the j-th word.

The persistence parameters αt and βt are esti-
mated by maximizing the joint likelihood eq.(4),
using a fixed point iteration (Minka, 2000). The
update formulas are as follows:

αnewt,u = αt,u

∑
k
ϕ̂t−1,u,kAt,u,k

Ψ(ct,u + αt,u)−Ψ(αt,u) ,
(8)

where At,u,k = Ψ(ct,u,k + αt,uϕ̂t−1,u,k) −
Ψ(αt,uϕ̂t−1,u,k), and

βnewt,k = βt,k

∑
v
θ̂t−1,k,vBt,k,v

Ψ(nt,k + βt,k)−Ψ(βt,k) ,
(9)

where Bt,k,v = Ψ(nt,k,v + βt,kθ̂t−1,k,v) −
Ψ(βt,kθ̂t−1,k,v). We can estimate latent variables
Zt, Yt, and parameters αt and βt by iterating
Gibbs sampling with eq.(5), eq.(6), and eq.(7) and
maximum joint likelihood with eq.(8) and eq.(9).
After the iterations, the means of ϕt,u,k and θt,k,v
are obtained as follows.

ϕ̂t,u,k =
ct,u,k + αt,uϕ̂t−1,u,k

ct,u + αt,u
, (10)

θ̂t,k,v =
nt,k,v + βt,kθ̂t−1,k,v

nt,k + βt,k
. (11)

These estimates are used as the hyper parameters
of the prior distributions at the next time period
t + 1.

4 Related Work

Recently, topic models for Twitter have been pro-
posed. Diao et al. (2012) proposed a topic
model that considers both the temporal informa-
tion of tweets and user’s personal interests. They
applied their model to find bursty topics from
Twitter. Yan et al. (2013) proposed a biterm
topic model (BTM), which assumes that a word-
pair is independently drawn from a specific topic.
They demonstrated that BTM can effectively cap-
ture the topics within short texts such as tweets
compared with LDA. Chua and Asur (2013) pro-
posed two topic models considering time order
and tweet intervals to extract the tweets summa-
rizing a given event. The models mentioned above
do not consider the dynamics of user interests, nor

1981



1. Draw θt,B ∼Dirichlet(λ)
2. For each topic k = 1, ...,K,

(a) draw θt,k ∼Dirichlet(βt,kθ̂t−1,k)
3. For each user u = 1, ..., U ,

(a) draw ϕt,u ∼Dirichlet(αt,uϕ̂t−1,u)
(b) draw πt,u ∼Beta(γ)
(c) for each tweet s = 1, ..., Nu

i. draw zt,u,s ∼Multinomial(ϕt,u)
ii. for each word n = 1, ..., Nu,s

A. draw yt,u,s,n ∼Bernoulli(πt,u)
B. draw wt,u,s,n ∼

Multinomial(θt,B) if yt,u,s,n = 0
or Multinomial(θt,zt,u,s)
if yt,u,s,n = 1

Figure 3: Generative process of tweets in Twitter-
TTM

do they have the capability of online inference;
thus, they cannot efficiently model the large num-
ber of tweets generated everyday, whereas Twitter-
TTM can capture the dynamics of user interests
and topic trends and has the capability of online
inference.

Some online topic models have also been pro-
posed. TM-LDA was proposed by Wang et al.
(2012), which can efficiently model online the top-
ics and topic transitions that naturally arise in a
tweet stream. Their model learns the transition
parameters among topics by minimizing the pre-
diction error on topic distribution in subsequent
tweets. However, the TM-LDA does not con-
sider dynamic word distributions. In other words,
their model can not capture the dynamics of topic
trends. Lau et al. (2012) proposed a topic model
implementing a dynamic vocabulary based on on-
line LDA (OLDA) (AlSumait et al., 2008) and ap-
plied it to track emerging events on Twitter. An
online variational Bayes algorithm for LDA is also
proposed (Hoffman et al., 2010). However, these
methods are based on LDA and do not consider
the shortness of a tweet. Twitter-TTM tackles
the shortness of a tweet by assuming that a single
tweet consists of a single topic. This assumption
is based on the following observation: a tweet is
much shorter than a normal document, so a single
tweet rarely contains multiple topics but rather a
single one.

w 

α 

θk 

K 

Nu,s 

Φ 

z 

U 

Nu 

y 

π 

θB
 w 

θk
 

K 

Nu,s 

Φ 

z 

U 

Nu 

y 

π 

θB
 

λ 

α 

γ γ 

α 

λ 

β β 

t-1 t 

Figure 4: Graphical model of Twitter-TTM

5 Experiment

5.1 Setting

We evaluated the effectiveness of the proposed
Twitter-TTM using an actual Twitter data set. The
original Twitter data set contains 15,962 users and
4,146,672 tweets collected from October 18 to 31,
2013. We then removed words that occurred less
than 30 times and stop words. After this prepro-
cessing, we obtained the final data set with 15,944
users, 3,679,481 tweets, and 30,096 vocabularies.

We compared the predictive performance of
Twitter-TTM with LDA, TTM, Twitter-LDA,
Twitter-LDA+TTM, and the improved model
based on the perplexity for the next time tweets.
Twitter-LDA+TTM is a combination of Twitter-
LDA and TTM. It is equivalent to Twitter-TTM,
except that the rate between background and topic
words is different for each user. We set the num-
ber of topics K at 100, the iteration of each model
at 500, and the unit time interval at one day. The
hyper parameters in these models were optimized
in each Gibbs sampling iteration by maximizing
likelihood using fixed iterations (Minka, 2000).
The inferences of LDA, Twitter-LDA, and the im-
proved model were made for current time tweets.

5.2 Result

Figure 5 shows the perplexity of each model for
each time, where t = 1 in the horizontal axis rep-
resents October 18, t = 2 represents October 19,
..., and t = 13 represents October 31. The perplex-
ity at time t represents the predictive performance
of each model inferred by previous time tweets to

1982



the current time tweets. Note that at t = 1, the per-
formance of LDA and TTM, that of Twitter-LDA
and Twitter-LDA+TTM, and that of Twitter-TTM
and the improved model were found to be equiva-
lent.

As shown in Figure 5(a), the proposed Twitter-
TTM shows lower perplexity compared with con-
ventional models, such as LDA, Twitter-LDA, and
TTM at any time, which implies that Twitter-TTM
can appropriately model the dynamics of user in-
terests and topic trends in Twitter. TTM could
not have perplexity lower than LDA although it
considers the dynamics. If LDA could not ap-
propriately model the tweets, then the user inter-
ests Φ̂t−1 and topic trends Θ̂t−1 in the previous
time are not estimated well in TTM. Figure 5(b)
shows the perplexities of the improved model and
Twitter-TTM. From t = 2, Twitter-TTM shows
lower perplexity than the improved model for each
time. The reason for the high perplexity of the im-
proved model is that it does not consider the dy-
namics. Twitter-TTM also shows lower perplexity
than Twitter-LDA+TTM for each time, as shown
in Figure 5(c), because Twitter-TTM’s assumption
that the rate between background and topic words
is different for each user is more appropriate, as
demonstrated in Section 2.2. These results imply
that Twitter-TTM also outperforms other conven-
tional methods, such as DTM, OLDA, and TM-
LDA, which do not consider the shortness of a
tweet or the dynamics of user interests or topic
trends .

Table 3 shows two topic examples of the topic
evolution analyzed by Twitter-TTM, and Figure 6
shows the trend persistence parameters β of each
topic at each time. The persistence parameters of
the topic “Football” are lower than those of “Birth-
day” because it is strongly affected by trends in the
real world. In fact, the top words in “Football”
change more dynamically than those of “Birth-
day.” For example, in the “Football” topic, though
‘Arsenal’ is usually popular, ‘Madrid’ becomes
more popular on October 24.

6 Conclusion

We first proposed an improved model based
on Twitter-LDA, which estimates the rate be-
tween background and topic words for each user.
We demonstrated that the improved model could
model tweets more efficiently than LDA and
Twitter-LDA. Next we proposed a novel proba-

1000 

1500 

2000 

2500 

3000 

3500 

4000 

4500 

1 2 3 4 5 6 7 8 9 10 11 12 13 

p
er

p
le

xi
ty

 

t 

LDA 

Twitter-LDA 

TTM 

Twitter-TTM 

(a) Comparison with LDA, Twitter-LDA, and TTM

1000 

1500 

2000 

2500 

3000 

3500 

4000 

4500 

1 2 3 4 5 6 7 8 9 10 11 12 13 

p
er

p
le

xi
ty

 

t 

Improved-Model 

Twitter-TTM 

(b) Comparison with Improved-model

1000 

1500 

2000 

2500 

3000 

3500 

4000 

4500 

1 2 3 4 5 6 7 8 9 10 11 12 13 

p
er

p
le

xi
ty

 

t 

Twitter-LDA+TTM 

Twitter-TTM 

(c) Comparison with Twitter-LDA+TTM

Figure 5: Perplexity for each time

bilistic topic model for Twitter, called Twitter-
TTM, which can capture the dynamics of user in-
terests and topic trends and is capable of online
inference. We evaluated Twitter-TTM using an ac-
tual Twitter data set and demonstrated that it could
model more accurately tweets than conventional

1983



methods.
The proposed method currently needs to prede-

termine the number of topics each time, and it is
fixed. In future work, we plan to extend the pro-
posed method to capture the birth and death of
topics along the timeline with a variable number
of topics, such as the model proposed by Ahmed
(Ahmed and Xing, 2010). We also plan to ap-
ply the proposed method to content recommenda-
tions and trend analysis in Twitter to investigate
this method further.

References

Amr Ahmed and Eric P. Xing. 2010. Timeline: A
dynamic hierarchical Dirichlet process model for re-
covering birth/death and evolution of topics in text
stream. In Proceedings of the 26th Conference on
Uncertainty in Artificial Intelligence (UAI), 20–29.

Loulwah AlSumait, Daniel Barbará and Carlotta
Domeniconi. 2008. On-line LDA: Adaptive topic
models for mining text streams with applications
to topic detection and tracking. In Proceedings of
the IEEE International Conference on Data Mining
(ICDM), 3-12.

David M. Blei., and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd Inter-
national Conference on Machine learning (ICML),
113-120.

David M. Blei, Andrew Y. Ng and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3: 993-1022.

Freddy C. T. Chua and Sitaram Asur. 2013. Automatic
summarization of events from social media. In Pro-
ceedings of the International AAAI Conference on
Weblogs and Social Media (ICWSM).

Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim
2012. Finding bursty topics from microblogs. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), 536–
544.

Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(1):5228-5235.

Matthew D. Hoffman, Francis Bach and David M. Blei.
2010. Online learning for latent dirichlet allocation.
In Proceedings of the Advances in Neural Informa-
tion Processing Systems (NIPS), 856–864.

Liangjie Hong and Brian D. Davison. 2010. Empiri-
cal study of topic modeling in twitter. In Proceed-
ings of the First Workshop on Social Media Analyt-
ics (SOMA), 80–88.

Tomoharu Iwata, Shinji Watanabe, Takeshi Yamada.
and Naonori Ueda. 2009. Topic tracking model for
analyzing consumer purchase behavior. In Proceed-
ings of the International Joint Conferences on Arti-
ficial Intelligence (IJCAI),1427–1432.

JeyHan Lau, Nigel Collier and Timothy Baldwin.
2012. On-line trend analysis with topic models:#
twitter trends detection topic model online. In Pro-
ceedings of the 23th International Conference on
Computational Linguistics (COLING), 1519–1534.

Thomas P. Minka 2000. Estimating a Dirichlet distri-
bution Technical report, MIT.

Marco Pennacchiotti and Ana-Maria Popescu. 2011.
A machine learning approach to Twitter user clas-
sification. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM),
281–288.

Takeshi Sakaki, Makoto Okazaki and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: realtime
event detection by social sensors. In Proceedings of
the World Wide Web Conference (WWW), 851–860.

Hanna M. Wallach 2006. Topic modeling: beyond
bag-of-words. In Proceedings of the 23rd Inter-
national Conference on Machine Learning (ICML),
977–984.

Xuerui Wang and Andrew McCallum. 2006. Topics
over time: a non-Markov continuous-time model of
topical trends. In Proceedings of the International
Conference on Knowledge Discovery and Data Min-
ing (KDD), 424–433.

Yu Wang, Eugene Agichtein and Michele Benz. 2012.
TM-LDA: efficient online modeling of the latent
topic transitions in social media. In Proceedings of
the International Conference on Knowledge Discov-
ery and Data Mining (KDD), 123–131.

Jianshu Weng, Ee Peng Lim, Jing Jiang and Qi He.
2010. Twitterrank: finding topic-sensitive influen-
tial twitterers. In Proceedings of the 3rd ACM Inter-
national Conference on Web Search and Data Min-
ing (WSDM), 261–270.

Xiaohui Yan, Jiafeng Guo, Yanyan Lan and Xueqi
Cheng 2013. A biterm topic model for short texts.
In Proceedings of the World Wide Web Conference
(WWW), 1445–1456.

Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In Advances in Information Retrieval, 338–
349.

1984



0 

100 

200 

300 

400 

500 

600 

10/19 10/20 10/21 10/22 10/23 10/24 

β

 

date 

Birthday 

Football 

Figure 6: Trend persistence parameters β of each topic at each time estimated by Twitter-TTM

Table 3: Two examples of topic evolution analyzed by Twitter-TTM
Label Date Top words
Birthday 10/18 birthday,happy,maria,hope,good,love,thanks,bday,lovely,enjoy

10/19 happy,birthday,good,hope,thank,enjoy,love,bday,lovely,great
10/20 birthday,happy,hope,good,love,lovely,great,enjoy,thank,beautiful
10/21 birthday,happy,hope,good,beautiful,love,lovely,bday,great,thank
10/22 birthday,happy,hope,good,beautiful,love,bless,thank,today,bday
10/23 birthday,happy,thank,good,love,hope,beautiful,enjoy,channing,wish
10/24 birthday,happy,thank,love,hope,good,beautiful,fresh,thanks,jamz

Football 10/18 arsenal,ozil,game,team,cazorla,league,wenger,play,season,good
10/19 goal,liverpool,gerrard,arsenal,ozil,league,newcastle,suarez,goals,team
10/20 arsenal,ozil,goal,ramsey,norwich,goals,league,wilshere,mesut,premier
10/21 arsenal,goal,goals,league,townsend,spurs,player,season,wenger,ozil
10/22 arsenal,goal,wenger,ozil,league,arsene,goals,birthday,happy,team
10/23 arsenal,dortmund,ozil,fans,wilshere,borussia,ramsey,lewandowski,giroud,league
10/24 madrid,goals,ronaldo,cska,real,league,city,moscow,champions,yaya

1985


