










































Authorship Attribution of Micro-Messages


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1880–1891,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Authorship Attribution of Micro-Messages

Roy Schwartz Oren Tsur Ari Rappoport

Institute of Computer Science
Hebrew University of Jerusalem

{roys02|oren|arir}@cs.huji.ac.il

Moshe Koppel

Department of Computer Science
Bar Ilan University

koppel@macs.biu.ac.il

Abstract

Work on authorship attribution has tradition-

ally focused on long texts. In this work, we

tackle the question of whether the author of

a very short text can be successfully iden-

tified. We use Twitter as an experimental

testbed. We introduce the concept of an au-

thor’s unique “signature”, and show that such

signatures are typical of many authors when

writing very short texts. We also present a new

authorship attribution feature (“flexible pat-

terns”) and demonstrate a significant improve-

ment over our baselines. Our results show that

the author of a single tweet can be identified

with good accuracy in an array of flavors of

the authorship attribution task.

1 Introduction

Research in authorship attribution has developed

substantially over the last decade (Stamatatos,

2009). The vast majority of such research has been

dedicated towards finding the author of long texts,

ranging from single passages to book chapters. In

recent years, the growing popularity of social me-

dia has created special interest, both theoretical and

computational, in short texts. This has led to many

recent authorship attribution projects that experi-

mented with web data such as emails (Abbasi and

Chen, 2008), web forum messages (Solorio et al.,

2011) and blogs (Koppel et al., 2011b). This paper

addresses the question to what extent the authors of

very short texts can be identified. To answer this

question, we experiment with Twitter tweets.

Twitter messages (tweets) are limited to 140 char-

acters. This restriction imposes major difficulties on

authorship attribution systems, since authorship at-

tribution methods that work well on long texts are

often not as useful when applied to short texts (Bur-

rows, 2002; Sanderson and Guenter, 2006).

Nonetheless, tweets are relatively self-contained

and have smaller sentence length variance com-

pared to excerpts from longer texts (see Section 3).

These characteristics make Twitter data appealing as

a testbed when focusing on short texts. Moreover,

an authorship attribution system of tweets may have

various applications. Specifically, a range of cyber-

crimes can be addressed using such a system, includ-

ing identity fraud and phishing.

In this paper, we introduce the concept of k-
signatures. We denote the k-signatures of an author
a as the features that appear in at least k% of a’s
training samples, while not appearing in the training

set of any other author. When k is large, such signa-
tures capture a unique style used by a. An analysis
of our training set reveals that unique k-signatures
are typical of many authors. Moreover, a substantial

portion of the tweets in our training set contain at

least one such signature. These findings suggest that

a single tweet, although short and sparse, often con-

tains sufficient information for identifying its author.

Our results show that this is indeed the case.

We train an SVM classifier with a set of features

that include character n-grams and word n-grams.

We use a rigorous experimental setup, with varying

number of authors (values between 50-1,000) and

various sizes of the training set, ranging from 50 to

1,000 tweets per author. In all our experiments, a

single tweet is used as test document. We also use

a setting in which the system is allowed to respond

don’t know in cases of uncertainty. Applying this

option results in higher precision, at the expense of

1880



lower recall.

Our results show that the author of a tweet can be

successfully identified. For example, when using a

dataset of as many as 1,000 authors with 200 train-

ing tweets per author, we are able to obtain 30.3%

accuracy (as opposed to a random baseline of only

0.1%). Using a dataset of 50 authors with as few

as 50 training tweets per author, we obtain 50.7%

accuracy. Using a dataset of 50 authors with 1,000

training tweets per author, our results reach as high

as 71.2% in the standard classification setting, and

exceed 91% accuracy with 60% recall in the don’t

know setting.

We also apply a new set of features, never previ-

ously used for this task – flexible patterns. Flexi-

ble patterns essentially capture the context in which

function words are used. The effectiveness of func-

tion words as authorship attribution features (Koppel

et al., 2009) suggests using flexible pattern features.

The fact that flexible patterns are learned from plain

text in a fully unsupervised manner makes them

domain and language independent. We demon-

strate that using flexible patterns gives significant

improvement over our baseline system. Further-

more, using flexible patterns, our system obtains a

6.1% improvement over current state-of-the-art re-

sults in authorship attribution on Twitter.

To summarize, the contribution of this paper is

threefold.

• We provide the most extensive research to date
on authorship attribution of micro-messages,

and show that authors of very short texts can

be successfully identified.

• We introduce the concept of an author’s unique
k-signature, and demonstrate that such signa-
tures are used by many authors in their writing

of micro-messages.

• We present a new feature for authorship attri-
bution – flexible patterns – and show its sig-

nificant added value over other methods. Us-

ing this feature, our system obtains a 6.1% im-

provement over the current state-of-the-art.

The rest of the paper is organized as follows. Sec-

tions 2 and 3 describe our methods and our experi-

mental testbed (Twitter). Section 4 presents the con-

cept of k-signatures. Sections 5 and 6 present our

experiments and results. Flexible patterns are pre-

sented in Section 7 and related work is presented in

Section 8.

2 Methodology

In the following we briefly describe the main fea-

tures employed by our system. The features below

are binary features.

Character n-grams. Character n-gram features

are especially useful for authorship attribution on

micro-messages since they are relatively tolerant

to typos and non-standard use of punctuation (Sta-

matatos, 2009). These are common in the non-

formal style generally applied in social media ser-

vices. Consider the example of misspelling “Brit-

ney” as “Brittney”. The misspelled name shares the

4-grams “Brit” and “tney” with the correct name. As

a result, these features provide information about the

author’s style (or at least her topic of interest), which

is not available through lexical features.

Following standard practice, we use 4-grams

(Sanderson and Guenter, 2006; Layton et al., 2010;

Koppel et al., 2011b). White spaces are considered

characters (i.e., a character n-gram may be com-

posed of letters from two different words). A sin-

gle white-space is appended to the beginning and

the end of each tweet. For efficiency, we consider

only character n-gram features that appear at least

tcng times in the training set of at least one author
(see Section 5).

Word n-grams. We hypothesize that word n-gram

features would be useful for authorship attribution

on micro-messages. We assume that under a strict

length restriction, many authors would prefer using

short, repeating phrases (word n-grams).

In our experiments, we consider 2 ≤ n ≤ 5.1

We regard sequences of punctuation marks as words.

Two special words are added to each tweet to indi-

cate the beginning and the end of the tweet. For effi-

ciency, we consider only word n-gram features that

appear at least twng times in the training set of at
least one author (see Section 5).

Model. We use libsvm’s Matlab implementation

of a multi-class SVM classifier with a linear kernel

1We skip unigrams as they are generally captured by the

character n-gram features.

1881



(Chang and Lin, 2011). We use ten-fold cross vali-

dation on the training set to select the best regular-

ization factor between 0.5 and 0.005.2

3 Experimental Testbed

Our main research question in this paper is to deter-

mine the extent to which authors of very short texts

can be identified. A major issue in working with

short texts is selecting the right dataset. One ap-

proach is breaking longer texts into shorter chunks

(Sanderson and Guenter, 2006). We take a differ-

ent approach and experiment with micro-messages

(specifically, tweets).

Tweets have several properties making them an

ideal testbed for authorship attribution of short texts.

First, tweets are posted as single units and do not

necessarily refer to each other. As a result, they tend

to be self contained. Second, tweets have more stan-

dardized length distribution compared to other types

of web data. We compared the mean and standard

deviation of sentence length in our Twitter dataset

and in a corpus of English web data (Ferraresi et al.,

2008).3 We found that (a) tweets are shorter than

standard web data (14.2 words compared to 20.9),

and (b) the standard deviation of the length of tweets

is much smaller (6.4 vs. 21.4).

Pre-Processing. We use a Twitter corpus that in-

cludes approximately 5 × 108 tweets.4 All non-
English tweets and tweets that contain fewer than

3 words are removed from the dataset. We also re-

move tweets marked as retweets (using the RT sign,

a standard Twitter symbol to indicate that this tweet

was written by a different user). As some users

retweet without using the RT sign, we also remove

tweets that are an exact copy of an existing tweet

posted in the previous seven days.

Apart from plain text, some tweets contain ref-

erences to other Twitter users (in the format of

@<user>). Since using reference information
makes this task substantially easier (Layton et al.,

2010), we replace each user reference with the spe-

cial meta tag REF. For sparsity reasons, we also re-

place web addresses with the meta tag URL, num-

2In practice, 0.05 or 0.1 are selected in almost all cases.
3
http://wacky.sslmit.unibo.it

4These comprise ∼15% of all public tweets created from

May 2009 to March 2010.

0 5 10 15 20 25 30 35 40 45 >50
0

10

20

30

40

50

60

70

80

90

Number of k−signatures per user

N
um

be
r 

of
 U

se
rs

 

 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%

Figure 1: Number of users with at least x k-signatures
(100 authors, 180 training tweets per author).

bers with the meta tag NUM, time of day with the

meta tag TIME and dates with the meta tag DATE.

4 k-Signatures

In this section, we show that many authors adopt

a unique style when writing micro-messages. This

style can be detected by a strong classification algo-

rithm (such as SVM), and be sufficient to correctly

identify the author of a single tweet.

We define the concept of the k-signature of an au-
thor a to be a feature that appears in at least k% of
a’s training set, while not appearing in the training
set of any other user. Such signatures can be useful

for identifying future (unlabeled) tweets written by

a.

To validate our hypothesis, we use a dataset of

100 authors with 180 tweets per author. We com-

pute the number of k-signatures used by each of
the authors in our dataset. Figure 1 shows our re-

sults for a range of k values (2%, 5%, 10%, 20%
and 50%). Results demonstrate that 81 users use

at least one 2%-signature, 43 users use at least one
5%-signature, and 17 users use at least one 10%-
signature. These results indicate that a large portion

of the users adopt a unique signature (or set of sig-

natures) when writing short texts. Table 1 provides

examples of 10%-signatures.

1882



Signature Type 10%-signature Examples

Character n-grams

‘ ˆ ˆ’

REF oh ok ˆ ˆ Glad you found it!

Hope everyone is having a good afternoon ˆ ˆ

REF Smirnoff lol keeping the goose in the freezer ˆ ˆ

‘yew ’

gurl yew serving me tea nooch

REF about wen yew and ronnie see each other

REF lol so yew goin to check out tini’s tonight huh???

Word n-grams

.. lal

REF aww those are cool where u get those.. how do ppl react.. lal

Ludas album is gone be hott.. lal

Dayum refs don’t get injury timeouts.. lal.. get him off the field..

smoochies , e3

I’m just back after takin’ a very long, icy cold

shower........Shivering smoochies,E3 http://bit.ly/4CzzP9

A blue stout or two would be nice as well, Purr!Blue smooth

smoochies,E3 http://bit.ly/75D4fO

That is sooooooooooooooooooo unfair!Double smoochies,E3

http://bit.ly/07sXRGX

Table 1: Examples of 10%-signatures.

Results also show that seven users use one or

more 20%-signatures, and five users even use one
or more 50%-signatures. Looking carefully at these
users, we find that they write very structured mes-

sages, and are probably bots, such as news feeds,

bidding systems, etc. Table 2 provides examples of

tweets posted by such users.5

Another interesting question is how many tweets

contain at least one k-signature. Figure 2 shows
for each user the number of tweets in her training

set for which at least one k-signature is found. Re-
sults demonstrate that a total of 18.6% of the train-

ing tweets contain at least one 2%-signature, 10.3%
the training tweets contain at least one 5%-signature
and 6.5% of the training tweets contain at least one

10%-signature. These findings validate our assump-
tion that many users use k-signatures in short texts.

These findings also have direct implications on

authorship attribution of micro-messages, since k-
signatures are reliable classification features. As

a result, texts written by authors that tend to use

k-signatures are likely to be easily identified by a
reasonable classification algorithm. Consequently,

k-signatures provide a possible explanation for the
high quality results presented in this paper.

In the broader context, the presence (and contri-

5Our k-signature method can actually be useful for automat-

ically identifying such users. We defer this to future work.

0 20 40 60 80 100 120 140 160 180
0

10

20

30

40

50

60

70

80

90

Number of Tweets with at least one k−Signature

N
um

be
r 

of
 U

se
rs

 

 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%

Figure 2: Number of users with at least x training tweets
that contain at least one k-signature (100 authors, 180
training tweets per author).

bution) of k-signatures is in line with the hypothesis
proposed by (Davidov et al., 2010a): while still us-

ing an informal and unstructured (grammatical) lan-

guage, authors tend to use typical and unique struc-

tures in order to allow a short message to stand alone

without a clear conversational context.

1883



User 20%-signature Examples

1 I’m listening to :

I’m listening to: Sigur R?s ? Intro:

http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHyb

I’m listening to: Tina Arena ? In Command:

http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25

I’m listening to: Midnight Oil ? Under the Overpass:

http://www.last.fm/music/Midnight+Oil http://bit.ly/7IH4cg

2 news now ( str )

#Hotel News Now(STR) 5 things to know: 27 May 2009: From the desks of

the HotelNewsNow.com editor... http://bit.ly/aZTZOq #Tourism #Lodging

#Hotel News Now(STR) Five sales renegotiating tactics: As bookings rep-

resentatives press to reneg... http://bit.ly/bHPn2L

#Hotel News Now(STR) Risk of hotel recession retreats: The Hotel Indus-

try’s Pulse Index increases... http://bit.ly/a8EKrm #Tourism #Lodging

3
( NUM bids )

end date :

NEW PINK NINTENDO DS LITE CONSOLE WITH 21 GIFTS +

CASE: &#163;66.50 (13 Bids) End Date: Tuesday Dec-08-2009 17:..

http://bit.ly/7uPt6V

Microsoft Xbox 360 Game System - Console Only - Working: US $51.99

(25 Bids) End Date: Saturday Dec-12-2009 13:.. http://bit.ly/8VgdTv

Microsoft Sony Playstation 3 (80 GB) Console 6 Months Old:

&#163;190.00 (25 Bids) End Date: Sunday Dec-13-2009 21:21:39 G..

http://bit.ly/7kwtDS

Table 2: Examples of tweets published by very structured users, suspected to be bots, along with one of their 20%-
signatures.

5 Experiments

We report of three different experimental configu-

rations. In the experiments described below, each

dataset is divided into training and test sets using

ten-fold cross validation. On the test phase, each

document contains a single tweet.

Experimenting with varying Training Set Sizes.

In order to test the affect of the training set size,

we experiment with an increasingly larger number

of tweets per author. Experimenting with a range of

training set sizes serves two purposes: (a) to check

whether the author of a tweet can be identified us-

ing a very small number of (short) training samples,

and (b) check how much our system can benefit from

training on a larger corpus.

In our experiments we only consider users who

posted between 1,000–2,000 tweets6 (a total of

6This range is selected since on one hand we want at least

1,000 tweets per author for our experiments, and on the other

hand we noticed that users with a larger number of tweets in

corpus tend to be spammers or bots that are very easy to identify,

so we limit this number to 2,000.

10,183 users), and randomly select 1,000 tweets per

user. From these users, we select 10 groups of 50

users each.7 We perform a set of classification ex-

periments, selecting for each author an increasingly

larger subset of her 1,000 tweets as training set. Sub-

set sizes are (50, 100, 200, 500, 1,000). Thresh-

old values for our features in each setting (see Sec-

tion 2) are (2, 2, 4, 10, 20) for tcng and (2, 2, 2, 3, 5)
for twng, respectively.

Experimenting with varying Numbers of Au-

thors. In a second set of experiments, we use an

increasingly larger number of authors (values be-

tween 100-1,000), in order to check whether the au-

thor of a very short text can be identified in a “needle

in a haystack” type of setting.

Due to complexity issues, we only experiment

with 200 tweets per author as training set. We se-

lect groups of size 100, 200, 500 and 1,000 users

(one group per size). We use the same threshold val-

ues as the 200 tweets per author setting previously

described (tcng = 4, twng = 2).

7An eleventh group is selected as development set.

1884



0 100 200 300 400 500 600 700 800 900 1000
45

50

55

60

65

70

Training Set Size

A
cc

ur
ac

y 
(%

)

 

 

Char. N−grams + Word N−grams
Char. N−grams

Figure 3: Authorship attribution accuracy for 50 authors

with various training set sizes. The values are averaged

over 10 groups. The random baseline is 2%.

Recall-Precision Tradeoff. Another aspect of our

research question is the level of certainty our system

has when suggesting an author for a given tweet.

In cases of uncertainty, many real life applications

would prefer not to get any response instead of get-

ting a response with low certainty. Moreover, in real

life applications we are often not even sure that the

real author is part of our training set. Consequently,

we allow our system to respond “don’t know” in

cases of low confidence (Koppel et al., 2006; Kop-

pel et al., 2011b). This allows our system to obtain

higher precision, at the expense of lower recall.

To implement this feature, we use SVM’s proba-

bility estimates, as implemented in libsvm. These

estimates give a score to each potential author.

These scores reflect the probability that this author

is the correct author, as decided by the prediction

model. The selected author is always the one with

the highest probability estimate.

As selection criterion, we use a set of increasingly

larger thresholds (0.05-0.9) for the probability of the

selected author. This means that we do not select test

samples for which the selected author has a proba-

bility estimate value lower than the threshold.

0 100 200 300 400 500 600 700 800 900 1000
25

30

35

40

45

50

55

60

Number of Candidate Authors

A
cc

ur
ac

y 
(%

)

 

 
Char. N−grams + Word N−grams
Char. N−grams

Figure 4: Authorship attribution accuracy with varying

number of candidate authors, using 200 training tweets

per author. The random baselines for 509, 100, 200, 500

and 1,000 authors are 2%, 1%, 0.5%, 0.2% and 0.1%,

respectively.

6 Basic Results

Experimenting with varying Training Set Sizes.

Figure 3 shows results for our experiments with

50 authors and various training set sizes. Results

demonstrate that authors of very short texts can be

successfully identified, even with as few as 50 tweets

per author (49.5%). When given more training sam-

ples, authors are identified much more accurately

(up to 69.7%). Results also show that, according to

our hypothesis, word n-gram features substantially

improve over character n-grams features only (3%

averaged improvement over all settings).

Experimenting with varying Numbers of Au-

thors. Figure 4 shows our results for various num-

bers of authors, using 200 tweets per author as train-

ing set. Results demonstrate that authors of an

unknown tweet can be identified to a large extent

even when there are as many as 1,000 candidate au-

thors (30.3%, as opposed to a random baseline of

only 0.1%). Results further validate that word n-

gram features substantially improve over character

9Results for 50 authors with 200 tweets per author are taken

from Figure 3.

1885



0 10 20 30 40 50 60 70 80 90 100
40

50

60

70

80

90

100

Recall (%)

P
re

ci
si

on
 (

%
)

 

 

1,000 tweets/author
500 tweets/author
200 tweets/author
100 tweets/author
50 tweets/author

Figure 5: Recall-precision curves for 50 authors with

varying training set sizes.

n-grams features (2.6% averaged improvement).

Recall-Precision Tradeoff. Figure 5 shows the

recall-precision curves for our experiments with 50

authors and varying training set sizes. Results

demonstrate that we are able to obtain very high pre-

cision (over 90%) while still maintaining a relatively

high recall (from ∼35% recall for 50 tweets per au-
thor up to > 60% recall for 1,000 tweets per author).

Figure 6 shows the recall-precision curves for our

experiments with varying number of authors. Re-

sults demonstrate that even in the 1,000 authors set-

ting, we are able to obtain high precision values

(90% and 70%) with reasonable recall values (18%

and ∼30%, respectively).

7 Flexible Patterns

In previous sections we provided strong evidence

that authors of micro-messages can be successfully

identified using standard methods. In this section we

present a new feature, never previously used for this

task – flexible patterns. We show that flexible pat-

terns can be used to improve classification results.

Flexible patterns are a generalization of word n-

grams, in the sense that they capture potentially un-

seen word n-grams. As a result, flexible patterns

can pick up fine-grained differences between au-

thors’ styles. Unlike other types of pattern features,

0 10 20 30 40 50 60 70 80 90 100
30

40

50

60

70

80

90

100

Recall (%)

P
re

ci
si

on
 (

%
)

 

 

50 authors
100 authors
200 authors
500 authors
1,000 authors

Figure 6: Recall-precision curves for varying number of

authors.

flexible patterns are computed automatically from

plain text. As such, they can be applied to various

tasks, independently of domain and language. We

describe them in detail.

Word Frequency. Flexible patterns are composed

of high frequency words (HFW) and content words

(CW). Every word in the corpus is defined as either

HFW or CW. This clustering is performed by count-

ing the number of times each word appears in the

corpus of size s. A word that appears more than
10−4×s times in a corpus is considered HFW. A
word that appears less than 10−3×s times in a cor-
pus is considered CW. Some words may serve both

as HFWs and CWs (see Davidov and Rappoport

(2008b) for discussion).

Structure of a Flexible Pattern. Flexible patterns

start and end with an HFW. A sequence of zero or

more CWs separates consecutive HFWs. At least

one CW must appear in every pattern.10 For effi-

ciency, at most six HFWs (and as a result, five CW

sequences) may appear in a flexible pattern. Exam-

ples of flexible patterns include

1. “theHFW CW ofHFW theHFW”

10Omitting this treats word n-grams as flexible patterns.

1886



Flexible Pattern Features. Flexible patterns can

serve as binary classification features; a tweet

matches a given flexible pattern if it contains the

flexible pattern sequence. For example, (1) is

matched by (2).

2. “Go to theHFW houseCW ofHFW theHFW rising sun”

Partial Flexible Patterns. A flexible pattern may

appear in a given tweet with additional words not

originally found in the flexible pattern, and/or with

only a subset of the HFWs (Davidov et al., 2010a).

For example, (3) is a partial match of (1), since the

word “great” is not part of the original flexible pat-

tern. Similarly, (4) is another partial match of (1),

since (a) the word “good” is not part of the original

flexible pattern and (b) the second occurrence of the

word “the” does not appear in (4) (missing word is

marked by ).

3. “TheHFW greatHFW kingCW ofHFW theHFW ring”

4. “TheHFW goodHFW kingCW ofHFW Spain”

We use such cases as features with lower weight,

proportional to the number of found HFWs in the

tweet (w =
0.5×nfound
nexpected

). For example, (1) receives a

weight of 1 (complete match) against (2). Against

(3), it receives a weight of 0.5 (= 0.5×3
3

, partial

match with no missing HFWs). Against (4) it re-

ceives a weight of 1/3 (= 0.5×2
3

, partial match with

only 2/3 HFWs found).

Experimenting with Flexible Pattern Features.

We repeat our experiments with varying training set

sizes (see Section 5) with two more systems: one

that uses character n-grams and flexible pattern fea-

tures, and another that uses character n-grams, word

n-grams and flexible patterns. High frequency word

counts are computed separately for each author us-

ing her training set. We only consider flexible pat-

tern features that appear at least tfp times in the
training set of at least one author. Values of tfp for
training set sizes (50, 100, 200, 500, 1,000) are (2,

3, 7, 7, 8), respectively.

Results. Figure 7 shows our results. Results

demonstrate that flexible pattern features have an

added value over both character n-grams alone (av-

eraged 2.9% improvement) and over character n-

grams and word n-grams together (averaged 1.5%

0 100 200 300 400 500 600 700 800 900 1000
35

40

45

50

55

60

65

70

75

Training Set Size

A
cc

ur
ac

y 
(%

)

 

 

Char. N−grams, Word N−grams &
Flex. Patt. Feats.
Char. N−grams + Flex. Patt. Feats.
Char. N−grams + Word N−grams
Char. N−grams
SCAP
Naive Bayes

Figure 7: Authorship attribution accuracy for 50 authors

with various training set sizes and various feature sets.

The values are averaged over 10 groups. The random

baseline is 2%.

Comparison to previous work: SCAP – SCAP algo-

rithm results, as reported by (Layton et al., 2010), Naive

Bayes – Naive Bayes algorithm results, as reported by

(Boutwell, 2011).

improvement). We perform t-tests on each of our
training set sizes to check whether the latter im-

provement is significant. Results demonstrate that

it is highly significant in all settings, with p-values
smaller than values between 10−3 (for 50 tweets per

author) and 10−8 (1,000 tweets per author).

Comparison to Previous Works. Figure 7 also

shows results for the only two works that experi-

mented in some of the settings we experimented in:

Layton et al. (2010) and Boutwell (2011) (see Sec-

tion 8). Our system substantially outperforms these

two systems, by margins of 5.9% to 19%. These

margins are explained by the choice of algorithm

(SVM and not SCAP/naive Bayes) and our set of

features (character n-grams + word n-grams + flex-

ible patterns compared to character n-grams only).

In order to rule out the possibility that these mar-

gins stem from using different datasets, we tested

our system on the dataset used in (Layton et al.,

2010). Our system obtains even higher results on

this dataset than on our datasets (61.6%, a total im-

1887



provement of 6.1% over (Layton et al., 2010)).

Discussion. To illustrate the additional contribu-

tion of flexible patterns over word n-grams, consider

the following tweets, written by the same author.

5. “. . . theHFW wayCW IHFW treatedCW herHFW”

6. “. . . half of theHFW thingsCW IHFW have seen”

7. “. . . theHFW friendsCW IHFW have had for years”

8. “. . . in theHFW neighborhoodCW IHFW grew up in”

Consider a case where (5) is part of the test set,

while (6-8) appear in the training set. As (5) shares

no sequence of words with (6-8), no word n-gram

feature is able to identify the author’s style in (5).

However, this style can be successfully identified us-

ing the flexible pattern (9), shared by (5-8).

9. theHFW CW IHFW

This demonstrates the added value flexible pattern

features have over word n-gram features.

8 Related Work

Authorship attribution dates back to the end of 19th

century, when (Mendenhall, 1887) applied sentence

length and word length features to plays of Shake-

speare. Ever since, many methods have been devel-

oped for this task. For recent surveys, see (Koppel

et al., 2009; Stamatatos, 2009; Juola, 2012).

Authorship attribution methods can be generally

divided into two categories (Stamatatos, 2009). In

similarity-based methods, an anonymous text is at-

tributed to some author whose writing style is most

similar (by some distance metric). In machine learn-

ing methods, which we follow in this paper, anony-

mous texts are classified, using machine learning al-

gorithms, into different categories (in this case, dif-

ferent authors).

Machine learning papers differ from each other by

the features and machine learning algorithm. Exam-

ples of features include HFWs (Mosteller and Wal-

lace, 1964; Argamon et al., 2007), character n-gram

(Kjell, 1994; Hoorn et al., 1999; Stamatatos, 2008),

word n-grams (Peng et al., 2004), part-of-speech

n-grams (Koppel and Schler, 2003; Koppel et al.,

2005) and vocabulary richness (Abbasi and Chen,

2005).

The various machine learning algorithms used in-

clude naive Bayes (Mosteller and Wallace, 1964;

Kjell, 1994), neural networks (Matthews and Mer-

riam, 1993; Kjell, 1994), K-nearest neighbors (Kjell

et al., 1995; Hoorn et al., 1999) and SVM (De Vel et

al., 2001; Diederich et al., 2003; Koppel and Schler,

2003).

Traditionally, authorship attribution systems have

mainly been evaluated against long texts such as

theater plays (Mendenhall, 1887), essays (Yule,

1939; Mosteller and Wallace, 1964), biblical books

(Mealand, 1995; Koppel et al., 2011a) and book

chapters (Argamon et al., 2007; Koppel et al., 2007).

In recent year, many works focused on web data

such as emails (De Vel et al., 2001; Koppel and

Schler, 2003; Abbasi and Chen, 2008), web forum

messages (Abbasi and Chen, 2005; Solorio et al.,

2011), blogs (Koppel et al., 2006; Koppel et al.,

2011b) and chat messages (Abbasi and Chen, 2008).

Some works focused on SMS messages (Mohan et

al., 2010; Ishihara, 2011).

Authorship Attribution on Twitter. The perfor-

mance of authorship attribution systems on short

texts is affected by several factors (Stamatatos,

2009). These factors include the number of candi-

date authors, the training set size and the size of the

test document.

Very few authorship attribution works experi-

mented with Twitter. Unlike our work, all used a

single group of authors (group sizes varied between

3-50). Layton et al. (2010) used the SCAP method-

ology (Frantzeskou et al., 2007) with character n-

gram features. They experimented with 50 authors

and compared different numbers of tweets per au-

thor (values between 20-200). Surprisingly, they

showed that their system does not improve when

given more training tweets. In our work, we no-

ticed a different trend, and showed that more data

can be extremely valuable for authorship attribution

systems on micro-messages (see Section 6). Silva

et al. (2011) trained an SVM classifier with various

features (e.g., punctuation and vocabulary features)

on a small dataset of three authors only, with vary-

ing training set size. Although their work used a

set of Twitter-specific features that we do not explic-

itly use, our features implicitly cover a large portion

of their features (such as punctuation and emoticon

1888



features, which are largely covered by character n-

grams).

Boutwell (2011) used a naive Bayes classifier

with character n-gram features. She experimented

with 50 authors and two training size values (120

and 230). She also provided a set of experiments that

studied the effect of joining several tweets into a sin-

gle document. Mikros and Perifanos (2013) trained

an SVM classifier with character n-gram and word

n-grams. They experimented with 10 authors of

Greek text, and also joined several tweets into a sin-

gle document. Joining several tweets into a longer

document is appealing since it can lead to substantial

improvement of the classification results, as demon-

strated by the works above. However, this approach

requires the test data to contain several tweets that

are known a-priori to be written by the same author.

This assumption is not always realistic. In our paper,

we intentionally focus on a single tweet as document

size.

Flexible Patterns. Patterns were introduced by

(Hearst, 1992), who used hand crafted patterns

to discover hyponyms. Hard coded patterns

were used for many tasks, such as discovering

meronymy (Berland and Charniak, 1999), noun cat-

egories (Widdows and Dorow, 2002), verb relations

(Chklovski and Pantel, 2004) and semantic class

learning (Kozareva et al., 2008).

Patterns were first extracted in a fully unsuper-

vised manner (“flexible patterns”) by (Davidov and

Rappoport, 2006), who used flexible patterns in or-

der to establish noun categories, and (Biciçi and

Yuret, 2006) who used them for analogy question

answering. Ever since, flexible patterns were used

as features for various tasks such as extraction of

semantic relationships (Davidov et al., 2007; Tur-

ney, 2008b; Bollegala et al., 2009), detection of

synonyms (Turney, 2008a), disambiguation of nom-

inal compound relations (Davidov and Rappoport,

2008a), sentiment analysis (Davidov et al., 2010b)

and detection of sarcasm (Tsur et al., 2010).

9 Conclusion

The main goal of this paper is to measure to what

extent authors of micro-messages can be identified.

We have shown that authors of very short texts

can be successfully identified in an array of au-

thorship attribution settings reported for long doc-

uments. This is the first work on micro-messages

to address some of these settings. We introduced

the concept of k-signature. Using this concept, we
proposed an interpretation of our results. Last, we

presented the first authorship attribution system that

uses flexible patterns, and demonstrated that using

these features significantly improves over other sys-

tems. Our system obtains 6.1% improvement over

the current state-of-the-art.

Acknowledgments

We would like to thank Elad Eban and Susan Good-

man for their helpful advice, as well as Robert Lay-

ton for providing us with his dataset. This research

was funded (in part) by the Harry and Sylvia Hoff-

man leadership and responsibility program (for the

first author) and the Intel Collaborative Research In-

stitute for Computational Intelligence (ICRI-CI).

References

Ahmed Abbasi and Hsinchun Chen. 2005. Applying au-

thorship analysis to extremist-group web forum mes-

sages. IEEE Intelligent Systems, 20:67–75.

Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints:

A stylometric approach to identity-level identification

and similarity detection in cyberspace. ACM Transac-

tions on Information Systems, 26(2):7:1–7:29.

Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-

han Raj Hota, Navendu Garg, and Shlomo Levitan.

2007. Stylistic text classification using functional lex-

ical features: Research articles. J. Am. Soc. Inf. Sci.

Technol., 58(6):802–822.

Matthew Berland and Eugene Charniak. 1999. Finding

parts in very large corpora. In Proc. of ACL, pages

57–64, College Park, Maryland, USA.

Ergun Biciçi and Deniz Yuret. 2006. Clustering word

pairs to answer analogy questions. In Proc. of TAINN,

pages 1–8.

Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru

Ishizuka. 2009. Measuring the similarity between

implicit semantic relations from the web. In Proc. of

WWW, New York, New York, USA. ACM Press.

Sarah R. Boutwell. 2011. Authorship Attribution of

Short Messages Using Multimodal Features. Master’s

thesis, Naval Postgraduate School.

John Burrows. 2002. ‘Delta’: a Measure of Stylistic

Difference and a Guide to Likely Authorship. Literary

and Linguistic Computing, 17(3):267–287.

1889



Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:

A library for support vector machines. ACM Transac-

tions on Intelligent Systems and Technology, 2:27:1–

27:27. Software available at http://www.csie.

ntu.edu.tw/˜cjlin/libsvm.

Timothy Chklovski and Patrick Pantel. 2004. Verbo-

cean: Mining the web for fine-grained semantic verb

relations. In Dekang Lin and Dekai Wu, editors, Proc.

of EMNLP, pages 33–40, Barcelona, Spain.

Dmitry Davidov and Ari Rappoport. 2006. Efficient un-

supervised discovery of word categories using sym-

metric patterns and high frequency words. In Proc.

of ACL-Coling, pages 297–304, Sydney, Australia.

Dmitry Davidov and Ari Rappoport. 2008a. Classifi-

cation of semantic relationships between nominals us-

ing pattern clusters. In Proceedings of ACL-08: HLT,

pages 227–235, Columbus, Ohio, June. Association

for Computational Linguistics.

Dmitry Davidov and Ari Rappoport. 2008b. Unsuper-

vised discovery of generic relationships using pattern

clusters and its evaluation by automatically generated

SAT analogy questions. In Proc. of ACL-HLT, pages

692–700, Columbus, Ohio.

Dmitry Davidov, Ari Rappoport, and Moshe Koppel.

2007. Fully unsupervised discovery of concept-

specific relationships by web mining. In Proc. of ACL,

pages 232–239, Prague, Czech Republic.

Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010a.

Semi-supervised recognition of sarcastic sentences in

twitter and amazon. In Proc. of CoNLL, pages 107–

116, Uppsala, Sweden.

Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b.

Enhanced sentiment learning using twitter hashtags

and smileys. In Proc. of Coling, pages 241–249, Bei-

jing, China.

Olivier De Vel, Alison Anderson, Malcolm Corney, and

George Mohay. 2001. Mining e-mail content for au-

thor identification forensics. ACM Sigmod Record,

30(4):55–64.

Joachim Diederich, Jörg Kindermann, Edda Leopold, and

Gerhard Paass. 2003. Authorship attribution with

support vector machines. Applied intelligence, 19(1-

2):109–123.

Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and

Silvia Bernardini. 2008. Introducing and evaluating

ukwac, a very large web-derived corpus of english. In

Proc. of the 4th Web as Corpus Workshop, WAC-4.

Georgia Frantzeskou, Efstathios Stamatatos, Stefanos

Gritzalis, and Carole E Chaski. 2007. Identifying au-

thorship by byte-level n-grams: The source code au-

thor profile (scap) method. Int Journal of Digital Evi-

dence, 6(1):1–18.

Marti A. Hearst. 1992. Automatic acquisition of hy-

ponyms from large text corpora. In Proc. of Coling

– Volume 2, pages 539–545, Stroudsburg, PA, USA.

Johan F Hoorn, Stefan L Frank, Wojtek Kowalczyk, and

Floor van der Ham. 1999. Neural network identifi-

cation of poets using letter sequences. Literary and

Linguistic Computing, 14(3):311–338.

Shunichi Ishihara. 2011. A forensic authorship clas-

sification in sms messages: A likelihood ratio based

approach using n-gram. In Proc. of the Australasian

Language Technology Association Workshop 2011,

pages 47–56, Canberra, Australia.

Patrick Juola. 2012. Large-scale experiments in author-

ship attribution. English Studies, 93(3):275–283.

Bradley Kjell, W Addison Woods, and Ophir Frieder.

1995. Information retrieval using letter tuples with

neural network and nearest neighbor classifiers. In

IEEE International Conference on Systems, Man and

Cybernetics, volume 2, pages 1222–1226. IEEE.

Bradley Kjell. 1994. Authorship determination using let-

ter pair frequency features with neural network classi-

fiers. Literary and Linguistic Computing, 9(2):119–

124.

Moshe Koppel and Jonathan Schler. 2003. Exploiting

stylistic idiosyncrasies for authorship attribution. In

Proc. of IJCAI’03 Workshop on Computational Ap-

proaches to Style Analysis and Synthesis, volume 69,

page 72.

Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.

Determining an author’s native language by mining a

text for errors. In Proc. of the eleventh ACM SIGKDD

international conference on Knowledge discovery in

data mining, KDD ’05, pages 624–628, New York,

NY, USA.

Moshe Koppel, Jonathan Schler, Shlomo Argamon, and

Eran Messeri. 2006. Authorship attribution with thou-

sands of candidate authors. In SIGIR, pages 659–660.

Moshe Koppel, Jonathan Schler, and Elisheva Bonchek-

Dokow. 2007. Measuring differentiability: Unmask-

ing pseudonymous authors. JMLR, 8:1261–1276.

Moshe Koppel, Jonathan Schler, and Shlomo Argamon.

2009. Computational methods in authorship attribu-

tion. J. Am. Soc. Inf. Sci. Technol., 60(1):9–26.

Moshe Koppel, Navot Akiva, Idan Dershowitz, and

Nachum Dershowitz. 2011a. Unsupervised decom-

position of a document into authorial components. In

Proc. of ACL-HLT, pages 1356–1364, Portland, Ore-

gon, USA.

Moshe Koppel, Jonathan Schler, and Shlomo Argamon.

2011b. Authorship attribution in the wild. Language

Resources and Evaluation, 45(1):83–94.

Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.

Semantic class learning from the web with hyponym

1890



pattern linkage graphs. In Proc. of ACL-HLT, pages

1048–1056, Columbus, Ohio.

Robert Layton, Paul Watters, and Richard Dazeley. 2010.

Authorship attribution for twitter in 140 characters or

less. In Proc. of the 2010 Second Cybercrime and

Trustworthy Computing Workshop, CTC ’10, pages 1–

8, Washington, DC, USA. IEEE Computer Society.

Robert AJ Matthews and Thomas VN Merriam. 1993.

Neural computation in stylometry i: An application to

the works of shakespeare and fletcher. Literary and

Linguistic Computing, 8(4):203–209.

DL Mealand. 1995. Correspondence analysis of luke.

Literary and linguistic computing, 10(3):171–182.

Thomas Corwin Mendenhall. 1887. The characteristic

curves of composition. Science, ns-9(214S):237–246.

George K Mikros and Kostas Perifanos. 2013. Author-

ship attribution in greek tweets using authors multi-

level n-gram profiles. In 2013 AAAI Spring Sympo-

sium Series.

Ashwin Mohan, Ibrahim M Baggili, and Marcus K

Rogers. 2010. Authorship attribution of sms mes-

sages using an n-grams approach. Technical report,

CERIAS Tech Report 2011.

Frederick Mosteller and David Lee Wallace. 1964.

Inference and disputed authorship: The Federalist.

Addison-Wesley.

Fuchun Peng, Dale Schuurmans, and Shaojun Wang.

2004. Augmenting naive bayes classifiers with sta-

tistical language models. Information Retrieval, 7(3-

4):317–345.

Conrad Sanderson and Simon Guenter. 2006. Short text

authorship attribution via sequence kernels, markov

chains and author unmasking: An investigation. In

Proc. of EMNLP, pages 482–491, Sydney, Australia.

Rui Sousa Silva, Gustavo Laboreiro, Luı́s Sarmento, Tim

Grant, Eugénio Oliveira, and Belinda Maia. 2011.

‘twazn me!!! ;(’ automatic authorship analysis of

micro-blogging messages. In Proc. of the 16th inter-

national conference on Natural language processing

and information systems, NLDB’11, pages 161–168,

Berlin, Heidelberg. Springer-Verlag.

Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and

Manuel Montes-Gomez. 2011. Modality specific

meta features for authorship attribution in web forum

posts. In Proc. of IJCNLP, pages 156–164, Chiang

Mai, Thailand, November.

Efstathios Stamatatos. 2008. Author identification: Us-

ing text sampling to handle the class imbalance prob-

lem. Inf. Process. Manage., 44(2):790–799.

Efstathios Stamatatos. 2009. A survey of modern au-

thorship attribution methods. Journal of the Ameri-

can Society for Information Science and Technology,

60(3):538–556.

Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.

Icwsm–a great catchy name: Semi-supervised recog-

nition of sarcastic sentences in online product reviews.

In Proc. of ICWSM.

Peter Turney. 2008a. A uniform approach to analogies,

synonyms, antonyms, and associations. In Proc. of

Coling, pages 905–912, Manchester, UK, August. Col-

ing 2008 Organizing Committee.

Peter D. Turney. 2008b. The latent relation mapping en-

gine: Algorithm and experiments. Journal of Artificial

Intelligence Research, 33:615–655.

Dominic Widdows and Beate Dorow. 2002. A graph

model for unsupervised lexical acquisition. In Proc.

of Coling, pages 1–7, Stroudsburg, PA, USA.

George Udny Yule. 1939. On sentence-length as a statis-

tical characteristic of style in prose: with application

to two cases of disputed authorship. Biometrika, 30(3-

4):363–390.

1891


