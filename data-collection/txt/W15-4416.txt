



















































Condition Random Fields-based Grammatical Error Detection for Chinese as Second Language


Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 105â€“110,
Beijing, China, July 31, 2015. cÂ©2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing

 

 

Condition Random Fields-based Grammatical Error Detection for 

Chinese as Second Language 

 

Jui-Feng Yeh, Chan-Kun Yeh, Kai-Hsiang Yu, Ya-Ting Li, Wan-Ling Tsai 
Department of Computer Science and Information Engineering, National Chiayi University 

No.300 Syuefu Rd., Chiayi City 60004, Taiwan (R.O.C.) 

{ralph, s1030484, s1030495, s1013037, s1013048 }@mail.ncyu.edu.tw 

 

 

Abstract 

The foreign learners are not easy to learn 

Chinese as a second language. Because there 

are many special rules different from other 

languages in Chinese. When the people learn 

Chinese as a foreign language usually make 

some grammatical errors, such as missing, re-

dundant, selection and disorder. In this paper, 

we proposed the conditional random fields 

(CRFs) to detect the grammatical errors. The 

features based on statistical word and part-of-

speech (POS) pattern were adopted here. The 

relationships between words by part-of-speech 

are helpful for Chinese grammatical error de-

tection. Finally, we according to CRF deter-

mined which error types in sentences. Accord-

ing to the observation of experimental results, 

the performance of the proposed model is ac-

ceptable in precision and recall rates. 

1 Introduction 

As the world globalize, travel around the world 

is quicker than before. With the growth of Chinese 

market and more and more china town. There are 

more than 1.3 billion people who speak Chinese. 

That means there is one speak Chinese out of 

every five people. Chinese is the most spoken lan-

guage in the world. Sell products to the Chinese 

people, study and travel around Asia is much eas-

ier than before. To speak with foreigners and trade 

with foreigners we have to understand their lan-

guage first. So we believe that learning Chinese is 

important now. 

To learn Chinese as second language we have 

to know not only pronunciations and glyph of the 

word, but also grammar and the part of speech of 

Chinese. For example, there are eight parts of 

speech (nouns, pronouns, adjectives, verbs, ad-

verbs, prepositions, conjunctions, and interjec-

tions) in English. But in Chinese there are ten 

parts of speech (nouns, adjectives, verbs, adverbs, 

pronouns, interjections, prepositions, conjunc-

tions, auxiliary words, and quantifiers). Nouns in-

dicate the names of people or things, they can be 

further divided into four sub sorts, proper nouns, 

common nouns, abstract nouns, time nouns, place 

nouns. Adjective show the quality or forms of 

people or things, or the state of action or behavior. 

Verbs indicate the behaviors, actions or changes 

of people or things. They have several subsidiary 

categories: modal verbs, tendency verbs and de-

ciding verb. Adverb is used in front of verbs or 

adjectives to show degree, extent, time or nega-

tion. Pronoun is replace nouns or numerals. Prep-

osition introduces nouns, pronouns or other lin-

guistic units to verbs or adjectives and show the 

relationship between time, space, objects or meth-

ods. Conjunction connects words, phrases or sen-

tences.  

With Chinese become more popular. But it is 

not an easy language to learn, you will make a fool 

of yourself even if just one word mistake. More 

and more people pay their attention to Chinese 

grammar error. We may not write the right Chi-

nese sentence all the time. Sometimes we make 

some mistakes such as, overuse preposition, over-

use of "a/an", semantic overlap and quantifier er-

ror. In the past, the way to detect the grammar 

mistakes is extremely inefficient. People usually 

correct grammar mistakes by manual work. 

In recent year, there are many researches about 

Chinese grammar. There are few papers help us as 

reference. Li et al. (2012) proposed a hierarchical 

structure of dependency relations based on CDG 

for Chinese, in which the constraints have been 

partitioned into three hierarchies: in-the-phrase, 

between-the-phrase and between-the simple-sen-

tence. And Li, Z., Zhang et al (2014) proposed to 

integrate the POS (part-of-speech) tagging and 

parsing can reduce the complexity and improve 

the accuracy of parsing. Jiang et al. (2012) divided  

105



 

 

Table 1. Example of error types. 

 

Chinese grammar into three groups: morphology 

of content words, morphology of empty words 

and syntax. And each group is subdivided. Jiang 

et al. (2012) proposed the effectiveness of the 

XML and the goodness of XML structure these 

two parts compose XML syntax check. They im-

proved local tree grammar of the XML document 

type definition, and then do XML validity check-

ing in the grammatical structure based on the doc-

ument type definition. Rozovskaya et al. (2012) 

presented a linguistically-motivated, holistic 

framework for correcting grammatical verb mis-

takes. Describe and evaluate several methods of 

selecting verb candidates, an algorithm for deter-

mining the verb type, and a type-driven verb error 

correction system. And they gloss a subset of the 

FCE dataset with gold verb candidates and gold 

verb type. Lee et al. (2014) develops a sentence 

judgment system using both rule-based and n-

gram statistical methods to detect grammatical er-

rors in sentences written by CFL learners. Users 

can input Chinese sentences into the proposed 

system to check for possible grammatical errors. 

Wu et al. (2012) through examining the collected 

English-to-Chinese corpus composed of error sen-

tences, in contrast to the errors commonly made 

by learners of ESL such as the use of articles and 

prepositions, we found that learners of Chinese 

whose L1 is English tend to produce sentences 

with word order, lexical choice, redundancy, and 

omission errors. And they present an approach us-

ing the proposed Relative Position Language 

Model (RP) and Parse Template Language Model 

(PT) to deal with the error correction problem, 

which is especially suitable for the correction of 

word order errors that comprise about one third of 

the errors made by learners of Chinese as a Sec-

ond Language. The four error types considered for 

correction in their paper are errors of Lexical 

Choice, Redundancy, Omission, and Word Order. 

In this paper, we show the four error types in Ta-

ble 1. Islam et al. (2010) use the Google n-gram 

data set in a back-off fashion. And it increases the 

performance of the method. Their method can be 

applied to other languages for which Google n-

grams are available. Sun, X., & Nan, X. (2010) 

defined the phraseâ€™s format to â€œModifier + head + 

complementâ€.  

In our method, we tag some labels such as POS 

and binary variables in the sentences. In the sec-

tion II, we described the models how to train the 

corpus by CRF. And show the experiment in the 

section III. 

2 Method 

In this section, the architecture of our system is il-

lustrated in Figure 1. Then we will describe the gram-

matical error detection using CRF in the section 2.2. 

And the procedures distinguish into two parts: training 

phase and test phase. 

 

Table 2. Punctuation tagged by CKIP Autotag. 

COLONCATEGORY (ï¼š) 

COMMACATEGORY (ï¼Œ) 

DASHCATEGORY (ï¼) 

ETCCATEGORY (â€¦) 

EXCLAMATIONCATEGORY (ï¼) 

PARENTHESISCATEGORY (ï¼ˆï¼‰) 

PAUSECATEGORY (ã€) 

PERIODCATEGORY (ã€‚) 

QUESTIONCATEGORY (ï¼Ÿ) 

SEMICOLONCATEGORY (ï¼›) 

SPCHANGECATEGORY (||) 

 

Error Types Error Sentence Correct Sentence 

Missing Error æˆ‘(Nh) é€(VD) ä½ (Nh) é‚£è£¡(D) æˆ‘(Nh) é€(VD) ä½ (Nh) åˆ°(VCL) é‚£è£¡(Ncd) 

Redundant Error 
ä»–(Nh) æ˜¯(SHI) æˆ‘(Nh) çš„(DE)  
ä»¥å‰(Nd) çš„(DE) å®¤å‹(Na) 

ä»–(Nh) æ˜¯(SHI) æˆ‘(Nh) ä»¥å‰(Nd) 

çš„(DE) å®¤å‹(Na) 

Selection Error 
å³(Nb) å…ˆç”Ÿ(Na) æ˜¯(SHI) ä¿®ç†(VC) 

è…³è¸è»Š(Na) çš„(DE) æ‹¿æ‰‹(Nv) 
å³(Nb) å…ˆç”Ÿ(Na) æ˜¯(SHI) ä¿®ç†(VC) 

è…³è¸è»Š(Na) çš„(DE) å¥½æ‰‹(Na) 

Disorder Error 
æ‰€ä»¥(Cbb) æˆ‘(Nh) ä¸æœƒ(D) è®“(VL) 

å¤±æœ›(VH) å¥¹(Nh) 
æ‰€ä»¥(Cbb) æˆ‘(Nh) ä¸æœƒ(D) è®“(VL) 

å¥¹(Nh) å¤±æœ›(VH) 

106



 

 

 

Figure 1. The Architecture of Our System 

2.1 Data Preprocessing 

CKIP Autotag is a word segment system which 

made by Taiwan Academia Sinica. CKIP Autotag 

can segment a long Chinese sentence into Chinese 

word. And then tag the punctuation (punctuation 

are listed in Table 2) and the POS to the word. 

This system classifies Chinese word into 47 dif-

ferent POS. 

We use this system to chunking words and tag-

ging the POS on the sentence. We survey the 

grammar of Chinese sentence by CKIP Autotag. 

And observe the relation between the words by the 

POS.  

2.2 Condition Random Fields 

Conditional random fields (CRFs) is a class of 

statistical modelling method that is generally ap-

plied in machine learning and pattern recognition, 

where they are used for structured prediction. It 

was an extension of both Maximum Entropy 

Model (MEMs) and Hidden Markov Models 

(HMMs) that was firstly introduced by Lafferty et 

al., 2001.Whereas an ordinary classifier predicts a 

label for a single sample without regard to adja-

cent samples. A CRF can take context into ac-

count. Itâ€™s a discriminative of undirected probabil-

istic graphical model. It is used to encode known 

relationships between observations and construct 

consistent interpretations. Conditional random 

field defined conditional probability distribution 

P(Y|X) of given sequence given input sentence. Y 

is the â€œclass labelâ€ sequence and X denotes as the 

observation word sequence.  

A common used special case of CRFs is linear 

chain, which has a distribution of:  

 

PÎ›(y|x) =
exp(âˆ‘ âˆ‘ Î»kfk(ytâˆ’1,yt,x,t)k

T
t=1 )

Zx
 ( 1 ) 

 

Where ğ‘“ğ‘˜(ğ‘¦ğ‘¡âˆ’1, ğ‘¦ğ‘¡ , ğ‘¥, ğ‘¡) is usually an indicator 
function.   Î»ğ‘˜   is the learned weight of the feature 
and Zğ‘¥  is the normalization factor that sum the 
probability of all state sequences. The feature 

functions can measure any aspect of a state transi-

tion, ğ‘¦ğ‘¡âˆ’1 ğ‘¡ğ‘œ ğ‘¦ğ‘¡  and the entire observation se-
quence, x, centered at the current time step, t.  

Here we use three conditional random field 

models to calculate the conditional probability of 

the missing sentences, redundant sentences, disor-

der sentences and error selection sentences.  

In training phase, we give the matrix {Word, 

POS, TAG} to denote the sentence of the words 

in the train set. Such as {å», VCL, T} or {å», D, 

F}, the word â€œå»(go)â€ has many part-of-speech 
in different sentences. The tag â€œTâ€ means correct 

word in current sentence and tag â€œFâ€ means error 

word in current sentence. Then we use this train-

ing data to generate the model by Conditional ran-

dom fields. 

In testing phase, we segment and tag POS la-

beling by CKIP Autotag. Then we also use the 

matrix {Word, POS} to denote the words. After 

preprocessing, we can get the tagâ€™s probability of 

testing words by our training models using 

CRF++.  

For example, input the sentence of â€œä½†æ˜¯(but)

é§•é§›(driver) éƒ½(neither) è£ä½œ(pretend) æ²’(not) 

107



 

 

çœ‹åˆ° (see) æˆ–è€… (or) è½åˆ° (hear) æˆ‘ (me) äº†
(interjection)â€. There are some probabilities from 

different models, â€œMissing 0.872773, Redundant 

0.465524, Selection 0.832839â€ and judge it is a 

redundant error. So we found every wordâ€™s prob-

ability in this sentence. The probability of words 

are show in Table 3. And we found the error word 

is â€œäº†â€. 
 

Table 3. Probability of Words in the Sentence. 

Word POS Probability 

ä½†æ˜¯ Cbb T/0.963663 

é§•é§› VC T/0.986188 

éƒ½ D T/0.975163 

è£ä½œ VF T/0.970347 

æ²’ D T/0.962676 

çœ‹åˆ° VE T/0.984734 

æˆ–è€… Caa T/0.953170 

è½åˆ° VE T/0.988986 

æˆ‘ Nh T/0.997955 

äº† T F/0.579991 

 

According the probability of tagging, we can 

determine what typeâ€™s error and speculate the po-

sition in the sentence. 

2.3 Rule Induction 

There are many special cases of selection error 

types in Chinese. Such as quantifier is one case of 

all. In English, we usually use â€œaâ€ or â€œanâ€ to de-

note quantifier. 

But Chinese needs more different quantifiers 

then the other language. In many cases, Chinese 

use â€˜å€‹â€™ as a quantifier. There are more times we 

do not use â€˜å€‹â€™ as a quantifier. About quantifier of 

human we should use â€˜ä½â€™ or â€˜å€‹â€™. About quanti-

fier of animals we should use â€˜éš»â€™, â€˜åŒ¹â€™, â€˜é ­â€™, or 

â€˜æ¢â€™. About quantifier of things we should use 

â€˜ä»¶â€™. About quantifier of buildings we should use 

â€˜åº§â€™ or â€˜æ£Ÿâ€™. About quantifier of transportations 

we should use â€˜è‡ºâ€™, â€˜è¼›â€™, â€˜æ¶â€™ or â€˜è‰˜â€™ etc. 

We also focused on finding the ordering type of 

the wrong words. There are some rules which we 

follow to finding ordering error. 

 

ï‚· Behind the words â€œæŠŠ (let)â€ is connected 
the POS â€˜Nhâ€™ or â€˜Naâ€™ or â€˜Nepâ€™. 

ï‚· Behind the POS â€˜VAâ€™ is connected the 

word â€œè·Ÿ(with)â€, and the POS â€˜Nhâ€™ or 
â€˜Naâ€™ also is connected behind the words 

â€œè·Ÿ(with)â€. 

ï‚· Behind the words â€œæ‡‰è©²(maybe)â€ or  â€œ å¥½

åƒ(like)â€ or â€œåˆ°åº•(at last)â€ is connected 
the POS â€˜Nhâ€™ or â€˜Naâ€™. 

ï‚· Behind the word â€œå·²ç¶“(already)â€ is con-

nected the POS â€˜Neqaâ€™ or â€˜Neuâ€™, and the 

POS â€˜Pâ€™ or â€˜Naâ€™ or â€˜VAâ€™ is connected be-

hind the POS â€˜Neqaâ€™ or â€˜Neuâ€™. 

Above those rules, we can enhance our 

method during the detection grammatical errors. 

3 Result 

To evaluate the performance of our system, we 

used three parameters: precision, recall and f-

score. 

Precision is the fraction of retrieved documents 

that are relevant to the query. 

 

Precision =  
ğ‘¡ğ‘

ğ‘¡ğ‘ + ğ‘“ğ‘
 

 

Recall is the fraction of the documents that are 

relevant to the query that are successfully re-

trieved. 

Recall =  
ğ‘¡ğ‘

ğ‘¡ğ‘ + ğ‘“ğ‘›
 

 

F1-Score is a measure of a test's accuracy. It 

considers both the precision and the recall of the 

test to compute the score. 

 

F1 âˆ’ score =  2 âˆ—
ğ‘ƒğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘ ğ‘–ğ‘œğ‘› âˆ— ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

ğ‘ƒğ‘Ÿğ‘’ğ‘ ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
 

 

The comparative study of all the three cases has 

done. And a result of these cases are given in the 

Table 4 and Table 5 with the NLP-TEA 2014 da-

taset.  

In this paper, we collect 2,212 sentences in 

training dataset. And it contains 622 sentences of 

missing, 435 sentences of redundant, 849 sen-

tences of selection and 306 sentences of disorder. 

Then we use two dataset 1,750 sentences from 

NLP-TEA 2014 and 1,000 sentences from NLP-

TEA 2015. 

We can find only use CRF it canâ€™t find many 

error but its precision is better. Then add the rule 

induction can promote the recall means we can 

find more error from test data. Although its preci-

sion is reduced. 

108



 

 

 

Table 4. Detection level 

Method Precision Recall F1 

CRF 0.6863 0.2000 0.3097 

CRF + Rule 

Induction 
0.5257 0.4674 0.4949 

Table 5. Identification level 

Method Precision Recall F1-Score 

CRF 0.5897 0.1314 0.2150 

CRF + Rule 

Induction 
0.3549 0.2320 0.2806 

 

Table 6, Table 7, and Table 8 are the perfor-

mance with the NLP-TEA 2015 dataset and com-

pare the other team  

 

Table 6. Detection level 

 Accuracy Precision Recall F1 

NCYU 0.607 0.6112 0.588 0.5994 

CYUT 0.579 0.7453 0.240 0.3631 

NTOU 0.531 0.5164 0.976 0.6754 

 

Table 7. Identification level 

 Accuracy Precision Recall F1 

NCYU 0.463 0.4451 0.300 0.3584 

CYUT 0.525 0.6168 0.132 0.2175 

NTOU 0.225 0.2848 0.364 0.3196 

 

Table 8. Position level 

 Accuracy Precision Recall F1 

NCYU 0.374 0.2460 0.122 0.1631 

CYUT 0.505 0.5287 0.092 0.1567 

NTOU 0.123 0.1490 0.160 0.1543 

 

In detection level (see the Table 6.), our recall 

is better than CYUTâ€™s method. It means we can 

find more error in dataset. And our precision is 

better than NTOUâ€™s method. It means our find 

correct error rate is better, although we find error 

quantity less than NTOU. 

In identification level (Table 7.), it show who 

can find most error and error type is correct. In our 

method, our recall is nearly NTOUâ€™s method, it 

means we find more correct error type than 

CYUTâ€™s method. But our precision is better than 

NTOUâ€™s method. And our F1-Score is the best in 

this level. 

In position level (Table 8.), our methodâ€˜s preci-

sion and recall are between the CYUTâ€™s method 

and NTOUâ€™s method. It means our method is not 

illustrious in this level. We consider the reasons 

are our correction is not enough standard. 

4 Conclusion 

In this paper, we present a method using condi-

tional random field model for predicting the gram-

matical error diagnosis for learning Chinese. 

After observe the experiment results, our 

method is acceptable in NLP-TEA 2015. We be-

lieve this system is feasible. This system is useful 

for a foreign who learn Chinese as a second lan-

guage. Even the people who use Chinese as a first 

language might use the wrong grammars. 

There are some issues should be revise. First, 

the CRF models can be improved in some ways, 

such as words tagging or using the parsing tree. 

Second, increase the ranking mechanism to find 

the optimal words to correct the sentence. 
In the future, we will pay attention to improve the 

precision and recall rates in this system. And let it can 

automatic correct the error if the people input the sen-

tences. 

Reference 

Islam, A., & Inkpen, D. (2010, August). An unsuper-

vised approach to preposition error correction. In 

Natural Language Processing and Knowledge Engi-

neering (NLP-KE), 2010 International Conference 

on (pp. 1-4). IEEE. 

 Jiang, Y., Wang, T., Lin, T., Wang, F., Cheng, W., Liu, 

X., ... & Zhang, W. (2012, June). A rule based Chi-

nese spelling and grammar detection system utility. 

In System Science and Engineering (ICSSE), 2012 

International Conference on (pp. 437-440). IEEE. 

 Jiang, Y., Zhou, Z., Wan, L., Li, M., Zhao, W., Jing, 

M., & Liu, X. (2012, October). Cross sentence ori-

ented complicated Chinese grammar proofreading 

method and practice. In Information Management, 

Innovation Management and Industrial Engineering 

(ICIII), 2012 International Conference on (Vol. 3, 

pp. 254-258). IEEE. 

Lafferty, J., McCallum, A., and Pereira, F. 2001. Con-

ditional Random Field: Probabilistic models for seg-

menting and labeling sequence data. In Proceedings 

of the International Conference on Machine Learn-

ing.  

Lance A. Ramshaw and Mitchell P. Marcus. 1995. 

Text chunking using transformation-based learning. 

In Proceedings of the 3rd Workshop on Very Large 

109



 

 

Corpora, pages 82-94. Nocedal, J., and Wright, S. 

1999. Numerical optimization. Springer. 

Lee, L. H., Yu, L. C., Lee, K. C., Tseng, Y. H., Chang, 

L. P., & Chen, H. H. (2014). A Sentence Judgment 

System for Grammatical Error Detection. COLING 

2014, 67. 

Liang-Chih Yu, Lung-Hao Lee, and Li-Ping Chang 

(2014). Overview of Grammatical Error Diagnosis 

for Learning Chinese as a Foreign Language. Pro-

ceedings of the 1st Workshop on Natural Language 

Processing Techniques for Educational Applica-

tions (NLPTEA'14), Nara, Japan, 30 November, 

2014, pp. 42-47. 

Li, P., Liao, L., & Li, X. (2012, July). A hierarchy-

based constraint dependency grammar parsing for 

Chinese. In Audio, Language and Image Processing 

(ICALIP), 2012 International Conference on (pp. 

328-332). IEEE.  

Li, Z., Zhang, M., Che, W., Liu, T., & Chen, W. (2014). 

Joint Optimization for Chinese POS Tagging and 

Dependency Parsing. Audio, Speech, and Language 

Processing, IEEE/ACM Transactions on, 22(1), 

274-286. 

 Rozovskaya, A., Roth, D., & Srikumar, V. (2014, 

April). Correcting grammatical verb errors. In Pro-

ceedings of EACL.  

Chang, R. Y., Wu, C. H., & Prasetyo, P. K. (2012). Er-

ror diagnosis of Chinese sentences using inductive 

learning algorithm and decomposition-based testing 

mechanism. ACM Transactions on Asian Language 

Information Processing (TALIP), 11(1), 3. 

Sun, X., & Nan, X. (2010, August). Chinese base 

phrases chunking based on latent semi-CRF model. 

In Natural Language Processing and Knowledge 

Engineering (NLP-KE), 2010 International Confer-

ence on (pp. 1-7). IEEE.  

Wu, C. H., Liu, C. H., Harris, M., & Yu, L. C. (2010). 

Sentence correction incorporating relative position 

and parse template language models. Audio, Speech, 

and Language Processing, IEEE Transactions on, 

18(6), 1170-1181. 

Yu, C. H., & Chen, H. H. (2012). Detecting Word Or-

dering Errors in Chinese Sentences for Learning 

Chinese as a Foreign Language. In COLING (pp. 

3003-3018). 

Academia Sinica CKIP. 

http://ckipsvr.iis.sinica.edu.tw/  

 CRF++: Yet Another CRF toolkit 

http://taku910.github.io/crfpp/ 

 

 

110


