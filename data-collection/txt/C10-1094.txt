Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 833–841,

Beijing, August 2010

833

Evaluation of Dependency Parsers on Unbounded Dependencies

Joakim Nivre

Laura Rimell

Uppsala University Univ. of Cambridge
joakim.nivre@lingﬁl.uu.se
laura.rimell@cl.cam.ac.uk

Ryan McDonald Carlos G´omez-Rodr´ıguez
Universidade da Coru˜na

Google Inc.

ryanmcd@google.com

cgomezr@udc.es

Abstract

We evaluate two dependency parsers,
MSTParser and MaltParser, with respect
to their capacity to recover unbounded de-
pendencies in English, a type of evalu-
ation that has been applied to grammar-
based parsers and statistical phrase struc-
ture parsers but not to dependency parsers.
The evaluation shows that when combined
with simple post-processing heuristics,
the parsers correctly recall unbounded
dependencies roughly 50% of the time,
which is only slightly worse than two
grammar-based parsers speciﬁcally de-
signed to cope with such dependencies.

Introduction

1
Though syntactic parsers for English are re-
ported to have accuracies over 90% on the Wall
Street Journal (WSJ) section of the Penn Tree-
bank (PTB) (McDonald et al., 2005; Sagae and
Lavie, 2006; Huang, 2008; Carreras et al., 2008),
broad-coverage parsing is still far from being a
solved problem. In particular, metrics like attach-
ment score for dependency parsers (Buchholz and
Marsi, 2006) and Parseval for constituency parsers
(Black et al., 1991) suffer from being an aver-
age over a highly skewed distribution of differ-
ent grammatical constructions. As a result, in-
frequent yet semantically important construction
types could be parsed with accuracies far below
what one might expect.

This shortcoming of aggregate parsing met-
rics was highlighted in a recent study by Rimell
et al. (2009), introducing a new parser evalua-
tion corpus containing around 700 sentences an-
notated with unbounded dependencies in seven
different grammatical constructions. This corpus
was used to evaluate ﬁve state-of-the-art parsers

for English, focusing on grammar-based and sta-
tistical phrase structure parsers. For example, in
the sentence By Monday, they hope to have a
sheaf of documents both sides can trust., parsers
should recognize that there is a dependency be-
tween trust and documents, an instance of object
extraction out of a (reduced) relative clause. In the
evaluation, the recall of state-of-the-art parsers on
this kind of dependency varies from a high of 65%
to a low of 1%. When averaging over the seven
constructions in the corpus, none of the parsers
had an accuracy higher than 61%.

In this paper, we extend the evaluation of
Rimell et al. (2009) to two dependency parsers,
MSTParser (McDonald, 2006) and MaltParser
(Nivre et al., 2006a), trained on data from the
PTB, converted to Stanford typed dependencies
(de Marneffe et al., 2006), and combined with a
simple post-processor to extract unbounded de-
pendencies from the basic dependency tree. Ex-
tending the evaluation to dependency parsers is of
interest because it sheds light on whether highly
tuned grammars or computationally expensive
parsing formalisms are necessary for extracting
complex linguistic phenomena in practice. Unlike
the best performing grammar-based parsers stud-
ied in Rimell et al. (2009), neither MSTParser nor
MaltParser was developed speciﬁcally as a parser
for English, and neither has any special mecha-
nism for dealing with unbounded dependencies.
Dependency parsers are also often asymptotically
faster than grammar-based or constituent parsers,
e.g., MaltParser parses sentences in linear time.

Our evaluation ultimately shows that the re-
call of MSTParser and MaltParser on unbounded
dependencies is much lower than the average
(un)labeled attachment score for each system.
Nevertheless,
the two dependency parsers are
found to perform only slightly worse than the best
grammar-based parsers evaluated in Rimell et al.

834

Figure 1: Examples of seven unbounded dependency constructions (a–g). Arcs drawn below each sentence represent the
dependencies scored in the evaluation, while the tree above each sentence is the Stanford basic dependency representation,
with solid arcs indicating crucial dependencies (cf. Section 4). All examples are from the development sets.

(2009) and considerably better than the other sta-
tistical parsers in that evaluation.
Interestingly,
though the two systems have similar accuracies
overall, there is a clear distinction between the
kinds of errors each system makes, which we ar-
gue is consistent with observations by McDonald
and Nivre (2007).

2 Unbounded Dependency Evaluation

An unbounded dependency involves a word or
phrase interpreted at a distance from its surface
position, where an unlimited number of clause
boundaries may in principle intervene.
The
unbounded dependency corpus of Rimell et al.
(2009) includes seven grammatical constructions:
object extraction from a relative clause (ObRC),
object extraction from a reduced relative clause
(ObRed), subject extraction from a relative clause
(SbRC), free relatives (Free), object questions
(ObQ), right node raising (RNR), and subject ex-
traction from an embedded clause (SbEm), all
chosen for being relatively frequent and easy to
identify in PTB trees. Examples of the con-
structions can be seen in Figure 1. The evalu-
ation set contains 80 sentences per construction
(which may translate into more than 80 depen-
dencies, since sentences containing coordinations
may have more than one gold-standard depen-
dency), while the development set contains be-
tween 13 and 37 sentences per construction. The
data for ObQ sentences was obtained from various
years of TREC, and for the rest of the construc-

tions from the WSJ (0-1 and 22-24) and Brown
sections of the PTB.

Each sentence is annotated with one or more
gold-standard dependency relations representing
the relevant unbounded dependency. The gold-
standard dependencies are shown as arcs below
the sentences in Figure 1. The format of the de-
pendencies in the corpus is loosely based on the
Stanford typed dependency scheme, although the
evaluation procedure permits alternative represen-
tations and does not require that the parser out-
put match the gold-standard exactly, as long as the
“spirit” of the construction is correct.

The ability to recover unbounded dependencies
is important because they frequently form part of
the basic predicate-argument structure of a sen-
tence. Subject and object dependencies in par-
ticular are crucial for a number of tasks, includ-
ing information extraction and question answer-
ing. Moreover, Rimell et al. (2009) show that,
although individual types of unbounded depen-
dencies may be rare, the unbounded dependency
types in the corpus, considered as a class, occur in
as many as 10% of sentences in the PTB.

In Rimell et al. (2009), ﬁve state-of-the-art
parsers were evaluated for their recall on the gold-
standard dependencies. Three of the parsers were
based on grammars automatically extracted from
the PTB: the C&C CCG parser (Clark and Curran,
2007), the Enju HPSG parser (Miyao and Tsujii,
2005), and the Stanford parser (Klein and Man-
ning, 2003). The two remaining systems were the

a: ObRC

nsubj

au x

dobj

poss
poss've

prep

pobj

det

rcm od

dobj

nsubj

prep

pobj

rcm od

nu m

nsubj

au x

con j

prep

pobj

cc

nu m

b: ObRed

Each must match Wisman 's "pie" with the fragment that he carries with him

nsubj

c: SbRC

au x

prep

pobj

det
am od

rcm od

nsubj

dobj

dobj

nu m

am od

They will remain on a lower-priority list that includes 17 other countries

Five things you can do for $ 15,0 0 0  or less

dobj

dobj

csubj

nsubj

cop

am od

d: Free

det

e: ObQ
au x

dobj

nsubj

det

prep

pobj

What you see are self-help projects

dobj

What effect does a prism have on light

dobj

ns ubj

f : RNR

nsubjpass

advm od

cop

prep

con j

det

pobj

advm od

pobj

det
am od

adv
m od

acom p

prep

nsubj

pobj

det

am od

rcm od

g: SbEm

prep

nsubj

xcom p

prt

pobj

The men were at first puz z led, then angered by the aimless tacking

Now he felt ready for the many actions he saw spreading out before him

prep

ns ubj

835

RASP parser (Briscoe et al., 2006), using a man-
ually constructed grammar and a statistical parse
selection component, and the DCU post-processor
of PTB parsers (Cahill et al., 2004) using the out-
put of the Charniak and Johnson reranking parser
(Charniak and Johnson, 2005). Because of the
wide variation in parser output representations, a
mostly manual evaluation was performed to en-
sure that each parser got credit for the construc-
tions it recovered correctly. The parsers were run
essentially “out of the box”, meaning that the de-
velopment set was used to conﬁrm input and out-
put formats, but no real tuning was performed. In
addition, since a separate question model is avail-
able for C&C, this was also evaluated on ObQ
sentences. The best overall performers were C&C
and Enju, which is unsurprising since they are
deep parsers based on grammar formalisms de-
signed to recover just such dependencies. The
DCU post-processor performed somewhat worse
than expected, often identifying the existence of
an unbounded dependency but failing to iden-
tify the grammatical class (subject, object, etc.).
RASP and Stanford, although not designed to re-
cover such dependencies, nevertheless recovered
a subset of them. Performance of the parsers also
varied widely across the different constructions.

3 Dependency Parsers

In this paper we repeat the study of Rimell et al.
(2009) for two dependency parsers, with the goal
of evaluating how parsers based on dependency
grammars perform on unbounded dependencies.
MSTParser1 is a freely available implementa-
tion of the parsing models described in McDon-
ald (2006). According to the categorization of
parsers in K¨ubler et al. (2008) it is a graph-based
parsing system in that core parsing algorithms can
be equated to ﬁnding directed maximum span-
ning trees (either projective or non-projective)
from a dense graph representation of the sentence.
Graph-based parsers typically rely on global train-
ing and inference algorithms, where the goal is to
learn models in which the weight/probability of
correct trees is higher than that of incorrect trees.
At inference time a global search is run to ﬁnd the

highest weighted dependency tree. Unfortunately,
global inference and learning for graph-based de-
pendency parsing is typically NP-hard (McDonald
and Satta, 2007). As a result, graph-based parsers
(including MSTParser) often limit the scope of
their features to a small number of adjacent arcs
(usually two) and/or resort to approximate infer-
ence (McDonald and Pereira, 2006).

MaltParser2 is a freely available implementa-
tion of the parsing models described in Nivre et
al. (2006a) and Nivre et al. (2006b). MaltParser is
categorized as a transition-based parsing system,
characterized by parsing algorithms that produce
dependency trees by transitioning through abstract
state machines (K¨ubler et al., 2008). Transition-
based parsers learn models that predict the next
state given the current state of the system as well
as features over the history of parsing decisions
and the input sentence. At inference time, the
parser starts in an initial state, then greedily moves
to subsequent states – based on the predictions of
the model – until a termination state is reached.
Transition-based parsing is highly efﬁcient, with
run-times often linear in sentence length. Further-
more, transition-based parsers can easily incorpo-
rate arbitrary non-local features, since the current
parse structure is ﬁxed by the state. However, the
greedy nature of these systems can lead to error
propagation if early predictions place the parser
in incorrect states.

McDonald and Nivre (2007) compared the ac-
curacy of MSTParser and MaltParser along a
number of structural and linguistic dimensions.
They observed that, though the two parsers ex-
hibit indistinguishable accuracies overall, MST-
Parser tends to outperform MaltParser on longer
dependencies as well as those dependencies closer
to the root of the tree (e.g., verb, conjunction and
preposition dependencies), whereas MaltParser
performs better on short dependencies and those
further from the root (e.g., pronouns and noun de-
pendencies). Since long dependencies and those
near to the root are typically the last constructed
in transition-based parsing systems, it was con-
cluded that MaltParser does suffer from some
form of error propagation. On the other hand, the

1http://mstparser.sourceforge.net

2http://www.maltparser.org

836

richer feature representations of MaltParser led to
improved performance in cases where error prop-
agation has not occurred. However, that study did
not investigate unbounded dependencies.

4 Methodology
In this section, we describe the methodological
setup for the evaluation, including parser training,
post-processing, and evaluation.3

4.1 Parser Training
One important difference between MSTParser and
MaltParser, on the one hand, and the best perform-
ing parsers evaluated in Rimell et al. (2009), on
the other, is that the former were never developed
speciﬁcally as parsers for English. Instead, they
are best understood as data-driven parser gener-
ators, that is, tools for generating a parser given
a training set of sentences annotated with de-
pendency structures. Over the years, both sys-
tems have been applied to a wide range of lan-
guages (see, e.g., McDonald et al. (2006), Mc-
Donald (2006), Nivre et al. (2006b), Hall et al.
(2007), Nivre et al. (2007)), but they come with
no language-speciﬁc enhancements and are not
equipped speciﬁcally to deal with unbounded de-
pendencies.

Since the dependency representation used in
the evaluation corpus is based on the Stanford
typed dependency scheme (de Marneffe et al.,
2006), we opted for using the WSJ section of
the PTB, converted to Stanford dependencies, as
our primary source of training data. Thus, both
parsers were trained on section 2–21 of the WSJ
data, which we converted to Stanford dependen-
cies using the Stanford parser (Klein and Man-
ning, 2003). The Stanford scheme comes in sev-
eral varieties, but because both parsers require the
dependency structure for each sentence to be a
tree, we had to use the so-called basic variety (de
Marneffe et al., 2006).

It is well known that questions are very rare
in the WSJ data, and Rimell et al. (2009) found
that parsers trained only on WSJ data generally
performed badly on the questions included in the

3To ensure replicability, we provide all experimental
settings, post-processing scripts and additional information
about the evaluation at http://stp.ling.uu.se/∼nivre/exp/.

evaluation corpus, while the C&C parser equipped
with a model trained on a combination of WSJ
and question data had much better performance.
To investigate whether the performance of MST-
Parser and MaltParser on questions could also be
improved by adding more questions to the train-
ing data, we trained one variant of each parser
using data that was extended with 3924 ques-
tions taken from QuestionBank (QB) (Judge et al.,
2006).4 Since the QB sentences are annotated in
PTB style, it was possible to use the same conver-
sion procedure as for the WSJ data. However, it is
clear that the conversion did not always produce
adequate dependency structures for the questions,
an observation that we will return to in the error
analysis below.

In comparison to the ﬁve parsers evaluated in
Rimell et al. (2009), it is worth noting that MST-
Parser and MaltParser were trained on the same
basic data as four of the ﬁve, but with a differ-
ent kind of syntactic representation – dependency
trees instead of phrase structure trees or theory-
speciﬁc representations from CCG and HPSG. It
is especially interesting to compare MSTParser
and MaltParser to the Stanford parser, which es-
sentially produces the same kind of dependency
structures as output but uses the original phrase
structure trees from the PTB as input to training.

For our experiments we used MSTParser with
the same parsing algorithms and features as re-
ported in McDonald et al. (2006). However, un-
like that work we used an atomic maximum en-
tropy model as the second stage arc predictor as
opposed to the more time consuming sequence la-
beler. McDonald et al. (2006) showed that there is
negligible accuracy loss when using atomic rather
than structured labeling. For MaltParser we used
the projective Stack algorithm (Nivre, 2009) with
default settings and a slightly enriched feature
model. All parsing was projective because the
Stanford dependency trees are strictly projective.

4QB contains 4000 questions, but we removed all ques-
tions that also occurred in the test or development set of
Rimell et al. (2009), who sampled their questions from the
same TREC QA test sets.

837

4.2 Post-Processing
All the development and test sets in the corpus
of Rimell et al. (2009) were parsed using MST-
Parser and MaltParser after part-of-speech tagging
the input using SVMTool (Gim´enez and M`arquez,
2004) trained on section 2–21 of the WSJ data in
Stanford basic dependency format. The Stanford
parser has an internal module that converts the
basic dependency representation to the collapsed
representation, which explicitly represents addi-
tional dependencies, including unbounded depen-
dencies, that can be inferred from the basic rep-
resentation (de Marneffe et al., 2006). We per-
formed a similar conversion using our own tool.

Broadly speaking, there are three ways in which
unbounded dependencies can be inferred from the
Stanford basic dependency trees, which we will
refer to as simple, complex, and indirect. In the
simple case, the dependency coincides with a sin-
gle, direct dependency relation in the tree. This
is the case, for example, in Figure 1d–e, where
all that is required is that the parser identiﬁes
the dependency relation from a governor to an
argument (dobj(see, What), dobj(have,
effect)), which we call the Arg relation; no
post-processing is needed.

In the complex case, the dependency is repre-
sented by a path of direct dependencies in the tree,
as exempliﬁed in Figure 1a.
In this case, it is
not enough that the parser correctly identiﬁes the
Arg relation dobj(carries, that); it must
also ﬁnd the dependency rcmod(fragment,
carries). We call this the Link relation, be-
cause it links the argument role inside the relative
clause to an element outside the clause. Other ex-
amples of the complex case are found in Figure 1c
and in Figure 1f.

In the indirect case, ﬁnally, the dependency
cannot be deﬁned by a path of labeled depen-
dencies, whether simple or complex, but must
be inferred from a larger context of the tree us-
ing heuristics. Consider Figure 1b, where there
is a Link relation (rcmod(things, do)), but
no corresponding Arg relation inside the relative
clause (because there is no overt relative pro-
noun). However, given the other dependencies,
we can infer with high probability that the im-
plicit relation is dobj. Another example of the

indirect case is in Figure 1g. Our post-processing
tool performs more heuristic inference for the in-
direct case than the Stanford parser does (cf. Sec-
tion 4.3).

In order to handle the complex and indirect
cases, our post-processor is triggered by the oc-
currence of a Link relation (rcmod or conj) and
ﬁrst tries to add dependencies that are directly im-
plied by a single Arg relation (relations involving
relative pronouns for rcmod, shared heads and
dependents for conj). If there is no overt rela-
tive pronoun, or the function of the relative pro-
noun is underspeciﬁed, the post-processor relies
on the obliqueness hierarchy subj < dobj <
pobj and simply picks the ﬁrst “missing func-
tion”, unless it ﬁnds a clausal complement (indi-
cated by the labels ccomp and xcomp), in which
case it descends to the lower clause and restarts
the search there.

4.3 Parser Evaluation
The evaluation was performed using the same cri-
teria as in Rimell et al. (2009). A dependency
was considered correctly recovered if the gold-
standard head and dependent were correct and
the label was an “acceptable match” to the gold-
standard label, indicating the grammatical func-
tion of the extracted element at least to the level
of subject, passive subject, object, or adjunct.

The evaluation in Rimell et al. (2009) took
into account a wide variety of parser output for-
mats, some of which differed signiﬁcantly from
the gold-standard. Since MSTParser and Malt-
Parser produced Stanford dependencies for this
experiment, evaluation required less manual ex-
amination than for some of the other parsers, as
was also the case for the output of the Stanford
parser in the original evaluation. However, a man-
ual evaluation was still performed in order to re-
solve questionable cases.

5 Results
The results are shown in Table 1, where the ac-
curacy for each construction is the percentage of
gold-standard dependencies recovered correctly.
The Avg column represents a macroaverage, i.e.
the average of the individual scores on the seven
constructions, while the WAvg column represents

838

Parser
MST
Malt
MST-Q
Malt-Q

Parser
C&C
Enju
MST
Malt
DCU
RASP
Stanford

ObRC ObRed
47.3
50.5

34.1
40.7

SbRC Free ObQ RNR SbEm
37.6
23.5

45.4
39.7

78.9
84.2

65.5
70.2

13.8
16.2
41.2
31.2

Avg WAvg
46.1
63.4
66.9
46.4
50.0
48.5

Table 1: Parser accuracy on the unbounded dependency corpus.

ObRC ObRed
62.6
65.9
47.3
50.5
41.8
1.1
1.1

59.3
47.3
34.1
40.7
23.1
16.5
22.0

SbRC Free ObQ RNR SbEm
22.4
32.9
37.6
23.5
5.9
15.3
10.6

81.2
32.5
41.2
31.2
27.5
27.5
41.2

49.4
47.1
45.4
39.7
40.8
34.5
45.4

80.0
82.1
78.9
84.2
56.8
53.7
74.7

72.6
76.2
65.5
70.2
46.4
17.9
64.3

Avg WAvg
61.1
69.9
70.9
54.9
63.4
50.0
66.9
48.5
47.0
34.6
23.8
34.1
50.3
37.0

Table 2: Parser accuracy on the unbounded dependency corpus. The ObQ score for C&C, MSTParser, and MaltParser is for
a model trained with additional questions (without this C&C scored 27.5; MSTParser and MaltParser as in Table 1).

a weighted macroaverage, where the construc-
tions are weighted proportionally to their relative
frequency in the PTB. WAvg excludes ObQ sen-
tences, since frequency statistics were not avail-
able for this construction in Rimell et al. (2009).
Our ﬁrst observation is that the accuracies for
both systems are considerably below the ∼90%
unlabeled and ∼88% labeled attachment scores
for English that have been reported previously
(McDonald and Pereira, 2006; Hall et al., 2006).
Comparing the two parsers, we see that Malt-
Parser is more accurate on dependencies in rela-
tive clause constructions (ObRC, ObRed, SbRC,
and Free), where argument relations tend to be
relatively local, while MSTParser is more accu-
rate on dependencies in RNR and SbEm, which
involve more distant relations. Without the ad-
ditional QB training data, the average scores for
the two parsers are indistinguishable, but MST-
Parser appears to have been better able to take
advantage of the question training, since MST-Q
performs better than Malt-Q on ObQ sentences.
On the weighted average MaltParser scores 3.5
points higher, because the constructions on which
it outperforms MSTParser are more frequent in
the PTB, and because WAvg excludes ObQ, where
MSTParser is more accurate.

Table 2 shows the results for MSTParser and
MaltParser in the context of the other parsers eval-
uated in Rimell et al. (2009).5 For the parsers

5The average scores reported differ slightly from those in

which have a model trained on questions, namely
C&C, MSTParser, and MaltParser,
the ﬁgure
shown for ObQ sentences is that of the question
model. It can be seen that MSTParser and Malt-
Parser perform below C&C and Enju, but above
the other parsers, and that MSTParser achieves the
highest score on SbEm sentences and MaltParser
on SbRC sentences. It should be noted, however,
that Table 2 does not represent a direct compar-
ison across all parsers, since most of the other
parsers would have beneﬁted from heuristic post-
processing of the kind implemented here for MST-
Parser and MaltParser. This is especially true for
RASP, where the grammar explicitly leaves some
types of attachment decisions for post-processing.
For DCU, improved labeling heuristics would sig-
niﬁcantly improve performance. It is instructive to
compare the dependency parsers to the Stanford
parser, which uses the same output representation
and has been used to prepare the training data for
our experiments. Stanford has very low recall on
ObRed and SbEm, the categories where heuristic
inference plays the largest role, but mirrors MST-
Parser for most other categories.

6 Error Analysis

We now proceed to a more detailed error analy-
sis, based on the development sets, and classify

Rimell et al. (2009), where a microaverage (i.e., average over
all dependencies in the corpus, regardless of construction)
was reported.

839

the errors made by the parsers into three cate-
gories: A global error is one where the parser
completely fails to build the relevant clausal struc-
ture – the relative clause in ObRC, ObRed, SbRC,
Free, SbEmb; the interrogative clause in ObQ; and
the clause headed by the higher conjunct in RNR
– often as a result of surrounding parsing errors.
When a global error occurs, it is usually mean-
ingless to further classify the error, which means
that this category excludes the other two. An Arg
error is one where the parser has constructed the
relevant clausal structure but fails to ﬁnd the Arg
relation – in the simple and complex cases – or the
set of surrounding Arg relations needed to infer
an implicit Arg relation – in the indirect case (cf.
Section 4.2). A Link error is one where the parser
fails to ﬁnd the crucial Link relation – rcmod
in ObRC, ObRed, SbRC, SbEmb; conj in RNR
(cf. Section 4.2). Link errors are not relevant for
Free and ObQ, where all the crucial relations are
clause-internal.

Table 3 shows the frequency of different error
types for MSTParser (ﬁrst) and MaltParser (sec-
ond) in the seven development sets. First of all,
we can see that the overall error distribution is
very similar for the two parsers, which is proba-
bly due to the fact that they have been trained on
exactly the same data with exactly the same an-
notation (unlike the ﬁve parsers previously eval-
uated). However, there is a tendency for MST-
Parser to make fewer Link errors, especially in
the relative clause categories ObRC, ObRed and
SbRC, which is compatible with the observation
from the test results that MSTParser does better
on more global dependencies, while MaltParser
has an advantage on more local dependencies, al-
though this is not evident from the statistics from
the relatively small development set.

Comparing the different grammatical construc-
tions, we see that Link errors dominate for the rel-
ative clause categories ObRC, ObRed and SbRC,
where the parsers make very few errors with
respect to the internal structure of the relative
clauses (in fact, no errors at all for MaltParser
on SbRC). This is different for SbEm, where the
analysis of the argument structure is more com-
plex, both because there are (at least) two clauses
involved and because the unbounded dependency

Global

0/1
0/1
2/1
2/1
4/7
6/4
3/4

Arg
1/1
0/1
1/0
3/5
13/13
4/6
3/2

Link
7/11
6/7
7/13

–
–
0/0
0/0

A+L
5/3
3/4
0/0
–
–
4/5
3/3

Errors

13/16
9/13
10/14
5/6
17/20
14/15
9/9

Deps
#
20
23
43
22
25
28
13

Type
ObRC
ObRed
SbRC
Free
ObQ
RNR
SbEm

Table 3: Distribution of error types in the development
sets; frequencies for MSTParser listed ﬁrst and MaltParser
second. The columns Arg and Link give frequencies for
Arg/Link errors occurring without the other error type, while
A+L give frequencies for joint Arg and Link errors.

can only be inferred indirectly from the basic de-
pendency representation (cf. Section 4.2). An-
other category where Arg errors are frequent is
RNR, where all such errors consist in attaching
the relevant dependent to the second conjunct in-
stead of to the ﬁrst.6 Thus, in the example in Fig-
ure 1f, both parsers found the conj relation be-
tween puzzled and angered but attached by to the
second verb.

Global errors are most frequent for RNR, prob-
ably indicating that coordinate structures are difﬁ-
cult to parse in general, and for ObQ (especially
for MaltParser), probably indicating that ques-
tions are not well represented in the training set
even after the addition of QB data.7 As noted
in Section 4.1, this may be partly due to the fact
that conversion to Stanford dependencies did not
seem to work as well for QB as for the WSJ data.
Another problem is that the part-of-speech tagger
used was trained on WSJ data only and did not
perform as well on the ObQ data. Uses of What as
a determiner were consistently mistagged as pro-
nouns, which led to errors in parsing. Thus, for
the example in Figure 1e, both parsers produced
the correct analysis except that, because of the tag-
ging error, they treated What rather than effect as
the head of the wh-phrase, which counts as an er-
ror in the evaluation.

In order to get a closer look speciﬁcally at the
Arg errors, Table 4 gives the confusion matrix

6In the Stanford scheme, an argument or adjunct must be
attached to the ﬁrst conjunct in a coordination to indicate that
it belongs to both conjuncts.

7Parsers trained without QB had twice as many global

errors.

840

Sb Ob POb EmSb EmOb Other Total
–
2/1
Sb
6/6
2/3
Ob
14/13
2/0
POb
6/5
EmSb
1/1
EmOb 0/0
4/7
32/32
Total

0/0
–
7/5
4/2
3/1
5/4 14/8

2/1
4/2
5/8
1/2
1/6
13/19

0/0
0/0
–
0/0
0/0
0/0

0/0
0/1
0/0
–
0/0
0/1

0/0
0/0
0/0
0/0
–
0/0

Table 4: Confusion matrix for Arg errors (excluding RNR
and using parsers trained on QB for ObQ); frequencies for
MSTParser listed ﬁrst and MaltParser second. The column
Other covers errors where the function is left unspeciﬁed or
the argument is attached to the wrong head.

for such errors, showing which grammatical func-
tions are mistaken for each other, with an extra
category Other for cases where the function is left
unspeciﬁed by the parser or the error is an attach-
ment error rather than a labeling error (and ex-
cluding the RNR category because of the special
nature of the Arg errors in this category). The
results again conﬁrm that the two parsers make
very few errors on subjects and objects clause-
internally. The few cases where an object is
mistaken as a subject occur in ObQ, where both
parsers perform rather poorly in general. By con-
trast, there are many more errors on prepositional
objects and on embedded subjects and objects. We
believe an important part of the explanation for
this pattern is to be found in the Stanford depen-
dency representation, where subjects and objects
are marked as such but all other functions real-
ized by wh elements are left unspeciﬁed (using the
generic rel dependency), which means that the re-
covery of these functions currently has to rely on
heuristic rules as described in Section 4.2. Finally,
we think it is possible to observe the tendency for
MaltParser to be more accurate at local labeling
decisions – reﬂected in fewer cross-label confu-
sions – and for MSTParser to perform better on
more distant attachment decisions – reﬂected in
fewer errors in the Other category (and in fewer
Link errors).

7 Conclusion

In conclusion,
the capacity of MSTParser and
MaltParser to recover unbounded dependencies is
very similar on the macro and weighted macro
level, but
there is a clear distinction in their
strengths – constructions involving more distant

dependencies such as ObQ, RNR and SbEm for
MSTParser and constructions with more locally
deﬁned conﬁgurations such as ObRC, ObRed,
SbRC and Free for MaltParser. This is a pattern
that has been observed in previous evaluations of
the parsers and can be explained by the global
learning and inference strategy of MSTParser and
the richer feature space of MaltParser (McDonald
and Nivre, 2007).

Perhaps more interestingly, the accuracies of
MSTParser and MaltParser are only slightly be-
low the best performing systems in Rimell et al.
(2009) – C&C and Enju. This is true even though
MSTParser and MaltParser have not been engi-
neered speciﬁcally for English and lack special
mechanisms for handling unbounded dependen-
cies, beyond the simple post-processing heuristic
used to extract them from the output trees. Thus,
it is reasonable to speculate that the addition of
such mechanisms could lead to computationally
lightweight parsers with the ability to extract un-
bounded dependencies with high accuracy.

Acknowledgments
We thank Marie-Catherine de Marneffe for great
help with the Stanford parser and dependency
scheme, Llu´ıs M`arquez and Jes´us Gim´enez for
great support with SVMTool, Josef van Gen-
abith for sharing the QuestionBank data, and
Stephen Clark and Mark Steedman for helpful
comments on the evaluation process and the pa-
per. Laura Rimell was supported by EPSRC grant
EP/E035698/1 and Carlos G´omez-Rodr´ıguez
by MEC/FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus,
Bolsas Estadas INCITE/FSE coﬁnanced).

References
Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quanti-
tatively comparing the syntactic coverage of English
grammars. In Proceedings of 4th DARPA Workshop,
306–311.

Briscoe, T., J. Carroll, and R. Watson. 2006. The sec-
In Proceedings

ond release of the RASP system.

841

McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of EACL, 81–88.

McDonald, R. and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing.
In Proceedings of IWPT, 122–131.

McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, 91–98.

McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL, 216–
220.

McDonald, R.. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.

Miyao, Y. and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proceedings of ACL, 83–90.

Nivre, J., J. Hall, and J. Nilsson. 2006a. MaltParser:
A data-driven parser-generator for dependency pars-
ing. In Proceedings of LREC, 2216–2219.

Nivre, J., J. Hall, J. Nilsson, G. Eryi˘git, and S. Mari-
nov. 2006b. Labeled pseudo-projective dependency
parsing with support vector machines. In Proceed-
ings of CoNLL, 221–225.

Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryiˇgit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13:95–135.

Nivre, J. 2009. Non-projective dependency parsing
In Proceedings of ACL-

in expected linear time.
IJCNLP, 351–359.

Rimell, L., S. Clark, and M. Steedman. 2009. Un-
bounded dependency recovery for parser evaluation.
In Proceedings EMNLP, 813–821.

Sagae, K. and A. Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL HLT: Short
Papers, 129–132.

of the COLING/ACL 2006 Interactive Presentation
Sessions, 77–80.

Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
In Pro-

task on multilingual dependency parsing.
ceedings of CoNLL, 149–164.

Cahill, A., M. Burke, R. O’Donovan, J. Van Genabith,
and A. Way.
2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of ACL, 320–327.

Carreras, X., M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for ef-
ﬁcient, feature-rich parsing.
In Proceedings of
CoNLL, 9–16.

Charniak, E. and M. Johnson. 2005. Coarse-to-ﬁne n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL, 173–180.

Clark, S. and J. R. Curran. 2007. Wide-coverage ef-
ﬁcient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493–552.

de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses.
In Proceedings of
LREC.

Gim´enez, J. and L. M`arquez. 2004. SVMTool: A gen-
eral POS tagger generator based on support vector
machines. In Proceedings of LREC.

Hall, J., J. Nivre, and J. Nilsson. 2006. Discriminative
classiﬁers for deterministic dependency parsing. In
Proceedings of the COLING/ACL 2006 Main Con-
ference Poster Sessions, 316–323.

Hall, J., J. Nilsson, J. Nivre, G. Eryi˘git, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task.

Huang, L. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, 586–594.

Judge, J., A. Cahill, and J. van Genabith. 2006. Ques-
tionBank: Creating a corpus of parse-annotated
questions. In Proceedings of COLING-ACL, 497–
504.

Klein, D. and C. D. Manning. 2003. Accurate unlexi-

calized parsing. In Proceedings of ACL, 423–430.

K¨ubler, S., R. McDonald, and J. Nivre. 2008. Depen-

dency Parsing. Morgan and Claypool.

McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, 122–131.

