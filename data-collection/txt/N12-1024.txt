










































Implicitly Intersecting Weighted Automata using Dual Decomposition


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 232–242,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Implicitly Intersecting Weighted Automata using Dual Decomposition∗

Michael J. Paul and Jason Eisner
Department of Computer Science / Johns Hopkins University

Baltimore, MD 21218, USA
{mpaul,jason}@cs.jhu.edu

Abstract

We propose an algorithm to find the best path
through an intersection of arbitrarily many
weighted automata, without actually perform-
ing the intersection. The algorithm is based on
dual decomposition: the automata attempt to
agree on a string by communicating about fea-
tures of the string. We demonstrate the algo-
rithm on the Steiner consensus string problem,
both on synthetic data and on consensus de-
coding for speech recognition. This involves
implicitly intersecting up to 100 automata.

1 Introduction

Many tasks in natural language processing in-
volve functions that assign scores—such as log-
probabilities—to candidate strings or sequences.
Often such a function can be represented compactly
as a weighted finite state automaton (WFSA). Find-
ing the best-scoring string according to a WFSA is
straightforward using standard best-path algorithms.

It is common to construct a scoring WFSA by
combining two or more simpler WFSAs, taking ad-
vantage of the closure properties of WFSAs. For ex-
ample, consider noisy channel approaches to speech
recognition (Pereira and Riley, 1997) or machine
translation (Knight and Al-Onaizan, 1998). Given
an input f , the score of a possible English tran-
scription or translation e is the sum of its language
model score log p(e) and its channel model score
log p(f | e). If each of these functions of e is repre-
sented as a WFSA, then their sum is represented as
the intersection of those two WFSAs.

WFSA intersection corresponds to constraint con-
junction, and hence is often a mathematically natu-
ral way to specify a solution to a problem involving

∗The authors are grateful to Damianos Karakos for provid-
ing tools and data for the ASR experiments. This work was
supported in part by an NSF Graduate Research Fellowship.

multiple soft constraints on a desired string. Unfor-
tunately, the intersection may be computationally in-
efficient in practice. The intersection of K WFSAs
having n1, n2, . . . , nK states may have n1·n2 · · ·nK
states in the worst case.1

In this paper, we propose a more efficient method
for finding the best path in an intersection without
actually computing the full intersection. Our ap-
proach is based on dual decomposition, a combina-
torial optimization technique that was recently intro-
duced to the vision (Komodakis et al., 2007) and lan-
guage processing communities (Rush et al., 2010;
Koo et al., 2010). Our idea is to interrogate the
several WFSAs separately, repeatedly visiting each
WFSA to seek a high-scoring path in each WFSA
that agrees with the current paths found in the other
WSFAs. This iterative negotiation is reminiscent of
message-passing algorithms (Sontag et al., 2008),
while the queries to the WFSAs are reminiscent of
loss-augmented inference (Taskar et al., 2005).

We remark that a general solution whose asymp-
totic worst-case runtime beat that of naive intersec-
tion would have important implications for com-
plexity theory (Karakostas et al., 2003). Our ap-
proach is not such a solution. We have no worst-case
bounds on how long dual decomposition will take to
converge in our setting, and indeed it can fail to con-
verge altogether.2 However, when it does converge,
we have a “certificate” that the solution is optimal.

Dual decomposition is usually regarded as a
method for finding an optimal vector in Rd, sub-
ject to several constraints. However, it is not ob-
vious how best to represent strings as vectors—they

1Most regular expression operators combine WFSA sizes
additively. It is primarily intersection and its close relative,
composition, that do so multiplicatively, leading to inefficiency
when two large WFSAs are combined, and to exponential
blowup when many WFSAs are combined. Yet these operations
are crucially important in practice.

2An example that oscillates can be constructed along lines
similar to the one given by Rush et al. (2010).

232



have unbounded length, and furthermore the abso-
lute position of a symbol is not usually significant in
evaluating its contribution to the score.3 One con-
tribution of this work is that we propose a general,
flexible scheme for converting strings to feature vec-
tors on which the WFSAs must agree. In principle
the number of features may be infinite, but the set
of “active” features is expanded only as needed un-
til the algorithm converges. Our experiments use a
particular instantiation of our general scheme, based
on n-gram features.

We apply our method to a particular task: finding
the Steiner consensus string (Gusfield, 1997) that
has low total edit distance to a number of given, un-
aligned strings. As an illustration, we are pleased to
report that “alia” and “aian” are the consensus
popular names for girls and boys born in the U.S. in
2010. We use this technique for consensus decoding
from speech recognition lattices, and to reconstruct
the common source of up to 100 strings corrupted by
random noise. Explicit intersection would be astro-
nomically expensive in these cases. We demonstrate
that our approach tends to converge rather quickly,
and that it finds good solutions quickly in any case.

2 Preliminaries

2.1 Weighted Finite State Automata

A weighted finite state automaton (WFSA) over the
finite alphabet Σ is an FSA that has a cost or weight
associated with each arc. We consider the case of
real-valued weights in the tropical semiring. This is
a fancy way of saying that the weight of a path is the
sum of its arc weights, and that the weight of a string
is the minimum weight of all its accepting paths (or
∞ if there are none).

When we intersect two WFSAs F and G, the ef-
fect is to add string weights: (F ∩G)(x) = F (x) +
G(x). Our problem is to find the x that minimizes
this sum, but without constructing F ∩ G to run a
shortest-path algorithm on it.

2.2 Dual Decomposition

The trick in dual decomposition is to decompose
an intractable global problem into two or more

3Such difficulties are typical when trying to apply structured
prediction or optimization techniques to predict linguistic ob-
jects such as strings or trees, rather than vectors.

tractable subproblems that can be solved indepen-
dently. If we can somehow combine the solutions
from the subproblems into a “valid” solution to the
global problem, then we can avoid optimizing the
joint problem directly. A valid solution is one in
which the individual solutions of each subproblem
all agree on the variables which are shared in the
joint problem. For example, if we are combining a
parser with a part-of-speech tagger, the tag assign-
ments from both models must agree in the final so-
lution (Rush et al., 2010); if we are intersecting a
translation model with a language model, then it is
the words that must agree (Rush and Collins, 2011).

More formally, suppose we want to find a global
solution that is jointly optimized among K sub-
problems: argminx

∑K
k=1 fk(x). Suppose that x

ranges over vectors. Introducing an auxiliary vari-
able xk for each subproblem fk allows us to equiv-
alently formulate this as the following constrained
optimization problem:

argmin
{x,x1,...,xK}

K∑
k=1

fk(xk) s.t. (∀k)xk = x (1)

For any set of vectors λk that sum to 0,
∑K

k=1 λk =
0, Komodakis et al. (2007) show that the following
Lagrangian dual is a lower bound on (1):4

min
{x1,...,xK}

K∑
k=1

fk(xk) + λk · xk (2)

where the Lagrange multiplier vectors λk can be
used to penalize solutions that do not satisfy the
agreement constraints (∀k)xk = x. Our goal is to
maximize this lower bound and hope that the result
does satisfy the constraints. The graphs in Fig. 2
illustrate how we increase the lower bound over
time, using a subgradient algorithm to adjust the λ’s.
At each subgradient step, (2) can be computed by
choosing each xk = argminxk fk(xk) +λk ·xk sep-
arately. In effect, each subproblem makes an inde-
pendent prediction xk influenced by λk, and if these
outputs do not yet satisfy the agreement constraints,
then the λk are adjusted to encourage the subprob-
lems to agree on the next iteration. See Sontag et al.
(2011) for a detailed tutorial on dual decomposition.

4The objective in (2) can always be made as small as in (1)
by choosing the vectors (x1, . . . xK) that minimize (1) (because
then

P
k λk · xk =

P
k λk · x = 0 · x = 0). Hence (2) ≤ (1).

233



3 WFSAs and Dual Decomposition

Given K WFSAs, F1, . . . , FK , we are interested in
finding the string x which has the best score in the
intersection F1∩ . . .∩FK . The lowest-cost string in
the intersection of all K machines is defined as:

argmin
x

∑
k

Fk(x) (3)

As explained above, the trick in dual decomposi-
tion is to recast (3) as independent problems of the
form argminxk Fk(xk), subject to constraints that
all xk are the same. However, it is not so clear how
to define agreement constraints on strings. Perhaps
a natural formulation is that Fk should be urged to
favor strings xk that would be read by Fk′ along a
similar path to that of xk′ . But Fk cannot keep track
of the state of Fk′ for all k′ without solving the full
intersection—precisely what we are trying to avoid.

Instead of requiring the strings xk to be equal as
in (1), we will require their features to be equal:

(∀k) γ(xk) = γ(x) (4)

Of course, we must define the features. We will use
an infinite feature vector γ(x) that completely char-
acterizes x, so that agreement of the feature vectors
implies agreement of the strings. At each subgradi-
ent step, however, we will only allow finitely many
elements of λk to become nonzero, so only a finite
portion of γ(xk) needs to be computed.5

We will define these “active” features of a string
x by constructing some unweighted deterministic
FSA, G (described in §4). The active features of x
are determined by the collection of arcs on the ac-
cepting path of x in G. Thus, to satisfy the agree-
ment constraint, xi and xj must be accepted using
the same arcs ofG (or more generally, arcs that have
the same features).

We relax the constraints by introducing a col-
lection λ = λ1, . . . , λK of Lagrange multipliers,

5The simplest scheme would define a binary feature for each
string in Σ∗. Then the nonzero elements of λk would spec-
ify punishments and rewards for outputting various strings that
had been encountered at earlier iterations: “Try subproblem k
again, and try harder not to output michael this time, as it still
didn’t agree with other subproblems: try jason instead.” This
scheme would converge glacially if at all. We instead focus on
featurizations that let subproblems negotiate about substrings:
“Try again, avoiding mi if possible and favoring ja instead.”

and defining Gλk(x) such that the features of G are
weighted by the vector λk (all of whose nonzero el-
ements must correspond to features in G). As in (2),
we assume λ ∈ Λ, where Λ = {λ :

∑
k λk = 0}.

This gives the objective:

h(λ) = min
{x1,...,xK}

∑
k

(Fk(xk) +Gλk(xk)) (5)

This minimization fully decomposes into K sub-
problems that can be solved independently. The kth
subproblem is to find argminxk Fk(xk)+Gλk(xk),
which is straightforward to solve with finite-state
methods. It is the string on the lowest-cost path
through Hk = Fk ∩ Gλk , as found with standard
path algorithms (Mohri, 2002).

The dual problem we wish to solve is
maxλ∈Λ h(λ), where h(λ) itself is a min over
{x1, . . . , xK}. We optimize λ via projected subgra-
dient ascent (Komodakis et al., 2007). The update
equation for λk at iteration t is then:

λ
(t+1)
k = λ

(t)
k + ηt

(
γ(x(t)k )−

∑
k′ γ(x

(t)
k′ )

K

)
(6)

where ηt > 0 is the step size at iteration t. This up-
date is intuitive. It moves away from the current so-
lution and toward the average solution (where they
differ), by increasing the cost of the former’s fea-
tures and reducing the cost of the latter’s features.

This update may be very dense, however, since
γ(x) is an infinite vector. So we usually only up-
date the elements of λk that correspond to the small
finite set of active features (the other elements are
still “frozen” at 0), denoted Θ. This is still a valid
subgradient step. This strategy is incorrect only if
the updates for all active features are 0—in other
words, only if we have achieved equality of the cur-
rently active features and yet still the {xk} do not
agree. In that case, we must choose some inactive
features that are still unequal and allow the subgra-
dient step to update their λ coefficients to nonzero,
making them active. At the next step of optimiza-
tion, we must expand G to consider this enlarged set
of active features.

4 The Agreement Machine

The agreement machine (or constraint machine) G
can be thought of as a way of encoding features of

234



strings on which we enforce agreement. There are a
number of different topologies for G that might be
considered, with varying degrees of efficiency and
utility. Constructing G essentially amounts to fea-
ture engineering; as such, it is unlikely that there
is a universally optimal topology of G. Neverthe-
less, there are clearly bad ways to build G, as not
all topologies are guaranteed to lead to an optimal
solution. In this section, we lay out some abstract
guidelines for appropriateG construction, before we
describe specific topologies in the later subsections.

Most importantly, we should design G so that it
accepts all strings in F1∩ . . .∩FK . This is to ensure
that it accepts the string that is the optimal solution
to the joint problem. If G did not accept that string,
then neither would Hk = Fk ∩G, and our algorithm
would not be able to find it.

Even ifHk can accept the optimal string, it is pos-
sible that this string would never be the best path in
this machine, regardless of λ. For example, suppose
G is a single-state machine with self-loops accept-
ing each symbol in the alphabet (i.e. a unigram ma-
chine). Suppose Hk outputs the string aaa in the
current iteration, but we would like the machines to
converge to aaaaa. We would lower the weight of
λa to encourage Hk to output more of the symbol a.
However, if Hk has a cyclic topology, then it could
happen that a negative value of λa could create a
negative-weight cycle, in which the lowest-cost path
throughHk is infinitely long. It might be that adjust-
ing λa can change the best string to either aaa or
aaaaaaaaa. . . (depending on whether a cycle af-
ter the initial aaa has positive or negative weight),
but never the optimal aaaaa. On the other hand,
if G instead encoded 5-grams, this would not be a
problem because a path through a 5-gram machine
could accept aaaaa without traversing a cycle.

Finally, agreeing on (active) features does not
necessarily mean that all xk are the same string. For
example, if we again use a unigram G (that is, Θ =
Σ, the set of unigrams), then γΘ(abc) = γΘ(cba),
where γΘ returns a feature vector where all but the
active features are zeroed out. In this instance, we
satisfy the constraints imposed by G, even though
we have not satisfied the constraint we truly care
about: that the strings agree.

To summarize, we will aim to choose Θ such that
G has the following characteristics:

1. The language L(Fk ∩G) = L(Fk); i.e. G does
not restrict the set of strings accepted by Fk.

2. When γΘ(xi) = γΘ(xj), typically xi = xj .

3. ∃λ ∈ Λ s.t. argminx Fk(x) +Gλk(x) =
argminx

∑
k′ Fk′(x), i.e., the optimal string

can be the best path in Fk ∩ G.6 This may not
be the case if G is cyclic.

The first of these is required during every itera-
tion of the algorithm in order to maintain optimality
guarantees. However, even if we do not satisfy the
latter two points, we may get lucky and the strings
themselves will agree upon convergence, and no fur-
ther work is required. Furthermore, the unigram ma-
chine G used in the above examples, despite break-
ing these requirements, has the advantage of being
very efficient to intersect with F . This motivates
our “active feature” strategy of using a simpleG ini-
tially, and incrementally altering it as needed, for ex-
ample if we satisfy the constraints but the strings do
not yet match. We discuss this in §4.2.

4.1 N-Gram Construction of G
In principle, it is valid to use any G that satisfies the
guidelines above, but in practice, some topologies
will lead to faster convergence than others.

Perhaps the most obvious form is a simple vector
encoding of strings, e.g. “a at position 1”, “b at po-
sition 2”, and so on. As a WFSA, this would simply
have one state represent each position, with arcs for
each symbol going from position i to i + 1. This is
essentially a unigram machine where the loops have
been “unrolled” to also keep track of position.

However, early experiments showed that with
this topology for G, our algorithm converged very
slowly, if at all. What goes wrong? The problem
stems from the fact that the strings are unaligned and
of varying length, and it is difficult to get the strings
to agree quickly at specific positions. For example,
if two subproblems have b at positions 6 and 8 in the
current iteration, they might agree at position 7—but
our features don’t encourage this. The Lagrangian
update would discourage accepting b at 6 and en-
courage b at 8 (and vice versa), without giving credit

6It is not always possible to construct a G to satisfy this
property, as the Lagrangian dual may not be a tight bound to the
original problem.

235



for meeting in the middle. Further, these features do
not encourage the subproblems to preserve the rela-
tive order of neighboring symbols, and strings which
are almost the same but slightly misaligned will be
penalized essentially everywhere. This is an ineffec-
tive way for the subproblems to communicate.

In this paper, we focus on the feature set we
found to work the best in our experiments: the
strings should agree on their n-gram features, such
as “number of occurrences of the bigram ab.” Even
if we don’t yet know precisely where ab should ap-
pear in the string, we can still move toward conver-
gence if we try to force the subproblems to agree on
whether and how often ab appears at all.

To encode n-gram features in a WFSA, each state
represents the (n−1)-gram history, and all arcs leav-
ing the state represent the final symbol in the n-
gram, weighted by the score of that n-gram. The
machine will also contain start and end states, with
appropriate transitions to/from the n-gram states.
For example, if the trigram abc has weight λabc,
then the trigram machine will encode this as an arc
with the symbol c leaving the state representing ab,
and this arc will have weight λabc. If our feature
set also contains 1- and 2-grams, then the arc in this
example would incorporate the weights of all of the
corresponding features: λabc + λbc + λc.

A drawback is that these features give no infor-
mation about where in the string the n-grams should
occur. In a long string, we might want to encour-
age or discourage an n-gram in a certain “region” of
the string. Our features can only encourage or dis-
courage it everywhere in the string, which may lead
to slow convergence. Nevertheless, in our particular
experimental settings, we find that this works better
than other topologies we have considered.

Sparse N-Gram Encoding A full n-gram lan-
guage model requires ≈ |Σ|n arcs to encode as a
WFSA. This could be quite expensive. Fortunately,
large n-gram models can be compacted by using
failure arcs (φ-arcs) to encode backoff (Allauzen et
al., 2003). These arcs act as �-transitions that can
be taken only when no other transition is available.
They allow us to encode the sparse subset of n-
grams that have nonzero Lagrangians. We encodeG
such that all features whose λ value is 0 will back off
to the next largest n-gram having nonzero weight.

This form of G still accepts Σ∗ and has the same
weights as a dense representation, but could require
substantially fewer states.

4.2 Incrementally Expanding G
As mentioned above, we may need to alter G as we
go along. Intuitively, we may want to start with fea-
tures that are cheap to encode, to move the param-
eters λ to a good part of the solution space, then
incrementally bring in more expensive features as
needed. Shorter n-grams require a smaller G and
will require a shorter runtime per iteration, but if
they are too short to be informative, then they may
require many more iterations to reach convergence.
In an extreme case, we may reach a point where
the subproblems all agree on n-grams currently in
Θ, but the actual strings still do not match. Wait-
ing until we hit such a point may be unnecessarily
slow. We experimented with periodically increas-
ing n (e.g. adding trigrams to the feature set if we
haven’t converged with bigrams after a fixed num-
ber of iterations), but this is expensive, and it is not
clear how to define a schedule for increasing the or-
der of n. We instead present a simple and effective
heuristic for bringing in more features.

The idea is that if the subproblem solutions cur-
rently disagree on counts of the bigrams ab and
bc, then an abc feature may be unnecessary, since
the subproblems could still make progress with only
these bigram constraints. However, once the sub-
problems agree on these two bigrams, but disagree
on trigram abc, we bring this into the feature set Θ.
More generally, we add an (n+ 1)-gram to the fea-
ture set if the current strings disagree on its counts
despite agreeing on its n-gram prefix and n-gram
suffix (which need not necessarily be Θ). This se-
lectively brings in larger n-grams to target portions
of the strings that may require longer context, while
keeping the agreement machine small.

Algorithm 1 gives pseudocode for our complete
algorithm when using n-gram features with this in-
cremental strategy. To summarize, we solve for each
xk using the current λk, and if all the strings agree,
we return them as the optimal solution. Otherwise,
we update λk and repeat. At each iteration, we check
for n-gram agreement, and bring in select (n + 1)-
grams to the feature set as appropriate.

Finally, there is another instance where we might

236



Algorithm 1 The dual decomposition algorithm
with n-gram features.

Initialize Θ to some initial set of n-gram features.
for t = 1 to T do

for k = 1 to K do
Solve xk = argminx(Fk ∩ Gλk)(x) with a
shortest-path algorithm

end for
if (∀i, j)xi = xj then

return {x1, . . . , xK}
else

Θ = Θ ∪ {z ∈ Σ∗ : all xk agree on the features
corresponding to the length-(|z| − 1) prefix and
suffix of z, but not on z itself}
for k = 1 to K do

Update λk according to equation (6)
Create Gλk to encode the features Θ

end for
end if

end for

need to expand G, which we omit from the pseu-
docode for conciseness. If both Fk and G are cyclic,
then there is a chance that there will be a negative-
weight cycle inFk∩Gλk . (If at least one of these ma-
chines is acyclic, then this is not a problem, because
their intersection yields a finite set.) In the case of
a negative-weight cycle, the best path is infinitely
long, and so the algorithm will either return an error
or fail to terminate. If this happens, then we need to
backtrack, and either decrease the subgradient step
size to avoid moving into this territory, or alter G to
expand the cycles. This can be done by unrolling
loops to keep track of more information—when en-
coding n-gram features with G, this amounts to ex-
panding G to encode higher order n-grams. When
using a sparse G with φ-arcs, it may also be neces-
sary to increase the minimum n-gram history that
is used for back-off. For example, instead of al-
lowing bigrams to back off to unigrams, we might
force G to encode the full set of bigrams (not just
bigrams with nonzero λ) in order to avoid cycles
in the lower order states. Our strategy for avoiding
negative-weight cycles is detailed in §5.1.

5 Experiments with Consensus Decoding

To best highlight the utility of our approach, we con-
sider applications that must (implicitly) intersect a
large number of WFSAs. We will demonstrate that,

in many cases, our algorithm converges to an exact
solution on problems involving 10, 25, and even 100
machines, all of which would be hopeless to solve
by taking the full intersection.

We focus on the problem of solving for the Steiner
consensus string: given a set of K strings, find the
string in Σ∗ that has minimal total edit distance to
all strings in the set. This is an NP-hard problem
that can be solved as an intersection of K machines,
as we will describe in §5.2. The consensus string
also gives an implicit multiple sequence alignment,
as we discuss in §6.

We begin with the application of minimum Bayes
risk decoding of speech lattices, which we show can
reduce to the consensus string problem. We then ex-
plore the consensus problem in depth by applying it
to a variety of different inputs.

5.1 Experimental Details
We initialize Θ to include both unigrams and bi-
grams, as we find that unigrams alone are not pro-
ductive features in these experiments. As we expand
Θ, we allow it to include n-grams up to length five.

We run our algorithm for a maximum of 1000 iter-
ations, using a subgradient step size of α/(t + 500)
at iteration t, which satisfies the general properties
to guarantee asymptotic convergence (Spall, 2003).
We initialize α to 1 and 10 in the two subsections, re-
spectively. We halve α whenever we hit a negative-
weight cycle and need to backtrack. If we still get
negative-weight cycles after α ≤ 10−4 then we reset
α and increase the minimum order of n which is en-
coded in G. (If n is already at our maximum of five,
then we simply end without converging.) In the case
of non-convergence after 1000 iterations, we select
the best string (according to the objective) from the
set of strings that were solutions to any subproblem
at any point during optimization.

Our implementation uses OpenFST 1.2.8 (Al-
lauzen et al., 2007).

5.2 Minimum Bayes Risk Decoding for ASR
We first consider the task of automatic speech recog-
nition (ASR). Suppose x∗ is the true transcription
(a string) of an spoken utterance, and π(w) is an
ASR system’s probability distribution over possi-
ble transcriptions w. The Bayes risk of an out-
put transcription x is defined as the expectation

237



∑
w π(w) `(x,w) for some loss function ` (Bickel

and Doksum, 2006). Minimum Bayes risk decoding
(Goel and Byrne, 2003) involves choosing the x that
minimizes the Bayes risk, rather than simply choos-
ing the x that maximizes π(x) as in MAP decoding.

As a reasonable approximation, we will take the
expectation over just the strings w1, . . . , wK that are
most probable under π. A common loss function
is the Levenshtein distance because this is generally
used to measure the word error rate of ASR output.
Thus, we seek a consensus transcription

argmin
x

K∑
k=1

πk d(x,wk) (7)

that minimizes a weighted sum of edit distances to
all of the top-K strings, where high edit distance
to more probable strings is more strongly penal-
ized. Here d(x,w) is the unweighted Levenshtein
distance between two strings, and πk = π(wk). If
each πk = 1/K, then argminx is known as the
Steiner consensus string, which is NP-hard to find
(Sim and Park, 2003). Equation (7) is a weighted
generalization of the Steiner problem.

Given an input string wk, it is straightforward
to define our WFSA Fk such that Fk(x) computes
πk d(x,wk). A direct construction of Fk is as fol-
lows. First, create a “straight line” WFSA whose
single path accepts (only) wk; each each state corre-
sponds to a position in wk. These arcs all have cost
0. Now add various arcs with cost πk that permit
edit operations. For each arc labeled with a symbol
a ∈ Σ, add competing “substitution” arcs labeled
with the other symbols in Σ, and a competing “dele-
tion” arc labeled with �; these have the same source
and target as the original arc. Also, at each state, add
a self-loop labeled with each symbol in Σ; these are
“insertion” arcs. Each arc that deviates from wk has
a cost of πk, and thus the lowest-cost path through
Fk accepting x has weight πk d(x,wk).

The consensus objective in Equation (7) can be
solved by finding the lowest-cost path in F1 ∩ . . . ∩
FK , and we can solve this best-path problem using
the dual decomposition algorithm described above.

5.2.1 Experiments
We ran our algorithm on Broadcast News data, us-

ing 226 lattices produced by the IBM Attila decoder

0 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WE WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I DON’T WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WELL I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> THEY WANT TO BE TAKING A DEEP BREATH NOW </s>

300 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WE WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I DON’T WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WELL I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> WELL WANT TO BE TAKING A DEEP BREATH NOW </s>

375 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I DON’T WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>

472 <s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>
<s> I WANT TO BE TAKING A DEEP BREATH NOW </s>

Figure 1: Example run of the consensus problem on
K = 25 strings on a Broadcast News utterance, showing
x1, . . . , x5 at the 0th, 300th, 375th, and 472nd iterations.

(Chen et al., 2006; Soltau et al., 2010) on a subset
of the NIST dev04f data, using models trained by
Zweig et al. (2011). For each lattice, we found the
consensus of the top K = 25 strings.

85% of the problems converged within 1000 it-
erations, with an average of 147.4 iterations. We
found that the true consensus was often the most
likely string under π, but not always—this was true
70% of the time. In the Bayes risk objective we are
optimizing in equation (7)—the expected loss—our
approach averaged a score of 1.59, while always tak-
ing the top string gives only a slightly worse average
of 1.66. 8% of the problems encountered negative-
weight cycles, which were all resolved either by de-
creasing the step size or encoding larger n-grams.

5.3 Investigating Consensus Performance with
Synthetic Data

The above experiments demonstrate that we can ex-
actly find the best path in the intersection of 25
machines—an intersection that could not feasibly be
constructed in practice. However, these experiments
do not exhaustively explore how dual decomposition
behaves on the Steiner string problem in general.

Above, we experimented with only a fixed num-
ber of input strings, which were generally similar to
one another. There are a variety of other inputs to the
consensus problem which might lead to different be-
havior and convergence results, however. If we were
to instead run this experiment on DNA sequences
(for example, if we posit that the strings are all muta-
tions of the same ancestor), the alphabet {A,T,C,G}

238



0 5 10 15 20

Runtime (s)

0

10

20

30

40

50

60

70

80

90

S
co

re

K=50,`=10,|Σ|=10,µ=0.2

Primal

Dual

0 10 20 30 40 50 60

Runtime (s)

0

10

20

30

40

50

S
co

re

K=10,`=15,|Σ|=20,µ=0.4

Primal

Dual

0 5 10 15 20 25 30 35

Runtime (s)

0

5

10

15

20

25

30

35

40

S
co

re

K=5,`=50,|Σ|=5,µ=0.1

Primal

Dual

Figure 2: The algorithm’s behavior on three specific consensus problems. The curves show the current values of
the primal bound (based on the best string at the current iteration) and dual bound h(λ). The horizontal axis shows
runtime. Red upper triangles are placed every 10 iterations, while blue lower triangles are placed for every 10%
increase in the size of the feature set Θ.

K ` |Σ| µ Conv. Iters. Red.
5 100 5 0.1 68% 257 (±110) 24%
5 100 5 0.2 0% – 8%
5 50 5 0.1 80% 123 (± 65) 20%
5 50 5 0.2 10% 436 (±195) 18%

10 50 5 0.1 69% 228 (±164) 18%
10 50 5 0.2 0% – 8%
10 50 5 0.4 0% – 3%
10 30 10 0.1 100% 50 (± 69) 13%
10 30 10 0.2 93% 146 (±142) 20%
10 30 10 0.4 0% – 16%
10 15 20 0.1 100% 26 (± 6) 1%
10 15 20 0.2 98% 43 (± 18) 10%
10 15 20 0.4 63% 289 (±217) 18%
10 15 20 0.8 0% – 11%
25 15 20 0.1 98% 30 (± 5) 0%
25 15 20 0.2 92% 69 (±112) 6%
25 15 20 0.4 55% 257 (±149) 16%
25 15 20 0.8 0% – 12%
50 10 10 0.2 68% 84 (±141) 0%
50 10 10 0.4 21% 173 (± 94) 9%

100 10 10 0.2 44% 147 (±220) 0%
100 10 10 0.4 13% 201 (±138) 6%

Table 1: A summary of results for various consensus
problems, as described in §5.3.

is so small that n-grams are likely to be repeated in
many parts of the strings, and the lack of position in-
formation in our features could make it hard to reach
agreement. Another interesting case is when the in-
put strings have little or nothing in common—can
we still converge to an optimal consensus in a rea-
sonable number of iterations?

We can investigate many different cases by cre-
ating synthetic data, where we tune the number of
input strings K, the length of the strings, the size of
the vocabulary |Σ|, as well as how similar the strings
are. We do this by randomly generating a base string
x∗ ∈ Σ` of length `. We then generate K random

strings w1, . . . , wK , each by passing x∗ through a
noisy edit channel, where each position has inde-
pendent probability µ of making an edit. For each
position in x∗, we uniformly sample once among
the three types of edits (substitution, insertion, dele-
tion), and in the case of the first two, we uniformly
sample from the vocabulary (excluding the current
symbol for substitution). The larger µ, the more mu-
tated the strings will be. For small µ or large K, the
optimal consensus ofw1, . . . , wK will usually be x∗.

Table 1 shows results under various settings. Each
line presents the percentage of 100 examples that
converge within the iteration limit, the average num-
ber of iterations to convergence (± standard devi-
ation) for those that converged, and the reduction
in the objective value that is obtained over a sim-
ple baseline of choosing the best string in the input
set, to show how much progress the algorithm makes
between the 0th and final iteration.

As expected, a higher mutation probability slows
convergence in all cases, as does having longer in-
put strings. These results also confirm our hypothe-
sis that a small alphabet would lead to slow conver-
gence when using small n-gram features. For these
types of strings, which might show up in biological
data, one would likely need more informative con-
straints than position-agnostic n-grams.

Figure 2 shows example runs on problems gen-
erated at three different parameter settings. We plot
the objective value as a function of runtime, showing
both the primal objective (3) that we hope to mini-
mize, which we measure as the quality of the best
solution among the {xk} that are output at the cur-

239



rent iteration, and the dual objective (5) that our al-
gorithm is maximizing. The dual problem (which is
concave in λ) lower bounds the primal. If the two
functions ever touch, we know the solution to the
dual problem is in the set of feasible solutions to the
original primal problem we are attempting to solve,
and indeed must be optimal. The figure shows that
the dual function always has an initial value of 0,
since we initialize each λk = 0, and then Fk will
simply return the input wk as its best solution (since
wk has zero distance to itself). As the algorithm be-
gins to enforce the agreement constraints, the value
of the relaxed dual problem gradually worsens, until
it fully satisfies the constraints.

These plots indicate the number of iterations that
have passed and the number of active features. We
see that the time per iteration increases as the num-
ber of features increases, as expected, because more
(and longer) n-grams are being encoded by G.

The three patterns shown are typical of almost all
the trials we examined. When the solution is in the
original input set (a likely occurrence for large K or
small µ · `), the primal value will be optimal from
the start, and our algorithm only has to prove its op-
timality. For more challenging problems, the primal
solution may jump around in quality at each iteration
before settling into a stable part of the space.

To investigate how different n-gram sizes affect
convergence rates, we experiment with using the en-
tire set of n-grams (for a fixed n) for the duration
of the optimization procedure. Figure 3 shows con-
vergence rates (based on both iterations and run-
time) of different values of n for one set of param-
eters. While bigrams are very fast (average runtime
of 14s among those that converged), this converged
within 1000 iterations only 78% of the time, and
the remaining 22% end up bringing down the av-
erage speed (with an overall average runtime over a
minute). All larger n-grams converged every time;
trigrams had an average runtime of 32s. Our algo-
rithm, which begins with bigrams but brings in more
features (up to 5-grams) as needed, had an average
runtime of 19s (with 98% convergence).

6 Discussion and Future Work

An important (and motivating) property of La-
grangian relaxation methods is the certificate of op-

0 50 100 150 200

Number of Iterations
0

100

200

300

400

500

R
u
n
ti

m
e
 (

s)

K=10,`=15,|Σ|=20,µ=0.2
n=2

n=3

n=4

n=5

Figure 3: Convergence rates for a fixed set of n-grams.

timality. Even in instances where approximate algo-
rithms perform well, it could be useful to have a true
optimality guarantee. For example, our algorithm
can be used to produce reference solutions, which
are important to have for research purposes.

Under a sum-of-pairs Levenshtein objective, the
exact multi-sequence alignment can be directly ob-
tained from the Steiner consensus string and vice
versa (Gusfield, 1997). This implies that our ex-
act algorithm could be also used to find exact multi-
sequence alignments, an important problem in nat-
ural language processing (Barzilay and Lee, 2003)
and computational biology (Durbin et al., 2006) that
is almost always solved with approximate methods.

We have noted that some constraints are more
useful than others. Position-specific information is
hard to agree on and leads to slow convergence,
while pure n-gram constraints do not work as well
for long strings where the position may be impor-
tant. One avenue we are investigating is the use
of a non-deterministic G, which would allow us to
encode latent variables (Dreyer et al., 2008), such
as loosely defined “regions” within a string, and to
allow for the encoding of alignments between the
input strings. We would also like to extend these
methods to other combinatorial optimization prob-
lems involving strings, such as inference in graphi-
cal models over strings (Dreyer and Eisner, 2009).

To conclude, we have presented a general frame-
work for applying dual decomposition to implicit
WFSA intersection. This could be applied to a num-
ber of NLP problems such as language model and
lattice intersection. To demonstrate its utility on a
large number of automata, we applied it to consen-
sus decoding, determining the true optimum in a rea-
sonable amount of time on a large majority of cases.

240



References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.

Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 40–47.

Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the 12th International Con-
ference on Implementation and Application of Au-
tomata, CIAA’07, pages 11–23.

Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ’03, pages 16–
23.

Peter J. Bickel and Kjell A. Doksum. 2006. Mathemat-
ical Statistics: Basic Ideas and Selected Topics, vol-
ume 1. Pearson Prentice Hall.

Stanley F. Chen, Brian Kingsbury, Lidia Mangu, Daniel
Povey, George Saon, Hagen Soltau, and Geoffrey
Zweig. 2006. Advances in speech transcription at
IBM under the DARPA EARS program. IEEE Trans-
actions on Audio, Speech & Language Processing,
14(5):1596–1608.

Markus Dreyer and Jason Eisner. 2009. Graphical mod-
els over multiple strings. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’09, pages 101–110. As-
sociation for Computational Linguistics.

Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1080–1089, Honolulu, October.

R. Durbin, S. Eddy, A. Krogh, and G. Mitchison. 2006.
Biological Sequence Analysis. Cambridge University
Press.

Vaibhava Goel and William J. Byrne. 2003. Mini-
mum Bayes risk methods in automatic speech recog-
nition. In Wu Chou and Biing-Hwang Juan, editors,
Pattern Recognition in Speech and Language Process-
ing. CRC Press.

Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational Bi-
ology. Cambridge University Press.

George Karakostas, Richard J Lipton, and Anastasios Vi-
glas. 2003. On the complexity of intersecting finite
state automata and NL versus NP. Theoretical Com-
puter Science, pages 257–274.

Kevin Knight and Yaser Al-Onaizan. 1998. Translation
with finite-state devices. In AMTA’98, pages 421–437.

N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
Passing revisited. In Computer Vision, 2007. ICCV
2007. IEEE 11th International Conference on, pages
1–8.

Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 1288–1298.

Mehryar Mohri. 2002. Semiring frameworks and algo-
rithms for shortest-distance problems. J. Autom. Lang.
Comb., 7:321–350, January.

Fernando C. N. Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. CoRR.

Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through La-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ’11, pages 72–82.

Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’10, pages 1–11.

Jeong Seop Sim and Kunsoo Park. 2003. The consen-
sus string problem for a metric is NP-complete. J. of
Discrete Algorithms, 1:111–117, February.

H. Soltau, G. Saon, and B. Kingsbury. 2010. The IBM
Attila speech recognition toolkit. In Proc. IEEE Work-
shop on Spoken Language Technology, pages 97–102.

David Sontag, Talya Meltzer, Amir Globerson, Yair
Weiss, and Tommi Jaakkola. 2008. Tightening LP
relaxations for MAP using message-passing. In 24th
Conference in Uncertainty in Artificial Intelligence,
pages 503–510. AUAI Press.

David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Machine
Learning. MIT Press.

James C. Spall. 2003. Introduction to Stochastic Search
and Optimization. John Wiley & Sons, Inc., New
York, NY, USA, 1 edition.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured prediction
models: A large margin approach. In Proceedings of

241



the 22nd international conference on Machine learn-
ing, ICML ’05, pages 896–903.

Geoffrey Zweig, Patrick Nguyen, Dirk Van Compernolle,
Kris Demuynck, Les E. Atlas, Pascal Clark, Gregory
Sell, Meihong Wang, Fei Sha, Hynek Hermansky,
Damianos Karakos, Aren Jansen, Samuel Thomas,
Sivaram G. S. V. S., Sam Bowman, and Justine T. Kao.
2011. Speech recognition with segmental conditional
random fields: A summary of the JHU CLSP 2010
Summer Workshop. In ICASSP.

242


