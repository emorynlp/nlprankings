










































Staying Informed: Supervised and Semi-Supervised Multi-View Topical Analysis of Ideological Perspective


Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1140–1150,
MIT, Massachusetts, USA, 9-11 October 2010. c©2010 Association for Computational Linguistics

Staying Informed: Supervised and Semi-Supervised Multi-view
Topical Analysis of Ideological Perspective

Amr Ahmed
School of Computer Science
Carnegie Mellon University
amahmed@cs.cmu.edu

Eric P. Xing
School of Computer Science
Carnegie Mellon University
epxing@cs.cmu.edu

Abstract

With the proliferation of user-generated arti-
cles over the web, it becomes imperative to de-
velop automated methods that are aware of the
ideological-bias implicit in a document col-
lection. While there exist methods that can
classify the ideological bias of a given docu-
ment, little has been done toward understand-
ing the nature of this bias on a topical-level. In
this paper we address the problem of modeling
ideological perspective on a topical level using
a factored topic model. We develop efficient
inference algorithms using Collapsed Gibbs
sampling for posterior inference, and give var-
ious evaluations and illustrations of the util-
ity of our model on various document collec-
tions with promising results. Finally we give a
Metropolis-Hasting inference algorithm for a
semi-supervised extension with decent results.

1 Introduction

With the avalanche of user-generated articles over
the web, it is quite important to develop models that
can recognize the ideological bias behind a given
document, summarize where this bias is manifested
on a topical level, and provide the user with alter-
nate views that would help him/her staying informed
about different perspectives. In this paper, we fol-
low the notion of ideology as defines by Van Dijk
in (Dijk, 1998) as “a set of general abstract beliefs
commonly shared by a group of people.” In other
words, an ideology is a set of ideas that directs one’s
goals, expectations, and actions. For instance, free-
dom of choice is a general aim that directs the ac-
tions of“liberals”, whereas conservation of values is
the parallel for “conservatives”.

We can attribute the lexical variations of the word
content of a document to three factors:

• Writer Ideological Belief. A liberal writer
might use words like freedom and choice re-
gardless of the topical content of the document.
These words define the abstract notion of be-
lief held by the writer and its frequency in the
document largely depends on the writer’s style.

• Topical Content. This constitutes the main
source of the lexical variations in a given docu-
ment. For instance, a document about abortion
is more likely to have facts related to abortion,
health, marriage and relationships.

• Topic-Ideology Interaction. When a liberal
thinker writes about abortion, his/her abstract
beliefs are materialized into a set of concrete
opinions and stances, therefore, we might find
words like: pro-choice and feminism. On the
contrary, a conservative writer might stress is-
sues like pro-life, God and faith.

Given a collection of ideologically-labeled docu-
ments, our goal is to develop a computer model that
factors the document collection into a representation
that reflects the aforementioned three sources of lex-
ical variations. This representation can then be used
for:

• Visualization. By visualizing the abstract no-
tion of belief in each ideology, and the way
each ideology approaches and views main-
stream topics, the user can view and contrast
each ideology side-by-side and build the right
mental landscape that acts as the basis for
his/her future decision making.

1140



• Classification or Ideology Identification.
Given a document, we would like to tell the
user from which side it was written, and ex-
plain the ideological bias in the document at a
topical level.

• Staying Informed: Getting alternative
views1. Given a document written from per-
spective A, we would like the model to provide
the user with other documents that represent al-
ternative views about the same topic addressed
in the original document.

In this paper, we approach this problem using
Topic Models (Blei et al., 2003). We introduce a
factored topic model that we call multi-view Latent
Dirichlet Allocation or mview-LDA for short. Our
model views the word content of each document as
the result of the interaction between the document’s
idealogical and topical dimensions. The rest of this
paper is organized as follows. First, in Section 2,
we review related work, and then present our model
in Section 3. Then in Section 4, we detail a col-
lapsed Gibbs sampling algorithm for posterior infer-
ence. Sections 5 and 6 give details about the dataset
used in the evaluation and illustrate the capabilities
of our model using both qualitative and quantitative
measures. Section 7 describes and evaluates the ef-
ficacy of a semi-supervised extension, and finally in
Section 8 we conclude and list several directions for
future research.

2 Related Work

Ideological text is inherently subjective, thus our
work is related to the growing area of subjectiv-
ity analysis(Wiebe et al., 2004; Riloff et al., 2003).
The goal of this area of research is to learn to dis-
criminate between subjective and objective text. In
contrast,in modeling ideology, we aim toward con-
trasting two or more ideological perspectives each of
which is subjective in nature. Further more, subjec-
tive text can be classified into sentiments which gave
rise to a surge of work in automatic opinion min-
ing (Wiebe et al., 2004; Yu and Hatzivassiloglou,
2003; Pang et al., 2002; Turney and Littman, 2003;
Popescu and Etzioni, 2005) as well as sentiment

1In this paper, we use the words ideology, view, perspective
interchangeably to denote the same concept

analysis and product review mining (Nasukawa and
Yi, 2003; Hu and Liu, 2004; Pang and Lee, 2008;
Branavan et al., 2008; Titov and McDonald, 2008;
Titov and McDonald, 2008; Mei et al., 2007; Ling
et al., 2008). The research goal of sentiment anal-
ysis and classification is to identify language used
to convey positive and negative opinions, which dif-
fers from contrasting two ideological perspectives.
While ideology can be expressed in the form of a
sentiment toward a given topic,like abortion, ideo-
logical perspectives are reflected in many ways other
than sentiments as we will illustrate later in the pa-
per. Perhaps more related to this paper is the work
of (Fortuna et al., 2008; Lin et al., 2008) whose
goal is to detect bias in news articles via discrimina-
tive and generative approaches, respectively. How-
ever, this work still addresses ideology at an abstract
level as opposed to our approach of modeling ideol-
ogy at a topical level. Finally, independently, (Paul
and Girju, 2009) gives a construction similar to ours
however for a different task 2.

3 Multi-View Topic Models

In this section we introduce multi-view topic mod-
els, or mview-LDA for short. Our model, mview-
LDA, views each document as the result of the in-
teraction between its topical and idealogical dimen-
sions. The model seeks to explain lexical variabili-
ties in the document by attributing this variabilities
to one of those dimensions or to their interactions.
Topic models, like LDA, define a generative process
for a document collection based on a set of parame-
ters. LDA employs a semantic entity known as topic
to drive the generation of the document in question.
Each topic is represented by a topic-specific word
distribution which is modeled as a multinomial dis-
tribution over words, denoted by Multi(β). The
generative process of LDA proceeds as follows:

1. Draw topic proportions θd|α ∼ Dir(α).
2. For each word

(a) Draw a topic zn|θd ∼ Mult(θd).
(b) Draw a word wn|zn, β ∼ Multi(βzn).

In step 1 each document d samples a topic-mixing
vector θd from a Dirichlet prior. The component θd,k

2In fact, we only get to know about this related work after
our paper was accepted

1141



D

N

v

z

w

V

a2 b2

KV

a1 b1

x2

x1

Variable Meaning
w word
v document’s ideology
z topic
x1, x2 word switches, one per word (see text)
θ document-specific distribution over topics
ξ document’s expected usage of the ideology’s

background topic
Ω ideology’s background-topic
β ideology-independent topic distribution
φ ideology-specific topic distribution
λ topic bias across ideology

Figure 1: A plate diagram of the graphical model.

of this vector defines how likely topic k will appear
in document d. For each word in the document wn,
a topic indicator zn is sampled from θd, and then
the word itself is sampled from a topic-specific word
distribution specified by this indicator. Thus LDA
can capture and represent lexical variabilities via the
components of θd which represents the topical con-
tent of the document. In the next section we will ex-
plain how our new model mview-LDA can capture
other sources of lexical variabilities beyond topical
content.

3.1 Multi-View LDA

As we noted earlier, LDA captures lexical variabili-
ties due to topical content via θd and the set of top-
ics β1:K . In mview-LDA each document d is tagged
with the ideological view it represents via the ob-
served variable vd which takes values in the discrete
range: {1, 2, · · · , V } as shown in Fig. 1. For sim-
plicity, lets first assume that V = 2. The topics β1:K
retain the same meaning: a set of K multinomial
distributions each of which represents a given theme
or factual topic. In addition, we utilize an ideology-
specific topic Ωv which is again a multinomial dis-
tribution over the same vocabulary. Ωv models the
abstract belief shared by all the documents written
from view v. In other words, if v denotes the liberal
perspective, then Ωv gives high probability to words
like progressive, choice, etc. Moreover, we defined
a set of K × V topics that we refer to as ideology-
specific topics. For example, topic φv,k represents
how ideology v addresses topic k. The generative
process of a document d with ideological view vd

proceeds as follows:

1. Draw ξd ∼ Beta(a1, b1)

2. Draw topic proportions θd|α ∼ Dir(α2).

3. For each word wn

(a) Draw xn,1 ∼ Bernoulli(ξd)
(b) If(xn,1 = 1)

i. Draw wn|xn,1 = 1 ∼ Multi(Ωvd)
(c) If(xn,1 = 0)

i. Draw zn|θd ∼ Mult(θd).
ii. Draw xn,2|vd, zn ∼ Bernoulli(λzn)

iii. If(xn,2 = 1)
A. Draw wn|zn, β ∼ Multi(βzn).

iv. If(xn,2 = 0)
A. Draw wn|vd, zn ∼ Multi(φvd,zn).

In step 1, we draw a document-specific biased
coin,ξd. The bias of this coin determines the pro-
portions of words in the document that are gener-
ated from its ideology background topic Ωvd . As in
LDA, we draw the document-specific topic propor-
tion θd from a Dirichlet prior. θd thus controls the
lexical variabilities due to topical content inside the
document.

To generate a word wn, we first generate a coin
flip xn,1 from the coin ξd. If it turns head, then
we proceed to generate this word from the ideology-
specific topic associated with the document’s ideo-
logical view vd. In this case, the word is drawn in-
dependently of the topical content of the document,
and thus accounts for the lexical variation due to the
ideology associated with the document. The propor-
tion of such words is document-specific by design

1142



and depends on the writer’s style to a large degree.
If xn,1 turns to be tail,we proceed to the next step
and draw a topic-indicator zn. Now, we have two
choices: either to generate this word directly from
the ideology-independent portion of the topic βzn ,
or to draw the word from the ideology-specific por-
tion φvd,zn . The choice here is not document spe-
cific, but rather depends on the interaction between
the ideology and the specific topic in question. If
the ideology associated with the document holds a
strong opinion or view with regard to this topic,
then we expect that most of the time we will take
the second choice, and generate wn from φvd,zn ;
and vice versa. This decision is controlled by the
Bernoulli variable λzn . Therefore, in step c.ii, we
first generate a coin flip xn,2 from λzn . Based on
xn,2 we either generate the word from the ideology-
independent portion of the topic βzn , and this con-
stitutes how the model accounts for lexical variation
due to the topical content of the document, or gen-
erate the word from the ideology-specific portion of
the topic φvd,zn , and this specifies how the model
accounts for lexical variation due to the interaction
between the topical and ideological dimensions of
the document.

Finally, it is worth mentioning that the decision to
model λzn

3 at the topic-ideology level rather than at
the document level, as we have done with ξd, stems
from our goal to capture ideology-specific behavior
on a corpus level rather than capturing document-
specific writing style. However, it is worth mention-
ing that if one truly seeks to measure the degree of
bias associated with a given document,then one can
compute the frequency of the event xn,2 = 0 from
posterior samples. In this case, λzn acts as the prior
bias only. Moreover, computing the frequency of
the event xn,2 = 0 and zn = k gives the document’s
bias toward topic k per se.

Finally, it is worth mentioning that all multino-
mial topics in the model: β,Ω, φ are generated once
for the whole collection from a symmetric Dirichlet
prior, similarly, all bias variables, λ1:K are sampled
from a Beta distribution also once at the beginning
of the generative process.

3In an earlier version of the work we modeled λ on a per-
ideology basis, however, we found that using a single shared λ
results in more robust results

4 Posterior Inference Via Collapsed Gibbs
Sampling

The main tasks can be summarized as follows:

• Learning: Given a collection of documents,
find a point estimate of the model parameters
(i.e. β,Ω, φ, λ,etc.).

• Inference: Given a new document, and a point
estimate of the model parameters, find the pos-
terior distribution of the latent variables associ-
ated with the document at hand:
(θd, {xn,1}, {zn}, {xn,2}).

Under a hierarchical Bayesian setting, like the ap-
proach we took in this paper, both of these tasks can
be handled via posterior inference. Under the gener-
ative process, and hyperparmaters choices, outlined
in section 3, we seek to compute:

P (d1:D, β1:K ,Ω1:V , φ1:V,1:K, λ1:K |α, a, b,w,v),

where d is a shorthand for the hidden variables
(θd, ξd, z,x1,x2) in document d. The above poste-
rior probability is unfortunately intractable,and we
approximate it via a collapsed Gibbs sampling pro-
cedure (Griffiths and Steyvers, 2004; Gelman et al.,
2003) by integrating out, i.e. collapsing, the fol-
lowing hidden variables: the topic-mixing vectors
θd and the ideology bias ξd for each document, as
well as all the multinomial topic distributions: (β,Ω
and φ) in addition to the ideology-topic biases given
by the set of λ random variables.

Therefore, the state of the sampler at each itera-
tion contains only the following topic indicators and
coin flips for each document:(z,x1,x2). We alter-
nate sampling each of these variables conditioned on
its Markov blanket until convergence. At conver-
gence, we can calculate expected values for all the
parameters that were integrated out, especially for
the topic distributions, for each document’s latent
representation (mixing-vector) and for all coin bi-
ases. To ease the calculation of the Gibbs sampling
update equations we keep a set of sufficient statistics
(SS) in the form of co-occurrence counts and sum
matrices of the form CEQeq to denote the number of
times instance e appeared with instance q. For ex-
ample, CWKwk gives the number of times word w was
sampled from the ideology-independent portion of

1143



topic k. Moreover, we follow the standard practice
of using the subscript −i to denote the same quan-
tity it is added to without the contribution of item
i. For example,CWKwk,−i is the same as C

WK
wk with-

out the contribution of word wi. For simplicity, we
might drop dependencies on the document whenever
the meaning is implicit form the context.

For word wn in document d, instead of sampling
zn, xn,1, xn,2 independently, we sample them as a
block as follows:

P (xn,1 = 1|wn = w, vd = v) ∝

(CDX1d1,−n + a1)×
CVWvw,−n + α1∑
w′(C

VW
vw′,−n + α1)

P (xn,1 = 0, x2,n = 1, zn = k|wn = w, vd = v)

∝ (CDX1d0,−n + b1)×
CKX2k1,−n + a2

CKX2k1,−n + C
KX2
k0,−n + a2 + b2

×
CKWkw,−n + α1∑
w′(C

KW
kw′,−n + α1)

×
CDKdk,−n + α2∑
k′(C

DK
dk′,−n + α2)

P (xn,1 = 0, x2,n = 0, zn = k|wn = w, vd = v)

∝ (CDX1d0,−n + b1)×
CKX2k0,−n + b2

CKX2k1,−n + C
KX2
k0,−n + a2 + b2

×
CV KWvkw,−n + α1∑
w′(C

V KW
vkw′,−n + α1)

×
CDKdk,−n + α2∑
k′(C

DK
dk′,−n + α2)

The above three equations can be normalized to
form a 2 ∗ K + 1 multinomial distribution: one
component for generating a word from the ideol-
ogy topic, K components for generating the word
from the ideology-independent portion of topic k =
1, · · · ,K, and finally K components for generat-
ing the word from the ideology-specific portion of
topic k = 1, · · · ,K. Each of these 2 ∗ K + 1
cases corresponds to a unique assignment of the
variables zn, xn,1, xn,2. Therefore, our Gibbs sam-
pler just repeatedly draws sample from this 2∗K+1-
components multinomial distribution until conver-
gence. Upon convergence, we compute point es-
timates for all the collapsed variables by a simple
marginalization of the appropriate count matrices.
During inference, we hold the corpus-level count
matrices fixed, and keep sampling from the above

2∗K+1-component multinomial while only chang-
ing the document-level count matrices: CDK , CDX1
until convergence. Upon convergence, we compute
estimates for ξd and θd by normalizing CDK and
CDX1 (or possibly averaging this quantity across
posterior samples). As we mentioned in Section 3,
to compute the ideology-bias in addressing a given
topic say k in a given document, say d, we can sim-
ply compute the expected value of the event xn,2 =
0 and zn = k across posterior samples.

5 Data Sets

We evaluated our model over three datasets: the bit-
terlemons croups and a two political blog-data set.
Below we give details of each dataset.

5.1 The Bitterlemons dataset
The bitterlemons corpus consists of
the articles published on the website
http://bitterlemons.org/. The website
is set up to contribute to mutual understanding
between Palestinians and Israelis through the
open exchange of ideas. Every week, an issue
about the Israeli-Palestinian conflict is selected for
discussion, and a Palestinian editor and an Israeli
editor contribute one article each addressing the
issue. In addition, the Israeli and Palestinian editors
invite one Israeli and one Palestinian to express
their views on the issue. The data was collected
and pre-proceed as describes in (Lin et al., 2008).
Overall, the dataset contains 297 documents written
from the Israeli’s point of view, and 297 documents
written from the Palestinian’s point of view. On
average each document contains around 740 words.
After trimming words appearing less than 5 times,
we ended up with a vocabulary size of 4100 words.
We split the dataset randomly and used 80% of the
documents for training and the rest for testing.

5.2 The Political Blog Datasets
The first dataset refereed to as Blog-1 is a subset
of the data collected and processed in (Yano et al.,
2009). The authors in (Yano et al., 2009) collected
blog posts from blog sites focusing on American
politics during the period November 2007 to Oc-
tober 2008. We selected three blog sites from this
dataset: the Right Wing News (right-ideology) ;
the Carpetbagger, and Daily Kos as representatives

1144



palestinian
israeli
peace
year 

political 
process 

state 
end 
right 

government 
need 

conflict
way

security

palestinian
israeli
Peace

political 
occupation 

process
end 

security 
conflict 

way 
government 

people
time year

force 
negotiation

bush US president american
sharon administration prime 
settlement pressure policy 

washington ariel new middle

unit state american george
powell minister colin visit 
internal policy statement 

express pro previous package 
work transfer european
administration receive

arafat state leader roadmap 
george election month iraq
week peace june realistic 
yasir senior involvement 
clinton november post 

mandate terrorism

US  roleIsraeli View

roadmap phase violence 
security ceasefire state plan 

international step implement 
authority final quartet issue 

map effort

roadmap end settlement 
implementation obligation 

stop expansion commitment 
consolidate fulfill unit illegal 

present previou assassination 
meet forward negative calm

process force terrorism unit 
road demand provide 

confidence element interim 
discussion want union succee
point build positive recognize 

present timetable

Roadmap process

israel syria syrian negotiate 
lebanon deal conference 

concession asad agreement 
regional october initiative 

relationship

track negotiation official 
leadership position 

withdrawal time victory 
present second stand 

circumstance represent sense 
talk strategy issue participant 

parti negotiator

peace strategic plo hizballah
islamic neighbor territorial 
radical iran relation think 
obviou countri mandate 

greater conventional intifada 
affect jihad time

Arab Involvement

Palestinian  View

Israeli 
Background

topic

Palestinian
Background

topic

Figure 2: Illustrating the big picture overview over the bitterlemons dataset using few topics. Each box lists the top
words in the corresponding multinomial topic distribution. See text for more details

of the liberal view (left-ideology). After trimming
short posts of less than 20 words, we ended up with
2040 posts distributed as 1400 from the left-wing
and the rest from the right-wing. On average, each
post contains around 100 words and the total size of
the vocabulary is 14276 words. For this dataset, we
followed the train-test split in (Yano et al., 2009).
In this split each blog is represented in both train-
ing and test sets. Thus this dataset does not measure
the model’s ability to generalize to a totally different
writing style.

The second dataset refereed to as Blog-2 is sim-
ilar to Blog-1 in its topical content and time frame
but larger in its blog coverage (Eisenstein and Xing,
2010). Blog-2 spans 6 blogs: three from the left-
wing and three from the right-wing. The dataset
contains 13246 posts. After removing words that
appear less then 20 times, the total vocabulary be-
comes 13236 with an average of 200 words per post.
We used 4 blogs (2 from each view) for training
and held two blogs (one from each view) for test-
ing. Thus this dataset measures the model’s ability

to generalize to a totally new blog.

6 Experimental Results
In this section we gave various qualitative and quan-
titative evaluations of our model over the datasets
listed in Section 5. For all experiments, we set
α1 = .01, α2 = .1, a = 1 and b = 1. We run Gibbs
sampling during training for 1000 iterations. During
inference, we ran Gibbs sampling for 300 iterations,
and took 10 samples, with 50-iterations lag, for eval-
uations.

6.1 Visualization and Browsing
One advantage of our approach is its ability to create
a “big-picture” overview of the interaction between
ideology and topics. In figure 2 we show a portion of
that diagram over the bitterlemons dataset. First note
how the ideology-specific topics in both ideology
share the top-three words, which highlights that the
two ideologies seek peace even though they still both
disagree on other issues. The figure gives example
of three topics: the US role, the Roadmap peace
process, and the Arab involvement in the conflict

1145



(the name of these topics were hand-crafted). For
each topic, we display the top words in the ideology-
independent part of the topic (β), along with top
words in each ideology’s view of the topic (φ).

For example, when discussing the roadmap pro-
cess, the Palestinian view brings the following is-
sues: [the Israeli side should] implement the oblig-
atory points in this agreement, stop expansion of
settlements, and move forward to the commitments
brought by this process. On the other hand, the Is-
raeli side brings the following points: [Israelis] need
to build confidence [with Palestinian], address the
role of terrorism on the implementation of the pro-
cess, and ask for a positive recognition of Israel
from the different Palestinian political parties. As
we can see, the ideology-specific portion of the topic
needn’t always represent a sentiment shared by its
members toward a given topic, but it might rather
includes extra important dimensions that need to be
taken into consideration when addressing this topic.

Another interesting topic addresses the involve-
ment of the neighboring Arab countries in the con-
flict. From the Israeli point of view, Israel is worried
about the existence of hizballah [in lebanon] and its
relationship with radical Iran, and how this might
affect the Palestinian-uprising (Intifada) and Jihad.
From the other side, the Palestinians think that the
Arab neighbors need to be involved in the peace pro-
cess and negotiations as some of these countries like
Syria and Lebanon are involved in the conflict.

The user can use the above chart as an entry point
to retrieve various documents pertinent to a given
topic or to a given view over a specific topic. For
instance, if the user asks for a representative sam-
ple of the Israeli(Palestinian) view with regard to the
roadmap process, the system can first retrieve docu-
ments tagged with the Israeli(Palestinian) view and
having a high topical value in their latent representa-
tion θ over this topic. Finally, the system then sorts
these documents by how much bias they show over
this topic. As we discussed in Section 4, this can be
done by computing the expected value of the event
xn,2 = 0 and zn = k where k is the topic under
consideration.

6.2 Classification
We have also performed a classification task over
all the datasets. The Scenario proceeded as follows.

We train a model over the training data with various
number of topics. Then given a test document, we
predict its ideology using the following equation:

vd = argmaxv∈V P (wd|v); (1)

We use three baselines. The first baseline
is an SVM classifier trained on the normalized
word frequency of each document. We trained
SVM using a regularization parameter in the range
{1, 10, 20, · · · , 100} and report the best result (i.e.
no cross-validation was performed). The other
two are supervised LDA models: supervised LDA
(sLDA) (Wang et. al., 2009; Blei and McCauliffe,
2007) and discLDA (Lacoste-Julien et al., 2008).
discLDA is a conditional model that divides the
available number of topics into class-specific top-
ics and shared-topics. Since the code is not publicly
available, we followed the same strategy in the orig-
inal paper and share 0.1K topics across ideologies
and then divide the rest of the topics between ide-
ologies4. However, unlike our model, there are no
internal relationships between these two sets of top-
ics. The decision rule employed by discLDA is very
similar to the one we used for mview-LDA in Eq
(1). For sLDA, we used the publicly available code
by the authors.

As shown in Figure 3, our model performs better
than the baselines over the three datasets. We should
note from this figure that mview-LDA peaks at a
small number of topics, however, each topic is repre-
sented by three multinomials. Moreover, it is evident
from the figure that the experiment over the blog-
2 dataset which measures each model’s ability to
generalizes to a totally unseen new blog is a harder
task than generalizing to unseen posts form the same
blog. However, our model still performs competi-
tively with the SVM baseline. We believe that sep-
arating each topic into an ideology-independent part
and ideology-specific part is the key behind this per-
formance, as it is expected that the new blogs would
still share much of the ideology-independent parts
of the topics and hopefully would use similar (but

4(Lacoste-Julien et al., 2008) gave an optimization algo-
rithm for learning the topic structure (transformation matrix),
however since the code is not available, we resorted to one of
the fixed splitting strategies mentioned in the paper. We tried
other splits but this one gives the best results

1146



(a) (b) (c)

Figure 3: Classification accuracy over the Bitterlemons dataset in (a) and over the two blog datasets in (b) and (c). For SVM we
give the best result obtained across a wide range of the SVM’s regularization parameter(not the cross-validation result).

no necessarily all) words from the ideology-specific
parts of each topic when addressing this topic.

Finally, it should be noted that the bitterlemons
dataset is a multi-author dataset and thus the models
were tested on some authors that were not seen dur-
ing training, however, two factors contributed to the
good performance by all models over this dataset.
The first being the larger size of each document (740
words per document as compared to 200 words per
post in blog-2) and the second being the more formal
writing style in the bitterlemons dataset.

6.3 An Ablation Study
To understand the contribution of each component of
our model, we conducted an ablation study over the
bitterlemons dataset. In this experiment we turned-
off one feature of our model at a time and mea-
sured the classification performance. The results are
shown in Figure 4. Full, refers to the full model; No-
Ω refers to a model in which the ideology-specific
background topic Ω is turned-off; and No-φ refers
to a model in which the ideology-specific portions of
the topics are turned-off. As evident from the figure,
φ is more important to the model than Ω and the dif-
ference in performance between the full model and
the No-φ model is rather significant. In fact without
φ the model has little power to discriminate between
ideologies beyond the ideology-specific background
topic Ω.

6.4 Retrieval: Getting the Other Views
To evaluate the ability of our model in finding al-
ternative views toward a given topic, we conducted
the following experiment over the Bitterlemons cor-
pus. In this corpus each document is associated with
a meta-topic that highlights the issues addressed in
this document like: “A possible Jordanian role”,

Figure 4: An Ablation study over the bitterlemons dataset.

“Demography and the conflict”,etc. There are a to-
tal of 148 meta-topics. These topics were not used
in fitting our model but we use them in the evalu-
ation as follows. We divided the dataset into 60%
for training and 40% for testing. We trained mview-
LDA over the training set, and then used the learned
model to infer the latent representation of the test
documents as well as their ideologies. We then used
each document in the training set as a query to re-
trieve documents from the test set that address the
same meta-topic in the query document but from the
other-side’s perspective. Note that we have access to
the view of the query document but not the view of
the test document. Moreover, the value of the meta-
topic is only used to construct the ground-truth result
of each query over the test set. In addition to mview-
LDA, we also implemented a strong baseline using
SVM+Dirichlet smoothing that we will refer to as
LM. In this baseline, we build an SVM classifier
over the training set, and use Dirichlet-smoothing
to represent each document (in test and training set)
as a multinomial-distribution over the vocabulary.
Given a query document d, we rank documents in

1147



Figure 5: Evaluating the performance of the view-Retrieval task. Figure compare performance between mview-LD vs. an SVM+a
smoothed language model approach using three measures: average rank, best rank and rank at full recall. ( Lower better)

.

the test set by each model as follows:

• mview-LDA: we computed the cosine-distance
between θmv−LDA−sharedd and θ

mv−LDA−shared
d′

weighted by the probability that d′ is written
from a different view than vd. The latter
quantity can be computed by normaliz-
ing P (v|d′). Moreover, θmv−LDA−sharedd,k ∝∑

n I
[
(xn,1 = 0) and (xn,2 = 1) and (zn = k)

]
,

and n ranges over words in document d. In-
tuitively, we would like θmv−LDA−sharedd to
reflect variation due to the topical content, but
not ideological view of the document.

• LM: For a document d′, we apply the SVM
classifier to get P (v|d′), then we measure sim-
ilarity by computing the cosine-distance be-
tween the smoothed multinomial-distribution
of d and d′. We combine these two components
as in mview-LDA.

Finally we rank documents in the test set in a
descending-order and evaluate the resulting rank-
ing using three measures: the rank at full recall
(lowest rank), average rank, and best rank of the
ground-truth documents as they appear in the pre-
dicted ranking. Figure 5 shows the results across a
number of topics. From this figure, it is clear that
our model outperforms this baseline over all mea-
sures. It should be noted that this is a very hard
task since the meta-topics are very fine-grained like:
Settlements revisited, The status of the settlements,
Is the roadmap still relevant?,The ceasefire and the
roadmap: a progress report,etc. We did not attempt
to cluster these meta-topics since our goal is just to
compare our model against the baseline.

7 A Semi-Supervised Extension

In this section we present and assess the efficacy of
a semi-supervised extension of mview-LDA. In this
setting, the model is given a set of ideologically-
labeled documents and a set of unlabeled docu-
ments. One of the key advantages of using a prob-
abilistic graphical model is the ability to deal with
hidden variables in a principled way. Thus the only
change needed in this case is adding a single step in
the sampling algorithm to sample the ideology v of
an unlabeled document as follows:

P (vd = v|rest) ∝ P (wd|vd = v, zd,x1,d,x2,d)

Note that the probability of the indicators
(x1,d,x2,d, zd) do not depend on the view of the
document and thus got absorbed in the normaliza-
tion constant, and thus one only needs to measures
the likelihood of generating the words in the doc-
ument under the view v. We divide the words
into three groups: Ad = {wn|xn,1 = 1} is the
set of words generated from the view-background
topic, Bd,k = {wn|zn = k, xn,1 = 0, xn,2 =
1} is the set of words generated from βk, and
Cd,k = {wn|zn = k, xn,1 = 0, xn,2 = 0} is the
set of words generated from φk,v. The probabil-
ity of Bd,k does not depend on the value of v and
thus can be absorbed into the normalization factor.
Therefore, we only need to compute the following
probability:P (Ad, Cd,1:K |vd = v, rest)=∏

k

∫
φk,v

P (Cd,k|φk,v, rest)p(φk,v|rest)dφk,v

×
∫

Ω
P (Ad|Ω, rest)p(Ω|rest)dΩ (2)

1148



All the integrals in (2) reduce to the ratio of two
log partition functions. For example, the product of
integrals containing Cd,k reduce to:

∏
k

∏
w Γ
(
CDKW,X2=0dkw + C

V KW
vkw,−d + α1

)
Γ
(∑

w

[
CDKW,X2=0dkw + C

V KW
vkw,−d + α1

])
×

Γ
(∑

w

[
CV KWvkw + α1

])
∏
w Γ
(
CV KWvkw,−d + α1

) (3)
Unfortunately, the above scheme does not mix

well because the value of the integrals in (2) are
very low for any view other than the view of the
document in the current state of the sampler. This
happens because of the tight coupling between vd
and the indicators (x1,x2, z). To remedy this prob-
lem we used a Metropolis-Hasting step to sample
(vd,x1,x2, z) jointly. We construct a set of V pro-
posals each of which is indexed by a possible view:
qv(x1,x2, z) = P (x1,x2, z|vd = v,wd). Since
we have a collection of proposal distributions, we
select one of them at random at each step. To
generate a sample from qv∗(), we run a few it-
erations of a restricted Gibbs scan over the docu-
ment d conditioned on fixing vd = v∗ and then
take the last sample jointly with v∗ as our pro-
posed new state. With probability min(r,1), the new
state (v∗,x1∗,x2∗, z∗) is accepted, otherwise the
old state is retained. The acceptance ratio,r, is com-
puted as: r = p(wd|v∗,x1∗,x2∗,z∗)p(wd|v,x1,x2,z) , where the non-*
variables represent the current state of the sampler.
It is interesting to note that the above acceptance ra-
tio is equivalent to a likelihood ratio test. We com-
pute the marginal probability P (wd|..) using the
estimated-theta method (Wallach et al., 2009).

We evaluated the semi-supervised extension using
the blog-2 dataset as follows. We reveal R% of the
labels in the training set; then we train mview-LDA
only over the labeled portion and train the semi-
supervised version (ss-mview-LDA) on both the la-
beled and unlabeled documents. Finally we evaluate
the classification performance on the test set. We
used R = {20, 40, 80}. The results are given in Ta-
ble 1 which shows a decent improvement over the
supervised mview-LDA.

R mview-LDA ss-mview-LDA
80 65.60 66.41
60 62.31 65.43
20 60.87 63.25

Table 1: Classification performance of the semi-
supervised model. R is the ratio of labeled documents.

8 Discussion and Future Work

In this paper, we addressed the problem of model-
ing ideological perspective at a topical level. We
developed a factored topic model that we called
multiView-LDA or mview-LDA for short. mview-
LDA factors a document collection into three set
of topics: ideology-specific, topic-specific, and
ideology-topic ones. We showed that the resulting
representation can be used to give a bird-eyes’ view
to where each ideology stands with regard to main-
stream topics. Moreover, we illustrated how the la-
tent structure induced by the model can by used to
perform bias-detection at the document and topic
level, and retrieve documents that represent alterna-
tive views.

It is important to mention that our model induces
a hierarchical structure over the topics, and thus it
is interesting to contrast it with hierarchical topic
models like hLDA (Blei et al., 2003) and PAM (Li
and McCallum, 2006; Mimno et al., 2007). First,
these models are unsupervised in nature, while our
model is supervised. Second, the semantic of the
hierarchical structure in our model is different than
the one induced by those models since documents in
our model are constrained to use a specific portion
of the topic structure while in those models docu-
ments can freely sample words from any topic. Fi-
nally,in the future we plan to extend our model to
perform joint modeling and summarization of ide-
alogical discourse.

9 Acknowledgment
We thank Jacob Eisenstein, John Lafferty, Tom
Mitchell, and the anonymous reviewers for their
helpful comments and suggestions. This work is
supported in part by grants NSF IIS- 0713379,
NSF DBI-0546594 career award to EPX, ONR
N000140910758, DARPA NBCH1080007, and
AFOSR FA9550010247. EPX is supported by an
Alfred P. Sloan Research Fellowship.

1149



References

J. Wiebe, T. Wilson, R.Bruce, M. Bell, and M. Martin.
Learning subjective language. Computational Linguis-
tics, 30(3), 2004.

H. Yu and V. Hatzivassiloglou. Towards answering opin-
ion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Pro-
ceedings of EMNLP-2003

B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP-2002.

P. Turney and M. Littman. Measuring praise and criti-
cism: Inference of semantic orientation from associa-
tion. ACM TOIS, 21(4):315346, 2003

A. Popescu and O. Etzioni. Extracting product fea-
tures and opinions from reviews. In Proceedings of
HLT/EMNLP-2005, pages 339346, 2005.

T. Nasukawa and J. Yi. Sentiment analysis: Capturing
favorability using natural language processing. In Pro-
ceedings of K-CAP, 2003.

M. Hu and B. Liu. Mining and summarizing customer
reviews. In Proceedings of KDD, 2004.

B. Pang and L. Lee. Opinion mining and sentiment anal-
ysis. Foundations and Trends in Information Retrieval,
2(12), 1135, 2008.

S. Branavan, H. Chen, J. Eisenstein and R. Barzilay.
Learning Document-Level Semantic Properties from
Free-text Annotations, Proceedings of ACL, 2008.

I. Titov and R. McDonald. Modeling Online Reviews
with Multi-Grain Topic Models International World
Wide Web Conference (WWW), 2008.

I. Titov and R. McDonald. A Joint Model of Text and
Aspect Ratings for Sentiment Summarization Associ-
ation for Computational Linguistics (ACL), 2008.

Q. Mei, X. Ling, M. Wondra, H. Su, ChengXiang Zhai.
Topic Sentiment Mixture: Modeling Facets and Opin-
ions in Weblogs, Proceedings of the 16th International
World Wide Web Conference (WWW), pages 171-
180, 2007.

X. Ling, Q. Mei, C. Zhai, B. Schatz. Mining Multi-
Faceted Overviews of Arbitrary Topics in a Text Col-
lection, Proceedings of the 15th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD’ 08), pages 497-505, 2008

B. Fortuna , C. Galleguillos, N. Cristianini. Detecting
the bias in media with statistical learning methods.
In: Text Mining: Theory and Applications. Taylor and
Francis Publisher,2008.

W. Lin, E.P. Xing, and A. Hauptmann. A Joint Topic and
Perspective Model for Ideological Discourse Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), 2008.

T. A. Van Dijk. Ideology: A multidisciplinary approach.
Sage Publications, 1998.

T. Griffiths, M. Steyvers Finding scientific topics.PNAS,
101:5228-5235, 2004.

A. Gelman, J. Carlin, Hal Stern, and Donald Ru-
bin. Bayesian Data Analysis, Chapman-Hall, 2 edi-
tion,2003.

D. Blei, A. Ng, and M. Jordan. Latent Dirichlet al-
location. Journal of Machine Learning Research,
3:9931022, January 2003.

T. Yano, W. W. Cohen, and N. A. Smith. Predicting
Response to Political Blog Posts with Topic Models.
NAACL-HLT 2009, Boulder, CO, MayJune 2009

J. Eisenstein and E.P. Xing.The CMU-2008 Political
Blog Corpus. CMU-ML-10-101 Technical Report,
2010.

C. Wang, D. Blei and L. Fei-Fei. Simultaneous image
classification and annotation. CVPR, 2010.

D. Blei and J. McAuliffe. Supervised topic models. NIPS,
2007.

S. Lacoste-Julien, F. Sha, and M. Jordan. DiscLDA:
Discriminative Learning for Dimensionality Reduc-
tion and Classification. Neural Information Process-
ing Systems Conference (NIPS08), Vancouver, British
Columbia, December 2008.

E. Riloff, J. Wiebe, and T. Wilson. Learning subjective
nouns using extraction pattern bootstrapping. In Pro-
ceedings of CoNLL-2003.

D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. Hier-
archical topic models and the nested Chinese restau-
rant process. In Neural Information Processing Sys-
tems (NIPS)16, 2003.

D. Mimno, W. Li and A. McCallum. Mixtures of Hier-
archical Topics with Pachinko Allocation. In Interna-
tional Conference of Machine Learning, ICML, 2007.

W. Li, and A. McCallum. Pachinko Allocation: DAG-
structured Mixture Models of Topic Correlations. In
International Conference of Machine Learning, ICML,
2006.

M. Paul and R. Girju. Cross-cultural Analysis of Blogs
and Forums with Mixed-Collection Topic Models.
EMNLP 2009.

H. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno.
Evaluation Methods for Topic Models. ICML 2009.

1150


