



















































Even Unassociated Features Can Improve Lexical Distributional Similarity


Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 32–39,
Beijing, August 2010

Even Unassociated Features Can Improve
Lexical Distributional Similarity

Kazuhide Yamamoto and Takeshi Asakura
Department of Electrical Engineering

Nagaoka University of Technology
{yamamoto, asakura}@jnlp.org

Abstract

This paper presents a new computation
of lexical distributional similarity, which
is a corpus-based method for computing
similarity of any two words. Although
the conventional method focuses on em-
phasizing features with which a given
word is associated, we propose that even
unassociated features of two input words
can further improve the performance in
total. We also report in addition that
more than 90% of the features has no
contribution and thus could be reduced
in future.

1 Introduction

Similarity calculation is one of essential tasks in
natural language processing (1990; 1992; 1994;
1997; 1998; 1999; 2005). We look for a seman-
tically similar word to do corpus-driven summa-
rization, machine translation, language genera-
tion, recognition of textual entailment and other
tasks. In task of language modeling and disam-
biguation we also need to semantically general-
ize words or cluster words into some groups. As
the amount of text increases more and more in
the contemporary world, the importance of sim-
ilarity calculation also increases concurrently.

Similarity is computed by roughly two ap-
proaches: based on thesaurus and based on cor-
pus. The former idea uses thesaurus, such as
WordNet, that is a knowledge resource of hi-
erarchical word classification. The latter idea,
that is the target of our work, originates from
Harris’s distributional hypothesis more than four

decades ago (1968), stating that semantically
similar words tend to appear in similar contexts.
In many cases a context of a word is represented
as a feature vector, where each feature is another
expression that co-occurs with the given word in
the context.

Over a long period of its history, in partic-
ular in recent years, several works have been
done on distributional similarity calculation. Al-
though the conventional works have attained the
fine performance, we attempt to further improve
the quality of this measure. Our motivation of
this work simply comes from our observation
and analysis of the output by conventional meth-
ods; Japanese, our target language here, is writ-
ten in a mixture of four scripts: Chinese char-
acters, Latin alphabet, and two Japanese-origin
characters. In this writing environment some
words which have same meaning and same pro-
nunciation are written in two (or more) different
scripts. This is interesting in terms of similarity
calculation since these two words are completely
same in semantics so the similarity should be
ideally 1.0. However, the reality is, as far as we
have explored, that the score is far from 1.0 in
many same word pairs. This fact implies that the
conventional calculation methods are far enough
to the goal and are expected to improve further.

The basic framework for computing distribu-
tional similarity is same; for each of two input
words a context (i.e., surrounding words) is ex-
tracted from a corpus, a vector is made in which
an element of the vector is a value or a weight,
and two vectors are compared with a formula to
compute similarity. Among these processes we
have focused on features, that are elements of

32



the vector, some of which, we think, adversely
affect the performance. That is, traditional ap-
proaches such as Lin (1998) basically use all of
observed words as context, that causes noise in
feature vector comparison. One may agree that
the number of the characteristic words to deter-
mine the meaning of a word is some, not all, of
words around the target word. Thus our goal is
to detect and reduce such noisy features.

Zhitomirsky-Geffet and Dagan (2009) have
same motivation with us and introduced a boot-
strapping strategy that changes the original fea-
tures weights. The general idea here is to pro-
mote the weights of features that are common
for associated words, since these features are
likely to be most characteristic for determining
the word’s meaning. In this paper, we propose
instead a method to using features that are both
unassociated to the two input words, in addition
to use of features that are associated to the input.

2 Method

The lexical distributional similarity of the input
two words is computed by comparing two vec-
tors that express the context of the word. In this
section we first explain the feature vector, and
how we define initial weight for each feature of
the vector. We then introduce in Subsection 2.3
the way to compute similarity by two vectors.
After that, we emphasize some of the features by
their association to the word, that is explained in
Subsection 2.4. We finally present in Subsection
2.5 feature reduction which is our core contribu-
tion of this work. Although our target language
is Japanese, we use English examples in order to
provide better understanding to the readers.

2.1 Feature Vector

We first explain how to construct our feature vec-
tor from a text corpus.

A word is represented by a feature vector,
where features are collection of syntactically de-
pendent words co-occurred in a given corpus.
Thus, we first collect syntactically dependent
words for each word. This is defined, as in
Lin (1998), as a triple (w, r,w′), where w and
w′ are words and r is a syntactic role. As for

definition of word, we use not only words given
by a morphological analyzer but also compound
words. Nine case particles are used as syntactic
roles, that roughly express subject, object, modi-
fier, and so on, since they are easy to be obtained
from text with no need of semantic analysis. In
order to reduce noise we delete triples that ap-
pears only once in the corpus.

We then construct a feature vector out of col-
lection of the triples. A feature of a word is an
another word syntactically dependent with a cer-
tain role. In other words, given a triple (w, r,w′),
a feature of w corresponds to a dependent word
with a role (r,w′).

2.2 (Initial) Filtering of Features

There are several weighting functions to deter-
mine a value for each feature element. As far
as we have investigated the literature the most
widely used feature weighting function is point-
wise mutual information (MI), that is defined as
follows:

MI(w, r,w′) = log2
freq(w, r,w′)S

freq(w)freq(r,w′)
(1)

where freq(r,w′) is the frequency of the co-
occurrence word w′ with role r, freq(w)
is the independent frequency of a word w,
freq(w, r,w′) is the frequency of the triples
(w, r,w′), and S is the number of all triples.

In this paper we do not discuss what is the
best weighting functions, since this is out of tar-
get. We use mutual information here because it
is most widely used, i.e., in order to compare per-
formance with others we want to adopt the stan-
dard approach.

As other works do, we filter out features that
have a value lower than a minimal weight thresh-
olds α. The thresholds are determined according
to our preliminary experiment, that is explained
later.

2.3 Vector Similarity

Similarity measures of the two vectors are com-
puted by various measures. Shibata and Kuro-
hashi (2009) have compared several similarity
measures including Cosine (Ruge, 1992), (Lin,

33



� �
(input word) w: boy
(feature) v: guardOBJ
(synonyms of w, shown with its similarity to w) Syn(w) =
{ child(0.135), girl(0.271), pupil(0.143), woman(0.142), young people(0.147) }� �

(feature vectors V ):
V(boy) = { parentsMOD, runawaySUBJ, reclaimOBJ, fatherMOD, guardOBJ , · · · }

V(child) = { guardOBJ, lookOBJ, bringOBJ, give birthOBJ, careOBJ , · · · }
V(girl) = { parentsMOD, guardOBJ, fatherMOD, testifySUBJ, lookOBJ, · · · }
V(pupil) = { targetOBJ, guardOBJ, careOBJ, aimOBJ, increaseSUBJ, · · · }
V(woman) = { nameMOD, give birthOBJ, groupMOD, together+with, parentsMOD , · · · }
V(young people) = { harmfulTO, globalMOD, reclaimOBJ, wrongdoingMOD , · · · }� �

(words that has feature v) Asc(v) = {boy, child, girl, pupil, · · ·}

weight(w, v) = weight (boy, guardOBJ) =
∑

wf∈Asc(v)∩Syn(w) sim(w,wf )
= 0.135 + 0.271 + 0.143 = 0.549� �

Figure 1: Example of feature weighting for word boy.

1998), (Lin, 2002), Simpson, Simpson-Jaccard,
and conclude that Simpson-Jaccard index attains
best performance of all. Simpson-Jaccard index
is an arithmetic mean of Simpson index and Jac-
card index, defined in the following equation:

sim(w1, w2) =
1

2
(simJ (w1, w2)+simS(w1, w2))

(2)

simJ(w1, w2) =
|V1 ∩ V2|
|V1 ∪ V2|

(3)

simS(w1, w2) =
|V1 ∩ V2|

min(|V1|, |V2|)
(4)

where V1 and V2 is set of features for w1 and
w2, respectively, and |A| is the number of set A.
It is interesting to note that both Simpson and
Jaccard compute similarity according to degree
of overlaps of the two input sets, that is, the re-
ported best measure computes similarity by ig-
noring the weight of the features. In this paper
we adopt Simpson-Jaccard index, sim, which

indicates that the weight of features that is ex-
plained below is only used for feature reduction,
not for similarity calculation.

2.4 Feature Weighting by Association

We then compute weights of the features of the
word w according to the degree of semantic as-
sociation to w. The weight is biased because all
of the features, i.e., the surrounding words, are
not equally characteristic to the input word. The
core idea for feature weighting is that a feature
v in w is more weighted when more synonyms
(words of high similarity) of w also have v.

Figure 1 illustrates this process by examples.
Now we calculate a feature guardOBJ for a word
boy, we first collect synonyms of w, denoted by
Syn(w), from a thesaurus. We then compute
similarities between w and each word in Syn(w)
by Equation 2. The weight is the sum of the sim-
ilarities of words in Syn(w) that have feature v,
defined in Equation 5.

weight(w, v) =
∑

wf∈Asc(v)∩Syn(w)
sim(w,wf )

(5)

34



Figure 2: An illustration of similarity calculation of Zhitomirsky-Geffet and Dagan (2009) (a) and
the proposed method (b1 and b2) in feature space. In order to measure the distance of the two words
(shown in black dots) they use only associated words, while we additionally use unassociated words
in which the distances to the words are similar.

2.5 Feature Reduction

We finally reduce features according to the dif-
ference of weights of each feature in words we
compare. In computing similarity of two words,
w1 and w2, a feature v satisfying Equation 6 is
reduced.

abs(weight(w1, v) − weight(w2, v)) > β (6)
where abs() is a function of absolute value, and
β is a threshold for feature reduction.

Figure 2 illustrates our idea and compares
the similar approach proposed by Zhitomirsky-
Geffet and Dagan (2009). Roughly speaking,
Zhitomirsky-Geffet and Dagan (2009) compute
similarity of two words, shown as black dots
in (a), mainly according to associated features
(dark-colored circle), or features that has high
weights in Equation 5. And the associated fea-
tures are determined word by word indepen-
dently.

In contrast, the proposed method relatively re-
duces features, depending on location of input
two words. At (b1) in the figure, not only asso-
ciated (high-colored area) but unassociated fea-
tures (light-colored area) are used for similar-
ity computation in our method. As Equation 6

shows, regardless of how much a feature is as-
sociated to the word, the feature is not reduced
when it has similar weight to both w1 and w2,
located at the middle area of the two words in
the figure.

This idea seems to work more effectively,
compared with Zhitomirsky-Geffet and Da-
gan (2009), in case that input two words are not
so similar, that is shown at (b2) of the figure.
As they define associated features independently,
it is likely that the overlapped area is little or
none between the two words. In contrast, our
method uses features at the middle area of two
input words, where there is always certain fea-
tures provided for similarity computation, shown
in case (b2). Simplified explanation is that our
similarity is computed as the ratio of the associ-
ated area to the unassociated area in the figure.
We will verify later if the method works better in
low similarity calculation.

2.6 Final Similarity

The final similarity of two words are calculated
by two shrunk vectors (or feature sets) and Equa-
tion 2, that gives a value between 0 and 1.

35



3 Evaluation

3.1 Evaluation Method

In general it is difficult to answer how similar
two given words are. Human have no way to
judge correctness if computed similarity of two
words is, for instance, 0.7. However, given two
word pairs, such as (w,w1) and (w,w2), we may
answer which of two words, w1 or w2, is more
similar to w than the other one. That is, degree
of similarity is defined relatively hence accuracy
of similarity measures is evaluated by way of rel-
ative comparisons.

In this paper we employ an automatic eval-
uation method in order to reduce time, human
labor, and individual variations. We first col-
lect four levels of similar word pairs from a the-
saurus1. Thesaurus is a resource of hierarchi-
cal words classification, hence we can collect
several levels of similar word pairs according
to the depth of common parent nodes that two
words have. Accordingly, we constructed four
levels of similarity pairs, Level 0, 1, 2, and 3,
where the number increases as the similarity in-
creases. Each level includes 800 word pairs that
are randomly selected. The following examples
are pairs with word Asia in each Level.� �

Example: Four similarity levels for pair of
Asia.

Level 3(high): Asia vs. Europe
Level 2: Asia vs. Brazil
Level 1: Asia vs. my country
Level 0(low): Asia vs. system� �

We then combine word pairs of adjacent sim-
ilarity Levels, such as Level 0 and 1, that is a
test set to see low-level similarity discrimination
power. The performance is calculated in terms
of how clearly the measure distinguishes the dif-
ferent levels. In a similar fashion, Level 1 and 2,
as well as 2 and 3, are combined and tested for
middle-level and high-level similarity discrimi-
nation, respectively. The number of pairs in each

1In this experiment we use Bunrui Goi Hyo also for
evaluation. Therefore, this experimental setting is a kind
of closed test. However, we see that the advantage to use
the same thesaurus in the evaluation seems to be small.

Figure 3: Relation between threshold α and per-
formance in F-measures for Level 3+2 test set.

test set is 1,600 as two Levels are combined.

3.2 Experimental Setting

The corpus we use in this experiment is all the
articles in The Nihon Keizai Shimbun Database,
a Japanese business newspaper corpus cover-
ing the years 1990 through 2004. As morpho-
logical analyzer we use Chasen 2.3.3 with IPA
morpheme dictionary. The number of collected
triples is 2,584,905, that excludes deleted ones
due to one time appearance and words including
some symbols.

In Subsection 2.4 we use Bunrui Goi Hyo, a
Japanese thesaurus for synonym collection. The
potential target words are all content words, ex-
cept words that have less than twenty features.
The number of words after exclusion is 75,530.
Moreover, words that have four or less words in
the same category in the thesaurus are regarded
as out of target in this paper, due to limitation
of Syn(w) in Subsection 2.4. Also, in order
to avoid word sense ambiguity, words that have
more than two meanings, i.e., those classified in
more than two categories in the thesaurus, also
remain to be solved.

3.3 Threshold for Initial Filtering

Figure 3 shows relation between threshold α and
the performance of similarity distinction that is
drawn in F-measures, for Level 3+2 test set. As
can be seen, the plots seem to be concave down

36



Figure 4: Threshold vs. accuracy in Level 3+2
set.

Figure 5: Threshold vs. accuracy in Level 2+1
set.

and there is a clear peak when α is between 2
and 3.

In the following experiments we set α the
value where the best performance is given for
each test set. We have observed similar phenom-
ena in other test sets. The thresholds we use is
2.1 for Level 3+2, 2.4 for Level 2+1, and 2.4 for
Level 1+0.

3.4 Threshold for Weighting Function

Figure 4, 5, and 6 show relation between thresh-
old β and performance in Level 3+2, 2+1, 1+0
test set, respectively. The threshold at the point
where highest performance is obtained greatly
depends on Levels: 0.3 in Level 3+2, 0.5 in Level
2+1, and 0.9 in Level 1+0. Comparison of these
three figures indicates that similarity distinction

Figure 6: Threshold vs. accuracy in Level 1+0
set.

Table 1: Performance comparison of three meth-
ods in each task (in F-measures).

Level S&K ZG&D proposed
Lvl.3+Lvl.2 0.702 0.791 0.797
Lvl.2+Lvl.1 0.747 0.771 0.773
Lvl.1+Lvl.0 0.838 0.789 0.840

power in higher similarity region requires lower
threshold, i.e., fewer features. In contrast, con-
ducting fine distinction in lower similarity level
requires higher threshold, i.e., a lot of features
most of which may be unassociated ones.

3.5 Performance

Table 1 shows performance of the pro-
posed method, compared with Shibata
and Kurohashi (2009) (S&K in the table)
and Zhitomirsky-Geffet and Dagan (2009)
(ZG&D)2. The method of Shibata and Kuro-
hashi (2009) here is the best one among those
compared. It uses only initial filtering described
in Subsection 2.2. The method of Zhitomirsky-
Geffet and Dagan (2009) in addition emphasize
associated features as explained in Subsection
2.4. All of the results in the table are the best
ones among several threshold settings.

The result shows that the accuracy is 0.797
(+0.006) in Level 3+2, 0.773 (+0.002) in Level

2The implementations of providing associated words
and the bootstrapping are slightly different to Zhitomirsky-
Geffet and Dagan (2009).

37



2+1, and 0.840 (+0.001) in Level 1+0, where the
degree of improvement here are those compared
with best ones except our proposed method. This
confirms that our method attains equivalent or
better performance in all of low, middle, and
high similarity levels.

We also see in the table that S&K and ZG&D
show different behavior according to the Level.
However, it is important to note here that our
proposed method performs equivalent or outper-
forms both methods in all Levels.

4 Discussions

4.1 Behavior at Each Similarity Level

As we have discussed in Subsection 2.5, our
method is expected to perform better than
Zhitomirsky-Geffet and Dagan (2009) in distinc-
tion in lower similarity area. Roughly speak-
ing, we interpret the results as follows. Shi-
bata and Kurohashi (2009) always has many fea-
tures that degrades the performance in higher
similarity level, since the ratio of noisy fea-
tures may throw into confusion. Zhitomirsky-
Geffet and Dagan (2009) reduces such noise
that gives better performance in higher similarity
level and is stable in all levels. And our proposed
method maintains performance of Zhitomirsky-
Geffet and Dagan (2009) in higher level while
improves performance that is close to Shibata
and Kurohashi (2009) in lower level, utilizing
fewer features. We think our method can include
advantages over the two methods.

4.2 Error Analysis

We overview the result and see that the major er-
rors are NOT due to lack of features. Table 2
illustrates the statistics of words with a few fea-
tures (less than 50 or 20). This table clearly tells
us that, in the low similarity level (Level 1+0) in
particular, there are few pairs in which the word
has less than 50 or 20, that is, these pairs are con-
sidered that the features are erroneously reduced.

4.3 Estimation of Potential Feature
Reduction

It is interesting to note that we may reduce 81%
of features in Level 3+2 test set while keeping

Table 2: Relation of errors and words with a few
features. In the table, (h) and (l) shows pairs that
are judged higher (lower) by the system. Column
of < 50 (< 20) means number of pairs each of
which has less than 50 (20) features.

Level #errs < 50 fea. < 20 fea.
Lvl.3+2 (h) 125 76 (61%) 32 (26%)
Lvl.3+2 (l) 220 150 (68%) 60 (27%)
Lvl.2+1 (h) 137 75 (55%) 32 (23%)
Lvl.2+1 (l) 253 135 (53%) 52 (21%)
Lvl.1+0 (h) 149 23 (15%) 4 ( 3%)
Lvl.1+0 (l) 100 17 (17%) 3 ( 3%)

the performance, if we can reduce them prop-
erly. In a same way, 87% of features in Level
2+1 set, and 52% of features in Level 1+0 set,
may also be reduced. These numbers are given
at the situation in which F-measure attains best
performance. Here, it is not to say that we are
sure to reduce them in future, but to estimate how
many features are really effective to distinguish
the similarity.

Here we have more look at the statistics. The
number of initial features on average is 609 in
Level 3+2 test set. If we decrease threshold by
0.1, we can reduce 98% of features at the thresh-
old of 0.8, where the performance remains best
(0.791). This is a surprising fact for us since
only 12 (; 609×(1−0.98)) features really con-
tribute the performance. Therefore, we estimate
that there is a lot to be reduced further in order
to purify the features.

5 Conclusion and Future Work

This paper illustrates improvement of lexical
distributional similarity by not only associated
features but also utilizing unassociated features.
The core idea is simple, and is reasonable when
we look at machine learning; in many cases we
use training instances of not only something pos-
itive but something negative to make the distinc-
tion of the two sides clearer. Similarly, in our
task we use features of not only associated but
unassociated to make computation of similarity
(or distance in semantic space) clearer. We as-

38



sert in this work that a feature that has similar
weight to two given words also plays important
role, regardless of how much it is associated to
the given words.

Among several future works we need to fur-
ther explore reduction of features. It is reported
by some literature such as Hagiwara et al. (2006)
that we can reduce so many features while pre-
serving the same accuracy in distributional sim-
ilarity calculation. This implies that, some of
them are still harmful and are expected to be re-
duced further.

List of Tools and Resources

1. Chasen, a morphological analyzer,
Ver.2.3.3. Matsumoto Lab., Nara Institute
of Science and Technology. http://chasen-
legacy.sourceforge.jp/

2. IPADIC, a dictionary for morphologi-
cal analyzer. Ver.2.7.0. Information-
Technology Promotion Agency, Japan.
http://sourceforge.jp/projects/ipadic/

3. Bunrui Goihyo, a word list by semantic
principles, revised and enlarged edi-
tion. The National Institute for Japanese
Language. http://www.kokken.go.jp
/en/publications/bunrui goihyo/

4. Nihon Keizai Shimbun Newspaper Corpus,
years 1990-2004, Nihon Keizai Shimbun,
Inc.

References

Dagan, Ido, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based Models of Co-occurrence Proba-
bilities. Machine Learning, 34(1-3):43–69.

Grefenstette, Gregory. 1994. Exploration in Auto-
matic Thesaurus Discovery. Kluwer Academic
Publishers. Norwell, MA.

Hagiwara, Masato, Yasuhiro Ogawa, Katsuhiko
Toyama. 2006. Selection of Effective Contextual
Information for Automatic Synonym Acquisition.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pp.353–360.

Harris, Zelig S. 1968. Mathematical Structures of
Language. Wiley, New Jersey.

Hindle, Donald. 1990. Noun Classification from
Predicate-Argument Structures. In Proceedings
of the 28th Annual Meeting of the Association for
Computational Linguistics, pp.268–275.

Lee, Lillian. 1997. Similarity-Based Approaches to
Natural Language Processing. Ph.D. thesis, Har-
vard University, Cambridge, MA.

Lee, Lillian. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pp. 25–32, College Park, MD.

Lin, Dekang. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Confer-
ence on Computational Linguistics, pp.768–774.
Montreal.

Lin, Dekang and and Patrick Pantel. 2002. Con-
cept Discovery from Text. In Proceedings of 19th
International Conference on Computational Lin-
guistics, pp.577–583. Taipei.

Ruge, Gerda. 1992. Experiments of Linguistically-
based Term Associations. Information Processing
& Management, 28(3):317–332.

Shibata, Tomohide and Sadao Kurohashi. 2009. Dis-
tributional similarity calculation using very large
scale Web corpus. In Proceedings of Annual Meet-
ing of Association for Natural Language Process-
ing. pp. 705–708.

Weeds, Julie and David Weir. 2005. Co-occurrence
retrieval: A Flexible Framework for Lexical Dis-
tributional Similarity. Computational Linguistics.
31(4):439–476.

Zhitomirsky-Geffet, Maayan and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Qual-
ity. Computational Linguistics, 35(3):435–461.

39


