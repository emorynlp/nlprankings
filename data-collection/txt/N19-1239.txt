




































Open Information Extraction from Question-Answer Pairs


Proceedings of NAACL-HLT 2019, pages 2294–2305
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2294

Open Information Extraction from Question-Answer Pairs

Nikita Bhutani ∗
University of Michigan

nbhutani@umich.edu

Yoshihiko Suhara
Megagon Labs

yoshi@megagon.ai

Wang-Chiew Tan
Megagon Labs

wangchiew@megagon.ai

Alon Halevy
Megagon Labs

alon@megagon.ai

H. V. Jagadish
University of Michigan

jag@eecs.umich.edu

Abstract
Open Information Extraction (OPENIE) ex-
tracts meaningful structured tuples from free-
form text. Most previous work on OPENIE
considers extracting data from one sentence at
a time. We describe NEURON, a system for
extracting tuples from question-answer pairs.
Since real questions and answers often con-
tain precisely the information that users care
about, such information is particularly desir-
able to extend a knowledge base with.

NEURON addresses several challenges. First,
an answer text is often hard to understand
without knowing the question, and second, rel-
evant information can span multiple sentences.
To address these, NEURON formulates extrac-
tion as a multi-source sequence-to-sequence
learning task, wherein it combines distributed
representations of a question and an answer to
generate knowledge facts. We describe exper-
iments on two real-world datasets that demon-
strate that NEURON can find a significant num-
ber of new and interesting facts to extend a
knowledge base compared to state-of-the-art
OPENIE methods.

1 Introduction

Open Information Extraction (OPENIE) (Banko
et al., 2007) is the problem of extracting structured
data from a text corpus, without knowing a priori
which relations will be extracted. It is one of the
primary technologies used in building knowledge
bases (KBs) that, in turn, power question answer-
ing (Berant et al., 2013). The vast majority of pre-
vious work on OPENIE extracts structured informa-
tion (e.g., triples) from individual sentences.

This paper addresses the problem of extract-
ing structured data from conversational question-
answer (CQA) data. Often, CQA data contains pre-
cisely the knowledge that users care about. As
∗ Part of the work was done while the author was at

Megagon Labs.

such, this data offers a goal-directed method for
extending existing knowledge bases. Consider, for
example, a KB about a hotel that is used to power
its website and/or a conversational interface for
hotel guests. The KB provides information about
the hotel’s services: complimentary breakfast, free
wifi, spa. However, it may not include information
about the menu/times for the breakfast, credentials
for the wifi, or the cancellation policy for a spa
appointment at the hotel. Given the wide range
of information that may be of interest to guests,
it is not clear how to extend the KB in the most
effective way. However, the conversational logs,
which many hotels keep, contain the actual ques-
tions from guests, and can therefore be used as a
resource for extending the KB. Following exam-
ples illustrate the kind of data we aim to extract:

Example 1. Q: Does the hotel have a gym?
A: It is located on the third floor and is 24/7.
Tuple: 〈gym, is located on, third floor〉
Example 2. Q: What time does the pool open?
A: 6:00am daily.
Tuple: 〈pool, open, 6:00am daily〉

As can be seen from these examples, harvest-
ing facts from CQA data presents significant chal-
lenges. In particular, the system must interpret in-
formation collectively between the questions and
answers. In this case, it must realize that ‘third
floor’ refers to the location of the ‘gym’ and that
6:00am refers to the opening time of the pool.
OPENIE systems that operate over individual sen-
tences ignore the discourse and context in a QA
pair. Without knowing the question, they either
fail to or incorrectly interpret the answer.

This paper describes NEURON, an end-to-end
system for extracting information from CQA data.
We cast OPENIE from CQA as a multi-source
sequence-to-sequence generation problem to ex-
plicitly model both the question and answer



2295

in a QA pair. We propose a multi-encoder,
constrained-decoder framework that uses two en-
coders to encode each of the question and answer
to an internal representation. The two representa-
tions are then used by a decoder to generate an out-
put sequence corresponding to an extracted tuple.
For example, the output sequence of Example 2 is:

〈arg1〉 pool 〈/arg1〉〈rel〉 open 〈/rel〉〈arg2〉 6:00am daily 〈/arg2〉

While encoder-decoder frameworks have been
used extensively for machine translation and sum-
marization, there are two key technical challenges
in extending them for information extraction from
CQA data. First, it is vital for the translation model
to learn constraints such as, arguments and rela-
tions are sub-spans from the input sequence, out-
put sequence must have a valid syntax (e.g., 〈arg1〉
must precede 〈rel〉). These and other constraints
can be integrated as hard constraints in the de-
coder. Second, the model must recognize auxiliary
information that is irrelevant to the KB. For exam-
ple, in the hotel application, NEURON must learn to
discard greetings in the data. Since existing facts
in the KB are representative of the domain of the
KB, this prior knowledge can be incorporated as
soft constraints in the decoder to rank various out-
put sequences based on their relevance. Our con-
tributions are summarized below:

• We develop NEURON, a system for extracting in-
formation from CQA data. NEURON is a novel
multi-encoder constrained-decoder method that
explicitly models both the question and the an-
swer of a QA pair. It incorporates vocabulary
and syntax as hard constraints and prior knowl-
edge as soft constraints in the decoder.
• We conduct comprehensive experiments on two

real-world CQA datasets. Our experimental re-
sults show that the use of hard and soft con-
straints improves the extraction accuracy and
NEURON achieves the highest accuracy in ex-
tracting tuples from QA pairs compared with
state-of-the-art sentence-based models, with a
relative improvement as high as 13.3%. NEU-
RON’s higher accuracy and ability to discover
15-25% tuples that are not extracted by state-
of-the-art models make it suitable as a tuple ex-
traction tool for KB extension.
• We present a case study to demonstrate how a

KB can be extended iteratively using tuples ex-
tracted using NEURON. In each iteration, only
relevant tuples are included in the KB. In turn,
the extended KB is used to improve relevance

scoring for subsequent iterations.

2 Task Formulation

In this work, we choose to model an OPENIE ex-
traction from a question-answer (QA) pair as a
tuple consisting of a single relation with two ar-
guments, where the relation and arguments are
contiguous spans from the QA pair. Formally,
let (q, a) be a QA pair, where question q =
(q1, q2, ..., qm) and answer a = (a1, a2, ..., an)
are word sequences. The output is a triple
(arg1,rel,arg2) extracted from (q, a). The output
triple can be naturally interpreted as a sequence
y = (y1, y2, ..., yo) where yi is either a word or a
placeholder tag (〈arg1〉, 〈rel〉, 〈arg2〉) that marks
relevant portions of the triple. In OPENIE, the ex-
tracted tuple should be asserted by the input QA
pair. Formulating this, therefore, requires the vo-
cabulary of y to be restricted to the vocabulary of
(q, a) and placeholder tags.

Following this definition, our aim is to di-
rectly model the conditional probability p(y|q, a)
of mapping input sequences q and a into an output
sequence:

P (y|q, a) =
o∏

i=1

p(yi|y1, . . . , yi−1, q, a). (1)

In our formulation, a triple is generated as a se-
quence: a head argument phrase arg1, followed
by a relation phrase rel and a tail argument phrase
arg2. It is possible to consider different orderings
in the output sequence (such as (rel,arg1,arg2)).
However, the goal of OPENIE is to identify the re-
lation phrase that holds between a pair of argu-
ments. Our representation is, thus, consistent with
this definition as it models the relation phrase to
depend on the head argument.

3 NeurON: A Multi-Encoder
Constrained-Decoder Model

Overview of NEURON We propose to extract
tuples using a variation of an encoder-decoder
RNN architecture (Cho et al., 2014) operating on
variable-length sequences of tokens. Fig. 1 shows
the architecture of NEURON. It uses two encoders
to encode question and answer sequences in a QA
pair separately into fixed-length vector represen-
tations. A decoder then decodes the vector rep-
resentations into a variable-length sequence corre-
sponding to the tuple. The decoder is integrated



2296

Is the indoor open

Question Encoder

Constrained-Decoder

vocab-mask 

combiner

tag-mask

x

x

⟨arg1⟩ indoor pool ..

It opens at 10am

Answer Encoder

attention

relevance scoring

⟨s⟩ ⟨arg1⟩ indoor ..

Soft Constraints

Hard Constraints

]

]

Figure 1: Multi-Encoder, Constrained-Decoder model for tuple extraction from (q, a).

with a set of hard constraints (e.g., output vocabu-
lary) and soft constraints (e.g., relevance scoring)
suited for the extraction task.

3.1 Multiple Encoders

Given an input QA pair, two RNN encoders sepa-
rately encode the question and answer. The ques-
tion encoder converts q into hidden representa-
tion hq = (hq1, ..., h

q
m) and the answer encoder

converts a into ha = (ha1, ..., h
q
n), where h

q
t =

lstm(qt, h
q
t−1) is a non-linear function represented

by the long short-term memory (LSTM) cell. The
combiner combines the encoders’ states and ini-
tializes the hidden states h for the decoder:

h = tanh(Wc[h
q ◦ ha]),

where ◦ denotes concatenation. The decoder stage
uses the hidden states to generate the output y
with another LSTM-based RNN. The probability
of each token is defined as:

p(yt) = softmax((st ◦ cqt ◦ cat )Wy), (2)

where st denotes the decoder state, s0 = h and
st = lstm((yt−1 ◦ cqt ◦ cat )Ws, st−1). The de-
coder is initialized by the last hidden state from
the combiner. It uses the previous output token
at each step. Both Wy and Ws are learned matri-
ces. Each decoder state is concatenated with con-
text vectors derived from the hidden states of the
encoders. Context vector ct is the weighted sum of
the encoder hidden states, i.e. cqt =

∑m
i=1 αtih

q
i ,

where αti corresponds to an attention weight. The
attention model (Bahdanau et al., 2015) helps the
model learn to focus on specific parts of the in-
put sequences, instead of solely relying on hidden
vectors of the decoders’ LSTM. This is crucial for
extraction from (q, a) pairs where input sequences
tend to be long.

3.2 Constrained Decoder
The decoder finds the best hypothesis (i.e., the best
output sequence) for the given input representa-
tions. Typically, the output sequence is generated,
one unit at a time, using beam search. At each
time step, the decoder stores the top-k scoring par-
tial sequences, considers all possible single token
extensions of them, and keeps k most-likely se-
quences based on model’s probabilities (Eq. 1). As
soon as the 〈/S〉 symbol is appended, the sequence
is removed from the beam and added to the set of
complete sequences. The most-likely complete se-
quence is finally generated.
Hard Constraints While such encoder-decoder
models typically outperform conventional ap-
proaches (Cho et al., 2014; Zoph and Knight,
2016; Xiong et al., 2017) on a wide variety of
tasks including machine translation and question
answering, the accuracy and training efficiency
has been shown to improve when the model is in-
tegrated with the constraints of the output domain
(Xiao et al., 2016; Yin and Neubig, 2017). Moti-
vated by these, NEURON allows constraints relevant
to information extraction to be incorporated in the
model. Specifically, we describe how the decoder
can enforce vocabulary and structural constraints
on the output.

• Vocabulary constraints. Since the arguments
and relations in the extracted tuples typically cor-
respond to the input QA pair, the decoder must
constraint the space of next valid tokens when gen-
erating the output sequence. NEURON uses a mask-
ing technique in the decoder to mask the probabil-
ity of tokens (as in Eq. 2) that do not appear in the
input (q, a) pair. Specifically, it computes a binary
mask vector v, where |v| is vocabulary size and
vi = 1 if and only if i-th token appears in q or a.
The probability of each token is modified as:

p(yt) = softmax((st ◦ cqt ◦ cat )Wy ⊗ v), (3)



2297

⟨S ⟩

⟨S ⟩ :
(V ∪ T )\⟨arg1⟩

B-arg1

I-arg1

I-arg′�1

B-rel

I-rel

I-rel′�

B-arg1

I-arg1

I-arg′�1 ⟨/S ⟩

⟨rel⟩ : T ⟨arg2⟩ : T⟨arg1⟩ : T

w ∈ V : T \⟨/arg1⟩

w ∈ V : T \⟨/arg1⟩ w ∈ V : T \⟨/rel⟩

w ∈ V : T \⟨/rel⟩

w ∈ V : T \⟨/arg2⟩

w ∈ V : T \⟨/arg2⟩

(inpu t) : (tag mask)⟨/arg1⟩ :(V ∪ T )\⟨rel⟩
⟨/rel⟩ :
(V ∪ T )\⟨arg2⟩

⟨/arg2⟩ :
(V ∪ T )\⟨/S ⟩

Figure 2: State diagram for tag masking rules. V is the
vocabulary including placeholder tags, T is the set of
placeholder tags.

where ⊗ indicates element-wise multiplication.

• Structural constraints. For the output se-
quence to correspond to a valid tuple with non-
empty arguments, the decoding process must con-
form to the underlying grammar of a tuple. For
instance, decoding should always begin in the 〈S〉
state, where only 〈arg1〉 can be generated. In
subsequent time steps, all other placeholders ex-
cept 〈/arg1〉 should be restricted to ensure a non-
empty argument. Once 〈/arg1〉 is generated, 〈rel〉
must be generated in the next time step and so on.
The various states and grammar rules can be de-
scribed as a finite state transducer as shown in Fig-
ure 2.

Depending upon the state, NEURON generates a
mask r based on this grammar and uses r to further
modify the probabilities of the tokens as follows:

p(yt) = softmax((st ◦ cqt ◦ cat )Wy ⊗ v ⊗ r). (4)

Soft Constraints OPENIE systems are typically
used to extract broad-coverage facts to extend ex-
isting KBs. Facts already existing in the KB are
representative of the domain of the KB. It is, there-
fore, useful to incorporate this prior knowledge in
the extraction itself. NEURON is able to use prior
knowledge (incorporated as soft constraints) in the
decoder to understand the relevance of candidate
extractions and adjust the ranking of various out-
put sequences accordingly. To see why such soft
constraints can be useful, consider the example:

Example 3. Q: “Is the pool open?”
A: “I am sorry but our pool reopens at 7:00am.”
Tuple: 〈I, am, sorry〉; 〈pool, reopens at, 7:00am〉
Both the tuple facts are correct given the input QA
pair but only the second tuple contains useful in-
formation. Filtering such irrelevant facts is diffi-
cult without additional evidence.

The multi-encoder and constrained-decoder in
NEURON are jointly optimized to maximize the

log probability of output sequence conditioned on
the input sequences. At inference, the decoder
estimates the likelihood of various candidate se-
quences and generates the sequence with the high-
est likelihood. As shown in Eq. 1, this likelihood
is conditioned solely on the input (q, a) pair, thus
increasing the possibility of obtaining facts that
may be correct but irrelevant. Instead, if a rele-
vance scoring function were integrated at extrac-
tion time, the candidate output sequences could be
re-ranked so that the predicted output sequence is
likely to be both correct and relevant.

Learning a relevance scoring function can be
modeled as a KB completion task, where miss-
ing facts have to be inferred from existing ones.
A promising approach is to learn vector represen-
tations of entities and relations in a KB by max-
imizing the total plausibility of existing facts in
the KB (Wang et al., 2017). For a new candidate
output sequence, its plausibility can be predicted
using the learned embeddings for the entities and
relation in the sequence.

In NEURON, we learn the entity and relation
embeddings using Knowledge Embedding (KE)
methods such as TransE (Bordes et al., 2013) and
HolE (Nickel et al., 2016). Note that NEURON
is flexible with how the relevance scoring func-
tion is learned or which KE method is chosen. In
this paper, we use TransE for evaluation. TransE
computes the plausibility score S of a tuple y =
〈arg1, rel, arg2〉 as:

S(y) = ||varg1 + vrel − varg2 ||,

where varg1 , vrel, and varg2 are embedding vec-
tors for arg1, rel, and arg2 respectively. Follow-
ing (Jain et al., 2018), we compute the embed-
ding vectors of out-of-vocabulary arguments (and
relations) as the average of embedding vectors of
known arguments (and relations). We generate the
most-likely output based on its conditional proba-
bility and plausibility score:

ŷ = argmax
y

(logP (y|q, a) + γ logS(y)). (5)

To implement the tuple relevance scoring func-
tion, we employ the re-ranking approach, which
is a common technique for sequence genera-
tion methods (Luong et al., 2015b). Our re-
ranking method first obtains candidates from a
beam search decoder and then re-ranks the can-



2298

didates based on the objective function (Eq. 5).

4 Experiments

We evaluated the performance of NEURON on two
CQA datasets. In our analysis, we find that inte-
grating hard and soft constraints in the decoder
improved the extraction performance irrespective
of the number of encoders used. Also, 15-25% of
the tuples extracted by NEURON were not extracted
by state-of-the-art sentence-based methods.

4.1 Datasets and Data Preparation

ConciergeQA is a real-world internal corpus of
33,158 QA pairs collected via a multi-channel
communication platform for guests and hotel staff.
Questions (answers) are always made by guests
(staff). An utterance has 36 tokens on average, and
there are 25k unique tokens in the dataset. A QA
utterance has 2.8 sentences on average, with the
question utterance having 1.02 sentences on aver-
age and answer utterance having 1.78 sentences on
average.
AmazonQA (Wan and McAuley, 2016; McAuley
and Yang, 2016) is a public dataset with 314,264
QA pairs about electronic products on ama-
zon.com. The dataset contains longer and more
diverse utterances than the ConciergeQA dataset:
utterances have an average of 45 tokens and the
vocabulary has more than 50k unique tokens. A
QA utterance has 3.5 sentences on average. The
question utterances had 1.5 sentences on average
and the answer having 2 sentences.

For training NEURON, we bootstrapped a large
number of high-quality training examples using a
state-of-the-art OPENIE system. Such bootstrap-
ping has been shown to be effective in informa-
tion extraction tasks (Mausam et al., 2012; Saha
et al., 2017). The StanfordIE (Angeli et al., 2015)
system is used to extract tuples from QA pairs for
training examples. To further obtain high-quality
tuples, we filtered out tuples that occur too infre-
quently (< 5) or too frequently (> 100). For each
tuple in the set, we retrieved all QA pairs that con-
tain all the content words of the tuple and included
them in the training set. This helps create a train-
ing set encapsulating the multiplicity of ways in
which tuples are expressed across QA pairs. We
randomly sampled 100 QA pairs from our boot-
strapping set and found 74 of them supported the
corresponding tuples. We find this quality of boot-
strapped dataset satisfactory, since the seed tuples

Instance type ConciergeQA AmazonQA
Exclusively from question 13.9% 13.8%
Exclusively from answer 25.8% 17.6%
Ambiguous 36.9% 29.8%
Jointly from Q-A 23.4% 38.8%

Table 1: Various types of training instances.

Dataset Q-A |V| Train Dev Test (Q-A)
ConciergeQA 33k 25k 1.25M 128k 2,905
AmazonQA 314k 50k 1.43M 159k 39,663

Table 2: Training, Dev and Test splits.

for bootstrapping could be noisy as they were gen-
erated by another OPENIE system.

Our bootstrapped dataset included training in-
stances where a tuple matched (a) tokens in the
questions exclusively, (b) tokens in the answers
exclusively, (c) tokens from both questions and an-
swers. Table 1 shows the distribution of the var-
ious types of training instances. Less than 40%
(30%) of ground truth tuples for ConciergeQA
(AmazonQA) exclusively appear in the questions
or answers. Also, 22.1% (37.2%) of ground
truth tuples for ConciergeQA (AmazonQA) are ex-
tracted from the combination of questions and an-
swers. These numbers support our motivation of
extracting tuples from QA pairs. We used standard
techniques to construct training/dev/test splits so
that QA pairs in the three sets are disjoint. Table 2
shows the details of the various subsets.

4.2 Baseline Approaches

We compared NEURON with two methods that can
be trained for tuple extraction from QA pairs:
BILSTM-CRF (Huang et al., 2015) and NEURALOPE-
NIE (Cui et al., 2018). BILSTM-CRF is a sequence
tagging model that has achieved state-of-the-art
accuracy on POS, chunking, NER and OPENIE
(Stanovsky et al., 2018) tasks. For OPENIE, the
model predicts boundary labels (e.g., B-ARG1, I-
ARG1, B-ARG2, O) for the various tokens in a
QA pair. NEURALOPENIE is an encoder-decoder
model that generates a tuple sequence given an in-
put sequence. Since it uses a single encoder, we
generate the input sequence by concatenating the
question and answer in a QA pair. We trained all
the models using the same training data.

4.3 Performance Metrics

We examine the performance of different methods
using three metrics: precision, recall, and relative
coverage (RC). Given a QA pair, each system re-



2299

turns a sequence. We label the sequence correct
if it matches one of the ground-truth tuples for the
QA pair, incorrect otherwise. We then measure
precision of a method (i.e., # of correct predictions
of the method / # of question-answer pairs) and
recall (i.e., # of correct predictions of the method
/ # of correct predictions of any method) follow-
ing (Stanovsky and Dagan, 2016). To compare
the coverage of sequences extracted by NEURON
against the baseline method, we compute relative
coverage of NEURON as the fraction of all cor-
rect predictions that were generated exclusively by
NEURON. Specifically,

RC =
|TPNEURON\TPbaseline|
|TPNEURON

⋃
TPbaseline|

,

where TP denotes the correct predictions.

4.4 Model Training and Optimization

We implemented NEURON using OpenNMT-
tf (Klein et al., 2017) , an open-source neural
machine translation system that supports multi-
source encoder-decoder models. We implemented
NEURALOPENIE using the same system. We
used the open-source implementation of BILSTM-
CRF (Reimers and Gurevych, 2017). For fair com-
parison, we used identical configurations for NEU-
RON and NEURALOPENIE. Each encoder used a 3-
layer bidirectional LSTM and the decoder used
a 3-layer bidirectional LSTM. The models used
256-dimensional hidden states, 300-dimensional
word embeddings, and a vocabulary size of 50k.
The word embeddings were initialized with pre-
trained GloVe embeddings (glove.6B) (Penning-
ton et al., 2014). We used an initial learning rate of
1 and optimized the model with stochastic gradi-
ent descent. We used a decay rate of 0.7, a dropout
rate of 0.3 and a batch size of 64. The models
were trained for 1M steps for the ConciergeQA
dataset and 100k steps for the AmazonQA dataset.
We used TESLA K80 16GB GPU for training the
models. We trained the KE models for relevance
scoring using our bootstrapped training dataset.
For integrating the relevance scoring function, we
experimented with different values for γ and found
it not have a major impact within a range of 0.02
to 0.2. We used a value of 0.05 in all the experi-
ments.

4.5 Experimental Results

The BILSTM-CRF model showed extremely low
(2-15%) precision values. Very few of the tagged

Method P R RC
NEURALOPENIE (baseline) 0.769 0.580 -
+ hard constraints 0.776 0.585 -
+ hard and soft constraints 0.796 0.600 -

NEURON (our method) 0.791 0.597 0.224
+ hard constraints 0.792 0.597 0.204
+ hard and soft constraints 0.807 0.608 0.245

Table 3: Precision (P), Recall (R), and Relative Cover-
age (RC) results on ConciergeQA.

sequences (32-39%) could be converted to a tuple.
Most tagged sequences had multiple relations and
arguments, indicating that it is difficult to learn
how to tag a sequence corresponding to a tuple.
The model only learns how to best predict tags for
each token in the sequence, and does not take into
account the long-range dependencies to previously
predicted tags. This is still an open problem and is
outside the scope of this paper.

Tables 3 and 4 show the performance of NEU-
RALOPENIE and NEURON on the two CQA datasets.
NEURON achieves higher precision on both the
datasets. This is because NEURALOPENIE uses a sin-
gle encoder to interpret the question and answer in
the same vector space, which leads to lower per-
formance. Furthermore, concatenating the ques-
tion and answer makes the input sequence too long
for the decoder to capture long-distance depen-
dencies in history (Zhang et al., 2016; Toral and
Sánchez-Cartagena, 2017). Despite the attention
mechanism, the model ignores past alignment in-
formation. This makes it less effective than the
dual-encoder model used in NEURON.

The tables also show that incorporating task-
specific hard constraints helps further improve
the overall precision and recall, regardless of the
methods and the datasets. Re-ranking the tuples
based on the soft constraints derived from the ex-
isting KB further improves the performance of
both methods in ConciergeQA and NEURALOPE-
NIE in AmazonQA. The existing KB also helps
boost the likelihood of a correct candidate tuple se-
quence that was otherwise scored to be less likely.
Lastly, we found that NEURON has significant rela-
tive coverage; it discovered significant additional,
unique tuples missed by NEURALOPENIE.

Table 4 shows a slight decrease in performance
for NEURON after soft constraints are added. This
is likely caused by the lower quality KE model
due to the larger vocabulary in AmazonQA. In con-
trast, even with the lower quality KE model, NEU-



2300

Method P R RC
NEURALOPENIE (baseline) 0.557 0.594 -
+ hard constraints 0.563 0.601 -
+ hard and soft constraints 0.571 0.610 -

NEURON (our method) 0.610 0.652 0.139
+ hard constraints 0.631 0.674 0.164
+ hard and soft constraints 0.624 0.666 0.149

Table 4: Precision (P), Recall (R), and Relative Cover-
age (RC) results on AmazonQA dataset.

RALOPENIE improved slightly. This is likely be-
cause the NEURALOPENIE model, at this stage, still
had a larger margin for improvement. We note
however that learning the best KE model is not the
focus of this work.

AmazonQA is a more challenging dataset than
ConciergeQA: longer utterances (avg. 45 tokens
vs. 36 tokens) and richer vocabulary (> 50k
unique tokens vs. < 25k unique tokens). This
is reflected in lower precision and recall values of
both the systems on the AmazonQA dataset. While
the performance of end-to-end extraction systems
depends on the complexity and diversity of the
dataset, incorporating hard and soft constraints al-
leviates the problem to some extent.

End-to-end extraction systems tend to out-
perform rule-based systems on extraction from
CQA datasets. We observed that training data
for ConciergeQA had a large number (> 750k)
dependency-based pattern rules, of which < 5%
matched more than 5 QA pairs. The set of
rules is too large, diverse and sparse to train
an accurate rule-based extractor. Even though
our training data was generated by bootstrap-
ping from a rule-based extractor StanfordIE, we
found only 51.5% (30.7%) of correct tuples from
NEURON exactly matched the tuples from Stan-
fordIE in ConciergeQA (AmazonQA). This indi-
cates that NEURON combined information from
question and answer, otherwise not accessible to
sentence-wise extractors. As an evidence, we
found 11.4% (6.1%) of tuples were extracted from
answers, 16.8% (5.0%) from questions, while
79.6% (82.5%) combined information from ques-
tions and answers in ConciergeQA (AmazonQA).

Multiple Encoders: Our motivation to use dif-
ferent encoders for questions and answers is based
on the assumption that they use different vocab-
ulary and semantics. We found that there were
8k (72k) unique words in questions, 18k (114k)
unique words in answers, and the Jaccard co-

Figure 3: Example embedding vectors from question
and answer encoders. Underlines denote similar em-
bedding vectors in both the encoders.

efficient between two vocabulary sets was 0.25
(0.25) in ConciergeQA (AmazonQA), indicating
that two sources use significantly different vocab-
ulary. Also, the same word can have different
meanings depending on a speaker, and thus such
words in the two sources should be embedded dif-
ferently. To visualize the embedding vectors of
common words in ConciergeQA, we mapped them
into 2D space using t-SNE (Maaten and Hinton,
2008). Fig. 3 shows that subjective words that rep-
resents speakers attitude (e.g., “ready”, “guests”,
“time”) had significantly different embeddings in
the question and answer encoders. In contrast, ob-
jective words such as menu, or activity (e.g., “ba-
con”, “cruise”, “weekday”) had similar embed-
dings although the two encoders do not directly
share the embedding parameters. This indicates
that multiple encoders not only capture the differ-
ent meanings in questions and answers but also re-
tain consistent meanings for words that keep the
same meanings in the two sources.

Relevance Scoring: We compared with another
NEURON model that uses HolE (Nickel et al.,
2016) for relevance scoring. Both the HolE and
TransE models achieved the same precision of
80.7%, with HolE achieving slightly higher re-
call (+1.4%). This suggests that incorporating rel-
evance scoring in NEURON can robustly improve
the extraction accuracy, regardless of the choice of
the knowledge embedding method. We also esti-
mated the upper-bound precision by evaluating if
the correct tuple was included in the top-500 can-
didates. The upper-bound precision was 85.0% on
ConciergeQA, indicating that there is still room for
improvement on incorporating relevance scoring.



2301

1
Q: Tell me what the username and password is for WiFi
A: Absolutely! Both the username and passcode is C800.
StanfordIE: 〈 passcode, is, C800 〉
NEURON: 〈 password, is, C800 〉

2
Q: Do hotel guys have ice?
A: There is an ice machine on first floor lobby.
StanfordIE: 〈 hotel, do, ice 〉
NEURON: 〈 hotel, have, ice machine 〉

3
Q: Is there a charge for parking rental car on the property?
A: Self-parking will be $15 per night.
StanfordIE: None
NEURON: 〈 parking, will, charge 〉

4
Q: arrange late check out for tomorrow?
A: I have notated a 12 pm check out. Normal check out

time is at 11 am.
StanfordIE: 〈 normal check, is at, 11 am 〉
NEURON: 〈 check, is at, 11 〉

Table 5: Examples of successful cases (1 and 2) and
failed cases (3 and 4) from test data.

4.6 Error Analysis

We examined a random sample of 100 errors
shared by all the systems across the tested datasets.
Arguably, encoder-decoder models suffer when
extracting tuples from long utterances (avg. of 54
tokens), contributing to 43% of the errors. 34%
of the incorrectly extracted tuples used words that
were shared across the two sources. This indicates
that the extractor makes errors when resolving am-
biguity in tokens. 28% of the error cases used in-
formal language that is generally difficult for any
extractor to understand. We show some examples
(1 and 2 in Table 5) where NEURON successfully
combined information across two sources and ex-
amples (3 and 4 in Table 5) where it failed.

We further examined three different scenar-
ios: a) errors are shared by both NEURON and
NEURALOPENIE, b) errors are made exclusively by
NEURON, c) errors are made exclusively by NEU-
RALOPENIE. For each scenario, we examined a ran-
dom sample of 100 errors. We categorize the dif-
ferent sources of errors and report the results in
Table 6. As shown, NEURON is superior on longer
utterances compared to NEURALOPENIE (54 tokens
vs. 49 tokens). However, ambiguity in tokens in
the two sources is a concern for NEURON because
it has the flexibility to interpret the question and
answer differently. Not surprisingly, informal ut-
terances are hard to translate for both the systems.

5 Case Study - KB Extension

The extracted tuples from NEURON can be used to
extend a KB for a specific domain. However, au-
tomatically fusing the tuples with existing facts in

Error Category N , B N , B N , B
long utterances 43% 45% 40%
avg. length of utterance 54 tokens 49 tokens 54 tokens
ambiguity 34% 36% 48%
informal language 28% 36% 34%

Table 6: Different errors N and B made by NEURON
(N ) and NEURALOPENIE (B) respectively.

Knowledge Base

Integrator
Facts

Candidate Facts

Multi-Encoder 
Constrained Decoder

Knowledge 
Embedding ModelQA Corpus

Figure 4: Human-in-the-loop system for extending a
domain-specific KB.

the KB can have limited accuracy. This can be
due to noise in the source conversation, no prior
knowledge of join rules and more. One possi-
ble solution is to design a human-in-the-loop sys-
tem that iteratively extracts tuples and filters them
based on human feedback (Fig. 4). In each itera-
tion, a set of tuples is annotated by human annota-
tors based on their relevance to the domain of the
KB. The tuples marked relevant are added to the
KB and the relevance scoring function is updated
for extracting more relevant tuples from the corpus
in the next iteration.

We conducted a crowdsourced experiment1,
simulating the first iteration of the procedure i.e.,
when no KE model is available. We collected an-
notations on top-5 tuples extracted by NEURON for
200 QA pairs in the ConciergeQA dataset. For re-
liability, we hired five workers for each extraction.
The workers were asked to judge if a tuple is rel-
evant to the hotel domain and represents concrete
information to be added to a KB. We found preci-
sion@5 was 41.4%, and NEURON extracted at least
one useful tuple for 83.0% of the 200 QA pairs.
Overall, the system added 243 unique tuples (out
of 414 tuples extracted by NEURON) to the KB. We
also collected annotations for the tuples extracted
by NEURALOPENIE. The precision@5 and recall@5
values were 41.3% and 79.0% respectively. Al-
though the precision values are quite similar, NEU-
RON can extract correct tuples from more QA pairs
than NEURALOPENIE. While the precision can fur-
ther be improved, the preliminary results support
that NEURON is a good candidate for extraction in
1 https://www.figure-eight.com/



2302

a human-in-the-loop system for KB extension. We
did not use any sophisticated methods for ranking
tuples in our experiment. Thus, a better ranking
algorithm might lead to improved precision.

6 Related Work

There is a long history of OPENIE systems for
extracting tuples from plain text. They are
built on hand-crafted patterns over an intermedi-
ate representation of a sentence (e.g., POS tags
(Yates et al., 2007; Fader et al., 2011), depen-
dency trees (Bhutani et al., 2016; Mausam et al.,
2012)). Such rule-based systems require exten-
sive engineering when the patterns become di-
verse and sparse. Recently, OPENIE systems based
on end-to-end frameworks, such as sequence tag-
ging (Stanovsky et al., 2018) or sequence-to-
sequence generation (Cui et al., 2018), have been
shown to alleviate such engineering efforts. How-
ever, all these systems focus on sentence-level ex-
traction. We are the first to address the problem of
extracting tuples from question-answer pairs.

Our proposed system is based on an encoder-
decoder architecture, which was first introduced
by Cho et al. for machine translation. Atten-
tion mechanisms (Bahdanau et al., 2015; Luong
et al., 2015b) have been shown to be effective
for mitigating the problem of poor translation per-
formance on long sequences. Their model can
learn how much information to retrieve from spe-
cific parts of the input sequence at decoding time.
There is abundant research on generalizing such
frameworks for multiple tasks, specially by em-
ploying multiple encoders. Using multiple en-
coders has been shown to be useful in mutli-task
learning (Luong et al., 2015a), multi-source trans-
lation (Zoph and Knight, 2016) and reading com-
prehension (Xiong et al., 2017). We are the first to
explore a multi-source encoder-decoder architec-
ture for extracting tuples from CQA datasets.

Traditional encoder-decoder architectures are
not tailored for information extraction and knowl-
edge harvesting. To make them suitable for infor-
mation extraction, the sequence generation must
be subjected to several constraints on the vocabu-
lary, grammar etc. Recently, grammar structures
have been integrated into encoder-decoder mod-
els (Iyer et al., 2017; Zhang et al., 2017). There are
variations such as Pointer Networks (Vinyals et al.,
2015) that yield a succession of pointers to tokens
in the input sequence. All these studies share a

common idea with our paper, which is to enforce
constraints at sequence generation time. Since we
focus on extraction from CQA datasets, our work is
broadly related to the literature on relation extrac-
tion (Savenkov et al., 2015; Hixon et al., 2015; Wu
et al., 2018) and ontology extraction (S and Ku-
mar, 2018) from community generated question-
answer datasets. However, we differ in our under-
lying assumption that the relations and entities of
interest are not known in advance. Alternatively,
a CQA dataset could be transformed into declara-
tive sentences (Demszky et al., 2018) for a conven-
tional OPENIE system. However, such a two-stage
approach is susceptible to error propagation. We
adopt an end-to-end solution that is applicable to
generic CQA datasets.

7 Conclusions and Future Work

We have presented NEURON, a system for extract-
ing structured data from QA pairs for the purpose
of enriching knowledge bases. NEURON uses a
multi-encoder, constrained-decoder framework to
generate quality tuples from QA pairs.

NEURON achieves the highest precision and re-
call in extracting tuples from QA pairs compared
with state-of-the-art sentence-based models, with
a relative improvement as high as 13.3%. It can
discover 15-25% more tuples which makes it suit-
able as a tuple extraction tool for KB extension.

There are several directions for future research.
One interesting direction is to investigate whether
NEURON can be extended to work on open-domain
QA corpus, which may not be restricted to any
specific domain.

Acknowledgements

We thank Tom Mitchell and the anonymous re-
viewers for their constructive feedback. This work
was supported in part by the UM Office of Re-
search.

References
Gabor Angeli, Melvin Jose Johnson Premkumar, and

Christopher D Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proc. ACL ’15/IJCNLP ’15, pages 344–354.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. ICLR ’15.

Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.



2303

Open information extraction from the web. In Proc.
IJCAI ’07, pages 2670–2676.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proc. EMNLP ’13, pages
1533–1544.

Nikita Bhutani, HV Jagadish, and Dragomir Radev.
2016. Nested propositions in open information ex-
traction. In Proc. EMNLP ’16, pages 55–64.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proc. NIPS ’13, pages 2787–
2795.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proc. EMNLP
’14, pages 1724–1734.

Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural open
information extraction. In Proc. ACL ’18, pages
407–413.

Dorottya Demszky, Kelvin Guu, and Percy Liang.
2018. Transforming question answering datasets
into natural language inference datasets. arXiv
preprint arXiv:1809.02922.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proc. EMNLP ’11, pages 1535–1545.

Ben Hixon, Peter Clark, and Hannaneh Hajishirzi.
2015. Learning knowledge graphs for question an-
swering through conversational dialog. In Proc.
NAACL-HLT ’15, pages 851–861.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
arXiv preprint arXiv:1508.01991.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung,
Jayant Krishnamurthy, and Luke Zettlemoyer. 2017.
Learning a neural semantic parser from user feed-
back. In Proc. ACL ’17, pages 963–973.

Prachi Jain, Shikhar Murty, Mausam, and Soumen
Chakrabarti. 2018. Mitigating the effect of out-of-
vocabulary entity pairs in matrix factorization for
KB inference. In Proc. IJCAI ’18, pages 4122–
4129.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
In Proc. ACL ’17 (System Demonstrations), pages
67–72.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2015a. Multi-task se-
quence to sequence learning. In Proc. ICLR ’16.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015b. Effective approaches to attention-
based neural machine translation. In Proc. EMNLP
’15, pages 1412–1421.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(Nov):2579–2605.

Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, Oren Etzioni, et al. 2012. Open lan-
guage learning for information extraction. In Proc.
EMNLP ’12, pages 523–534.

Julian McAuley and Alex Yang. 2016. Addressing
complex and subjective product-related queries with
customer reviews. In Proc. WWW ’16, pages 625–
635.

Maximilian Nickel, Lorenzo Rosasco, and Tomaso
Poggio. 2016. Holographic embeddings of knowl-
edge graphs. In Proc. AAAI ’16, pages 1955–1961.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proc. EMNLP ’14, pages 1532–
1543.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of LSTM-networks for sequence tagging. In
Proc. EMNLP ’17, pages 338–348.

Subhashree S and P Sreenivasa Kumar. 2018. En-
riching domain ontologies using question-answer
datasets. In Proc. CoDS-COMAD ’18, pages 329–
332.

Swarnadeep Saha, Harinder Pal, et al. 2017. Bootstrap-
ping for numerical open ie. In Proc. ACL ’17, pages
317–323.

Denis Savenkov, Wei-Lwun Lu, Jeff Dalton, and Eu-
gene Agichtein. 2015. Relation extraction from
community generated question-answer pairs. In
Proc. NAACL-HLT ’15, pages 96–102.

Gabriel Stanovsky and Ido Dagan. 2016. Creating a
large benchmark for open information extraction. In
Proc. EMNLP ’16.

Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer,
and Ido Dagan. 2018. Supervised open information
extraction. In Proc. ACL ’18, pages 885–895.

Antonio Toral and Vı́ctor M. Sánchez-Cartagena. 2017.
A multifaceted evaluation of neural versus phrase-
based machine translation for 9 language directions.
In Proc. EACL ’17, pages 1063–1073.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Proc. NIPS ’15, pages
2692–2700.

Mengting Wan and Julian McAuley. 2016. Modeling
ambiguity, subjectivity, and diverging viewpoints
in opinion question answering systems. In Proc.
ICDM ’16, pages 489–498.



2304

Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
2017. Knowledge graph embedding: A survey of
approaches and applications. IEEE Transactions
on Knowledge and Data Engineering, 29(12):2724–
2743.

Zeqiu Wu, Xiang Ren, Frank F. Xu, Ji Li, and Jiawei
Han. 2018. Indirect supervision for relation extrac-
tion using question-answer pairs. In Proc. WSDM
’18, pages 646–654.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
mantic parsing. In Proc. ACL ’16, pages 1341–
1350.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for question
answering. In Proc. ICLR ’17.

Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: Open information
extraction on the web. In Proc. NAACL-HLT ’07
(Demonstrations), pages 25–26.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proc. ACL ’17, pages 440–450.

Biao Zhang, Deyi Xiong, and Jinsong Su. 2016.
Cseq2seq: Cyclic sequence-to-sequence learning.
arXiv preprint arXiv:1607.08725.

Yaoyuan Zhang, Zhenxu Ye, Yansong Feng, Dongyan
Zhao, and Rui Yan. 2017. A constrained sequence-
to-sequence neural model for sentence simplifica-
tion. arXiv preprint arXiv:1704.02312.

Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proc. NAACL-HLT ’16, pages
30–34.

A Supplementary Material

This supplementary material contains details of
the analysis settings described in Section 4 and ad-
ditional results not reported in the main paper.

A.1 Word Embedding Analysis
We investigated the embedding layers of the ques-
tion encoder and the answer encoder of the NEU-
RON model trained on the ConciergeQA dataset.

For robust analysis, we ignored non-English
words and any words that contained numerical
digits (e.g., #18D, $10). We used PYENCHANT2 for
filtering English words. For the remaining words,
we find their embedding vectors from the two en-
coders, concatenate them to create a single ma-
trix. This ensures that same embedding vectors
2 v2.0.0 https://github.com/rfk/pyenchant

are mapped to the same point in the visualization
space. We used t-SNE3 to map embedding vectors
into 2D space for visualization.

A.2 Crowdsourced Evaluation
Figure 5 shows the instructions and examples of
the crowdsouced task. In the crowdsourcing task,
crowdsourced workers were asked to judge after
reading an extracted tuple with the original QA
pair. Since it is difficult to define the usefulness
of the tuples without assuming a KB, we used rel-
evance and concreteness as criteria to grade ex-
tracted tuples. Specifically, each worker was asked
to choose one option from the three options: Not
relevant or unclear (0), Relevant (1),
Relevant and concrete (2).

We set $0.05 as payment for each annotation.
We carefully created 51 test questions which were
used to filter out untrusted judgments and work-
ers. The platform increases the number of anno-
tators so each tuple should always have 5 trusted
annotators. The 5 annotations for each tuple were
aggregated into a single label with a confidence
value that takes into account the accuracy rates of
the annotators based on the test questions.

3 TSNE v0.20.0 https://scikit-learn.org/stable/
with default configuration



2305

Figure 5: Screenshot of the instructions and examples of the crowdsourced task.


