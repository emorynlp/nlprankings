



















































LR Parsing for LCFRS


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1250–1255,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

LR Parsing for LCFRS

Laura Kallmeyer and Wolfgang Maier
Institute for Language and Information

University of Düsseldorf
Düsseldorf, Germany

{kallmeyer,maierwo}@phil.hhu.de

Abstract

LR parsing is a popular parsing strategy for
variants of Context-Free Grammar (CFG). It
has also been used for mildly context-sensitive
formalisms, such as Tree-Adjoining Gram-
mar. In this paper, we present the first LR-
style parsing algorithm for Linear Context-
Free Rewriting Systems (LCFRS), a mildly
context-sensitive extension of CFG which has
received considerable attention in the last
years.

1 Introduction

LR parsing is an incremental shift-reduce parsing
strategy in which the transitions between parser
states are guided by an automaton which is com-
piled offline. LR parsers were first introduced
for deterministic context-free languages (Knuth,
1965) and later generalized to context-free lan-
guages (Tomita, 1984) and tree-adjoining languages
(Nederhof, 1998; Prolo, 2003).

Linear Context-Free Rewriting System (LCFRS)
(Vijay-Shanker et al., 1987) is an immediate ex-
tension of CFG in which each non-terminal can
cover more than one continuous span of the in-
put string. LCFRS and equivalent formalisms
have been used for the modeling of discontinu-
ous constituents (Maier and Lichte, 2011) and non-
projective dependencies (Kuhlmann, 2013), as well
as for data-driven parsing of such structures (Maier
and Kallmeyer, 2010; Kallmeyer and Maier, 2013;
van Cranenburgh, 2012; Angelov and Ljunglöf,
2014). They have also been used for modeling

non-concatenative morphology (Botha and Blun-
som, 2013), for grammar engineering (Ranta, 2011),
and for modeling alignments in machine translation
(Søgaard, 2008; Kaeshammer, 2013). To our knowl-
edge, so far, no LR strategy for LCFRS has been
presented in the literature. In this paper, we present
an LR-style parser for LCFRS. It is based on the in-
cremental parsing strategy implemented by Thread
Automata (Villemonte de la Clergerie, 2002).

The remainder of the article is structured as fol-
lows. In the following section, we introduce LCFRS
and thread automata. Section 3 presents the algo-
rithm along an example. In particular, section 3.2
gives the algorithms for automaton and parse table
constructions, and section 3.3 presents the parsing
algorithm. Section 4 concludes the article.

2 Preliminaries

2.1 LCFRS

In this paper, we restrict ourselves to string rewriting
LCFRS and omit the more general definition (Weir,
1988).

In LCFRS, a single non-terminal can span k ≥ 1
continuous blocks of a string. A CFG is simply a
special case of an LCFRS in which k = 1. k is
called the fan-out of the non-terminal. We notate
LCFRS with the syntax of Simple Range Concate-
nation Grammars (SRCG) (Boullier, 1998), a for-
malism equivalent to LCFRS.

An LCFRS1 (Vijay-Shanker et al., 1987; Seki et
al., 1991) is a tuple G = (N,T, V, P, S) where N

1Note that for purposes of exposition, we limit ourselves to
ε-free LCFRS.

1250



is a finite set of non-terminals with a function dim:
N → N determining the fan-out of each A ∈ N ; T
and V are disjoint finite sets of terminals and vari-
ables; S ∈ N is the start symbol with dim(S) = 1.
P is a finite set of rewriting rules with rank m ≥

0. All γ ∈ P have the form

A(α0, . . . , αdim(A)−1)→ A1(X(1)0 , . . . , X(1)dim(A1)−1)
· · ·Am(X(m)0 , . . . , X(m)dim(Am)−1)

where A,A1, . . . , Am ∈ N , X(l)j ∈ V for 1 ≤
l ≤ m, 0 ≤ j < dim(Ai) and αi ∈ (V ∪ T )+ for
0 ≤ i < dim(A). All αi and X(l)j are called argu-
ments (or sometimes components); the elements in
αi are called argument elements. Aγ is the set of
all argument elements of γ. Variable occurrences in
the arguments of the non-terminals of γ are ordered
by a strict total order ≺. For all X1, X2 ∈ V oc-
curring in arguments of a non-terminal of γ, it holds
that X1 ≺ X2 iff either X1 precedes X2 in an argu-
ment of the non-terminal or the argument X1 occurs
in precedes the argument X2 occurs in.

For all γ ∈ P , every variable X occurring in γ
occurs exactly once in the left-hand side (LHS) and
exactly once in the right-hand side (RHS). Further-
more, if for two variables X1, X2 ∈ V , it holds that
X1 ≺ X2 on the RHS, then also X1 ≺ X2 on the
LHS. The rank of G is the maximal rank of any of
its rules, its fan-out is the maximal fan-out of any of
its non-terminals.

We use the following additional notation: For a
rule γ ∈ P , lhs(γ) gives the LHS non-terminal;
lhs(γ, i) gives the ith argument of the LHS and
lhs(γ, i, j) its jth symbol; rhs(γ, k) gives the kth
RHS non-terminal; and rhs(γ, k, l) gives the lth
component of the kth RHS element (starting with in-
dex 0 in all four cases). These function have value⊥
whenever there is no such element. Furthermore, in
the sense of dotted productions, we define for each
γ ∈ P a set of symbols denoting computation points
of γ, Cγ = {γi.j | 0 ≤ i < dimA, 0 ≤ j ≤ |αi|}, as
well as the set C = ⋃γ∈P Cγ .

A non-terminal A ∈ N can be instantiated
w.r.t. an input string w1 · · ·w|w| and a rule γ ∈ P
with lhs(γ) = A. An instantiation maps all argu-
ment elements of γ to spans ofw ((i−1, j)w denotes
the span wi · · ·wj , 1 ≤ i ≤ j ≤ n). All instantia-
tions are given by a function σ : Aγ → N×N where

α : S(xy)→ A(x, y) γ : A(a, b)→ ε
β : A(ax, ya)→ A(x, y)

Figure 1: LCFRS for {anaban |n ≥ 0}

for all x, y ∈ Aγ with x 6= y, σ(x) = (i, j)w and
σ(y) = (k, l)w it holds that i, k ≥ 0; j, l ≤ |w|; if
x (y) is a terminal, then j = i + 1 (l = k + 1),
otherwise j > i (k > l). Iff x ≺ y in γ, then
j ≤ k. A derivation rewrites strings of instantiated
non-terminals, i.e., given an instantiated clause, the
instantiated LHS non-terminal may be replaced with
the sequence of instantiated RHS terminals. The lan-
guage of the grammar is the set of strings which can
be reduced to the empty word, starting with S in-
stantiated to the input string.

See figure 1 for a sample LCFRS.

2.2 Thread Automata

Thread automata (TA) (Villemonte de la Clergerie,
2002) are a generic automaton model which can be
parametrized to recognize different mildly context-
sensitive languages. The TA for LCFRS (LCFRS-
TA) implements a prefix-valid top-down incremen-
tal parsing strategy similar to the ones of Kallmeyer
and Maier (2009) and Burden and Ljunglöf (2005).

An LCFRS-TA for some LCFRS G =
(N,T, V, P, S) works as follows. The process-
ing of a single rule is handled by a single thread
which will traverse the LHS arguments of the
rule. A thread is given by a pair p : X , where
p ∈ {1, . . . ,m}∗ with m the rank of G is the
address, and X ∈ N ∪ {ret} ∪ C where ret /∈ N
is the content of the thread. An automaton state
is given by a tuple 〈i, p, T 〉 where T is a set of
threads, the thread store, p is the address of the
active thread, and i ≥ 0 indicates that i tokens have
been recognized. We introduce a new start symbol
S′ /∈ N that expands to S and use 〈0, ε, {ε : S′}〉 as
start state.

The specific TA for a given LCFRS
G = (N,T, V, P, S) can be defined as tuple
〈N ′, T, S′, ret , δ,Θ〉 with N ′ = N ∪ C ∪ {S′, ret};
δ is a function from C to {1, . . . ,m} ∪ {⊥}
such that δ(γk,i) = j if there is a l such that
lhs(γ, k, i) = rhs(γ, j − 1, l), and δ(γk,i) = ⊥ if
lhs(γ, k, i) ∈ T ∪ {⊥} (intuitively, a δ value j tells
us that the next symbol to process is a variable that

1251



Call: S′ → [S′]S α0,0 → [α0,0]A β0,1 → [β0,1]A
Predict: S → α0,0 A→ β0,0 A→ γ0,0
Scan: β0,0

a→ β0,1 β1,1 a→ β1,2 γ0,0 a→ γ0,1 γ1,0 b→ γ1,1
Publish: α0,2 → ret β1,2 → ret γ1,1 → ret
Suspend: [α0,1]ret → α0,2 [β1,0]ret → β1,1

[α0,0]β0,2 → α0,1[β0,2] [α0,0]γ0,1 → α0,1[γ0,1] [β0,1]β0,2 → β0,2[β0,2] [β0,1]γ0,1 → β0,2[γ0,1]
Resume: α0,1[β0,2]→ [α0,1]β1,0 α0,1[γ0,1]→ [α0,1]γ1,0 β1,0[β0,2]→ [β1,0]β1,0 β1,0[γ0,1]→ [β1,0]γ1,0

Figure 2: TA transitions for the LCFRS from figure 1

is an argument of the jth RHS non-terminal); and
Θ is a finite set of transitions. Every transition has
the form α a→ β with a ∈ T ∪ {ε} and they roughly
indicate that in the thread store, α can be replaced
with β while scanning a. Square brackets in α and
β indicate parts that do not belong to the active
thread. This will be made more precise below. Θ
contains the following transitions (see figure 2):
• Call transitions start a new thread, either for

the start symbol or for a daughter non-terminal.
They move down in the parse tree.
S′ → [S′]S (initial call), γk,i → [γk,i]A if A =
rhs(γ, j − 1) and lhs(γ, k, i) = rhs(γ, j − 1, 0)
where j = δ(γk,i).
• Predict transitions predict a new rule for a non-

terminal A: A→ γ0,0 if A = lhs(γ).
• Scan reads a LHS terminal while scanning the

next input symbol:

γk,i
lhs(γ,k,i)→ γk,i+1 if lhs(γ, k, i) ∈ T .

• Publish marks the completion of a production,
i.e., its full recognition:
γk,j → ret if dim(lhs(γ)) = k + 1 and j =
|lhs(γ, k)|.
• Suspend suspends a daughter thread and re-

sumes the parent. i.e., moves up in the parse tree.
There are two cases:
(i) The daughter is completely recognized:

[γk,i]ret → γk,i+1 if lhs(γ, k, i) =
rhs(γ, δ(γk,i)−1, dim(rhs(δ(γk,i)−1))−1).

(ii) The daughter is not yet completely recog-
nized, we have only finished one of its
components: [γk,i]βl,j → γk,i+1[βl,j ] if
dim(lhs(β)) > l + 1, |lhs(β, l)| = j,
lhs(γ, k, i) = rhs(γ, δ(γk,i) − 1, l) and
rhs(γ, δ(γk,i)− 1) = lhs(β).

• Resume resumes an already present daughter
thread, i.e., moves down into some daughter that

has already been partly recognized.
γk,i[βl,j ] → [γk,i]βl+1,0 if lhs(γ, k, i) =
rhs(γ, δ(γk,i)− 1, l + 1), rhs(γ, δ(γk,i)− 1) =
lhs(β) and βl,j+1 /∈ C.

This is not exactly the TA for LCFRS proposed in
Villemonte de la Clergerie (2002) but rather the one
from Kallmeyer (2010), which is close to the Earley
parser from Burden and Ljunglöf (2005).

The set of configurations for a given inputw ∈ T ∗
is then defined by the deduction rules in figure 3 (the
use of set union S1 ∪ S2 in these rules assumes that
S1 ∩ S2 = ∅). The accepting state of the automaton
for some input w is 〈|w|, 1, {ε : S′, 1 : ret}〉.

2.3 LR Parsing

In an LR parser, the parser actions are guided by
an automaton, resp. a parse table which is com-
piled offline. Consider the context-free case. An LR
parser for CFG is a guided shift-reduce parser, in
which we first build the LR automaton. Its states are
sets of dotted productions closed under prediction,
and its transitions correspond to having recognized
a part of the input, e.g., to moving the dot over a
RHS element after having scanned a terminal or rec-
ognized a non-terminal. Given an automaton with n
states, we build the parse table with n rows. Each
row i, 0 ≤ i < n, describes the possible parser ac-
tions associated with the state qi, i.e., for each state
and each possible shift or reduce operation, it tells
us in which state to go after the operation.

3 LR for LCFRS

3.1 Intuition

The states in the automaton are predict and resume
closures of TA thread stores. In order to keep them
finite, we allow the addresses to be regular expres-
sions. A configuration of the parser consists of a

1252



Initial configuration: 〈0, ε, {ε : S′}〉 Initial call:
〈0, ε, {ε : S′}〉

〈0, 1, {ε : S′, 1 : S}〉
Further calls:

〈i, p,S ∪ p : γk,i〉
〈i, pj,S ∪ p : γk,i ∪ pj : A〉

γk,i → [γk,i]A ∈ Θ,
A ∈ N, δ(γk,i) = j + 1 Predict:

〈i, p,S ∪ p : A〉
〈i, p,S ∪ p : γ0,0〉

A ∈ N,
A→ γ1,0 ∈ Θ

Scan:
〈j, p,S ∪ p : γk,i〉

〈j + 1, p,S ∪ p : γk,i+1〉 γk,i
wj+1→ γk,i+1 ∈ Θ Publish:

〈i, p,S ∪ {p : γk,i}〉
〈i, p,S ∪ {p : ret}〉 γk,j → ret ∈ Θ

Suspend 1:
〈i, pj,S ∪ {p : γk,i, pj : ret}〉
〈i, p,S ∪ {p : γk,i+1}〉 [γk,i]ret → γk,i+1 ∈ Θ

Suspend 2:
〈i, pj,S ∪ {p : γk,i, pj : βl,m}〉
〈i, p,S ∪ {p : γk,i+1, pj : βl,m}〉 [γk,i]βl,m → γk,i+1[βl,m] ∈ Θ

Resume:
〈i, p,S ∪ {p : γk,i, pδ(γk,i) : βl,j}〉

〈i, pδ(γk,i),S ∪ {p : γk,i, pδ(γk,i) : βl+1,0}〉 γk,i[βl,j ]→ [γk,i]βl+1,0 ∈ Θ

Figure 3: Deduction rules for TA configurations

stack, a set of completed components and the re-
maining input. The completed components are of
the form p : γi where p is an address and γi
the component of a rule. The stack has the form
Γ1x1Γ2 . . . xn−1Γn where Γi is an address followed
by a state and xi ∈ T ∪ {Ak |A ∈ N, 1 ≤ k ≤
dim(A)}.

Shift: Whenever we have p : q on top of the stack
and an edge from q to q′ labeled with the next input
symbol and an address p′, we add the input symbol
followed by pp′ : q′ to the stack.

Suspend: Whenever the top of the stack is p1 : q
such that there is a γi−1,k ∈ q with k = |lhs(γ, i −
1)| and i < dim(γ), we can suspend. If i = 1,
we add p1 : γi to the set of completed components
and we remove |lhs(γ, i)| terminals/component non-
terminals and their preceding states from the stack.
If i ≥ 1, we check whether there is a p2 : γi−1 in the
set of completed components such that the intersec-
tion L(p1) ∩ L(p2) is not empty.2 We then remove
p2 : γi−1 from the set of complete components and
we add p : γi to it where p is a regular expression
denoting L(p1) ∩ L(p2). Suppose the topmost state
on the stack is now p′ : q′. We then have to follow
the edge leading from q′ to some q′′ labeled Ai : p′′

where A = lhs(γ). This means that we push Ai
followed by p′p′′ : q′′ on the stack.

2Note that the corresponding finite state automata can be de-
terministic; in this case the intersection is quadratic in the size
of the two automata.
In LCFRS without left recursion in any of the components, the
intersection is trivial since the regular expressions denote only
a single path each.

Reduce: Whenever there is a γi−1,k in our current
state with k = |lhs(γ, i − 1)| and i = dim(γ), we
can reduce, which is like suspend except that noth-
ing is added to the set of completed components.

3.2 Automaton and parse table construction

The states of the LR-automaton are sets of pairs p :
X where p is a regular expression over {1, . . . ,m},
m the rank of G, and X ∈ C ∪ {S′}. They represent
predict and resume closures.The predict/resume clo-
sure q of some set q is described by the deduction
rules in figure 4. This closure is not always finite.

ε : S′

1 : α0,0
lhs(α) = S

p : γi,j
pk : γ′l,0

lhs(γ, i, j) = rhs(γ, k − 1, l),
rhs(γ, k) = lhs(γ′)

Figure 4: Predict/resume closure

However, if it is not, we obtain a set of items that
can be represented by a finite set of pairs r : γi,j
plus eventually ε : S′ such that r is a regular ex-
pression denoting a set of possible addresses. As an
example for such a case, see q3 in figure 5.

The reason why we can represent these closures
by finite sets using regular expressions for paths
is the following: There is a finite number of pos-
sible elements γi,j . For each of these, the set
of possible addresses it might be combined with
in a state that is the closure of {ε : X1, ε :
X2, . . . , ε : Xn} is generated by the CFG 〈C∪{S′}∪
{Snew}, {1, . . . ,m}, P, Snew〉 with Snew → Xi ∈
P for all 1 ≤ i ≤ n, X → Y k ∈ P for all in-

1253



stances p:Xpk:Y of deduction rules and γi,j → ε. This
is a regular grammar, its string language can thus be
characterized by a regular expression.

The construction of the set of states starts with
q0 = {ε : S′}. For every state q, every non-
terminal A and every 1 ≤ i ≤ dim(A), we define
read(q, Ai, p) = {ε : γj,k+1 | p : γj,k ∈ q and there
is some l such that rhs(γ, l) = A and lhs(γ, j, k) =
rhs(γ, l, i−1)} and read(q, Ai, p) = read(q, Ai, p).
Similarly, for every such q and every a ∈ T , we de-
fine read(q, a, p) = {ε : γj,k+1 | p : γj,k ∈ q and
lhs(γ, j, k) = a} and read(q, a, p) = read(q, a, p).
The set of states of our automaton is then the closure
of {q0} under the application of the read-functions.
The edges in our automaton correspond to read-
transitions, where each edge is labeled with the cor-
responding pair Ai, p or a, p respectively. The au-
tomaton we obtain for the grammar in figure 1 is
shown in figure 5. The number of possible states

ε : S′, 1 : α0,0
11 : β0,0, 11 : γ0,0

q0

ε : β0,1, ε : γ0,1
1 : β0,0, 1 : γ0,0

q1

ε : β0,2

q2

ε : α0,1
1+ : β1,0, 1+ : γ1,0

q3

ε : β1,1

q4

ε : β1,2

q5

ε : γ1,1q6 ε : α0,2

q7

ε : S′•
q8

a, 11

a, 1

A1, ε

A1, 1

A2, 1+

b, 1+ A2, ε
a, ε

S1, ε

Figure 5: The automaton

is necessarily finite since each state is the closure
of some set containing only items with address ε.
There are only finitely many such sets.

In the parse table, our operations are s(p, q) for
shifting some terminal a followed by the old ad-
dress concatenated with p and state q and r(α, i)
for reducing the ith component of rule α. The
two reduce operations can be distinguished by the
component indices. Furthermore, the goto-part of
the table tells where to go when traversing a com-
ponent edge and which address to add then. The
parse table can be read off the automaton as fol-
lows: action(q, a) = s(p, q′) iff read(q, a, p) = q′;
action(q,−) = r(γ, i) iff there is some p : γi,k ∈ q
such that k = |lhs(γ, i)|. Concerning the goto
part of the table, we have goto(q, Ai) = 〈p, q′〉 iff
read(q, Ai, p) = q′. Figure 6 shows the parse table

a b A1 A2 S1
0 s(11, 1) 〈1, 3〉 〈ε, 8〉
1 s(1, 1) r(γ, 1) 〈ε, 2〉
2 r(β, 1)
3 s(1+, 6) 〈1+, 4〉,

〈ε, 7〉
4 s(ε, 5)
5 r(β, 2)
6 r(γ, 2)
7 r(α, 1)
8 acc

Figure 6: The parse table

stack completed input operation
ε:q0 [ ] aaba initial state
ε:q0 a 11:q1 [ ] aba shift a,11
ε:q0 a 11:q1 a 111:q1 [ ] ba shift a,1
ε:q0 a 11:q1 A1 11:q2 [111:γ1] ba suspend γ0,1
ε:q0 A1 1:q3 [111:γ1,11:β1] ba suspend β0,2
ε:q0 A1 1:q3 b 11+:q6 [111:γ1,11:β1]a shift b,1+

ε:q0 A1 1:q3 A2 11+:q4 [11:β1] a reduce γ1,1
ε:q0 A1 1:q3 A2 11+:q4 a 11+:q5 [11:β1] ε shift a,ε
ε:q0 A1 1:q3 A2 1:q4 [ ] ε reduce β1,2
ε:q0 S1 ε:q8 [ ] ε reduce α0,2

Figure 7: Sample run with w = aaba

for our example.

3.3 Parsing
We run the automaton with 〈ε : q0, [ ], w〉 and in-
put w = aaba. The trace is shown in figure 7. We
start in q0, and shift two as, which leads to q1. We
have then fully recognized the first components of γ
and β: We suspend them and keep them in the set of
completed components, which takes us to q3. Shift-
ing the b takes us to q6, from where we can reduce,
which finally takes us to q4. From there, we can shift
the remaining a (to q5), with which we have fully
recognized β. We can now reduce both β and with
that, α, which takes us to the accepting state q8.

4 Conclusion

We presented the first LR style algorithm for LCFRS
parsing. It offers a convenient factorization of pre-
dict/resume operations. We are currently exploring
the possibility to use it in data-driven parsing.

Acknowledgments

The work presented in this paper was partly funded
by the German Research Foundation (DFG). We
wish to thank three anonymous reviewers for their
valuable comments.

1254



References
Krasimir Angelov and Peter Ljunglöf. 2014. Fast statis-

tical parsing with parallel multiple context-free gram-
mars. In Proceedings of the 14th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 368–376, Gothenburg, Sweden.

Jan A. Botha and Phil Blunsom. 2013. Adaptor gram-
mars for learning non-concatenative morphology. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages 345–
356, Seattle, WA.

Pierre Boullier. 1998. Proposal for a Natural Language
Processing syntactic backbone. Research Report
3342, INRIA-Rocquencourt, Rocquencourt, France.

Håkan Burden and Peter Ljunglöf. 2005. Parsing linear
context-free rewriting systems. In Proceedings of the
Ninth International Workshop on Parsing Technology,
pages 11–17, Vancouver, BC.

Miriam Kaeshammer. 2013. Synchronous linear
context-free rewriting systems for machine translation.
In Proceedings of the Seventh Workshop on Syntax, Se-
mantics and Structure in Statistical Translation, pages
68–77, Atlanta, GA.

Laura Kallmeyer and Wolfgang Maier. 2009. An
incremental Earley parser for simple range con-
catenation grammar. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT’09), pages 61–64, Paris, France.

Laura Kallmeyer and Wolfgang Maier. 2013. Data-
driven parsing using probabilistic linear context-
free rewriting systems. Computational Linguistics,
39(1):87–119.

Laura Kallmeyer. 2010. Parsing beyond Context-Free
Grammar. Springer, Heidelberg.

Donald E. Knuth. 1965. On the translation of languages
from left to right. Information and Control, 8(6):607–
639, July.

Marco Kuhlmann. 2013. Mildly non-projective
dependency grammar. Computational Linguistics,
39(2):355–387.

Wolfgang Maier and Laura Kallmeyer. 2010. Discon-
tinuity and non-projectivity: Using mildly context-
sensitive formalisms for data-driven parsing. In
Proceedings of the Tenth International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+10), pages 119–126, New Haven, CT.

Wolfgang Maier and Timm Lichte. 2011. Characteriz-
ing discontinuity in constituent treebanks. In Formal
Grammar. 14th International Conference, FG 2009.
Bordeaux, France, July 25-26, 2009. Revised Selected
Papers, volume 5591 of Lecture Notes in Artificial In-
telligence, pages 167–182, Berlin, Heidelberg, New
York. Springer-Verlag.

Mark-Jan Nederhof. 1998. An alternative LR algorithm
for TAGs. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and 17th International Conference on Computational
Linguistics, volume 1, pages 946–952, Montreal, QC.

Carlos A. Prolo. 2003. LR Parsing for Tree Adjoin-
ing Grammars and its Application to Corpus-based
Natural Language Parsing. Ph.D. thesis, Department
of Computer and Information Science, University of
Pennsylvania, Philadelphia, PA.

Aarne Ranta. 2011. Grammatical Framework: Pro-
gramming with Multilingual Grammars. CSLI Pub-
lications, Stanford.

Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88(2):191–229.

Anders Søgaard. 2008. Range concatenation grammars
for translation. In The 22nd International Conference
on Computational Linguistics (COLING), pages 103–
106, Manchester, England.

Masaru Tomita. 1984. LR parsers for natural lan-
guages. In Proceedings of COLING 1984: The 10th
International Conference on Computational Linguis-
tics, pages 354–357, Stanford University.

Andreas van Cranenburgh. 2012. Efficient parsing with
linear context-free rewriting systems. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics, pages
460–470, Avignon, France.

K. Vijay-Shanker, David Weir, and Aravind K. Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of the 25th Annual
Meeting of the Association for Computational Linguis-
tics, pages 104–111, Stanford, CA.

Éric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata. In
Proceedings of COLING 2002: The 19th International
Conference on Computational Linguistics, Taipei, Tai-
wan.

David Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univer-
sity of Pennsylviania, Philadelphia, PA.

1255


