



















































Open-Domain Why-Question Answering with Adversarial Learning to Encode Answer Texts


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4227â€“4237
Florence, Italy, July 28 - August 2, 2019. cÂ©2019 Association for Computational Linguistics

4227

Open-Domain Why-Question Answering with Adversarial Learning to
Encode Answer Texts

Jong-Hoon OhÂ§ Kazuma KadowakiÂ§â€¡ Julien KloetzerÂ§ Ryu IidaÂ§Â¶ Kentaro TorisawaÂ§Â¶

Data-driven Intelligent System Research Center (DIRECT),
National Institute of Information and Communications Technology (NICT)Â§

Advanced Technology Laboratory, The Japan Research Institute, Limited (JRI)â€¡

Graduate School of Science and Technology, NAISTÂ¶

{rovellia, kadowaki, julien, ryu.iida, torisawa}@nict.go.jp

Abstract

In this paper, we propose a method for why-
question answering (why-QA) that uses an ad-
versarial learning framework. Existing why-
QA methods retrieve answer passages that
usually consist of several sentences. These
multi-sentence passages contain not only the
reason sought by a why-question and its con-
nection to the why-question, but also redun-
dant and/or unrelated parts. We use our
proposed Adversarial networks for Generat-
ing compact-answer Representation (AGR) to
generate from a passage a vector representa-
tion of the non-redundant reason sought by
a why-question and exploit the representa-
tion for judging whether the passage actu-
ally answers the why-question. Through a se-
ries of experiments using Japanese why-QA
datasets, we show that these representations
improve the performance of our why-QA neu-
ral model as well as that of a BERT-based
why-QA model. We show that they also im-
prove a state-of-the-art distantly supervised
open-domain QA (DS-QA) method on pub-
licly available English datasets, even though
the target task is not a why-QA.

1 Introduction

Why-question answering (why-QA) tasks retrieve
from a text archive answers to such why-questions
as â€œWhy does honey last such a long time?â€ Previ-
ous why-QA methods retrieve from a text archive
answer passages, each of which consists of sev-
eral sentences, like A in Table 1 (Girju, 2003;
Higashinaka and Isozaki, 2008; Oh et al., 2012,
2013, 2016, 2017; dos Santos et al., 2016; Sharp
et al., 2016; Tan et al., 2016; Verberne et al., 2011),
and then determine whether the passages answer
the question. A proper answer passage must con-
tain (1) a paraphrase of the why-question (e.g., the
underlined texts in Table 1) and (2) the reasons
or the causes (e.g., the bold texts in Table 1) of

Q Why does honey last a long time?
A While excavating Egyptâ€™s pyramids, archaeologists

have found pots of honey in an ancient tomb: thou-
sands of years old and still preserved. Honey can
last a long time due to three special properties.
Its average pH is 3.9, which is quite acidic. Such
high level of acidity is certainly hostile and hin-
ders the growth of many microbes. Though honey
contains around 17â€“18% water, its water activity is
too low to support the growth of microbes. More-
over honey contains hydrogen peroxide, which is
thought to help prevent the growth of microbes in
honey. Despite these properties, honey can be con-
taminated under certain circumstances.

C Because its acidity, low water activity, and hydrogen
peroxide together hinder the growth of microbes.

Table 1: Answer passage A to why-question Q and its
compact answer C

the events described in the why-question, both of
which are often written in multiple non-adjacent
sentences. This multi-sentenceness implies that
the answer passages often contain redundant parts
that are not directly related to a why-question or its
reason/cause and whose presence complicates the
why-QA task. Highly accurate why-QA methods
should be able to find the exact reason sought by a
why-question in an answer passage without being
distracted by redundancy.

In this paper, we train a neural network (NN)
to generate, from an answer passage, a vector rep-
resentation of the non-redundant reason asked by
a why-question, and exploit the generated vec-
tor representation as evidence for judging whether
the passage answers the why-question. This idea
was inspired by Ishida et al. (2018), who used
a seq2seq model to automatically generate such
compact answers as C in Table 1 from the an-
swer passages retrieved by a why-QA method.
Compact answers are sentences or phrases that ex-
press the reasons for a given why-question with-
out redundancy. If we can use such automatically



4228

generated compact-answers to support a why-QA
method in finding the exact reason of a why-
question in these passages, why-QA accuracy may
be improved. We actually tried this idea in a
preliminary study in which we generated a com-
pact answer from a given question-passage pair by
using the compact-answer generation method of
Iida et al. (2019) and used the generated compact-
answer along with the given question-passage pair
to find proper answer passages. However, we were
disappointed by the small performance improve-
ment, as shown in our experimental results.

We chose an alternative approach. Instead of
generating a compact answer of an answer pas-
sage as word sequences, we devised a model to
generate a compact-answer representation, which
is a vector representation for a compact answer,
from an answer passage. Inspired by the genera-
tive adversarial network (GAN) approach (Good-
fellow et al., 2014), we developed an adversarial
network called the Adversarial networks for Gen-
erating compact-answer Representation (AGR).
Like the original GAN, an AGR is composed of a
generator and a discriminator: the generator net-
work is trained for generating (from answer pas-
sages) fake representations to make it hard for
the discriminator network to distinguish these fake
representations from the true representations de-
rived from manually created compact-answers.

We combined the generator network in the AGR
with an extension of the state-of-the-art why-QA
method (Oh et al., 2017). Our evaluation against
a Japanese open-domain why-QA dataset, which
was created using general web texts as a source of
answer passages, revealed that the generator net-
work significantly improved the accuracy of the
top-ranked answer passages and that the combi-
nation significantly outperformed several strong
baselines, including a combination of a genera-
tor network and a BERT model (Devlin et al.,
2019). This combination also outperformed a
vanilla BERT model, suggesting that the genera-
tor network in our AGR may be effective even if it
is combined with many types of NN architectures.
Another interesting point is that the performance
improved even when we replaced, as the inputs to
AGR, the word embedding vectors that represent
an answer passage, with a random vector. This
observation warrants further exploration in our fu-
ture work.

Finally, we applied our AGR to a distantly su-

rq

Question 
encoder

Question

rp

Passage 
encoder

Passage

Logistic regression
+softmax+dropout

F in AGR
(Pretrained)

rc

Correct
Answer

Passage 
representation

Compact-answer 
representation

Question 
representation

Incorrect
Answer

(a) Our why-QA model

F: Fake-
representation 

generator 

Q
ue

st
io

n
Pa

ss
ag

e 
Co

m
pa

ct
 

an
sw

er R: Real-
representation 

generator 

Real-
representation

D: 
Discriminator

Fake-
representation

Real

Fake

ð‘ž

ð‘

ð‘ ð¹(ð‘|ð‘ž)

ð‘…(ð‘|ð‘ž)

(b) AGR and its three subnetworks F , R, and D

Figure 1: System architecture

pervised open-domain QA (DS-QA) task (Chen
et al., 2017), which is an extension of a machine-
reading task, to check whether it is applicable
to other datasets. We combined our generator
network with a state-of-the-art DS-QA method,
OpenQA (Lin et al., 2018), and used a generated
compact-answer representation from a given pas-
sage as evidence to 1) select relevant passages
from the retrieved ones and 2) find an answer from
the selected passages. Although the task was not
our initial target (why-QA) and the answers in the
DS-QA task were considerably shorter than those
in the why-QA, experiments using three pub-
licly available datasets (Quasar-T (Dhingra et al.,
2017), SearchQA (Dunn et al., 2017), and Triv-
iaQA (Joshi et al., 2017)) revealed that the gen-
erator network improved the performance in most
cases. This suggests that AGR may be applicable
to many QA-like tasks.

2 Why-QA Model

Figure 1 illustrates the architecture of our why-QA
model and the AGR. Our why-QA model com-
putes the probability that a given answer passage
describes a proper answer to a given why-question



4229

using the representations of a question, an answer
passage, and a compact answer. The probability
(the why-QA modelâ€™s final output) is computed
from these representations by our answer selec-
tion module, which is a logistic regression layer
with dropout and softmax output.

The representations of why-questions and an-
swer passages are generated by Convolutional
Neural Networks (CNNs) (Collobert et al., 2011;
LeCun et al., 1998) that (1) are augmented by
two types of attention mechanisms, similarity-
attention (dos Santos et al., 2016; Tan et al.,
2016) and causality-attention (Oh et al., 2017),
and (2) are given two types of word embed-
dings, general word embeddings computed by
word2vec (Mikolov et al., 2013) using Wikipedia
and causal word embeddings (Sharp et al., 2016).
Note that in computing a questionâ€™s representa-
tion, the answer passage is given to the question
encoder to guide the computation. Likewise the
passage encoder is given the question and the rep-
resentation of the compact answer. We repre-
sent these information flows with dotted arrows in
Fig. 1(a).

The representations of compact answers are
created by a generator network called a fake-
representation generator (F in Fig. 1(a)), which
is pre-trained in an adversarial learning manner
(Fig. 1(b)). During the training of the whole why-
QA model, the generatorâ€™s parameters are fixed
and no further fine-tuning is conducted.

In the next section, we describe our main contri-
bution: the AGR and the fake-representation gen-
erator. The entire why-QA model can be seen
as an extension of the state-of-the-art why-QA
method (Oh et al., 2017). Its details are described
in Section A of the supplementary materials.

3 Adversarial Networks for Generating
Compact-answer Representation

3.1 Adversarial training

Generative adversarial networks (GANs) (Good-
fellow et al., 2014) are a framework for training
generative models based on game theory. Unlike
the original GANs, which generate such data sam-
ples as images and compact answers from noise,
our AGR generates useful vector representations
from meaningful text passages. To clarify the dif-
ference, we explain our AGR with three subnet-
works: two generators, F and R, and a discrimina-
tor, D, as in Fig. 1(b). Generator F takes as input

passage p drawn from prior passage distribution dp
and outputs vector pÌ„ as a fake representation of a
compact answer. We call F a fake-representation
generator. R, which we call a real-representation
generator, is given sample c taken from manu-
ally created compact-answers and provides vector
cÌ„ as a real-representation of the sampled compact-
answer. Discriminator D has to distinguish fake-
representation pÌ„ from real-representation cÌ„ of a
compact answer. These three networks play an ad-
versarial minimax game; fake-representation gen-
erator F creates a fake compact-answer represen-
tation that is hard for the discriminator to distin-
guish from the representations of manually created
compact-answers, and discriminator D and gener-
ator R simultaneously try to avoid being duped by
generator F . These processes should allow gener-
ator F to learn how to generate a representation of
a proper compact-answer from an answer passage.

In addition, since passage p and compact an-
swer c are dependent on question q, the generation
of the compact-answer representations by F and
R is conditioned by question q, like in the con-
ditional GANs (Mirza and Osindero, 2014). We
trained our AGR with the following minimax ob-
jective:

min
F

max
D,R

V (D,F,R) = Ecâˆ¼dc(c)[logD(R(c|q))]

+ Epâˆ¼dp(p)[log(1âˆ’D(F (p|q))].

3.2 Generator and discriminator
In our implementation, both F and R are networks
with identical structure called Encoder. They are
defined as follows, where p, c, and q are re-
spectively an answer passage, a manually created
compact-answer, and a why-question:

F (p|q) = Encoder(p; Î¸F , q)
R(c|q) = Encoder(c; Î¸R, q)

Here Î¸F and Î¸R represent the parameters of net-
works F and R. The details of Encoder are de-
scribed below.

Discriminator D(r) takes as input r, either the
output of F (p|q) or that of R(c|q), and computes
the probability that given representation r comes
from a real compact-answer using a feedforward
network with two hidden layers (100 nodes in the
first layer and 50 in the second layer) and a lo-
gistic regression layer on top of the hidden layers.
We used sigmoid outputs by the logistic regression
layer as the output probability.



4230

CNNs

Attention

Representation rt

ð­$%% : Attention-weighted 
word embedding

Word
embedding

Word
embedding

q t

t: passage or 
compact-answer 

q: question

Encoder

Figure 2: Architecture of generators F and R

3.3 Encoder
Figure 2 illustrates the architecture shared
by our fake-representation generator F and
real-representation generator R, namely,
Encoder(t; Î¸, q), where Î¸ is a set of parame-
ters, q is a why-question, and t is either an answer
passage or a manually created compact-answer.
Encoder(t; Î¸, q) first represents question q and
passage/compact-answer t with pre-trained
word embeddings, which are supplemented
with attention mechanisms. The resulting
attention-weighted word embeddings are given
to convolutional neural networks (CNNs) that
generate a single feature vector, which is an
output/value of Encoder(t; Î¸, q).

In the following, we give an overview of the
word embeddings, the attention mechanisms, and
the CNNs used in Encoder(t; Î¸, q). All of these
techniques were proposed by previous works. Fur-
ther details are given in Section B of the supple-
mentary materials.

3.3.1 Word embeddings
The pre-trained word embeddings used in
Encoder(t; Î¸, q) were obtained by concatenating
two types of d-dimensional word embeddings
(d = 300 in this work): general word embeddings
and causal word embeddings.

General word embeddings are widely used em-
bedding vectors (300 dimensions) that were pre-
trained for about 1.65 million words by applying
word2vec (Mikolov et al., 2013) to about 35 mil-
lion sentences from Japanese Wikipedia (January
2015 version).

Causal word embeddings (Sharp et al., 2016)
were proposed for representing the causal asso-
ciations between words. Sharp et al. (2016) cre-

ated a set of cause-effect word pairs by paring
each content word in a cause part with each con-
tent word in an effect part of the same causal-
ity expression, such as â€œVolcanoes erupt because
magma pushes through vents and fissures.â€ In this
work, we extracted 100 million causality expres-
sions from 4-billion Japanese web pages using the
causality recognizer of Oh et al. (2013). Then,
following Sharp et al. (2016), we trained 300-
dimensional causal word embeddings for about
1.85 million words by applying the generalized
skip-gram embedding model of Levy and Gold-
berg (2014) to the causality expressions.

3.3.2 Attention
We also applied two types of attention mecha-
nisms to the above word embeddings. The first
type of attention, similarity-attention (dos Santos
et al., 2016; Tan et al., 2016), was used for esti-
mating the similarities between words in question
q and those in passage/compact-answers t and fo-
cusing on the attended words as those that directly
indicate the connection between the question and
passage/compact-answers. Basically, the mecha-
nism computes the cosine similarity between the
embeddings of the words in q and t, and uses it
for producing attention feature vector asj âˆˆ R for
word tj in passage/compact-answers.

Another attention mechanism, causality-
attention (Oh et al., 2017), was proposed for
focusing on passage words causally associated
with question words. They used normalized
point-wise mutual information to measure the
strength of the causal associations with the
causality expressions used for creating the causal
embeddings. The scores are used for producing
causality-attention feature vector acj for word tj .

Finally, we form two attention feature vectors,
as = [as1, Â· Â· Â· , as|t|] and a

c = [ac1, Â· Â· Â· , ac|t|], con-
catenate them into a = [as; ac] âˆˆ R2Ã—|t|, and pro-
duce attention-weighted word embedding tatt of
given text t, which is either an answer passage or
a compact answer:

tatt = ReLU(Wtt + Waa)

where Wt âˆˆ R2dÃ—2d and Wa âˆˆ R2dÃ—2 are train-
able parameters, t is the representation of text t,
and ReLU represents the rectified linear units.

3.3.3 CNNs
tatt is given to CNNs to generate final represen-
tation rt of a given passage/compact-answer t.



4231

The CNNs resembles those in Kim (2014). Con-
volutions are performed over the word embed-
dings using both multiple filters and multiple fil-
ter windows (e.g., sliding over 1, 2, or 3 word
windows at a time and 100 filters for each win-
dow). An average pooling operation is applied to
the convolution results to generate representation
rt, which is the output/value of Encoder(t; Î¸, q);
rt = Encoder(t; Î¸, q). In our experiments, we set
the dimension of representation rt to 300.

4 Why-QA Experiments

4.1 Datasets

We used three datasets, WhySet, CmpAns, and
AddTr, for our why-QA experiments. WhySet
and AddTr were used for training and evaluating
the why-QA models, while CmpAns was used for
training AGR.

The WhySet dataset, which was used in
previous works for why-QA (Oh et al., 2012,
2013, 2016, 2017), is composed of 850 Japanese
why-questions and their top-20 answer passages
(17,000 question-passage pairs) obtained from
600 million Japanese web pages using the answer-
retrieval method of Murata et al. (2007), where
a question-passage pair is composed of a single-
sentence question and a five-sentence passage.
The label of each question-answer pair (i.e., cor-
rect answer and incorrect answer) was manually
annotated (See Oh et al. (2013) for more de-
tails). Oh et al. (2013) selected 10,000 question-
passage pairs as training and test data in 10-fold
cross-validation (9,000 for training and 1,000 for
testing) and used the remainder (7,000 question-
passage pairs) as additional training data during
the 10-fold cross-validation. We followed the set-
tings and, in each fold, we selected 1,000 pairs
from the 9,000 pairs for training to use as develop-
ment data for tuning hyperparameters. Note that
there are no shared questions in the training, de-
velopment, or test data.

For training the AGR, we used CmpAns, the
training data set created in Ishida et al. (2018) for
compact-answer generation; CmpAns consists of
15,130 triples of a why-question, an answer pas-
sage, and a manually-created compact answer.
These cover 2,060 unique why-questions. Note
that there was no overlap between the questions in
CmpAns and those in WhySet. CmpAns was
created in the following manner: 1) human anno-
tators manually came up with open-domain why-

questions, 2) retrieved the top-20 passages for
each why-question using the open-domain why-
QA module of a publicly available web-based
QA system WISDOM X (Mizuno et al., 2016;
Oh et al., 2016), and 3) three annotators created
(when possible) a compact answer for each of
the retrieved passages. The passages for which
no annotator could create a compact answer were
discarded, and were not included in the 15,130
triples mentioned previously. The average lengths
of questions, passages, and compact answers in
CmpAns were 10.5 words, 184.4 words, and 8.3
words, respectively.

Finally, we created additional training data
AddTr for training the why-QA models. If an an-
notator could write a compact answer for a ques-
tion and an answer passage, she/he probably rec-
ognized the passage as a proper answer passage
to the question. Based on this observation, we
built AddTr from CmpAns by applying a major-
ity vote. We only gave a correct answer label to a
question and a passage if at least two of the three
annotators wrote compact answers, and it received
an incorrect answer label otherwise. AddTr has
10,401 pairs in total. We used AddTr as addi-
tional training data for baselines that lack a mech-
anism for generating compact-answer representa-
tions, for a fair comparison with other methods
that use CmpAns for such mechanisms.

We processed all the data with MeCab1, a mor-
phological analyzer, to segment the words.

4.2 Training details

In our proposed methods and their variants, all the
weights in the CNNs were initialized using Heâ€™s
method (He et al., 2015), and the other weights
in our why-QA model were initialized randomly
with a uniform distribution in the range of (-0.01,
0.01). For the CNN-based components, we set
the window size of the filters to â€œ1,2,3â€ with 100
filters each2. We used dropout (Srivastava et al.,
2014) with probability 0.5 on the final logistic re-
gression layer. All of these hyper-parameters were
chosen with our development data. We optimized
the learned parameters with the Adam stochastic
gradient descent (Kingma and Ba, 2015). The
learning rate was set to 0.001, and the batch size

1http://taku910.github.io/mecab/
2We examined all of the following combinations of num-

ber of filters and filter window size: any of {25,50,75,100}
for the former, and any of {â€œ1,2,3â€, â€œ2,3,4â€, â€œ1,2,3,4â€} for
the latter.



4232

for each iteration was set to 20.

4.3 Compared methods
We tried three schemes for training our AGR in
our proposed method. In the first scheme, pairs of
passages and compact answers in CmpAns were
given to fake-representation generator F and real-
representation generator R as their inputs. We
called the fake-representation generator trained
in this way FOP and referred to our proposed
method using FOP as Ours(OP). In the second
scheme, we randomly sampled five-sentence pas-
sages that contain some clue words indicating the
existence of causal relations, such as â€œbecause,â€
from 4-billion web pages and fed them to fake-
representation generator F . We fed the same num-
ber of the sampled passages as in CmpAns for
fair comparison. We refer to the method trained
by this scheme as Ours(RP). In the final scheme,
we replaced the word embeddings for the passages
given to fake-representation generator F with ran-
dom vectors and used similarity-attention but not
causality-attention. The fake-representation gen-
erator trained in this way is called FRV , and our
proposed method using FRV is called Ours(RV).
This scheme is more similar to the original GAN
than the others because the fake-representation
generator is given random noises.

We implemented and evaluated the following
four why-QA models in previous works as base-
lines, using the same dataset as ours:

1) Oh et al. (2013): an SVM-based model that
uses bag-of-word and causality features;

2) Sharp et al. (2016): a CNN model that rep-
resents a question and its answer passage
separately by using cause and effect word-
embedding vectors, which were trained with
our causality expressions by the same way as
in Sharp et al. (2016);

3) Tan et al. (2016): an LSTM model with
similarity-attention;

4) Oh et al. (2017): the state-of-the-art neu-
ral model for Japanese why-QA that uses
causality-attention and causality expressions.

We also evaluated nine baseline neural mod-
els, four of which are BERT-based models (BERT,
BERT+AddTr, BERT+FOP, and BERT+FRV), to
show the effectiveness of our why-QA model and
AGR. They are listed in Table 2.

Method Description

BASE Proposed method from which we re-
moved fake-representation generator
F .

BASE+AddTr BASE that used both WhySet and
AddTr as its training data.

BASE+CAns On top of BASE, it additionally
used real-representation generator R
to encode compact answers, which
were generated by the compact-
answer generator of Iida et al.
(2019). R was trained alongside the
why-QA model using WhySet and
the compact-answer generator was
pre-trained with CmpAns.

BASE+CEnc On top of BASE, it additionally used
the encoder in the compact-answer
generator of Iida et al. (2019) to cre-
ate compact-answer representation.
The encoder was pre-trained with
CmpAns.

BASE+Enc Same as Ours(OP) except that the
fake-representation generator was
trained in a supervised manner
alongside the why-QA model using
WhySet and AddTr as the training
data.

BERT Same as BASE except that the CNN-
based encoders for questions and
passages were replaced with the
BERT (Devlin et al., 2019).

BERT+AddTr BERT, which used both WhySet
and AddTr as its training data.

BERT+FOP On top of BERT, it additionally used
compact-answer representation pro-
duced by FOP for answer selection.

BERT+FRV Same as BERT+FOP except that it
used FRV instead of FOP for pro-
ducing compact-answer representa-
tion.

Table 2: Baseline neural models

To pre-train the BERT-based models, we used a
combination of sentences extracted from Japanese
Wikipedia articles (August 2018 version) and
causality expressions automatically recognized
from a causality recognizer (Oh et al., 2013). This
data mix consists of 75% of sentences extracted
from Wikipedia (14,675,535 sentences taken out
of 784,869 articles randomly sampled) and 25%
of cause and effect phrases taken from causal-
ity expressions (4,891,846 phrases from 2,445,923
causal relations). This ratio was determined
through preliminary experiments using the devel-
opment data. For the pre-training parameters, we
followed the settings of BERTBASE in Devlin et al.



4233

[CLS]

Token embeddings

Segment embeddings

Position embeddings

ð‘¤' ð‘¤â€²)

+ + +

Attention feature 
embeddings

+ + +

+ + +

E[CLS] Eð‘¤' Eð‘¤â€²)

EQ EQ EP

E0 E1 EN+M+1

E[234]
567

ð‘¤8

+

+

+

Eð‘¤8

EQ

EN

ð‘¤â€²'

+

+

+

Eð‘¤â€²'

EP

EN+2

C T1 Tâ€™MTN Tâ€™1...

...

...

...

...

...

...

...

...

...

...

...

[SEP]

+

+

+

E[SEP]

EQ

EN+1

Input

+ + + + +

T[SEP]

Question Passage

Logistic regression layer

Correct Incorrect

E='
567 E=>

567 E[4?@]
567 E=AB

567E=C'
567

+
E[234]
DEF5 ......E='

DEF5 E=>
DEF5 E[4?@]

DEF5 E=AB
DEF5E=C'

DEF5

[SEP]

+

+

+

E[SEP]

EP

EN+M+2

+

T[SEP]

E[4?@]
567

E[4?@]
DEF5

Transformer encoder
E[CLS] E1 Eâ€™MEN E[SEP]... ...Eâ€™1 ... E[SEP]

Figure 3: Architecture of BERT: E represents input em-
bedding and Ti represents contextual representation of
token i. [CLS] is a special symbol added in front of
every input example, and [SEP] is a special separator
token (e.g. for separating questions/passages).

(2019)3 except for the batch size of 50. We ran
3 epochs with the learning rate of 1e-5 for fine-
tuning the BERT-based models4.

A BERT-based model, BERT, takes a question-
passage pair as input and computes the input
representation using token, segment, position,
and attention feature embeddings (Fig. 3). For
the input representation computation, the orig-
inal BERT only used the token, segment, and
position embeddings, while BERT additionally
used the attention feature embeddings5 to ex-
ploit the same similarity-attention and causality-
attention features used in our proposed method.
We used the attention feature embeddings during
the fine-tuning and testing, but not during the pre-
training of the BERT-based model. The atten-
tion feature embeddings for answer passages (i.e.,
Esimwâ€²1

, Â· Â· Â· , Esimwâ€²M , and E
caus
wâ€²1

, Â· Â· Â· , Ecauswâ€²M ) were
computed from the same attention feature vec-
tors, as and ac, as those in our proposed methods;
those for the other parts (i.e., questions, [CLS],
and [SEP]) were computed from a zero vector (in-
dicating no attention feature). The transformer en-
coder processed the input representation to gen-

312-layers, 768 hidden states, 12 heads and training for
1-million steps with the warmup rate of 1% using Adam op-
timizer with the learning rate of 1e-4.

4We tested all the combinations of epochs {1, 2, 3, 4, 5}
and learning rates of {1e-5, 2e-5, 3e-5} and chose the one
that maximized the performance on the development data in
WhySet.

5We also evaluated a BERT-based model that did not use
the attention feature embeddings, but its P@1 (41.4) was
much lower than that of BERT (51.2).

P@1 MAP
Oh et al. (2013) 41.8 41.0
Sharp et al. (2016) 33.2 32.2
Tan et al. (2016) 34.0 33.4
Oh et al. (2017) 47.6 45.0
BASE 51.4 50.4
BASE+AddTr 52.0 49.3
BASE+CAns 51.8 50.3
BASE+CEnc 52.4 51.5
BASE+Enc 52.2 50.6
BERT 51.2 50.8
BERT+AddTr 51.8 51.0
BERT+FOP 53.4 51.2
BERT+FRV 53.2 50.9
Ours(OP) 54.8 52.4
Ours(RP) 53.4 51.5
Ours(RV) 54.6 51.8
Oracle 60.4 60.4

Table 3: Why-QA performances

erate a representation for a question-passage pair
and passed the generated representation to the lo-
gistic regression layer for answer selection. BERT
was trained with the training data in WhySet.
BERT+AddTr is the same as BERT except that it
additionally used AddTr as training data. On top
of BERT, BERT+FOP and BERT+FRV additionally
used the compact-answer representation produced
by our fake-representation generator for answer
selection by giving it to the final logistic regres-
sion layer.

4.4 Results

Table 3 shows the performances of all the methods
in the Precision of the top answer (P@1) and the
Mean Average Precision (MAP) (Oh et al., 2013).
Note that the Oracle method indicates the per-
formance of a fictional method that ranks the an-
swer passages perfectly, i.e., it locates all the m
correct answers to a question in the top-m ranks,
based on the gold-standard labels. This perfor-
mance is the upper bound of those of all the im-
plementable methods.

Our proposed method, Ours(OP), outper-
formed all the other methods. Our starting point,
i.e., BASE, was already superior to the methods
in the previous works. Compared with BASE and
BASE+AddTr, neither of which used compact-
answer representations or fake-representation gen-
erator F , Ours(OP) gave 3.4% and 2.8% im-
provement in P@1, respectively. It also outper-
formed BASE+CAns and BASE+CEnc, which
generated compact-answer representations in a
way different from the proposed method, and
BASE+Enc, which trained the fake-representation



4234

generator without adversarial learning. These per-
formance differences were statistically significant
(p < 0.01 by the McNemarâ€™s test).
Ours(OP) also outperformed all the BERT-

based models but an interesting point is that fake-
representation generator F boosted the perfor-
mance of the BERT-based models (statistically
significant with p < 0.01 by the McNemarâ€™s test).
These results suggest that AGR is effective in both
our why-QA model and our BERT-based model.

4.5 Deeper analysis on the output of F
Another interesting point is that Ours(RV),
in which fake-representation generator FRV was
trained using random vectors, achieved almost the
same performance as that of Ours(OP). This re-
sult was puzzling, so we first checked whether
FRV â€™s output was not just random noise (which
could prevent the why-QA model from overfitting)
by replacing in Ours(RV) the output of FRV by
random vectors. Although we sampled the ran-
dom vectors from different distribution types with
various ranges, we obtained at best similar perfor-
mance to that of BASE: 51.6 in P@1. This result
confirms that it is not trivial to mimic FRV using
random vectors at least.

We investigated the FRV â€™s output to check
whether it actually focused on the compact an-
swer in a given passage. We computed the fol-
lowing three representation sets from a gold set of
3,608 triples of why-questions, answer passages
and manually created compact-answers that do not
overlap with CmpAns:

â€¢ {rorgi }: FRV â€™s output with the pairs of a why-
question and an answer passage in the gold
set as its input;

â€¢ {rini }: FRV â€™s output for the same input as
{rorgi }, where we replaced the word embed-
dings of all the content words in the answer
passages that also appeared in the associated
gold compact-answers with random vectors;

â€¢ {routi }: FRV â€™s output for the same input as
{rorgi }, where we replaced the word embed-
dings of all the content words in the answer
passages that did not appear in the associated
gold compact-answers with random vectors6.

If FRV perfectly focuses on the gold standard
compact-answers, for each question-passage pair,

6For both rini and routi , we never replaced the word em-
beddings for the words that also appeared in the question.

Train Dev Test
Quasar-T 37,012 3,000 3,000
SearchQA 99,811 13,893 27,247
TriviaQA 87,291 11,274 10,790âˆ—

SQuAD v1.1 87,599 10,570âˆ— NA

Table 4: Number of questions in each dataset: datasets
marked with âˆ— were not used in this experiment

routi should be the same as r
org
i and r

in
i should sig-

nificantly differ from rorgi . Next we computed the
average Euclidian distance among {rorgi }, {rini }
and {routi }. The average distance (2.67) between
{rorgi } and {routi } was much smaller than the av-
erage distance (13.3) between {rorgi } and {rini }.
Note that we replaced the word embeddings for
much more words with random vectors in the com-
putation of {routi } than those in the computation
of {rini } (38.1 words vs. 5.6 words). This implies
that the distance between {rorgi } and {routi } might
be much larger than that between {rorgi } and {rini }
if FRV focused equally on every answer passage
word. However, the actual results suggest that this
is not the case. Although we cannot draw deci-
sive conclusions due to the complex nature of neu-
ral networks, we believe from the results that FRV
does actually focus more on words that are a part
of a compact answer than on other words. We also
computed {rorgi }, {rini }, and {routi } with fake-
representation generator FOP in the same way and
observed the same tendency.

5 DS-QA Experiments

We tested our framework on another task, the dis-
tantly supervised open-domain question answer-
ing (DS-QA) task (Chen et al., 2017), to check
its generalizability. Table 4 shows the statistics
for the datasets used in this experiment. The first
three, Quasar-T, SearchQA, and TriviaQA pro-
vided by Lin et al. (2018), were used for train-
ing and evaluating DS-QA methods. The training
data of SQuAD v1.1 (Rajpurkar et al., 2016) was
used for training our AGR. The SQuAD dataset
consisted of the triples of a question, an answer,
and a paragraph that includes the answer. We as-
sume that the answers are our compact answers,
although the answers in the dataset are consec-
utive short word sequences (2.8 words on aver-
age), whose majority are noun phrases, unlike the
compact answers for our why-QA experiment, i.e.,
sentences or phrases (8.3 words on average).

We trained our AGR with all the triples of



4235

a question, an answer, and a paragraph in the
training data of SQuAD-v1.1 under the same set-
tings for the AGRâ€™s hyperparameters as in our
why-QA experiment except that we use neither
causal word embeddings nor causality-attention.
In this experiment, we used the AGR training
schemes for Ours(OP) and Ours(RV). We
used the 300-dimensional GloVe word embed-
dings learned from 840 billion tokens in the web
crawl data (Pennington et al., 2014), as gen-
eral word embeddings. Then we combined the
resulting fake-representation generator F in the
AGR with the state-of-the-art DS-QA method,
OpenQA (Lin et al., 2018)7. We also used the hy-
perparameters presented in Lin et al. (2018).

OpenQA is composed of two components: a
paragraph selector to choose relevant paragraphs
(or answer passages in our terms) from a set of
paragraphs and a paragraph reader to extract an-
swers from the selected paragraphs. For iden-
tifying answer a to given question q from set
of paragraphs P = {pi}, the paragraph selec-
tor and the paragraph reader respectively compute
probabilities Pr(pi|q, P ) and Pr(a|q, pi), and fi-
nal output Pr(a|q, P ) is obtained by combining
the probabilities. We introduced ci, which is a
compact-answer representation generated by fake-
representation generator F with question q and
paragraph pi as its input, to the computation of the
probabilities as follows:

Pr(a|q, P, C) =
âˆ‘
i

Pr(a|q, pi, ci)Pr(pi|q, P, ci)

In the original OpenQA, the paragraph selec-
tor and the reader use bidirectional stacked RNNs
for encoding paragraphs, where word embeddings
pi of a paragraph is used as the input. In our
implementation, we computed attention-weighted
embedding pÌ„i of a paragraph by using compact-
answer representation ci. Given word embedding
pji for the j-th word in paragraph pi, its attention-
weighted embedding pÌ„ji was computed by using a
bilinear function (Sutskever et al., 2009):

pÌ„ji = softmaxj(p
T
i Mci)p

j
i ,

where M âˆˆ RdÃ—d is a trainable matrix,
softmaxj(x) denotes the j-th element of the soft-
maxed vector of x, and d = 300. We gave [pji ; pÌ„

j
i ],

a concatenation of pji and pÌ„
j
i , as the word embed-

ding of the j-th word in paragraph pi to the bidi-
rectional stacked RNNs.

7https://github.com/thunlp/OpenQA

Quasar-T SearchQA TriviaQA
EM F1 EM F1 EM F1

R3 35.3 41.7 49.0 55.3 47.3 53.7
OpenQA 42.2 49.3 58.8 64.5 48.7 56.3
Ours(OP) 43.2Â§ 49.7 59.6â€  65.3â€  49.6â€  54.8
Ours(RV) 42.9Â§ 49.9 59.7â€  65.3â€  49.6â€  54.7

Table 5: DS-QA performances: Result of TriviaQA
dataset is on its development data. Â§ and â€  respectively
indicate statistical significance of difference in perfor-
mance between Ours(Â·) and OpenQA by the McNe-
marâ€™s test with p < 0.05 and p < 0.01

Table 5 shows the performances of the four
DS-QA methods: R3 (Wang et al., 2018),
OpenQA (Lin et al., 2018), Ours(OP), and
Ours(RV) evaluated against the Quasar-T,
SearchQA and TriviaQA datasets. All the meth-
ods were evaluated with EM and F1 scores, fol-
lowing Lin et al. (2018). EM measures the per-
centage of predictions that exactly match one of
the ground-truth answers and F1 is a metric that
loosely measures the average overlap between the
prediction and ground-truth answer. Note that
both Ours(OP) and Ours(RV) outperformed
both previous methods, R3 and OpenQA, except
for the F1 score for the TriviaQA dataset. Some
of the improvements over the previous state-of-
the-art method, OpenQA, were statistically signif-
icant. These findings suggest that our framework
can be effective for tasks other than the original
why-QA and the other datasets.

6 Conclusion and Future Work

We proposed a method for why-question answer-
ing (why-QA) that used an adversarial learning
framework. It employed adversarial learning to
generate vector representations of reasons or true
answers from answer passages and exploited the
representations for judging whether the passages
are proper answer passages to the given why-
questions. Through experiments using Japanese
why-QA datasets, we showed that this idea im-
proved why-QA performance. We also showed
that our method improved the performance in a
distantly supervised open-domain QA task.

In our why-QA method, causality expressions
extracted from the web were used as background
knowledge for computing causality-attention/em-
beddings. As a future work, we plan to introduce
a wider range of background knowledge including
another type of event causality (Hashimoto et al.,
2012, 2014, 2015; Kruengkrai et al., 2017).



4236

References
Danqi Chen, Adam Fisch, Jason Weston, and An-

toine Bordes. 2017. Reading Wikipedia to answer
open-domain questions. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1870â€“1879.

Ronan Collobert, Jason Weston, LeÌon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493â€“2537.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171â€“4186.

Bhuwan Dhingra, Kathryn Mazaitis, and William W
Cohen. 2017. Quasar: Datasets for question an-
swering by search and reading. arXiv preprint
arXiv:1707.03904.

Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur
GuÌˆney, Volkan Cirik, and Kyunghyun Cho.
2017. SearchQA: A new Q&A dataset aug-
mented with context from a search engine. CoRR,
abs/1704.05179.

Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In Proceedings of
the ACL 2003 workshop on Multilingual summariza-
tion and question answering, pages 76â€“83.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Proceedings of the 27th Interna-
tional Conference on Neural Information Process-
ing Systems - Volume 2, NIPSâ€™14, pages 2672â€“2680,
Cambridge, MA, USA. MIT Press.

Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jong-Hoon Oh, and Junâ€™ichi Kazama.
2012. Excitatory or inhibitory: A new semantic
orientation extracts contradiction and causality from
the web. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 619â€“630.

Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer,
and Jong-Hoon Oh. 2015. Generating event causal-
ity hypotheses through semantic relations. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on
Artificial Intelligence, (AAAI-15), pages 2396â€“2403.

Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer,
Motoki Sano, IstvaÌn Varga, Jong-Hoon Oh, and Yu-
taka Kidawara. 2014. Toward future scenario gener-
ation: Extracting event causality exploiting semantic

relation, context, and association features. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 987â€“997.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classifica-
tion. In Proceedings of the 2015 IEEE International
Conference on Computer Vision (ICCV), ICCV â€™15,
pages 1026â€“1034, Washington, DC, USA. IEEE
Computer Society.

Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-
questions. In Proceedings of the Third International
Joint Conference on Natural Language Processing:
Volume-I, pages 418â€“425.

Ryu Iida, Canasai Kruengkrai, Ryo Ishida, Kentaro
Torisawa, Jong-Hoon Oh, and Julien Kloetzer. 2019.
Exploiting background knowledge in compact an-
swer generation for why-questions. In Proceedings
of Thirty-Third AAAI Conference on Artificial Intel-
ligence 2019 (AAAI-19).

Ryo Ishida, Kentaro Torisawa, Jong-Hoon Oh, Ryu
Iida, Canasai Kruengkrai, and Julien Kloetzer. 2018.
Semi-distantly supervised neural model for gener-
ating compact answers to open-domain why ques-
tions. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence, (AAAI-18),
pages 5803â€“5811. AAAI Press.

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com-
prehension. In Proceedings of the 55th An-
nual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1601â€“1611.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746â€“1751.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: a
method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations (ICLR), pages 1â€“13.

Canasai Kruengkrai, Kentaro Torisawa, Chikara
Hashimoto, Julien Kloetzer, Jong-Hoon Oh, and
Masahiro Tanaka. 2017. Improving event causal-
ity recognition with multiple background knowl-
edge sources using multi-column convolutional neu-
ral networks. In Proceedings of the Thirty-First
AAAI Conference on Artificial Intelligence, Febru-
ary 4-9, 2017, San Francisco, California, USA.

Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE,
volume 86, pages 2278â€“2324.



4237

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems
27, pages 2177â€“2185. Curran Associates, Inc.

Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun.
2018. Denoising distantly supervised open-domain
question answering. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2018, pages 1736â€“1745.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111â€“3119. Curran Associates,
Inc.

Mehdi Mirza and Simon Osindero. 2014. Conditional
generative adversarial nets. CoRR, abs/1411.1784.

Junta Mizuno, Masahiro Tanaka, Kiyonori Ohtake,
Jong-Hoon Oh, Julien Kloetzer, Chikara Hashimoto,
and Kentaro Torisawa. 2016. WISDOM X, DIS-
AANA and D-SUMM: Large-scale NLP systems
for analyzing textual big data. In Proceedings of
COLING 2016, the 26th International Conference
on Computational Linguistics: System Demonstra-
tions, pages 263â€“267.

Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A sys-
tem for answering non-factoid Japanese questions
by using passage retrieval weighted based on type
of answer. In Proceedings of NTCIR-6.

Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Ryu Iida, Masahiro Tanaka, and Julien Kloetzer.
2016. A semi-supervised learning approach to why-
question answering. In Proceedings of the Thirtieth
AAAI Conference on Artificial Intelligence, pages
3022â€“3029.

Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Junâ€™ichi Kazama,
and Yiou Wang. 2012. Why question answering us-
ing sentiment analysis and word classes. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
368â€“378.

Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1733â€“1743.

Jong-Hoon Oh, Kentaro Torisawa, Canasai Kru-
engkrai, Ryu Iida, and Julien Kloetzer. 2017.
Multi-column convolutional neural networks with
causality-attention for why-question answering. In

Proceedings of the Tenth ACM International Confer-
ence on Web Search and Data Mining, WSDM â€™17,
pages 415â€“424.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532â€“1543.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383â€“2392.

CÄ±Ìcero Nogueira dos Santos, Ming Tan, Bing Xiang,
and Bowen Zhou. 2016. Attentive pooling net-
works. CoRR, abs/1602.03609.

Rebecca Sharp, Mihai Surdeanu, Peter Jansen, Pe-
ter Clark, and Michael Hammond. 2016. Creating
causal embeddings for question answering with min-
imal supervision. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 138â€“148.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929â€“1958.

Ilya Sutskever, Joshua B. Tenenbaum, and Ruslan R
Salakhutdinov. 2009. Modelling relational data us-
ing bayesian clustered tensor factorization. In Ad-
vances in Neural Information Processing Systems
22, pages 1821â€“1828. Curran Associates, Inc.

Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen
Zhou. 2016. Improved representation learning for
question answer matching. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
464â€“473, Berlin, Germany. Association for Compu-
tational Linguistics.

Suzan Verberne, Hans van Halteren, Daphne Theijssen,
Stephan Raaijmakers, and Lou Boves. 2011. Learn-
ing to rank for why-question answering. Inf. Retr.,
14(2):107â€“132.

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerry
Tesauro, Bowen Zhou, and Jing Jiang. 2018. R3:
Reinforced ranker-reader for open-domain question
answering. In Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, (AAAI-
18), pages 5981â€“5988.


