










































Word Alignment with Stochastic Bracketing Linear Inversion Transduction Grammar


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341–344,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Word Alignment with
Stochastic Bracketing Linear Inversion Transduction Grammar

Markus SAERS and Joakim NIVRE
Computational Linguistics Group
Dept. of Linguistics and Philology

Uppsala University
Sweden

first.last@lingfil.uu.se

Dekai WU
Human Language Technology Center

Dept. of Computer Science and Engineering
HKUST

Hong Kong
dekai@cs.ust.hk

Abstract

The class of Linear Inversion Transduction
Grammars (LITGs) is introduced, and used to
induce a word alignment over a parallel cor-
pus. We show that alignment via Stochas-
tic Bracketing LITGs is considerably faster
than Stochastic Bracketing ITGs, while still
yielding alignments superior to the widely-
used heuristic of intersecting bidirectional
IBM alignments. Performance is measured as
the translation quality of a phrase-based ma-
chine translation system built upon the word
alignments, and an improvement of 2.85 BLEU
points over baseline is noted for French–
English.

1 Introduction

Machine translation relies heavily on word align-
ments, which are usually produced by training IBM-
models (Brown et al., 1993) in both directions and
combining the resulting alignments via some heuris-
tic. Automatically training an Inversion Transduc-
tion Grammar (ITG) has been suggested as a viable
way of producing superior alignments (Saers and
Wu, 2009). The main problem of using Bracket-
ing ITGs for alignment is that exhaustive biparsing
runs in O(n6) time. Several ways to lower the com-
plexity of ITGs has been suggested, but in this paper,
a different approach is taken. Instead of using full
ITGs, we explore the possibility of subjecting the
grammar to a linear constraint, making exhaustive
biparsing of a sentence pair in O(n4) time possible.
This can be further improved by applying pruning.

2 Background

A transduction is the bilingual version of a language.
A language (Ll) can be formally viewed as a set of
sentences, sequences of tokens taken from a speci-
fied vocabulary (Vl). A transduction (Te,f ) between
two languages (Le and Lf ) is then a set of sentence
pairs, sequences of bitokens from the cross produc-
tion of the vocabularies of the two languages being
transduced (Ve,f = Ve × Vf ). This adds an extra
layer of complexity to finding transductions from
raw bitexts, as an alignment has to be imposed.

Simple (STG) and Syntax Directed (SDTG) Trans-
duction Grammars (Aho and Ullman, 1972) can
be used to parse transductions between context-free
languages. Both work fine as long as a grammar is
given and parsing is done as transduction, that is: a
sentence in one language is rewritten into the other
language. In NLP, interest has shifted away from
hand-crafted grammars, towards stochastic gram-
mars induced from corpora. To induce a stochas-
tic grammar from a parallel corpus, expectations of
all possible parses over a sentence pair are typically
needed. STGs can biparse sentence pairs in polyno-
mial time, but are unable to account for the complex-
ities typically found in natural languages. SDTGs do
account for the complexities in natural languages,
but are intractable for biparsing.

Inversion transductions (Wu, 1995; Wu, 1997) are
a special case of transductions that are not mono-
tone, but where permutations are severely limited.
By limiting the possible permutations, biparsing be-
comes tractable. This in turn means that ITGs can be
induced from parallel corpora in polynomial time,

341



as well as account for most of the reorderings found
between natural languages.

An Inversion transduction is limited so that it
must be expressible as non-overlapping groups, in-
ternally permuted either by the identity permuta-
tion or the inversion permutation (hence the name).
This requirement also means that the grammar is bi-
narizable, yielding a two-normal form. A produc-
tion with the identity permutation is written inside
square brackets, while productions with the inver-
sion permutation is written inside angled brackets.
This gives us a two-normal form that looks like this
(where e/f is a biterminal):

A → [B C]
A → 〈B C〉
A → e/f

The time complexity for exhaustive ITG biparsing is
O(Gn6), which is typically too large to be applica-
ble to large grammars and long sentence. The gram-
mar constant G can be eliminated by limiting the
grammar to a bracketing ITG (BITG), which only has
one nonterminal symbol. Saers & Wu (2009) show
that it is possible to apply exhaustive biparsing to a
large parallel corpus (∼ 100, 000 sentence pairs) of
short sentences (≤10 tokens in both language). The
word alignments read off the Viterbi parse also in-
creased translation quality when used instead of the
alignments from bidirectional IBM alignments.

The O(n6) time complexity is somewhat pro-
hibitive for large corpora, so pruning in some form is
needed. Saers, Nivre & Wu (2009) introduce a beam
pruning scheme, which reduces time complexity to
O(bn3). They also show that severe pruning is pos-
sible without significant deterioration in alignment
quality. Haghighi et. al (2009) use a simpler aligner
as guidance for pruning, which reduce the time com-
plexity by two orders of magnitude, and also intro-
duce block ITG, which gives many-to-one instead of
one-to-one alignments. Zhang et. al (2008) present
a method for evaluating spans in the sentence pair to
determine whether they should be excluded or not.
The algorithm has a best case time complexity of
O(n3).

In this paper we introduce Linear ITG (LITG), and
apply it to a word-alignment task which is evaluated
by the phrase-based statistical machine translation
(PBSMT) system that can be built from that.

3 Stochastic Bracketing Linear Inversion
Transduction Grammar

A Bracketing Linear Inversion Transduction Gram-
mar (BLITG) is a BITG where rules may have at most
one nonterminal symbol in their production. This
gives us a normal form that is somewhat different
from the usual ITG:

X → [Xe/f ]
X → [e/fX]
X → 〈Xe/f〉
X → 〈e/fX〉
X → ǫ/ǫ

where one but not both of the tokens in the bitermi-
nal may be the empty string ǫ, if a nonterminal is
produced. By associating each rule with a probabil-
ity, we get a Stochastic BLITG (SBLITG).

3.1 Biparsing Algorithm

The sentence pair to be biparsed consists of two vec-
tors of tokens (e and f ). An item is represented
as a nonterminal (X), and one span in each of the
languages (es..t and fu..v). For notational conve-
nience, an item will be written as the nonterminal
with the spans as subscripts (Xs,t,u,v). The length of
an item is defined as the sum of the length of the two
spans: |Xs,t,u,v| = t − s + v − u. Items are gath-
ered in buckets, Bn, according to their length so that
Xs,t,u,v ∈ B|Xs,t,u,v|. The algorithm is initialized
with the item spanning the entire sentence pair:

X0,|e|,0,|f | ∈ B|X0,|e|,0,|f||

Starting from this top bucket, buckets are processed
in larger to smaller order: Bn, Bn−1, . . . , B1. While
processing a bucket, only smaller items are added,
meaning that B0 is fully constructed by the time B1
has been processed. Each item in B0 can have the
rule X → ǫ/ǫ applied to it, eliminating the nonter-
minal and halting processing. If there are no items
in B0, parsing has failed.

To process a bucket, each item is extended by all
applicable rules, and the nonterminals in the produc-
tions are added to their respective buckets.

342



System BLEU NIST Phrases
GIZA++ (intersect) 0.2629 6.7968 146,581,109
GIZA++ (grow-diag-final) 0.2632 6.7410 1,298,566
GIZA++ (grow-diag-final-and) 0.2742 6.9228 7,340,369
SBLITG (b = 25) 0.3027 7.3664 13,551,915
SBLITG (b = ∞) 0.3008 7.3303 12,673,361

Table 1: Results for French–English.

Xs,t,u,v →
[es,s+1/fu,u+1 Xs+1,t,u+1,v]

| [Xs,t−1,u,v−1 et−1,t/fv−1,v ]
| 〈es,s+1/fv−1,v Xs+1,t,u,v−1〉
| 〈Xs,t−1,u+1,v et−1,t/fu,u+1〉
| [es,s+1/ǫ Xs+1,t,u,v] | 〈es,s+1/ǫ Xs+1,t,u,v〉
| [ǫ/fu,u+1 Xs,t,u+1,v] | 〈Xs,t,u+1,v ǫ/fu,u+1〉
| [Xs,t−1,u,v et−1,t/ǫ] | 〈Xs,t−1,u,v et−1,t/ǫ〉
| [Xs,t,u,v−1 ǫ/fv−1,v] | 〈ǫ/fv−1,v Xs,t,u,v−1〉

Note that there are two productions on each of the
four last rows. These are distinct rules, but the
symbols in the productions are identical. This phe-
nomenon is due to the fact that the empty sym-
bols can be “read” off either end of the span. In
our experiments, such rules were merged into their
non-inverting form, effectively eliminating the last
four inverted rules (productions enclosed in angled
brackets) above.

3.2 Analysis

Let n be the length of the longer sentence in the
pair. The number of buckets will be O(n), since
the longest item will be at most 2n long. Within a
bucket, there can be O(n2) starting points for items,
but once the length of one of the spans is fixed, the
length of the other follows, adding a factor O(n),
making the total number of items in a bucket O(n3).
Each item in a bucket can be analyzed in 8 possible
ways, requiring O(1) time. In summary, we have:
O(n)×O(n3)×O(1) = O(n4)

The pruning scheme works by limiting the num-
ber of items that are processed from each bucket, re-
ducing the cost of processing a bucket from O(n3)
to O(b), where b is the beam width. This gives time
complexity O(n)×O(b)×O(1) = O(bn).

4 Experiments

We used the guidelines of the shared task of
WMT’081 to train our baseline system as well as
our experimental system. This includes induction of
word alignments with GIZA++ (Och and Ney, 2003),
induction of a Phrase-based SMT system (Koehn et
al., 2007), and tuning with minimum error rate train-
ing (Och, 2003), as well as applying some utility
scripts provided for the workshop. The translation
model is combined with a 5-gram language model
(Stolcke, 2002).

Our experimental system uses alignments from
the Viterbi parses, extracted during EM training of an
SBLITG on the training corpus, instead of GIZA++.
Since EM will converge fairly slowly, it was limited
to 10 iterations, after which it was halted.

We used the French–English part of the WMT’08
shared task, but limited the training set to sentence
pairs where both sentences were of length 20 or less.
This was necessary in order to carry out exhaustive
search in the SBLITG algorithm. In total, we had
381,780 sentence pairs for training, and 2,000 sen-
tence pairs each for tuning and testing. The language
model was trained with the entire training set.

To evaluate the systems we used BLEU (Papineni
et al., 2002) and NIST (Doddington, 2002)

Results are presented in Table 1. It is interesting
to note that there is no correlation between the num-
ber of phrases extracted and translation quality. The
only explanation for the results we are seeing is that
the SBLITGs find better phrases. Since the only dif-
ference is the word alignment strategy, this suggests
that the word alignments from SBLITGs are better
suited for phrase extraction than those from bidirec-
tional IBM-models. The fact that SBLITGs extract
more phrases than bidirectional IBM-models under

1http://www.statmt.org/wmt08/

343



the grow-diag-x heuristics is significant, since
more phrases means that more translation possibil-
ities are extracted. The fact that SBLITGs extract
fewer phrases than bidirectional IBM-models under
the intersect heuristic is also significant, since
it implies that simply adding more phrases is a bad
strategy. Combined, the two observations leads us
to believe that there are some alignments missed by
the bidirectional IBM-models that are found by the
SBLITG-models. It is also interesting to see that the
pruned version outperforms the exhaustive version.
We believe this to be because the pruned version ap-
proaches the correct grammar faster than the exhaus-
tive. That would mean that the exhaustive SBLITG
would be better in the limit, but the experiment was
limited to 10 iterations.

5 Conclusion

In this paper we have focused on the benefits of ap-
plying SBLITGs to the task of inducing word align-
ments, which leads to a 2.85 BLEU points improve-
ment compared to the standard model (heuristically
combined bidirectional IBM-models). In the future,
we hope that LITGs will be a spring board towards
full ITGs, with more interesting nonterminals than
the BITGs seen in the literature so far. With the pos-
sibility of inducing full ITG from parallel corpora it
becomes viable to use ITG decoders directly as ma-
chine translation systems.

Acknowledgments

This work was funded by the Swedish National Gradu-
ate School of Language Technology (GSLT), the Defense
Advanced Research Projects Agency (DARPA) under
GALE Contract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under research
grants GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of DARPA.The computations were performed on
UPPMAX resources under project p2007020.

References

A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice-Halll, En-
glewood Cliffs, New Jersey.

P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.

G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology
conference (HLT-2002), San Diego, California.

A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings of ACL/IJCNLP 2009, pages 923–931,
Singapore.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
ACL 2007 Demo and Poster Session, pages 177–180,
Prague, Czech Republic.

F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.

F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL 2003,
pages 160–167, Sapporo, Japan.

K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, pages 311–
318, Philadelphia, Pennsylvania.

M. Saers and D. Wu. 2009. Improving phrase-based
translation via word alignments from Stochastic In-
version Transduction Grammars. In Proceedings of
SSST-3 at NAACL HLT 2009, pages 28–36, Boulder,
Colorado.

M. Saers, J. Nivre, and D. Wu. 2009. Learning stochas-
tic bracketing inversion transduction grammars with
a cubic time biparsing algorithm. In Proceedings of
IWPT’09, pages 29–32, Paris, France.

A. Stolcke. 2002. SRILM – an extensible language mod-
eling toolkit. In International Conference on Spoken
Language Processing, Denver, Colorado.

D. Wu. 1995. An algorithm for simultaneously bracket-
ing parallel texts by aligning words. In Proceedings of
WVLC-3, pages 69–82, Cambridge, Massachusetts.

D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–403.

H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL/HLT
2008, pages 97–105, Columbus, Ohio.

344


