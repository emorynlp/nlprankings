329

Coling 2010: Poster Volume, pages 329–337,

Beijing, August 2010

Utilizing User-input Contextual Terms for Query Disambiguation

Byron J. Gao

Texas State University
bgao@txstate.edu

David C. Anastasiu
Texas State University

Xing Jiang

Nanyang Technological University

da1143@txstate.edu

jian0008@ntu.edu.sg

Abstract

Precision-oriented search results such as
those typically returned by the major
search engines are vulnerable to issues of
polysemy. When the same term refers to
different things, the dominant sense is pre-
ferred in the rankings of search results.
In this paper, we propose a novel tech-
nique in the context of web search that uti-
lizes contextual terms provided by users
for query disambiguation, making it pos-
sible to prefer other senses without alter-
ing the original query.

1 Introduction

World Wide Web and search engines have become
an indispensable part of everyone’s everyday life.
While web search has come a long way over the
past 10 years, it still has a long way to go to re-
spond to the ever-increasing size of the web and
needs of web surfers. Today, web search is un-
der intensive and active research, drawing unpar-
alleled attention from both industry and academia.
Need of disambiguation. One of the major
challenges in web search lies in unsatisfactory rel-
evance of results caused by ambiguity. Query
terms are inherently ambiguous due to polysemy,
and most queries are short containing 1 to 3 terms
only (Jansen et al., 2000). Thus queries are in gen-
eral prone to ambiguity of user intent or informa-
tion needs, resulting in retrieval of enormous irrel-
evant pages. As the web increases in size at an in-
creasing rate, ambiguity becomes ubiquitous and
users are in increasing need of effective means of
disambiguation. The ambiguity issue and its con-
sequences are demonstrated in Example 1.

Example 1 There are 17 entries in Wikipedia for
different renown individuals under the same name
of “Jim Gray”, including a computer scientist, a
sportscaster, a zoologist, a politician, a ﬁlm di-
rector, a cricketer, and so on. Suppose we intend
to ﬁnd information about Jim Gray, the Turing
award winner, we can issue a query of “Jim Gray”
in Yahoo!For this extremely famous name in com-
puter science, only 3 are relevant in the top 10
results. They are his Wikipedia entry, homepage
at Microsoft Research, and DBLP entry.

Straightforward approach. One intuitive way
of disambiguation would be to apply available do-
main knowledge and reﬁne the query by adding
some conﬁning contextual terms. This would gen-
erally improve precision. However, there are sev-
eral inevitable problems in this approach. First,
the improvement on precision is at the sacriﬁce
of recall. For example, many Jim Gray pages
may not contain the added contextual terms and
are thus excluded from the search results.

Second, the query is altered, leading to unfavor-
able ranking of results. Term proximity matters
signiﬁcantly in ranking (Manning et al., 2008).
Some good pages w.r.t.
the original query may
be ranked low in the new search results because
of worsened term proximity and relevance w.r.t.
the new query. Thus, with this straightforward
approach only limited success can be expected at
best, as demonstrated in Example 2.

Example 2 Suppose we know that Jim Gray is
a computer scientist, we can issue a query of “Jim
Gray computer”. All the top 10 results are about
Jim Gray and relevant. However, many of them
are trivial pages, failing to include 2 of the 3 most
important ones. His DBLP entry appears as the

330

27th result, and his homepage at Microsoft Re-
search appears as the 51st result.

This limited success is achieved by using a
carefully selected contextual term. “Computer”
is a very general term appearing on most of the
Jim Gray pages. Also, there are no other com-
petitively known computer people with the same
name. Most other contextual terms would perform
much worse. Thus a third problem of this straight-
forward query reﬁnement approach is that only
few contextual terms, which may not be avail-
able to users, would possibly achieve the limited
success. Often, much of our domain knowledge
would cause more damage than repair and is prac-
tically unusable, as demonstrated in Example 3.

Example 3 Suppose we know that Jim Gray
has David DeWitt as a close friend and colleague,
we can issue a query of “Jim Gray David De-
Witt”. Again, all the top 10 results are about
Jim Gray and relevant. However, the theme of
the query is almost completely altered. Evidently,
the 1st result “Database Pioneer Joins Microsoft
to Start New Database Research Lab”, among
many others, talks about David DeWitt. It is rele-
vant to Jim Gray only because the lab is named
“Jim Gray Systems Lab” in honor of him.

The Bobo approach. Can we freely apply our
domain knowledge to effectively disambiguate
search intent and improve relevance of results
without altering the original query? For this pur-
pose, we propose and implement Bobo.1

For conceptual clarity, the Bobo interface fea-
tures two boxes. Besides a regular query box, an
additional box is used to take contextual terms
from users that capture helpful domain knowl-
edge. Contextual terms are used for disambigua-
tion purposes. They do not alter the original query
deﬁned by query terms. Particularly, unlike in
the straightforward approach, positive contextual
terms are not required to be included in search
results and negative contextual terms are not re-
quired to be excluded from search results. Con-
textual terms help estimate relevance of search re-
sults, routing them towards a user intended do-

1Bobo has been implemented using Yahoo! web search

API and maintained at http://dmlab.cs.txstate.edu/bobo/.

main, ﬁltering out those not-in-domain, or irrel-
evant, results.

Bobo works in two rounds.

In round I, a
query is issued using by default the combination
of query terms and contextual terms, or just the
contextual terms if the query returns too few re-
sults. Then from the results, some top-ranked
high-quality pages are (automatically) selected as
seeds.
In round II, a query is issued using the
query terms. Then the results are compared with
the seeds and their similarities are computed. The
similarity values reﬂect the degree of relevance of
search results to the user intent, based on which
the results are re-ranked.

Example 4 reports the Bobo experiment using

the same contextual terms as in Example 3.
Example 4 As in Example 3, suppose we know
Jim Gray has David DeWitt as a colleague.
Then with Bobo, we can enter “Jim Gray” in the
query box and “David DeWitt” in the auxiliary
box. As a result with default preferences, all the
top 10 results are relevant including all the top
3 important Jim Gray pages. From the top 10,
only 1 page, the DBLP entry, contains “David De-
Witt” as they coauthored papers. The theme of the
query is not altered whereas in Example 3, all the
top 10 results contain “David DeWitt”.

In Example 4, the selected seeds are relevant to
Jim Gray. Observe that seeds can be useful if
they are relevant to the user-intended domain, not
only the user-intended query. Bobo works effec-
tively with such seeds and thus can utilize a much
expanded range of domain knowledge. Helpful
contextual terms do not even need to co-occur
with query terms on any page. They only need to
occur, possibly separately, on some pages of the
same domain, as demonstrated in Example 5.
Example 5 Using the criteria of being in the
same community as Jim Gray but co-occuring
on no web pages, we randomly chose a student
name, Flavia Moser. In Bobo, we entered “Jim
Gray” in the query box, “Flavia Moser” in the
auxiliary box, and used only the contextual terms
for the round I query. As a result, 11 of the top 12
results were relevant including all the top 3 im-
portant Jim Gray pages. Of course, none of the
returned pages contains “Flavia Moser”.

331

2 Related Work

Disambiguating search intent, capturing informa-
tion needs and improving search performance
have been a fundamental research objective in
information retrieval and studied from different
perspectives. Voorhees (1993) shows that dis-
ambiguation cannot be easily resolved using the-
sauruses. The ﬁltering problem (Baeza-Yates and
Ribeiro-Neto, 1999; Schapire et al., 1998) views
disambiguation as a binary text classiﬁcation task
assigning documents into one of the two cate-
gories, relevant and irrelevant. The routing prob-
lem (Schutze et al., 1995; Singhal et al., 1997)
differs from text classiﬁcation in that search re-
sults need to be ranked instead of just classiﬁed
(Gkanogiannis and Kalamboukis, 2008).

Contextual search (Lawrence, 2000; Finkel-
stein et al., 2002; Kraft et al., 2006), personalized
search (Haveliwala, 2002; Teevan et al., 2005;
Zhu et al., 2008), and implicit relevance feed-
back (Kelly and Teevan, 2003; Joachims et al.,
2005; White et al., 2004) generally utilize long-
term search history to build user proﬁles. These
proﬁles are used on a regular basis to guide many
queries. Such approaches entail little extra user
involvement in search, but need to manage pro-
ﬁles, face the privacy issue, and swallow the in-
ﬂexibility in context switch.

Explicit and pseudo relevance feedback (RF)
techniques (Ruthven and Lalmas, 2003; Baeza-
Yates and Ribeiro-Neto, 1999; Manning et al.,
2008) are more related to Bobo in the sense that
they do not build long-term proﬁles. Instead, they
construct a one-time search context that are used
only once to guide a single query each time. Such
approaches enjoy the ﬂexibility to switch sponta-
neously from one domain to another in response
to different information needs.

RF is regarded as the most popular query ref-
ormation strategy (Baeza-Yates and Ribeiro-Neto,
1999).
It iterates in multiple rounds, typically
two, to modify a query step by step. Explicit RF
asks explicit feedback from users, whereas pseudo
(or blind) RF assumes relevance of top-ranked re-
sults. The problem of explicit RF is that it requires
too much user involvement. Users are often reluc-
tant to provide explicit feedback, or do not wish

to prolong the search interaction. Web search en-
gines of today do not provide this facility. Ex-
cite.com initially included but dropped it due to
the lack of use (Manning et al., 2008).

Pseudo RF, ﬁrst suggested by Croft and Harper
(1979) and since widely investigated, automates
the manual part of RF, so that users get im-
proved search performance without extended in-
teractions. Psuedo RF has been found to improve
performance in the TREC ad hoc task and Cornell
SMART system at TREC 4 (Buckley et al., 1995).
Unfortunately, pseudo RF suffers from a major
ﬂaw, the so-called query drift problem. Query
drift occurs when the feedback documents contain
few or no relevant ones. In this case, search results
will be routed farther away from the search intent,
resulting in even worse performance. Different
approaches (Mitra et al., 1998; Yu et al., 2003;
Lee et al., 2008)have been proposed to alleviate
query drift but with little success. Some queries
will be improved, others will be harmed (Ruthven
and Lalmas, 2003).

Similarly to RF, Bobo works in two rounds.
Similarly to pseudo RF, it makes use of top-ranked
round I results. However, Bobo and RF differ
fundamentally in various aspects.

Firstly, Bobo is not a query reformation tech-
nique as RF. In RF, the automatically generated
additional terms become part of the reformed
query to be issued in round II, while in Bobo, the
user-input contextual terms are not used in round
II. The terms generated by RF may work well
as contextual terms for Bobo but not the other
way around. In general, effective contextual terms
form a much larger set.

In query reformation, it is often hard to under-
stand why a particular document was retrieved af-
ter applying the technique (Manning et al., 2008).
In Bobo, the original query is kept intact and only
the ranking of search results is changed.

Secondly, in RF, only query terms are used in
round I queries. In Bobo, by default the combi-
nation of query terms and contextual terms, both
entered by users, is used, leading to much more
relevant seeds that are comparable to explicit RF.
In this sense, Bobo provides a novel and effective
remedy for query drift.

Beyond that, Bobo can use contextual terms

332

only to obtain seeds that are relevant to the user-
intended domain and not necessarily the user-
intended query, leading to effective utilization of
a largely expanded range of domain knowledge.

Thirdly, RF can have practical problems. The
typically long queries (usually more than 20
terms) generated by RF techniques are inefﬁcient
for IR systems, resulting in high computing cost
and long response time (Manning et al., 2008). In
Bobo, however, both query terms (1 to 3) and
contextual terms (1 to 2) are short. A round I
query combining the two would typically contain
2 to 5 terms only.

3 Overview

Bobo uses the vector space model, where both
documents and queries are represented as vectors
in a discretized vector space. Documents used in
similarity comparison can be in the form of ei-
ther full pages or snippets. Documents are pre-
processed and transformed into vectors based on
a chosen term weighting scheme, e.g., TF-IDF.

The architecture of Bobo is shown in Figure 1.
Without input of contextual terms, Bobo works
exactly like a mainstream search engine and the
dashed modules will not be executed.
Input of
contextual terms is optional in need of disam-
biguation of user intent. Domain knowledge, di-
rectly or indirectly associated with the query, can
be used as “pilot light” to guide the search towards
a user-intended domain.

With input of contextual terms, Bobo works
in two rounds. In round I, a query is issued us-
ing by default the combination of query terms and
contextual terms, or just the contextual terms if
they are unlikely to co-occur much with the query
terms. Then from the results, the top k documents
(full pages or snippets) satisfying certain qual-
ity conditions, e.g., number of terms contained
in each seed, are selected as seeds. Optionally,
seeds can be cleaned by removing the contained
query terms to reduce background noise of indi-
vidual seeds, or puriﬁed by removing possibly ir-
relevant seeds to improve overall concentration.
Contextual terms themselves can be used as an elf
seed, which is a special document allowing nega-
tive terms, functioning as an explicit feedback.

In round II, a query is issued using the query

Figure 1: Architecture of Bobo.

terms. Then, each returned result (full page or
snippet) is compared to the seeds to compute a
similarity using a designated similarity measure,
Jaccard coefﬁcient or Cosine coefﬁcient.
In the
computation, seeds can be combined to form a
prototype as in Rocchio, or not combined us-
ing none generalization as in instance-based lazy
learning to better capture locality and handle poly-
morphic domains. Based on the assumption that
seeds are highly relevant, the similarity values es-
timate the closeness of search results to the user
intent, based on which the results are re-ranked.

Bobo was implemented using Yahoo! web
search API. For each query, Bobo retrieves 30
HTML pages from the API. If snippets are used
for seeding and comparison, the response time
of Bobo is sufﬁciently fast.
If full pages are
used, page downloading and preprocessing are
prohibitively time-consuming. However, the goal
of Bobo is to illustrate the promise of the novel
disambiguation approach.
If Bobo were imple-
mented at the server (search engine) side, re-
sponse time would not be an issue.

4 Principles and Preferences

In this section, we introduce in detail the design
principles and preferences of Bobo regarding the
various key issues. We also discuss possible im-
provements in these aspects.

4.1 Use of Contextual Terms
How to use contextual terms has a fundamental
impact on the behavior and performance of Bobo.
In round I. By default, the combination of
query terms and contextual terms are used in
round I queries. This produces seeds that are rel-
evant to the user-intended query. For instance, in
Example 4, the seeds are relevant to Jim Gray.
This usage of contextual terms actually provides a
novel and effective remedy for query drift, thanks
to the input of domain knowledge.

round I 

query terms + contextual terms

or

contextual terms

search
engine 

round I
results 

seed 
quality
control

seeds

round II 

query terms 

search 
engine 

round II
results 

similarity 
computation 

re-ranked 
round II 
results

333

Generally, a large portion of domain knowledge
cannot be utilized in a straightforward manner,
due to the fact that contextual terms may co-occur
with query terms in very few or none web pages.
However, as shown in Example 5, Bobo allows
using only contextual terms for round I queries,
enabling utilization of indirectly associated do-
main knowledge.

As elf seed. Contextual terms can be consid-
ered forming a pseudo document, which can be
optionally used as a seed. We call such a seed elf
seed as it is actually a piece of explicit relevance
feedback. Unlike normal seeds, an elf seed may
contain positive as well as negative terms, provid-
ing a way of collecting positive as well as negative
explicit feedback.

Discussion. The option of combing query
terms and contextual terms in round I queries can
be automated. The idea is to combine the terms
ﬁrst, then test the kth result to see whether it con-
tains all the terms.
If not, only the contextual
terms should be used in the query.

4.2 Quality of Seeds
As in pseudo relevance feedback, quality of seeds
plays an critical role in search performance. The
difference is that in Bobo, input of contextual
terms is largely responsible for the much im-
proved relevance of seeds. To provide further
quality control, Bobo accepts several user-input
thresholds, e.g., number of seeds and number of
terms contained in each seed. Beyond that, Bobo
also provides the following options.

Removing query terms. By default, Bobo
uses a combination of contextual terms and query
terms in round I queries. Thus usually all the
seeds contain the query terms. Round II results
contain the query terms as well. Then, in simi-
larity computation against the seeds, those query
terms contribute almost equally to each round II
result. This amount of contribution then becomes
background noise, reducing the sensitivity in dif-
ferentiating round II results.

By default, Bobo removes query terms from
seeds. Although a simple step, this option signiﬁ-
cantly improves performance in our experiments.
Purifying seeds. Different approaches have
been proposed to alleviate query drift by improv-

ing relevance of pseudo feedback, but with limited
success (Ruthven and Lalmas, 2003). In Bobo,
due to the input of domain knowledge, we can
well assume that the majority of seeds are rele-
vant, based on which, we can design simple mech-
anisms to purify seeds. Brieﬂy, we ﬁrst calculate
the centroid of seeds. Then, we compute the sim-
ilarity of each seed against the centroid, and re-
move those outlying seeds with poor similarities.
Discussion. Current search engines take into
account link-based popularity scores in ranking
search results.
In Bobo, round I search results
are not used to directly meet information needs of
users. They are never browsed by users. Thus,
different search engines with alternative ranking
schemes may be used to better fulﬁll the purpose
of round I queries.

Round I queries do not need to be issued to the
same region as round II queries either. Working in
a more quality region may help avoid spamming
and retrieve better candidates for seed selection.

4.3 Term Weighting

Bobo uses two term weighting schemes. The de-
fault one is the conventional TF-IDF. The other
scheme, TF-IDF-TAI, uses term association to
favor terms that show high co-occurrence with
query terms. It is tailored to Bobo, where doc-
uments are not compared in isolation, but being
“watched” by a query. While TF-IDF can be con-
sidered global weighting independent of queries,
TF-IDF-TAI can be considered local weighting.
Here we omit the details due to the page limit.

IDF estimation. To estimate the IDF values
of terms, Bobo used 664, 103 documents in the
Ad-hoc track of TREC dataset.2 These documents
can produce a reasonable approximation as they
cover various domains such as newspapers, U.S.
patents, ﬁnancial reports, congressional records,
federal registers, and computer related contents.

n

In particular,

for a term A, IDF (A) =
DF (A), where DF (A) is the document fre-
log2
quency of A in the TREC data set and n =
664,103.

2http://trec.nist.gov/data/docs eng.html.

334

4.4 Similarity Computation
By computing similarities between round II re-
sults and seeds, Bobo estimates how close differ-
ent results are to the search intent.

Document type. Seeds can either be in type
of snippets (including titles) or full pages. So it
is with round II results. White et al. (2007) re-
ported that snippets performed even better than
full texts for the task of pseudo RF. In our experi-
ments, snippets also performed comparably to full
pages. Thus, Bobo uses “snippet” as the default
option for fast response time.

Similarity measure. Bobo uses two standard
similarity measures, Cosine coefﬁcient (default)
and Jaccard coefﬁcient. Both performed very
well in our experiments, with the default option
slightly better.

Prototype-based similarity.

Bobo imple-
ments two types of similarity computation meth-
ods, prototype-based or instance-based, with the
latter as the default option.

The prototype-based method is actually a form
of the well-known Rocchio algorithm (Rocchio,
1971; Salton and Buckley, 1997), which is efﬁ-
cient but would perform poorly in the presence of
polymorphic domains. In this method, the seeds
are combined and the centroid of seeds is used in
similarity computation. Given a set S of seeds, the
|S|Ps∈S ~s, where
centroid ~u is calculated as ~u = 1
~s is the vector space representation of seed s ∈ S.
Recall that the original Rocchio algorithm for
query reformation is deﬁned as follows,

~qe = α~q + β

1

|Dr| X~dj∈Dr

~dj − γ

1

|Dir| X~dj∈Dir

~dj

where q is the original query vector, qe is the
modiﬁed query vector, and Dr and Dir repre-
sent the sets of known relevant and irrelevant
document vectors respectively. α, β, and γ are
empirically-chosen tuning parameters.

If we assign α = 0 and γ = 0, the Rocchio
formula agrees with our deﬁnition of centroid of
seeds. We assign α = 0 because Bobo does not
target query reformation. We assign γ = 0 not
because of the lack of negative feedback, which
is not hard to identify from low-ranked round I
search results. The reason is that even in explicit

Figure 2: A Polymorphic Domain.

RF, there is no evidence that negative feedback
improves performance (Schutze et al., 1995).

Instance-based similarity. Rocchio is simple
and efﬁcient. However, it over-generalizes train-
ing data and is inaccurate in the presence of poly-
morphic, or disjunctive, domains. In Figure 2, the
10 seeds labeled by “+” are split into two sepa-
rate and rather distant sub-domains. The centroid
of seeds labeled by “⊕” is not local to any sub-
domain. Search result 1 is close to the centroid
whereas result 2 is not. Rocchio would give high
relevance score to result 1 and poor score to result
2. However, result 2 actually belongs to one of the
two sub-domains whereas result 1 does not.

To handle polymorphic domains and capture
locality, Bobo uses an instance-based approach,
where the similarity of a document against each
individual seed is computed, weighted, and ag-
gregated. Let sim(d, S) denote the similarity be-
tween a document d and a set S of seeds, then,

sim(d, S) =Xs∈S

sim(d, s) × sim(d, s)

Using this approach, result 2 will receive much

higher relevance score than result 1 in Figure 2.

Note that, this approach resembles instance-
based lazy learning such as k-nearest neighbor
classiﬁcation. Lazy learning generally has supe-
rior performance but would suffer from poor clas-
siﬁcation efﬁciency. This, however, is not a crit-
ical issue in our application because we do not
have many seeds. The default number of seeds
in Bobo is set to 10.

Discussion. While Bobo adopts rather stan-
dard approaches, we are aware of the many other
approaches proposed in the literature for pairwise
web page similarity computation. An interesting
direction to investigate would be a link-based or
hybrid approach. For example, Vassilvitskii and
Brill (2006) uses web-graph distance for relevance
feedback in web search.

+

+

+
+ +

+

+

+

+

+

(cid:312)

(cid:311)

335

5 Empirical Evaluation

We evaluated Bobo in comparison with regular
Yahoo! search with and without using contextual
terms. Results returned from Yahoo! may vary
with time. This, however, will not change the gen-
eral trends revealed by our empirical study. From
these trends we conclude that, Bobo is a sim-
ple yet effective paradigm for query intent disam-
biguation without altering the original query and
with maximized utilization of domain knowledge.

5.1 Experiment Setting and Methodology

Parameter setting. To emphasize the Bobo idea,
unless otherwise speciﬁed, we used default op-
tions in the experiments that implement conven-
tional approaches, e.g., TF-IDF for term weight-
ing and Cosine coefﬁcient for similarity compu-
tation. By default, number of seeds was set to
10 with each seed having at least 10 terms. The
number of layers was set such that round II results
were re-ranked in decreasing order of similarity.
Cleaning seeds was set to yes. Purifying seeds,
elf seed and weighting seeds were set to no.

Dataset. Finding information about people is
one of the most common search activities. Around
30% of web queries include person names (Artiles
et al., 2005). Person names, however, are highly
ambiguous, e.g., only 90,000 different names are
shared by 100 million people according to the
U.S. Census Bureau (Guha and Garg, 2004).

To test

the disambiguation effectiveness of
Bobo, we constructed 60 ambiguous name
queries and 180 test cases from the Wikipedia dis-
ambiguation pages.3

In Wikipedia, articles about two or more differ-
ent topics could have the same natural page title.
Disambiguation pages are then used to solve the
conﬂicts. From the various categories, we used
the human name category, containing disambigua-
tion pages for multiple people of the same name.
For each name, the disambiguation page lists all
the different people together with their brief in-
troductions. For example, an Alan Jackson is in-
troduced as “born 1958, American country music
singer and songwriter”.

3en.wikipedia.org/wiki/Category:Disambiguation pages.

Person names were chosen from the most com-
mon English ﬁrst and last names for the year 2000
published on Wikipedia. The ﬁrst 10 male and
ﬁrst 10 female given names were combined with
the ﬁrst 10 most common last names to make a list
of 200 possible names. From this list, names were
chosen based on the following criteria. For each
name, there are at least 2 distinct people with the
same name, each having at least 3 relevant pages
in the returned 30 results.

In total 60 names were chosen as ambiguous
queries. For each query, the actual information
need was predetermined in a random manner.
Then, for this predetermined person, 3 contextual
terms were selected from her brief introduction, or
her Wikipedia page in case the introduction was
too short. For example, for the above Alan Jack-
son example, “music”, “singer”, or “songwriter”
can be selected as contextual terms. Contextual
terms were used one at a time, thus there are 3 test
cases for each ambiguous query.

The identiﬁcation of relevance of search results
was done manually. For each query, let R30 be the
set of relevant pages w.r.t.
the information need
contained in the 30 retrieved results. R30 can be
considered containing the most important relevant
pages for the original query.

Comparison partners and evaluation mea-
sures. To compare with Bobo,
two types of
regular search methods were used. The Yahoo!
method uses the original query and performs the
simplest Yahoo! web search, returning the same
set of results as Bobo but without re-ranking.

To demonstrate the relevance improvement of
Bobo over the Yahoo! method, we used a couple
of standard ranking-aware evaluation measures,
which were 11-point precision-recall graph, pre-
cision at k graph, Mean Average Precision (MAP)
and R-precision.

The Yahoo!-reﬁned method is the straightfor-
ward query reﬁnement approach we previously
discussed. It reﬁnes the original query by adding
some contextual terms. The reﬁned query alters
the original query, leading to unfavorable ranking
of results and failing to include many important
relevant pages, i.e., R30 pages, in the top results.
To demonstrate this point, we used the recall at
k evaluation measure, which measures the frac-

336

Figure 3: Bobo vs. Yahoo! on Averaged 11-point
Precision-Recall.

Figure 5: Bobo vs. Yahoo!-reﬁned on Averaged
Recall at k.

spectively. The improvement of Bobo over Ya-
hoo! is about 33% for both measures.

Bobo vs. Yahoo!-reﬁned. The recall at k
graphs are presented in Figure 5. From the ﬁg-
ure we can see that for k = 15, k = 10 and k = 5,
the recall (of important R30 pages) improvement
of Bobo over Yahoo! is roughly 30%.

The results demonstrated that, although the
straightforward query reﬁnement approach can ef-
fectively improve relevance, it fails to rank those
important relevant pages high, as it alters the orig-
inal query and changes the query themes. Bobo,
on the contrary, overcomes this problem by us-
ing the contextual terms “in the backstage”, ef-
fectively improving relevance while keeping the
original query intact.

Due to the page limit, here we omit other se-
ries of experiments that evaluated the ﬂexibility of
Bobo in choosing effective contextual terms and
how the varied user preferences affect its perfor-
mance. A user study was also conducted to test
the usability and performance of Bobo.

6 Conclusions

As the web increases in size at an increasing rate,
ambiguity becomes ubiquitous.
In this paper,
we introduced a novel Bobo approach to achieve
simple yet effective search intent disambiguation
without altering the original query and with max-
imized domain knowledge utilization.

Although we introduce Bobo in the context of
web search, the idea can be applied to the set-
tings of traditional archival information retrieval
or multimedia information retrieval.

Figure 4: Bobo vs. Yahoo! on Averaged Preci-
sion at k.

tion of relevant pages (here, ones in R30) con-
tained in the top k results.

In the entire empirical study, the Yahoo! results
were averaged over 60 queries, whereas all other
results were averaged over 180 test cases.

5.2 Evaluation Results

In Figures 3, 4 and 5, Bobo results using both Co-
sine similarity and Jaccard coefﬁcient are shown.
The two performed similarly, with the former (de-
fault) slightly better.

Bobo vs. Yahoo!. The 11-point precision-
recall graphs and precision at k graphs are pre-
sented in Figure 3 and Figure 4 respectively.

Web search users would typically browse a few
top-ranked results. From Figure 4 we can see that
for k =15, 10 and 5, the precision improvement
of Bobo over Yahoo! is roughly 20% – 40%.

In addition,

the MAP and R-precision val-
ues for Bobo are 0.812 and 0.740 respectively,
whereas they are 0.479 and 0.405 for Yahoo! re-

1

0.8

0.6

0.4

0.2

n
o
i
s
i
c
e
r
P

0

0

BoBo - Cosine

BoBo - Jaccard

Yahoo!

0.2

0.4

0.6

0.8

1

Recall

1

0.8

0.6

0.4

0.2

n
o
i
s
i
c
e
r
P

0

5

BoBo - Cosine

BoBo - Jaccard

Yahoo!

10

15

20

25

30

k

l
l
a
c
e
R

1

0.8

0.6

0.4

0.2

0

5

BoBo - Cosine

BoBo - Jaccard

Yahoo!-refined

10

15

20

25

30

k

337

References
Artiles, Javier, Julio Gonzalo, and Felisa Verdejo.
2005. A testbed for people searching strategies in
the WWW. In SIGIR.

Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern

Information Retrieval. Addison-Wesley.

Buckley, Chris, Singhal Amit, , and Mitra Mandar.
1995. New retrieval approaches using smart: Trec
4. In TREC.

Croft, W. and D. Harper. 1979. Using probabilistic
models of information retrieval without relevance
information. Journal of Documentation, 35(4):285–
295.

Finkelstein, Lev, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the con-
cept revisited. ACM Transactions on Information
Systems, 20(1):116–131.

Gkanogiannis, Anestis and Theodore Kalamboukis.
2008. An algorithm for text categorization. In SI-
GIR.

Guha, R. V. and A. Garg. 2004. Disambiguating peo-

ple in search. In WWW.

Haveliwala, Taher H. 2002. Topic-sensitive pagerank.

In WWW.

Jansen, B. J., A. Spink, , and T. Saracevic. 2000. Real
life, real users and real needs: A study and analysis
of users queries on the web. Information Processing
and Management, 36(2):207–227.

Joachims, Thorsten, Laura Granka, Bing Pan, Helene
Hembrooke, and Geri Gay. 2005. Accurately inter-
preting clickthrough data as implicit feedback.
In
SIGIR.

Kelly, Diane and Jaime Teevan. 2003. Implicit feed-
back for inferring user preference: a bibliography.
SIGIR Forum, 37(2):18–28.

Kraft, Reiner, Chi Chao Chang, Farzin Maghoul, and
In

Ravi Kumar. 2006. Searching with context.
WWW.

Lawrence, Steve. 2000. Context in web search. IEEE

Data Engineering Bulletin, 23(3):25–32.

Lee, Kyung Soon, W. Bruce Croft, and James Al-
lan. 2008. A cluster-based resampling method for
pseudo-relevance feedback. In SIGIR.

Manning, Christopher D., Prabhakar Raghavan, and
Hinrich Schutze. 2008. Introduction to Information
Retrieval. Cambridge University Press.

Mitra, M., A. Singhal, and C. Buckley. 1998. Improv-

ing automatic query expansion. In SIGIR.

Rocchio, J. 1971. Relevance Feedback in Informa-
In The SMART Retrieval System
tion Retrieval.
– Experiments in Automatic Document Processing.
Prentice-Hall.

Ruthven, Ian and Mounia Lalmas. 2003. A survey on
the use of relevance feedback for information access
systems. Knowledge Engineering Review, 18(1).

Salton, Gerard and Chris Buckley. 1997. Improving
retrieval performance by relevance feedback. Mor-
gan Kaufmann.

Schapire, Robert E., Yoram Singer, and Amit Singhal.
1998. Boosting and rocchio applied to text ﬁltering.
In SIGIR.

Schutze, Hinrich, David A. Hull, and Jan O. Peder-
sen. 1995. A comparison of classiﬁers and doc-
ument representations for the routing problem.
In
SIGIR.

Singhal, Amit, Mandar Mitra, and Chris Buckley.
1997. Learning routing queries in a query zone. In
SIGIR.

Teevan, Jaime, Susan T. Dumais, and Eric Horvitz.
2005. Personalizing search via automated analysis
of interests and activities. In SIGIR.

Vassilvitskii, Sergei and Eric Brill. 2006. Using web-
graph for relevance feedback in web search. In SI-
GIR.

Voorhees, Ellen M. 1993. Using wordnet to disam-

biguate word senses for text retrieval. In SIGIR.

White, Ryen W., Joemon M. Jose, C. J. Van Rijsber-
gen, and Ian Ruthven. 2004. A simulated study of
implicit feedback models. In ECIR.

White, Ryen W., Charles L.A. Clarke, and Silviu
Cucerzan.
Comparing query logs and
pseudo-relevance feedback for web search query re-
ﬁnement. In SIGIR.

2007.

Yu, Shipeng, Deng Cai, Ji-Rong Wen, and Wei-Ying
Ma. 2003.
Improving pseudo-relevance feedback
in web information retrieval using web page seg-
mentation. In WWW.

Zhu, Yangbo, Jamie Callan, and Jaime Carbonell.
2008. The impact of history length on personalized
search. In SIGIR.

