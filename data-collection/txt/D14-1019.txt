



















































Syntax-Augmented Machine Translation using Syntax-Label Clustering


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 165–171,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Syntax-Augmented Machine Translation using Syntax-Label Clustering

Hideya Mino, Taro Watanabe and Eiichiro Sumita
National Institute of Information and Communications Technology

3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN
{hideya.mino, taro.watanabe, eiichiro.sumita}@nict.go.jp

Abstract

Recently, syntactic information has helped
significantly to improve statistical ma-
chine translation. However, the use of syn-
tactic information may have a negative im-
pact on the speed of translation because of
the large number of rules, especially when
syntax labels are projected from a parser in
syntax-augmented machine translation. In
this paper, we propose a syntax-label clus-
tering method that uses an exchange algo-
rithm in which syntax labels are clustered
together to reduce the number of rules.
The proposed method achieves clustering
by directly maximizing the likelihood of
synchronous rules, whereas previous work
considered only the similarity of proba-
bilistic distributions of labels. We tested
the proposed method on Japanese-English
and Chinese-English translation tasks and
found order-of-magnitude higher cluster-
ing speeds for reducing labels and gains
in translation quality compared with pre-
vious clustering method.

1 Introduction

In recent years, statistical machine translation
(SMT) models that use syntactic information have
received significant research attention. These
models use syntactic information on the source
side (Liu et al., 2006; Mylonakis and Sima’an,
2011), the target side (Galley et al., 2006; Huang
and Knight, 2006) or both sides (Chiang, 2010;
Hanneman and Lavie, 2013) produce syntactically
correct translations. Zollmann and Venugopal
(2006) proposed syntax-augmented MT (SAMT),
which is a MT system that uses syntax labels of a
parser. The SAMT grammar directly encodes syn-
tactic information into the synchronous context-
free grammar (SCFG) of Hiero (Chiang, 2007),

which relies on two nonterminal labels. One prob-
lem in adding syntax labels to Hiero-style rules
is that only partial phrases are assigned labels.
It is common practice to extend labels by us-
ing the idea of combinatory categorial grammar
(CCG) (Steedman, 2000) on the problem. Al-
though this extended syntactical information may
improve the coverage of rules and syntactic cor-
rectness in translation, the increased grammar size
causes serious speed and data-sparseness prob-
lems. To address these problems, Hanneman and
Lavie (2013) coarsen syntactic labels using the
similarity of the probabilistic distributions of la-
bels in synchronous rules and showed that perfor-
mance improved.

In the present work, we follow the idea of label-
set coarsening and propose a new method to group
syntax labels. First, as an optimization criterion,
we use the logarithm of the likelihood of syn-
chronous rules instead of the similarity of prob-
abilistic distributions of syntax labels. Second,
we use exchange clustering (Uszkoreit and Brants,
2008), which is faster than the agglomerative-
clustering algorithm used in the previous work.
We tested our proposed method on Japanese-
English and Chinese-English translation tasks and
observed gains comparable to those of previous
work with similar reductions in grammar size.

2 Syntax-Augmented Machine
Translation

SAMT is an instance of SCFG G, which can be
formally defined as

G = (N , S, Tσ, Tτ ,R)
where N is a set of nonterminals, S ∈ N is a
start label, Tσ and Tτ are the source- and target-
side terminals, andR is a set of synchronous rules.
Each synchronous rule in R takes the form

X → ⟨α, β,∼⟩

165



where X ∈ N is a nonterminal, α ∈ (N ∪ Tσ)∗
is a sequence of nonterminals or source-side ter-
minals, and β ∈ (N ∪ Tτ )∗ is a sequence of
nonterminals or target-side terminals. The num-
ber #NT (α) of nonterminals in α is equal to
the number #NT (β) of nonterminals in β, and
∼: {1, ..., #NT (α)} → {1, ..., #NT (β)} is a
one-to-one mapping from nonterminals in α to
nonterminals in β. For each synchronous rule, a
nonnegative real-value weight w(X → ⟨α, β,∼⟩)
is assigned and the sum of the weights of all rules
sharing the same left-hand side in a grammar is
unity.

Hierarchical phrase-based SMT (Hiero) (Chi-
ang, 2007) translates by using synchronous rules
that only have two nonterminal labels X and S but
have no linguistic information. SAMT augments
the Hiero-style rules with syntax labels from a
parser and extends these labels based on CCG.
Although the use of extended syntax labels may
increase the coverage of rules and improve the
potential for syntactically correct translations, the
growth of the nonterminal symbols significantly
affects the speed of decoding and causes a serious
data-sparseness problem.

To address these problems, Hanneman and
Lavie (2013) proposed a label-collapsing algo-
rithm, in which syntax labels are clustered by us-
ing the similarity of the probabilistic distributions
of clustered labels in synchronous rules. First,
Hanneman and Lavie defined the label-alignment
distribution as

P (s|t) = #(s, t)
#(t)

(1)

where Nσ and Nτ are the source- and target-side
nonterminals in synchronous rules, s ∈ Nσ and
t ∈ Nτ are syntax labels from the source and tar-
get sides, #(s, t) denotes the number of left-hand-
side label pairs, and #(t) denotes the number of
target-side labels. Second, for each target-side la-
bel pair (ti, tj), we calculate the total distance d of
the absolute differences in the likelihood of labels
that are aligned to a source-side label s:

d(ti, tj) =
∑

s∈Nσ
|P (s|ti)− P (s|tj)| (2)

Next, the closest syntax-label pair of t̂ and t̂′ is
combined into a new single label. The agglomera-
tive clustering is applied iteratively until the num-
ber of the syntax labels reaches a given value.

The clustering of Hanneman and Lavie proved
successful in decreasing the grammar size and pro-
viding a statistically significant improvement in
translation quality. However, their method relies
on an agglomerative clustering with a worst-case
time complexity of O(|N |2 log |N |). Also, clus-
tering based on label distributions does not al-
ways imply higher-quality rules, because it does
not consider the interactions of the nonterminals
on the left-hand side and the right-hand side in
each synchronous rule.

3 Syntax-Label Clustering

As an alternative to using the similarity of proba-
bilistic distributions as a criterion for syntax-label
clustering, we propose a clustering method based
on the maximum likelihood of the synchronous
rules in a training data D. We uses the idea
of maximizing the Bayesian posterior probability
P (M |D) of the overall model structure M given
data D (Stolcke and Omohundro, 1994). While
their goal is to maximize the posterior

P (M |D) ∝ P (M)P (D|M) (3)
we omit the prior term P (M) and directly max-
imize the P (D|M). A model M is a clustering
structure1 . The synchronous rule in the data D
for SAMT with target-side syntax labels is repre-
sented as

X → ⟨a1Y (1)a2Z(2)a3, b1Y (1)b2Z(2)b3⟩ (4)
where a1, a2, a3 and b1, b2, b3 are the source- and
target-side terminals, respectively X , Y , Z are
nonterminal syntax labels, and the superscript
number indicates alignment between the source-
and target-side nonterminals. Using Equation (4)
we maximize the posterior probability P (D|M)
which we define as the probability of right-hand
side given the syntax label X of the left-hand side
rule in the training data as follows:∑

X→⟨α,β,∼⟩∈D
log Pr(⟨α, β,∼⟩|X) (5)

For the sake of simplicity, we assume that the
generative probability for each rule does not de-
pend on the existence of terminal symbols and that
the reordering in the target side may be ignored.
Therefore, Equation (5) simplifies to∑
X→⟨a1Y (1)a2Z(2)a3,b1Y (1)b2Z(2)b3⟩

log p(Y, Z|X) (6)

1P (M) is reflected by the number of clusters.

166



3.1 Optimization Criterion
The generative probability in each rule of the form
of Equation (6) can be approximated by clustering
nonterminal symbols as follows:

p(Y, Z|X) ≈ p(Y |c(Y )) · p(Z|c(Z))
·p(c(Y ), c(Z)|c(X)) (7)

where we map a syntax label X to its equivalence
cluster c(X). This can be regarded as the cluster-
ing criterion usually used in a class-based n-gram
language model (Brown et al., 1992). If each label
on the right-hand side of a synchronous rule (4) is
independent of each other, we can factor the joint
model as follows:

p(Y, Z|X) ≈ p(Y |c(Y )) · p(Z|c(Z))
·p(c(Y )|c(X))·p(c(Z)|c(X)) (8)

We introduce the predictive idea of Uszkoreit and
Brants (2008) to Equation (8), which doesn’t con-
dition on the clustered label c(X), but directly on
the syntax label X:

p(Y, Z|X) ≈ p(Y |c(Y )) · p(Z|c(Z))
·p(c(Y )|X) · p(c(Z)|X) (9)

The objective in Equation (9) is represented using
the frequency in the training data as

N(Y )
N(c(Y ))

·N(X, c(Y ))
N(X)

· N(Z)
N(c(Z))

·N(X, c(Z))
N(X)

(10)

where N(X) and N(c(X)) denote the frequency2

of X and c(X), and N(X, K) denotes the fre-
quency of cluster K in the right-hand side of a
synchronous rule whose left-hand side syntax la-
bel is X . By replacing the rule probabilities in
Equation (9) with Equation (10) and plugging the
result into Equation (6), our objective becomes

F (C) =
∑

Y ∈N
N(Y ) · log N(Y )

N(c(Y ))

+
∑

X∈N ,K∈C
N(X,K) · log N(X, K)

N(X)

=
∑

Y ∈N
N(Y ) · log N(Y )

−
∑

Y ∈N
N(Y ) · log N(c(Y ))

+
∑

X∈N ,K∈C
N(X,K) · log N(X,K)

−
∑

X∈N ,K∈C
N(X,K) · log N(X)(11)

2We use a fractional count (Chiang, 2007) which adds up
to one as a frequency.

start with the initial mapping (label X → c(X))
compute objective function F (C)

for each label X do
remove label X from c(X)
for each cluster K do

move label X tentatively to cluster K
compute F (C) for this exchange

move label X to cluster with maximum F (C)
do until the cluster mapping does not change

Table 1: Outline of syntax-label clustering method

where C denotes all clusters and N denotes all
syntax labels. For Equation (11), the last summa-
tion is equivalent to the sum of the occurrences
of all syntax labels, and canceled out by the first
summation. K in the third summation consid-
ers clusters in a synchronous rule whose left-hand
side label is X , and we let ch(X) denote a set
of those clusters. The second summation equals∑

K∈C N(K) · log N(K). As a result, Equation
(11) simplifies to

F (C) =
∑

X∈N ,K∈ch(X)
N(X, K) · log N(X,K)

−
∑
K∈C

N(K) · log N(K) (12)

3.2 Exchange Clustering

We used an exchange clustering algorithm
(Uszkoreit and Brants, 2008) which was proven
to be very efficient in word clustering with a vo-
cabulary of over 1 million words. The exchange
clustering for words begins with the initial cluster-
ing of words and greedily exchanges words from
one cluster to another such that an optimization
criterion is maximized after the move. While ag-
glomerative clustering requires recalculation for
all pair-wise distances between words, exchange
clustering only demands computing the difference
of the objective for the word pair involved in a par-
ticular movement. We applied this exchange clus-
tering to syntax-label clustering. Table 1 shows
the outline. For initial clustering, we partitioned
all the syntax labels into clusters according to the
frequency of syntax labels in synchronous rules. If
remove and move are as computationally inten-
sive as computing the change in F (C) in Equation
(12), then the time complexity of remove and
move is O(K) (Martin et al., 1998), where K is
the number of clusters. Since the remove proce-
dure is called once for each label and, for a given
label, the move procedure is called K − 1 times

167



Data Lang Training Development Test
sent src-tokens tgt-tokens sent tgt-tokens sent tgt-tokens

IWSLT07 J to E 40 K 483 K 369 K 500 7.4 K 489 3.7 K
FBIS C to E 302 K 2.7 M 3.4 M 1,664 47 K 919 30 K

NIST08 1 M 15 M 17 M

Table 2: Data sets: The “sent” column indicates the number of sentences. The “src-tokens” and “tgt-
tokens” columns indicate the number of words in the source- and the target-side sentences.

to find the maximum F (C), the worst-time com-
plexity for one iteration of the syntax-label clus-
tering is O(|N |K2). The exchange procedure is
continued until the cluster mapping is stable or the
number of iterations reaches a threshold value of
100.

4 Experiments

4.1 Data

We conducted experiments on Japanese-English
(ja-en) and Chinese-English (zh-en) translation
tasks. The ja-en data comes from IWSLT07
(Fordyce, 2007) in a spoken travel domain. The
tuning set has seven English references and the test
set has six English references. For zh-en data we
prepared two kind of data. The one is extracted
from FBIS3, which is a collection of news arti-
cles. The other is 1 M sentences extracted ron-
domly from NIST Open MT 2008 task (NIST08).
We use the NIST Open MT 2006 for tuning and
the MT 2003 for testing. The tuning and test sets
have four English references. Table 2 shows the
details for each corpus. Each corpus is tokenized,
put in lower-case, and sentences with over 40 to-
kens on either side are removed from the training
data. We use KyTea (Neubig et al., 2011) to to-
kenize the Japanese data and Stanford Word Seg-
menter (Tseng et al., 2005) to tokenize the Chinese
data. We parse the English data with the Berkeley
parser (Petrov and Klein, 2007).

4.2 Experiment design

We did experiments with the SAMT (Zollmann
and Venugopal, 2006) model with the Moses
(Koehn et al., 2007). For the SAMT model, we
conducted experiments with two label sets. One
is extracted from the phrase structure parses and
the other is extended with CCG4. We applied the
proposed method (+clustering) and the baseline
method (+coarsening), which uses the Hanneman

3LDC2003E14
4Using the relax-parse with option SAMT 4 for IWSLT07

and FBIS and SAMT 2 for NIST08 in the Moses

Label set Label Rule F(C) SD
parse 63 0.3 K - -
CCG 3,147 4.2 M - -
+ coarsening 80 2.4 M -3.8 e+08 249
+ clustering 80 3.8 M -7.2 e+07 73

Table 3: SAMT grammars on ja-en experiments

Label set Label Rule F(C) SD
FBIS
parse 70 2.1 M - -
CCG 5,460 60 M - -
+ coarsening 80 32 M -1.5 e+10 526
+ clustering 80 38 M -7.9 e+09 154

NIST08
parse 70 12 M - -
CCG 7,328 120 M - -
+ clustering 80 100 M -2.6 e+10 218

Table 4: SAMT grammars on zh-en experiments

label-collapsing algorithm described in Section 2,
for syntax-label clustering to the SAMT models
with CCG. The number of clusters for each clus-
tering was set to 80. The language models were
built using SRILM Toolkits (Stolcke, 2002). The
language model with the IWSLT07 is a 5-gram
model trained on the training data, and the lan-
guage model with the FBIS and NIST08 is a 5-
gram model trained on the Xinhua portion of En-
glish GigaWord. For word alignments, we used
MGIZA++ (Gao and Vogel, 2008). To tune the
weights for BLEU (Papineni et al., 2002), we used
the n-best batch MIRA (Cherry and Foster, 2012).

5 Results and analysis

Tables 3 and 4 present the details of SAMT gram-
mars with each label set learned by the exper-
iments using the IWSLT07 (ja-en), FBIS and
NIST08 (zh-en), which include the number of syn-
tax labels and synchronous rules, the values of the
objective (F (C)), and the standard deviation (SD)
of the number of labels assigned to each cluster.
For NIST08 we applied only the + clustering be-
cause the + coarsening needs a huge amount of
computation time. Table 5 shows the differences
between the BLEU score and the rule number for

168



each cluster number when using the IWSLT07
dataset.

Since the +clustering maximizes the likelihood
of synchronous rules, it can introduce appropriate
rules adapted to training data given a fixed number
of clusters. For each experiment, SAMT gram-
mars with the +clustering have a greater number
of rules than with the +coarsening and, as shown
in Table 5, the number of synchronous rules with
+clustering increase with the number of clusters.
For +clustering with eight clusters and +coars-
ening with 80 clusters, which have almost 2.4M
rules, the BLEU score of +clustering with eight
clusters is higher. Also, the SD of the number
of labels, which indicates the balance of the num-
ber of labels among clusters, with +clustering is
smaller than with +coarsening. These results sug-
gest that +clustering maintain a large-scale varia-
tion of synchronous rules for high performance by
balancing the number of labels in each cluster.

The number of synchronous rules grows as you
progress from +coarsening to +clustering and fi-
nally to raw label with CCG. To confirm the ef-
fect of the number of rules, we measured the de-
coding time per sentence for translating the test
set by taking the average of ten runs with FBIS
corpus. +coarsening takes 0.14 s and +clustering
takes 0.16 s while raw label with CCG takes 0.37s.
Thus the increase in the number of synchronous
rules adversely affects the decoding speed.

Table 6 presents the results for the experiments5

using ja-en and zh-en with the BLEU metric.
SAMT with parse have the lowest BLEU scores.
It appears that the linguistic information of the
raw syntax labels of the phrase structure parses
is not enough to improve the translation perfor-
mance. Hiero has the higher BLEU score than
SAMT with CCG on zh-en. This is likely due to
the low accuracy of the parses, on which SAMT
relies while Hiero doesn’t. SAMT with + clus-
tering have the higher BLEU score than raw label
with CCG. For SAMT with CCG using IWSLT07
and FBIS, though the statistical significance tests
were not significant when p < 0.05, +clustering
have the higher BLEU scores than +coarsening.
For these results, the performance of +clustering
is comparable to that of +coarsening. For the
complexity of both clustering algorithm, though it
is difficult to evaluate directly because the speed

5As another baseline, we also used Phrase-based SMT
(Koehn et al., 2003) and Hiero (Chiang, 2007).

+clustering +coarsening
Cluster 80 40 8 4 80

BLEU 50.21 49.49 49.96 50.25 49.54
Rule 3.8 M 3.5 M 2.4 M 2.2 M 2.4 M

Table 5: BLEU score and rule number for each
cluster number using IWSLT07

ja-en zh-en
Model parse CCG parse CCG parse CCG

SAMT 42.58 48.77 23.66 26.97 24.67 27.28
+coarsening - 49.54 - 27.12 - -
+clustering - 50.21 - 27.47 - 27.29

Hiero 48.91 28.31 27.62
PB-SMT 49.14 26.88 26.71

Table 6: BLEU scores on each experiments

depends on how each algorithm is implemented,
+clustering is an order of magnitude faster than
+coarsening. For the clustering experiment that
groups 5460 raw labels with CCG into 80 clus-
ters using FBIS corpus, +coarsening takes about
1 week whereas +clustering takes about 10 min-
utes.

6 Conclusion

In this paper, we propose syntax-label clustering
for SAMT, which uses syntax-label information to
generate syntactically correct translations. One of
the problems of SAMT is the large grammar size
when a CCG-style extended label set is used in the
grammar, which make decoding slower. We clus-
ter syntax labels with a very fast exchange algo-
rithm in which the generative probabilities of syn-
chronous rules are maximized. We demonstrate
the effectiveness of the proposed method by us-
ing it to translate Japanese-English and Chinese-
English tasks and measuring the decoding speed,
the accuracy and the clustering speed. Future work
involves improving the optimization criterion. We
expect to make a new objective that includes the
terminal symbols and the reordering of nontermi-
nal symbols that were ignored in this work. An-
other interesting direction is to determine the ap-
propriate number of clusters for each corpus and
the initialization method for clustering.

Acknowledgments

We thank the anonymous reviewers for their sug-
gestions and helpful comments on the early ver-
sion of this paper.

169



References
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-

ouza, Jenifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.

Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427–436, Montréal, Canada, June. Association for
Computational Linguistics.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 201–228,
June.

David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1443–1452, Uppsala, Sweden, July.
Association for Computational Linguistics.

Cameron Shaw Fordyce. 2007. Overview of the 4th
international workshop on spoken language transla-
tion iwslt 2007 evaluation campaign. In In Proceed-
ings of IWSLT 2007, pages 1–12, Trento, Italy, Oc-
tober.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961–968, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.

Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49–57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.

Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 288–297, Atlanta, Geor-
gia, June. Association for Computational Linguis-
tics.

Bryant Huang and Kevin Knight. 2006. Relabeling
syntax trees to improve syntax-based machine trans-
lation quality. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 240–247, New York City, USA,
June. Association for Computational Linguistics.

Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In In

Proceedings of HLT-NAACL, pages 48–54, Edmon-
ton, Canada, May/July.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Republic,
June. Association for Computational Linguistics.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609–616, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.

Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 19–37.

Markos Mylonakis and Khalil Sima’an. 2011. Learn-
ing hierarchical translation structure with linguis-
tic annotations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
642–652, Portland, Oregon, USA, June. Association
for Computational Linguistics.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 529–533, Portland, Oregon, USA, June.
Association for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.

Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Association for Computational Linguistics.

Mark Steedman. 2000. The syntactic process, vol-
ume 27. MIT Press.

Andreas Stolcke and Stephen Omohundro. 1994. In-
ducing probabilistic grammars by bayesian model

170



merging. In R. C. Carrasco and J. Oncina, editors,
Grammatical Inference and Applications (ICGI-94),
pages 106–118. Berlin, Heidelberg.

Andreas Stolcke. 2002. Srilm an extensible language
modeling toolkit. In In Proceedings of the Seventh
International Conference on Spoken Language Pro-
cessing, pages 901–904.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter. In Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 168–171. Jeju Island, Korea.

Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Pro-
ceedings of ACL-08: HLT, pages 755–762, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.

Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138–141, New York City,
June. Association for Computational Linguistics.

171


