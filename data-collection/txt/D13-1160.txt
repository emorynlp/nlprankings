










































Semantic Parsing on Freebase from Question-Answer Pairs


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Semantic Parsing on Freebase from Question-Answer Pairs

Jonathan Berant Andrew Chou Roy Frostig Percy Liang
Computer Science Department, Stanford University

{joberant,akchou}@stanford.edu {rf,pliang}@cs.stanford.edu

Abstract

In this paper, we train a semantic parser that
scales up to Freebase. Instead of relying on
annotated logical forms, which is especially
expensive to obtain at large scale, we learn
from question-answer pairs. The main chal-
lenge in this setting is narrowing down the
huge number of possible logical predicates for
a given question. We tackle this problem in
two ways: First, we build a coarse mapping
from phrases to predicates using a knowledge
base and a large text corpus. Second, we
use a bridging operation to generate additional
predicates based on neighboring predicates.
On the dataset of Cai and Yates (2013), despite
not having annotated logical forms, our sys-
tem outperforms their state-of-the-art parser.
Additionally, we collected a more realistic and
challenging dataset of question-answer pairs
and improves over a natural baseline.

1 Introduction

We focus on the problem of semantic parsing nat-
ural language utterances into logical forms that can
be executed to produce denotations. Traditional se-
mantic parsers (Zelle and Mooney, 1996; Zettle-
moyer and Collins, 2005; Wong and Mooney, 2007;
Kwiatkowski et al., 2010) have two limitations: (i)
they require annotated logical forms as supervision,
and (ii) they operate in limited domains with a small
number of logical predicates. Recent developments
aim to lift these limitations, either by reducing the
amount of supervision (Clarke et al., 2010; Liang et
al., 2011; Goldwasser et al., 2011; Artzi and Zettle-
moyer, 2011) or by increasing the number of logical

Occidental College, Columbia University

Execute on Database

Type.University u Education.BarackObama

Type.University

Education

BarackObama

Which college did Obama go to ?

alignment

alignment

bridging

Figure 1: Our task is to map questions to answers via la-
tent logical forms. To narrow down the space of logical
predicates, we use a (i) coarse alignment based on Free-
base and a text corpus and (ii) a bridging operation that
generates predicates compatible with neighboring predi-
cates.

predicates (Cai and Yates, 2013). The goal of this
paper is to do both: learn a semantic parser with-
out annotated logical forms that scales to the large
number of predicates on Freebase.

At the lexical level, a major challenge in semantic
parsing is mapping natural language phrases (e.g.,
“attend”) to logical predicates (e.g., Education).
While limited-domain semantic parsers are able
to learn the lexicon from per-example supervision
(Kwiatkowski et al., 2011; Liang et al., 2011), at
large scale they have inadequate coverage (Cai and
Yates, 2013). Previous work on semantic parsing on
Freebase uses a combination of manual rules (Yahya
et al., 2012; Unger et al., 2012), distant supervision
(Krishnamurthy and Mitchell, 2012), and schema

1533



matching (Cai and Yates, 2013). We use a large
amount of web text and a knowledge base to build a
coarse alignment between phrases and predicates—
an approach similar in spirit to Cai and Yates (2013).

However, this alignment only allows us to gen-
erate a subset of the desired predicates. Aligning
light verbs (e.g., “go”) and prepositions is not very
informative due to polysemy, and rare predicates
(e.g., “cover price”) are difficult to cover even given
a large corpus. To improve coverage, we propose
a new bridging operation that generates predicates
based on adjacent predicates rather than on words.

At the compositional level, a semantic parser must
combine the predicates into a coherent logical form.
Previous work based on CCG requires manually
specifying combination rules (Krishnamurthy and
Mitchell, 2012) or inducing the rules from anno-
tated logical forms (Kwiatkowski et al., 2010; Cai
and Yates, 2013). We instead define a few simple
composition rules which over-generate and then use
model features to simulate soft rules and categories.
In particular, we use POS tag features and features
on the denotations of the predicted logical forms.

We experimented with two question answering
datasets on Freebase. First, on the dataset of Cai
and Yates (2013), we showed that our system out-
performs their state-of-the-art system 62% to 59%,
despite using no annotated logical forms. Second,
we collected a new realistic dataset of questions by
performing a breadth-first search using the Google
Suggest API; these questions are then answered by
Amazon Mechanical Turk workers. Although this
dataset is much more challenging and noisy, we are
still able to achieve 31.4% accuracy, a 4.5% ab-
solute improvement over a natural baseline. Both
datasets as well as the source code for SEMPRE, our
semantic parser, are publicly released and can be
downloaded from http://nlp.stanford.edu/
software/sempre/.

2 Setup

Problem Statement Our task is as follows: Given
(i) a knowledge base K, and (ii) a training set of
question-answer pairs {(xi, yi)}ni=1, output a se-
mantic parser that maps new questions x to answers
y via latent logical forms z and the knowledge base
K.

2.1 Knowledge base

Let E denote a set of entities (e.g., BarackObama),
and let P denote a set of properties (e.g.,
PlaceOfBirth). A knowledge base K is a set
of assertions (e1, p, e2) ∈ E × P × E (e.g.,
(BarackObama, PlaceOfBirth, Honolulu)).

We use the Freebase knowledge base (Google,
2013), which has 41M non-numeric entities, 19K
properties, and 596M assertions.1

2.2 Logical forms

To query the knowledge base, we use a logical lan-
guage called Lambda Dependency-Based Compo-
sitional Semantics (λ-DCS)—see Liang (2013) for
details. For the purposes of this paper, we use a re-
stricted subset called simple λ-DCS, which we will
define below for the sake of completeness.

The chief motivation of λ-DCS is to produce
logical forms that are simpler than lambda cal-
culus forms. For example, λx.∃a.p1(x, a) ∧
∃b.p2(a, b) ∧ p3(b, e) is expressed compactly in
λ-DCS as p1.p2.p3.e. Like DCS (Liang et al.,
2011), λ-DCS makes existential quantification im-
plicit, thereby reducing the number of variables.
Variables are only used for anaphora and building
composite binary predicates; these do not appear in
simple λ-DCS.

Each logical form in simple λ-DCS is either a
unary (which denotes a subset of E) or a binary
(which denotes a subset of E × E). The basic λ-
DCS logical forms z and their denotations JzKK are
defined recursively as follows:
• Unary base case: If e ∈ E is an entity (e.g.,
Seattle), then e is a unary logical form with
JzKK = {e}.
• Binary base case: If p ∈ P is a property (e.g.,
PlaceOfBirth), then p is a binary logical form
with JpKK = {(e1, e2) : (e1, p, e2) ∈ K}.2
• Join: If b is a binary and u is a unary, then b.u

(e.g., PlaceOfBirth.Seattle) is a unary de-
noting a join and project: Jb.uKK = {e1 ∈ E :
∃e2.(e1, e2) ∈ JbKK ∧ e2 ∈ JuKK}.

1In this paper, we condense Freebase names for readability
(/people/person becomes Person).

2Binaries can be also built out of lambda abstractions (e.g.,
λx.Performance.Actor.x), but as these constructions are
not central to this paper, we defer to (Liang, 2013).

1534



• Intersection: If u1 and u2 are both unaries,
then u1 u u2 (e.g., Profession.Scientist u
PlaceOfBirth.Seattle) denotes set intersec-
tion: Ju1 u u2KK = Ju1KK ∩ Ju2KK.
• Aggregation: If u is a unary, then count(u)

denotes the cardinality: Jcount(u)KK =
{|JuKK|}.

As a final example, “number of dramas star-
ring Tom Cruise” in lambda calculus would
be represented as count(λx.Genre(x, Drama) ∧
∃y.Performance(x, y) ∧ Actor(y, TomCruise));
in λ-DCS, it is simply count(Genre.Drama u
Performance.Actor.TomCruise).

It is useful to think of the knowledge base K as
a directed graph in which entities are nodes and
properties are labels on the edges. Then simple λ-
DCS unary logical forms are tree-like graph patterns
which pick out a subset of the nodes.

2.3 Framework

Given an utterance x, our semantic parser constructs
a distribution over possible derivations D(x). Each
derivation d ∈ D(x) is a tree specifying the appli-
cation of a set of combination rules that culminates
in the logical form d.z at the root of the tree—see
Figure 2 for an example.

Composition Derivations are constructed recur-
sively based on (i) a lexicon mapping natural lan-
guage phrases to knowledge base predicates, and (ii)
a small set of composition rules.

More specifically, we build a set of derivations for
each span of the utterance. We first use the lexicon to
generate single-predicate derivations for any match-
ing span (e.g., “born” maps to PeopleBornHere).
Then, given any logical form z1 that has been con-
structed over the span [i1 : j1] and z2 over a non-
overlapping span [i2 : j2], we generate the following
logical forms over the enclosing span [min(i1, i2) :
max(j1, j2)]: intersection z1 u z2, join z1.z2, ag-
gregation z1(z2) (e.g., if z1 = count), or bridging
z1 u p.z2 for any property p ∈ P (explained more in
Section 3.2).3

Note that the construction of derivations D(x)
allows us to skip any words, and in general heav-

3We also discard logical forms are incompatible according
to the Freebase types (e.g., Profession.Politician u
Type.City would be rejected).

Type.Locationu PeopleBornHere.BarackObama

Type.Location

where

was PeopleBornHere.BarackObama

BarackObama

Obama

PeopleBornHere

born

?
join

intersection

lexicon

lexicon lexicon

Figure 2: An example of a derivation d of the utterance
“Where was Obama born?” and its sub-derivations, each
labeled with composition rule (in blue) and logical form
(in red). The derivation d skips the words “was” and “?”.

ily over-generates. We instead rely on features and
learning to guide us away from the bad derivations.

Modeling Following Zettlemoyer and Collins
(2005) and Liang et al. (2011), we define a
discriminative log-linear model over derivations
d ∈ D(x) given utterances x: pθ(d | x) =

exp{φ(x,d)>θ}∑
d′∈D(x) exp{φ(x,d′)>θ}

, where φ(x, d) is a feature

vector extracted from the utterance and the deriva-
tion, and θ ∈ Rb is the vector of parameters to
be learned. As our training data consists only of
question-answer pairs (xi, yi), we maximize the log-
likelihood of the correct answer (Jd.zKK = yi), sum-
ming over the latent derivation d. Formally, our
training objective is

O(θ) =
n∑
i=1

log
∑

d∈D(x):Jd.zKK=yi

pθ(d | xi). (1)

Section 4 describes an approximation of this ob-
jective that we maximize to choose parameters θ.

3 Approach

Our knowledge base has more than 19,000 proper-
ties, so a major challenge is generating a manage-
able set of predicates for an utterance. We propose
two strategies for doing this. First (Section 3.1),
we construct a lexicon that maps natural language
phrases to logical predicates by aligning a large text
corpus to Freebase, reminiscent of Cai and Yates
(2013). Second, we generate logical predicates com-
patible with neighboring predicates using the bridg-
ing operation (Section 3.2). Bridging is crucial when
aligning phrases is difficult or even impossible. The
derivations produced by combining these predicates

1535



grew up in[Person,Location]

born in[Person,Date]

married in[Person,Date]

born in[Person,Location]

DateOfBirth

PlaceOfBirth

Marriage.StartDate

PlacesLived.Location

(BarackObama,Honolulu)

(MichelleObama,Chicago)

(BarackObama,Chicago)(RandomPerson,Seattle)

F(r1) F(r2)

C(r1, r2)

Alignment features

log-phrase-count:log(15765)

log-predicate-count: log(9182)

log-intersection-count: log(6048)

KB-best-match: 0

Figure 3: We construct a bipartite graph over phrasesR1
and predicates R2. Each edge (r1, r2) is associated with
alignment features.

are scored using features that capture lexical, syn-
tactic and semantic regularities (Section 3.3).

3.1 Alignment

We now discuss the construction of a lexicon L,
which is a mapping from natural language phrases
to logical predicates accompanied by a set of fea-
tures. Specifically, for a phrase w (e.g., “born in”),
L(w) is a set of entries (z, s), where z is a predicate
and s is the set of features. A lexicon is constructed
by alignment of a large text corpus to the knowledge
base (KB). Intuitively, a phrase and a predicate align
if they co-occur with many of the same entities.

Here is a summary of our alignment proce-
dure: We construct a set of typed4 phrases
R1 (e.g., “born in”[Person,Location]) and pred-
icates R2 (e.g., PlaceOfBirth). For each
r ∈ R1 ∪ R2, we create its extension
F(r), which is a set of co-occurring entity-
pairs (e.g., F(“born in”[Person,Location]) =
{(BarackObama, Honolulu), . . . }. The lexicon is
generated based on the overlap F(r1) ∩ F(r2), for
r1 ∈ R1 and r2 ∈ R2.

Typed phrases 15 million triples (e1, r, e2) (e.g.,
(“Obama”, “was also born in”, “August 1961”))

4Freebase associates each entity with a set of types using the
Type property.

were extracted from ClueWeb09 using the ReVerb
open IE system (Fader et al., 2011). Lin et al. (2012)
released a subset of these triples5 where they were
able to substitute the subject arguments with KB en-
tities. We downloaded their dataset and heuristically
replaced object arguments with KB entities by walk-
ing on the Freebase graph from subject KB entities
and performing simple string matching. In addition,
we normalized dates with SUTime (Chang and Man-
ning, 2012).

We lemmatize and normalize each text phrase
r ∈ R1 and augment it with a type signature
[t1, t2] to deal with polysemy (“born in” could ei-
ther map to PlaceOfBirth or DateOfBirth). We
add an entity pair (e1, e2) to the extension of
F(r[t1, t2]) if the (Freebase) type of e1 (e2) is t1
(t2). For example, (BarackObama, 1961) is added
to F(“born in”[Person, Date]). We perform a simi-
lar procedure that uses a Hearst-like pattern (Hearst,
1992) to map phrases to unary predicates. If a
text phrase r ∈ R1 matches the pattern “(is|was
a|the) x IN”, where IN is a preposition, then we
add e1 to F(x). For (Honolulu, “is a city in”,
Hawaii), we extract x = “city ′′ and add Honolulu
to F(“city”). From the initial 15M triples, we ex-
tracted 55,081 typed binary phrases (9,456 untyped)
and 6,299 unary phrases.

Logical predicates Binary logical predicates con-
tain (i) all KB properties6 and (ii) concatenations of
two properties p1.p2 if the intermediate type repre-
sents an event (e.g., the married to relation is rep-
resented by Marriage.Spouse). For unary pred-
icates, we consider all logical forms Type.t and
Profession.t for all (abstract) entities t ∈ E (e.g.
Type.Book and Profession.Author). The types
of logical predicates considered during alignment is
restricted in this paper, but automatic induction of
more compositional logical predicates is an interest-
ing direction. Finally, we define the extension of a
logical predicate r ∈ R2 to be its denotation, that is,
the corresponding set of entities or entity pairs.

Lexicon construction Given typed phrases R1,
logical predicates R2, and their extensions F , we
now generate the lexicon. It is useful to think of a

5http://knowitall.cs.washington.edu/
linked_extractions/

6We filter properties from the domains user and base.

1536



Category Description
Alignment Log of # entity pairs that occur with the

phrase r1 (|F(r1)|)
Log of # entity pairs that occur with the
logical predicate r2 (|F(r2)|)
Log of # entity pairs that occur with both
r1 and r2 (|F(r1) ∩ F(r2)|)
Whether r2 is the best match for r1 (r2 =
arg maxr |F(r1) ∩ F(r)|)

Lexicalized Conjunction of phrase w and predicate z
Text similarity Phrase r1 is equal/prefix/suffix of s2

Phrase overlap of r1 and s2
Bridging Log of # entity pairs that occur with bridg-

ing predicate b (|F(b)|)
Kind of bridging (# unaries involved)
The binary b injected

Composition # of intersect/join/bridging operations
POS tags in join/bridging and skipped
words
Size of denotation of logical form

Table 1: Full set of features. For the alignment and text sim-
ilarity, r1 is a phrase, r2 is a predicate with Freebase name s2,
and b is a binary predicate with type signature (t1, t2).

bipartite graph with left nodes R1 and right nodes
R2 (Figure 3). We add an edge (r1, r2) if (i) the
type signatures of r1 and r2 match7 and (ii) their ex-
tensions have non-empty overlap (F(r1)∩F(r2) 6=
∅). Our final graph contains 109K edges for binary
predicates and 294K edges for unary predicates.

Naturally, non-zero overlap by no means guaran-
tees that r1 should map to r2. In our noisy data,
even “born in” and Marriage.EndDate co-occur 4
times. Rather than thresholding based on some cri-
terion, we compute a set of features, which are used
by the model downstream in conjunction with other
sources of information.

We compute three types of features (Table 1).
Alignment features are unlexicalized and measure
association based on argument overlap. Lexicalized
features are standard conjunctions of the phrase w
and the logical form z. Text similarity features com-
pare the (untyped) phrase (e.g., “born”) to the Free-
base name of the logical predicate (e.g., “People
born here”): Given the phrase r1 and the Freebase
name s2 of the predicate r2, we compute string sim-
ilarity features such as whether r1 and s2 are equal,

7Each Freebase property has a designated type signa-
ture, which can be extended to composite predicates, e.g.,
sig(Marriage.StartDate) = (Person,Date).

as well as some other measures of token overlap.

3.2 Bridging
While alignment can cover many predicates, it is un-
reliable for cases where the predicates are expressed
weakly or implicitly. For example, in “What govern-
ment does Chile have?”, the predicate is expressed
by the light verb have, in “What actors are in Top
Gun?”, it is expressed by a highly ambiguous prepo-
sition, and in “What is Italy money?” [sic], it is
omitted altogether. Since natural language doesn’t
offer much help here, let us turn elsewhere for guid-
ance. Recall that at this point our main goal is to
generate a manageable set of candidate logical forms
to be scored by the log-linear model.

In the first example, suppose the phrases “Chile”
and “government” are parsed as Chile and
Type.FormOfGovernment, respectively, and we hy-
pothesize a connecting binary. The two predicates
impose strong type constraints on that binary, so we
can afford to generate all the binary predicates that
type check (see Table 2). More formally, given two
unaries z1 and z2 with types t1 and t2, we generate a
logical form z1 u b.z2 for each binary b whose type
signature is (t1, t2). Figure 1 visualizes bridging of
the unaries Type.University and Obama.

Now consider the example “What is the
cover price of X-men?” Here, the binary
ComicBookCoverPrice is expressed explicitly, but
is not in our lexicon since the language use is rare.
To handle this, we allow bridging to generate a bi-
nary based on a single unary; in this case, based on
the unary X-Men (Table 2), we generate several bina-
ries including ComicBookCoverPrice. Generically,
given a unary z with type t, we construct a logical
form b.z for any predicate b with type (∗, t).

Finally, consider the question “Who did
Tom Cruise marry in 2006?”. Suppose we
parse the phrase “Tom Cruise marry” into
Marriage.Spouse.TomCruise, or more explicitly,
λx.∃e.Marriage(x, e) ∧ Spouse(e, TomCruise).
Here, the neo-Davidsonian event variable e is an
intermediate quantity, but needs to be further mod-
ified (in this case, by the temporal modifier 2006).
To handle this, we apply bridging to a unary and the
intermediate event (see Table 2). Generically, given
a logical form p1.p2.z′ where p2 has type (t1, ∗),
and a unary z with type t, bridging injects z and

1537



# Form 1 Form 2 Bridging
1 Type.FormOfGovernment Chile Type.FormOfGovernmentu GovernmentTypeOf.Chile
2 X-Men ComicBookCoverPriceOf.X-Men
3 Marriage.Spouse.TomCruise 2006 Marriage.(Spouse.TomCruise u StartDate.2006)

Table 2: Three examples of the bridging operation. The bridging binary predicate b is in boldface.

constructs a logical form p1.(p2.z′ u b.z) for each
logical predicate b with type (t1, t).

In each of the three examples, bridging gener-
ates a binary predicate based on neighboring logi-
cal predicates rather than on explicit lexical material.
In a way, our bridging operation shares with bridg-
ing anaphora (Clark, 1975) the idea of establishing
a novel relation between distinct parts of a sentence.
Naturally, we need features to distinguish between
the generated predicates, or decide whether bridging
is even appropriate at all. Given a binary b, features
include the log of the predicate count log |F(b)|, in-
dicators for the kind of bridging, an indicator on the
binary b for injections (Table 1). In addition, we add
all text similarity features by comparing the Free-
base name of b with content words in the question.

3.3 Composition

So far, we have mainly focused on the generation of
predicates. We now discuss three classes of features
pertaining to their composition.

Rule features Each derivation d is the result of ap-
plying some number of intersection, join, and bridg-
ing operations. To control this number, we define
indicator features on each of these counts. This is in
contrast to the norm of having a single feature whose
value is equal to the count, which can only repre-
sent one-sided preferences for having more or fewer
of a given operation. Indicator features stabilize the
model, preferring derivations with a well-balanced
inventory of operations.

Part-of-speech tag features To guide the compo-
sition of predicates, we use POS tags in two ways.
First, we introduce features indicating when a word
of a given POS tag is skipped, which could capture
the fact that skipping auxiliaries is generally accept-
able, while skipping proper nouns is not. Second,
we introduce features on the POS tags involved in a
composition, inspired by dependency parsing (Mc-
Donald et al., 2005). Specifically, when we combine

logical forms z1 and z2 via a join or bridging, we
include a feature on the POS tag of (the first word
spanned by) z1 conjoined with the POS tag corre-
sponding to z2. Rather than using head-modifier in-
formation from dependency trees (Branavan et al.,
2012; Krishnamurthy and Mitchell, 2012; Cai and
Yates, 2013; Poon, 2013), we can learn the appro-
priate relationships tailored for downstream accu-
racy. For example, the phrase “located” is aligned
to the predicate ContainedBy. POS features can de-
tect that if “located” precedes a noun phrase (“What
is located in Beijing?”), then the noun phrase is the
object of the predicate, and if it follows the noun
phrase (“Where is Beijing located?”), then it is in
subject position.

Note that our three operations (intersection, join,
and bridging) are quite permissive, and we rely on
features, which encode soft, overlapping rules. In
contrast, CCG-based methods (Kwiatkowski et al.,
2010; Kwiatkowski et al., 2011) encode the com-
bination preferences structurally in non-overlapping
rules; these could be emulated with features with
weights clamped to −∞.

Denotation features While it is clear that learning
from denotations rather than logical forms is a draw-
back since it provides less information, it is less ob-
vious that working with denotations actually gives
us additional information. Specifically, we include
four features indicating whether the denotation of
the predicted logical form has size 0, 1, 2, or at least
3. This feature encodes presupposition constraints
in a soft way: when people ask a question, usually
there is an answer and it is often unique. This allows
us to favor logical forms with this property.

4 Experiments

We now evaluate our semantic parser empirically.
In Section 4.1, we compare our approach to Cai
and Yates (2013) on their recently released dataset
(henceforth, FREE917) and present results on a new

1538



dataset that we collected (henceforth, WEBQUES-
TIONS). In Section 4.2, we provide detailed experi-
ments to provide additional insight on our system.

Setup We implemented a standard beam-based
bottom-up parser which stores the k-best derivations
for each span. We use k = 500 for all our experi-
ments on FREE917 and k = 200 on WEBQUES-
TIONS. The root beam yields the candidate set D̃(x)
and is used to approximate the sum in the objective
functionO(θ) in (1). In experiments on WEBQUES-
TIONS, D̃(x) contained 197 derivations on average.

We write the approximate objective as O(θ; θ̃) =∑
i log

∑
d∈D̃(xi;θ̃):Jd.zKK=yi

p(d | xi; θ) to explic-
itly show dependence on the parameters θ̃ used for
beam search. We optimize the objective by initial-
izing θ0 to 0 and applying AdaGrad (stochastic gra-
dient ascent with per-feature adaptive step size con-
trol) (Duchi et al., 2010), so that θt+1 is set based on
taking a stochastic approximation of ∂O(θ;θt)∂θ

∣∣
θ=θt

.
We make six passes over the training examples.

We used POS tagging and named-entity recogni-
tion to restrict what phrases in the utterance could
be mapped by the lexicon. Entities must be named
entities, proper nouns or a sequence of at least two
tokens. Unaries must be a sequence of nouns, and
binaries must be either a content word, or a verb fol-
lowed by either a noun phrase or a particle. In addi-
tion, we used 17 hand-written rules to map question
words such as “where” and “how many” to logical
forms such as Type.Location and Count.

To compute denotations, we convert a logical
form z into a SPARQL query and execute it on our
copy of Freebase using the Virtuoso engine. On
WEBQUESTIONS, a full run over the training exam-
ples involves approximately 600,000 queries. For
evaluation, we predict the answer from the deriva-
tion with highest probability.

4.1 Main results
4.1.1 FREE917

Cai and Yates (2013) created a dataset consist-
ing of 917 questions involving 635 Freebase rela-
tions, annotated with lambda calculus forms. We
converted all 917 questions into simple λ-DCS, ex-
ecuted them on Freebase and used the resulting an-
swers to train and evaluate. To map phrases to Free-
base entities we used the manually-created entity

lexicon used by Cai and Yates (2013), which con-
tains 1,100 entries. Because entity disambiguation
is a challenging problem in semantic parsing, the en-
tity lexicon simplifies the problem.

Following Cai and Yates (2013), we held out 30%
of the examples for the final test, and performed all
development on the remaining 70%. During devel-
opment, we split the data and used 512 examples
(80%) for training and the remaining 129 (20%) for
validation. All reported development numbers are
averaged across 3 random splits. We evaluated us-
ing accuracy, the fraction of examples where the pre-
dicted answer exactly matched the correct answer.

Our main empirical result is that our system,
which was trained only on question-answer pairs,
obtained 62% accuracy on the test set, outperform-
ing the 59% accuracy reported by Cai and Yates
(2013), who trained on full logical forms.

4.1.2 WEBQUESTIONS
Dataset collection Because FREE917 requires
logical forms, it is difficult to scale up due to the
required expertise of annotating logical forms. We
therefore created a new dataset, WEBQUESTIONS,
of question-answer pairs obtained from non-experts.

To collect this dataset, we used the Google Sug-
gest API to obtain questions that begin with a wh-
word and contain exactly one entity. We started with
the question “Where was Barack Obama born?”
and performed a breadth-first search over questions
(nodes), using the Google Suggest API supplying
the edges of the graph. Specifically, we queried the
question excluding the entity, the phrase before the
entity, or the phrase after it; each query generates 5
candidate questions, which are added to the queue.
We iterated until 1M questions were visited; a ran-
dom 100K were submitted to Amazon Mechanical
Turk (AMT).

The AMT task requested that workers answer the
question using only the Freebase page of the ques-
tions’ entity, or otherwise mark it as unanswerable
by Freebase. The answer was restricted to be one of
the possible entities, values, or list of entities on the
page. As this list was long, we allowed the user to
filter the list by typing. We paid the workers $0.03
per question. Out of 100K questions, 6,642 were
annotated identically by at least two AMT workers.

We again held out a 35% random subset of the

1539



Dataset # examples # word types
GeoQuery 880 279
ATIS 5,418 936
FREE917 917 2,036
WEBQUESTIONS 5,810 4,525

Table 3: Statistics on various semantic parsing datasets. Our
new dataset, WEBQUESTIONS, is much larger than FREE917
and much more lexically diverse than ATIS.

questions for the final test, and performed all devel-
opment on the remaining 65%, which was further
divided into an 80%–20% split for training and val-
idation. To map entities, we built a Lucene index
over the 41M Freebase entities.

Table 3 provides some statistics about the new
questions. One major difference in the datasets is
the distribution of questions: FREE917 starts from
Freebase properties and solicits questions about
these properties; these questions tend to be tai-
lored to the properties. WEBQUESTIONS starts from
questions completely independent of Freebase, and
therefore the questions tend to be more natural and
varied. For example, for the Freebase property
ComicGenre, FREE917 contains the question “What
genre is Doonesbury?”, while WEBQUESTIONS for
the property MusicGenre contains “What music did
Beethoven compose?”.

The number of word types in WEBQUESTIONS is
larger than in datasets such as ATIS and GeoQuery
(Table 3), making lexical mapping much more chal-
lenging. On the other hand, in terms of structural
complexity WEBQUESTIONS is simpler and many
questions contain a unary, a binary and an entity.

In some questions, the answer provided by AMT
workers is only roughly accurate, because workers
are restricted to selecting answers from the Freebase
page. For example, the answer given by workers to
the question “What is James Madison most famous
for?” is “President of the United States” rather than
“Authoring the Bill of Rights”.

Results AMT workers sometimes provide partial
answers, e.g., the answer to “What movies does Tay-
lor Lautner play in?” is a set of 17 entities, out
of which only 10 appear on the Freebase page. We
therefore allow partial credit and score an answer us-
ing the F1 measure, comparing the predicted set of
entities to the annotated set of entities.

System FREE917 WebQ.
ALIGNMENT 38.0 30.6
BRIDGING 66.9 21.2
ALIGNMENT+BRIDGING 71.3 32.9

Table 4: Accuracies on the development set under different
schemes of binary predicate generation. In ALIGNMENT, bi-
naries are generated only via the alignment lexicon. In BRIDG-
ING, binaries are generated through the bridging operation only.
ALIGNMENT+BRIDGING corresponds to the full system.

As a baseline, we omit from our system the main
contributions presented in this paper—that is, we
disallow bridging, and remove denotation and align-
ment features. The accuracy on the test set of this
system is 26.9%, whereas our full system obtains
31.4%, a significant improvement.

Note that the number of possible derivations for
questions in WEBQUESTIONS is quite large. In the
question “What kind of system of government does
the United States have?” the phrase “United States”
maps to 231 entities in our lexicon, the verb “have”
maps to 203 binaries, and the phrases “kind”, “sys-
tem”, and “government” all map to many different
unary and binary predicates. Parsing correctly in-
volves skipping some words, mapping other words
to predicates, while resolving many ambiguities in
the way that the various predicates can combine.

4.2 Detailed analysis

We now delve deeper to explore the contributions of
the various components of our system. All ablation
results reported next were run on the development
set (over 3 random splits).

Generation of binary predicates Recall that our
system has two mechanisms for suggesting binaries:
from the alignment lexicon or via the bridging op-
eration. Table 4 shows accuracies when only one or
both is used. Interestingly, alignment alone is better
than bridging alone on WEBQUESTIONS, whereas
for FREE917, it is the opposite. The reason for this
is that FREE917 contains questions on rare pred-
icates. These are often missing from the lexicon,
but tend to have distinctive types and hence can be
predicted from neighboring predicates. In contrast,
WEBQUESTIONS contains questions that are com-
monly searched for and focuses on popular predi-
cates, therefore exhibiting larger lexical variation.

1540



System FREE917 WebQ.
FULL 71.3 32.9
-POS 70.5 28.9
-DENOTATION 58.6 28.0

Table 5: Accuracies on the development set with features re-
moved. POS and DENOTATION refer to the POS tag and deno-
tation features from Section 3.3.

System FREE917 WebQ.
ALIGNMENT 71.3 32.9
LEXICALIZED 68.5 34.2
LEXICALIZED+ALIGNMENT 69.0 36.4

Table 6: Accuracies on the development set using either
unlexicalized alignment features (ALIGNMENT) or lexicalized
features (LEXICALIZED).

For instance, when training without an align-
ment lexicon, the system errs on “When did Nathan
Smith die?”. Bridging suggests binaries that are
compatible with the common types Person and
Datetime, and the binary PlaceOfBirth is cho-
sen. On the other hand, without bridging, the sys-
tem errs on “In which comic book issue did Kitty
Pryde first appear?”, which refers to the rare pred-
icate ComicBookFirstAppearance. With bridging,
the parser can identify the correct binary, by linking
the types ComicBook and ComicBookCharacter. On
both datasets, best performance is achieved by com-
bining the two sources of information.

Overall, running on WEBQUESTIONS, the parser
constructs derivations that contain about 12,000 dis-
tinct binary predicates.

Feature variations Table 5 shows the results of
feature ablation studies. Accuracy drops when POS
tag features are omitted, e.g., in the question “What
number is Kevin Youkilis on the Boston Red Sox” the
parser happily skips the NNPs “Kevin Youkilis” and
returns the numbers of all players on the Boston Red
Sox. A significant loss is incurred without denota-
tion features, largely due to the parser returning log-
ical forms with empty denotations. For instance, the
question “How many people were at the 2006 FIFA
world cup final?” is answered with a logical form
containing the property PeopleInvolved rather than
SoccerMatchAttendance, resulting in an empty de-
notation.

Next we study the impact of lexicalized versus

0 iterations 1 iterations 2 iterations

Figure 4: Beam of candidate derivations D̃(x) for 50
WEBQUESTIONS examples. In each matrix, columns
correspond to examples and rows correspond to beam po-
sition (ranked by decreasing model score). Green cells
mark the positions of derivations with correct denota-
tions. Note that both the number of good derivations and
their positions improve as θ is optimized.

 0

 0.2

 0.4

 0.6

 0.8

 1

 0  100  200  300  400  500

oracle
accuracy

(a) FREE917

 0

 0.2

 0.4

 0.6

 0.8

 1

 0  100  200

oracle
accuracy

(b) WEBQUESTIONS

Figure 5: Accuracy and oracle as beam size k increases.

unlexicalized features (Table 6). In the large WE-
BQUESTIONS dataset, lexicalized features helped,
and so we added those features to our model when
running on the test set. In FREE917 lexicalized fea-
tures result in overfitting due to the small number of
training examples. Thus, we ran our final parser on
the test set without lexicalized features.

Effect of beam size An intrinsic challenge in se-
mantic parsing is to handle the exponentially large
set of possible derivations. We rely heavily on the
k-best beam approximation in the parser keeping
good derivations that lead to the correct answer. Re-
call that the set of candidate derivations D̃(x) de-
pends on the parameters θ. In the initial stages of
learning, θ is far from optimal, so good derivations
are likely to fall below the k-best cutoff of inter-
nal parser beams. As a result, D̃(x) contains few
derivations with the correct answer. Still, placing
these few derivations on the beam allows the train-
ing procedure to bootstrap θ into a good solution.
Figure 4 illustrates this improvement in D̃(x) across
early training iterations.

Smaller choices of k yield a coarser approxima-

1541



tion in beam search. As we increase k (Figure 5), we
see a tapering improvement in accuracy. We also see
a widening gap between accuracy and oracle score,8

as including a good derivation in D̃(x) is made eas-
ier but the learning problem is made more difficult.

Error analysis The accuracy on WEBQUES-
TIONS is much lower than on FREE917. We an-
alyzed WEBQUESTIONS examples and found sev-
eral main causes of error: (i) Disambiguating en-
tities in WEBQUESTIONS is much harder because
the entity lexicon has 41M entities. For example,
given “Where did the battle of New Orleans start?”
the system identifies “New Orleans” as the target
entity rather than its surrounding noun phrase. Re-
call that all FREE917 experiments used a carefully
chosen entity lexicon. (ii) Bridging can often fail
when the question’s entity is compatible with many
binaries. For example, in “What did Charles Bab-
bage make?”, the system chooses a wrong binary
compatible with the type Person. (iii) The system
sometimes incorrectly draws verbs from subordinate
clauses. For example, in “Where did Walt Disney
live before he died?” it returns the place of death of
Walt Disney, ignoring the matrix verb live.

5 Discussion

Our work intersects with two strands of work.
The first involves learning models of semantics
guided by denotations or interactions with the world.
Besides semantic parsing for querying databases
(Popescu et al., 2003; Clarke et al., 2010; Liang
et al., 2011), previous work has looked at inter-
preting natural language for performing program-
ming tasks (Kushman and Barzilay, 2013; Lei et
al., 2013), playing computer games (Branavan et al.,
2010; Branavan et al., 2011), following navigational
instructions (Chen, 2012; Artzi and Zettlemoyer,
2013), and interacting in the real world via percep-
tion (Matuszek et al., 2012; Tellex et al., 2011; Kr-
ishnamurthy and Kollar, 2013). Our system uses
denotations rather than logical forms as a training
signal, but also benefits from denotation features,
which becomes possible in the grounded setting.

The second body of work involves connecting
natural language and open-domain databases. Sev-

8Oracle score is the fraction of examples for which D̃(x)
contains any derivation with the correct denotation.

eral works perform relation extraction using dis-
tant supervision from a knowledge base (Riedel et
al., 2010; Carlson et al., 2010; Hoffmann et al.,
2011; Surdeanu et al., 2012). While similar in spirit
to our alignment procedure for building the lexi-
con, one difference is that relation extraction cares
about facts, aggregating over phrases, whereas a
lexicon concerns specific phrases, thus aggregating
over facts. On the question answering side, recent
methods have made progress in building semantic
parsers for the open domain, but still require a fair
amount of manual effort (Yahya et al., 2012; Unger
et al., 2012; Cai and Yates, 2013). Our system re-
duces the amount of supervision and has a more ex-
tensive evaluation on a new dataset.

Finally, although Freebase has thousands of prop-
erties, open information extraction (Banko et al.,
2007; Fader et al., 2011; Masaum et al., 2012)
and associated question answering systems (Fader
et al., 2013) work over an even larger open-ended
set of properties. The drawback of this regime is
that the noise and the difficulty in canonicaliza-
tion make it hard to perform reliable composition,
thereby nullifying one of the key benefits of se-
mantic parsing. An interesting midpoint involves
keeping the structured knowledge base but aug-
menting the predicates, for example using random
walks (Lao et al., 2011) or Markov logic (Zhang
et al., 2012). This would allow us to map atomic
words (e.g., “wife”) to composite predicates (e.g.,
λx.Marriage.Spouse.(Gender.Femaleux)). Learn-
ing these composite predicates would drastically in-
crease the possible space of logical forms, but we
believe that the methods proposed in this paper—
alignment via distant supervision and bridging—can
provide some traction on this problem.

Acknowledgments

We would like to thank Thomas Lin, Mausam and
Oren Etzioni for providing us with open IE triples
that are partially-linked to Freebase, and also Arun
Chaganty for helpful comments. The authors grate-
fully acknowledge the support of the Defense Ad-
vanced Research Projects Agency (DARPA) Deep
Exploration and Filtering of Text (DEFT) Program
under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-13-2-0040.

1542



References

Y. Artzi and L. Zettlemoyer. 2011. Bootstrapping
semantic parsers from conversations. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 421–432.

Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instructions
to actions. Transactions of the Association for Com-
putational Linguistics (TACL), 1:49–62.

M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In International Joint Conference on
Artificial Intelligence (IJCAI), pages 2670–2676.

S. Branavan, L. Zettlemoyer, and R. Barzilay. 2010.
Reading between the lines: Learning to map high-level
instructions to commands. In Association for Compu-
tational Linguistics (ACL), pages 1268–1277.

S. Branavan, D. Silver, and R. Barzilay. 2011. Learning
to win by reading manuals in a Monte-Carlo frame-
work. In Association for Computational Linguistics
(ACL), pages 268–277.

S. Branavan, N. Kushman, T. Lei, and R. Barzilay. 2012.
Learning high-level planning from text. In Association
for Computational Linguistics (ACL), pages 126–135.

Q. Cai and A. Yates. 2013. Large-scale semantic parsing
via schema matching and lexicon extension. In Asso-
ciation for Computational Linguistics (ACL).

A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H.
Jr, and T. M. Mitchell. 2010. Toward an architecture
for never-ending language learning. In Association for
the Advancement of Artificial Intelligence (AAAI).

A. X. Chang and C. Manning. 2012. SUTime: A library
for recognizing and normalizing time expressions. In
Language Resources and Evaluation (LREC), pages
3735–3740.

D. Chen. 2012. Fast online lexicon learning for grounded
language acquisition. In Association for Computa-
tional Linguistics (ACL).

H. H. Clark. 1975. Bridging. In Workshop on theoretical
issues in natural language processing, pages 169–174.

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL), pages 18–27.

J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive
subgradient methods for online learning and stochas-
tic optimization. In Conference on Learning Theory
(COLT).

A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Empirical
Methods in Natural Language Processing (EMNLP).

A. Fader, L. Zettlemoyer, and O. Etzioni. 2013.
Paraphrase-driven learning for open question answer-
ing. In Association for Computational Linguistics
(ACL).

D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL), pages 1486–1495.

Google. 2013. Freebase data dumps (2013-06-
09). https://developers.google.com/
freebase/data.

M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Interational Conference on
Computational linguistics, pages 539–545.

R. Hoffmann, C. Zhang, X. Ling, L. S. Zettlemoyer, and
D. S. Weld. 2011. Knowledge-based weak super-
vision for information extraction of overlapping rela-
tions. In Association for Computational Linguistics
(ACL), pages 541–550.

J. Krishnamurthy and T. Kollar. 2013. Jointly learning
to parse and perceive: Connecting natural language to
the physical world. Transactions of the Association for
Computational Linguistics (TACL), 1:193–206.

J. Krishnamurthy and T. Mitchell. 2012. Weakly super-
vised training of semantic parsers. In Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP/CoNLL),
pages 754–765.

N. Kushman and R. Barzilay. 2013. Using semantic uni-
fication to generate regular expressions from natural
language. In Human Language Technology and North
American Association for Computational Linguistics
(HLT/NAACL), pages 826–836.

T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2010. Inducing probabilistic CCG
grammars from logical form with higher-order unifi-
cation. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1223–1233.

T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman. 2011. Lexical generalization in CCG
grammar induction for semantic parsing. In Empirical
Methods in Natural Language Processing (EMNLP),
pages 1512–1523.

N. Lao, T. Mitchell, and W. W. Cohen. 2011. Random
walk inference and learning in a large scale knowledge
base. In Empirical Methods in Natural Language Pro-
cessing (EMNLP).

T. Lei, F. Long, R. Barzilay, and M. Rinard. 2013.
From natural language specifications to program input
parsers. In Association for Computational Linguistics
(ACL).

P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In As-

1543



sociation for Computational Linguistics (ACL), pages
590–599.

P. Liang. 2013. Lambda dependency-based composi-
tional semantics. Technical report, ArXiv.

T. Lin, Mausam, and O. Etzioni. 2012. Entity link-
ing at web scale. In Knowledge Extraction Workshop
(AKBC-WEKEX).

Masaum, M. Schmitz, R. Bart, S. Soderland, and O. Et-
zioni. 2012. Open language learning for informa-
tion extraction. In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL), pages 523–534.

C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and
D. Fox. 2012. A joint model of language and percep-
tion for grounded attribute learning. In International
Conference on Machine Learning (ICML).

R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In As-
sociation for Computational Linguistics (ACL), pages
91–98.

H. Poon. 2013. Grounded unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL).

A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards
a theory of natural language interfaces to databases.
In International Conference on Intelligent User Inter-
faces (IUI), pages 149–157.

S. Riedel, L. Yao, and A. McCallum. 2010. Model-
ing relations and their mentions without labeled text.
In Machine Learning and Knowledge Discovery in
Databases (ECML PKDD), pages 148–163.

M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D. Man-
ning. 2012. Multi-instance multi-label learning for
relation extraction. In Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning (EMNLP/CoNLL), pages 455–
465.

S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. J. Teller, and N. Roy. 2011. Understand-
ing natural language commands for robotic navigation
and mobile manipulation. In Association for the Ad-
vancement of Artificial Intelligence (AAAI).

C. Unger, L. Bhmann, J. Lehmann, A. Ngonga, D. Ger-
ber, and P. Cimiano. 2012. Template-based ques-
tion answering over RDF data. In World Wide Web
(WWW), pages 639–648.

Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Association for Computational Linguis-
tics (ACL), pages 960–967.

M. Yahya, K. Berberich, S. Elbassuoni, M. Ramanath,
V. Tresp, and G. Weikum. 2012. Natural language
questions for the web of data. In Empirical Methods

in Natural Language Processing and Computational
Natural Language Learning (EMNLP/CoNLL), pages
379–390.

M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming. In
Association for the Advancement of Artificial Intelli-
gence (AAAI), pages 1050–1055.

L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Uncer-
tainty in Artificial Intelligence (UAI), pages 658–666.

C. Zhang, R. Hoffmann, and D. S. Weld. 2012. Onto-
logical smoothing for relation extraction with minimal
supervision. In Association for the Advancement of
Artificial Intelligence (AAAI).

1544


