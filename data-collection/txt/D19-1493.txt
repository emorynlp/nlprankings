



















































Neural News Recommendation with Heterogeneous User Behavior


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4874â€“4883,
Hong Kong, China, November 3â€“7, 2019. cÂ©2019 Association for Computational Linguistics

4874

Neural News Recommendation with Heterogeneous User Behavior
Chuhan Wu1, Fangzhao Wu2, Mingxiao An3, Tao Qi1, Jianqiang Huang4,

Yongfeng Huang1, and Xing Xie2
1Department of Electronic Engineering, Tsinghua University, Beijing 100084, China

2Microsoft Research Asia, Beijing 100080, China
3University of Science and Technology of China, Hefei 230026, China

4Peking University, Beijing 100871, China
{wu-ch19, qit16, yfhuang}@mails.tsinghua.edu.cn,

{fangzwu, xing.xie}@microsoft.com,
anmx@mail.ustc.edu.cn, 1701210864@pku.edu.cn

Abstract

News recommendation is important for online
news platforms to help users find interested
news and alleviate information overload. Ex-
isting news recommendation methods usually
rely on the news click history to model user
interest. However, these methods may suf-
fer from the data sparsity problem, since the
news click behaviors of many users in online
news platforms are usually very limited. For-
tunately, some other kinds of user behaviors
such as webpage browsing and search queries
can also provide useful clues of usersâ€™ news
reading interest. In this paper, we propose a
neural news recommendation approach which
can exploit heterogeneous user behaviors. Our
approach contains two major modules, i.e.,
news representation and user representation.
In the news representation module, we learn
representations of news from their titles via
CNN networks, and apply attention networks
to select important words. In the user rep-
resentation module, we propose an attentive
multi-view learning framework to learn uni-
fied representations of users from their het-
erogeneous behaviors such as search queries,
clicked news and browsed webpages. In ad-
dition, we use word- and record-level atten-
tions to select informative words and behavior
records. Experiments on a real-world dataset
validate the effectiveness of our approach.

1 Introduction

Online news platforms such as Google News1 and
MSN2 News have gained huge popularity for on-
line digital news reading (Das et al., 2007). Tens
of thousands of news articles are streamed from
various sources every day, making it very difficult
for users to read all news to find their interested
content (Phelan et al., 2011). Thus, personalized

1https://news.google.com/
2https://www.msn.com/en-us/news

whether
google map
safe cars
toyota crown

Twitter - Login
Free and Safe Emails

How to Buy a Safe Car
Pros and Cons of Toyota

Search Queries

Browsed Webpages

Browse

Snow : Winter 
Weather This Week

The 2020 Toyota Supra Is 
Here and It Looks Glorious

NFL Winners and 
Losers Today

Will Click
?

Clicked News

Cli
ck

Figure 1: An illustrative example of news recommen-
dation based on heterogeneous user behaviors.

news recommendation is critical for online news
platforms to target user interests and alleviate in-
formation overload (IJntema et al., 2010).

News and user representation learning is criti-
cal for news recommendation. Many deep learn-
ing based methods have been proposed for this
task (Okura et al., 2017; Wang et al., 2018). For
example, Okura et al. (2017) proposed to learn
news representations from news bodies via auto-
encoders, and learn user representations from their
clicked news via a gated recurrent unit (GRU) net-
work. Wang et al. (2018) proposed to learn news
representations from news titles via a knowledge-
aware convolutional neural network (CNN), and
learn user representations from their clicked news
using the similarities between candidate news and
clicked news. These methods rely on the news
click histories to model usersâ€™ news reading inter-
est. However, these methods usually suffer from
the data sparsity problem, since the news click be-
haviors of many users on news platforms are very
limited, making it difficult for these methods to
learn accurate representations of these users.

Luckily, there are several other kinds of user



4875

behaviors such as search queries and webpage
browsing which are useful for news recommen-
dation. Many online users frequently use search
engines such as Google and Bing to search for de-
sired information (Wu et al.). In addition, they
may browse related webpages to get more detailed
information. Thus, the search and browsing data
accumulated by commercial search engines can
cover a large number of online users. In addi-
tion, the search and browsing behaviors of users
can provide rich information for inferring user in-
terests. For example, in Fig. 1, since the user posts
the search query â€œtoyota crownâ€, and browses the
webpage of â€œPros and Cons of Toyotaâ€ for de-
tailed information, she may have high interest in
Toyota cars and is likely to read news articles re-
lated to Toyota cars. However, this user may click
only a few news articles, and her interest in toy-
ota cars cannot be mined from the news click his-
tory. Thus, incorporating the heterogeneous be-
haviors of online users such as search queries and
webpage browsing has the potential to improve the
performance of news recommendation.

Our work is motivated by following observa-
tions. First, the characteristics of search queries,
webpage titles and news have huge differences.
For example, in Fig. 1, search queries are usu-
ally phrase pieces with a few words, while news
and webpage titles are usually complete sentences.
Thus, they should be handled differently. Second,
different words usually have different importance
in representing behavior records. For example, in
Fig. 1 the word â€œNFLâ€ is more informative than
â€œTodayâ€. Third, different behavior records may
also have different importance for representing
users. For instance, the search query â€œsafe carsâ€ in
Fig. 1 is more informative than the query â€œgoogle
mapâ€ in modeling user interest. Fourth, differ-
ent kinds of user behaviors usually have different
informativeness for user representation learning.
For example, in Fig. 1 since the user clicks very
few news articles, her news click behaviors are not
very informative for modeling this user.

In this paper, we propose a neural news recom-
mendation approach with heterogeneous user be-
havior (NRHUB). The core of our approach is a
news representation module and a user represen-
tation module. In the news representation module,
we learn news representations from news titles via
a CNN network with word-level attentions to se-
lect important words. In the user representation

module, we propose an attentive multi-view learn-
ing framework to incorporate the different kinds
of user behavior data into our model to learn uni-
fied user representations. In each view, we pro-
pose to use hierarchical user encoders to first learn
record representations from words, and then learn
user representations from different kinds of be-
havior records. Since different views may have
different informativeness for modeling users, we
propose to use a view-level attention network to
select important views for learning informative
user representations. In addition, since different
words and records may also have different infor-
mativeness, we apply word-level and record-level
attention networks to select important words and
records. Extensive experiments are conducted on
a real-world news recommendation dataset. The
results show that our approach can effectively im-
prove the performance of news recommendation,
especially in the cold-start scenario.

2 Related Work

News recommendation is an important task in
both natural language processing and data min-
ing fields, and has wide applications (Okura et al.,
2017; Zheng et al., 2018). Learning accurate news
and user representations is critical for news rec-
ommendation. Many of existing methods for news
recommendation rely on manual feature engineer-
ing to build news and user representations (Liu
et al., 2010; Son et al., 2013; Bansal et al., 2015).
For example, Liu et al. (2010) proposed to use
topic categories and interest features generated by
a Bayesian model to build news and user repre-
sentations. Son et al. (2013) proposed an Explicit
Localized Semantic Analysis (ELSA) model for
location-based news recommendation. They pro-
posed to extract topic and location features from
Wikipedia pages to build news representations.
Lian et al. (2018) proposed a deep fusion model
(DMF) to learn news representations from various
handcrafted features such as title length, entities
and news topics, and learn user representations
from features extracted from the news reading,
web browsing, and searching behaviors of users.
However, these methods usually rely on manual
feature engineering, which necessities massive do-
main knowledge and time to design. In addition,
these methods cannot exploit contextual informa-
tion in the behavior records of users, which may be
insufficient to learn accurate user representations.



4876

In recent years, several deep learning based
methods are proposed for news recommenda-
tion (Okura et al., 2017; Wang et al., 2018; Khat-
tar et al., 2018; Kumar et al., 2017; Zheng et al.,
2018; Wu et al., 2019b; An et al., 2019; Wu
et al., 2019c,a; Zhu et al., 2019). For example,
Okura et al. (2017) proposed to learn news rep-
resentations from news bodies via denoising au-
toencoders, and learn user representations from
the news browsing sequence via a GRU network.
Wang et al. (2018) proposed to learn news repre-
sentations from news titles via a knowledge-aware
CNN network, which can incorporate useful en-
tity information from knowledge graphs. Wu et
al. (2019b) proposed to learn news representations
from news titles, and apply personalized attention
mechanism at both word- and news-level to gen-
erate representations of news and users according
to user preferences. However, these methods only
learn user representations from a single kind of
user behavior, i.e., news click, which may be in-
sufficient. Different from these methods, our ap-
proach can learn unified user representations by
incorporating heterogeneous user behaviors via an
attentive multi-view learning framework. Exten-
sive experiments on real-world dataset validate our
approach can learn better news and user represen-
tations, and achieve better performance on news
recommendation than existing methods.

3 Our Approach

In this section, we introduce our neural news rec-
ommendation approach with heterogeneous user
behavior (NRHUB) in detail. The architecture of
our NRHUB approach is shown in Fig. 2. Our ap-
proach has three major modules, i.e., news repre-
sentation, user representation and click predictor.
Next, we introduce the details of each module.

3.1 News Representation Learning

The news representation module is used to learn
representations of news articles from their titles. It
contains three layers.

The first one is a word embedding layer, which
is used to convert a news title from a sequence of
words into a sequence of low-dimensional seman-
tic vectors. Denote a news title with M words
as [w1, w2, ..., wM ]. It is converted into a vector
sequence [e1, e2, ..., eM ] via a pre-trained embed-
ding matrix.

The second one is a CNN layer (Kim, 2014).

Usually, local contexts are very important in rep-
resenting the news titles. For example, in the news
title â€œToyota Crown Classics for Saleâ€, local con-
texts of â€œCrownâ€ such as â€œToyotaâ€ are useful for
inferring the topic of this news, i.e., cars. Thus,
we apply a CNN network to learn contextual rep-
resentations of words within news titles by captur-
ing their contexts. The CNN layer computes the
contextual representations of each word, and out-
puts a vector sequence [c1, c2, ..., cM ].

The third one is a word-level attention network.
Different words in the same news title may have
different importance in representing this news. For
example, in the second news in Fig. 1, the word
â€œNFLâ€ is more informative than â€œTodayâ€ for rep-
resenting this news. Thus, we propose to use at-
tention mechanism to select important words in
news titles for learning informative news repre-
sentations. Denote the attention weight of the i-th
word in a news title as Î±wi :

awi = v
n
w Ã— tanh(Vw Ã— ci + vw),

Î±wi =
exp(awi )âˆ‘M
j=1 exp(a

w
j )
,

(1)

where Vw and vw are projection parameters, vnw
is the query vector in this attention network. The
final contextual representation of a news title is the
summation of its word representations weighted
by their attention weights as follows:

rn =
Mâˆ‘
i=1

Î±wi ci. (2)

3.2 User Representation Learning
The user representation module is used to learn
the representations of users from different kinds
of user behaviors, e.g., web searching, news click
and webpage browsing. Since these user behav-
iors usually have different characteristics, simply
aggregate them together into a long document may
not be optimal for news representation. Thus, we
propose an attentive multi-view learning frame-
work to learn unified user representations by in-
corporating these three kinds of user behavior in-
formation as different views of news.

The first view is news encoder, which is used
to learn user representations from his/her clicked
news. There are two major submodules in this
view. The first one is news representation, as de-
scribed in section 3.1. Denote the sequence of
news clicked by a user as [D1, D2, ..., DN ], where



4877

CNN

Word Embedding

ğ’†ğ’†1 ğ’†ğ’†ğ‘–ğ‘– ğ’†ğ’†ğ‘€ğ‘€

ğ’„ğ’„1 ğ’„ğ’„ğ‘–ğ‘– ğ’„ğ’„ğ‘€ğ‘€

ğ‘¤ğ‘¤1 ğ‘¤ğ‘¤ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘€ğ‘€

ğ›¼ğ›¼1ğ’˜ğ’˜ ğ›¼ğ›¼2ğ’˜ğ’˜ ğ›¼ğ›¼ğ‘€ğ‘€ğ’˜ğ’˜

ğ’“ğ’“ğ’ğ’

CNN

Word Embedding

ğ’†ğ’†1 ğ’†ğ’†ğ‘–ğ‘– ğ’†ğ’†ğ‘€ğ‘€

ğ’„ğ’„1 ğ’„ğ’„ğ‘–ğ‘– ğ’„ğ’„ğ‘€ğ‘€

ğ‘¤ğ‘¤1 ğ‘¤ğ‘¤ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘€ğ‘€

ğ›¼ğ›¼1ğ’˜ğ’˜ ğ›¼ğ›¼2ğ’˜ğ’˜ ğ›¼ğ›¼ğ‘€ğ‘€ğ’˜ğ’˜

ğ’“ğ’“ğŸğŸğ’ğ’

CNN

Word Embedding

ğ’†ğ’†1 ğ’†ğ’†ğ‘–ğ‘– ğ’†ğ’†ğ‘€ğ‘€

ğ’„ğ’„1 ğ’„ğ’„ğ‘–ğ‘– ğ’„ğ’„ğ‘€ğ‘€

ğ‘¤ğ‘¤1 ğ‘¤ğ‘¤ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘€ğ‘€

ğ›¼ğ›¼1ğ’˜ğ’˜ ğ›¼ğ›¼2ğ’˜ğ’˜ ğ›¼ğ›¼ğ‘€ğ‘€ğ’˜ğ’˜
ğ’“ğ’“ğ‘µğ‘µğ’ğ’

ğ’—ğ’—ğ’˜ğ’˜ğ’ğ’

ğ·ğ·1

Clicked News

ğ·ğ·ğ‘ğ‘ğ·ğ·ğ‘ğ‘

ğ’“ğ’“ğ’Šğ’Šğ’ğ’
ğ’—ğ’—ğ’ğ’

ğ›¼ğ›¼1ğ‘›ğ‘› ğ›¼ğ›¼2ğ‘›ğ‘› ğ›¼ğ›¼ğ‘ğ‘
ğ‘›ğ‘›

ğ’–ğ’–ğ’ğ’

Dot
ï¿½ğ’šğ’šClick Probability

Candidate News

CNN

Word Embedding

ğ’†ğ’†ğ‘–ğ‘– ğ’†ğ’†ğ‘ƒğ‘ƒ

ğ’„ğ’„1 ğ’„ğ’„ğ‘–ğ‘– ğ’„ğ’„ğ‘ƒğ‘ƒ

ğ‘¤ğ‘¤1 ğ‘¤ğ‘¤ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘ƒğ‘ƒ

ğ›½ğ›½1ğ’˜ğ’˜ ğ›½ğ›½2ğ’˜ğ’˜ ğ›½ğ›½ğ‘ƒğ‘ƒğ’˜ğ’˜

ğ’“ğ’“ğŸğŸ
ğ’’ğ’’

CNN

Word Embedding

ğ’†ğ’†1 ğ’†ğ’†ğ‘–ğ‘– ğ’†ğ’†ğ‘ƒğ‘ƒ

ğ’„ğ’„1 ğ’„ğ’„ğ‘–ğ‘– ğ’„ğ’„ğ‘ƒğ‘ƒ

ğ‘¤ğ‘¤1 ğ‘¤ğ‘¤ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘ƒğ‘ƒ

ğ›½ğ›½1ğ’˜ğ’˜ ğ›½ğ›½2ğ’˜ğ’˜ ğ›½ğ›½ğ‘ƒğ‘ƒğ’˜ğ’˜
ğ’“ğ’“ğ‘µğ‘µ
ğ’’ğ’’

ğ’—ğ’—ğ’˜ğ’˜
ğ’’ğ’’

ğ‘„ğ‘„1

Search Queries

ğ‘„ğ‘„ğ‘ğ‘

ğ’“ğ’“ğ’Šğ’Š
ğ’’ğ’’ ğ’—ğ’—ğ’’ğ’’

ğ›¼ğ›¼1
ğ‘ğ‘ ğ›¼ğ›¼2

ğ‘ğ‘ ğ›¼ğ›¼ğ‘ğ‘
ğ‘ğ‘

CNN

Word Embedding

ğ’†ğ’†ğ‘–ğ‘– ğ’†ğ’†ğ‘„ğ‘„

ğ’„ğ’„1 ğ’„ğ’„ğ‘–ğ‘– ğ’„ğ’„ğ‘„ğ‘„

ğ‘¤ğ‘¤1 ğ‘¤ğ‘¤ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘„ğ‘„

ğ›¾ğ›¾1ğ’˜ğ’˜ ğ›¾ğ›¾2ğ’˜ğ’˜ ğ›¾ğ›¾ğ‘„ğ‘„ğ’˜ğ’˜

ğ’“ğ’“ğŸğŸğ’•ğ’•

CNN

Word Embedding

ğ’†ğ’†1 ğ’†ğ’†ğ‘–ğ‘– ğ’†ğ’†ğ‘„ğ‘„

ğ’„ğ’„1 ğ’„ğ’„ğ‘–ğ‘– ğ’„ğ’„ğ‘„ğ‘„

ğ‘¤ğ‘¤1 ğ‘¤ğ‘¤ğ‘–ğ‘– ğ‘¤ğ‘¤ğ‘„ğ‘„

ğ›¾ğ›¾1ğ’˜ğ’˜ ğ›¾ğ›¾2ğ’˜ğ’˜ ğ›¾ğ›¾ğ‘„ğ‘„ğ’˜ğ’˜
ğ’“ğ’“ğ‘µğ‘µğ’•ğ’•

ğ’—ğ’—ğ’˜ğ’˜ğ’•ğ’•

ğ‘‡ğ‘‡1

Browsed Webpages 

ğ‘‡ğ‘‡ğ‘ğ‘

ğ’“ğ’“ğ’Šğ’Šğ’•ğ’•
ğ’—ğ’—ğ’•ğ’•

ğ›¼ğ›¼1ğ‘¡ğ‘¡ ğ›¼ğ›¼2ğ‘¡ğ‘¡ ğ›¼ğ›¼ğ‘ğ‘
ğ‘¡ğ‘¡

ğ’†ğ’†1 ğ’†ğ’†1

Query Query

ğ’–ğ’–ğ’’ğ’’

ğ’—ğ’—

ğ›¼ğ›¼ğ‘›ğ‘› ğ›¼ğ›¼ğ‘ğ‘
ğ›¼ğ›¼ğ‘¡ğ‘¡
ğ’–ğ’–ğ’•ğ’•

ğ’–ğ’–

Word
Encoder

Record
Encoder

View
Encoder

News Representation User Representation

ğ’—ğ’—ğ’ğ’ğ’˜ğ’˜

Figure 2: The framework of our NRHUB approach.

N is the number of clicked news articles. It is
transformed into a sequence of hidden news rep-
resentations, denoted as [rn1 , r

n
2 , ..., r

n
N ].

The second one is a news-level attention net-
work. Different news clicked by the same user
may have different informativeness for represent-
ing this user. For example, in Fig. 1, the sec-
ond news is more informative than the first news
in modeling user preferences, since the first one
is usually not audience sensitive. Thus, we use
a news-level attention network to select impor-
tant news for learning informative user representa-
tions. Denote the attention weight of the i-th news
clicked by a user as Î±ni , which is computed as:

ani = vn Ã— tanh(Vn Ã— ri + bn),

Î±ni =
exp(ani )âˆ‘N
j=1 exp(a

n
j )
,

(3)

where Vn and bn are parameters, vn is the atten-
tion query vector. The news based representation
of a user u is the summation of the representations
of clicked news articles weighted by their attention
weights as follows:

un =
Nâˆ‘
i=1

Î±ni r
n
i . (4)

The second view is query encoder, which is
used to learn representations of users from their
search queries. There are two submodules in this
view. The first module is query representation,

which is used to learn query representations from
words. It also has three layers.

The first one is a word embedding layer,
which is shared with the news representation
view. Denote a search query q with P words as
q = [w1, w2, ..., wP ]. It is converted into a se-
quence of hidden word representations, denoted as
[e1, e2, ..., eP ].

The second one is a CNN layer. Local con-
texts are very important in learning query repre-
sentations. For example, in the query â€œxbox one
playerâ€, the local contexts of â€œoneâ€, i.e., â€œxboxâ€
and â€œplayerâ€ are important in representing this
query. Thus, we apply CNN to capture the local
contexts within queries, and compute the contex-
tual representations of each word. The outputs of
this CNN layer is denoted as [c1, c2, ..., cP ].

The third one is a word-level attention network.
Different words in the same search query usu-
ally have different informativeness for represen-
tation learning. For example, in the query â€œhot
movies this weekâ€ â€œmoviesâ€ is more informative
than â€œweekâ€ in modeling this query. Thus, we
propose to use a word-level attention network to
recognize important words in search queries for
learning informative query representations. The
attention weight Î²wi of the i-th word in a search
query is computed as:

bwi = v
q
w Ã— tanh(Vq Ã— ci + vq),

Î²wi =
exp(bwi )âˆ‘P
j=1 exp(b

w
j )
,

(5)



4878

where Vq, vq and v
q
w are parameters in the atten-

tion network. The contextual representation of this
query is the summation of its word representations
weighted by their attention weights, as follows:

rq =

Pâˆ‘
i=1

Î²wi ci. (6)

The second module is a query-level attention
network, which is used to learn user represen-
tations from query representations. Different
queries posted by the same user usually have dif-
ferent importance in characterizing this user. For
example, the query â€œtoyota carsâ€ is more infor-
mative than the query â€œgoogle mailâ€, since the
later one does not contain information of user pref-
erence. To select important search queries for
learning informative user representations, we pro-
pose to incorporate a query-level attention net-
work. The attention weight Î±qi of the query Qi
is computed as:

aqi = vq Ã— tanh(Uq Ã— ri + uq),

Î±qi =
exp(aqi )âˆ‘N
j=1 exp(a

q
j)
,

(7)

where Uq, uq and vq are attention parameters.
The user representation learned from queries is the
summation of the query representations weighted
by their attention weights as:

uq =
Nâˆ‘
i=1

Î±qi r
q
i . (8)

The third view is webpage encoder. It is used
to learn representations of users from the titles of
their browsed webpages. There are two submod-
ules in this view. The first one is webpage rep-
resentation, which is used to learn the representa-
tions of webpage titles from their words, and the
second one is a webpage-level attention network
which learns user representation from webpages.
In the webpage representation module, we first
use a CNN network to learn contextual represen-
tations of the words in webpage titles, and then we
use a word-level attention network to highlight im-
portant words. The hidden representation of each
webpage is the summation of the contextual rep-
resentations of words weighted by their attention
weights, which is similar to the search query and
news views described previously. The webpage-
level attention network is used to select infor-
mative webpages for user representation learning,

and the final webpage based user representation ut
is the summation of the webpage representations
weighted by their attention weights.

The last component in the user representation
module is a view encoder. Usually, different views
may have different informativeness for modeling
users. For example, a user may never or rarely cl
news, and his/her news click data may be unin-
formative for representing this user. Thus, mod-
eling the informativeness of different views may
benefit user representation learning. Motivated by
these observations, we propose a view-level atten-
tion network to select important views for learn-
ing informative user representations. Denote the
attention weights of the news, query and webpage
views as Î±n, Î±q and Î±t, respectively. For instance,
the attention weight of the news title view is com-
puted as follows:

an = v Ã— tanh(Uv Ã— un + uv),

Î±n =
exp(an)

exp(an) + exp(aq) + exp(at)
,

(9)

where v, Uv and uv are parameters. The atten-
tion weights of the search query and webpage ti-
tle views are computed similarly. The unified user
representation is the summation of the user repre-
sentations from different views weighted by their
attention weights, which is formulated as

u = Î±nun + Î±quq + Î±tut. (10)

3.3 Click Predictor
The click predictor module is used to predict the
probability of a user clicking a candidate news ar-
ticle from their representations. Denote the repre-
sentation of the candidate news Dc as rn. Follow-
ing (Okura et al., 2017), the click probability score
yÌ‚ is computed by the inner product of the user and
candidate news representations, i.e., yÌ‚ = uT rn.

3.4 Model Training
Motivated by (Huang et al., 2013), we propose
to use the negative sampling technique for model
training. For each news clicked by a user which
is regarded as a positive sample, we randomly
sample K news in the same impression but not
clicked by this user as the negative samples. De-
note the click probability score of the positive
news as yÌ‚+, and scores of the K negative news
as [yÌ‚âˆ’1 , yÌ‚

âˆ’
2 , ..., yÌ‚

âˆ’
K ]. We normalize these scores us-

ing the softmax function to compute the posterior



4879

# users 10,000 avg. # words per new title 11.29
# news 42,255 avg. # displays per news 247.89
# queries 923,041 # user w/ queries 5,282
# webpage titles 813,852 # user w/ webpages 6,134
# impressions 360,428 avg. # words per query 3.25
# samples 10,474,493 avg. # words per webpage title 9.01
# positive samples 503,698 # negative samples 9,970,795

Table 1: Statistics of our dataset.

click probability of a positive sample as follows:

pi =
exp(yÌ‚+i )

exp(yÌ‚+i ) +
âˆ‘K

j=1 exp(yÌ‚
âˆ’
i,j)

. (11)

The loss function for model training is the negative
log-likelihood of all positive samples:

L = âˆ’ 1
|S|

âˆ‘
iâˆˆS

log(pi), (12)

where S is the set of positive training samples.

4 Experiments

4.1 Datasets and Experimental Settings
We conducted experiments on a real-world news
recommendation dataset3 collected from MSN
News4 logs during Dec. 13, 2018 and Jan. 12,
2019. In addition, we crawled the search queries
and titles of browsed webpages from the logs of
the Bing search engine. The detailed statistics of
this dataset are summarized in Table 1. The news
data in the last week is used for test, and the rest is
used for model training. In addition, we randomly
sampled 10% of the training data for validation.

In our experiments, the word embeddings were
300-dimensional and we used the pre-trained
Glove embedding for initialization (Pennington
et al., 2014). All CNN networks had 400 filters,
and their window size was set to 3. The dimension
of attention query vectors was 200. Following (Wu
et al., 2019b), the negative sampling ratio K was
set to 4. Adam (Kingma and Ba, 2014) was used
as the optimization algorithm. We applied 30%
dropout to the word embedding and CNN net-
works to mitigate overfitting. The training batch
size was 64. These hyperparameters were all tuned
on the validation set. We independently repeated
each experiment 10 times and reported the average

3Some publicly available resources can be found at
https://github.com/wuch15/NRHUB.

4https://www.msn.com/en-us/news

results in AUC, MRR, nDCG@5 and nDCG@10
of all impressions.

4.2 Performance Evaluation

We evaluate the performance of our NRHUB ap-
proach by comparing it with many baseline meth-
ods, including: (1) LibFM (Rendle, 2012), a
matrix factorization method for recommendation;
(2) DSSM (Huang et al., 2013), deep structured
semantic model; (3) Wide&Deep (Cheng et al.,
2016), a popular neural recommendation method
with a wide linear model and a deep neural net-
work; (4) DeepFM (Guo et al., 2017), a famous
neural recommendation method with a factoriza-
tion machine and a deep neural network; (5)
DFM (Lian et al., 2018), a deep fusion model
for news recommendation; (6) GRU (Okura et al.,
2017), a neural news recommendation method
based on autoencoder and GRU to learn news and
user representations; (7) DKN (Wang et al., 2018),
a deep knowledge-aware news recommendation
method; (8) Conv3D (Kumar et al., 2017; Khat-
tar et al., 2018), a nerual news recommendation
method based on a 3-D convolutional neural net-
work to learn user representations from the se-
quence of clicked news5; (9) NRHUB, our neu-
ral new recommendation approach with heteroge-
neous user behavior; (10) NRHUB-basic, a variant
of our approach with only news click behavior;
(11) NRHUB-concat, a variant of our approach
which aggregates different kinds of user behaviors
as a single view. Following (Wang et al., 2018), in
methods (1, 3-5), we use one-hot encoded user ID,
news ID and the TF-IDF features of news titles as
the input features. The results are summarized in
Table 2.

From Table 2, we have several observations.
First, deep learning based news recommendation
methods (e.g., DSSM and NRHUB) can outper-
form LibFM. This is because neural networks can

5We only use news titles for fair comparison.



4880

Methods AUC MRR nDCG@5 nDCG@10
LibFM 0.5661 0.2414 0.2689 0.3552
DSSM 0.5949 0.2675 0.2881 0.3800

Wide & Deep 0.5812 0.2546 0.2765 0.3674
DeepFM 0.5830 0.2570 0.2802 0.3707

DFM 0.5861 0.2609 0.2844 0.3742
GRU 0.6114 0.2823 0.3006 0.3931
DKN 0.6070 0.2801 0.3012 0.3933

Conv3D 0.6051 0.2765 0.2987 0.3904
NRHUB-basic 0.6232 0.2927 0.3158 0.3986

NRHUB-concat 0.6265 0.2945 0.3177 0.4010
NRHUB* 0.6317 0.3020 0.3260 0.4076

Table 2: The results of different methods. *The improvement is significant at p < 0.01.

learn better news and user representations than
traditional matrix factorization methods, which is
beneficial for more accurate news recommenda-
tion. Second, among deep learning based meth-
ods, both NRHUB-basic and NRHUB can out-
perform all baseline methods. This is because
our approaches incorporate both word- and news-
level attention networks to simultaneously select
important words and news for learning more in-
formative news and user representations. Thus,
our approaches can outperform baseline methods.
Third, NRHUB and NRHUB-concat methods out-
perform NRHUB-basic. This is because search
queries and browsed webpage can provide rich
information of user interests, which is useful for
learning user representation for news recommen-
dation. Fourth, NRHUB consistently outperforms
NRHUB-concat. This is because the characteris-
tics of news titles, search queries and webpage ti-
tles are quite different, and they should be handled
differently. Thus, simply merging them together is
not the optimal way to exploit them. These results
validate the effectiveness of our NRHUB approach
in incorporating heterogeneous user behaviors for
news recommendation.

4.3 Effectiveness of Attentive Multi-view
Learning

In this section, we explore the effectiveness of the
attentive multi-view learning framework in our ap-
proach. First, we want to verify the effectiveness
of incorporating heterogeneous user behaviors as
different views. We compare the performance of
our NRHUB approach with its variants with dif-
ferent combinations of views, and the results are
shown in Fig. 3. From Fig. 3, we have several ob-

Figure 3: Effect of heterogeneous user behaviors.

servations. First, both search queries and browsed
webpage titles are useful for improving the per-
formance of news recommendation. This may be
because users usually use post search queries to
search engines to seek for interested content, and
may browse several related webpages for detailed
information. Thus, search queries and browsed
webpage titles contain rich information of user
preferences, which is useful for learning accurate
user representations. Second, webpage titles are
more important than search queries. This may be
because webpage titles have larger coverage in our
dataset. In addition, search queries are usually
very short with a few words, while webpage ti-
tles are often like complete sentences with much
more words. Thus, webpage titles are more in-
formative for learning user representations. Third,
incorporating both kinds of user behaviors can fur-
ther improve the performance of our approach.
These results validate the effectiveness of incor-
porating heterogeneous user behaviors via multi-
view learning.



4881

Figure 4: Influence of different attention networks.

Next, we explore the effectiveness of differ-
ent attention networks in our approach. We con-
duct experiments using the leave-one-out scheme
to evaluate the contribution of each attention net-
work. The results are shown in Fig. 4. From
Fig. 4, we find the word-level attention network
can effectively improve the performance of our
approach. This is because different words in the
same news, query or webpage title may have dif-
ferent importance for learning user representa-
tions. Thus, selecting important words can help
learn more informative news, query and webpage
representations. In addition, the news-, query- and
webpage-level attention networks are also impor-
tant to our approach. This is because different
news articles, search queries or webpages usually
have different importance in representing users,
and selecting important news, queries and web-
pages is beneficial for learning accurate user rep-
resentations. Besides, the view-level attention net-
work is also useful. This is because different views
also have different informativeness in modeling
user preferences. Thus, evaluating the informa-
tiveness of different views via a view-level atten-
tion network can learn better user representations.
Moreover, combining all these kinds of attention
networks can further improve the performance of
our approach. These results validate the effective-
ness of the attentive multi-view learning frame-
work in our approach.

4.4 Cold-start Performance

In this section, we explore the cold-start perfor-
mance of our approach. In practical use, massive
users may rarely or have never clicked news arti-
cles on the target news platform, making it diffi-
cult to effectively recommend news to these users.
Luckily, the search and browsing behaviors can

Figure 5: Cold-start performance of our approach.

Figure 6: Distributions of view-level attention weights.
The left, middle and right curves respectively denote
search query, browsed webpage and clicked news.

cover many of these users. These user behaviors
usually contain rich information of user prefer-
ences, and have the potential to address the cold-
start problem. To validate the effectiveness of our
approach, we compare the performance of several
variants of our approach without the news encoder
view to simulate the cold-start situation. The re-
sults are shown in Fig. 5. According to the re-
sults, we find our approach can still achieve sat-
isfactory performance in the cold-start situation.
This may be because search queries and webpage
titles are important clues of user interests, which
are very useful for news recommendation. Thus,
our approach can recommend news effectively to
the users with very limited or even no news brows-
ing histories, which can effectively improve their
experiences. In addition, combining the informa-
tion of queries and webpage titles can further im-
prove the performance of our approach. These re-
sults validate the effectiveness of our approach.

4.5 Case Study

In this section, we conduct several case studies to
visually explore the effectiveness of different at-



4882

Search 
Query

medical care in chico , californoia
google
uber and starbucks
keto diet reviews

Webpage 
Title

women 's boots | nordstrom
women 's clothing , shoes & accessories | nordstrom 
10 foods that fight inflammation
gmail inbox

News 
Title

fashion hits and misses of 2018
nude dresses trend at the 2019 golden globes
new year 's eve weather forecast
best games in this season

Figure 7: Visualization of the word-level and record-
level attention weights from a randomly selected user.

tention networks in our approach. First, we vi-
sualize the distributions of the view-level atten-
tion weights in our NRHUB approach, and the re-
sults are shown in Fig. 6. According to the results,
we find the attention weights of the news encoder
view can be relatively low. This may be because
the news browsing histories of some users are less
informative for modeling these users. In addition,
we find the attention weight of the webpage en-
coder view is higher than the query encoder view.
This may be because webpage titles are usually
complete sentences, while search queries are usu-
ally pieces of words. Thus, webpage titles con-
tain richer information of user interests than search
queries, which are more informative for represent-
ing users.

Next, we illustrate the attention weights in the
word-level and record (news, query and webpage)-
level attention networks. The results are shown
in Fig. 7. From Fig. 7, we have several obser-
vations. First, we find search queries and web-
page titles can provide much information of user
interests which is not covered by the news brows-
ing histories of users. For example, we can in-
fer that this user is interested in the health topic
from her queries and browsed webpages, which
is not revealed by her clicked news. Thus, in-
corporating heterogeneous behaviors of users are
useful for learning more accurate user representa-
tions. Second, we find that our approach can ef-
fectively select important words in queries, news
and webpage titles. For example, the word â€œmedi-
calâ€ gains high attention weight since it is infor-
mative for learning query representations, while
the word â€œinâ€ receives low attentions. In addi-
tion, our approach can effectively select impor-
tant queries, news and webpages. For example,
the query â€œdeto diet reviewsâ€ and the webpage of
â€œ10 foods that fight inflammationâ€ are assigned

high attention weights since they are informative
for learning user representations, but the query
â€œgoogleâ€ and the webpage of â€œgmail inboxâ€ are
assigned low attention weights since they are unin-
formative. These results validate the effectiveness
of the attention networks in our approach.

5 Conclusion and Furture Work

In this paper, we propose a neural news recom-
mendation approach which can exploit heteroge-
neous user behaviors. In our approach, we pro-
pose an attentive multi-view learning framework
to learn unified user representations from their
search queries, clicked news, and browsed web-
pages by regarding them as different views. We
propose to apply a view-level attention network to
select important views for informative user repre-
sentation learning. Besides, we propose to employ
both word- and record-level attention networks to
learn more informative news and user represen-
tations. Extensive experiments on a real-world
dataset collected from MSN News show our ap-
proach can effectively improve the performance of
neural news recommendation, and can achieve sat-
isfactory results in cold-start scenarios.

In our future work, we will explore three poten-
tial directions. The first one is how to incorporate
the interactions between different kinds of user be-
haviors, since the relatedness of them may be use-
ful for modeling the interest evolution of users.
The second one is how to incorporate the activi-
ties of users on social media platforms, since the
opinions, experiences and events shared by users
on social media are usually useful clues to indicate
their interests. The third one is how to incorporate
language models to generate context-aware word
embeddings, which may enhance the representa-
tion learning of contexts in news and other kinds
of user generated content.

Acknowledgments

The authors would like to thank Microsoft News
for providing technical support and data in the ex-
periments, and Jiun-Hung Chen (Microsoft News)
and Ying Qiao (Microsoft News) for their support
and discussions. This work was supported by the
National Key Research and Development Program
of China under Grant No. 2018YFC1604002, the
National Natural Science Foundation of China un-
der Grant Nos. U1836204, U1705261, U1636113,
U1536201, and U1536207.



4883

References
Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang,

Zheng Liu, and Xing Xie. 2019. Neural news rec-
ommendation with long-and short-term user repre-
sentations. In ACL, pages 336â€“345.

Trapit Bansal, Mrinal Das, and Chiranjib Bhat-
tacharyya. 2015. Content driven user profiling
for comment-worthy recommendations of news and
blog articles. In RecSys., pages 195â€“202. ACM.

Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal
Shaked, Tushar Chandra, Hrishi Aradhye, Glen An-
derson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.
2016. Wide & deep learning for recommender sys-
tems. In DLRS, pages 7â€“10. ACM.

Abhinandan S Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personal-
ization: scalable online collaborative filtering. In
WWW, pages 271â€“280. ACM.

Huifeng Guo, Ruiming Tang, Yunming Ye, Zhen-
guo Li, and Xiuqiang He. 2017. Deepfm: a
factorization-machine based neural network for ctr
prediction. In AAAI, pages 1725â€“1731. AAAI Press.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In CIKM, pages 2333â€“2338.
ACM.

Wouter IJntema, Frank Goossen, Flavius Frasincar,
and Frederik Hogenboom. 2010. Ontology-based
news recommendation. In Proceedings of the 2010
EDBT/ICDT Workshops, page 16. ACM.

Dhruv Khattar, Vaibhav Kumar, Vasudeva Varma, and
Manish Gupta. 2018. Weave&rec: A word embed-
ding based 3-d convolutional network for news rec-
ommendation. In CIKM, pages 1855â€“1858. ACM.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP, pages 1746â€“
1751.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Vaibhav Kumar, Dhruv Khattar, Shashank Gupta, and
Vasudeva Varma. 2017. Word semantics based 3-d
convolutional neural networks for news recommen-
dation. In 2017 IEEE International Conference on
Data Mining Workshops (ICDMW), pages 761â€“764.
IEEE.

Jianxun Lian, Fuzheng Zhang, Xing Xie, and
Guangzhong Sun. 2018. Towards better represen-
tation learning for personalized news recommenda-
tion: a multi-channel deep fusion approach. In IJ-
CAI, pages 3805â€“3811.

Jiahui Liu, Peter Dolan, and Elin RÃ¸nby Pedersen.
2010. Personalized news recommendation based on
click behavior. In IUI, pages 31â€“40. ACM.

Shumpei Okura, Yukihiro Tagami, Shingo Ono, and
Akira Tajima. 2017. Embedding-based news rec-
ommendation for millions of users. In KDD, pages
1933â€“1942. ACM.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, pages 1532â€“1543.

Owen Phelan, Kevin McCarthy, Mike Bennett, and
Barry Smyth. 2011. Terms of a feather: Content-
based news recommendation and discovery using
twitter. In ECIR, pages 448â€“459. Springer.

Steffen Rendle. 2012. Factorization machines with
libfm. TIST, 3(3):57.

Jeong-Woo Son, A Kim, Seong-Bae Park, et al. 2013.
A location-based news article recommendation with
explicit localized semantic analysis. In SIGIR, pages
293â€“302. ACM.

Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi
Guo. 2018. Dkn: Deep knowledge-aware network
for news recommendation. In WWW, pages 1835â€“
1844.

Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang
Huang, Yongfeng Huang, and Xing Xie. 2019a.
Neural news recommendation with attentive multi-
view learning. In IJCAI, pages 3863â€“3869.

Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang
Huang, Yongfeng Huang, and Xing Xie. 2019b.
Npa: Neural news recommendation with personal-
ized attention. In KDD, pages 2576â€“2584. ACM.

Chuhan Wu, Fangzhao Wu, Mingxiao An, Yongfeng
Huang, and Xing Xie. 2019c. Neural news recom-
mendation with topic-aware news representation. In
ACL, pages 1154â€“1159.

Chuhan Wu, Fangzhao Wu, Junxin Liu, Shaojian He,
Yongfeng Huang, and Xing Xie. Neural demo-
graphic prediction using search query.

Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang
Xiang, Nicholas Jing Yuan, Xing Xie, and Zhen-
hui Li. 2018. Drn: A deep reinforcement learning
framework for news recommendation. In WWW,
pages 167â€“176.

Qiannan Zhu, Xiaofei Zhou, Zeliang Song, Jianlong
Tan, and Li Guo. 2019. Dan: Deep attention neural
network for news recommendation. In AAAI, vol-
ume 33, pages 5973â€“5980.


