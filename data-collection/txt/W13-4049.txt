



















































In-Context Evaluation of Unsupervised Dialogue Act Models for Tutorial Dialogue


Proceedings of the SIGDIAL 2013 Conference, pages 324–328,
Metz, France, 22-24 August 2013. c©2013 Association for Computational Linguistics

In-Context Evaluation of Unsupervised Dialogue Act Models  
for Tutorial Dialogue 

 
 

Aysu Ezen-Can 
Department of Computer Science 
North Carolina State University 
Raleigh, North Carolina 27695 

aezen@ncsu.edu 

Kristy Elizabeth Boyer 
Department of Computer Science 
North Carolina State University 
Raleigh, North Carolina 27695 

keboyer@ncsu.edu 
 

  
 

Abstract 

Unsupervised dialogue act modeling holds 
great promise for decreasing the develop-
ment time to build dialogue systems. 
Work to date has utilized manual annota-
tion or a synthetic task to evaluate unsu-
pervised dialogue act models, but each of 
these evaluation approaches has substan-
tial limitations. This paper presents an in-
context evaluation framework for an un-
supervised dialogue act model within tuto-
rial dialogue. The clusters generated by 
the model are mapped to tutor responses 
by a handcrafted policy, which is applied 
to unseen test data and evaluated by hu-
man judges. The results suggest that in-
context evaluation may better reflect the 
performance of a model than comparing 
against manual dialogue act labels. 

1 Introduction 
A central focus within the dialogue systems re-
search community is developing techniques for 
rapidly constructing dialogue systems. One tech-
nique that has proven highly promising is to take 
a corpus-based approach to dialogue system au-
thoring, for example by bootstrapping policy 
learning (Henderson, Lemon, & Georgila, 2008; 
Williams & Young, 2003), predicting what a 
human agent would do (Bangalore, Di Fabbrizio, 
& Stent, 2008), or learning supervised dialogue 
act models (Stolcke et al., 2000). Traditionally, 
these corpus-based approaches require some 
amount of manual annotation prior to learning 
the dialogue models.  In many cases, this manual 
annotation is a problematic bottleneck for system 
development.  

For tutorial dialogue systems, which aim to 
support students in acquiring skills or 
knowledge, heavy manual annotation is often 
required for learning models that classify student 
utterances with respect to dialogue acts (Forbes-
Riley & Litman, 2005; Serafin & Di Eugenio, 
2004), questioning strategies (Becker, Palmer, 
Vuuren, & Ward, 2012), or information sharing 
(Mayfield, Adamson, & Rosé, 2012) 

For dialogue act modeling in particular, recent 
work has demonstrated the great promise of un-
supervised approaches, which are learned with-
out the use of manual labels (Crook, Granell, & 
Pulman, 2009; Ezen-Can & Boyer, 2013; Ritter, 
Cherry, & Dolan, 2010). However, because gold 
standard labels are not a part of model learning, 
how to best evaluate unsupervised models repre-
sents a significant open research question 
(Vlachos, 2011).  

Most quantitative evaluations of unsupervised 
dialogue act models have relied on agreement 
with manual dialogue act annotations, though 
these annotations were not used in model learn-
ing (Crook et al., 2009; Rus, Moldovan, Niraula, 
& Graesser, 2012; Ezen-Can & Boyer, 2013). 
Relying on manually tagged dialogue act labels 
to evaluate an unsupervised model has two major 
drawbacks: it does not fully avoid the manual 
annotation bottleneck, and it imposes a hand-
authored criterion onto a fully data-driven model, 
which may be unnecessarily limiting. Distinc-
tions made by an unsupervised model may be 
useful within a dialogue system, even if these 
categories are different from the distinctions 
made within a hand-authored dialogue act tagset.  

This paper presents a novel evaluation 
framework for unsupervised dialogue act classi-
fication of user utterances within tutorial dia-
logue. Instead of attempting to evaluate the mod-
el intrinsically, we evaluate its performance on 

324



an external task: triggering an appropriate utter-
ance via a simple dialogue policy. This evalua-
tion, which does not require an end-to-end dia-
logue system, judges the model in the simulated 
context of the target task. The results demon-
strate that this in-context evaluation may be 
equally useful as comparing against gold stand-
ard dialogue act labels, while substantially reduc-
ing the time required for human annotation.   

2 Related Work 
Perhaps the earliest unsupervised approach for 
dialogue act modeling investigated hidden Mar-
kov models with a bag-of-words approach in a 
meeting scheduling domain (Woszczyna & 
Waibel, 1994), using perplexity with respect to 
manual labels for evaluating the number of hid-
den states. Dirichlet process clustering has been 
investigated for dialogue act classification in the 
train fares and scheduling domain (Crook et al., 
2009), evaluating on intra-cluster similarity and 
inter-cluster similarity along with error rates with 
respect to manual labels. Another Bayesian ap-
proach utilized hidden Markov models and topic 
modeling to classify Twitter posts (Ritter et al., 
2010). Notably, Ritter et al. utilize an utterance 
ordering task, rather than manual labels, for 
quantitative evaluation. Most recently, standard 
k-means and EM clustering algorithms were used 
for dialogue act clustering on an educational cor-
pus, and the model’s accuracy was again evalu-
ated with respect to manual labels (Rus et al., 
2012). The current paper builds on these prior 
findings by applying a recently developed clus-
tering framework and proposing a novel in-
context evaluation scheme that can be used re-
gardless of the unsupervised dialogue act model-
ing technique underlying it. 

3 Dialogue Act Clustering 
We consider an unsupervised dialogue act classi-
fication model on a corpus of human-human stu-
dent and tutor dialogues centered on a computer 
programming task within a textual dialogue envi-
ronment (Boyer et al., 2009). There are 1,525 
student utterances and 3,332 tutor utterances in 
the corpus. This paper focuses on dialogue act 
classification for student utterances, since in a 
tutorial dialogue system the tutor dialogue acts 
are system-generated.  

The corpus was manually labeled in prior 
work with nine dialogue acts tailored to capture 
phenomena of interest within tutorial dialogue: 
general Question, Evaluation Question (request 

specific feedback on the task), Statement, Posi-
tive Feedback, Lukewarm Feedback, Negative 
Feedback, Grounding, Greeting, and Extra-
domain (utterances that are off topic). The Kappa 
for agreement on these manual tags was 0.76. 
These tags will be used within the present work 
to compare the in-context performance of the 
unsupervised policy with a manual-tag policy, 
but the tags are not used to learn or tune the un-
supervised model. 

The unsupervised dialogue act model evaluat-
ed here is based on a recently developed ap-
proach that adapts the query-likelihood technique 
from information retrieval to rank utterances 
similar to each target utterance (Ezen-Can & 
Boyer, 2013). Each utterance within the training 
set is queried against all other utterances within 
the training set using bigram features.  

Vectors encode the resulting utterance simi-
larity, and these vectors are provided to a k-
means clustering algorithm to partition the utter-
ances into dialogue acts. Our recent work (Ezen-
Can & Boyer, 2013) evaluated query-likelihood 
dialogue act clustering against two other ap-
proaches with respect to classifying manual la-
bels, and the query-likelihood approach outper-
formed k-means clustering using leading tokens 
(Rus et al., 2012) and Dirichlet process cluster-
ing (Crook et al., 2009). In the current work we 
add to the feature vectors the first level of the 
parse tree as provided by the Stanford parser 
(Klein & Manning, 2003).  

The number of clusters was selected based on 
sum of squared errors (SSE). As with many pa-
rameterized models, model fit tends to increase 
with more parameters, but there are important 
tradeoffs in computation time and risk of overfit-
ting. In experiments, k=number of clusters 
ranged from 2 to 24. 21 clusters were chosen, 
corresponding to the rightmost “knee” within the 
SSE graph (see Appendix).1 

4 Evaluation Framework 
Evaluating unsupervised dialogue act clusters 
presents numerous challenges. In prior evalua-
tions of query-likelihood clustering, we comput-
ed accuracy with respect to the manually applied 
dialogue act tags described earlier, demonstrating 
41.64% accuracy for a model with 8 clusters, 
compared to 34.90% accuracy for the Rus et al. 
                                                
1 Selecting the number of clusters is a subjective deci-
sion. Nonparametric techniques, such as variations on 
Dirichlet process clustering, hold promise for address-
ing this limitation in the future. 

325



(2012) k-means approach and 24.48% accuracy 
for Dirichlet process clustering (Crook et al., 
2009) on our corpus. However, the goal of the 
current work is to substantially reduce the human 
tagging required to evaluate the model. We also 
aim to test the hypothesis that comparing against 
manual labels under-represents the utility of the 
unsupervised model. That is, a dialogue policy 
built on the unsupervised model could perform 
better than the relatively low classification accu-
racy for manual tags would suggest. Our evalua-
tion will explore this hypothesis. 

In order to achieve these goals, we first trained 
an unsupervised dialogue act model on 75% of 
the corpus using the query-likelihood approach 
described in Section 3. The resulting model has 
21 clusters. Then, we handcrafted a dialogue pol-
icy for tutor responses by qualitatively examin-
ing each cluster of training data and creating one 
tutor response for each cluster. Some clusters and 
their corresponding tutor utterances are depicted 
in Figure 1. This policy was applied by classify-
ing unseen utterances from a held-out test set 
(25% of the corpus) using the learned model 
(Figure 2). The result of this process is that for 
each student utterance from the test set, a tutor 
response is generated based on the policy. This 
process resulted in 373 student utterances, one 
for each utterance in the 25% testing set, each 
paired with a corresponding tutor response gen-
erated by the hand-authored policy. 

The evaluation goal is to determine whether 
the responses made by this policy are reasonable, 
which will represent the utility of the unsuper-
vised dialogue act model for its intended use 
within a dialogue manager. We used human 
judges to rate the output of the policy. Thirty 
student utterances and tutor responses were ran-
domly selected from the available utterances 
generated by the test set. An example set of ut-
terances and policies can be seen in the Appen-
dix. These items were placed in a survey that 
asked the reader to rate the extent to which each 
tutor response makes sense given the student ut-
terance. (One item was inadvertently omitted 
from the survey, resulting in 29 items that were 
evaluated by the judges and that will be analyzed 
here.) To avoid bias introduced by the ordering 
of items, they were presented in a different ran-
domized order for each of the seven judges who 
completed the survey. (29 items from a compari-
son condition using manual tags were also ran-
domly interleaved into the survey, as described 
later in this section.) Judges used a rating scale 
from 1 to 4 (1=makes no sense, 2=makes a little 

sense, 3=makes a lot of sense, and 4=makes per-
fect sense). Since the models only used the cur-
rent student utterance, the dialogue history was 
also not shown to the human raters.  

Across the seven judges, the average rating of 
the tutor responses selected by the unsupervised 
policy was 2.35. We also collapsed the ratings 
into positive (≥2.5 average across seven judges) 
and negative (<2.5 average). With this binary 
categorization, 44.8% of the time tutor responses 
generated by the unsupervised policy were rated 
positively. It is important to note that no infor-
mation other than dialogue act was considered 
for generating the tutor responses; the tutor utter-
ances were relatively content-free and based only 
on the dialogue act categorization given by the 
unsupervised model. 

 
 
 
 
 
 
 
 
 
 
 
 

 

 
Figure 1: Clusters from unsupervised dialogue act 

modeling and corresponding dialogue policy  
(typographical errors originated in corpus) 

 

For comparison, we also constructed a hand-
crafted dialogue policy using the manual dia-
logue act labels and applied this policy to the 
same utterances as were used to evaluate the un-
supervised model. These pairs of student utter-
ances and tutor responses were interleaved ran-
domly on the same survey provided to seven 
human judges. The same tutor responses as in the 
unsupervised policy were used whenever possi-
ble for this manual-tag policy. The tutor respons-
es generated from the manual-tag policy received 
an average score of 2.22, slightly lower than the 
average of 2.35 for tutor responses generated by 
the unsupervised policy. The binary positive-
negative split for these ratings reveals that 31% 
were rated positively (≥2.5 average), compared 
to 44.8% for the unsupervised policy. 

Direct comparisons between the unsupervised 
policy and the manual-tag policy must be inter-
preted with caution, in part because the unsuper-
vised policy was more granular (based on 21 

326



clusters) than the manual-tag policy (based on 9 
tags) and also because it can be difficult to en-
sure that the two policies were of equal quality. 
On the other hand, the unsupervised policy uti-
lized no manual labels and was applied to an un-
seen test set, while the manual-tag policy was 
based on reliable tags applied to the actual utter-
ances from the testing set.  
 
 
 
 
 
 
 
 
 
 
 
 

 
Finally, we evaluated the extent to which the 

4-category rating scheme was reliable across 
judges. The weighted Kappa (Cohen, 1968), used 
for ordinal scales because it penalizes disagree-
ments less if they are closer together, was 0.30 
averaged across all pairs of judges, indicating 
fair agreement (Landis & Koch, 2013). For the 
collapsed binary ratings, average pairwise ordi-
nary Kappa was 0.36. 

5 Discussion 
It was hypothesized that evaluating an unsuper-
vised dialogue act model against manual labels 
may be an inappropriately strict metric, requiring 
the model to conform to the criteria used by hu-
mans to handcraft the manual tagset. Indeed, the 
accuracy of the unsupervised dialogue act model 
presented here with 21 clusters was 30.4% for 
identifying manual labels (arrived at by assigning 
the majority class tag to each unsupervised clus-
ter after clustering was complete). The majority 
class baseline (most frequent student dialogue act 
tag) was Evaluation Question with a relative fre-
quency of 25.87%, so on accuracy for identifying 
manual labels, the unsupervised model improved 
modestly over baseline. In contrast, when this 
unsupervised model was used to select a tutor 
response within a dialogue policy, the response 
was judged positively 44.8% of the time by hu-
man judges. Moreover, recall that the tutor re-
sponses were content free and took only the dia-

logue act label into account (no information state 
or topic). Therefore, it is meaningful to consider 
what percent of the time the responses were rated 
as making some sense (receiving a 2, 3, or 4 rat-
ing average across the human judges). By this 
criterion, 65.5% of tutor responses selected by 
the unsupervised policy were rated as sensible.  

Finally, this evaluation approach demon-
strates promise for alleviating the bottleneck of 
manual annotation for dialogue act models. Each 
item within the current evaluation survey re-
quired approximately 15 seconds to judge, using 
untrained human judges, for a total of approxi-
mately 1 hour of effort across all seven judges. 
The time required for handcrafting policies was 
relatively small, approximately 1 hour. In con-
trast, the dialogue act annotation scheme re-
quired approximately 35 seconds per utterance 
(amortizing substantial up-front training time for 
each annotator) when applied as part of previous 
work, for a total of approximately 50 hours per 
annotator.  

6 Conclusion 
Unsupervised dialogue act modeling holds great 
promise for decreasing development time of dia-
logue systems. We have presented an unsuper-
vised dialogue act model and an evaluation 
framework to judge the utility of the unsuper-
vised model within a dialogue management task. 
The results demonstrate that in-context evalua-
tion of an unsupervised dialogue act model, ra-
ther than accuracy against manual labels, may 
better reflect the usefulness of the model for dia-
logue management. Furthermore, this evaluation 
technique may greatly reduce the time required 
by human judges to evaluate the model.  

One of the most promising directions for fu-
ture work involves devising unsupervised dia-
logue act models that leverage a richer represen-
tation in order to perform better. These rich fea-
tures may include dialogue history, adjacency 
pair information, and topic modeling. Addition-
ally, it is important for the community to evalu-
ate unsupervised dialogue models in the full con-
text of deployed systems. 
Acknowledgments. This material is based upon 
work supported by the National Science Founda-
tion under Grants DRL-1007962 and CNS-
1042468. Any opinions, findings, conclusions, or 
recommendations expressed in this report are 
those of the authors and do not necessarily repre-
sent the views of the National Science Founda-
tion. 

Figure 2: Evaluation framework structure 

327



References  
Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). 

Learning the Structure of Task-Driven Human-
Human Dialogs. IEEE Transactions on Audio, 
Speech and Language Processing, 16(7), 1249–
1259. 

Becker, L., Palmer, M., Vuuren, S. Van, & Ward, W. 
(2012). Learning to Tutor Like a Tutor: Ranking 
Questions in Context. Proceedings of the Inter-
national Conference on Intelligent Tutoring 
Systems, 368–378. 

Boyer, K. E., Philips, R., Ha, E. Y., Wallis, M. D., 
Vouk, M. A., & Lester, J. C. (2009). Modeling 
Dialogue Structure with Adjacency Pair 
Analysis and Hidden Markov Models. 
Proceedings of NAACL HLT, 49–52. 

Cohen, J. (1968). Weighted kappa: Nominal scale 
agreement provision for scaled disagreement or 
partial credit. Psychological Bulletin, 70(4), 
213-220. 

Crook, N., Granell, R., & Pulman, S. (2009). 
Unsupervised Classification of Dialogue Acts 
Using a Dirichlet Process Mixture Model. 
Proceedings of SIGDIAL, 341–348. 

Ezen-Can, A., & Boyer, K. E. (2013). Unsupervised 
Classification of Student Dialogue Acts With 
Query-likelihood Clustering. International 
Conference on Educational Data Mining, 20–
27. 

Forbes-Riley, K., & Litman, D. J. (2005). Using 
Bigrams to Identify Relationships Between 
Student Certainness States and Tutor Responses 
in a Spoken Dialogue Corpus. Proceedings of 
SIGDIAL, 87–96.  

Henderson, J., Lemon, O., & Georgila, K. (2008). 
Hybrid Reinforcement / Supervised Learning of 
Dialogue Policies from Fixed Data Sets. 
Computational Linguistics, 34(4), 487-511. 

Klein, D., & Manning, C. D. (2003). Accurate 
Unlexicalized Parsing. Proceedings of ACL, 
423–430. 

Landis, J. R., & Koch, G. G. (1994). The 
Measurement of Observer Agreement for 
Categorical Data Data for Categorical of 
Observer Agreement The Measurement. 
International Biometric Society, 33(1), 159–
174. 

Mayfield, E., Adamson, D., & Rosé, C. P. (2012). 
Hierarchical Conversation Structure Prediction 
in Multi-Party Chat. Proceedings of SIGDIAL, 
60–69. 

Ritter, A., Cherry, C., & Dolan, B. (2010). 
Unsupervised Modeling of Twitter 
Conversations. Proceedings of NAACL HLT, 
172–180. 

Rus, V., Moldovan, C., Niraula, N., & Graesser, A. C. 
(2012). Automated Discovery of Speech Act 

Categories in Educational Games. Proceedings 
of the International Conference on Educational 
Data Mining, 25-32. 

Serafin, R., & Di Eugenio, B. (2004). FLSA  : 
Extending Latent Semantic Analysis with 
features for dialogue act classification. 
Proceedings of ACL, 692–699. 

Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, 
R., Jurafsky, D., Taylor, P., et al. (2000). 
Dialogue Act Modeling for Automatic Tagging 
and Recognition of Conversational Speech. 
Computational Linguistics, 26(3), 339–373. 

Vlachos, A. (2011). Evaluating unsupervised learning 
for natural language processing tasks. 
Proceedings of EMNLP, 35–42. 

Williams, J. D., & Young, S. (2003). Using Wizard-
of-Oz simulations to bootstrap Reinforcement-
Learning- based dialog management systems. 
Proceedings of SIGDIAL, 135–139. 

Woszczyna, M., & Waibel, A. (1994). Inferring 
linguistic structure in spoken language. 
Proceedings of ICSLP, 847-850. 

Appendix 

 
Figure 3: Sum of squared errors graph 

 
Table 1: Example student utterances  

and tutor responses 
Student  
Utterance 

Tutor Response 
(Unsupervised 
Policy) 

Tutor Re-
sponse (Manu-
al-Tag Policy) 

I'm trying to think, 
heh                                                                                                                                      

Don't worry 
about it. I'm here 
to help. 

That part is 
tricky. 
 

how can I pull 
values out of an 
array or can I 
reference them 
with code like 
zipDigits[1]?                                                                                                                                                                       

Great question. 
Let's look at the 
task decription 
together to see if 
it can help. 

Good question. 
Let's analyze the 
code together to 
see if it is right. 

thanks for the 
reminder                                                                                                                                                                                                                                              

I'm here to help! Great, seems 
like we agreed.  

does that mean I 
should declare it 
at the top of the 
code?                                                                                                                                                             

Maybe you 
should try it out. 

Good question. 
Let's analyze the 
code together to 
see if it is right. 

 

328


