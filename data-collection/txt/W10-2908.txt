










































(Invited Talk) Bayesian Hidden Markov Models and Extensions


Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 56,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Bayesian Hidden Markov Models and Extensions
Invited Talk

Zoubin Ghahramani
Engineering Department, University of Cambridge, Cambridge, UK

zoubin@eng.cam.ac.uk

Hidden Markov models (HMMs) are one of the cornerstones of time-series modelling. I will review
HMMs, motivations for Bayesian approaches to inference in them, and our work on variational Bayesian
learning. I will then focus on recent nonparametric extensions to HMMs. Traditionally, HMMs have
a known structure with a fixed number of states and are trained using maximum likelihood techniques.
The infinite HMM (iHMM) allows a potentially unbounded number of hidden states, letting the model
use as many states as it needs for the data. The recent development of ’Beam Sampling’ — an efficient
inference algorithm for iHMMs based on dynamic programming — makes it possible to apply iHMMs to
large problems. I will show some applications of iHMMs to unsupervised POS tagging and experiments
with parallel and distributed implementations. I will also describe a factorial generalisation of the iHMM
which makes it possible to have an unbounded number of binary state variables, and can be thought of
as a time-series generalisation of the Indian buffet process. I will conclude with thoughts on future
directions in Bayesian modelling of sequential data.

56


