










































General estimation and evaluation of compositional distributional semantic models


Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 50–58,
Sofia, Bulgaria, August 9 2013. c©2013 Association for Computational Linguistics

General estimation and evaluation
of compositional distributional semantic models

Georgiana Dinu and Nghia The Pham and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)

(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it

Abstract

In recent years, there has been widespread
interest in compositional distributional
semantic models (cDSMs), that derive
meaning representations for phrases from
their parts. We present an evaluation of al-
ternative cDSMs under truly comparable
conditions. In particular, we extend the
idea of Baroni and Zamparelli (2010) and
Guevara (2010) to use corpus-extracted
examples of the target phrases for param-
eter estimation to the other models pro-
posed in the literature, so that all models
can be tested under the same training con-
ditions. The linguistically motivated func-
tional model of Baroni and Zamparelli
(2010) and Coecke et al. (2010) emerges
as the winner in all our tests.

1 Introduction

The need to assess similarity in meaning is cen-
tral to many language technology applications,
and distributional methods are the most robust ap-
proach to the task. These methods measure word
similarity based on patterns of occurrence in large
corpora, following the intuition that similar words
occur in similar contexts. More precisely, vector
space models, the most widely used distributional
models, represent words as high-dimensional vec-
tors, where the dimensions represent (functions
of) context features, such as co-occurring context
words. The relatedness of two words is assessed
by comparing their vector representations.

The question of assessing meaning similarity
above the word level within the distributional
paradigm has received a lot of attention in re-
cent years. A number of compositional frame-
works have been proposed in the literature, each
of these defining operations to combine word vec-
tors into representations for phrases or even en-

tire sentences. These range from simple but ro-
bust methods such as vector addition to more ad-
vanced methods, such as learning function words
as tensors and composing constituents through in-
ner product operations. Empirical evaluations in
which alternative methods are tested in compara-
ble settings are thus called for. This is compli-
cated by the fact that the proposed compositional
frameworks package together a number of choices
that are conceptually distinct, but difficult to disen-
tangle. Broadly, these concern (i) the input repre-
sentations fed to composition; (ii) the composition
operation proper; (iii) the method to estimate the
parameters of the composition operation.

For example, Mitchell and Lapata in their clas-
sic 2010 study propose a set of composition op-
erations (multiplicative, additive, etc.), but they
also experiment with two different kinds of input
representations (vectors recording co-occurrence
with words vs. distributions over latent topics) and
use supervised training via a grid search over pa-
rameter settings to estimate their models. Gue-
vara (2010), to give just one further example, is
not only proposing a different composition method
with respect to Mitchell and Lapata, but he is
also adopting different input vectors (word co-
occurrences compressed via SVD) and an unsu-
pervised estimation method based on minimizing
the distance of composed vectors to their equiva-
lents directly extracted from the source corpus.

Blacoe and Lapata (2012) have recently high-
lighted the importance of teasing apart the differ-
ent aspects of a composition framework, present-
ing an evaluation in which different input vector
representations are crossed with different compo-
sition methods. However, two out of three com-
position methods they evaluate are parameter-free,
so that they can side-step the issue of fixing the pa-
rameter estimation method.

In this work, we evaluate all composition meth-
ods we know of, excluding a few that lag be-

50



hind the state of the art or are special cases of
those we consider, while keeping the estimation
method constant. This evaluation is made pos-
sible by our extension to all target composition
models of the corpus-extracted phrase approxima-
tion method originally proposed in ad-hoc settings
by Baroni and Zamparelli (2010) and Guevara
(2010). For the models for which it is feasible,
we compare the phrase approximation approach
to supervised estimation with crossvalidation, and
show that phrase approximation is competitive,
thus confirming that we are not comparing mod-
els under poor training conditions. Our tests are
conducted over three tasks that involve different
syntactic constructions and evaluation setups. Fi-
nally, we consider a range of parameter settings for
the input vector representations, to insure that our
results are not too brittle or parameter-dependent.1

2 Composition frameworks

Distributional semantic models (DSMs) approxi-
mate word meanings with vectors recording their
patterns of co-occurrence with corpus contexts
(e.g., other words). There is an extensive literature
on how to develop such models and on their eval-
uation (see, e.g., Clark (2012), Erk (2012), Tur-
ney and Pantel (2010)). We focus here on compo-
sitional DSMs (cDSMs). After discussing some
options pertaining to the input vectors, we review
all the composition operations we are aware of
(excluding only the tensor-product-based models
shown by Mitchell and Lapata (2010) to be much
worse than simpler models),2 and then methods to
estimate their parameters.

Input vectors Different studies have assumed
different distributional inputs to composition.
These include bag-of-words co-occurrence vec-
tors, possibly mapped to lower dimensionality
with SVD or other techniques (Mitchell and La-
pata (2010) and many others), vectors whose di-

1We made the software we used to construct seman-
tic models and estimate and test composition methods
available online at http://clic.cimec.unitn.it/
composes/toolkit/

2Erk and Padó (2008) and Thater et al. (2010) use in-
put vectors that have been adapted to their phrasal contexts,
but then apply straightforward composition operations such
as addition and multiplication to these contextualized vec-
tors. Their approaches are thus not alternative cDSMs, but
special ways to construct the input vectors. Grefenstette and
Sadrzadeh (2011a; 2011b) and Kartsaklis et al. (2012) pro-
pose estimation techniques for the tensors in the functional
model of Coecke et al. (2010). Turney (2012) does not com-
pose representations but similarity scores.

Model Composition function Parameters
Add w1~u + w2~v w1, w2
Mult ~uw1 � ~vw2 w1, w2
Dil ||~u||22~v + (λ − 1)〈~u,~v〉~u λ
Fulladd W1~u + W2~v W1, W2 ∈ Rm×m
Lexfunc Au~v Au ∈ Rm×m

Fulllex tanh([W1, W2]
h

Au~v
Av~u

i
) W1, W2,

Au, Av ∈ Rm×m

Table 1: Composition functions of inputs (u, v).

mensions record the syntactic link between targets
and collocates (Erk and Padó, 2008; Thater et al.,
2010), and most recently vectors based on neural
language models (Socher et al., 2011; Socher et
al., 2012). Blacoe and Lapata (2012) compared
the three representations on phrase similarity and
paraphrase detection, concluding that “simple is
best”, that is, the bag-of-words approach performs
at least as good or better than either syntax-based
or neural representations across the board. Here,
we take their message home and we focus on bag-
of-words representations, exploring the impact of
various parameters within this approach.

Most frameworks assume that word vectors
constitute rigid inputs fixed before composition,
often using a separate word-similarity task inde-
pendent of composition. The only exception is
Socher et al. (2012), where the values in the in-
put vectors are re-estimated during composition
parameter optimization. Our re-implementation of
their method assumes rigid input vectors instead.

Composition operations Mitchell and Lapata
(2008; 2010) present a set of simple but effec-
tive models in which each component of the output
vector is a function of the corresponding compo-
nents of the inputs. Given input vectors ~u and ~v,
the weighted additive model (Add) returns their
weighted sum: ~p = w1~u + w2~v. In the dilation
model (Dil), the output vector is obtained by de-
composing one of the input vectors, say ~v, into
a vector parallel to ~u and an orthogonal vector,
and then dilating only the parallel vector by a fac-
tor λ before re-combining (formula in Table 1).
Mitchell and Lapata also propose a simple mul-
tiplicative model in which the output components
are obtained by component-wise multiplication of
the corresponding input components. We intro-
duce here its natural weighted extension (Mult),
that takes w1 and w2 powers of the components
before multiplying, such that each phrase compo-
nent pi is given by: pi = uw1i v

w2
i .

51



Guevara (2010) and Zanzotto et al. (2010) ex-
plore a full form of the additive model (Fulladd),
where the two vectors entering a composition pro-
cess are pre-multiplied by weight matrices before
being added, so that each output component is
a weighted sum of all input components: ~p =
W1~u + W2~v.

Baroni and Zamparelli (2010) and Coecke et
al. (2010), taking inspiration from formal seman-
tics, characterize composition as function applica-
tion. For example, Baroni and Zamparelli model
adjective-noun phrases by treating the adjective
as a function from nouns onto (modified) nouns.
Given that linear functions can be expressed by
matrices and their application by matrix-by-vector
multiplication, a functor (such as the adjective) is
represented by a matrix Au to be composed with
the argument vector ~v (e.g., the noun) by multi-
plication, returning the lexical function (Lexfunc)
representation of the phrase: ~p = Au~v.

The method proposed by Socher et al. (2012)
(see Socher et al. (2011) for an earlier proposal
from the same team) can be seen as a combination
and non-linear extension of Fulladd and Lexfunc
(that we thus call Fulllex) in which both phrase
elements act as functors (matrices) and arguments
(vectors). Given input terms u and v represented
by (~u, Au) and (~v, Av), respectively, their com-
position vector is obtained by applying first a lin-
ear transformation and then the hyperbolic tangent
function to the concatenation of the products Au~v
and Av~u (see Table 1 for the equation). Socher
and colleagues also present a way to construct ma-
trix representations for specific phrases, needed
to scale this composition method to larger con-
stituents. We ignore it here since we focus on the
two-word case.

Estimating composition parameters If we
have manually labeled example data for a target
task, we can use supervised machine learning to
optimize parameters. Mitchell and Lapata (2008;
2010), since their models have just a few param-
eters to optimize, use a direct grid search for the
parameter setting that performs best on the train-
ing data. Socher et al. (2012) train their models
using multinomial softmax classifiers.

If our goal is to develop a cDSM optimized for
a specific task, supervised methods are undoubt-
edly the most promising approach. However, ev-
ery time we face a new task, parameters must be
re-estimated from scratch, which goes against the

idea of distributional semantics as a general sim-
ilarity resource (Baroni and Lenci, 2010). More-
over, supervised methods are highly composition-
model-dependent, and for models such as Fulladd
and Lexfunc we are not aware of proposals about
how to estimate them in a supervised manner.

Socher et al. (2011) propose an autoencoding
strategy. Given a decomposition function that re-
constructs the constituent vectors from a phrase
vector (e.g., it re-generates green and jacket vec-
tors from the composed green jacket vector), the
composition parameters minimize the distance be-
tween the original and reconstructed input vectors.
This method does not require hand-labeled train-
ing data, but it is restricted to cDSMs for which
an appropriate decomposition function can be de-
fined, and even in this case the learning problem
might lack a closed-form solution.

Guevara (2010) and Baroni and Zamparelli
(2010) optimize parameters using examples of
how the output vectors should look like that are
directly extracted from the corpus. To learn, say, a
Lexfunc matrix representing the adjective green,
we extract from the corpus example vectors of
〈N, green N〉 pairs that occur with sufficient fre-
quency (〈car, green car〉, 〈jacket, green jacket〉,
〈politician, green politician〉, . . . ). We then use
least-squares methods to find weights for the green
matrix that minimize the distance between the
green N vectors generated by the model given the
input N and the corresponding corpus-observed
phrase vectors. This is a very general approach, it
does not require hand-labeled data, and it has the
nice property that corpus-harvested phrase vec-
tors provide direct evidence of the polysemous be-
haviour of functors (the green jacket vs. politician
contexts, for example, will be very different). In
the next section, we extend the corpus-extracted
phrase approximation method to all cDSMs de-
scribed above, with closed-form solutions for all
but the Fulllex model, for which we propose a
rapidly converging iterative estimation method.

3 Least-squares model estimation using
corpus-extracted phrase vectors3

Notation Given two matrices X, Y ∈ Rm×n we
denote their inner product by 〈X, Y 〉, (〈X, Y 〉 =∑m

i=1

∑n
j=1 xijyij). Similarly we denote by

〈u, v〉 the dot product of two vectors u, v ∈ Rm×1
and by ||u|| the Euclidean norm of a vector:

3Proofs omitted due to space constraints.

52



||u|| = 〈u, u〉1/2. We use the following Frobe-
nius norm notation: ||X||F = 〈X, X〉1/2. Vectors
are assumed to be column vectors and we use xi
to stand for the i-th (m × 1)-dimensional column
of matrix X . We use [X, Y ] ∈ Rm×2n to denote
the horizontal concatenation of two matrices while[

X
Y

]
∈ R2m×n is their vertical concatenation.

General problem statement We assume vocab-
ularies of constituents U , V and that of resulting
phrases P . The training data consist of a set of
tuples (u, v, p) where p stands for the phrase asso-
ciated to the constituents u and v:

T = {(ui, vi, pi)|(ui, vi, pi) ∈ U×V×P, 1 ≤ i ≤ k}

We build the matrices U, V, P ∈ Rm×k by con-
catenating the vectors associated to the training
data elements as columns.4

Given the training data matrices, the general
problem can be stated as:

θ∗ = arg min
θ

||P − fcompθ(U, V )||F

where fcompθ is a composition function and θ
stands for a list of parameters that this composition
function is associated to. The composition func-
tions are defined: fcompθ : Rm×1 × Rm×1 →
Rm×1 and fcompθ(U, V ) stands for their natural
extension when applied on the individual columns
of the U and V matrices.

Add The weighted additive model returns the
sum of the composing vectors which have been
re-weighted by some scalars w1 and w2: ~p =
w1~u + w2~v. The problem becomes:

w∗1, w
∗
2 = arg min

w1,w2∈R
||P − w1U − w2V ||F

The optimal w1 and w2 are given by:

w∗1 =
||V ||2F 〈U,P 〉 − 〈U, V 〉〈V, P 〉
||U ||2F ||V ||2F − 〈U, V 〉2

(1)

w∗2 =
||U ||2F 〈V, P 〉 − 〈U, V 〉〈U,P 〉
||U ||2F ||V ||2F − 〈U, V 〉2

(2)

4In reality, not all composition models require u, v and p
to have the same dimensionality.

Dil Given two vectors ~u and ~v, the dilation
model computes the phrase vector ~p = ||~u||2~v +
(λ − 1)〈~u,~v〉~u where the parameter λ is a scalar.
The problem becomes:

λ∗ = arg min
λ∈R

||P −V D||ui||2−UD(λ−1)〈ui,vi〉||F

where by D||ui||2 and D(λ−1)〈ui,vi〉 we denote
diagonal matrices with diagonal elements (i, i)
given by ||ui||2 and (λ − 1)〈ui, vi〉 respectively.
The solution is:

λ∗ = 1−
∑k

i=1〈ui, (||ui||2vi − pi)〉〈ui, vi〉∑k
i=1〈ui, vi〉2||ui||2

Mult Given two vectors ~u and ~v, the weighted
multiplicative model computes the phrase vector
~p = ~uw1 � ~vw2 where � stands for component-
wise multiplication. We assume for this model that
U, V, P ∈ Rm×n++ , i.e. that the entries are strictly
larger than 0: in practice we add a small smooth-
ing constant to all elements to achieve this (Mult
performs badly on negative entries, such as those
produced by SVD). We use the w1 and w2 weights
obtained when solving the much simpler related
problem:5

w∗1, w
∗
2 = arg min

w1,w2∈R
||log(P )−log(U.∧w1�V.∧w2)||F

where .∧ stands for the component-wise power op-
eration. The solution is the same as that for Add,
given in equations (1) and (2), with U → log(U),
V → log(V ) and P → log(P ).

Fulladd The full additive model assumes the
composition of two vectors to be ~p = W1~u+W2~v
where W1,W2 ∈ Rm×m. The problem is:

[W1,W2]∗ = arg min
[W1,W2]∈Rm×2m

||P−[W1W2]
[
U

V

]
||

This is a multivariate linear regression prob-
lem (Hastie et al., 2009) for which the least
squares estimate is given by: [W1,W2] =
((XT X)−1XT Y )T where we use X = [UT , V T ]
and Y = P T .

Lexfunc The lexical function composition
method learns a matrix representation for each
functor (given by U here) and defines composition
as matrix-vector multiplication. More precisely:

5In practice training Mult this way achieves similar or
lower errors in comparison to Add.

53



~p = Au~v where Au is a matrix associated to each
functor u ∈ U . We denote by Tu the training
data subset associated to an element u, which
contains only tuples which have u as first element.
Learning the matrix representations amounts to
solving the set of problems:

Au = arg min
Au∈Rm×m

||Pu −AuVu||

for each u ∈ U where Pu, Vu ∈ Rm×|Tu|
are the matrices corresponding to the Tu train-
ing subset. The solutions are given by: Au =
((VuV Tu )

−1VuP
T
u )

T . This composition function
does not use the functor vectors.

Fulllex This model can be seen as a generaliza-
tion of Lexfunc which makes no assumption on
which of the constituents is a functor, so that both
words get a matrix and a vector representation.
The composition function is:

~p = tanh([W1,W2]
[
Au~v

Av~u

]
)

where Au and Av are the matrices associated to
constituents u and v and [W1,W2] ∈ Rm×2m.
The estimation problem is given in Figure 1.

This is the only composition model which does
not have a closed-form solution. We use a block
coordinate descent method, in which we fix each
of the matrix variables but one and solve the corre-
sponding least-squares linear regression problem,
for which we can use the closed-form solution.
Fixing everything but [W1,W2]:

[W ∗1 ,W
∗
2 ] = ((X

T X)−1XT Y )T

X =
[
[Au1 ~v1, ..., Auk ~vk]
[Av1 ~u1, ..., Avk ~uk]

]T
Y = atanh(P T )

Fixing everything but Au for some element u,
the objective function becomes:

||atanh(Pu)−W1AuVu−W2[Av1~u, ..., Avk′~u]||F

where v1...vk′ ∈ V are the elements occurring
with u in the training data and Vu the matrix result-
ing from their concatenation. The update formula
for the Au matrices becomes:

A∗u = W
−1
1 ((X

T X)−1XT Y )T

X = V Tu
Y = (atanh(Pu)−W2[Av1~u, ..., Avk′~u])

T

In all our experiments, Fulllex estimation con-
verges after very few passes though the matrices.
Despite the very large number of parameters of
this model, when evaluating on the test data we ob-
serve that using a higher dimensional space (such
as 200 dimensions) still performs better than a
lower dimensional one (e.g., 50 dimensions).

4 Evaluation setup and implementation

4.1 Datasets

We evaluate the composition methods on three
phrase-based benchmarks that test the models on
a variety of composition processes and similarity-
based tasks.

Intransitive sentences The first dataset, intro-
duced by Mitchell and Lapata (2008), focuses on
simple sentences consisting of intransitive verbs
and their noun subjects. It contains a total of
120 sentence pairs together with human similar-
ity judgments on a 7-point scale. For exam-
ple, conflict erupts/conflict bursts is scored 7, skin
glows/skin burns is scored 1. On average, each
pair is rated by 30 participants. Rather than eval-
uating against mean scores, we use each rating as
a separate data point, as done by Mitchell and La-
pata. We report Spearman correlations between
human-assigned scores and model cosine scores.

Adjective-noun phrases Turney (2012) intro-
duced a dataset including both noun-noun com-
pounds and adjective-noun phrases (ANs). We
focus on the latter, and we frame the task dif-
ferently from Turney’s original definition due to
data sparsity issues.6 In our version, the dataset
contains 620 ANs, each paired with a single-
noun paraphrase. Examples include: archaeolog-
ical site/dig, spousal relationship/marriage and
dangerous undertaking/adventure. We evaluate a
model by computing the cosine of all 20K nouns in
our semantic space with the target AN, and look-
ing at the rank of the correct paraphrase in this list.
The lower the rank, the better the model. We re-
port median rank across the test items.

Determiner phrases The last dataset, intro-
duced in Bernardi et al. (2013), focuses on a
class of grammatical terms (rather than content

6Turney used a corpus of about 50 billion words, almost
20 times larger than ours, and we have very poor or no cov-
erage of many original items, making the “multiple-choice”
evaluation proposed by Turney meaningless in our case.

54



W ∗1 ,W
∗
2 , A

∗
u1 , ..., A

∗
v1 , ... =arg min

Rm×m
||atanh(P T )− [W1,W2]

[
[Au1 ~v1, ..., Auk ~vk]
[Av1 ~u1, ..., Avk ~uk]

]
||F

=arg min
Rm×m

||atanh(P T )−W1[Au1 ~v1, ..., Auk ~vk]−W2[Av1 ~u1, ..., Avk ~uk]||F

Figure 1: Fulllex estimation problem.

words), namely determiners. It is a multiple-
choice test where target nouns (e.g., amnesia)
must be matched with the most closely related
determiner(-noun) phrases (DPs) (e.g., no mem-
ory). The task differs from the previous one also
because here the targets are single words, and the
related items are composite. There are 173 tar-
get nouns in total, each paired with one correct
DP response, as well as 5 foils, namely the de-
terminer (no) and noun (memory) from the correct
response and three more DPs, two of which con-
tain the same noun as the correct phrase (less mem-
ory, all memory), the third the same determiner
(no repertoire). Other examples of targets/related-
phrases are polysemy/several senses and tril-
ogy/three books. The models compute cosines be-
tween target noun and responses and are scored
based on their accuracy at ranking the correct
phrase first.

4.2 Input vectors

We extracted distributional semantic vectors us-
ing as source corpus the concatenation of ukWaC,
Wikipedia (2009 dump) and BNC, 2.8 billion to-
kens in total.7 We use a bag-of-words approach
and we count co-occurrences within sentences and
with a limit of maximally 50 words surrounding
the target word. By tuning on the MEN lexical
relatedness dataset,8 we decided to use the top
10K most frequent content lemmas as context fea-
tures (vs. top 10K inflected forms), and we experi-
mented with positive Pointwise and Local Mutual
Information (Evert, 2005) as association measures
(vs. raw counts, log transform and a probability
ratio measure) and dimensionality reduction by
Non-negative Matrix Factorization (NMF, Lee and
Seung (2000)) and Singular Value Decomposition
(SVD, Golub and Van Loan (1996)) (both outper-
forming full dimensionality vectors on MEN). For

7http://wacky.sslmit.unibo.it;
http://www.natcorp.ox.ac.uk

8http://clic.cimec.unitn.it/∼elia.
bruni/MEN

both reduction techniques, we varied the number
of dimensions to be preserved from 50 to 300 in
50-unit intervals. As Local Mutual Information
performed very poorly across composition exper-
iments and other parameter choices, we dropped
it. We will thus report, for each experiment and
composition method, the distribution of the rele-
vant performance measure across 12 input settings
(NMF vs. SVD times 6 dimensionalities). How-
ever, since the Mult model, as expected, worked
very poorly when the input vectors contained neg-
ative values, as is the case with SVD, for this
model we report result distributions across the 6
NMF variations only.

4.3 Composition model estimation

Training by approximating the corpus-extracted
phrase vectors requires corpus-based examples of
input (constituent word) and output (phrase) vec-
tors for the composition processes to be learned.
In all cases, training examples are simply selected
based on corpus frequency. For the first experi-
ment, we have 42 distinct target verbs and a total
of ≈20K training instances, that is, 〈〈noun, verb〉,
noun-verb〉 tuples (505 per verb on average). For
the second experiment, we have 479 adjectives and
≈1 million 〈〈adjective, noun〉, adjective-noun〉
training tuples (2K per adjective on average). In
the third, 50 determiners and 50K 〈〈determiner,
noun〉, determiner-noun〉 tuples (1K per deter-
miner). For all models except Lexfunc and Ful-
llex, training examples are pooled across target el-
ements to learn a single set of parameters. The
Lexfunc model takes only argument word vectors
as inputs (the functors in the three datasets are
verbs, adjectives and determiners, respectively). A
separate weight matrix is learned for each func-
tor, using the corresponding training data.9 The
Fulllex method jointly learns distinct matrix rep-
resentations for both left- and right-hand side con-

9For the Lexfunc model we have experimented with least
squeares regression with and without regularization, obtain-
ing similar results.

55



stituents. For this reason, we must train this model
on balanced datasets. More precisely, for the in-
transitive verb experiments, we use training data
containing noun-verb phrases in which the verbs
and the nouns are present in the lists of 1,500
most frequent verbs/nouns respectively, adding to
these the verbs and nouns present in our dataset.
We obtain 400K training tuples. We create the
training data similarity for the other datasets ob-
taining 440K adjective-noun and 50K determiner
phrase training tuples, respectively (we also exper-
imented with Fulllex trained on the same tuples
used for the other models, obtaining considerably
worse results than those reported). Finally, for Dil
we treat direction of stretching as a further param-
eter to be optimized, and find that for intransitives
it is better to stretch verbs, in the other datasets
nouns.

For the simple composition models for which
parameters consist of one or two scalars, namely
Add, Mult and Dil, we also tune the parame-
ters through 5-fold crossvalidation on the datasets,
directly optimizing the parameters on the target
tasks. For Add and Mult, we search w1, w2
through the crossproduct of the interval [0 : 5] in
0.2-sized steps. For Dil we use λ ∈ [0 : 20], again
in 0.2-sized steps.

5 Evaluation results

We begin with some remarks pertaining to the
overall quality of and motivation for corpus-
phrase-based estimation. In seven out of nine
comparisons of this unsupervised technique with
fully supervised crossvalidation (3 “simple” mod-
els –Add, Dil and Mult– times 3 test sets), there
was no significant difference between the two esti-
mation methods.10 Supervised estimation outper-
formed the corpus-phrase-based method only for
Dil on the intransitive sentence and AN bench-
marks, but crossvalidated Dil was outperformed
by at least one phrase-estimated simple model on
both benchmarks.

The rightmost boxes in the panels of Fig-
ure 2 depict the performance distribution for us-
ing phrase vectors directly extracted from the
corpus to tackle the various tasks. This non-
compositional approach outperforms all composi-
tional methods in two tasks over three, and it is
one of the best approaches in the third, although

10Significance assessed through Tukey Honestly Signifi-
cant Difference tests (Abdi and Williams, 2010), α = 0.05.

in all cases even its top scores are far from the
theoretical ceiling. Still, performance is impres-
sive, especially in light of the fact that the non-
compositional approach suffers of serious data-
sparseness problems. Performance on the intran-
sitive task is above state-of-the-art despite the fact
that for almost half of the cases one test phrase
is not in the corpus, resulting in 0 vectors and
consequently 0 similarity pairs. The other bench-
marks have better corpus-phrase coverage (nearly
perfect AN coverage; for DPs, about 90% correct
phrase responses are in the corpus), but many tar-
get phrases occur only rarely, leading to unreliable
distributional vectors. We interpret these results as
a good motivation for corpus-phrase-based estima-
tion. On the one hand they show how good these
vectors are, and thus that they are sensible targets
of learning. On the other hand, they do not suffice,
since natural language is infinitely productive and
thus no corpus can provide full phrase coverage,
justifying the whole compositional enterprise.

The other boxes in Figure 2 report the perfor-
mance of the composition methods trained by cor-
pus phrase approximation. Nearly all models are
significantly above chance in all tasks, except for
Fulladd on intransitive sentences. To put AN me-
dian ranks into perspective, consider that a median
rank as high as 8,300 has near-0 probability to oc-
cur by chance. For DP accuracy, random guessing
gets 0.17% accuracy.

Lexfunc emerges consistently as the best model.
On intransitive constructions, it significantly out-
performs all other models except Mult, but the dif-
ference approaches significance even with respect
to the latter (p = 0.071). On this task, Lexfunc’s
median correlation (0.26) is nearly equivalent to
the best correlation across a wide range of parame-
ters reported by Erk and Padó (2008) (0.27). In the
AN task, Lexfunc significantly outperforms Ful-
llex and Dil and, visually, its distribution is slightly
more skewed towards lower (better) ranks than any
other model. In the DP task, Lexfunc significantly
outperforms Add and Mult and, visually, most of
its distribution lies above that of the other mod-
els. Most importantly, Lexfunc is the only model
that is consistent across the three tasks, with all
other models displaying instead a brittle perfor-
mance pattern.11

Still, the top-performance range of all models
11No systematic trend emerged pertaining to the input vec-

tor parameters (SVD vs. NMF and retained dimension num-
ber).

56



A
d

d

D
il

M
u

lt

F
u

lla
d

d

L
e
x
fu

n
c

F
u

lll
e
x

C
o

rp
u

s

0
.0

0
0

.0
5

0
.1

0
0

.1
5

0
.2

0
0

.2
5

0
.3

0 Intransitive sentences

●

●

●

●

●

A
d

d

D
il

M
u

lt

F
u

lla
d

d

L
e
x
fu

n
c

F
u

lll
e
x

C
o

rp
u

s

1
0

0
0

8
0

0
6

0
0

4
0

0
2

0
0

ANs

●

●

A
d

d

D
il

M
u

lt

F
u

lla
d

d

L
e
x
fu

n
c

F
u

lll
e
x

C
o

rp
u

s

0
.1

5
0

.2
0

0
.2

5
0

.3
0

0
.3

5

DPs

Figure 2: Boxplots displaying composition model performance distribution on three benchmarks, across
input vector settings (6 datapoints for Mult, 12 for all other models). For intransitive sentences, figure of
merit is Spearman correlation, for ANs median rank of correct paraphrase, and for DPs correct response
accuracy. The boxplots display the distribution median as a thick horizontal line within a box extending
from first to third quartile. Whiskers cover 1.5 of interquartile range in each direction from the box, and
extreme outliers outside this extended range are plotted as circles.

on the three tasks is underwhelming, and none of
them succeeds in exploiting compositionality to
do significantly better than using whatever phrase
vectors can be extracted from the corpus directly.
Clearly, much work is still needed to develop truly
successful cDSMs.

The AN results might look particularly worry-
ing, considering that even the top (lowest) median
ranks are above 100. A qualitative analysis, how-
ever, suggests that the actual performance is not
as bad as the numerical scores suggest, since of-
ten the nearest neighbours of the ANs to be para-
phrased are nouns that are as strongly related to
the ANs as the gold standard response (although
not necessarily proper paraphrases). For example,
the gold response to colorimetric analysis is col-
orimetry, whereas the Lexfunc (NMF, 300 dimen-
sions) nearest neighbour is chromatography; the
gold response to heavy particle is baryon, whereas
Lexfunc proposes muon; for melodic phrase the
gold is tune and Lexfunc has appoggiatura; for in-
door garden, the gold is hothouse but Lexfunc pro-
poses glasshouse (followed by the more sophisti-
cated orangery!), and so on and so forth.

6 Conclusion

We extended the unsupervised corpus-extracted
phrase approximation method of Guevara (2010)
and Baroni and Zamparelli (2010) to estimate

all known state-of-the-art cDSMs, using closed-
form solutions or simple iterative procedures in
all cases. Equipped with a general estimation ap-
proach, we thoroughly evaluated the cDSMs in
a comparable setting. The linguistically moti-
vated Lexfunc model of Baroni and Zamparelli
(2010) and Coecke et al. (2010) was the win-
ner across three composition tasks, also outper-
forming the more complex Fulllex model, our re-
implementation of Socher et al.’s (2012) compo-
sition method (of course, the composition method
is only one aspect of Socher et al.’s architecture).
All other composition methods behaved inconsis-
tently.

In the near future, we want to focus on improv-
ing estimation itself. In particular, we want to
explore ways to automatically select good phrase
examples for training, beyond simple frequency
thresholds. We tested composition methods on
two-word phrase benchmarks. Another natural
next step is to apply the composition rules recur-
sively, to obtain representations of larger chunks,
up to full sentences, coming, in this way, nearer to
the ultimate goal of compositional distributional
semantics.

Acknowledgments

We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).

57



References
Hervé Abdi and Lynne Williams. 2010. Newman-

Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897–904. Sage, Thousand
Oaks, CA.

Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.

Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of ACL (Short
Papers), Sofia, Bulgaria. In press.

William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546–556, Jeju Island, Korea.

Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.

Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345–384.

Katrin Erk and Sebastian Padó. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897–906, Honolulu,
HI.

Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635–653.

Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.

Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.

Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Pro-
ceedings of EMNLP, pages 1394–1404, Edinburgh,
UK.

Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a Dis-
CoCat. In Proceedings of GEMS, pages 62–66, Ed-
inburgh, UK.

Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33–37,
Uppsala, Sweden.

Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2009. The Elements of Statistical Learning,
2nd ed. Springer, New York.

Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for
categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING: Posters, pages 549–558, Mumbai, India.

Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556–562.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236–244, Columbus, OH.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.

Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801–809, Granada, Spain.

Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Jeju Island, Ko-
rea.

Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948–957, Uppsala, Sweden.

Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.

Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.

Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263–
1271, Beijing, China.

58


