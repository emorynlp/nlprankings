










































Towards Semi-Supervised Classification of Discourse Relations using Feature Correlations


Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 55–58,
The University of Tokyo, September 24-25, 2010. c©2010 Association for Computational Linguistics

Towards Semi-Supervised Classification of Discourse Relations using
Feature Correlations

Hugo Hernault and Danushka Bollegala and Mitsuru Ishizuka
Graduate School of Information Science & Technology

The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan

hugo@mi.ci.i.u-tokyo.ac.jp
danushka@iba.t.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp

Abstract
Two of the main corpora available for
training discourse relation classifiers are
the RST Discourse Treebank (RST-DT)
and the Penn Discourse Treebank (PDTB),
which are both based on the Wall Street
Journal corpus. Most recent work us-
ing discourse relation classifiers have em-
ployed fully-supervised methods on these
corpora. However, certain discourse rela-
tions have little labeled data, causing low
classification performance for their asso-
ciated classes. In this paper, we attempt
to tackle this problem by employing a
semi-supervised method for discourse re-
lation classification. The proposed method
is based on the analysis of feature co-
occurrences in unlabeled data. This in-
formation is then used as a basis to ex-
tend the feature vectors during training.
The proposed method is evaluated on both
RST-DT and PDTB, where it significantly
outperformed baseline classifiers. We be-
lieve that the proposed method is a first
step towards improving classification per-
formance, particularly for discourse rela-
tions lacking annotated data.

1 Introduction

The RST Discourse Treebank (RST-DT) (Carl-
son et al., 2001), based on the Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
framework, and the Penn Discourse Treebank
(PDTB) (Prasad et al., 2008), are two of the most
widely-used corpora for training discourse rela-
tion classifiers. They are both based on the Wall
Street Journal (WSJ) corpus, although there are
substantial differences in the relation taxonomy
used to annotate the corpus. These corpora have
been used in most of the recent work employ-
ing discourse relation classifiers, which are based

on fully-supervised machine learning approaches
(duVerle and Prendinger, 2009; Pitler et al., 2009;
Lin et al., 2009).

Still, when building a discourse relation clas-
sifier on either corpus, one is faced with the
same practical issue: Certain relations are very
prevalent, such as ELABORATION[N][S] (RST-
DT), with more than 4000 instances, whereas
other occur rarely, such as EVALUATION[N][N]1

(RST-DT), with three instances, or COMPARI-
SON.PRAGMATIC CONCESSION (PDTB), with 12
instances. This lack of training data causes poor
classification performance on the classes associ-
ated to these relations.

In this paper, we try to tackle this problem by
using feature co-occurrence information, extracted
from unlabeled data, as a way to inform the classi-
fier when unseen features are found in test vectors.
The advantage of the method is that it relies solely
on unlabeled data, which is abundant, and cheap
to collect.

The contributions of this paper are the follow-
ing: First, we propose a semi-supervised method
that exploits the abundant, freely-available un-
labeled data, which is harvested for feature co-
occurrence information, and used as a basis to ex-
tend feature vectors to help classification for cases
where unknown features are found in test vec-
tors. Second, the proposed method is evaluated
on the RST-DT and PDTB corpus, where it signif-
icantly improves F-score when trained on moder-
ately small datasets. For instance, when trained on
a dataset with around 1000 instances, the proposed
method increases the macro-average F-score up to
30%, compared to a baseline classifier.

2 Related Work

Since the release in 2002 of the RST-DT corpus,
several fully-supervised discourse parsers have

1We use the notation [N] and [S] respectively to denote
the nucleus and satellite in a RST discourse relation.

55



been built in the RST framework. In duVerle and
Prendinger (2009), a discourse parser based on
Support Vector Machines (SVM) (Vapnik, 1995)
is proposed. Shallow lexical, syntactic and struc-
tural features, including ‘dominance sets’ (Soricut
and Marcu, 2003) are used.

The unsupervised method of Marcu and Echi-
habi (2002) was the first to try to detect ‘implicit’
relations (i.e. relations not accompanied by a cue
phrase, such as ‘however’, ‘but’), using word pairs
extracted from two spans of text. Their method
attempts to capture the difference of polarity in
words.

Discourse relation classifiers have also been
trained using PDTB. Pitler et al. (2008) performed
a corpus study of the PDTB, and found that ‘ex-
plicit’ relations can be most of the times distin-
guished by their discourse connectives.

Lin et al. (2009) studied the problem of detect-
ing implicit relations in PDTB. Their relational
classifier is trained using features extracted from
dependency paths, contextual information, word
pairs and production rules in parse trees. For the
same task, Pitler et al. (2009) also use word pairs,
as well as several other types of features such as
verb classes, modality, context, and lexical fea-
tures.

In this paper, we are not aiming at defining
novel features for improving performance in RST
or PDTB relation classification. Instead we incor-
porate features that have already shown to be use-
ful for discourse relation learning and explore the
possibilities of using unlabeled data for this task.

3 Method

In this section, we describe a semi-supervised
method for relation classification, based on feature
vector extension. The extension process employs
feature co-occurrence information. Co-occurrence
information is useful in this context as, for in-
stance, we might know that the word pair (for,
when) is a good indicator of a TEMPORAL rela-
tion. Or, after analyzing a large body of unlabeled
data, we might also notice that this word pair co-
occurs often with the word ‘run-up’ placed at the
end of a span of text. Suppose now that we have to
classify a test instance containing the feature ‘run-
up’, but not the word pair (for, when). In this case,
by using the co-occurrence information, we know
that the instance has a chance of being a TEM-
PORAL relation. We first explain how to compute

a feature correlation matrix, using unlabeled data.
In a second section, we show how to extend fea-
ture vectors in order to include co-occurrence in-
formation. Finally, we describe the features used
in the discourse relation classifiers.

3.1 Feature Correlation Matrix
A training/test instance is represented using a d-
dimensional feature vector f = [f1, . . . , fd]T,
where fi ∈ {0, 1}. We define a feature correla-
tion matrix, C such that the (i, j)-th element of
C, C(i,j) ∈ {0, 1} denotes the correlation between
the two features fi and fj . If both fi and fj appear
in a feature vector then we define them to be co-
occurring. The number of different feature vectors
in which fi and fj co-occur is used as a basis to
compute C(i,j). Importantly, feature correlations
can be calculated using only unlabeled data.

It is noteworthy that feature correlation matri-
ces can be computed using any correlation mea-
sure. For the current task we use the χ2-measure
(Plackett, 1983) as the preferred correlation mea-
sure because of its simplicity. We create the fea-
ture correlation matrix C, such that, for all pairs of
features (fi, fj),

C(i,j) =
{

1 if χ2i,j > c
0 otherwise

. (1)

Here c is the critical value, which, for a confi-
dence level of 0.05 and one degree of freedom, can
be set to 3.84.

3.2 Feature Vector Extension
Once the feature correlation matrix is computed
using unlabeled data as described in Section 3.1,
we can use it to extend a feature vector during
testing. One of the reasons explaining why a clas-
sifier might perform poorly on a test instance, is
that there are features in the test instance that were
not observed during training. Let us represent the
feature vector corresponding to a test instance x
by fx. Then, we use the feature correlation ma-
trix to find the set of correlated features Fc(fi) of
a particular feature fi that occur in fx.

Specifically, for a feature fi ∈ fx, F ′(fi) con-
sists of features fj , where C(i,j) = 1. We define
the extended feature vector f ′x of fx as the union of
all the features that appear in fx and Fc(fx). Since
a discourse relation is defined between two spans
of short texts (elementary discourse units), which
are typically two clauses or sentences, a particu-
lar feature does not usually occur more than once

56



in a feature vector. Therefore, we introduced the
proposed method in the context of binary valued
features. However, the above mentioned discus-
sion can be naturally extended to cover real-valued
features.

3.3 Features

Figure 1 shows the parse tree for a sentence com-
posed of two discourse units, which serve as argu-
ments of a discourse relation we want to generate
a feature vector from. Lexical heads have been
calculated using the projection rules of Magerman
(1995), and indicated between brackets. For each
argument, surrounded by dots, is the minimal set
of sub-parse trees containing strictly all the words
of the argument.

We extract all possible lemmatized word pairs
from the two arguments. Next, we extract from
left and right argument separately, all production
rules from the sub-parse trees. Finally, we encode
in our features three nodes of the parse tree, which
capture the local context at the connection point
between the two arguments (Soricut and Marcu,
2003): The first node, which we call Nw, is the
highest ancestor of the first argument’s last word
w, and is such that Nw’s right-sibling is the an-
cestor of the second argument’s first word. Nw’s
right-sibling node is calledNr. Finally, we callNp
the parent of Nw and Nr. For each node, we en-
code in the feature vector its part-of-speech (POS)
and lexical head. For instance, in Figure 1, we
have Nw = S(comment), Nr = SBAR(when), and
Np = VP(declined).

4 Experiments

It is worth noting that the proposed method is inde-
pendent of any particular classification algorithm.
As our goal is strictly to evaluate the relative ben-
efit of employing the proposed method, we se-
lect a logistic regression classifier, for its simplic-
ity. We used the multi-class logistic regression
(maximum entropy model) implemented in Clas-
sias (Okazaki, 2009). Regularization parameters
are set to their default value of one.

Unlabeled instances are created by selecting
texts of the WSJ, and segmenting them into ele-
mentary discourse units (EDUs) using our sequen-
tial discourse segmenter (Hernault et al., 2010).
As there is no segmentation tool for the PDTB
framework, we assumed that feature correlation
information taken from EDUs created using a RST

segmenter is also useful for extending feature vec-
tors of PDTB relations.

Since we are interested in measuring the over-
all performance of a discourse relation classifier
across all relation types, we use macro-averaged
F-score as the preferred evaluation metric for this
task. We train a multi-class logistic regression
model without extending the feature vectors as
a baseline method. This baseline is expected to
show the effect of using the proposed feature ex-
tension approach for the task of discourse relation
learning.

Experimental results on RST-DT and PDTB
datasets are depicted in Figures 2 and 3. We ob-
serve that the proposed feature extension method
outperforms the baseline for both RST-DT and
PDTB datasets for the full range of training dataset
sizes. However, the difference between the two
methods decreases as we increase the amount of
training data. Specifically, with 200 training in-
stances, for RST-DT, the baseline method has a
macro-averaged F-score of 0.079, whereas the the
proposed method has a macro-averaged F-score
of 0.159 (around 101% increase in F-score). For
1000 training instances, the F-score for RST-DT
increases by 29.2%, from 0.143 to 0.185, while
the F-score for PDTB increases by 27.9%, from
0.109 to 0.139. However, the difference between
the two methods diminishes beyond 10000 train-
ing instances.

0 5000 10000 15000 20000
Number of training instances

0.05

0.10

0.15

0.20

0.25

0.30

M
a
cr

o
-a

v
e
ra

g
e
 F

-s
co

re

Proposed method

Baseline RST-DT

Figure 2: Macro-average F-score (RST-DT) as a
function of the number of training instances used.

5 Conclusion

We presented a semi-supervised method for im-
proving the performance of discourse relation
classifiers. The proposed method is based on
the analysis of co-occurrence information har-
vested from unlabeled data only. We evaluated

57



NP (Sherry)

S (declined)

VP (declined)
NNP NNP

declined

VBD (declined)

Mr. Sherry to

VP (comment)

comment when asked about the sales

TO VP

SBAR (when)

WHADVP (when)

WRB

S (asked)

VP (asked)

VBN PP (about)

IN NP (sales)
DT NNS

.

. (.)

Argument 1 Argument 2

VB

S (comment)

Figure 1: Two arguments of a discourse relation, and the minimum set of subtrees that contain them—
lexical heads are indicated between brackets.

0 2000 4000 6000 8000 10000
Number of training instances

0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

M
a
cr

o
-a

v
e
ra

g
e
 F

-s
co

re

Proposed method

Baseline PDTB

Figure 3: Macro-average F-score (PDTB) as a
function of the number of training instances used.

the method on two of the most widely-used dis-
course corpora, RST-DT and PDTB. The method
performs significantly better than a baseline classi-
fier trained on the same features, especially when
the number of labeled instances used for training is
small. For instance, using 1000 training instances,
we observed an increase of nearly 30% in macro-
average F-score. This is an interesting perspective
for improving classification performance of rela-
tions with little training data. In the future, we
plan to improve the method by employing ranked
co-occurrences. This way, only the most relevant
correlated features can be selected during feature
vector extension. Finally, we plan to investigate
using larger amounts of unlabeled training data.

References

L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. Proc. of Sec-
ond SIGdial Workshop on Discourse and Dialogue-
Volume 16, pages 1–10.

D. A. duVerle and H. Prendinger. 2009. A novel
discourse parser based on Support Vector Machine
classification. In Proc. of ACL’09, pages 665–673.

H. Hernault, D. Bollegala, and M. Ishizuka. 2010.
A sequential model for discourse segmentation. In
Proc. of CICLing’10, pages 315–326.

Z. Lin, M-Y. Kan, and H. T. Ng. 2009. Recognizing
implicit discourse relations in the Penn Discourse
Treebank. In Proc. of EMNLP’09, pages 343–351.

D. M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. Proc. of ACL’95, pages 276–283.

W. C. Mann and S. A. Thompson. 1988. Rhetorical
Structure Theory: Toward a functional theory of text
organization. Text, 8(3):243–281.

D. Marcu and A. Echihabi. 2002. An unsupervised ap-
proach to recognizing discourse relations. In Proc.
of ACL’02, pages 368–375.

N. Okazaki. 2009. Classias: A collection of machine-
learning algorithms for classification.

E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In Proc. of COLING’08 (Posters),
pages 87–90.

E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. In Proc. of ACL’09, pages 683–691.

R. L. Plackett. 1983. Karl Pearson and the chi-squared
test. International Statistical Review, 51(1):59–72.

R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
Penn Discourse Treebank 2.0. In Proc. of LREC’08.

R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. Proc. of NA-ACL’03, 1:149–156.

V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag New York, Inc.

58


