



















































Submodularity for Data Selection in Machine Translation


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 131–141,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Submodularity for Data Selection in Statistical Machine Translation

Katrin Kirchhoff
Department of Electrical Engineering

University of Washington
Seattle, WA, USA

kk2@u.washington.edu

Jeff Bilmes
Department of Electrical Engineering

University of Washington
Seattle, WA, USA

bilmes@u.washington.edu

Abstract

We introduce submodular optimization
to the problem of training data subset
selection for statistical machine translation
(SMT). By explicitly formulating data
selection as a submodular program, we ob-
tain fast scalable selection algorithms with
mathematical performance guarantees, re-
sulting in a unified framework that clarifies
existing approaches and also makes both
new and many previous approaches easily
accessible. We present a new class of
submodular functions designed specifically
for SMT and evaluate them on two differ-
ent translation tasks. Our results show that
our best submodular method significantly
outperforms several baseline methods,
including the widely-used cross-entropy
based data selection method. In addition,
our approach easily scales to large data sets
and is applicable to other data selection
problems in natural language processing.

1 Introduction

SMT has made significant progress over the last
decade, not least due to the availability of increas-
ingly larger data sets. Large-scale SMT systems
are now routinely trained on millions of sentences
of parallel data, and billions of words of mono-
lingual data for language modeling. Large data
sets are often beneficial, but they do create certain
other problems. First, they place higher demands
on computational resources (storage and compute).
Hence, existing software infrastructure may need
to be adapted and optimized to handle such large
data sets. Second, experimental turn-around time
is increased as well, making it more difficult to
quickly train, fine-tune, and evaluate novel model-
ing approaches. Most importantly, however, SMT
performance does not increase linearly with the
training data size but levels off after a certain point.
This is because the additional training data may be

noisy, irrelevant to the task at hand, or inherently
redundant. Thus, a linear increase in the amount of
training data typically leads to a sublinear increase
in performance, an effect known as diminishing
returns. Several recent papers (Bloodgood and
Callison-Burch, 2010; Turchi et al., 2012a; Turchi
et al., 2012b) have amply demonstrated this effect.

A way to counteract this is to perform data sub-
set selection, i.e., choose a subset of the available
training data to optimize a particular quality cri-
terion. One scheme is to select a subset that ex-
presses as much of the information in the original
data set as possible - i.e., the data set should be
“summarized” by excluding redundant information.
Another scheme, popular in the context of SMT, is
to subselect the original training set to match the
properties of a particular test set.

In this paper, we introduce submodularity for
subselecting SMT training data, a methodology
that follows both of the above schemes.1 Sub-
modular functions (Fujishige, 2005) are a class
of discrete set functions having the property of di-
minishing returns. They occur naturally in a wide
range of problems in a diverse set of fields includ-
ing economics, game theory, operations research,
circuit theory, and more recently machine learn-
ing. Submodular functions share certain properties
with convexity (e.g., naturalness and mathematical
tractability) although submodularity is still quite
distinct from convexity.

We present a novel class of submodular func-
tions particularly suited for SMT subselection and
evaluate it against state-of-the-art baseline meth-
ods on two different translation tasks, showing that
our method outperforms them significantly in most
cases. While many approaches to SMT data se-
lection have been developed previously (a detailed
overview is provided in Section 3), many of them
are heuristic and do not offer performance guaran-
tees. Certain previous approaches, however, have

1As far as we know, submodularity has not before been
explicitly utilized for SMT subset selection.

131



inadvertently made use of submodular methods.
This, in addition to our own positive results, pro-
vides strong evidence that submodularity is a natu-
ral and practical framework for data subset selec-
tion in SMT and related fields.

An additional advantage of this framework is
that many submodular programs (e.g., the greedy
procedure reviewed in Section 2) are fast and
scalable to large data sets. By contrast, trying
to solve a submodular problem using, say, an
integer-linear programming (ILP) procedure,
would lead to impenetrable scalability problems.

Initial value f(X) = 2 colors in urn.
Updated value f(X∪{v}) = 3 with 
added blue ball.

Initial value f(Y) = 3 colors in urn.
Updated value f(Y∪{v}) = 3 with 
added blue ball.

X Y

v v

Figure 1: f (Y ) measures the number of distinct col-
ors in the set of balls Y , and hence is submodular.

This paper makes several contributions: First, we
present a brief overview of submodular functions
(Section 2) and their potential application to natural
language processing (NLP). Next we review pre-
vious approaches to MT data selection (Section 3)
and analyze them with respect to their submodular
properties. We find that some previous approaches
are submodular in nature although this connection
was not heretofore made explicit. Section 4 details
our new approach. We discuss desirable properties
of an SMT data selection objective and present a
new class of submodular functions tailored towards
this problem. Section 5 presents the data and
systems used for the experiments, and results are
reported in Section 6. Section 7 then concludes.

2 Submodular Functions/Optimization

Submodular functions (Edmonds, 1970; Fujishige,
2005), are widely used in mathematics, economics,
circuit theory (Narayanan, 1997), and operations
research. More recently, they have attracted much
interest in machine learning (e.g., (Narasimhan
and Bilmes, 2004; Kolmogorov and Zabih, 2004;
Krause et al., 2008; Krause and Guestrin, 2011;
Jegelka and Bilmes, 2011; Iyer and Bilmes, 2013)),
where they have been applied to a variety of prob-
lems. In natural language and speech processing,
they have been applied to document summariza-
tion (Lin and Bilmes, 2011; Lin and Bilmes, 2012)
and speech data selection (Wei et al., 2013).

We are given a finite size-n set of objects V (i.e.,
|V |= n). A valuation function f : 2V → R+ is de-

fined that returns a non-negative real value for any
subset X ⊆V . The function f is said to be submodu-
lar if it satisfies the property of diminishing returns:
namely, for all X ⊆ Y and v /∈ Y , we must have:

f (X ∪{v})− f (X)≥ f (Y ∪{v})− f (Y ). (1)
This means that the incremental value (or gain) of
element v decreases when the context in which v
is considered grows from X to Y ⊇ X . We define
the “gain” as f (v|X) , f (X ∪{v})− f (X). Hence,
f is submodular if f (v|X)≥ f (v|Y ). We note that
a function m : 2V → R+ is said to be modular
if it satisfies the above with equality, meaning
m(v|X) = m(v|Y ) for all X ⊆ Y ⊆ V \ {v}. If m
is modular and m( /0) = 0, it can be written as
m(X) = ∑x∈X m(x) and, moreover, is seen simply
as a n-dimensional vector m ∈ RV .

As an example, suppose we have a set V of balls
and f (X) counts the number of colors present
in any subset X ⊆ V . In Figure 1, |X | = 5 and
f (X) = 2, |Y | = 7 and f (Y ) = 3, and X ⊂ Y .
Adding v (a blue ball) to X has a unity gain
f (v|X) = 1 but since a blue ball exists in Y , we
have f (v|Y ) = 0 < f (v|X) = 1.

Submodularity is a natural model for data subset
selection in SMT. In this case, each v ∈ V is a
distinct training data sentence and V corresponds
to a training set. An important characteristic of
any good model for this problem is that we wish
to decrease the “value” of a sentence v ∈V based
on how much that sentence has in common with
those sentences, say X , that have already been
chosen. The value f (v|X) of a given sentence
v in a context of previously chosen sentences
X ⊆ V further diminishes as the context grows
Y ⊇ X . When, for example, a sentence’s value is
represented as the value of its set of features (e.g.,
n-grams), it is natural for those features’ values to
be discounted based on how much representation
of those features already exists in a previously
chosen subset. This corresponds to submodularity,
which can easily be expressed mathematically by
functions such as Eqn. (4) below.

Not only are submodular functions natural for
SMT subset selection, they can also be optimized
efficiently and scalably such that the result has
mathematical performance guarantees. In the re-
mainder of this paper we will assume that f is not
only submodular, but also non-negative ( f (X)≥ 0
for all X), and monotone non-decreasing ( f (X)≤
f (Y ) for all X ⊆ Y ). Such functions are trivial to
uselessly maximize, since f (V ) is the largest possi-
ble valuation. Typically, however, we wish to have

132



Algorithm 1: The Greedy Algorithm
1 Input: Submodular function f : 2V → R+,

cost vector m, budget b, finite set V .
2 Output: Xk where k is the number of

iterations.
3 Set X0← /0 ; i← 0 ;
4 while m(Xi) < b do
5 Choose vi as follows:

vi ∈
{

argmaxv∈V\Xi
f ({v}|Xi)

m(v)

}
;

6 Xi+1← Xi∪{vi} ; i← i+1 ;

a valuable subset of bounded and small cost, where
cost is measured based on a modular function m(X).
For example, the cost m(v) of a sentence v ∈ V
might be its length, so m(X) = ∑x∈X m(x) is a sum
of sentence lengths. This leads to the following
optimization problem:

X∗ ∈ argmax
X⊆V,m(X)≤b

f (X), (2)

where b is a known budget. Solving this problem
exactly is NP-complete (Feige, 1998), and express-
ing it as an ILP procedure renders it impractical for
large data sizes. When f is submodular the cost is
just size (m(X) = |X |), then the simple greedy algo-
rithm (detailed below) will have a worst-case guar-
antee of f (X̃∗) ≥ (1− 1/e) f (Xopt) ≈ 0.63 f (Xopt)
where Xopt is the optimal and X̃∗ is the greedy so-
lution (Nemhauser et al., 1978).

This constant factor guarantee has practical im-
portance. First, a constant factor guarantee stays
the same as n grows, so the relative worst-case qual-
ity of the solution is the same for small and for big
problem instances. Second, the worst-case result
is achieved only by very contrived and unrealistic
function instances — the typical case is almost al-
ways much better. Third, the worst-case guarantee
improves depending on the “curvature” κ ∈ [0,1]
of the submodular function (Conforti and Cornue-
jols, 1984). When the submodular function is not
fully curved (κ < 1, something true of the func-
tions used in this paper), the worst case guarantee
is better, namely 1κ (1−e−κ) (e.g., a function f with
κ = 0.2 has a worst-case guarantee of 0.91). Lastly,
when the cost m is not just cardinality but an arbi-
trary non-negative modular function, a greedy al-
gorithm has similar guarantees (Sviridenko, 2004),
and a scalable variant has a worst-case guarantee
of 1−1/√e (Lin and Bilmes, 2010).

The basic greedy algorithm has a very simple
form. Starting with X ← /0, we repeat the operation

X ← X ∪ argmaxv∈V\X f (v|X)m(v) until the budget is
exceeded (m(X) > b) and then backoff to the
previous iteration (complete details are given in
Algorithm 1). While the algorithm has complexity
O(n2), there is an accelerated instance of this
algorithm (Minoux, 1978; Leskovec et al., 2007)
that has empirical computational complexity of
O(n logn) where n = |V |. The greedy algorithm,
therefore, scales practically to very large n.
Recently, still much faster (Wei et al., 2014) and
also parallel distributed (Mirzasoleiman et al.,
2013) greedy procedures have been advanced
offering still better scalability.

There are many submodular functions that
are appropriate for subset selection (Lin and
Bilmes, 2011; Lin and Bilmes, 2012). Some
of them are graph-based, where we are given a
non-negative weighted graph G = (V,E,w) and
w : E→R+ is a set of edge weights (i.e., w(x,y) is
a non-negative similarity score between sentences
x and y). A submodular function is obtained via
a graph cut function f (X) = ∑x∈X ,y∈V\X w(x,y)
or via a monotone truncated graph cut
function f (X) = ∑v∈V min(Cv(X),αCv(V ))
where α ∈ (0,1) is a scalar parameter and
Cv(X) = ∑x∈X w(v,x) is a v-specific modular
function. Alternatively, the class of facility loca-
tion functions f (X) = ∑v∈V maxx∈X w(x,v) have
been widely and successfully used in the field of
operations research, and are also applicable here.

In the worse case, the required graph construc-
tion has a worst-case complexity of O(n2). While
sparse graphs can be used, this can be prohibitive
when n = |V | gets large. Another class of sub-
modular functions that does not have this prob-
lem is based on a weighted bipartite graph G =
(V,U,E,w) where V are the left vertices, U are the
right vertices, E ⊆ V ×U is a set of edges, and
w : U→R+ is a set of non-negative weights on the
vertices U . For X ⊆V , the bipartite neighborhood
function is defined as:

f (X) = w({u ∈U : ∃x ∈ X with (x,u) ∈ E}) (3)
This function is interesting for NLP applications

since U can be seen as a set of “features” of the ele-
ments v∈V (i.e., if V is a set of sentences, U can be
the collective set of n-grams for multiple values of
n, and f (X) is the weight of the n-grams contained
collectively in sentences X).2 Given a set X ⊆ V ,

2To be consistent with standard notation in previous liter-
ature, we overload the use of n in “n-grams” and the size of
our set “n = |V |”, even though the two ns have no relationship
with each other.

133



we get value from the features of the elements
x ∈ X , but we get credit for each feature only one
time — once a given object x ∈ X has a given fea-
ture u∈U , any additions to X by elements also hav-
ing feature u offer no further credit via that feature.

Another interesting class of submodular func-
tions, allowing additional credit from an element
even when its features already exist in X , are what
we call feature-based submodular functions. They
involve sums of non-decreasing concave functions
applied to modular functions (Stobbe and Krause,
2010) and take the following form:

f (X) = ∑
u∈U

wuφu(mu(X)) (4)

where wu > 0 is a feature weight, mu(X) =
∑x∈X mu(x) is a non-negative modular function
specific to feature u, mu(x) is a relevance score (a
non-negative scalar score indicating the relevance
of feature u in object x), and φu is a u-specific
non-negative non-decreasing concave function.
The gain is f (v|X) = ∑u∈U

(
φ(mu(X ∪ {v}))−

φ(mu(X))
)

, and thanks to φu’s concavity, the
term φ(mu(X ∪{v}))−φ(mu(X)) for each feature
u ∈U is decaying as X grows. The rate of decay,
and hence the degree of diminishing returns and
ultimately the measure of redundancy of the
information provided by the feature, is controlled
by the concave function. The rate of decay is
also related to the curvature κ of the submodular
function (c.f. §2), with more aggressive decay
having higher curvature (and a worse worst-case
guarantee). The decay is a modeling choice that
should be decided based on a given application.

Feature-based functions have the advantage that
they do not require the construction of a pairwise
graph; they have a cost of only O(n|U |), which is
linear in the data size and therefore scalable to
large data set sizes.

We utilize this class for our subset selection ex-
periments described in Section 4, where we use one
global concave function φu = φ for all u ∈U . In
this work we chose one particular set of features U .
However, given the large body of research into NLP
feature engineering (Jurafsky and Martin, 2009),
this class is extensible beyond just this set, which
makes it suitable for many other NLP applications.

Before describing our SMT-specific functions in
detail, we review previous work on subset selection
for SMT in the context of submodularity.

3 Previous Approaches

There have been many previous approaches to data
subset selection in SMT. In this section, we show
that some of them in fact correspond to submodular
methods, thus introducing a connection between
submodularity and the practical problem of SMT
data selection. The fact that submodularity is
implicitly and unintentionally used in previous
work suggests that it is natural for this problem.

A currently widely-used data selection method in
SMT (which we also use as a baseline in Section 6)
uses the cross-entropy between two language mod-
els (Moore and Lewis, 2010), one trained on the
test set of interest, and another trained on a large set
of generic or out-of-domain training data. We call
this the cross-entropy method. This method trains
a test-set specific (or in-domain) language model,
LMin, and a generic (out-of- or mixed-domain) lan-
guage model, LMout. Each sentence x ∈ V in the
training data is given a probability score with both
language models and then ranked in descending
order based on the log ratio

mce(x) =
1

`(x)
log[Pr(x|LMin)/Pr(x|LMout)] (5)

where `(x) is the length of sentence x. Finally, the
top N sentences are chosen. In (Axelrod et al.,
2011) this method is extended to take both sides
of the parallel corpus into account rather than just
the source side. The cross-entropy approach values
each sentence individually, without regard to any in-
teraction with already selected sentences. This ap-
proach, therefore, is modular (a special case of sub-
modular) and values a set X via m(X) = ∑x∈X m(x).
Moreover, the thresholding method for choosing
a subset corresponds exactly to the optimization
problem in Eqn. (2) where f ← m and the budget
b is set to the sum of the top N sentence scores.
Thanks to modularity, the problem is no longer NP-
complete, and the threshold method solves Eqn. (2)
exactly. On the other hand, a modular function
does not have the diminishing returns property, and
thus has no chance to represent interaction or re-
dundancy between sentences. The chosen subset,
therefore, might have an enormous overrepresenta-
tion of one aspect of the training data while having
minimal or no representation of another aspect, a
major vulnerability of this approach.

Other methods use information retrieval (Hilde-
brand et al., 2005; Lü et al., 2007) which can also
be described as modular function optimization
(e.g., take the top k scoring sentences). Duplicate

134



sentence removal is easily represented by a feature-
based submodular function, Equation (4), where
there is one sentence-specific feature per sentence
and where φu(mu(X)) = min(|X ∩{u}|,1) — once
a sentence is chosen, its contribution is saturated
so any duplicate sentence has a gain of zero. Also,
the unseen n-gram function of (Eck et al., 2005;
Bloodgood and Callison-Burch, 2010) corresponds
to a bipartite neighborhood submodular function,
with a weight function defined based on n-gram
counts. Moreover their functions are optimized
using the greedy algorithm; hence they in fact
have a 1− 1/e guarantee. Other methods have
noted and dealt with the existence of redundancy
in phrase-based systems (Ittycheriah and Roukos,
2007) by limiting the set of phrases — submodular
optimization inherently removes redundancy. Also,
(Callison-Burch et al., 2005; Lopez, 2007) involve
modular functions but where selection is over
subsets of phrases (rather than sentences as in our
current work) and where multiple selections occur,
each specific to an individual test set sentence
rather than the entire test set.

In the feature-decay method, presented in (Biçici,
2011; Biçici and Yuret, 2011; Biçici, 2013), the
value of a sentence is based on its decomposition
into a set of feature values. As sentences are added
to a set, the feature decay approach in general di-
minishes the value of each feature depending on
how much of that feature has already been covered
by those sentences previously chosen — the pa-
pers define a set of feature decay functions for this
purpose.

Our analysis of (Biçici, 2011; Biçici and Yuret,
2011; Biçici, 2013), from the perspective of sub-
modularity, has revealed an interesting connection.
The feature decay functions used in these papers
turn out to be derivatives of non-decreasing con-
cave functions. For example, in one case φ ′(a) =
1/(1 + a) which is the derivative of the concave
function φ(a) = ln(1+a). We are given a constant
initialization wu for feature u ∈U — in the papers,
they set either wu← 1, or wu← log(m(V )/mu(V )),
or wu ← log(m(V )/(1 + mu(V ))), where m(V ) =
∑u mu(V ), and where mu(X) = ∑x∈X mu(x) is the
count of feature u within the set of sentences
X ⊆V . This yields the submodular feature function
fu(X) = wuφ(mu(X)). The value of sentence v as
measured by feature u in the context of X is the gain
fu(v|X), which is a discrete derivative correspond-
ing to wu/(1+mu(X ∪{v})). An alternative decay
function they define is given as φ ′(a) = 1/(1+ba)
for a base b (they set b← 2) which is the derivative

of the following non-decreasing concave function:

φ(a) =
[
1− 1

ln(b)
ln
(

1+ exp
(−a ln(b)))] (6)

We note that this function is saturating, meaning
that it quickly reaches its asymptote at its maxi-
mum possible value. We can, once again, define
a function specific for feature u ∈U as fu(X) =
wuφ(mu(X)) with a gain fu(v|X) being a discrete
derivative corresponding to wu/(1+bmu(X∪{v})).

The connection between this work and submod-
ularity is not complete, however, without consider-
ing the method used for optimization. In fact, Algo-
rithm 1 of (Biçici and Yuret, 2011) is precisely the
accelerated greedy algorithm of (Minoux, 1978)
applied to the submodular function corresponding
to f (X) = ∑u∈U fu(X), and Algorithm 1 of (Biçici,
2013) is the cost-normalized variant of this greedy
algorithm corresponding to a knapsack constraint
(Sviridenko, 2004). Thus, our analysis shows that
these methods also have a 1− 1/e performance
guarantee and also the O(n logn) empirical com-
plexity mentioned in Section 2. This is an impor-
tant connection, as it furthers the evidence that
submodularity is natural for the problem of SMT
subset selection. This also increases the accessibil-
ity of this method since we may view it as a special
case of Equation (4).

Another class of approaches focuses on active
learning. In (Haffari et al., 2009) a large corpus
of noisy parallel data is created automatically; a
smaller set of samples is then selected from this
set that receive human translations. A combination
of several “informativeness” scores is computed
on a sentence-level basis, and samples are selected
via hierarchical adaptive sampling (Dasgupta and
Hsu, 2008). In (Mandal et al., 2008) a measure
of disagreement between different MT systems, as
well as an entropy-based criterion are used to select
additional data for annotation. In (Bloodgood and
Callison-Burch, 2010) and (Ambati et al., 2010),
active learning is combined with crowd-sourced an-
notations to produce large, human-translated data
sets that are as informative as possible. In (Cao
and Khudanpur, 2012), samples are selected for
discriminative training of an MT system accord-
ing to a greedy algorithm that tries to maximize
overall quality. These methods address a differ-
ent scenario (data selection for annotation or dis-
criminative training) than the one considered here;
however, we also note that the actual selection tech-
niques employed in these papers do not appear to
be submodular.

135



4 Novel Submodular Functions for SMT

In this section, we design a parameterized class
of submodular functions useful for SMT training
data subset selection. By staying within the realm
of submodularity, we retain the advantages of the
greedy algorithm, its theoretical performance as-
surances, and its scalability properties. At the same
time this opens the door to a general framework for
quickly exploring a much larger class of functions
(with the same desirable properties) than before.

It is important to note that we are using sub-
modularity as a “model” of the selection process,
and the submodular objective acts as a surrogate
for the actual SMT objective function. Thus, the
mathematical guarantee we have is in terms of the
surrogate objective rather than the true SMT ob-
jective. Evaluating one point of the actual SMT
objective would require the complete training and
testing of an SMT system, so even an algorithm as
efficient as Algorithm 1 would be infeasible, even
on small data. It is therefore important to design a
natural and scalable surrogate objective.

We do not consider the graph-based functions
discussed in Section 2 here since they require a
pairwise similarity matrix over all training sen-
tences and thus have O(n2) worst-case complexity.
For large tasks with millions or even billions of
sentences, this eventually becomes impractical.
Instead we focus on feature-based functions of the
type presented in Eqn. (4), where each sentence
is represented as a set of features rather than as a
vertex in a graph. In this function there are four
components to specify: 1) U , the linguistic feature
set; 2) mu(x), the relevance scores for each feature
u and sentence x; 3) wu, the feature weights; and
4) φ , the concave function (we use one concave
function, so φu = φ for all u ∈U).
Feature set: U is the set of n-grams from either
the source language U src, or from both the source
and target language U src ∪U tgt (see Section 6);
since we are interested in selecting a training set
that matches a given test set, we use the set of n-
grams that occur both in the training set and in
the development/test data (for target features, only
development set features are used). I.e., U src =
(U srcdev∪U srctest)∩U srctrain and U tgt = U tgtdev∩U tgttrain.
Relevance scores: A feature u within a sentence
x should be valued based on how salient that fea-
ture is within the “document” in which it occurs;
here, the “document” is the set of training sen-
tences. This is a task well suited to TFIDF. As
an alternative to raw feature counts we thus also

consider scores of the form mu(x)← tfidf(u,x) =
tf(u,x)× idftrn(u), where tf(u,x) and idftrn(u) are
defined as usual.
Feature weights: We wish to select those training
samples that contain features occurring frequently
in the test data while avoiding the over-selection
of features that are very frequent in the training
data because those are likely to be translated
correctly anyway. This is similar to the approach
in (Moore and Lewis, 2010) (see Equation (5)),
where a log-probability ratio of in-domain to
out-of-domain language model is utilized. In the
present case, we need a value that is specific to
feature u ∈U ; a natural approach is to use the ratio
of counts ctst(u)/ctrn(u) where ctst(u) is the raw
count of u in the development/test data, and ctrn(u)
is its raw count in the training data (note that
ctrn(u) is never zero due to the way U is defined).
As an additional factor we allow feature length
to have an influence. In general, longer n-grams
might be considered more valuable since they
typically lead to better translations and are more
relevant for BLEU. Thus, we include a reward
term for longer n-grams in the form of β |u| where
β ≥ 1 and |u| is the length of feature u. This gives
greater weight to longer n-grams when β > 1.
Concave function: It is imperative to find the right
form of concave function since, as described in Sec-
tion 2, the concave shape determines the degree to
which redundancy and diminishing returns are rep-
resented. Intuitively, when the shape of the concave
function for a feature becomes “flat” rapidly, that
feature quickly looses its ability to provide addi-
tional value to a candidate subset. Many different
concave functions were tested for φ , including one
of the two functions implicit in (Biçici and Yuret,
2011) and derived in Section 3, and a variety of
roots of the form φ(a) = aα for 0 < α < 1. In
Table 2, for example, we find evidence that the
simple square root φ(a) =

√
a performs slightly

better than the log function. The square root is
much less curved and decays much more gradually
than either of the two functions implicit in (Biçici
and Yuret, 2011), of which one is a log form and
the other is even more curved and quickly satu-
rates (see §3). The square root function yields a
less curved submodular function, in the sense of
(Conforti and Cornuejols, 1984), resulting in better
worst-case guarantees. Indeed, Table 1 in (Biçici
and Yuret, 2011) corroborates by showing that the
more curved saturating function does worse than
the less curved log function.
Four Components Together: Different instantia-

136



tions of the four components discussed above will
result in different submodular functions of the gen-
eral class defined in Eqn. (4). Particular settings
of these general parameters produce the methods
considered in (Biçici and Yuret, 2011), thus mak-
ing that approach easily accessible once the general
submodular framework is set up. As a very special
case, this is also true of the cross-entropy method
(Moore and Lewis, 2010), where |U |= 1, mu(x)←
exp(mce(x)) of Equation (5) 3, wu← 1, and φ(a) =
a is the identity function. In Section 6, we specify
the parameter settings used in our experiments.

Task Train Dev Test LM
NIST 189M 48k 49k 2.5B
Europarl 52.8M 57.7k 58.1k 53M

Table 1: Data set sizes (number of source-side
words) for MT tasks. LM = language model data.

5 Data and Systems

We evaluate our approach on the NIST Arabic-
English translation task, using the NIST 2006 set
for development and the NIST 2009 set for eval-
uation. The training data consists of all Modern
Standard Arabic-English parallel LDC corpora per-
mitted in the NIST evaluations (minus the restricted
time periods). Together these sets form a mixed-
domain training set containing relevant in-domain
data similar to the NIST data sets but also less rele-
vant data (e.g., the UN parallel corpora); we thus
expect data selection to work well on this task. Ad-
ditional English language modeling data was drawn
from several other LDC corpora (English Giga-
word, AQUAINT, HARD, ANC/DCI and the Amer-
ican National Corpus). Preprocessing included con-
version of the Arabic data to Buckwalter format,
tokenization, spelling normalization, and morpho-
logical segmentation using MADA (Habash et al.,
2009). Numbers and URLs were replaced with
variables. The English data was tokenized and
lowercased. Postprocessing involved recasing the
translation output, replacing variable names with
their original corresponding tokens, and normal-
izing spelling and stray punctuation marks. The
recasing model is an SMT system without reorder-
ing, trained on parallel cased and lowercased ver-
sions of the training data. The recasing model re-
mains fixed for all experiments and is not retrained

3Due to modularity, any monotone increasing transforma-
tion from mce(x) to mu(x) that ensures mu(x)≥ 0 is equivalent.

for different sizes of the training data. Evalua-
tion follows the NIST guidelines and was done by
computing BLEU scores using the official NIST
evaluation tool mteval-v13a.pl with the −c flag
for case-sensitive scoring. In addition to the NIST
task we also applied our method to the Europarl
German-English translation task. The training data
comes from the Europarl-v7 collection4; the devel-
opment set is the 2006 dev set, and the test set is the
2007 test set. The number of reference translations
is 1. The German data was preprocessed by tok-
enization, lower-casing, splitting noun compounds
and lemmatization to address morphological vari-
ation in German. The English side was tokenized
and lowercased. Evaluation was done by comput-
ing BLEU on the lowercased versions of the data.
Since test and training data for this task come from
largely the same domain we expect the training
data to be less redundant or irrelevant; nevertheless
it will be interesting to see how much different data
selection methods can contribute even to in-domain
translation tasks. The sizes of the various data sets
are shown in Table 1.

All translation systems were trained using the
GIZA++/Moses infrastructure (Koehn et al., 2007).
The translation model is a standard phrase-based
model with a maximum phrase length of 7. Since a
large number of experiments had to be run for this
study, more complex hierarchical or syntax-based
translation models were deliberately excluded in
order to limit the experimental turn-around time
needed for each experiment. The reordering model
is a hierarchical model according to (Galley and
Manning, 2008). The feature weights in the log-
linear function were optimized on the development
set BLEU score using minimum error-rate training.
The language models for the NIST task (5-grams)
were trained on three different data sources (Gi-
gaword, GALE data, and all remaining corpora),
which were then interpolated into a single model.
The interpolation weights were optimized sepa-
rately for the two different genres present in the
NIST task (newswire and web text). All models
used Witten-Bell discounting and interpolation of
higher-order and lower-order models. Language
models remain fixed for all experiments, i.e., the
language model training data is not subselected
since we were interested in the effect of data subset
selection on the translation model only. The lan-
guage model for the Europarl system was a 5-gram
trained on Europarl data only.

4http://http://www.statmt.org/europarl/

137



Method Data Subset Sizes
10% 20% 30% 40%

Rand 0.3991 (± 0.004) 0.4142 (± 0.003) 0.4205 (± 0.002) 0.4220 (± 0.002)
Xent 0.4235 (± 0.004) 0.4292 (± 0.002) 0.4290 (± 0.003) 0.4292 (± 0.001)
SM-1 0.4309 (± 0.000) 0.4367 (± 0.001) 0.4330 (± 0.004) 0.4351 (± 0.002)
SM-2 0.4330∗ (± 0.001) 0.4395∗ (± 0.003) 0.4333 (± 0.001) 0.4366∗ (± 0.003)
SM-3 0.4313∗ (± 0.002) 0.4338 (± 0.002) 0.4361∗ (± 0.002) 0.4351 (± 0.003)
SM-4 0.4276 (± 0.003) 0.4303 (± 0.002) 0.4324 (± 0.002) 0.4329 (± 0.000)
SM-5 0.4285 (± 0.004) 0.4356 (± 0.002) 0.4333 (± 0.003) 0.4324 (± 0.002)
SM-6 0.4302∗ (± 0.004) 0.4334 (± 0.003) 0.4371∗ (± 0.002) 0.4349 (± 0.003)
SM-7 0.4295 (± 0.002) 0.4374 (± 0.002) 0.4344 (± 0.001) 0.4314 (± 0.0004)
SM-8 0.4304∗ (± 0.002) 0.4323 (± 0.000) 0.4358 (± 0.003) 0.4337 (± 0.001)
100% 0.4257

Table 2: BLEU scores (standard deviations) on the NIST 2009 (Ara-En) test set for random (Rand),
cross-entropy (Xent), and submodular (SM) data selection methods defined in Table 4. 100% = system
using all of the training data. Boldface numbers indicate a statistically significant improvement (p≤ 0.05)
over the median Xent system. Starred scores are also significantly better than SM-5.

Method Data Subset Sizes
10% 20% 30% 40%

Rand 0.2590 (± 0.003) 0.2652 (± 0.001) 0.2677 (± 0.002) 0.2697 (± 0.001)
Xent 0.2639 (± 0.002) 0.2687 (± 0.002) 0.2704 (± 0.001) 0.2723 (± 0.001)
SM-5 0.2653 (± 0.001) 0.2727 (± 0.000) 0.2697 (± 0.002) 0.2720 (± 0.002)
SM-6 0.2697∗ (± 0.001) 0.2700 (± 0.002) 0.2740∗ (± 0.002) 0.2723 (± 0.000)
100% 0.2651

Table 3: BLEU scores (standard deviation) on the Europarl translation task for random (Rand), cross-
entropy (Xent), and submodular (SM) data selection methods. 100% = system using all of the training
data. Boldface numbers indicate a statistically significant improvement (p≤ 0.05) over the median Xent
system. Starred scores are significantly better than SM-5.

6 Experiments

Function parameters
w(u) φ(a) mu(x) U

SM-1 c
tst(u)

ctrn(u) β
|u| √a tfidf(u,x) U src

SM-2
√

ctst(u)
ctrn(u) β

|u| √a tfidf(u,x) U src∪U tgt
SM-3 c

tst(u)
ctrn(u) β

|u| √a c(u,x) U src
SM-4 ctst(u)

√
a tfidf(u,x) U src

SM-5 1 ln(1+a) c(u,x) U src

SM-6
√

ctst(u)
ctrn(u)

√
a tfidf(u,x) U src

SM-7 c
tst(u)

ctrn(u)

√
(a) tfidf(u,x) U src∪U tgt

SM-8 c
tst(u)

ctrn(u) ln(1+a) tfidf(u,x) U
src∪U tgt

Table 4: Different instantiations of the general sub-
modular function in Eq. 4 (β = 1.5 in all cases).

We first trained a baseline system on 100% of
the training data. Different data selection methods
were then used to select subsets of 10%, 20%, 30%

and 40% of the data. While not reported in the
tables, above 40%, the performance slowly drops
to the 100% performance.

The first baseline selection method utilizes ran-
dom data selection, for which 3 different data sets
of the specified size were drawn randomly from
the training data. Individual systems were trained
on all random subsets of the same size, and their
scores were averaged. The second baseline is
the cross-entropy method by (Moore and Lewis,
2010). In-domain language models were trained on
the combined development and test data, and out-
of-domain models were trained on an equivalent
amount of data drawn randomly from the training
set. Sentences were ranked by the function in Eq. 5,
and the top k percent were chosen. The order of the
n-gram models was optimized on the development
set and was found to be 3. Larger model orders
resulted in worse performance, possibly due to the

138



limited size of the data used for their training. Since
this method also involves random data selection,
we report the average BLEU score over 5 different
trials. For the submodular selection method, Ta-
ble 4 shows the different values that were tested for
the four components listed in Section 4. The combi-
nation was optimized on the development set. The
selection algorithm (Alg. 1) runs within a few min-
utes on our complete training set of 189M words.

Results on the NIST 2009 test set are shown in
Table 2. The scores for the submodular systems
are averages over 3 different runs of MERT tuning.
Random data subset selection (Row 1) falls short
of the baseline system using 100% of the training
data. The cross-entropy method (Row 2) surpasses
the performance of the baseline system at about
20% of the data, demonstrating that data subset
selection is a suitable technique for such mixed-
domain translation tasks. The following rows show
results for the various submodular functions shown
in Table 4. Out of these, SM-5 corresponds to the
best approach in (Biçici and Yuret, 2011). SM-6
is our own best-performing function, beating the
cross-entropy method by a statistically significant
margin (p≤ 0.05) under all conditions.5 SM-6 is
also significantly better than SM-5 in two cases.
Finally, it surpasses the performance of the all-data
system at only 10% of the training data; possibly,
even smaller training data sets could be used
but this option was not investigated. While the
bilingual submodular functions SM-2 and SM-7)
yield an improvement of up to 0.015 BLEU points
on the dev set (not shown in the table), they do not
consistently outperform the monolingual functions
on the test set. Since test set target features cannot
be used in our scenario, bilingual features are
only helpful to the extent that the development set
closely matches the test set. However, target fea-
tures should be quite helpful when selecting data
from an out-of-domain set to match an in-domain
training set (as in e.g. (Axelrod et al., 2011)). We
found no gain from the length reward β |u|.

The Europarl results (Table 3) show a similar
pattern. Although the differences in BLEU scores
are smaller overall (as expected on an in-domain
translation task), data subset selection improves
over the all-data baseline system in this case as
well. The cross-entropy method again outperforms
random data selection. On this task we only tested
our submodular function that worked best on the

5Statistical significance was measured using the paired
bootstrap resampling test of (Koehn, 2004), applied to the
systems with the median BLEU scores.

NIST task; again we find that it outperforms the
cross-entropy method. In two conditions (10% and
30%) these differences are statistically significant.
10% of the training data suffices to outperform the
all-data system, and up to a full BLEU point can be
gained on this task using 20-30% of the data and a
submodular data selection method.

7 Conclusions

We have introduced submodularity to SMT data
subset selection, generalizing previous approaches
to this problem. Our method has theoretical perfor-
mance guarantees, comes with scalable algorithms,
and significantly improves over current, widely-
used data selection methods on two different trans-
lation tasks. There are many possible extensions
to this work. One strategy would be to extend the
feature set U with features representing different
types of linguistic information - e.g., when using
a syntax-based system it might be advantageous
to select training data that covers the set of syn-
tactic structures seen in the test data. Secondly,
the selected data was test data specific. In some
contexts, it is not possible to train test data spe-
cific systems dynamically; in that case, different
submodular functions could be designed to select
a representative “summary” of the training data.
Finally, the use of submodular functions for subset
selection is applicable to other data sets that can
be represented as features or as a pairwise similar-
ity graph. Submodularity thus can be applied to a
wide range of problems in NLP beyond machine
translation.

Acknowledgments
This material is based on research sponsored by
Intelligence Advanced Research Projects Activity
(IARPA) under agreement number FA8650-12-2-
7263, and is also supported by the National Science
Foundation under Grant No. (IIS-1162606), and by
a Google, a Microsoft, and an Intel research award.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies or
endorsements, either expressed or implied, of In-
telligence Advanced Research Projects Activity
(IARPA) or the U.S. Government.

139



References
[Ambati et al.2010] V. Ambati, S. Vogel, and J. Car-

bonell. 2010. Active learning and crowd-sourcing
for machine translation. In Proceedings of LREC,
pages 2169–2174, Valletta, Malta.

[Axelrod et al.2011] A. Axelrod, X. He, and J. Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355–
362, Edinburgh, Scotland.

[Biçici and Yuret2011] E. Biçici and D. Yuret. 2011.
Instance selection for machine translation using fea-
ture decay algorithms. In Proceedings of the 6th
Workshop on Statistical Machine Translation, pages
272–283.

[Biçici2013] E. Biçici. 2013. Feature decay algorithms
for fast deployment of accurate statistical machine
translation systems. In Proceedings of the 8th Work-
shop on Statistical Machine Translation, pages 78–
84.

[Biçici2011] E. Biçici. 2011. The Regression Model of
Machine Translation. Ph.D. thesis, KOÇ University.

[Bloodgood and Callison-Burch2010] M. Bloodgood
and C. Callison-Burch. 2010. Bucking the trend:
large-scale cost-focused active learning for statis-
tical machine translation. In Proceedings of ACL,
pages 854–864.

[Callison-Burch et al.2005] Chris Callison-Burch,
Colin Bannard, and Josh Schroeder. 2005. Scaling
phrase-based statistical machine translation to larger
corpora and longer phrases. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 255–262. Association for
Computational Linguistics.

[Cao and Khudanpur2012] Y. Cao and S. Khudanpur.
2012. Sample selection for large-scale MT discrimi-
native training. In Proceedings of AMTA.

[Conforti and Cornuejols1984] M. Conforti and G. Cor-
nuejols. 1984. Submodular set functions, matroids
and the greedy algorithm: tight worst-case bounds
and some generalizations of the Rado-Edmonds the-
orem. Discrete Applied Mathematics, 7(3):251–
274.

[Dasgupta and Hsu2008] S. Dasgupta and D. Hsu.
2008. Hierarchical sampling for active learning. In
Proceedings of ICML.

[Eck et al.2005] M. Eck, S. Vogel, and A. Waibel. 2005.
Low cost portability for statistical machine transla-
tion based on n-gram frequency and tf-idf. In Pro-
ceedings of the 10th Machine Translation Summit X,
pages 227–234.

[Edmonds1970] J. Edmonds, 1970. Combinatorial
Structures and their Applications, chapter Submodu-
lar functions, matroids and certain polyhedra, pages
69–87. Gordon and Breach.

[Feige1998] U. Feige. 1998. A threshold of ln n for ap-
proximating set cover. Journal of the ACM (JACM),
45(4):634–652.

[Fujishige2005] S. Fujishige. 2005. Submodular func-
tions and optimization. Annals of Discrete Mathe-
matics, volume 58. Elsevier Science.

[Galley and Manning2008] M. Galley and C. D. Man-
ning. 2008. A simple and effective hierarchical
phrase reordering model. In Proceedings of EMNLP,
pages 847–855.

[Habash et al.2009] N. Habash, O. Rambow, and
R. Roth. 2009. A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proceed-
ings of the MEDAR conference, pages 102–109.

[Haffari et al.2009] G. Haffari, M. Roy, and A. Sarkar.
2009. Active learning for statistical machine transla-
tion. In Proceedings of HLT, pages 415–423.

[Hildebrand et al.2005] A. Hildebrand, M. Eck, S. Vo-
gel, and A. Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of EAMT,
pages 133–142.

[Ittycheriah and Roukos2007] A. Ittycheriah and
S. Roukos. 2007. Direct translation model 2. In
Proceedings of HLT/NAACL, page 5764.

[Iyer and Bilmes2013] R. Iyer and J. Bilmes. 2013.
Submodular optimization with submodular cover
and submodular knapsack constraints. In Neural In-
formation Processing Society (NIPS), Lake Tahoe,
CA, December.

[Jegelka and Bilmes2011] Stefanie Jegelka and Jeff A.
Bilmes. 2011. Submodularity beyond submodular
energies: coupling edges in graph cuts. In Computer
Vision and Pattern Recognition (CVPR), Colorado
Springs, CO, June.

[Jurafsky and Martin2009] D. Jurafsky and J. H. Mar-
tin. 2009. Speech and Language Processing:
An Introduction to Natural Language Processing,
Speech Recognition, and Computational Linguistics.
Prentice-Hall, 2nd edition.

[Koehn et al.2007] P. Koehn, H. Hoang, A. Birch,
C. Callison-Burch, M. Federico, N. Bertoldi,
B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer,
O. Bojar, A. Constantin, and E. Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL.

[Koehn2004] P. Koehn. 2004. Statistical significance
tests for machine translation evaluation. In Proceed-
ings of EMNLP.

[Kolmogorov and Zabih2004] V. Kolmogorov and
R. Zabih. 2004. What energy functions can
be minimized via graph cuts? IEEE TPAMI,
26(2):147–159.

140



[Krause and Guestrin2011] A. Krause and C. Guestrin.
2011. Submodularity and its applications in opti-
mized information gathering. ACM Transactions on
Intelligent Systems and Technology, 2(4).

[Krause et al.2008] A. Krause, H.B. McMahan,
C. Guestrin, and A. Gupta. 2008. Robust sub-
modular observation selection. Journal of Machine
Learning Research, 9:2761–2801.

[Leskovec et al.2007] J. Leskovec, A. Krause,
C. Guestrin, C. Faloutsos, J. VanBriesen, and
N. Glance. 2007. Cost-effective outbreak detection
in networks. In Proceedings of the 13th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 420–429.

[Lin and Bilmes2010] H. Lin and J. Bilmes. 2010.
Multi-document summarization via budgeted maxi-
mization of submodular functions. In Proceedings
of NAACL-HLT, pages 2761–2801.

[Lin and Bilmes2011] H. Lin and J. Bilmes. 2011. A
class of submodular functions for document summa-
rization. In Proceedings of ACL, pages 510–520.

[Lin and Bilmes2012] H. Lin and J. Bilmes. 2012.
Learning mixtures of submodular shells with appli-
cation to document summarization. In Uncertainty
in Artifical Intelligence (UAI), Catalina Island, USA,
July. AUAI.

[Lopez2007] A. Lopez. 2007. Hierarchical phrase-
based translation with suffix arrays. In EMNLP-
CoNLL, pages 976–985.

[Lü et al.2007] Y. Lü, J. Huang, and Q. Liu. 2007. Im-
proving statistical machine translation performance
by training data selection and optimization. In Pro-
ceedings of EMNLP, pages 343–350.

[Mandal et al.2008] A. Mandal, D. Vergyri, W. Wang,
J. Zheng, A. Stolcke, D. Hakkani-Tür G. Tür, and
N.F. Ayan. 2008. Efficient data selection for ma-
chine translation. In Proceedings of the Spoken Lan-
guage Technology Workshop, pages 261–264.

[Minoux1978] M. Minoux. 1978. Accelerated greedy
algorithms for maximizing submodular functions.
In Lecture Notes in Control and Information Sci-
ences, volume 7, pages 234–243.

[Mirzasoleiman et al.2013] B. Mirzasoleiman, A. Kar-
basi, R. Sarkar, and A. Krause. 2013. Distributed
submodular maximization: Identifying representa-
tive elements in massive data. In Neural Information
Processing Systems (NIPS).

[Moore and Lewis2010] R. Moore and W. Lewis. 2010.
Intelligent selection of language model training data.
In Proceedings of the Association for Computational
Linguistics, pages 220–224.

[Narasimhan and Bilmes2004] M. Narasimhan and
J. Bilmes. 2004. PAC-learning bounded tree-width

graphical models. In Uncertainty in Artificial Intel-
ligence: Proceedings of the Twentieth Conference
(UAI-2004). Morgan Kaufmann Publishers, July.

[Narayanan1997] H. Narayanan. 1997. Submodular
functions and electrical networks. Elsevier.

[Nemhauser et al.1978] G.L. Nemhauser, L.A. Wolsey,
and M.L. Fisher. 1978. An analysis of approxi-
mations for maximizing submodular set functions i.
Mathematical Programming, 14:265–294.

[Stobbe and Krause2010] P. Stobbe and A. Krause.
2010. Efficient minimization of decomposable sub-
modular functions. In NIPS.

[Sviridenko2004] M. Sviridenko. 2004. A note on
maximizing a submodular set function subject to a
knapsack constraint. Operations Research Letters,
32(1):41–43.

[Turchi et al.2012a] M. Turchi, T. de Bie, C. Goutte,
and N. Cristianini. 2012a. Learning to translate: A
statistical and computational analysis. Advances in
Artificial Intellligence, 2012:484580:15 pages.

[Turchi et al.2012b] M. Turchi, C. Goutte, and N. Cris-
tianini. 2012b. Learning machine translation from
in-domain and out-of-domain data. In Proceedings
of EAMT, Trento, Italy.

[Wei et al.2013] K. Wei, Y. Liu, K. Kirchhoff, and
J. Bilmes. 2013. Using document summarization
techniques for speech data subset selection. In Pro-
ceedings of NAACL, pages 721–726, Atlanta, Geor-
gia, June.

[Wei et al.2014] K. Wei, R. Iyer, and Jeff Bilmes. 2014.
Fast multi-stage submodular maximization. In Pro-
ceedings of ICML, Beijing, China.

141


