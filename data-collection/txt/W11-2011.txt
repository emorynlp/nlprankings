










































Optimising Natural Language Generation Decision Making For Situated Dialogue


Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 78–87,
Portland, Oregon, June 17-18, 2011. c©2011 Association for Computational Linguistics

Optimising Natural Language Generation Decision Making
For Situated Dialogue

Nina Dethlefs
Department of Linguistics,

University of Bremen
dethlefs@uni-bremen.de

Heriberto Cuayáhuitl
German Research Centre

for Artificial Intelligence (DFKI)
heriberto.cuayahuitl@dfki.de

Jette Viethen
Centre for Language Technology

acquarie University
jviethen@ics.mq.edu.au

Abstract

Natural language generators are faced with a
multitude of different decisions during their
generation process. We address the joint opti-
misation of navigation strategies and referring
expressions in a situated setting with respect to
task success and human-likeness. To this end,
we present a novel, comprehensive framework
that combines supervised learning, Hierarchi-
cal Reinforcement Learning and a hierarchical
Information State. A human evaluation shows
that our learnt instructions are rated similar
to human instructions, and significantly better
than the supervised learning baseline.

1 Introduction

Natural Language Generation (NLG) systems are
typically faced with a multitude of decisions dur-
ing their generation process due to nondeterminacy
between a semantic input to a generator and its re-
alised output. This is especially true in situated set-
tings, where sudden changes of context can occur
at anytime. Sources of uncertainty include (a) the
situational context, such as visible objects, or task
complexity, (b) the user, including their behaviour
and reactions, and (c) the dialogue history, includ-
ing shared knowledge or patterns of linguistic con-
sistency (Halliday and Hasan, 1976) and alignment
(Pickering and Garrod, 2004).

Previous work on context-sensitive generation in
situated domains includes Stoia et al. (2006) and
Garoufi and Koller (2010). Stoia et al. present a
supervised learning approach for situated referring
expression generation (REG). Garoufi and Koller

use techniques from AI planning for the combined
generation of navigation instructions and referring
expressions (RE). More generally, the NLG prob-
lem of non-deterministic decision making has been
addressed from many different angles, including
PENMAN-style choosers (Mann and Matthiessen,
1983), corpus-based statistical knowledge (Langk-
ilde and Knight, 1998), tree-based stochastic models
(Bangalore and Rambow, 2000), maximum entropy-
based ranking (Ratnaparkhi, 2000), combinatorial
pattern discovery (Duboue and McKeown, 2001),
instance-based ranking (Varges, 2003), chart gen-
eration (White, 2004), planning (Koller and Stone,
2007), or probabilistic generation spaces (Belz,
2008) to name just a few.

More recently, there have been several approaches
towards using Reinforcement Learning (RL) (Rieser
et al., 2010; Janarthanam and Lemon, 2010) or Hi-
erarchical Reinforcement Learning (HRL) (Deth-
lefs and Cuayáhuitl, 2010) for NLG decision mak-
ing. All of these approaches have demonstrated that
HRL/RL offers a powerful mechanism for learn-
ing generation policies in the absence of complete
knowledge about the environment or the user. It
overcomes the need for large amounts of hand-
crafted knowledge or data in rule-based or super-
vised learning accounts. On the other hand, RL
can have difficulties to find an optimal policy in a
large search space, and is therefore often limited to
small-scale applications. Pruning the search space
of a learning agent by including prior knowledge is
therefore attractive, since it finds solutions faster, re-
duces computational demands, incorporates expert
knowledge, and scales to complex problems. Sug-

78



gestions to use such prior knowledge include Lit-
man et al. (2000) and Singh et al. (2002), who
hand-craft rules of prior knowledge obvious to the
system designer. Cuayáhuitl (2009) suggests us-
ing Hierarchical Abstract Machines to partially pre-
specify dialogue strategies, and Heeman (2007) uses
a combination of RL and Information State (IS)
to also pre-specify dialogue strategies. Williams
(2008) presents an approach of combining Partially-
Observable Markov Decision Processes with con-
ventional dialogue systems. The Information State
approach is well-established in dialogue manage-
ment (e.g., Bohlin et al. (1999) and Larsson and
Traum (2000)). It allows the system designer to
specify dialogue strategies in a principled and sys-
tematic way. A disadvantage is that random design
decisions need to be made in cases where the best
action, or sequence of actions, is not obvious.

The contribution of this paper consists in a com-
prehensive account of constrained Hierarchical Re-
inforcement Learning through a combination with
a hierarchical Information State (HIS), which is in-
formed by prior knowledge induced from decision
trees. We apply our framework to the generation
of navigation strategies and referring expressions in
a situated setting, jointly optimised for task suc-
cess and linguistic consistency. An evaluation shows
that humans prefer our learnt instructions to the su-
pervised learning-based instructions, and rate them
equal to human instructions. Simulation-based re-
sults show that our semi-learnt approach learns more
quickly than the fully-learnt baseline, which makes
it suitable for large and complex problems. Our ap-
proach differs from Heeman’s in that we transfer it
to NLG and to a hierarchical setting. Although Hee-
man was able to show that his combined approach
learns more quickly than pure RL, it is limited to
small-scale systems. Our ‘divide-and-conquer’ ap-
proach, on the other hand, scales up to large search
spaces and allows us to address complex problems.

2 The Generation Tasks

2.1 The GIVE-2 Domain

Our domain is the generation of navigation instruc-
tions and referring expressions in a virtual 3D world
in the GIVE scenario (Koller et al., 2010). In this
task, two people engage in a ‘treasure hunt’, where

an instruction giver (IG) navigates an instruction fol-
lower (IF) through the world, pressing a sequence of
buttons and completing the task by obtaining a tro-
phy. Pairs take part in three dialogues (in three dif-
ferent worlds); after the first dialogue, they switch
roles. The GIVE-2 corpus (Gargett et al., 2010) pro-
vides transcripts of such dialogues in English and
German. For this paper, we complemented the En-
glish dialogues of the corpus with a set of seman-
tic annotations.1 The feature set is organised in five
groups (Table 1). The first two groups cover manip-
ulation instructions (i.e., instructions to press a but-
ton), including distractors2 and landmarks (Gargett
et al., 2010). The third group describes high- and
low-level navigation, the fourth group describes the
user. The fifth group finally contains grammatical
information.

2.2 Navigation and Manipulation Instructions

Navigation instructions can take many forms, even
for the same route. For example, a way to another
room can be described as ‘go to the room with the
lamp’, ‘go left and through the door’, or ‘turn 90
degrees, left, straight’. Choosing among these vari-
ants is a highly context- and speaker-dependent task.
Figure 1 shows the six user strategies we identified
from the corpus based on an analysis of the combi-
nation of navigation level (‘high’ vs. ‘low’) and con-
tent (‘destination’, ‘direction’, ‘orientation’, ‘path’,
‘straight’). User models are based on the navigation
level and content decisions made in a sequence of in-
structions, so that different sequences, with a certain
distribution, lead to different user model classifica-
tions. The proportions are shown in Figure 1. We
found that 75% of all speakers use the same strat-
egy in consecutive rounds/games. 62.5% of pairs
are consistent over all three dialogues, indicating
inter-speaker alignment. These high measures of
human consistency suggest that this phenomenon
is worth modelling in a learning agent, and there-
fore provides the motivation of including linguis-
tic consistency in our agent’s behaviour. Manipula-
tion instructions were treated as an REG task, which
needs to be sensitive to the properties of the referent
and distractors (e.g, size, colour, or spatial relation

1The annotations are available on request.
2Distractors are objects of the same type as the referent.

79



ID Feature Type Description

f1 absolute property(referent) boolean Is the colour of the referent mentioned?
f2 absolute property(distractor) boolean Is the colour of the distractor mentioned?
f3 discriminative colour(referent) boolean Is the colour of the referent discriminating?
f4 discriminative colour(distractor) boolean Is the colour of the distractor discriminating?
f5 mention(distractor) boolean Is a distractor mentioned?
f6 first mention(referent) boolean Is this the first reference to the referent?
f7 mention(macro landmark) boolean Is a macro (non-movable) landmark mentioned?
f8 mention(micro landmark) boolean Is a micro (movable) landmark mentioned?
f9 num(distractors) integer How many distractors are present?
f10 num(micro landmarks) integer How many micro landmarks are present?
f11 spatial rel(referent,obj) string Which spatial relation(s) are used in the RE?
f12 taxonomic property(referent) boolean Is the type of the distractor mentioned?
f13 within field of vision(referent) boolean Is the referent within the user’s field of vision?

f14 mention(colour, lm) boolean Is the colour of a macro- / micro lm mentioned?
f15 mention(size, lm) boolean Is the size of a macro- / micro lm mentioned?

f16 abstractness(nav instruction) string Is the instruction explicit or implicit?
f17 content(nav instruction) string Vals: destination, direction, orientation, path, straight
f18 level(nav instruction) string Is the instruction high- or low-level?

f19 position(user) string Is the user on track or off track?
f20 reaction(user) string Vals: take action, take wrong action, wait, req help
f21 type(user) string Vals: likes waiting, likes exploring, in between
f22 waits(user) boolean Is the user waiting for the next instruction?
f23 model(user) string User model/navig. strategy used (cf. Fig.1)?

f24 actor(instruction) boolean Is the actor of the instruction inserted?
f25 mood(instruction) boolean Is the mood of the instruction inserted?
f26 process(instruction) boolean Is the process of the instruction inserted?
f27 locational phrase(instruction) boolean Is the loc. phrase (path, straight, etc.) inserted?

Table 1: Corpus annotation features that were used as knowledge of the learning agent and the Information State. Fea-
tures are presented in groups, describing the properties of referents in the environment (f1...f13) and their distractors
(f14...f15), features of high- and low-level navigation (f16...f18), the user (f19...f23), and grammatical information
about constituents (f24...f27).

with respect to the referent) to be natural and dis-
tinguishing. We also considered the visual salience
of objects, and the type of spatial relation involved,
since recent studies indicate the potential relevance
of these features (Viethen and Dale, 2008). Given
these observations, we aim to optimise the task suc-
cess and linguistic consistency of instructions. Task
success is measured from user reactions after each
instruction (Section 5.1). Linguistic consistency is
achieved by rewarding the agent for generating in-
structions that belong to the same user model as the
previous one. The agent has the same probability
for choosing any pattern, but is then rewarded for

consistency. Table 3 (in Section 5.2) presents an ex-
ample dialogue generated by our system.

3 Constrained Hierarchical Reinforcement
Learning for NLG

3.1 Hierarchical Reinforcement Learning

Our idea of language generation as an optimisa-
tion problem is as follows: given a set of genera-
tion states, a set of actions, and an objective reward
function, an optimal generation strategy maximises
the objective function by choosing the actions lead-
ing to the highest reward for every reached state.
Such states describe the system’s knowledge about

80



Figure 1: Decision tree for the classification of user
models (UM) defined by the use of navigation level and
content. UM 0=high-level, UM 1=low-level (LL), UM
2=orientation-based LL, UM 3=orientation-based mix-
ture (M), UM 4=path-based M, UM 5=pure M.

the generation task (e.g. navigation strategy, or re-
ferring expressions). The action set describes the
system’s capabilities (e.g. ‘use high level naviga-
tion strategy’, ‘mention colour of referent’, etc.).
The reward function assigns a numeric value for
each action taken. In this way, language generation
can be seen as a finite sequence of states, actions
and rewards {s0, a0, r1, s1, a1, ..., rt−1, st}, where
the goal is to find an optimal strategy automatically.
To do this we use RL with a divide-and-conquer ap-
proach in order to optimise a hierarchy of generation
policies rather than a single policy. The hierarchy of
RL agents consists of L levels and N models per
level, denoted as M ij , where j ∈ {0, ..., N − 1}
and i ∈ {0, ..., L − 1}. Each agent of the hierar-
chy is defined as a Semi-Markov Decision Process
(SMDP) consisting of a 4-tuple < Sij, A

i
j , T

i
j , R

i
j >.

Sij is a set of states, A
i
j is a set of actions, T

i
j is

a transition function that determines the next state
s′ from the current state s and the performed ac-
tion a, and Rij is a reward function that specifies
the reward that an agent receives for taking an ac-
tion a in state s lasting τ time steps. The random
variable τ represents the number of time steps the
agent takes to complete a subtask. Actions can be
either primitive or composite. The former yield sin-
gle rewards, the latter correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optimal policy that max-

imises the reward for each visited state, according to
π∗ij(s) = arg maxa∈Ai

j
Q∗ij(s, a), where Q

∗i
j (s, a)

specifies the expected cumulative reward for exe-
cuting action a in state s and then following pol-
icy π∗ij . We use HSMQ-Learning (Dietterich, 1999)
for learning a hierarchy of generation policies. This
hierarchical approach has been applied successfully
to dialogue strategy learning by Cuayáhuitl et al.
(2010).

3.2 Information State

The notion of an Information State has traditionally
been applied to dialogue, where it encodes all infor-
mation relevant to the current state of the dialogue.
This includes, for example, the context of the in-
teraction, participants and their beliefs, and the sta-
tus of grounding. An IS consists of a set of infor-
mational components, encoding the information of
the dialogue, formal representations of these com-
ponents, a set of dialogue moves leading to the up-
date of the IS, a set of update rules which govern the
update, and finally an update strategy, which speci-
fies which update rule to apply in case more than one
applies (Larsson and Traum (2000), p. 2-3). In this
paper, we apply the theory of IS to language gener-
ation. For this purpose we define the informational
components of an IS to represent the (situational and
linguistic) knowledge of the generator (Section 4.2).
Update rules are triggered by generator actions, such
as the decision to insert a new constituent into the
current logical form, or the decision to prefer one
word order sequence over another. We use the DIP-
PER toolkit (Bos et al., 2003)3 for our implementa-
tion of the IS.

3.3 Combining Hierarchical Reinforcement
Learning and Information State

Previous work has suggested the HSMQ-Learning
algorithm for optimizing text generation strategies
(Dethlefs and Cuayáhuitl, 2010). Because such an
algorithm uses all available actions in each state,
an important extension is to constrain the actions
available with some prior expert knowledge, aim-
ing to combine behaviour specified by human de-
signers and behaviour automatically inferred by re-
inforcement learning agents. To that end, we sug-

3http://www.ltg.ed.ac.uk/dipper

81



Figure 2: (Left:) Hierarchy of learning agents executed from top to bottom for generating instructions. (Right:) State
representations for the agents shown in the hierarchy on the left. The features f1...f27 refer back to the features used
in the annotation given in the first column of Table 1. Note that agents can share information across levels.

gest combining the Information State approach with
hierarchical reinforcement learning. We therefore
re-define the characterisation of each Semi-Markov
Decision Process (SMDP) in the hierarchy as a 5-
tuple model M ij =< S

i
j, A

i
j , T

i
j , R

i
j , I

i
j >, where

Sij , A
i
j , T

i
j and R

i
j are as before, and the additional

element Iij is an Information State used as knowl-
edge base and rule-based decision maker. In this ex-
tended model, action selection is based on a con-
strained set of actions provided by the IS update
rules. We assume that the names of update rules
in Iij represent the agent actions A

i
j . The goal of

each SMDP is then to find an optimal policy that
maximises the reward for each visited state, accord-
ing to π∗ij(s) = arg maxa∈Ai

j
∩Ii

j
Q∗ij(s, a), where

Q∗ij (s, a) specifies the expected cumulative reward
for executing constrained action a in state s and then
following π∗ij thereafter. For learning such poli-
cies we use a modified version of HSMQ-Learning.
This algorithm receives subtask M ij and Information
State Iij used to initialise state s, performs similarly
to Q-Learning for primitive actions, but for compos-
ite actions it invokes recursively with a child sub-
task. In contrast to HSMQ-Learning, this algorithm
chooses actions from a subset derived by applying
the IS update rules to the current state of the world.
When the subtask is completed, it returns a cumu-
lative reward rt+τ , and continues its execution until

finding a goal state for the root subtask. This process
iterates until convergence occurs to optimal context-
independent policies, as in HSMQ-Learning.

4 Experimental Setting

4.1 Hierarchy of Agents

Figure 2 shows a (hand-crafted) hierarchy of learn-
ing agents for navigating and acting in a situated en-
vironment. Each of these agents represents an indi-
vidual generation task. Model M00 is the root agent
and is responsible for ensuring that a set of naviga-
tion instructions guide the user to the next referent,
where an RE is generated. Model M10 is responsible
for the generation of the RE that best describes an
intended referent. Subtasks M20 ... M

2
2 realise sur-

face forms of possible distractors, or macro- / micro
landmarks. Model M12 is responsible for the gener-
ation of navigation instructions which smoothly fit
into the linguistic consistency pattern chosen. Part
of this task is choosing between a low-level (model
M23 ) and a high-level (model M

2
4 ) instruction. Sub-

tasks M30 ...M
3
4 realise the actual instructions, des-

tination, direction, orientation, path, and ‘straight’,
respectively.4 Finally, model M11 can repair previ-
ous system utterances.

4Note that navigation instructions and REs correspond to se-
quences of actions, not to a single one.

82



Model(s) Actions

M0
0

navigation, manipulation, confirmation, stop, repair system act, repair no system act

M10 insert distractor, insert no distractor, insert no absolute property, insert micro relatum, insert macro relatum
insert no taxonomic property, insert absolute property, insert no macro relatum, insert taxonomic property

M12 choose high level, choose low level, get route, choose easy route, choose short route

M2
0
... M2

2
exp head, exp no head, insert colour, insert no colour, insert size, insert no size, exp spatial relation

M2
3

choose explicit abstractness, choose implicit abstractness, destination instruction, path instruction

M2
4

choose explicit abstractness, choose implicit abstractness, direction instr, orientation instr, straight instr

M30 ... M
3
4 exp actor, exp no actor, exp mood, exp loc phrase, exp no loc phrase, exp process, exp no process

Table 2: Action set of the learning agents and Information States.

4.2 State and Action Sets

The HRL agent’s knowledge base consists of all sit-
uational and linguistic knowledge the agent needs
for decision making. Figure 2 shows the hierarchy
of learning agents together with the knowledge base
of the learning agent with respect to the semantic
features shown in Table 1 that were used for the an-
notation of the GIVE-2 corpus dialogues. The first
column of the table in Figure 2 indicates the respec-
tive model, also referred to as agent, or subtask, and
the second column refers to the knowledge variable
it uses (in the form of the feature index given in the
first column of Table 1). In the agent, boolean values
and strings were represented as integers. The HIS
shares all information of the learning agent, but has
an additional set of relational feature-value pairs for
each slot. For example, if the agent knows that the
slot content(nav instruction) has value 1 (mean-
ing ‘filled’), the HIS knows also which value it was
filled with, such as path. Such additional knowledge
is required for the supervised learning baseline (Sec-
tion 5). The action set of the hierarchical learning
agent and the hierarchical information state is given
in Table 2. The state-action space size of a flat learn-
ing agent would be |S ×A| = 1011, the hierarchical
setting has a state-action space size of 2.4 × 107.
The average state-action space size of all subtasks is
|S × A|/14 = 1.7 × 107. Generation actions can
be primitive or composite. While the former corre-
spond to single generation decisions, the latter rep-
resent separate generation subtasks (Fig. 2).

4.3 Prior Knowledge

Prior knowledge can include decisions obvious to
the system designer, expert knowledge, or general

intuitions. In our case, we use a supervised learn-
ing approach to induce prior knowledge into our
HRL agent. We trained decision trees on our anno-
tated corpus data using Weka’s (Witten and Frank,
2005) J48 decision tree classifer. A separate tree
was trained for each semantic attribute (cf. Table
1). The obtained decision trees represent our super-
vised learning baseline. They achieved an accuracy
of 91% in a ten-fold cross-validation. For our semi-
learnt combination of HRL and HIS, we performed a
manual analysis of the resulting rules to assess their
impact on a learning agent.5 In the end, the fol-
lowing rules were used to constrain the agent’s be-
haviour: (1) In REs, always use a referent’s colour,
except in cases of repair when colour is not discrim-
inating; (2) mention a distractor or micro landmark,
if the colour of the referent is not discriminating;
(3) in navigation, always make orientation instruc-
tions explicit. All remaining behaviour was subject
to learning.

4.4 Reward Function

We use the following reward function to train the hi-
erarchy of policies of our HRL agent. It aims to re-
duce discourse length at maximal task success6 us-
ing a consistent navigation strategy.

R =



















0 for reaching the goal state
-2 for an already invoked subtask
+1 for generating instruction u con-

sistent with instruction u
−1

-1 otherwise.

5We excluded rules that always choose the same value, since
they would work against our aim of generating consistent, but
variable instructions.

6Task success is addressed by that the user has to ‘accept’
each instruction for a state transition.

83



The third reward that encourages consistency of in-
structions rewards a sequence of actions that allow
the last generated instruction to be classified as be-
longing to the same navigation strategy/user model
as the previously generated instruction (cf. 2.2).

5 Experiments and Results

5.1 The Simulated Environment

The simulated environment contains two kinds of
uncertainties: (1) uncertainty regarding the state of
the environment, and (2) uncertainty concerning the
user’s reaction to a system utterance. The first aspect
is represented by a set of contextual variables de-
scribing the environment, 7 and user behaviour.8 Al-
together, this leads to 115 thousand different contex-
tual configurations, which are estimated from data
(cf. Section 2.1). The uncertainty regarding the
user’s reaction to an utterance is represented by a
Naive Bayes classifier, which is passed a set of
contextual features describing the situation, mapped
with a set of semantic features describing the utter-
ance.9 From these data, the classifier specifies the
most likely user reaction (after each system act) of
perform desired action, perform undesired action, wait
and request help.10 The classifier was trained on the
annotated data and reached an accuracy of 82% in a
ten-fold cross validation.

5.2 Learnt Policies

With respect to REs, the fully-learnt policy (only
HRL) uses colour when it is discriminating, and a
distractor or micro landmark otherwise. The semi-
learnt policy (HRL with HIS) behaves as defined in
Section 4.3. The supervised learning policy (only
HIS) uses the rules learnt by the decision trees. Both
learnt policies learn to maximise task success, and
to generate consistent navigation strategies.11 The

7previous system act, route length, route status
(known/unknown), objects within vision, objects within
dialogue history, number of instructions, alignment(proportion)

8previous user reaction, user position, user wait-
ing(true/false), user type(explorative/hesitant/medium)

9navigation level(high / low), abstractness(implicit / ex-
plicit), repair(yes / no), instruction type(destination / direction /
orientation / path / straight)

10User reactions measure the system’s task success.
11They thereby also learn to adapt their semantic choices to

those most frequently made by humans.

10
1

10
2

10
3

10
4

−80

−70

−60

−50

−40

−30

−20

−10

0

A
ve

ra
ge

 R
ew

ar
d

Episodes

 

 

Deterministic
Semi−Learnt
Fully−Learnt

Figure 3: Comparison of fully-learnt, semi-learnt, and su-
pervised learning (deterministic) behaviours.

supervised learning policy generates successful in-
structions from the start. Note that we are not ac-
tually learning dialogue strategies, but rather gen-
eration strategies using dialogue features. There-
fore the described policies, fully-learnt, semi-learnt
and supervised-learning, exclusively guide the sys-
tem’s behaviour in the interaction with the simulated
user. An example dialogue is shown in Table 3. We
can observe that the agent starts using a low level
navigation strategy, and then switches to high level.
When the user gets confused, the system temporar-
ily switches back to low level. For referring expres-
sions, it first attempts to locate the referent by ref-
erence to a distractor, and then repairs by using a
micro landmark. The surface forms of instructions
were realised from templates, since the NLG system
so far only generates a sequence of content selection
decisions. We address surface realisation in Dethlefs
and Cuayáhuitl (2011).

We compared our semi-learnt policy against a
fully-learnt, and a supervised learning baseline. All
policies were trained for 40 thousand episodes. For
training, the step-size parameter α, which indicates
the learning rate, was initiated with 1 and then re-
duced over time by α = 1

1+t
, where t is the time step.

The discount rate γ, which indicates the relevance of
future rewards in relation to immediate rewards, was
set to 0.99, and the probability of a random action ǫ
was 0.01. See (Sutton and Barto, 1998) for details
on these parameters.

5.3 Comparison of Policies

Figure 3 compares the rewards averaged over 10
runs. In terms of learning speed, we can see that

84



Agent Policy Action (composite in italics) Utterance
Usr request route How do I get to the trophy?
Sys π00, π

1
2 root, navigation, dontRepair, orientation

π23, π
3
0 low level navigation, add direction+degrees First, turn 90 degrees left.

π23, π
3
1 low level navigation, ‘straight’ Then go straight ahead.

Usr perform desired action
Sys π00 confirmation OK.

π24, π
3
4, π

3
2 high level navigation, destination, direction,

π24, π
3
4, π

3
2 addLocation Turn left at the end of the hallway.

Usr wait
Sys π11 repair system act, change navigation strategy

π23, π
3
2 low level navigation, direction Turn right

π24, π
3
3 high level navigation, path and go down the hallway.

Usr perform desired action
Sys π23, π

3
2 low level navigation, direction Now turn left.

Usr perform desired action
Sys π00 confirmation OK.

π24, π
3
4 high level navigation, destination,

π24, π
3
4 addLocation Enter to the room in front of you.

Usr perform desired action
Sys π10, π

2
1 manipulation, use colour, use distractor Press the red button beside the blue.

Usr request help
Sys π11 repair system act, change granularity

π24, π
1
0 high level navigation, manipulation

π34, π
2
2 destination, use colour, use micro landmark Go to the red button beside the chair.

Usr perform desired action
Sys π10 manipulation Press it.
Usr perform desired action
Sys π00 confirmation Well done.

Table 3: Sample dialogue in the GIVE-2 scenario showing the dynamics of generation policies. See Figure 2 for the
corresponding hierarchy models, and Table 2 for the action set. See Section 5.2 for an explantation of the dialogue.

while the semi-learnt behaviour is able to follow a
near-optimal policy from the beginning, the fully-
learnt policy takes about 40 thousand episodes to
reach the same performance. In terms of simulated
task success, we see that while the supervised learn-
ing behaviour follows a good policy from the start,
it is eventually beaten by the learnt policies.

5.4 Human Evaluation Study

We asked 11 participants12 to rate altogether 132
sets of instructions, where each set contained a spa-
tial graphical scene containing a person, mapped
with one human, one learnt, and one supervised

126 female, 5 male with an age average of 26.4.

learning instruction. Instructions consisted of a nav-
igation instruction followed by a referring expres-
sion. Subjects were asked to rate instructions on a
1-5 Likert scale (where 5 is the best) for their help-
fulness on guiding the displayed person from its ori-
gin to pressing the intended button. We selected
six different scenarios for the evaluation: (a) only
one button is present, (b) two buttons are present,
the referent and a distractor of the same colour as
the referent, (c) two buttons are present, the referent
and a distractor of a different colour than the refer-
ent, (d) one micro landmark is present and one dis-
tractor of the same colour as the referent, (e) one
micro landmark is present and one distractor of a
different colour than the referent. All scenarios oc-

85



Figure 4: Example scenario of the human evaluation study.

curred twice in each evaluation sheet, their specific
instances were drawn from the GIVE-2 corpus at
random. Scenes and instructions were presented in
a randomised order. Figure 4 presents an example
evaluation scene. Finally, we asked subjects to cir-
cle the object they thought was the intended refer-
ent. Subjects rated the human instructions with an
average of 3.82, the learnt instructions with an aver-
age of 3.55, and the supervised learning instructions
with an average of 2.39. The difference between hu-
man and learnt is not significant. The difference be-
tween learnt and supervised learning is significant at
p < 0.003, and the difference between human and
supervised learning is significant at p < 0.0002. In
96% of all cases, users were able to identify the in-
tended referent.

6 Conclusion and Discussion

We have presented a combination of HRL with a hi-
erarchical IS, which was informed by prior knowl-
edge from decision trees. Such a combined frame-
work has the advantage that it allows us to system-
atically pre-specify (obvious) generation strategies,
and thereby find solutions faster, reduce computa-
tional demands, scale to complex domains, and in-
corporate expert knowledge. By applying HRL to
the remaining (non-obvious) action set, we are able
to learn a flexible, generalisable NLG policy, which
will take the best action even under uncertainty. As
an application of our approach and its generalisabil-
ity across domains, we have presented the joint op-
timisation of two separate NLG tasks, navigation in-

structions and referring expressions, in situated dia-
logue under the aspects of task success and linguis-
tic consistency. Based on an evaluation in a simu-
lated environment estimated from data, we showed
that our semi-learnt behaviour outperformed a fully-
learnt baseline in terms of learning speed, and a su-
pervised learning baseline in terms of average re-
wards. Human judges rated our instructions signif-
icantly better than the supervised learning instruc-
tions, and close to human quality. The study re-
vealed a task success rate of 96%. Future work
can transfer our approach to different applications to
confirm its benefits, and induce the agent’s reward
function from data to test in a more realistic setting.

Acknowledgments

Thanks to the German Research Foundation DFG
and the Transregional Collaborative Research Cen-
tre SFB/TR8 ‘Spatial Cognition’ and the EU-FP7
project ALIZ-E (ICT-248116) for partial support of
this work. Also, thanks to John Bateman for com-
ments on an earlier draft of this paper.

References

Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th conference on Computa-
tional linguistics - Volume 1, pages 42–48.

Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 1:1–26.

86



Peter Bohlin, Robin Cooper, Elisabet Engdahl, and
Staffan Larsson. 1999. Information states and di-
alogue move engines. In IJCAI-99 Workshop on
Knowledge and Reasoning in Practical Dialogue Sys-
tems.

Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi Oka.
2003. DIPPER: Description and Formalisation of an
Information-State Update Dialogue System Architec-
ture. In 4th SIGDial Workshop on Discourse and Dia-
logue, pages 115–124.

Heriberto Cuayáhuitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395–429.

Heriberto Cuayáhuitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. thesis,
School of Informatics, University of Edinburgh.

Nina Dethlefs and Heriberto Cuayáhuitl. 2010. Hi-
erarchical Reinforcement Learning for Adaptive Text
Generation. Proceedings of INLG ’10.

Nina Dethlefs and Heriberto Cuayáhuitl. 2011. Hier-
archical Reinforcement Learning and Hidden Markov
Models for Task-Oriented Natural Language Genera-
tion. In Proceedings of ACL-HLT 2011, Portland, OR.

Thomas G. Dietterich. 1999. Hierarchical reinforce-
ment learning with the maxq value function decom-
position. Journal of Artificial Intelligence Research,
13:227–303.

Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In ACL ’01, pages 172–179.

Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The give-2 corpus of
giving instructions in virtual environments. In LREC.

Konstantina Garoufi and Alexander Koller. 2010. Au-
tomated planning for situated natural language gener-
ation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1573–1582, July.

Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.

Peter Heeman. 2007. Combining reinforcement learning
with information-state update rules. In Human Tech-
nology Conference (HLT), pages 268–275.

Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In ACL ’10,
pages 69–78.

Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of ACL-07.

Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon

Oberlander. 2010. The first challenge on generat-
ing instructions in virtual environments. In M. The-
une and E. Krahmer, editors, Empirical Methods
on Natural Language Generation, pages 337–361,
Berlin/Heidelberg, Germany. Springer.

Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
ACL-36, pages 704–710.

Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Nat. Lang. Eng., 6(3-
4):323–340.

Diane J. Litman, Michael S. Kearns, Satinder Singh, and
Marilyn A. Walker. 2000. Automatic optimization of
dialogue management. In Proceedings of the 18th con-
ference on Computational linguistics, pages 502–508.

William Mann and Christian M I M Matthiessen. 1983.
NIGEL: A systemic grammar for text generation.
Technical report, ISI/RR-85-105.

Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistc psychology of dialog. Behavioral and
Brain Sciences, 27.

Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL, pages 194–201.

Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In ACL ’10, pages 1009–1018.

Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105–133.

Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase gen-
eration for situated dialogs. In Proceedings of INLG
’06, pages 81–88.

Richard S Sutton and Andrew G Barto. 1998. Reinforce-
ment Learning: An Introduction. MIT Press, Cam-
bridge, MA, USA.

Sebastian Varges. 2003. Instance-based Natural Lan-
guage Generation. Ph.D. thesis, School of Informat-
ics, University of Edinburgh.

Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of INLG ’08, INLG ’08, pages 59–67.

Michael White. 2004. Reining in CCG chart realization.
In In Proc. INLG-04, pages 182–191.

Jason Williams. 2008. The best of both worlds: Uni-
fying conventional dialog systems and POMDPs. In
Interspeech, Brisbane.

Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. 2. edi-
tion.

87


