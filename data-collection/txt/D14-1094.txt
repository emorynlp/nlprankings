



















































Balanced Korean Word Spacing with Structural SVM


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875â€“879,
October 25-29, 2014, Doha, Qatar. cÂ©2014 Association for Computational Linguistics

Balanced Korean Word Spacing with Structural SVM 

Changki Lee*                  Edward Choi                  Hyunki Kim 
*Kangwon National University, Chuncheon-si, Gangwondo, 200-701, Korea 

Electronics and Telecommunications Research Institute, Daejeon, 305-350, Korea 
leeck@kangwon.ac.kr             mp2893@gmail.com                 hkk@etri.re.kr 

 

Abstract 

Most studies on statistical Korean word spac-
ing do not utilize the information provided by 
the input sentence and assume that it was 
completely concatenated. This makes the word 
spacer ignore the correct spaced parts of the 
input sentence and erroneously alter them. To 
overcome such limit, this paper proposes a 
structural SVM-based Korean word spacing 
method that can utilize the space information 
of the input sentence. The experiment on sen-
tences with 10% spacing errors showed that 
our method achieved 96.81% F-score, while 
the basic structural SVM method only 
achieved 92.53% F-score. The more the input 
sentence was correctly spaced, the more accu-
rately our method performed. 

1 Introduction 
Automatic word spacing is a task to decide 
boundaries between words, which is frequently 
used for correcting spacing errors of text mes-
sages, Tweets, or Internet comments before using 
them in information retrieval applications (Lee 
and Kim, 2012). It is also often used in post-
processing optical character recognition (OCR) 
or voice recognition (Lee et al., 2007). Except 
for some Asian languages such as Chinese, Japa-
nese and Thai, most languages have explicit 
word spacing that improves the readability of the 
text and helps readers better understand the 
meaning of it. Korean especially has a tricky 
word spacing system and users often make mis-
takes, which makes automatic word spacing an 
interesting and essential task. 

In order to easily acquire the training data, 
most studies on statistical Korean word spacing 
assume that well-spaced raw text (e.g. newspaper 
articles) is perfectly spaced and use it for training 
(Lee and Kim, 2012; Lee and Kim, 2013; Lee et 
al., 2007; Shim, 2011). This approach, however, 
cannot observe incorrect spacing since the as-
sumption makes the training data devoid of nega-
tive example. Consequently, word spacers cannot 
use the spacing information given by the user, 
and erroneously alter the correctly spaced parts 

of the sentence. To utilize the user-given spacing 
information, a corpus of input sentences and their 
correctly spaced version is necessary. Construct-
ing such corpus, however, requires much time 
and resource. 

In this paper, to resolve such issue, we propose 
a structural SVM-based Korean word spacing 
model that can utilize the word spacing infor-
mation given by the user. We name the proposed 
model â€œBalanced Word Spacing Model 
(BWSM)â€. Our approach trains a basic structural 
SVM-based Korean word spacing model as in 
(Lee and Kim, 2013), and tries to obtain the sen-
tence which achieves the maximum score for the 
basic model while minimally altering the input 
sentence. 

In the following section, we discuss related 
studies. In Section 3, the proposed method and 
its relation to Karush-Kuhn-Tucker (KKT) con-
dition are explained. The experiment and discus-
sion is presented in Section 4. Finally, in Section 
5, the conclusion and future work for this study 
is given. 

2 Related Work 
There are two common approaches to Korean 
word spacing: rule-based approach and statistical 
approach. In rule-based approach, it is not easy 
to construct rules and maintain them. Further-
more, it requires morphological analysis to apply 
rule-based approach, which slows down the pro-
cess. Recent studies, therefore, mostly focus on 
the statistical approach.  

Most statistical approaches use well-spaced 
raw corpus as training data (e.g. newspaper arti-
cles) assuming that they are perfectly spaced. 
This is to avoid the expensive job of constructing 
new training data. Lee et al. (2007) treated the 
word spacing task as a sequence labeling prob-
lem on the input sentence which is a sequence of 
syllables. They proposed a method based on 
Hidden Markov Model (HMM). Shim (2011) 
also considered the word spacing task as a se-
quence labeling problem and proposed a method 
using Conditional Random Field (CRF) (Lafferty 
et al., 2001), which is a well-known powerful 
model for sequence labeling tasks. Lee and Kim 

875



(2013) tried to solve the sequence labeling prob-
lem using structural SVM (Tsochantaridis et al., 
2004; Joachims et al., 2009; Lee and Jang 2010; 
Shalev-Shwartz et al., 2011). 

The studies above (Lee and Kim, 2013; Lee et 
al., 2007; Shim, 2011), however, do not take ad-
vantage of the spacing information provided by 
the user, and often erroneously alter the correctly 
spaced part of the sentence. Lee et al. (2007) 
tries to resolve this issue by combining an HMM 
model with an additional confidence model con-
structed from another corpus. Given an input 
sentence, they first apply the basic HMM model 
to obtain a candidate sentence. For every differ-
ent word spacing between the input sentence and 
the candidate sentence, they calculate and com-
pare the confidence using the confidence model, 
and whichever gets the higher confidence is used. 
The spacing accuracy was improved from 97.52% 
to 97.64%1.  

This study is similar to (Lee et al., 2007) in 
that it utilizes the spacing information given by 
the user. But unlike (Lee et al., 2007), BWSM 
uses structural SVM as the basic model and do 
not require an additional confidence model. Fur-
thermore, while Lee et al. (2007) compares the 
spacing confidence for each syllable to obtain the 
final outcome, BWSM considers the whole sen-
tence when altering its spacing, enabling it to 
achieve higher improvement on performance 
(from 92.53% F-score to 96.81% F-score). 

3 Balanced Word Spacing Model 
Like previous studies, the proposed model treats 
the Korean word spacing task as a sequence la-
beling problem. The label consists of B and I, 
which are assigned to each syllable of the sen-
tence. Assuming that x = <x1, x2, â€¦, xT> is a se-
quence of total T syllables of the input sentence 
and y = <y1, y2, â€¦, yT> is a sequence of labels 
for each syllable, an example could be given as 
follows2:  
 

Input: ah/beo/ji/ga   bang/eh   deul/eo/ga/sin/da 
(Father entered the room) 
x = <ah, beo, ji, ga, bang, eh, deul, eo, ga, sin, da> 
y = <B,    I,    I,   I,    B,     I,     B,    I,   I,    I,    I> 

Figure 1: An example of word spacing. 
 
In order to utilize the spacing information pro-

vided by the user, we propose a new model, the 

                                                
1 Accuracy was calculated based on syllables. 
2 Slashes are used for distinguishing between syllables. 

Balanced Word Spacing Model that adheres to 
the following principles: 

 
1. The model must obtain the most likely se-

quence of labels(y*), while minimally altering 
the user-given sequence of labels (ğ²!"#$%). 

2. We assume that it costs Î± per syllable to 
change the spacing of the original sentence, in 
order to keep the original spacing information 
as much as possible. 

 
Mathematically formulating the above princi-

ples would give us the following equation: 
 

ğ²âˆ— = argmax ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ² âˆ’ ğ›¼ âˆ™ ğ¿ ğ²!"#$% , ğ²          (1) 
 
In Equation 1, ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ²  calculates how com-
patible the sequence of label y is with the input 
sentence x. It is calculated by a basic word spac-
ing model as in (Lee and Kim, 2013). 
ğ¿ ğ²!"#$% , ğ²  counts the number of different la-
bels between the user-given sequence of labels 
ğ²!"#$% and an arbitrary sequence of labels ğ². ğ²âˆ— 
of Equation 1 can be obtained by setting the gra-
dient of ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ² âˆ’ ğ›¼ âˆ™ ğ¿ ğ²!"#$% , ğ²  to 0, 
which is equivalent to the following equation: 
 
âˆ‡ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ²âˆ— =  Â ğ›¼ âˆ™ âˆ‡ğ¿ ğ²!"#$% , ğ²âˆ—                         (2) 
 

In order to view the proposed model in a dif-
ferent perspective, we consider BWSM in terms 
of Karush-Kuhn-Tucker (KKT) condition. KKT 
condition is a technique for solving optimization 
problems with inequality constraints. It is a gen-
eralized version of Lagrange multipliers, which 
is a technique for solving optimization problems 
with equality constraints. Converting the afore-
mentioned principles to a constrained optimiza-
tion problem gives: 
 
Maximize: Â ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ²   
subject Â to Â ğ¿ ğ²!"#$% , ğ² â‰¤ ğ‘                                      (3) 
 
Equation 3 tries to obtain y that maximizes 
ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ² , namely the score of the basic model, 
while maintaining ğ¿ ğ²!"#$% , ğ²  below b, which is 
equivalent to altering the word spacing of the 
input sentence less than or equal to b times. To 
solve this constrained optimization problem, we 
apply KKT condition and define a new Lagran-
gian function as follows: 
 
Î› ğ±, ğ²,ğ›¼ = ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ² âˆ’ ğ›¼ ğ¿ ğ²!!"#$ , ğ² âˆ’ ğ‘       (4) 
 

876



Setting the gradient of the Equation 4 to zero, 
namely âˆ‡Î› ğ±, ğ²,ğ›¼ = 0 , we get the following 
necessary conditions: 
 
Stationarity: Â âˆ‡ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ²âˆ— = ğ›¼âˆ— âˆ‡ğ¿ ğ²!"#$% , ğ²âˆ—   
Primal feasibility: ğ¿ ğ²!"#$% , ğ²âˆ— â‰¤ ğ‘  
Dual Â feasibility: Â ğ›¼âˆ— â‰¥ 0  
Complementary Â slackness: Â ğ›¼âˆ— ğ¿ ğ²!"#$% , ğ²âˆ— âˆ’ ğ‘ = 0   (5) 
 

Comparing Equation 1 with Equation 4 reveals 
that they are the same except the constant b. And 
ğ²âˆ— which satisfies the conditions of Equation 5, 
and hence the solution to Equation 4, is also the 
same as ğ²âˆ— which satisfies Equation 2, and hence 
the solution to Equation 1. 

For the basic word spacing model, we use 
margin rescaled version of structural SVM as 
Lee and Kim (2013). The objective function of 
structural SVM is as follows: 
 
minğ’˜,!

!
!
âˆ¥ ğ° âˆ¥! + !

!
ğœ‰!!! , Â  Â  Â ğ‘ . ğ‘¡. Â  Â âˆ€ğ‘–, ğœ‰! â‰¥ 0  

âˆ€ğ‘–,âˆ€ğ² âˆˆ Y\ğ²!:ğ°!ğ›¿Î¨ ğ±! , ğ² â‰¥ ğ¿ ğ²! , ğ² âˆ’ ğœ‰!  
where Â ğ›¿Î¨ ğ±! , ğ² = Î¨ ğ±! , ğ²! âˆ’ Î¨ ğ±! , ğ²                  (6) 
 

In Equation 6, ğ±! , ğ²!  represents the i-th se-
quence of syllables and its correct spacing labels. 
ğ¿ ğ²! , ğ²  is a loss function that counts the number 
of different labels between the correct labels ğ²! 
and the predicted sequence of labels ğ². Î¨ ğ±, ğ²  is 
a typical feature vector function. The features 
used for the basic word spacing model are the 
same features used in (Lee and Kim, 2013). 
Since structural SVM was used for the basic 
word spacing model, the score function of Equa-
tion 1 becomes ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ² = ğ°!Î¨ ğ±, ğ² . 

We propose two approaches for implementing 
Equation 1.  

 
1. N-best re-ranking: N-best sequences of spac-

ing labels are obtained using the basic struc-
tural SVM model. For each of the sequence, 
ğ›¼âˆ—ğ¿ ğ²!"#$% , ğ²âˆ—  is calculated and subtracted 
from ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ ğ±, ğ² . The result of the subtrac-
tion is used to re-rank the sequences, and the 
one with the highest rank is chosen. 

2. Modified Viterbi search: Viterbi search algo-
rithm, which is used in the basic word spacing 
model to solve ğ²âˆ— = argmax ğ°!Î¨ ğ±, ğ² , is 
modified to solve ğ²âˆ— = argmax ğ°!Î¨ ğ±, ğ² âˆ’
ğ›¼ âˆ™ ğ¿ ğ²!"#$% , ğ² . Both Î¨ ğ±, ğ²  and 
ğ¿ ğ²!"#$% , ğ²  can be calculated syllable by syl-
lable, which makes it easy to modify Viterbi 
search algorithm. 

 

The first approach seems straightforward and 
easy, but it would take a long time to obtain N-
best sequences of labels. Furthermore, the correct 
label sequence might not be in those N-best se-
quences, hence degrading the overall perfor-
mance. The second approach is fast since it does 
not calculate N-best sequences, and unlike the 
first approach, will always consider the correct 
label sequence as a candidate. 

4 Experiment 
In order to compare the performance of BWSM 
with HMM-based Korean word spacing and 
structural SVM-based Korean word spacing, we 
use Sejong raw corpus (Kang and Kim, 2004) as 
train data and ETRI POS tagging corpus as test 
data3. Pegasos-struct algorithm from (Lee and 
Kim, 2013) was used to train the basic structural 
SVM-based model. The optimal value for the 
tradeoff variable C of structural SVM was found 
after conducting several experiments4.  

The rate of word spacing error varies depend-
ing on the corpus. Newspaper articles rarely have 
word spacing errors but text messages or Tweets 
frequently contain word spacing errors. To re-
flect such variety, we randomly insert spacing 
errors into the test set to produce various test sets 
with spacing error rate 0%, 10%, 20%, 35%, 
50%, 60%, and 70%5. 

 

 
Figure 2: Word-based F-score of N-best re-

ranking approach. 
 

Figure 2 shows the relation between ğ›¼(x-axis) 
and word-based F-score6(y-axis) of N-best re-

                                                
3 The number of words for the training set and test set are 26 
million and 290,000 respectively. 
4 We experimented with 10, 100, 1000, 10000, 100000 and 
1000000, the optimal value being 100000. 
5 We altered the input to the system and retained the origi-
nal gold standardâ€™s space unit. 
6 Word-based F-score = 2*Precword*Recallword / (Precword + 
Recallword), 
Precword = (# of correctly spaced words) / (the total number 
of words produced by the system), 

877



ranking approach using test sets with different 
spacing error rate. When ğ›¼ = 0 , BWSM be-
comes a normal structural SVM-based model. As 
ğ›¼ increases, F-score also increases for a while 
but decreases afterward. And F-score increases 
more when using test sets with low error rate. It 
is worth noticing that when using the test set 
with 0% error rate, as ğ›¼ increases, F-score con-
verges to 98%. The reason it does not reach 100% 
is that the correct label sequence is sometimes 
not included in the N-best sequences. 
 

 
Figure 3: Word-based F-score of modified 

Viterbi search. 
 

Figure 3 shows the relation between ğ›¼(x-axis) 
and word-based F-score(y-axis) of modified 
Viterbi search approach using test sets with dif-
ferent spacing error rate. The graphs are similar 
to Figure 2, but F-score reaches higher values 
compared to N-best re-ranking approach. Notice 
that, when using the test set with 0% error rate, 
F-score becomes 100% as ğ›¼ surpasses 3. This is 
because, unlike N-best re-ranking approach, 
modified Viterbi search approach considers all 
possible sequences as candidates.  

From Figure 2 and 3, it can be seen that 
BWSM, which takes into consideration the spac-
ing information provided by the user, can im-
prove performance significantly. It is also appar-
ent that modified Viterbi search approach outper-
forms N-best re-ranking approach. The optimal 
value for ğ›¼ varies as test sets with different error 
rate are used. It is natural that, for test sets with 
low error rate, the optimal value of ğ›¼ increases, 
thus forcing the model to more utilize the user-
given spacing information. It is difficult to auto-
matically obtain the optimal ğ›¼ for an arbitrary 
input sentence. Therefore we set ğ›¼ to 1, which, 
according to Figure 3, is more or less the optimal 
value for most of the test sets. 
 

                                                                       
Recallword = (# of correctly spaced words) / (the total num-
ber of words in the test data) 

Model 
Syllable 
based 
precision 

Word  
based 
precision 

HMM (Lee et al., 2007) 
S-SVM (Lee and Kim, 2013) 

98.44 
99.01 

90.31 
92.53 

Modified Viterbi (error rate 10%) 
Modified Viterbi (error rate 20%) 
Modified Viterbi (error rate 35%) 

99.64 
99.55 
99.35 

96.81 
96.21 
95.01 

Table 1: Precision of BWSM and previous  
studies 

 
With ğ›¼ set to 1, and using modified Viterbi 

search algorithm, the performance of BWSM is 
shown in Table 1 with other previous studies 
(Lee and Kim, 2013; Lee et al., 2007). Table 1 
shows that BWSM gives superior performance 
than other studies that do not utilize user-given 
spacing information. 
 

 
Figure 4: Word-based F-score of modified 

Viterbi search on Tweets. 
 

We also collected Tweets from Twitter and 
tested modified Viterbi algorithm on them. Fig-
ure 4 shows the relation between ğ›¼ (x-axis) and 
word-based F-score (y-axis). The raw Tweets 
showed word-based F-score of approximate 91%, 
and the basic structural SVM model (ğ›¼ = 0) 
showed somewhat inferior 88%. Modified 
Viterbi algorithm showed the similar behavior as 
Figure 3, showing 93.2~93.4% word-based F-
score when ğ›¼ was set to 0.5~1. Figure 4 shows 
that BWSM is effective not only on text with 
randomly inserted spacing errors, but also on 
actual data, Tweets. 

5 Conclusion 
In this paper, we proposed BWSM, a new struc-
tural SVM-based Korean word spacing model 
that utilizes user-given spacing information. 
BWSM can obtain the most likely sequence of 
spacing labels while minimally altering the word 
spacing of the input sentence. Experiments on 
test sets with various error rate showed that 
BWSM significantly improved word-based F-

878



score, from 95.47% to 98.39% in case of the test 
set with 10% error rate.  

For future work, there are two interesting di-
rections. First is to improve BWSM so that it can 
automatically obtain the optimal value of ğ›¼ for 
an arbitrary sentence. This will require a training 
set consisting of text with actual human spacing 
errors and its corrected version. Second is to ap-
ply BWSM to other interesting problems such as 
named entity recognition (NER). Newspaper ar-
ticles often use certain symbols such as quotation 
marks or brackets around the titles of movies, 
songs and books. Such symbols can be viewed as 
user-given input, which BWSM will try to re-
spect as much as possible while trying to find the 
most likely named entities. 

Acknowledgments  
This work was supported by the IT R&D pro-
gram of MSIP/KEIT (10044577, Development of 
Knowledge Evolutionary WiseQA Platform 
Technology for Human Knowledge Augmented 
Services). We would like to thank the anony-
mous reviewers for their comments. 

References  
Thorsten Joachims, Thomas Finley, and Chun-Nam 

John Yu. 2009. Cutting-plane training of structural 
SVMs. Machine Learning, Vol. 77, No. 1. 

Beom-mo Kang and Hunggyu Kim. 2004. Sejong 
Korean Corpora in the making. In Proceedings of 
the LREC, 1747-1750. 

John Lafferty, Andrew McCallum and Fernando Pe-
reira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence 
data. In Proceedings of the ICML, 282-289. 

Changki Lee and Myung-Gil Jang. 2010. A Modified 
Fixed-threshold SMO for 1-Slack Structural SVM. 
ETRI Journal, Vol.32, No.1, 120-128. 

Changki Lee and Hyunki Kim. 2012. Automatic Ko-
rean word spacing using structural SVM. In Pro-
ceedings of the KCC, 270-272. 

Changki Lee and Hyunki Kim. 2013. Automatic Ko-
rean word spacing using Pegasos algorithm. Infor-
mation Processing and Management, Vol. 49, No. 
1, 370-379. 

Do-Gil Lee, Hae-Chang Rim and Dongsuk Yook. 
2007. Automatic word spacing using probabilistic 
models based on character n-grams. Intelligent Sys-
tems IEEE, Vol. 22, No. 1, 28-35. 

Seung-Wook Lee, Hae-Chang Rim and So-Young 
Park. 2007. A new approach for Korean word spac-

ing incorporating confidence value of userâ€™s input. 
In Proceedings of the ALPIT, 92-97. 

Kwang-Sup Shim. 2011. Automatic word spacing 
based on Conditional Random Fields. Korean 
Journal of Cognitive Science, Vol. 22, No. 2, 217-
233. 

Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2004. Pegasos: Primal estimated sub-gradient 
solver for SVM. Mathematical Programming, Vol. 
127, No. 1. 

Ioannis Tsochantaridis, Thomas Hofmann, Thorsten 
Joachims and Yasemin Altun. 2004. Support vector 
machine learning for interdependent and structured 
output spaces. In Proceedings of the ICML, 104-
111. 

 

 

879


