525

Coling 2010: Poster Volume, pages 525–533,

Beijing, August 2010

A Comparative Study on Ranking and Selection Strategies for 

Multi-Document Summarization 

Feng Jin, Minlie Huang, Xiaoyan Zhu 

State Key Laboratory of Intelligent Technology and Systems 

Tsinghua National Laboratory for Information Science and Technology 

Dept. of Computer Science and Technology, Tsinghua University 

jinfengfeng@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn

Abstract 

This paper presents a comparative study 
on two key problems existing in extrac-
tive summarization: the ranking problem 
and  the  selection  problem.  To  this  end, 
we  presented  a  systematic  study  of 
comparing different learning-to-rank al-
gorithms and comparing different selec-
tion strategies. This is the first work of 
providing  systematic  analysis  on  these 
problems.  Experimental  results  on  two 
benchmark  datasets  demonstrate  three 
findings: (1) pairwise and listwise learn-
ing-to-rank  algorithms  outperform  the 
baselines  significantly;  (2)  there  is  no 
significant  difference  among  the  learn-
ing-to-rank  algorithms;  and  (3)  the  in-
teger 
selection 
strategy  generally  outperformed  Maxi-
mum Marginal Relevance and Diversity 
Penalty strategies. 

linear  programming 

Introduction 

1 
As the rapid development of the Internet, docu-
ment  summarization  has  become  an  important 
task  since  document  collections  are  growing 
larger  and  larger.  Document  summarization, 
which aims at producing a condensed version of 
the original document(s), helps users to acquire 
information that is both important and relevant 
to  their  information  need.    So  far,  researchers 
have  mainly  focused  on  extractive  methods 
which  choose  a  set  of  salient  textual  units  to 
form a summary.  Such textual units are typical-
ly  sentences,  sub-sentences  (Gillick  and  Favre, 
2009), or excerpts (Sauper and Barzilay, 2009).  
Almost all extractive summarization methods 
face two key problems: the first problem is how 
to rank textual units, and the second one is how 

to  select  a  subset  of  those  ranked  units.  The 
ranking  problem  requires  systems  model  the 
relevance of a textual unit to a topic or a query. 
In this paper, the ranking problem refers to ei-
ther sentence ranking or concept ranking. Con-
cepts  can  be  unigrams,  bigrams,  semantic  con-
tent units, etc., although in our experiment, only 
bigrams  are  used  as  concepts.  The  selection 
problem  requires  systems  improve  diversity  or 
remove  redundancy  so  that  more  relevant  in-
formation can be covered by the summary as its 
length  is  limited.  As  our  paper  focuses  on  ex-
tractive  summarization,  the  selection  problem 
refers  to  selecting  sentences.  However,  the  se-
lection  framework  presented  here  is  universal 
for selecting arbitrary textual units, as discussed 
in Section 4. 

There  have  been  a  variety  of  studies  to  ap-
proach the ranking problem. These include both 
unsupervised  sentence  ranking  (Luhn,  1958; 
Radev and Jing, 2004, Erkan and Radev, 2004), 
and  supervised  methods  (Ouyang  et  al.,  2007; 
Shen et al., 2007; Li et al., 2009). Even given a 
list of ranked sentences, it is not trivial to select 
a subset of sentences to form a good summary 
which  includes  diverse  information  within  a 
length limit. Three common selection strategies 
have been studied to address this problem: Max-
imum  Marginal  Relevance  (MMR)  (Carbonell 
and  Goldstein,  1998),  Diversity  Penalty  (DiP) 
(Wan,  2007),  and  integer  linear  programming 
(ILP)  (McDonald,  2007;  Gillick  and  Favre, 
2009).  As  different  methods  were  often  eva-
luated on different datasets, it is of great value 
to systematically compare ranking and selection 
strategies on the same dataset. However, to the 
best of our knowledge, there is still no work to 
compare different ranking strategies or compare 
different selection strategies.  

In  this  paper,  we  presented  a  comparative 
study on the ranking problem and the selection 

526

problem  for  extractive  summarization.  We 
compared  three  genres  of  learning-to-rank  me-
thods for ranking sentences or concepts: SVR, a 
pointwise  ranking  algorithm;  RankNet,  a  pair-
wise learning-to-rank algorithm; and ListNet, a 
listwise learning-to-rank algorithm. We adopted 
an  ILP  framework  that  is  able  to  select  sen-
tences  based  on  sentence  ranking  or  concept 
ranking.  We  compared  it  with  other  selection 
strategies such as MMR and Diversity Penalty. 
We conducted our comparative experiments on 
the TAC 2008 and TAC 2009 datasets, respec-
tively.  Our  contributions are  two-fold:  First,  to 
the best of our knowledge, this is the first work 
of  presenting  systematic  and  in-depth  analysis 
on comparing ranking strategies and comparing 
selection  strategies.  Second,  this  is  the  first 
work  using  pairwise  and  liswise  learning-to-
rank  algorithms  to  perform  concept  (word  bi-
gram) ranking for extractive summarization.  

The rest of this paper is organized as follows. 
We introduce the related work in Section 2. In 
Section 3, we present three ranking algorithms, 
SVR,  RankNet,  and  ListNet.  We  describe  the 
sentence selection problem with an ILP frame-
work described in Section 4. We introduce fea-
tures  in  Section  5.  Evaluation  and  experiments 
are presented in Section 6. Finally, we conclude 
this paper in Section 7. 
2  Related Work 
A  number  of  extractive  summarization  studies 
used  unsupervised  methods  with  surface  fea-
tures, linguistic features, and statistical features 
to  guide  sentence  ranking  (Edmundson,  1969; 
McKeown and Radev, 1995; Radev et al., 2004; 
Nekova  et  al.,  2006).  Recently,  graph-based 
ranking  methods  have  been  proposed  for  sen-
tence  ranking  and  scoring,  such  as  LexRank 
(Erkan and Radev, 2004) and TextRank (Mihal-
cea and Tarau, 2004).  

There  are  also  a  variety  of  studies  on  su-
pervised learning methods for sentence ranking 
and selection. Kupiec et al. (1995) developed a 
naive Bayes classifier to decide whether a sen-
tence is worthy to extract. Recently, Conditional 
Random Field (CRF) and Structural SVM have 
been employed for single document summariza-
tion (Shen et al., 2007; Li et al., 2009).  

Besides  ranking  sentences  directly,  there  are 
some approaches that select sentences based on 

concept ranking. Radev et al. (2004) used cen-
troid  words  whose  tf*idf  scores  are  above  a 
threshold. Filatova and Hatzivassiloglou (2004) 
used  atomic  event  as  concept.  Moreover,  sum-
marization  evaluation  metrics  such  as  Basic 
Element (Hovy et al., 2006), ROUGE (Lin and 
Hovy,  2003)  and  Pyramid  (Passonneau  et  al., 
2005)  are  all  counting  the  concept  overlap  be-
tween generated summaries and human-written 
summaries.  

Another important issue existing in extractive 
summarization  is  to  find  an  optimal  sentence 
subset  which  can  cover  diverse  information. 
Maximal  Marginal  Relevance  (MMR)  (Carbo-
nell and Goldstein, 1998) and Diversity Penalty 
(Wan,  2007)  are  most  widely  used  approaches 
to reduce redundancy. The two methods are es-
sentially  based  on  greedy  search.  By  contrast, 
ILP based approaches view summary generation 
as  a  global  optimization  problem.  McDonald 
(2007)  proposed  a  sentence-level  ILP  solution. 
Sauper  and  Barzilay  (2009)  presented  an  ex-
cerpt-level  ILP  method  to  generate  Wikipedia 
articles.  Gillick  and  Favre  (2009)  proposed  a 
concept-level ILP, but they used document fre-
quency to score concepts (bigrams), without any 
learning  process.  Some  recent  studies  (Gillick 
and Favre, 2009; Martins and Smith, 2009) also 
modeled  sentence  selection  and  compression 
jointly using ILP. Our ILP framework proposed 
here is based on these studies. Although various 
selection strategies have been proposed, there is 
no work to systematically compare these strate-
gies yet. 

summarization, 

Learning  to  rank  attracts  much  attention  in 
the  information  retrieval  community  recently. 
Pointwise,  pairwise  and  listwise  learning-to-
rank  approaches  have  been  extensively  studied 
(Liu, 2009). Some of those have been applied to 
document 
as  SVR 
(Ouyang  et  al.,  2007),  classification  SVM 
(Wang et al., 2007), and RankNet (Svore et al., 
2007). Again, there is no work to systematically 
compare  these  ranking  algorithms.  To  the  best 
of  our  knowledge,  this  is  the  first  time  that  a 
listwise 
learning-to-rank  algorithm,  ListNet 
(Cao et al., 2007), is adapted to document sum-
marization  in  this  paper.  Moreover,  pairwise 
and  listwise  learning-to-rank  algorithms  have 
never been used to perform concept ranking for 
extractive summarization.  

such 

527

3  Ranking Sentences or Concepts 
Given a query and a collection of relevant doc-
uments,  an  extractive  summarization  system  is 
required to generate a summary consisting of a 
set  of  text  units  (usually  sentences).  The  first 
problem we need to consider is to determine the 
importance  of  these  sentences  according  to  the 
input query. We approach this ranking problem 
in two ways: the first way is to score sentences 
directly  using  learning-to-rank  algorithms,  and 
thus  the  goal  of  summarization  is  to  select  a 
subset of sentences, considering both relevance 
and  redundancy.  The  second  way  is  to  score 
concepts  within  the  document  collection,  and 
then  the  summarization  task  is  to  select  a  sen-
tence subset that can cover those important con-
cepts  maximally.  The  problem  of  sentence  se-
lection will be described in Section 4.  

Suppose the relevant document collection for 
a query q is Dq. From this collection, we obtain 
a  set  of  sentences  or  concepts  (e.g.,  word  bi-
grams), S={s1,s2,…,sn} or C={c1,c2,…, cn}. Be-
fore  training,  each  si  or  ci  is  associated  with  a 
gold  standard  score,  yi.  A  feature  vector,  xj= 
Φ(sj/cj,q,Dq), is constructed for each sentence or 
concept.  The  learning  algorithm  will  learn  a 
ranking  function  f(xj)  from  a  collection  of 
query-document pairs {(qi,Dqi)|i= 1, 2,…,m}.  

We  investigated  three  learning-to-rank  me-
thods to learn f(xj). The first one is a pointwise 
ranking  algorithm,  support  vector  regression 
(SVR). This algorithm treats sentences (or con-
cepts)  independently.  The  second  method  is  a 
pairwise  ranking  algorithm,  RankNet,  which 
learns a ranking function from a list of sentence 
(or  concept)  pairs.  Each  pair  is  labeled  as  1  if 
the first sentence si (or concept ci) ranks ahead 
of the second sj (or cj), and 0 otherwise. 

listwise  ranking  algorithm,  ListNet, 
learns  the  ranking  function  f(xj)  in  a  different 
way. A list of sentences (or concepts) is treated 
as a whole. Both RankNet and ListNet take into 
account  the  dependency  between  sentences  (or 
concepts). 
3.1 
Support Vector Regression (SVR), a generaliza-
tion of the classical SVM formulation, attempts 
to learn a regression model. SVR has been ap-
plied to summarization in (Ouyang et al., 2007; 
Metzler  and  Kanungo,  2008).  In  our  work,  we 

 Support Vector Regression  

The 

train  the  SVR  model  to  fit  the  gold  standard 
score of each sentence or concept.  

Formally, the objective of SVR is to minim-

ize the following objective: 

ξ
) =

w b
,
,
1
2

w

ℑ(
⎧
⎪
⎨
⎪
⎩

||

+

||
2

⎛
C v
⎜
⎝

⋅ +
ξ

1
N

∑

x
i

L y
(
i

−

f x
(
i

))

(1) 

⎞
⎟
⎠

⎫
⎪
⎬
⎪
⎭

where L(x)=|x|-ξif x > ξand otherwise L(x)=0; 
yi is the gold standard score of xi; f(x) =wTx+b, 
the predicted score of x; C and v are two para-
meters;  and  N  is  the  total  number  of  training 
examples.  
3.2  RankNet  
RankNet  is  a  pairwise  learning-to-rank  method 
(Burges et al., 2005). In this algorithm, training 
examples are handled pair by pair. Given a pair 
of  feature  vectors  (xi,  xj),  the  gold  standard 
ijP is  set  to  be  1  if  the  label  of  the 
probability 
pair is 1, which means xi ranks ahead of xj. The 
gold standard probability is 0 if the label of the 
pair  is  0.  Then  the  predicted  probability  Pij, 
which  defines  the  probability  of  xi  ranking 
ahead of xj by the model, is represented as a lo-
gistic function:  
=

            (2) 

exp(
+
1 exp(

f x
(
)
i
f x
(
i

−
)

f x
(
))
j
−
f x
))
(

j

P
ij

where f(x) is the ranking function. The objective 
of the algorithm is to minimize the cross entro-
py  between  the  gold  standard  probability  and 
the  predicted  probability,  which  is  defined  as 
follows: 

= −

)

C f
(
ij

    (3) 
A  three-layer  neural  network  is  used  as  the 

)log(1

log

P
ij

P
ij

P
ij

P
ij

)

− −
(1

−

ranking function, as follows:  
w x
21
jk
nk

w g
32
ij

f x
(
n

=

g

)

(

(

3

2

∑

∑

j

k

+

b

2
j

)

+

b
3
i

)

 (4) 

 

where for weights w and bias b, the superscripts 
indicate the node layer while the subscripts in-
dicate the node indexes within each layer. And 
xnk is the k-th component of input feature vector 
xn.  Then  a  gradient  descent  method  is  used  to 
learn  the  parameters.  For  details,  refer  to 
(Burges et al., 2005). 
3.3  ListNet 
ListNet takes a list of items as input in the learn-
ing process. More specifically, suppose we have 

528

a  list  of  feature  vectors  (x1, x2,…, xn) and  each 
feature  vector  xi  has  an  gold  standard  score  yi, 
which  has  been  assigned  before  training.  Ac-
cordingly, we have a list of gold standard scores 
(y1,  y2,…,yn).  We  also  have  a  list  of  scores  as-
signed  by  the  algorithm  during  training,  say, 
(f(x1), 
list 
S={s1,s2,…,sn}, the probability that xj will rank 
the first place among the n items is defined as 
follows: 

f(xn)).  Given  a  score 

f(x2),…, 

P j
( )
s

=

s
(
Φ∑

Φ
n
k

=

1

j

)
s
(

=

)

k

s

exp(
n
k

)
j
s
exp(

=

1

        (5) 

)

k

∑

It is easy to prove that (Ps(1), Ps(2), …, Ps(n)) is 
a  probability  distribution,  as  the  sum  of  them 
equals to 1. Therefore, the cross entropy can be 
used  to  define  the  loss  between  the  gold  stan-
dard distribution Py(j) and the distribution Pf(j), 
as follows:  

L y f
( ,

)

n

= −∑

=
1

j

P j
y

( )log

P j
( )
f

              (6) 

where  y  represents  the  gold  standard  score  list  
(y1,  y2,…,yn)  and  f=(f(x1),  f(x2),…,  f(xn))  is  the 
score list output by the ranking algorithm.  

The function f is defined as a linear function, 

as follows: 

=

w x
T
i

f

(

x
i

)

w

                         (7) 
Then  the  gradient  of  loss  function  L(y,f)  with 
respect to the parameter vector w can be calcu-
lated as follows:  
)

∂

∂

)

f

n

L y f
( ,
∂
w

w

= −

∑

=
1

j

P x
(
y

j

)

j

x
(
w
∂
w

Δ =
w

 (8) 

+

∑

n
j

=
1

1
exp(

f

w

(

x

j

))

n

∑

=
j
1
 

exp(

f

(

x

j

))

w

∂

f

)

j

x
(
w
∂
w

During training, w is updated in a gradient des-
cent  manner:  w=w  -η∆w  and  η  is  the  learning 
rate. For details, refer to (Cao et al., 2007). 
4 
ILP-based Selection Framework 
After  we  have  a  way  of  ranking  sentences  or 
concepts, we face a sentence selection problem: 
selecting  an  optimal  subset  of  sentences  as  the 
final  summary.  To  integrate  sentence/concept 
ranking, we adopted an integer linear program-
ming (ILP) framework to find the optimal sen-
tence  subset  (Filatova  and  Hatzivassiloglou, 
2004; McDonald, 2007; Gillick and Favre, 2009; 
Takamura and Okumura, 2009). ILP is a global 

optimization problem whose objective and con-
straints are linear in a set of integer variables.  

Formally, we define the problem of sentence 

selection as follows: 

maximize:  

f x
(
i

)*

∑

i

*|

≤

|

Lim

⎧
⎨
⎩
u

j

(9) 

z
x
i

 

⎫
⎬
⎭
       

         
 

u
j

u
j

z

       

∑
s t
. .     
j
∑
z
j
+
z
z
)*
x
x
i
j
z
z
,
        
x
i

      (

I i
j
* ( , )

≥

z
x
i

,   

∀
i

       

 

i

)

sim x x
,
∈
u
j

(
{0,1},      

< ∀
jδ
i
  
,
∀  
i
j
,

j

   

 

where: 
xi – the representation unit, such as  a  sentence 
or a concept. We term it representation unit be-
cause the summary quality is represented by the 
set of included xi; 
f(xi)  -  the  ranking  function  given  by  the  learn-
ing-to-rank algorithms; 
uj - the selection unit, for instance, a sentence in 
this paper. |uj| is the number of words in uj; 
iz -  the  indicator  variable  which  denotes  the 
x
presence or absence of xi in the summary; 
jz - the indicator variable which denotes inclu-
u
sion or exclusion of uj; 
I(i, j) - a  binary constant indicating that wheth-
er xi appears in uj. It is either 1 or 0; 
Lim - the length limit; 
sim(xi, xj) - a similarity measure for considering 
the redundancy; 
δ - the redundancy threshold.  

The first constraint indicates the length limit. 
The second constraint asserts that if a represen-
tation unit xi is included in a summary, at least 
one  selection  unit  that  contains  xi  must  be  se-
lected. The third constraint considers redundan-
cy.  If  the  representation  unit  is  sentence,  the 
similarity measure is defined as tf*idf similarity, 
and δ/2is  the  similarity  threshold,  which  was 
set  to  be  1  here.  For  concepts,  the  similarity 
measure can be defined as  
x
1,    
i
0,    otherwise

sim x x
,

=

x

(

)

 

j

i

j

⎧= ⎨
⎩

However,  other  definition  is  also  feasible,  de-
pending on what has been selected as represen-
tation unit. 

.

529

Note  that  this  framework  is  very  general.  If 
the representation unit xi is a sentence, the rank-
ing  function  is  defined  on  sentence.  Thus  the 
ILP framework will find a set of sentences that 
can  optimize  the  total  scores  of  selected  sen-
tences,  subject  to  several  constraints.  If  the  re-
presentation unit is a concept, the ranking func-
tion measures the importance of a concept to be 
included in a summary. Thus the goal of ILP is 
to  find  a  set  of  sentences  by  maximizing  the 
scores  of  concepts  covered  by  those  selected 
sentences. 

 
Dq 

d 
wi 
wiwi+1 
S 
tfd(wi) 
dfD(wi) 

relevant document collection in response 
to query q 
one single document 
unigram 
bigram 
sentence 
the frequency of wi occurring in d 
the number of documents containing wi 
in collection D 

Table 1. Notations for features. 

5  Features 
To  facilitate  the  following  description,  some 
notations are defined in Table 1. In our dataset, 
each query has a title and narrative to precisely 
define an information need. The following is a 
query example from the TAC 2008 test dataset:  
<topic id = "D0801A"> 
 
  <title> Airbus A380 </title> 
  <narrative> 

Describe  developments  in  the  production  and 
launch of the Airbus A380. 

  </narrative> 
</topic> 

Features  for  sentence  ranking  and  concept 
ranking are listed in the following. We use word 
bigrams as concept here. 
Sentence Features 
(1) Cluster frequency: 

)
i
 where d is a 
(2) Title frequency: 
new  document  that  consists  of  all  the  titles  of 
documents in Dq.  
 where d is 
(3) Query frequency: 
a document consisting of the title and narrative 
fields of the current topic.  

∈∑
w S tf w
(
D
q
∈∑
w S tf w
)
(

w S tf w
(

∈∑

)

 

d

d

i

i

i

i

i

∑

i

i

i

i

i

i

i

1

+
1

D

)

(

)

(

(

+
1

D
q

.  

w w
i
i

∈ ∧ ∈

,  the  fre-

frequency: 

∈∑

tf w w +
d
i
1

S df w w

w S w T tf w
(

tf w w +
qD
i
1
iw w + occurring in Dq.  
i
)

)
 
(4)  Theme 
where T is the top 10% frequent unigram words 
in Dq. 
(5) Document frequency of bigrams in the sen-
tence: 
(6)  PageRank  score:  as  described  in  (Mihalcea 
and Tarau, 2004), each sentence in Dq is a node 
in the graph and the cosine similarity between a 
pair of sentences is used as edge weight. 
Concept Features 
(1)  Cluster  frequency: 
quency of 
,  where  d  is  a 
(2)  Title  frequency: 
document  consisting  of  all  the  titles  of  docu-
ments in Dq. 
(3)  Query  Frequency:  the  frequency  of  the  bi-
gram occurring in the topic title and narrative. 
(4) Average term frequency: 
∈∑
)/ |
 
documents in the set. 
(5)  Document  frequency:  the  document  fre-
quency of this bigram. 
(6)  Minimal  position:  the  minimal  position  of 
this bigram relative to the document length.  
(7)  Average  position:  the  average  position  of 
this bigram in collection Dq . 
6  Experimental Results 
6.1  Data Preprocessing 
We  conducted  experiments  on  the  TAC  2008 
and TAC 2009 datasets. The task requires pro-
ducing a 100-word summary for each query (al-
so termed topic sometimes). There are 48 que-
ries in TAC 2008 and 44 queries in TAC 2009. 
A  query  example  has  been  given  in  Section  5. 
Relevant documents for these queries have been 
specified.  And  four  human-written  summaries 
were supplied as reference summaries for each 
query. 

. |Dq| is the number of 

d D tf w w

D
q

+
1

(

|

d

i

i

q

We  segmented  the  relevant  documents  into 
sentences  using  the  LingPipe  toolkit 1  and 
stemmed  words  using  the  Porter  Stemmer. 
Word bigrams are used as concepts in this paper. 
If  the  two  words  in  a  bigram  are  both  stop-
words,  the  bigram  will  be  discarded.  The  sen-
                                                 
1 http://alias-i.com/lingpipe/index.html 

530

tence features and bigram features are then cal-
culated. As our focus is on comparing different 
ranking  strategies  and  selection  strategies,  we 
did not apply any sophisticated linguistic or se-
mantic  processing  techniques  (as  pre-  or  post-
processing).  Thus  we  did  not  compare  our  re-
sults to those submitted to the TAC conferences.  
We train the learning algorithms on one data-
set and then evaluate the algorithms on the other. 
The  generated  summaries  are  evaluated  using 
the ROUGE toolkit (Lin and Hovy, 2003).  
6.2  Preparing Training Samples 
As our work includes both sentence ranking and 
concept ranking, we need to establish two types 
of training data. Fortunately, we are able to do 
this  based  on  the  reference  summaries  and  an-
notation  results  provided  by  the  TAC  confe-
rences.  

to 

For  the  sentence  ranking  problem,  we  com-
pute the average ROUGE-1 score for each sen-
tence  by  comparing  it  to  the  four  reference 
summaries for each query. This score is treated 
as  the  gold-standard  score.  In  ListNet,  these 
scores are directly used (see formula (5)). While 
in  RankNet,  the  sentences  for  a  query  are 
grouped 
their 
ROUGE-1 scores, and then we extract sentences 
from different bins respectively to form a pair. 
We  assume  that  a  sentence  in  a  higher  scored 
bin  should  rank  ahead  of  those  sentences  in 
lower scored bins.  

into  10  bins  according 

As  for  the  concept  ranking  problem,  gold-
standard  scores  are  obtained  from  the  human 
annotated  Pyramid  data.  The  weight  of  each 
semantic  content  unit  (SCU)  is  the  number  of 
reference summaries in which the SCU appears. 
So straightforwardly, the gold-standard score of 
a bigram is the largest weight of all SCUs that 
contain  the  bigram.  And  if  a  bigram  does  not 
occur in any SCU, its score will be 0. Thus the 
bigram  scores  belong  to  the  set  {0,1,2,3,4}  as 
there  are  four  human-written  summaries  for 
each  query.  These  scores  are  directly  used  in 
ListNet (see formula (5)). And in RankNet, bi-
gram  pairs  are  constructed  according  to  the 
gold-standard scores.  
6.3  Learning Parameters 
For SVR, the radial basis kernel function is em-
ployed and the optimal values for parameters C, 
v and g (for the kernel) are found using the gri-

dregression.py 
tool  provided  by  LibSVM 
(Chang and Lin, 2001) with a 5-fold cross vali-
dation on the training set.  

RankNet  applies  a  three-layer  (one  hidden 
layer) neural network with only one node in the 
output  layer,  as  described  in  (Burges  et  al., 
2005). The number of hidden neurons was em-
pirically set to be 10. The learning rate was set 
to  0.001  for  sentence  ranking  and  0.01  for  bi-
gram ranking.  

As for ListNet, the learning rate for sentence 
ranking and concept ranking are both set to be 
0.1 empirically.  
6.4  Comparing Ranking Strategies 
In  this  section,  we  compared  different  ranking 
strategies for both sentence ranking and concept 
ranking. The sentence selection strategies were 
fixed to the ILP selection framework as shown 
in  Section  4.  We  chose  ILP  as  the  selection 
strategy  because  we  want  to  compare  our  sys-
tem  with  the  following  two  methods  (as  base-
lines): 
(1)  SENT_ILP:  A  sentence-level  method  pro-
posed  by  McDonald  (2007)  with  ILP  formula-
tion.  We  implemented  the  query-focused  ver-
sion  of  the  formulae  as  TAC  2008  and  2009 
required query-focused summarization. 
(2) DF_ILP: A concept-level ILP method using 
document  frequency  to  score  word  bigrams 
(Gillick and Favre, 2009), without any learning 
process.  

The  differences  between  our  framework  and 
SENT_ILP are: a) SENT_ILP used a redundan-
cy  factor  in  the  objective  function  whereas  we 
modeled 
b) 
SENT_ILP  used  tf*idf  similarity  to  compute 
relevance scores whereas we used learning algo-
rithms.  

redundancy 

constraints; 

as 

The  ROUGE-1  and  ROUGE-2  measures  for 
each method are presented in Table 2 and Table 
3. Note that the performance on the TAC 2008 
dataset was obtained from the models that were 
trained on the TAC 2009 dataset. Then, the da-
tasets were interchanged for training and testing, 
respectively.  Different  learning-to-rank  strate-
gies (SVR, RankNet, ListNet) do not show sig-
nificant  differences  between  one  and  another, 
but they all outperform SENT_ILP substantially 
(p-value  <  0.0001).  And  for  concept  ranking, 
RankNet and ListNet both achieve significantly 
better ROUGE-2 results (p-value < 0.005) than 

531

DF_ILP. This infers that considering more fea-
tures  will  have  better  results  than  using  docu-
ment frequency to score concepts. The Wilcox-
on signed-rank test (Wilcoxon, 1945) is used for 
significance  tests  in  our  experiment.  A  good 
ranking  strategy  for  modeling  relevance  is  im-
portant  for  extractive  summarization.  RankNet 
which  used  a  three-layer  network  (non-linear 
function)  as  the  ranking  function  performs 
slightly better than ListNet which is based on a 
linear ranking function.  

 
Dataset 

TAC 
2008 

TAC 
2009 

Method 
SVR 
RankNet 
ListNet 
SENT_ILP
SVR 
RankNet 
ListNet 
SENT_ILP

ROUGE-1  ROUGE-2
0.35086 
0.36025 
0.35365 
0.31546 
0.36125 
0.36216 
0.35480 
0.31962 

0.08447 
0.09291 
0.09129 
0.06500 
0.09659 
0.09778 
0.09126 
0.07034 

Table 2. Results of sentence ranking strategies. 

Dataset 

TAC 2008 

TAC 2009 

Method  ROUGE-1  ROUGE-2
SVR 
0.36555 
RankNet  0.37564 
0.36863 
ListNet 
0.36922 
DF_ILP 
SVR 
0.37126 
RankNet  0.37513 
0.37499 
ListNet 
DF_ILP 
0.36347 

0.10291 
0.11213 
0.10660 
0.10373 
0.10698 
0.11364 
0.11313 
0.10156 

Table 3. Results of concept ranking strategies. 

 

 

It  is  worth  noting  that  Pyramid  annotations 
may not cover all important bigrams, partly be-
cause  SCUs  in  reference  summaries  have  been 
rephrased  by  human  annotators.  Note  that  we 
simply  extract  original  sentences  to  form  a 
summary, thus it is possible that a bigram which 
is  important  in  the  original  sentences  does  not 
appear  in  any  rephrased  SCUs  at  all.  Such  bi-
grams  will  have  a  gold-standard  score  of  0, 
which  is  erroneous  supervision.  For  example, 
the bigrams hurricane katrina in topic D0804A 
about  Katrina  pet  rescue  and  life  support  in 
D0806A about Terri Schiavo case are not anno-
tated  in  any  SCUs,  but  these  bigrams  are  both 
key terms for the topics.  
6.5  Comparing Selection Strategies 
In order to study the influence of different selec-
tion  strategies,  we  compare  the  ILP  selection 

1

)

=

(1

λ

{

− −

D q s
( ,

arg max
∈ −
s R S
i

in  (Carbonell  and 

strategy (as introduced in Section 4) with other 
popular selection strategies, based on the same 
sentence ranking algorithm (we chose sentence-
level  RankNet).  The  baselines  to  be  compared 
are as follows:  
(1)  MMR:  As  shown 
Goldstein, 1998), the formula of MMR is: 
MMR
D s s
,
where  q  is  the  given  query;  R  is  the  set  of  all 
sentences; S is the set of already included sen-
tences; D1 is the normalized ranking score f(xi) 
of si, and D2 is the cosine similarity of the fea-
ture  vectors  for  si   and  sj.  Our  implementation 
was  similar  to  the  MMR  strategy  in  the 
MEAD2summarizer. 
(2)  DiP:  Diversity  penalty  which  penalizes  the 
score  of  candidate  sentences  according  to  the 
already selected ones (Wan, 2007). 

) max
∈
S

}

λ

 

(

)

2

s

i

i

j

j

 
Dataset 

TAC 2008

Method ROUGE-1  ROUGE-2
ILP 
MMR 
DiP 
ILP 
MMR 
DiP 

0.09291 
0.09086 
0.08689 
0.09778 
0.08881 
0.08672 
Table 4. Comparing selection strategies. 

0.36025 
0.35459 
0.35263 
0.36216 
0.35148 
0.34714 

TAC 2009

 

The  corresponding  ROUGE  scores  are  pre-
sented in Table 4. ILP outperforms other selec-
tion  strategies  significantly  on  the  TAC  2009 
dataset  (both  ILP  vs.  MMR  and  ILP  vs.  DiP). 
Although improvements are observed with ILP 
on the TAC 2008 dataset, the difference is not 
significant (using ILP vs. using MMR). MMR is 
comparable  to  DiP  as  they  are  both  based  on 
greedy search in nature.  

To  investigate  the  difference  between  these 
strategies,  we  present  in-depth  analysis  here. 
First, the average length of summaries generat-
ed by ILP is 97.1, while that by MMR and DiP 
are  95.5  and  92.7,  respectively.  Note  that  the 
required  summary  length  is  100  and  that  more 
words  can  potentially  cover  more  information. 
Thus,  ILP  can  generate  summaries  with  more 
information. This is because ILP is a global op-
timization algorithm, subject to the length con-
straint.  Second,  the  average  rank  of  sentences 
selected by ILP is 12.6, while that by MMR and 
                                                 
2 http://www.summarization.com/mead/ 

532

DiP is about 5, which is substantially different. 
ILP  can  search  down  the  ranked  list  while  the 
other two methods tend to only select the very 
top sentences. Third, there are 4.1 sentences on 
average in each ILP-generated summary, while 
the  number  for  MMR  and  DiP  generated  sum-
maries  are  2.7  and  2.5,  respectively.  Thus  ILP 
tend to select shorter sentences than MMR and 
DiP. This may help reduce redundancy as long-
er  sentences  may  contain more  topic  irrelevant 
clauses or phrases.  
6.6  Discussions 
Interestingly,  although  the  learning-to-rank  al-
gorithms combined with the ILP selection strat-
egy perform well in summarization, the perfor-
mance is still far from that of manual summari-
zation.  In  this  study,  we  investigate  the  upper 
bound performance. We used the presented ILP 
framework to generate summaries based on the 
gold-standard scores, rather than the scores giv-
en  by  the  learning  algorithms.  In  other  words, 
f(xi)  in  formula  (9)  is  replaced  by  the  gold-
standard scores. The ROUGE results are shown 
in Table 5. We also listed the best/worst/average 
ROUGE scores of human summaries in TAC by 
comparing  one  human  summary  (as  generated 
summary)  to  the  other  three  human  summaries 
(as reference summaries). These results are sub-
stantially better than those by the learning algo-
rithms.  Sentence-  and  concept-  level  ranking 
produces very close results to best human sum-
maries. Some ROUGE-2 scores are even higher 
than those of human summaries. This is reason-
able as human annotators may have difficulty in 
organizing  content  when  there  are  many  docu-
ments  and  sentences.  The  results  reflect  that 
there  is  a  remarkable  gap  between  the  gold-
standard scores and the learned scores.  

 

TAC 
2008 

TAC 
2009 

Dataset  Method 

Sentence-level
Concept-level 
Human Best 
Human Average
Human Worst 
Sentence-level
Concept-level 
Human Best 
Human Average
Human Worst 

ROUGE-1  ROUGE-2
0.14842 
0.44216 
0.16018 
0.42222 
0.44220 
0.13079 
0.41417 
0.11606 
0.10736 
0.38005 
0.15565 
0.45500 
0.17118 
0.43526 
0.45663 
0.14864 
0.12680 
0.44443 
0.39652 
0.11109 

Table 5. Upper bound performance. 

7  Conclusion and Future Work 
We presented systematic and extensive analysis 
on  studying  two  key  problems  in  extractive 
summarization:  the  ranking  problem  and  the 
selection problem. We compared three genres of 
learning-to-rank  algorithms  for 
the  ranking 
problem,  and  investigated  ILP,  MMR,  and  Di-
versity Penalty strategies for the selection prob-
lem.  To  the  best  of  our  knowledge,  this  is  the 
first  work  of  presenting  systematic  comparison 
and  analysis  on  studying  these  problems.  We 
also at the first time proposed to use learning-to-
rank algorithms to perform concept ranking for 
extractive summarization.  

Our future work will focus on: (1) exploiting 
more features that can reflect summary quality; 
(2)  optimizing  summarization  evaluation  me-
trics directly with new learning algorithms. 

Acknowledgments 
This work was partly supported by the Chinese 
Natural  Science  Foundation  under  grant  No. 
60973104 and No. 60803075, and with the aid 
of  a  grant  from  the  International  Development 
Research  Center,  Ottawa,  Canada  IRCI  project 
from the International Development.  

References 
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, 
Matt Deeds, Nicole Hamilton and Greg Hullender. 
2005. Learning to Rank Using Gradient Descent. 
In  Proceedings  of  the  22nd  International  Confe-
rence on Machine Learning.  

Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai and 
Hang Li. 2007. Learning to Rank: from Pairwise 
Approach  to  Listwise  Approach.  In  Proceedings 
of ICML 2007.  

Jaime Carbonell and Jade Goldstein. 1998. The Use 
of MMR, Diversity-Based Reranking for Reorder-
ing  Documents  and  Producing  Summaries.  In 
Proceedings of SIGIR, August 1998, pp. 335 - 336.  
Chih-Chung  Chang  and  Chih-Jen  Lin.  2001. 
LIBSVM: a Library for Support Vector Machines. 
Software 
at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 

available 

H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) Archive,  
Volume 16, Issue 2 (April 1969) Pages: 264 - 285.  
G.  Erkan  and  Dragomir  R.  Radev.  2004.  LexPage-
Rank:  Prestige  in  Multi-Document  Text  Summa-

533

rization. In Proceedings of EMNLP 2004, Barce-
lona, Spain.  

Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based  Extractive  Summarization.  In  Pro-
ceedings  of  ACL  Workshop  on  Summarization, 
volume 111. 

Dan  Gillick  and  Benoit  Favre.  2009.  A  Scalable 
Global Model for Summarization. In Proceedings 
of the Workshop on Integer Linear Programming 
for Natural Language Processing.  

Eduard Hovy, Chin-yew Lin, Liang Zhou and Juni-
chi  Fukumoto.  2006.  Automated  Summarization 
Evaluation  with  Basic  Elements.  In  Proceedings 
of  the  Fifth  Conference  on  Language  Resources 
and Evaluation.  

Julian  Kupiec,  Jan  Pedersen  and  Francine  Chen. 
1995.  A  Trainable  Document  Summarizer.  In 
Proceedings  of  SIGIR'95,  pages  68  -  73,  New 
York, USA.  

Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha 
and Yong Yu. 2009. Enhancing Diversity, Cover-
age  and  Balance  for  Summarization  through 
Structure Learning. In Proceedings of the 18th In-
ternational Conference on World Wide Web.  

Chin-Yew  Lin  and  Eduard  Hovy.  2003.  Automatic 
Evaluation  of  Summaries  Using  N-gram  Co-
Occurrence  Statistics.  In  Proceedings  of  HLT-
NAACL, pages 71-78. 

Tie-Yan  Liu.  2009.  Learning  to  Rank  for  Informa-
tion  Retrieval,  Foundation  and  Trends  on  Infor-
mation Retrieval.  Now Publishers.  

H.P. Luhn. 1958. The Automatic Creation of Litera-
ture  Abstracts.  In  IBM  Journal  of  Research  and 
Development,  Vol.  2,  No.  2,  pp.  159-165,  April 
1958. 

André  F.  T.  Martins  and  Noah  A.  Smith.  2009. 
Summarization  with  a  Joint  Model  for  Sentence 
Extraction  and  Compression.  In  Proceedings  of 
the Workshop on Integer Linear Programming for 
Natural Langauge Processing\. 

Ryan McDonald. 2007. A Study of Global Inference 
Algorithms in Multi-Document Summarization. In 
Proceedings of the 29th ECIR.  

Kathleen McKeown and Dragomir R. Radev. 1995. 
Generating Summaries of Multiple News Articles. 
In Proceedings of SIGIR'95, pages 74–82.  

Donald Metzler and Tapas Kanungo. 2008. Machine 
Learned Sentence Selection Strategies for Query-
Biased  Summarization.  SIGIR  Learning  to  Rank 
Workshop.  

Rada  Mihalcea  and  Paul  Tarau.  2004.  TextRank: 
Bringing  Order  into  Texts.  In  Proceedings  of 
EMNLP 2004, Barcelona, Spain, July 2004.  

Ani  Nenkova,  Lucy  Vanderwende  and  Kathleen 
McKeown. 2006. A Compositional Context Sensi-
tive  Multi-document  Summarizer:  Exploring  the 
Factors  that  Influence  Summarization.  In  Pro-
ceedings of SIGIR 2006.  

You Ouyang, Sujian Li, Wenjie Li. 2007. Develop-
ing  Learning  Strategies  for  Topic-based  Summa-
rization.  In  Proceedings  of  the  sixteenth  ACM 
Conference on Information and Knowledge Man-
agement, 2007. 

Rebecca  J.  Passonneau,  Ani  Nenkova,  Kathleen 
McKeown and Sergey Sigelman. 2005. Applying 
the  Pyramid  Method  in  DUC  2005.  DUC  2005 
Workshop.  

Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, 
and Daniel Tam. 2004. Centroid-based Summari-
Information 
zation  of  Multiple  Documents. 
Processing and Management, 40:919–938.  

Christina  Sauper  and  Regina  Barzilay.  2009.  Auto-
matically Generating Wikipedia Articles: A Struc-
ture-Aware  Approach.  In  Proceedings  of  ACL 
2009.  

Dou  Shen,  Jian-Tao  Sun,  Hua  Li,  QiangYang  and 
Zheng Chen. 2007. Document Summarization Us-
ing  Conditional  Random  Fields.  In  IJCAI,  pages 
2862 - 2867, 2007.  

Krysta Svore, Lucy Vanderwende, and Chris Burges. 
2007.  Enhancing  Single-Document  Summariza-
tion  by  Combining  RankNet  and  Third-Party 
In  Proceedings  of  EMNLP-CoNLL 
Sources. 
(2007), pp. 448-457.. 

Hiroya  Takamura  and  Manabu  Okumura.  Text 
Summarization  Model  Based  on  Maximum  Cov-
erage  Problem  and  its  Variant.  In  Proceedings 
EACL, 2009.  

Xiaojun  Wan  and  Jianguo  Xiao.  2007.  Towards  a 
Unified  Approach  Based  on  Affinity  Graph  to 
Various  Multi-document  Summarizations.  ECDL 
2007, 297-308.  

Changhu  Wang,  Feng  Jing,  Lei  Zhang  and  Hong-
Jiang  Zhang.  2007.  Learning  Query-Biased  Web 
Page  Summarization.  In  Proceedings  of  the  six-
teenth  ACM  Conference  on  Information  and 
Knowledge Management.  

Frank  Wilcoxon.  1945.  Individual  comparisons  by 

ranking methods. Biometrics, 1, 80-83. 

