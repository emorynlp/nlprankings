










































The Effects of Discourse Connectives Prediction on Implicit Discourse Relation Recognition


Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 139–146,
The University of Tokyo, September 24-25, 2010. c©2010 Association for Computational Linguistics

The Effects of Discourse Connectives Prediction on Implicit Discourse
Relation Recognition

Zhi Min Zhou†, Man Lan†,§, Zheng Yu Niu‡, Yu Xu†, Jian Su§
†East China Normal University, Shanghai, PRC.

‡Baidu.com Inc., Beijing, PRC.
§Institute for Infocomm Research, Singapore.

51091201052@ecnu.cn, lanman.sg@gmail.com

Abstract

Implicit discourse relation recognition is
difficult due to the absence of explicit
discourse connectives between arbitrary
spans of text. In this paper, we use lan-
guage models to predict the discourse con-
nectives between the arguments pair. We
present two methods to apply the pre-
dicted connectives to implicit discourse
relation recognition. One is to use the
sense frequency of the specific connec-
tives in a supervised framework. The
other is to directly use the presence of the
predicted connectives in an unsupervised
way. Results on PDTB2 show that using
language model to predict the connectives
can achieve comparable F-scores to the
previous state-of-art method. Our method
is quite promising in that not only it has
a very small number of features but also
once a language model based on other re-
sources is trained it can be more adaptive
to other languages and domains.

1 Introduction

Discourse relation analysis involves identifying
the discourse relations (e.g., the comparison re-
lation) between arbitrary spans of text, where
the discourse connectives (e.g., “however”, “be-
cause”) may or may not explicitly exist in the text.
This analysis is one important application both as
an end in itself and as an intermediate step in var-
ious downstream NLP applications, such as text
summarization, question answering etc.

As discussed in (Pitler and Nenkova., 2009b),
although explicit discourse connectives may have
two types of ambiguity, i.e., one is discourse or
non-discourse usage (“once” can be either a tem-
poral connective or a word meaning “formerly”),
the other is discourse relation sense ambiguity

(“since” can serve as either a temporal or causal
connective), their study shows that for explicit
discourse relations in Penn Discourse Treebank
(PDTB) corpus, the most general 4 senses, i.e.,
Comparison (Comp.), Contingency (Cont.), Tem-
poral (Temp.) and Expansion (Exp.), can be eas-
ily addressed by the presence of discourse con-
nectives and a simple method only considering the
sense frequency of connectives can achieve more
than 93% accuracy. This indicates the importance
of connectives for discourse relation recognition.

However, with implicit discourse relation
recognition, there is no connective between the
textual arguments, which results in a very difficult
task. In recent years, a multitude of efforts have
been employed to solve this task. One approach
is to exploit various linguistically informed fea-
tures extracted from human-annotated corpora in
a supervised framework (Pitler et al., 2009a) and
(Lin et al., 2009). Another approach is to perform
recognition without human-annotated corpora by
creating synthetic examples of implicit relations in
an unsupervised way (Marcu and Echihabi, 2002).

Moreover, our initial study on PDTB implicit
relation data shows that the averaged F-score for
the most general 4 senses can reach 91.8% when
we obtain the sense of test examples by map-
ping each implicit connective to its most frequent
sense (i.e., sense recognition using gold-truth im-
plicit connectives). This high F-score performance
again proves that the connectives are very crucial
source for implicit relation recognition.

In this paper, we present a new method to ad-
dress the problem of recognizing implicit dis-
course relation. This method is inspired by the
above observations, especially the two gold-truth
results, which reveals that discourse connectives
are very important signals for discourse relation
recognition. Our basic idea is to recover the im-
plicit connectives (not present in real text) be-
tween two spans of text with the use of a language

139



model trained on large amount of raw data without
any human-annotation. Then we use these pre-
dicted connectives to generate feature vectors in
two ways for implicit discourse relation recogni-
tion. One is to use the sense frequency of the spe-
cific connectives in a supervised framework. The
other is to directly use the presence of the pre-
dicted connectives in an unsupervised way.

We performed evaluation on explicit and im-
plicit relation data sets in the PDTB 2 corpus. Ex-
perimental results showed that the two methods
achieved comparable F-scores to the state-of-art
methods. It indicates that the method using lan-
guage model to predict connectives is very useful
in solving this task.

The rest of this paper is organized as follows.
Section 2 reviews related work. Section 3 de-
scribes our methods for implicit discourse relation
recognition. Section 4 presents experiments and
results. Section 5 offers some conclusions.

2 Related Work

Existing works on automatic recognition of im-
plicit discourse relations fall into two categories
according to whether the method is supervised or
unsupervised.

Some works perform relation recognition with
supervised methods on human-annotated corpora,
for example, the RST Bank (Carlson et al., 2001)
used by (Soricut and Marcu, 2003), adhoc anno-
tations used by (Girju, 2003) and (Baldridge and
Lascarides, 2005), and the GraphBank (Wolf et al.,
2005) used by (Wellner et al., 2006).

Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al., 2006) has sig-
nificantly expanded the discourse-annotated cor-
pora available to researchers, using a comprehen-
sive scheme for both implicit and explicit rela-
tions. (Pitler et al., 2009a) performed implicit re-
lation classification on the second version of the
PDTB. They used several linguistically informed
features, such as word polarity, verb classes, and
word pairs, showing performance increases over a
random classification baseline. (Lin et al., 2009)
presented an implicit discourse relation classifier
in PDTB with the use of contextual relations, con-
stituent Parse Features, dependency parse features
and cross-argument word pairs. Although both of
two methods achieved the state of the art perfor-
mance for automatical recognition of implicit dis-
course relations, due to lack of human-annotated

corpora, their approaches are not very useful in the
real word.

Another line of research is to use the unsuper-
vised methods on unhuman-annotated corpus.

(Marcu and Echihabi, 2002) used several pat-
terns to extract instances of discourse relations
such as contrast and elaboration from unlabeled
corpora. Then they used word-pairs between argu-
ments as features for building classification mod-
els and tested their model on artificial data for im-
plicit relations.

Subsequently other studies attempt to ex-
tend the work of (Marcu and Echihabi, 2002).
(Sporleder and Lascarides, 2008) discovered that
Marcu and Echihabi’s models do not perform as
well on implicit relations as one might expect
from the test accuracy on synthetic data. (Gold-
ensohn, 2007) extended the work of (Marcu and
Echihabi, 2002) by refining the training and clas-
sification process using parameter optimization,
topic segmentation and syntactic parsing. (Saito
et al., 2006) followed the method of (Marcu and
Echihabi, 2002) and conducted experiments with
a combination of cross-argument word pairs and
phrasal patterns as features to recognize implicit
relations between adjacent sentences in a Japanese
corpus.

Previous work showed that with the use of some
patterns, structures, or the pairs of words, rela-
tion classification can be performed using unsu-
pervised methods.

In contrast to existing work, we investigated a
new knowledge source, i.e., implicit connectives
predicted using a language model, for implicit re-
lation recognition. Moreover, this method can
be applied in both supervised and unsupervised
ways by generating features on labeled and unla-
beled training data and then performing implicit
discourse connectives recognition.

3 Methodology

3.1 Predicting implicit connectives via a
language model

Previous work (Pitler and Nenkova., 2009b)
showed that with the presence of discourse con-
nectives, explicit discourse relations in PDTB can
be easily identified with more than 90% F-score.
Our initial study on PDTB human-annotated im-
plicit relation data shows that the averaged F-score
for the most general 4 senses can reach 91.8%
when we simply map each implicit connective to

140



its most frequent sense. These high F-scores indi-
cate that the connectives are very crucial source of
information for both explicit and implicit relation
recognition. However, for implicit relations, there
are no explicitly discourse connectives in real text.
This built-in absence makes the implicit relation
recognition task quite difficult. In this work we
overcome this difficulty by inserting connectives
into the two arguments with the use of a language
model.

Following the annotation scheme of PDTB, we
assume that each implicit connective takes two
arguments, denoted as Arg1 and Arg2. Typi-
cally, there are two possible positions for most
of implicit connectives, i.e., the position before
Arg1 and the position between Arg1 and Arg2.
Given a set of implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as Ppl(Sci,j). According to
the value of Ppl(Sci,j) (the lower the better), we
can rank these sentences and select the connec-
tives in top N sentences as implicit connectives
for this argument pair. Here the language model
may be trained on any large amount of unanno-
tated corpora that can be cheaply acquired. Typi-
cally, a large corpora with the same domain as the
test data will be used for training language model.
Therefore, we chose news corpora, such as North
American News Corpora.

After that, we use the top N predicted connec-
tives to generate different feature vectors and per-
form the classification in two ways. One is to use
the sense frequency of predicted connectives in a
supervised framework. The other is to directly use
the presence of the predicted connectives in an un-
supervised way. The two approaches are described
as follows.

3.2 Using sense frequency of predicted
discourse connectives as features

After the above procedure, we get a sorted set of
predicted discourse connectives. Due to the pres-
ence of an implicit connective, the implicit dis-
course relation recognition task can be addressed
with the methods for explicit relation recognition,
e.g., sense classification based only on connectives
(Pitler et al., 2009a). Inspired by their work, the
first approach is to use sense frequency of pre-

dicted discourse connectives as features. We take
the connective with the lowest perplexity value
(i.e., top 1 connective) as the real connective for
the arguments pair. Then we count the sense
frequency of this connective on the training set.
Figure 1 illustrates the procedure of generating
predicted discourse connective from a language
model and calculating its sense frequency from
training data. Here the calculation of sense fre-
quency of connective is based on the annotated
training data which has labeled discourse rela-
tions, thus this method is a supervised one.

Figure 1: Procedure of generating a predicted dis-
course connective and its sense frequency from the
training set and a language model.

Then we can directly use the sense frequency
to generate a 4-feature vector to perform the clas-
sification. For example, the sense frequency of
the connective but in the most general 4 senses
can be counted from training set as 691, 6, 49,
2, respectively. For a given pair of arguments,
if but is predicted as the top 1 connective based
on a language model, a 4-dimension feature vec-
tor (691, 6, 49, 2) is generated for this pair and
used for training and test procedure. Figure 2
and 3 show the training and test procedure for this
method.

Figure 2: Training procedure for the first ap-
proach.

141



Figure 3: Test procedure for the first approach.

3.3 Using presence or absence of predicted
discourse connective as features

(Pitler et al., 2008) showed that most connectives
are unambiguous and it is possible to obtain high-
accuracy in prediction of discourse senses due to
the simple mapping relation between connectives
and senses. Given two examples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey the Comparison and Contingency
senses respectively. In most cases, we can easily
recognize the relation sense by the appearance of
a discourse connective since it can be interpreted
in only one way. That means the ambiguity of
the mapping between sense and connective is quite
low. Therefore, the second approach is to use only
the presence of the top N predicted discourse con-
nectives to generate a feature vector for a given
pair of arguments.

4 Experiment

4.1 Data sets

We used PDTB as our data set to perform the eval-
uation of our methods. The corpus contains anno-
tations of explicit and implicit discourse relations.
The first evaluation is performed on the annotated
implicit data set. Following the work of (Pitler et
al., 2009a), we used sections 2-20 as the training
set, sections 21-22 as the test set and sections 0-
1 as the development set for parameter optimiza-
tion (e.g., N value). The second evaluation is per-

formed on the annotated explicit data set. We fol-
low the method used in (Sporleder and Lascarides,
2008) to remove the discourse connective from the
explicit instances and consider these processed in-
stances as implicit ones.

We constructed four binary classifiers to recog-
nize each main senses (i.e., Cont., Cont., Exp.,
Temp.) from the rest. For each sense we used
equal numbers of positive and negative instances
in training set. The negative instances were cho-
sen at random from the rest of training set. For
both evaluations all instances in sections 21-22
were used as test set. Table 1 lists the numbers
of positive and negative instances for each sense
in training, development and test sets of implicit
and explicit relation data sets.

4.2 Evaluation and classifier
To evaluate the performance of above systems, we
used two widely-used measures, F-score ( i.e., F1)
and accuracy. In addition, in this work we used
the LIBSVM toolkit to construct four linear SVM
classifiers for each sense.

4.3 Preprocessing
We used the SRILM toolkit to build a language
model and calculated the perplexity value for each
training and test sample. The steps are described
as follows. First, since perplexity is an intrin-
sic score to measure the similarity between train-
ing and test samples, in order to fit the restric-
tion of perplexity we chose 3 widely-used cor-
pora in the Newswire domain to train the language
model, i.e., (1) the New York part of BLLIP North
American News Text (Complete), (2) the Xin and
(3) the Ltw parts of the English Gigaword Fourth
Edition. For the BLLIP corpus with 1,796,386
automatically parsed English sentences, we con-
verted the parsed sentences into original textual
data. Some punctuation marks such as commas,
periods, minuses, right/left parentheses are con-
verted into their original form. For the Xin and
Ltw parts, we only used the Sentence Detector
toolkit in OpenNLP to split each sentence. Finally
we constructed 3-, 4- and 5-grams language mod-
els from these three corpora. Table 2 lists statis-
tics of different n-grams in the different language
models and different corpora.

Next, for each instance we combined its Arg1
and Arg2 with connectives obtained from PDTB.
There are two types of connectives, single con-
nectives (e.g. “because” and “but”) and paral-

142



Table 1: Statistics of positive and negative instances for each sense in training, development and test sets
of implicit and explicit relation data sets.

Implicit Explicit
Comp. Cont. Exp. Temp. Comp. Cont. Exp. Temp.

Train(Pos/Neg) 1927/1927 3375/3375 6052/6052 730/730 4080/4080 2732/2732 4609/4609 2663/2663
Dev(Pos/Neg) 191/997 292/896 651/537 54/1134 438/1071 295/1214 514/995 262/1247
Test(Pos/Neg) 146/912 276/782 556/502 67/991 388/1025 235/1178 501/912 289/1124

Table 2: Statistics of different n-grams in the dif-
ferent language models and different corpora.

n-gram BLLIP - Gigaword- Gigaword-
New York Xin Ltw

1-gram 1638156 2068538 2276491
2-grams 26156851 23961796 33504873
3-grams 80876435 77799100 101855639
4-grams 127142452 134410879 159791916
5-grams 146454530 168166195 183794771

lel connectives (such as “not only . . . , but also”).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of
possible implicit connectives {ci}, for a single
connective ci, we constructed two synthetic sen-
tences, ci+Arg1+Arg2 and Arg1+ci+Arg2. In case
of parallel connectives, we constructed one syn-
thetic sentence like ci,1+Arg1+ci,2+Arg2.

As a result, we obtain 198 synthetic sentences
(|ci| ∗ 2 for single connective or |ci| for parallel
connective) for each pair of arguments. Then we
converted all words to lower cases and used the
language model trained in the above step to calcu-
late its perplexity (the lower the better) value on
sentence level. The sentences were ranked from
low to high according to their perplexity scores.
For example, given a sentence with arguments pair
as follows:
Arg1: it increased its loan-loss reserves by $93
million after reviewing its loan portfolio,
Arg2: before the loan-loss addition it had operat-
ing profit of $10 million for the quarter.
we got the perplexity (Ppl) values for this argu-
ments pair in combination with two connectives
(but and by comparison) in two positions as fol-
lows:

1. but + Arg1 + Arg2: Ppl= 349.622

2. Arg1 + but + Arg2: Ppl= 399.339

3. by comparison + Arg1 + Arg2: Ppl= 472.206

4. Arg1 + by comparison + Arg2: Ppl= 543.051

In our second approach described in Section
3.3, we considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is,
the presence or absence of the specific connective.
According to the value of Ppl(Sci,j), we tried var-
ious N values on development set to get the opti-
mal N value.

4.4 Results
Table 3 summarizes the best performance
achieved using gold-truth implicit connectives,
the previous state-of-art performance achieved
by (Pitler et al., 2009a) and our approaches.
The first line shows the result by mapping the
gold-truth implicit connectives directly to the
relation’s sense. The second line presents the best
result of (Pitler et al., 2009a). One thing worth
mentioning here is that for the Expansion relation,
(Pitler et al., 2009a) expanded both training and
test sets by including EntRel relation as positive
examples, which makes it impossible to perform
direct comparison. The third and fourth lines
show the best results using our first approach,
where the sense frequency is counted on explicit
and implicit training set respectively. The last line
shows the best result of our second approach only
considering the presence of top N connectives.

Table 4 summarizes the best performance using
gold-truth explicit connectives reported in (Pitler
and Nenkova., 2009b) and our two approaches.

Figure 4 shows the curves of averaged F-scores
on implicit connective classification with differ-
ent n-gram language models. From this figure we
can see that all 4-grams language models achieved
around 0.5% better averaged F-score than 3-grams
models. And except for Ltw corpus, other 5-grams
models achieved lower averaged F-score than 4-
grams models. Specially the 5-grams result of
New York corpus is much lower than its 3-grams
result.

Figure 5 shows the averaged F-scores of dif-
ferent top N on the New York corpus with 3-,
4- and 5-grams language models. The essential

143



Table 3: Best result of implicit relations compared with state-of-art methods.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Averaged

F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using
gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02)
Best result in (Pitler et al., 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75)
Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95)
Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70)
Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74)

Table 4: Best result of explicit relation conversion to implicit relation compared with results using the
same method.

System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Average
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)

Sense recognition using gold-truth
explicit connectives in (Pitler et al., 2009a) N/A N/A N/A N/A N/A(93.67)
Use sense frequency in explicit training set 41.62(50.96) 27.46(59.24) 48.44(50.88) 35.14(54.28) 38.17(53.84)
Use presence of top N connectives only 42.92(55.77) 31.83(56.05) 47.26(55.77) 37.89(58.24) 39.98(56.46)

0 10 20 30 40 50 60 70 80 90 100110120130140150160170180190200
30.0

30.5

31.0

31.5

32.0

32.5

33.0

33.5

34.0

34.5

 

 NY 3-gram
 NY 4-gram
 NY 5-gram

Top N value

A
ve

ra
ge

d 
F-

S
co

re

Figure 5: Curves of averages F-score on New York 3-, 4- and 5-grams language models with different
top N values.

trend of these curves cannot be summarized in
one sentence. But we can see that the best aver-
aged F-scores mostly appeared in the range from
100 − 160. For 4-grams and 5-grams models, the
system achieved the top averaged F-scores when
N = 20 as well.

4.5 Discussion
Experimental results on PDTB showed that using
predicted connectives achieved the comparable F-
scores of the state-of-art method.

From Table 3 we can find that our results are
closely to the best performance of previous state-
of-art methods in terms of averaged F-score. On

the Comparison sense, our first approach has an
improvement of more than 4% F-score on the pre-
vious state-of-art method (Pitler et al., 2009a). As
we mentioned before, for the Expansion sense,
they included EntRel relation to expand the train-
ing set and test set, which makes it impossible to
perform a direct comparison. Since the positive in-
stances size has been increased by 50%, they may
achieve a higher F-score than our approach. For
other relations, our best performance is slightly
lower than theirs. While bearing in mind that our
approach only uses a very small amount of fea-
tures for implicit relation recognition. Compared

144



3-gram 4-gram 5-gram
31.0

31.2

31.4

31.6

31.8

32.0

32.2

32.4

32.6

 
 New York
 Xin
 Ltw

n-gram

A
ve

ra
ge

d 
F-

sc
or

e

Figure 4: Curves of averaged F-score on implicit
connective classification with n-Gram language
model.

with other approaches involving thousands of fea-
tures, our method is quite promising.

From Table 4 we observe comparable averaged
F-score (39.98% F-score) on explicit relation data
set to that on implicit relation data set. Previ-
ously, (Sporleder and Lascarides, 2008) also used
the same conversion method to perform implicit
relation recognition on different corpora and their
best result is around 33.69% F-score. Although
the two results cannot be compared directly due to
different data sets, the magnitude of performance
quantities is comparable and reliable.

By comparing with the above different systems,
we find several useful observations. First, our
method using predicted implicit connectives via a
language model can help the task of implicit dis-
course relation recognition. The results are com-
parable to the previous state-of-art studies. Sec-
ond, our method has a lot of advantages, i.e., a
very small amount of features (several or no more
than 200 vs. ten thousand), easy computation
(only based on the trained language model vs. us-
ing a lot of NLP tools to extract a large amount of
linguistically informed features) and fast running,
which makes it more practical in real world appli-
cation. Furthermore, since the language model can
be trained on many corpora whether annotated or
unannotated, this method is more adaptive to other
languages and domains.

5 Conclusions

In this paper we have presented an approach to
implicit discourse relation recognition using pre-
dicted implicit connectives via a language model.

The predicted connectives have been used for im-
plicit relation recognition in two ways, i.e., super-
vised and unsupervised framework. Results on the
Penn Discourse Treebank 2.0 show that the pre-
dicted discourse connectives can help implicit re-
lation recognition and the two algorithms achieve
comparable F-scores with the state-of-art method.
In addition, this method is quite promising due to
its simple, easy to retrieve, fast run and increased
adaptivity to other languages and domains.

Acknowledgments

We thank the reviewers for their helpful com-
ments and Jonathan Ginzburg for his mentor-
ing. This work is supported by grants from
National Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).

References
J. Baldridge and A. Lascarides. 2005. Probabilistic

head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.

L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.

B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, College
Park, MD,2001.

R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL 2003 Workshops.

S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.

M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.

Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.

D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics.

145



E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A.
Lee, A. Joshi. 2008. Easily Identifiable Dis-
course Relations. Coling 2008: Companion vol-
ume: Posters.

E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics.

E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.

M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.

R. Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. Proceedings of the COLING/ACL Work-
shop on Sentiment and Subjectivity in Text.

R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC’08.

M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.

R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Informa-
tion. Proceedings of the Human Language Technol-
ogy and North American Association for Computa-
tional Linguistics Conference.

C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.

B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.

F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.

146


