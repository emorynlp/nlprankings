



















































Transforming Dependencies into Phrase Structures


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 788–798,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Transforming Dependencies into Phrase Structures

Lingpeng Kong
School of Computer Science
Carnegie Mellon University

Pittsburgh, PA, USA
lingpenk@cs.cmu.edu

Alexander M. Rush
Facebook AI Research
New York, NY, USA

srush@seas.harvard.edu

Noah A. Smith
School of Computer Science
Carnegie Mellon University

Pittsburgh, PA, USA
nasmith@cs.cmu.edu

Abstract

We present a new algorithm for transforming
dependency parse trees into phrase-structure
parse trees. We cast the problem as struc-
tured prediction and learn a statistical model.
Our algorithm is faster than traditional phrase-
structure parsing and achieves 90.4% English
parsing accuracy and 82.4% Chinese parsing
accuracy, near to the state of the art on both
benchmarks.

1 Introduction

Natural language parsers typically produce phrase-
structure (constituent) trees or dependency trees.
These representations capture some of the same syn-
tactic phenomena, and the two can be produced
jointly (Klein and Manning, 2002; Hall and Nivre,
2008; Carreras et al., 2008; Rush et al., 2010). Yet
it appears to be completely unpredictable which will
be preferred by a particular subcommunity or used
in a particular application. Both continue to receive
the attention of parsing researchers.

Further, it appears to be a historical accident
that phrase-structure syntax was used in annotating
the Penn Treebank, and that English dependency
annotations are largely derived through mechani-
cal, rule-based transformations (reviewed in Sec-
tion 2). Indeed, despite extensive work on direct-
to-dependency parsing algorithms (which we call d-
parsing), the most accurate dependency parsers for
English still involve phrase-structure parsing (which
we call c-parsing) followed by rule-based extraction
of dependencies (Kong and Smith, 2014).

What if dependency annotations had come first?
Because d-parsers are generally much faster than

c-parsers, we consider an alternate pipeline (Sec-
tion 3): d-parse first, then transform the depen-
dency representation into a phrase-structure tree
constrained to be consistent with the dependency
parse. This idea was explored by Xia and Palmer
(2001) and Xia et al. (2009) using hand-written
rules. Instead, we present a data-driven algorithm
using the structured prediction framework (Sec-
tion 4). The approach can be understood as a
specially-trained coarse-to-fine decoding algorithm
where a d-parser provides “coarse” structure and the
second stage refines it (Charniak and Johnson, 2005;
Petrov and Klein, 2007).

Our lexicalized phrase-structure parser, PAD, is
asymptotically faster than parsing with a lexical-
ized context-free grammar: O(n2) plus d-parsing,
vs. O(n5) worst case runtime in sentence length
n, with the same grammar constant. Experiments
show that our approach achieves linear observable
runtime, and accuracy similar to state-of-the-art
phrase-structure parsers without reranking or semi-
supervised training (Section 7).

2 Background

We begin with the conventional development by first
introducing c-parsing and then defining d-parses
through a mechanical conversion using head rules.
In the next section, we consider the reverse transfor-
mation.

2.1 CFG Parsing
The phrase-structure trees annotated in the Penn
Treebank are derivation trees from a context-free
grammar. Define a binary1 context-free grammar

1For notational simplicity, we defer discussion of non-
binary rules to Section 3.3.

788



(CFG) as a 4-tuple (N ,G, T , r) where N is a set
of nonterminal symbols (e.g. NP, VP), T is a set
of terminal symbols, consisting of the words in the
language, G is a set of binary rules of the form
A → β1 β2, and r ∈ N is a distinguished root non-
terminal symbol.

Given an input sentence x1, . . . , xn of terminal
symbols from T , define the set of c-parses for the
sentence as Y(x). This set consists of all binary or-
dered trees with fringe x1, . . . , xn, internal nodes la-
beled from N , all tree productions A → β1 β2 con-
sisting of members of G, and root label r.

For a c-parse y ∈ Y(x), we further associate a
span 〈v⇐, v⇒〉 with each vertex in the tree. This
specifies the subsequence {xv⇐ , . . . , xv⇒} of the
sentence covered by this vertex.

2.2 Dependency Parsing

Dependency parses provide an alternative, and in
some sense simpler, representation of sentence
structure. These d-parses can be derived through
mechanical transformation from context-free trees.
There are several popular transformations in wide
use; each provides a different representation of a
sentence’s structure (Collins, 2003; De Marneffe
and Manning, 2008; Yamada and Matsumoto, 2003;
Johansson and Nugues, 2007).

We consider the class of transformations that are
defined through local head rules. For a binary CFG,
define a collection of head rules as a mapping from
each CFG rule to a head preference for its left or
right child. We use the notation A → β∗1 β2 and
A → β1 β∗2 to indicate a left- or right-headed rule,
respectively.

The head rules can be used to map a c-parse to a
dependency tree (d-parse). In a d-parse, each word
in the sentence is assigned as a dependent to a head
word, h ∈ {0, . . . , n}, where 0 is a special symbol
indicating the pseudo-root of the sentence. For each
h we define L(h) ⊂ {1, . . . , h− 1} as the set of left
dependencies of h, and R(h) ⊂ {h + 1, . . . , n} as
the set of right dependencies.

A d-parse can be constructed recursively from a
c-parse and the head rules. For each c-parse vertex
v with potential children vL and vR in bottom-up or-
der, we apply the following procedure to both assign
heads to the c-parse and construct the d-parse:

S(3)

. . .VP(3)

VBD∗(3)

sold3

NP(2)

NN∗(2)

automaker2

DT(1)

The1

The1 automaker2 sold3 . . .

Figure 1: Illustration of c-parse to d-parse conversion with head
rules {VP→ NP VBD∗,NP→ DT NN∗, . . .}. The c-parse is an
ordered tree with fringe x1, . . . , xn. Each vertex is annotated
with a terminal or nonterminal symbol and a derived head index.
The blue and red vertices have the words automaker2 and
sold3 as heads respectively. The vertex VP(3) implies that
automaker2 is a left-dependent of sold3, and that 2 ∈ L(3)
in the d-parse.

1. If the vertex is leaf xm, then head(v) = m.

2. If the next rule is A → β∗1 β2 then head(v) =
head(vL) and head(vR) ∈ R(head(v)), i.e.
the head of the right-child is a dependent of the
head word.

3. If the next rule is A → β1 β∗2 then head(v) =
head(vR) and head(vL) ∈ L(head(v)), i.e.
the head of the left-child is a dependent of the
head word.

Figure 1 shows an example conversion of a c-parse
to d-parse using this procedure.

By construction, these dependencies form a di-
rected tree with arcs (h,m) for all h ∈ {0, . . . , n}
and m ∈ L(h) ∪ R(h). While this tree differs
from the original c-parse, we can relate the two trees
through their spans. Define the dependency tree
span 〈h⇐, h⇒〉 as the contiguous sequence of words
reachable from word h in this tree.2 This span is
equivalent to the maximal span 〈v⇐, v⇒〉 of any c-
parse vertex with head(v) = h. This property will
be important for the parsing algorithm presented in
the next section.

2The conversion from a standard CFG tree to a d-parse pre-
serves this sequence property, known as projectivity. We leave
the question of non-projective d-parses and the related question
of traces and co-indexation in c-parses to future work.

2

789



I1 saw2 the3 man4

X(2)

X(4)

N

man4

D

the3

V

saw2

X(1)

N

I1

X(2)

X(2)

X(4)

N

man4

D

the3

V

saw2

N

I1

X(2)

X(4)

N

man4

D

the3

X(2)

V

saw2

N

I1

Figure 2: [Adapted from (Collins et al., 1999).] A d-parse
(left) and several c-parses consistent with it (right). Our goal is
to select the best parse from this set.

3 Parsing Dependencies

Now we consider flipping this setup. There has been
significant progress in developing efficient direct-
to-dependency parsers. These d-parsers are trained
only on dependency annotations and do not require
full phrase-structure trees.3 Some prefer this setup,
since it allows easy selection of the specific depen-
dencies of interest in a downstream task (e.g., infor-
mation extraction), and perhaps even training specif-
ically for those dependencies. Other applications
make use of phrase structures, so c-parsers enjoy
wide use as well.

With these latter applications in mind, we con-
sider the problem of converting a fixed d-parse
into a c-parse, with the intent of using off-the-shelf
d-parsers for constructing phrase-structure parses.
Since this problem is more challenging than its in-
verse, we use a structured prediction setup: we learn
a function to score possible c-parse conversions, and
then generate the highest-scoring c-parse given a d-
parse. A toy example of the problem is shown in
Figure 2.

3.1 Parsing Algorithm

Consider the classical problem of predicting the best
c-parse under a CFG with head rules, known as lex-
icalized context-free parsing. Assume that we are
given a binary CFG defining a set of valid c-parses
Y(x). The parsing problem is to find the highest-
scoring parse in this set, i.e. arg maxy∈Y(x) s(y;x)

3For English these parsers are still often trained on
trees converted from c-parses; however, for other languages,
dependency-only treebanks of directly-annotated d-parses are
common.

where s is a scoring function that factors over lexi-
calized tree productions.

This problem can be solved by extending the
CKY algorithm to propagate head information. The
algorithm can be compactly defined by the produc-
tions in Figure 3 (left). For example, one type of
production is of the form

(〈i, k〉,m, β1) (〈k + 1, j〉, h, β2)
(〈i, j〉, h, A)

for all rules A → β1 β∗2 ∈ G and spans i ≤ k < j.
This particular production indicates that rule A →
β1 β

∗
2 was applied at a vertex covering 〈i, j〉 to pro-

duce two vertices covering 〈i, k〉 and 〈k+ 1, j〉, and
that the new head is index h has dependent index m.
We say this production “completes” word m since it
can no longer be the head of a larger span.

Running the algorithm consists of bottom-up dy-
namic programming over these productions. How-
ever, applying this version of the CKY algorithm
requires O(n5|G|) time (linear in the number of
productions), which is not practical to run without
heavy pruning. Most lexicalized parsers therefore
make further assumptions on the scoring function
which can lead to asymptotically faster algorithms
(Eisner and Satta, 1999).

Instead, we consider the same objective, but con-
strain the c-parses to be consistent with a given d-
parse, d. By “consistent,” we mean that the c-
parse will be converted by the head rules to this ex-
act d-parse.4 Define the set of consistent c-parses
as Y(x, d) and the constrained search problem as
arg maxy∈Y(x,d) s(y;x, d).

Figure 3 (right) shows the algorithm for this new
problem. The algorithm has several nice proper-
ties. All rules now must select words h and m that
are consistent with the dependency parse (i.e., there
is an arc (h,m)) so these variables are no longer
free. Furthermore, since we have the full d-parse,
we can precompute the dependency span of each
word 〈m⇐,m⇒〉. By our definition of consistency,
this gives us the c-parse span of m before it is com-
pleted, and fixes two more free variables. Finally the
head item must have its alternative side index match

4An alternative, soft version of consistency, might enforce
that the c-parse is close to the d-parse. While this allows the al-
gorithm to potentially correct d-parse mistakes, it is much more
computationally expensive.

3

790



Premise:

(〈i, i〉, i, A) ∀i ∈ {1 . . . n}, A ∈ N

Rules:
For i ≤ h ≤ k < m ≤ j, and rule A→ β∗1 β2,

(〈i, k〉, h, β1) (〈k + 1, j〉,m, β2)
(〈i, j〉, h, A)

For i ≤ m ≤ k < h ≤ j, rule A→ β1 β∗2 ,
(〈i, k〉,m, β1) (〈k + 1, j〉, h, β2)

(〈i, j〉, h, A)

Goal:
(〈1, n〉,m, r) for any m

Premise:

(〈i, i〉, i, A) ∀i ∈ {1 . . . n}, A ∈ N

Rules:
For all h, m ∈ R(h), rule A→ β∗1 β2,

and i ∈ {m′⇐ : m′ ∈ L(h)} ∪ {h},
(〈i,m⇐ − 1〉, h, β1) (〈m⇐,m⇒〉,m, β2)

(〈i,m⇒〉, h, A)

For all h, m ∈ L(h), rule A→ β1 β∗2 ,
and j ∈ {m′⇒ : m′ ∈ R(h)} ∪ {h},

(〈m⇐,m⇒〉,m, β1) (〈m⇒ + 1, j〉, h, β2)
(〈m⇐, j〉, h, A)

Goal:
(〈1, n〉,m, r) for any m ∈ R(0)

Figure 3: The two algorithms written as deductive parsers. Starting from the premise, any valid application of rules that leads to a
goal is a valid parse. Left: lexicalized CKY algorithm for CFG parsing with head rules. For this algorithm there areO(n5|G|) rules
where n is the length of the sentence. Right: the constrained CKY parsing algorithm for Y(x, d). The algorithm is nearly identical
except that many of the free indices are now fixed given the dependency parse. Finding the optimal c-parse with the new algorithm
now requires O

(
(
∑

h |L(h)||R(h)|)|G|
)

time where L(h) andR(h) are the left and right dependents of word h.

a valid dependency span. For example, if for a word
h there are |L(h)| = 3 left dependents, then when
taking the next right-dependent there can only be 4
valid left boundary indices.

The runtime of the final algorithm reduces to
O(

∑
h |L(h)||R(h)||G|). While the terms |L(h)|

and |R(h)| could in theory make the runtime
quadratic, in practice the number of dependents is
almost always constant in the length of the sentence.
This leads to linear observed runtime in practice as
we will show in Section 7.

3.2 Pruning

In addition to constraining the number of c-parses,
the d-parse also provides valuable information about
the labeling and structure of the c-parse. We can use
this information to further prune the search space.
We employ two pruning methods:

Method 1 uses the part-of-speech tag of xh,
tag(h), to limit the possible rule productions at a
given span. We build tables Gtag(h) and restrict the
search to rules seen in training for a particular part-
of-speech tag.

Method 2 prunes based on the order in which de-
pendent words are added. By the constraints of the

algorithm, a head word xh must combine with each
of its left and right dependents. However, the or-
der of combination can lead to different tree struc-
tures (as illustrated in Figure 2). In total there are
|L(h)| × |R(h)| possible orderings of dependents.

In practice, though, it is often easy to predict
which side, left or right, will come next. We do this
by estimating the distribution,

p(side | tag(h), tag(m), tag(m′)),

wherem ∈ L(h) is the next left dependent andm′ ∈
R(h) is the next right dependent. If the conditional
probability of left or right is greater than a threshold
parameter γ, we make a hard decision to combine
with that side next. This pruning further reduces the
impact of outliers with multiple dependents on both
sides.

We empirically measure how these pruning meth-
ods affect observed runtime and oracle parsing per-
formance (i.e., how well a perfect scoring function
could do with a pruned Y(x, d)). Table 1 shows
a comparison of these pruning methods on devel-
opment data. The constrained parsing algorithm is
much faster than standard lexicalized parsing, and

4

791



Model Complexity Sent./s. Ora. F1

LEX CKY∗ n5|G| 0.25 100.0
DEP CKY

∑
h |L(h)||R(h)||G| 71.2 92.6

PRUNE1
∑

h |L(h)||R(h)||GT| 336.0 92.5
PRUNE2 – 96.6 92.5
PRUNE1+2 – 425.1 92.5

Table 1: Comparison of three parsing setups: LEX CKY∗

is the complete lexicalized c-parser on Y(x), but limited to
only sentences less than 20 words for tractability, DEP CKY
is the constrained c-parser on Y(x, d), PRUNE1, PRUNE2, and
PRUNE1+2 are combinations of the pruning methods described
in Section 3.2. The oracle is the best labeled F1 achievable on
the development data (§22, see Section 7).

pruning contributes even greater speed-ups. The or-
acle experiments show that the d-parse constraints
do contribute a large drop in oracle accuracy, while
pruning contributes a relatively small one. Still, this
upper-bound on accuracy is high enough to make it
possible to still recover c-parses at least as accurate
as state-of-the-art c-parsers. We will return to this
discussion in Section 7.

3.3 Binarization and Unary Rules
We have to this point developed the algorithm for
a strictly binary-branching grammar; however, we
need to produce trees have rules with varying size.
In order to apply the algorithm, we binarize the
grammar and add productions to handle unary rules.

Consider a non-binarized rule of the form A →
β1 . . . βm with head child β∗k . Relative to the head
child βk the rule has left-side β1 . . . βk−1 and right-
side βk+1 . . . βm. We replace this rule with new
binary rules and non-terminal symbols to produce
each side independently as a simple chain, left-side
first. The transformation introduces the following
new rules:5 A → β1 Ā∗, Ā → βi Ā∗ for i ∈
{2, . . . , k}, and Ā→ Ā∗ βi for i ∈ {k, . . . ,m}.

As an example consider the transformation of a
rule with four children:

S

NPNPVP∗NP

⇒ S

S̄∗

NPS̄∗

NPVP∗

NP

These rules can then be reversed deterministically to
produce a non-binary tree.

5These rules are slightly modified when k = 1.

We also explored binarization using horizontal
and vertical markovization to include additional
context of the tree, as found useful in unlexicalized
approaches (Klein and Manning, 2003). Preliminary
experiments showed that this increased the size of
the grammar, and the runtime of the algorithm, with-
out leading to improvements in accuracy.

Phrase-structure trees also include unary rules of
the form A→ β∗1 . To handle unary rules we modify
the parsing algorithms in Figure 3 to include a unary
completion rule,

(〈i, j〉, h, β1)
(〈i, j〉, h, A)

for all indices i ≤ h ≤ j that are consistent with
the dependency parse. In order to avoid unary re-
cursion, we limit the number of applications of this
rule at each span (preserving the runtime of the algo-
rithm). Preliminary experiments looked at collaps-
ing the unary rules into the nonterminal symbols,
but we found that this hurt performance compared
to explicit unary rules.

4 Structured Prediction

We learn the d-parse to c-parse conversion us-
ing a standard structured prediction setup. Define
the linear scoring function s for a conversion as
s(y;x, d, θ) = θ>f(x, d, y) where θ is a parameter
vector and f(x, d, y) is a feature function that maps
parse productions to sparse feature vectors. While
the parser only requires a d-parse at prediction time,
the parameters of this scoring function are learned
directly from a treebank of c-parses and a set of head
rules. The structured prediction model, in effect,
learns to invert the head rule transformation.

4.1 Features
The scoring function requires specifying a set of
parse features f which, in theory, could be directly
adapted from existing lexicalized c-parsers. How-
ever, the structure of the dependency parse greatly
limits the number of decisions that need to be made,
and allows for a smaller set of features.

We model our features after two bare-bones pars-
ing systems. The first set is the basic arc-factored
features used by McDonald (2006). These features
include combinations of: rule and top nonterminal,

5

792



For a production
(〈i, k〉,m, β1) (〈k + 1, j〉, h, β2)

(〈i, j〉, h, A)

Nonterm Features

(A, β1) (A, β1, tag(m))
(A, β2) (A, β2, tag(h))

Span Features

(rule, xi) (rule, xi−1)
(rule, xj) (rule, xj+1)
(rule, xk) (rule, xk+1)
(rule, bin(j − i))

Rule Features

(rule)
(rule, xh, tag(m))
(rule, tag(h), xm)
(rule, tag(h), tag(m))
(rule, xh)
(rule, tag(h))
(rule, xm)
(rule, tag(m))

Figure 4: The feature templates used in the function f(x, d, y).
For the span features, the symbol rule is expanded into both
A → B C and backoff symbol A. The function bin(i) parti-
tions a span length into one of 10 bins.

modifier word and part-of-speech, and head word
and part-of-speech.

The second set of features is modeled after the
span features described in the X-bar-style parser of
Hall et al. (2014). These include conjunctions of the
rule with: first and last word of current span, pre-
ceding and following word of current span, adjacent
words at split of current span, and binned length of
the span.

The full feature set is shown in Figure 4. After
training, there are a total of around 2 million non-
zero features. For efficiency, we use lossy feature
hashing. We found this had no impact on parsing
accuracy but made the parsing significantly faster.

4.2 Training
The parameters θ are estimated using a struc-
tural support vector machine (Taskar et al., 2004).
Given a set of gold-annotated c-parse examples,
(x1, y1), . . . , (xD, yD), and d-parses d1 . . . dD in-
duced from the head rules, we estimate the parame-
ters to minimize the regularized empirical risk

min
θ

D∑
i=1

`(xi, di, yi, θ) + λ||θ||1

where we define ` as `(x, d, y, θ) = −s(y) +
maxy′∈Y(x,d) (s(y′) + ∆(y, y′)) and where ∆ is a
problem specific cost-function. In experiments, we
use a Hamming loss ∆(y, y′) = |y − y′| where y is
an indicator for production rules firing over pairs of
adjacent spans (i.e., i, j, k).

PTB §22
Model Prec. Rec. F1

Xia et al. (2009) 88.1 90.7 89.4
PAD (§19) 95.9 95.9 95.9
PAD (§2–21) 97.5 97.8 97.7

Table 2: Comparison with the rule-based system of Xia et al.
(2009). Results are shown using gold-standard tags and depen-
dencies. Xia et al. report results consulting only §19 in devel-
opment and note that additional data had little effect. We show
our system’s results using §19 and the full training set.

The objective is optimized using AdaGrad (Duchi
et al., 2011). The gradient calculation requires com-
puting a loss-augmented max-scoring c-parse for
each training example which is done using the al-
gorithm of Figure 3 (right).

5 Related Work

The problem of converting dependency to phrase-
structured trees has been studied previously from the
perspective of building multi-representational tree-
banks. Xia and Palmer (2001) and Xia et al. (2009)
develop a rule-based system for the conversion of
human-annotated dependency parses. This work fo-
cuses on modeling the conversion decisions made
and capturing how researchers annotate specific phe-
nomena. Our work focuses on a different problem of
learning a data-driven structured prediction model
that is also able to handle automatically predicted
dependency parses as input. While the aim is dif-
ferent, Table 2 does give a direct comparison of our
system to that of Xia et al. (2009) on gold d-parse
data.

An important line of previous work also uses de-
pendency parsers to produce phrase-structure trees.
In particular Hall et al. (2007) and Hall and Nivre
(2008) develop a specialized dependency label set to
encode phrase-structure information in the d-parse.
After predicting a d-parse this label information can
be used to assemble a predicted c-parse. Our work
differs in that it does not make any assumptions on
the labeling of the dependency tree used and it uses
structured prediction to produce the final c-parse.

Very recently, Fernández-González and Martins
(2015) also show that an off-the-shelf, trainable,
dependency parser is enough to build a highly-
competitive constituent parser. They proposed

6

793



a new intermediate representation called “head-
ordered dependency trees”, which encode head or-
dering information in dependeny labels. Their al-
gorithm is based on a reduction of the constituent
parsing to dependency parsing of such trees.

There has been successful work combining de-
pendency and phrase-structure information to build
accurate c-parsers. Klein and Manning (2002) con-
struct a factored generative model that scores both
context-free syntactic productions and semantic de-
pendencies. Carreras et al. (2008) construct a state-
of-the-art parser that uses a dependency parsing
model both for pruning and within a richer lexical-
ized parser. Similarly, Rush et al. (2010) use dual
decomposition to combine a powerful dependency
parser with a lexicalized phrase-structure model.
This work differs in that we treat the dependency
parse as a hard constraint, hence largely reduce the
runtime of a fully lexicalized phrase structure pars-
ing model while maintaining the ability, at least
in principle, to generate highly accurate phrase-
structure parses.

Finally there have also been several papers that
use ideas from dependency parsing to simplify and
speed up phrase-structure prediction. Zhu et al.
(2013) build a high-accuracy phrase-structure parser
using a transition-based system. Hall et al. (2014)
use a stripped down parser based on a simple X-bar
grammar and a small set of lexicalized features.

6 Methods

We ran a series of experiments to assess the accu-
racy, efficiency, and applicability of our parser, PAD,
to several tasks. These experiments use the follow-
ing setup.

For English experiments we use the standard Penn
Treebank (PTB) experimental setup (Marcus et al.,
1993). Training is done on §2–21, development on
§22, and testing on §23. We use the development set
to tune the regularization parameter, λ = 1e−8, and
the pruning threshold, γ = 0.95.

For Chinese experiments, we use version 5.1 of
the Penn Chinese Treebank 5.1 (CTB) (Xue et al.,
2005). We followed previous work and used articles
001–270 and 440–1151 for training, 301–325 for de-
velopment, and 271–300 for test. We also use the
development set to tune the regularization parame-

ter, λ = 1e− 3.
Part-of-speech tagging is performed for all mod-

els using TurboTagger (Martins et al., 2013). Prior
to training the d-parser, the training sections are
automatically processed using 10-fold jackknifing
(Collins and Koo, 2005) for both dependency and
phrase structure trees. Zhu et al. (2013) found this
simple technique gives an improvement to depen-
dency accuracy of 0.4% on English and 2.0% on
Chinese in their system.

During training, we use the d-parses induced by
the head rules from the gold c-parses as constraints.
There is a slight mismatch here with test, since these
d-parses are guaranteed to be consistent with the tar-
get c-parse. We also experimented with using 10-
fold jacknifing of the d-parser during training to pro-
duce more realistic parses; however, we found that
this hurt performance of the parser.

Unless otherwise noted, in English the test d-
parsing is done using the RedShift implementation6

of the parser of Zhang and Nivre (2011), trained
to follow the conventions of Collins head rules
(Collins, 2003). This parser is a transition-based
beam search parser, and the size of the beam k con-
trols a speed/accuracy trade-off. By default we use
a beam of k = 16. We found that dependency la-
bels have a significant impact on the performance of
the RedShift parser, but not on English dependency
conversion. We therefore train a labeled parser, but
discard the labels.

For Chinese, we use the head rules compiled by
Ding and Palmer (2005)7. For this data-set we
trained the d-parser using the YaraParser implemen-
tation8 of the parser of Zhang and Nivre (2011), be-
cause it has a better Chinese implementation. We
use a beam of k = 64. In experiments, we found
that Chinese labels were quite helpful, and added
four additional features templates conjoining the la-
bel with the non-terminals of a rule.

Evaluation for phrase-structure parses is per-
formed using the evalb9 script with the standard
setup. We report labeled F1 scores as well as recall
and precision. For dependency parsing, we report

6https://github.com/syllog1sm/redshift
7http://stp.lingfil.uu.se/˜nivre/

research/chn_headrules.txt
8https://github.com/yahoo/YaraParser
9http://nlp.cs.nyu.edu/evalb

7

794



PTB §23
Model F1 Sent./s.

Charniak (2000) 89.5 –
Stanford PCFG (2003) 85.5 5.3
Petrov (2007) 90.1 8.6
Zhu (2013) 90.3 39.0
Carreras (2008) 91.1 –

CJ Reranking (2005) 91.5 4.3
Stanford RNN (2013) 90.0 2.8

PAD 90.4 34.3
PAD (Pruned) 90.3 58.6

CTB
Model F1

Charniak (2000) 80.8
Bikel (2004) 80.6
Petrov (2007) 83.3
Zhu (2013) 83.2

PAD 82.4

Table 3: Accuracy and speed on PTB §23 and CTB 5.1 test
split. Comparisons are to state-of-the-art non-reranking super-
vised phrase-structure parsers (Charniak, 2000; Klein and Man-
ning, 2003; Petrov and Klein, 2007; Carreras et al., 2008; Zhu
et al., 2013; Bikel, 2004), and semi-supervised and reranking
parsers (Charniak and Johnson, 2005; Socher et al., 2013).

unlabeled accuracy score (UAS).
We implemented the grammar binarization, head

rules, and pruning tables in Python, and the parser,
features, and training in C++. Experiments are per-
formed on a Lenovo ThinkCentre desktop computer
with 32GB of memory and Core i7-3770 3.4GHz
8M cache CPU.

7 Experiments

We ran experiments to assess the accuracy of the
method, its runtime efficiency, the effect of depen-
dency parsing accuracy, and the effect of the amount
of annotated phrase-structure data.

Parsing Accuracy Table 3 compares the accuracy
and speed of the phrase-structure trees produced by
the parser. For these experiments we treat our sys-
tem and the Zhang-Nivre parser as an independently
trained, but complete end-to-end c-parser. Runtime
for these experiments includes both the time for d-
parsing and conversion. Despite the fixed depen-

Model UAS F1 Sent./s. Oracle

MALTPARSER 89.7 85.5 240.7 87.8
RS-K1 90.1 86.6 233.9 87.6
RS-K4 92.5 90.1 151.3 91.5
RS-K16 93.1 90.6 58.6 92.5
YARA-K1 89.7 85.3 1265.8 86.7
YARA-K16 92.9 89.8 157.5 91.7
YARA-K32 93.1 90.4 48.3 92.0
YARA-K64 93.1 90.5 47.3 92.2
TP-BASIC 92.8 88.9 132.8 90.8
TP-STANDARD 93.3 90.9 27.2 92.6
TP-FULL 93.5 90.8 13.2 92.9

Table 4: The effect of d-parsing accuracy (PTB §22) on PAD
and an oracle converter. Runtime includes d-parsing and c-
parsing. Inputs include MaltParser (Nivre et al., 2006), the
RedShift and the Yara implementations of the parser of Zhang
and Nivre (2011) with various beam size, and three versions of
TurboParser trained with projective constraints (Martins et al.,
2013).

dency constraints, the English results show that the
parser is comparable in accuracy to many widely-
used systems, and is significantly faster. The parser
most competitive in both speed and accuracy is that
of Zhu et al. (2013), a fast shift-reduce phrase-
structure parser.

Furthermore, the Chinese results suggest that,
even without making language-specific changes in
the feature system we can still achieve competitive
parsing accuracy.

Effect of Dependencies Table 4 shows experi-
ments comparing the effect of different input d-
parses. For these experiments we used the same ver-
sion of PAD with 11 different d-parsers of varying
quality and speed. We measure for each parser: its
UAS, speed, and labeled F1 when used with PAD
and with an oracle converter.10 The paired figure

10For a gold parse y and predicted dependencies d̂, define the
oracle parse as y′ = arg miny′∈Y(x,d̂) ∆(y, y

′)
8

795



Figure 5: Empirical runtime of the parser on sentences of vary-
ing length, with and without pruning. Despite a worst-case
quadratic complexity, observed runtime is linear.

shows that there is a direct correlation between the
UAS of the inputs and labeled F1.

Runtime In Section 3 we considered the theoret-
ical complexity of the parsing model and presented
the main speed results in Table 1. Despite having
a quadratic theoretical complexity, the practical run-
time was quite fast. Here we consider the empiri-
cal complexity of the model by measuring the time
spent on individual sentences. Figure 5 shows parser
speed for sentences of varying length for both the
full algorithm and with pruning. In both cases the
observed runtime is linear.

Recovering Phrase-Structure Treebanks Anno-
tating phrase-structure trees is often more expensive
and slower than annotating unlabeled dependency
trees (Schneider et al., 2013). For low-resource lan-
guages, an alternative approach to developing fully
annotated phrase-structure treebanks might be to la-
bel a small amount of c-parses and a large amount of
cheaper d-parses. Assuming this setup, we ask how
many c-parses would be necessary to obtain reason-
able performance?

For this experiment, we train PAD on only 5%
of the PTB training set and apply it to predicted d-
parses from a fully-trained model. Even with this
small amount of data, we obtain a parser with de-
velopment score of F1 = 89.1%, which is compa-
rable to Charniak (2000) and Stanford PCFG (Klein
and Manning, 2003) trained on the complete c-parse
training set. Additionally, if the gold dependencies
are available, PAD with 5% training achieves F1 =
95.8% on development, demonstrating a strong abil-

Class Results
Dep. Span Split Count Acc.

(h,m) 〈i, j〉 k A
+ + + 32853 97.9
– + + 381 69.3
+ + – 802 83.3
– + – 496 85.9
+ – – 1717 0.0
– – – 1794 0.0

Table 5: Error analysis of binary CFG rules. Rules used are split
into classes based on correct (+) identification of dependency
(h,m), span 〈i, j〉, and split k. “Count” is the size of each
class. “Acc.” is the accuracy of span nonterminal identification.

ity to recover the phrase-structure trees from depen-
dency annotations.

Analysis Finally we consider an internal error
analysis of the parser. For this analysis, we group
each binary rule production selected by the parser
by three properties: Is its dependency (h,m) cor-
rect? Is its span 〈i, j〉 correct? Is its split k correct?
The first property is fully determined by the input
d-parse, the others are partially determined by PAD
itself.

Table 5 shows the breakdown. The conversion
is almost always accurate (∼98%) when the parser
has correct span and dependency information. As
expected, the difficult cases come when the depen-
dency was fully incorrect, or there is a propagated
span mistake. As dependency parsers improve, the
performance of PAD should improve as well.

8 Conclusion

With recent advances in statistical dependency pars-
ing, we find that fast, high-quality phrase-structure
parsing is achievable using dependency parsing first,
followed by a statistical conversion algorithm to
fill in phrase-structure trees. Our implementation
is available as open-source software at https://
github.com/ikekonglp/PAD.

Acknowledgments The authors thank the anony-
mous reviewers and André Martins, Chris Dyer, and
Slav Petrov for helpful feedback. This research was
supported in part by NSF grant IIS-1352440 and
computing resources provided by Google and the
Pittsburgh Supercomputing Center.

9

796



References
Daniel M Bikel. 2004. On the parameter space of gen-

erative lexicalized statistical parsing models. Ph.D.
thesis, University of Pennsylvania.

Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 9–16. Association for Compu-
tational Linguistics.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173–
180. Association for Computational Linguistics.

Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132–139. Association for Com-
putational Linguistics.

Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25–70.

Michael Collins, Lance Ramshaw, Jan Hajič, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 505–512. Associa-
tion for Computational Linguistics.

Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational linguis-
tics, 29(4):589–637.

Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1–8. Association for Computational
Linguistics.

Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 541–548. Association for Computational
Linguistics.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159.

Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proceedings of the 37th annual
meeting of the Association for Computational Linguis-
tics on Computational Linguistics, pages 457–464. As-
sociation for Computational Linguistics.

Daniel Fernández-González and André FT Martins.
2015. Parsing as reduction. arXiv preprint
arXiv:1503.00030.

Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for german dependency and constituency
representations. In Proceedings of the Workshop on
Parsing German, pages 47–54. Association for Com-
putational Linguistics.

Johan Hall, Joakim Nivre, and Jens Nilsson. 2007. A hy-
brid constituency-dependency parser for swedish. In
Proceedings of NODALIDA, pages 284–287.

David Hall, Greg Durrett, and Dan Klein. 2014. Less
grammar, more features. In ACL.

Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
16th Nordic Conference of Computational Linguistics,
pages 105–112. University of Tartu.

Dan Klein and Christopher D Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In Advances in neural information processing
systems, pages 3–10.

Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Associ-
ation for Computational Linguistics.

Lingpeng Kong and Noah A Smith. 2014. An empirical
comparison of parsing methods for stanford dependen-
cies. arXiv preprint arXiv:1404.4314.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational lin-
guistics, 19(2):313–330.

André FT Martins, Miguel Almeida, and Noah A Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In ACL (2), pages 617–622.

Ryan McDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency parsing.
Ph.D. thesis, University of Pennsylvania.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC, volume 6, pages
2216–2219.

Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL, pages 404–
411. Citeseer.

Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1–11. Association for Computational Linguis-
tics.

10

797



Nathan Schneider, Brendan O’Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A Smith,
Chris Dyer, and Jason Baldridge. 2013. A
framework for (under) specifying dependency syn-
tax without overloading annotators. arXiv preprint
arXiv:1306.2091.

Richard Socher, John Bauer, Christopher D Manning, and
Andrew Y Ng. 2013. Parsing with compositional vec-
tor grammars. In In Proceedings of the ACL confer-
ence.

Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin Markov networks. In Advances in Neural
Information Processing Systems 16.

Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Proceedings
of the first international conference on Human lan-
guage technology research, pages 1–5. Association for
Computational Linguistics.

Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,
and Dipti Misra Sharma. 2009. Towards a multi-
representational treebank. In The 7th International
Workshop on Treebanks and Linguistic Theories.
Groningen, Netherlands.

Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(02):207–238.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, volume 3.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188–193. Association for Computational Linguistics.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and
Jingbo Zhu. 2013. Fast and accurate shift-reduce con-
stituent parsing. In ACL (1), pages 434–443.

11

798


