



















































Topic Extraction from Microblog Posts Using Conversation Structures


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2114â€“2123,
Berlin, Germany, August 7-12, 2016. cÂ©2016 Association for Computational Linguistics

Topic Extraction from Microblog Posts Using Conversation Structures

Jing Li1,2âˆ—, Ming Liao1,2, Wei Gao3, Yulan He4 and Kam-Fai Wong1,2
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong

2MoE Key Laboratory of High Confidence Software Technologies, China
3Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar

4School of Engineering and Applied Science, Aston University, UK
{lijing,mliao,kfwong}@se.cuhk.edu.hk1,2
wgao@qf.org.qa3, y.he9@aston.ac.uk4

Abstract

Conventional topic models are ineffec-
tive for topic extraction from microblog
messages since the lack of structure and
context among the posts renders poor
message-level word co-occurrence pat-
terns. In this work, we organize microblog
posts as conversation trees based on re-
posting and replying relations, which en-
rich context information to alleviate data
sparseness. Our model generates words
according to topic dependencies derived
from the conversation structures. In spe-
cific, we differentiate messages as leader
messages, which initiate key aspects of
previously focused topics or shift the focus
to different topics, and follower messages
that do not introduce any new information
but simply echo topics from the messages
that they repost or reply. Our model cap-
tures the different extents that leader and
follower messages may contain the key
topical words, thus further enhances the
quality of the induced topics. The results
of thorough experiments demonstrate the
effectiveness of our proposed model.

1 Introduction

The increasing popularity of microblog platforms
results in a huge volume of user-generated short
posts. Automatically modeling topics out of such
massive microblog posts can uncover the hid-
den semantic structures of the underlying collec-
tion and can be useful to downstream applications
such as microblog summarization (Harabagiu and
Hickl, 2011), user profiling (Weng et al., 2010),
event tracking (Lin et al., 2010) and so on.

Popular topic models, like Probabilistic La-
tent Semantic Analysis (pLSA) (Hofmann, 1999)

âˆ—* Part of this work was conducted when the first author
was visiting Aston University.

and Latent Dirichlet Allocation (LDA) (Blei et
al., 2003b), model the semantic relationships be-
tween words based on their co-occurrences in doc-
uments. They have demonstrated their success
in conventional documents such as news reports
and scientific articles, but perform poorly when
directly applied to short and colloquial microblog
content due to severe sparsity in microblog mes-
sages (Wang and McCallum, 2006; Hong and
Davison, 2010).

A common way to deal with short text sparsity
is to aggregate short messages into long pseudo-
documents. Most of the studies heuristically
aggregate messages based on authorship (Zhao
et al., 2011; Hong and Davison, 2010), shared
words (Weng et al., 2010), or hashtags (Ram-
age et al., 2010; Mehrotra et al., 2013). Some
works directly take into account the word re-
lations to alleviate document-level word sparse-
ness (Yan et al., 2013; Sridhar, 2015). More
recently, a self-aggregation-based topic model
called SATM (Quan et al., 2015) was proposed to
aggregate texts jointly with topic inference.

However, we argue that the existing aggrega-
tion strategies are suboptimal for modeling top-
ics in short texts. Microblogs allow users to share
and comment on messages with friends through
reposting or replying, similar to our everyday con-
versations. Intuitively, the conversation structures
can not only enrich context, but also provide use-
ful clues for identifying relevant topics. This
is nonetheless ignored in previous approaches.
Moreover, the occurrence of non-topic words such
as emotional, sentimental, functional and even
meaningless words are very common in microblog
posts, which may distract the models from recog-
nizing topic-related key words and thus fail to pro-
duce coherent and meaningful topics.

We propose a novel topic model by utilizing the
structures of conversations in microblogs. We link
microblog posts using reposting and replying rela-

2114



tions to build conversation trees. Particularly, the
root of a conversation tree refers to the original
post and its edges represent the reposting/replying
relations.

[O] Just an hour ago, a series of coordinated

terrorist attacks occurred in Paris !!!

[R2] Gunmen and suicide bombers

hit a concert hall. More than 100

are killed already. 

[R1] OMG! I canâ€™t believe itâ€™s

real. Paris?! Iâ€™ve just been there

last month.

[R3] Oh no! @BonjourMarc

r u OK? please reply me for

godâ€™s sake!!!

[R4] My gosh!!! that

sucks Poor on u

guysâ€¦ 

[R7] For the safety of US, Iâ€™m 

for Trump to be president,

especially after this.

[R8] I repost to support 

@realDonaldTrump. Canâ€™t 

agree more 

[R10] R U CRAZY?! 

Trump is just a bigot sexist

and racist.

â€¦â€¦ â€¦â€¦ â€¦â€¦

[R9] thanks dude, youâ€™d 

never regret 

â€¦â€¦

[R5] Donâ€™t worry. I was 

home. 
[R6] poor guys, terrible

Figure 1: An example of conversation tree. [O]:
the original post; [Ri]: the i-th repost/reply; Ar-
row lines: reposting/replying relations; Dark black
posts: leaders to be detected; Underlined italic
words: key words representing topics

Figure 1 illustrates an example of a conversa-
tion tree, in which messages can initiate a new
topic such as [O] and [R7] or raise a new aspect
(subtopic) of the previously discussed topics such
as [R2] and [R10]. These messages are named as
leaders, which contain salient content in topic de-
scription, e.g., the italic and underlined words in
Figure 1. The remaining messages, named as fol-
lowers, do not raise new issues but simply respond
to their reposted or replied messages following
what has been raised by the leaders and often con-
tain non-topic words, e.g., OMG, OK, agree, etc.

Conversation tree structures from microblogs
have been previously shown helpful to microblog
summarization (Li et al., 2015), but have never
been explored for topic modeling. We follows Li
et al. (2015) to detect leaders and followers across
paths of conversation trees using Conditional Ran-
dom Fields (CRF) trained on annotated data. The
detected leader/follower information is then in-
corporated as prior knowledge into our proposed
topic model.

Our experimental results show that our model,
which captures parent-child topic correlations in
conversation trees and generates topics by consid-
ering messages being leaders or followers sepa-
rately, is able to induce high-quality topics and
outperforms a number of competitive baselines. In
summary, our contributions are three-fold:
â€¢ We propose a novel topic model, which ex-

plicitly exploits the topic dependencies contained
in conversation structures to enhance topic assign-
ments.
â€¢ Our model differentiates the generative pro-

cess of topical and non-topic words, according to
the message where a word is drawn from being
a leader or a follower. This helps the model dis-
tinguish the topic-specific information from back-
ground noise.
â€¢ Our model outperforms state-of-the-art topic

models when evaluated on a large real-world mi-
croblog dataset containing over 60K conversation
trees, which is publicly available1.

2 Related Works

Topic models aim to discover the latent seman-
tic information, i.e., topics, from texts and have
been extensively studied. One of the most popu-
lar and well-known topic models is LDA (Blei et
al., 2003b). It utilizes Dirichlet priors to generate
document-topic and topic-word distributions, and
has been shown effective in extracting topics from
conventional documents.

Nevertheless, prior research has demonstrated
that standard topic models, essentially focusing
on document-level word co-occurrences, are not
suitable for short and informal microblog mes-
sages due to severe data sparsity exhibited in
short texts (Wang and McCallum, 2006; Hong and
Davison, 2010). Therefore, how to enrich and ex-
ploit context information becomes a main concern.
Weng et al. (2010), Hong et al. (2010) and Zhao
et al. (2011) first heuristically aggregated mes-
sages posted by the same user or sharing the same
words before applying classic topic models to ex-
tract topics. However, such a simple strategy poses
some problems. For example, it is common that a
user has various interests and posts messages cov-
ering a wide range of topics. Ramage et al. (2010)
and Mehrotra et al. (2013) used hashtags as labels
to train supervised topic models. But these mod-
els depend on large-scale hashtag-labeled data for
model training, and their performance is inevitably
compromised when facing unseen topics irrelevant
to any hashtag in training data due to the rapid
change and wide variety of topics in social media.

SATM (Quan et al., 2015) combined short texts
aggregation and topic induction into a unified
model. But in their work, no prior knowledge

1http://www1.se.cuhk.edu.hk/Ëœlijing/
data/microblog-topic-extraction-data.zip

2115



was given to ensure the quality of text aggrega-
tion, which therefore can affect the performance
of topic inference. In this work, we organize mi-
croblog messages as conversation trees based on
reposting/reply relations, which is a more advan-
tageous message aggregation strategy.

Another line of research tackled the word
sparseness by modeling word relations instead of
word occurrences in documents. For example,
Gaussian Mixture Topic Model (GMTM) (Srid-
har, 2015) utilized word embeddings to model the
distributional similarities of words and then in-
ferred clusters of words represented by word dis-
tributions using Gaussian Mixture Model (GMM)
that capture the notion of latent topics. However,
GMTM heavily relies on meaningful word embed-
dings that require a large volume of high-quality
external resources for training.

Biterm Topic Model (BTM) (Yan et al.,
2013) directly explores unordered word-pair co-
occurrence patterns in each individual message.
Our model learns topics from aggregated mes-
sages based on conversation trees, which naturally
provide richer context since word co-occurrence
patterns can be captured from multiple relevant
messages.

3 LeadLDA Topic Model

In this section, we describe how to extract top-
ics from a microblog collection utilizing conversa-
tion tree structures, where the trees are organized
based on reposting and replying relations among
the messages2.

To identify key topic-related content from collo-
quial texts, we differentiate the messages as lead-
ers and followers. Following Li et al. (2015), we
extract all root-to-leaf paths on conversation trees
and utilize the state-of-the-art sequence learning
model CRF (Lafferty et al., 2001) to detect the
leaders3. As a result, the posterior probability of
each node being a leader or follower is obtained
by averaging the different marginal probabilities
of the same node over all the tree paths that contain
the node. Then, the obtained probability distribu-
tion is considered as the observed prior variable
input into our model.

2Reposting/replying relations are straightforward to ob-
tain by using microblog APIs from Twitter and Sina Weibo.

3The CRF model for leader detection was trained on a
public corpus with all the messages annotated on the tree
paths. Details are described in Section 4.

3.1 Topics and Conversation Trees

Previous works (Zhao et al., 2011; Yan et al.,
2013; Quan et al., 2015) have proven that assum-
ing each short post contains a single topic is useful
to alleviate the data sparsity problem. Thus, given
a corpus of microblog posts organized as conver-
sation trees and the estimated leader probabilities
of tree nodes, we assume that each message only
contains a single topic and a tree covers a mixture
of multiple topics. Since leader messages subsume
the content of their followers, the topic of a leader
can be generated from the topic distribution of the
entire tree. Consequently, the topic mixture of a
conversation tree is determined by the topic as-
signments to the leader messages on it. The topics
of followers, however, exhibit strong and explicit
dependencies on the topics of their ancestors. So,
their topics need to be generated in consideration
of local constraints. Here, we mainly address how
to model the topic dependencies of followers.

Enlighten by the general Structural Topic Model
(strTM) (Wang et al., 2011), which incorporates
document structures into topic model by explic-
itly modeling topic dependencies between adja-
cent sentences, we exploit the topical transitions
between parents and children in the trees for guid-
ing topic assignments.

Intuitively, the emergence of a leader results in
potential topic shift. It tends to weaken the topic
similarities between the emerging leaders and their
predecessors. For example, [R7] in Figure 1 trans-
fers the topic to a new focus, thus weakens the tie
with its parent. We can simplify our case by as-
suming that followers are topically responsive just
up to (hence not further than) their nearest ances-
tor leaders. Thus, we can dismantle each conver-
sation tree into forest by removing the links be-
tween leaders and their parents hence producing
a set of subgraphs like [R2]â€“[R6] and [R7]â€“[R9]
in Figure 1. Then, we model the internal topic
dependencies within each subgraph by inferring
the parent-child topic transition probabilities sat-
isfying the first-order Markov properties in a simi-
lar way as estimating the transition distribution of
adjacent sentences in strTM (Wang et al., 2011).
At topic assignment stage, the topic of a follower
will be assigned by referring to its parentâ€™s topic
and the transition distribution that captures topic
similarities of followers to their parents (see Sec-
tion 3.2).

In addition, every word in the corpus is either

2116



a topical or non-topic (i.e., background) word,
which highly depends on whether it occurs in a
leader or a follower message.

Figure 2 illustrates the graphical model of our
generative process, which is named as LeadLDA.

T
Mt

	 Â ğ›½

K
	 Â ğœ™$ 	 Â ğœ™%

	 Â ğœƒ'	 Â ğ‘§',*

Nt,m

	 Â 	 Â 	 Â ğ‘§',+(*)	 Â 

	 Â ğ‘¦',* 	 Â ğ‘™',*

	 Â ğ›¾

K
	 Â ğœ‹$

	 Â ğ‘¤',*,3

	 Â ğ‘¥',*,3

	 Â ğ›¿
2

	 Â ğœ7

	 Â Î±

Figure 2: Graphical Model of LeadLDA

3.2 Topic Modeling

Formally, we assume that the microblog posts are
organized as T conversation trees. Each tree t
contains Mt message nodes and each message m
contains Nt,m words in the vocabulary. The vo-
cabulary size is V and there are K topics em-
bedded in the corpus represented by word distri-
bution Ï†k âˆ¼ Dir(Î²) (k = 1, 2, ...,K). Also, a
background word distribution Ï†B âˆ¼ Dir(Î²) is in-
cluded to capture the general information, which is
not topic specific. Ï†k and Ï†B are multinomial dis-
tributions over the vocabulary. A tree t is modeled
as a mixture of topics Î¸t âˆ¼ Dir(Î±) and any mes-
sage m on t is assumed to contain a single topic
zt,m âˆˆ {1, 2, ...,K}.

(1) Topic assignments: The topic assignments
of LeadLDA is inspired by Griffiths et al. (2004)
that combines syntactic and semantic dependen-
cies between words. LeadLDA integrates the out-
comes of leader detection with a binomial switcher
yt,m âˆˆ {0, 1} indicating whether m is a leader
(yt,m = 1) or a follower (yt,m = 0), given each
message m on the tree t. yt,m is parameterized by
its leader probability lt,m, which is the posterior
probability output from the leader detection model
and serves as an observed prior variable.

According to the notion of leaders, they initiate
key aspects of previously discussed topics or sig-
nal a new topic shifting the focus of its descendant

followers. So, the topics of leaders on tree t are
directly sampled from the topic mixture Î¸t.

To model the internal topic correlations within
the subgraph of conversation tree consisting of a
leader and all its followers, we capture parent-
child topic transitions Ï€k âˆ¼ Dir(Î³), which is a
distribution over K topics, and use Ï€k,j to denote
the probability of a follower assigned topic j when
the topic of its parent is k. Specifically, if message
m is sampled as a follower and the topic assign-
ment to its parent message is zt,p(m), where p(m)
indexes the parent ofm, then zt,m (i.e., the topic of
m) is generated from topic transition distribution
Ï€zt,p(m) . In particular, since the root of a conver-
sation tree has no parent and can only be a leader,
we make the leader probability lt,root = 1 to force
its topic only to be generated from the topic distri-
bution of tree t.

(2) Topical and non-topic words: We sep-
arately model the distributions of leader and
follower messages emitting topical or non-topic
words with Ï„0 and Ï„1, respectively, both of which
are drawn from a symmetric Beta prior parame-
tererized by Î´. Specifically, for each word n in
message m on tree t, we add a binomial back-
ground switcher xt,m,n controlled by whether m
is a leader or a follower, i.e., xt,m,n âˆ¼ Bi(Ï„yt,m),
which indicates n is a topical word if xt,m,n = 0 or
a background word if xt,m,n = 1, and xt,m,n con-
trols n to be generated from the topic-word dis-
tribution Ï†zt,m , where zt,m is the topic of m, or
from background word distribution Ï†B modeling
non-topic information.

(3) Generation process: To sum up, condi-
tioned on the hyper-parameters Î˜ = (Î±, Î², Î³, Î´),
the generation process of a conversation tree t can
be described as follows:

â€¢ Draw Î¸t âˆ¼ Dir(Î±)
â€¢ For message m = 1 to Mt on tree t

â€“ Draw yt,m âˆ¼ Bi(lt,m)
â€“ If yt,m == 1
âˆ— Draw zt,m âˆ¼Mult(Î¸t)

â€“ If yt,m == 0
âˆ— Draw zt,m âˆ¼Mult(Ï€zt,p(m))

â€“ For word n = 1 to Nt,m in m
âˆ— Draw xt,m,n âˆ¼ Bi(Ï„yt,m)
âˆ— If xt,m,n == 0
Â· Draw wt,m,n âˆ¼Mult(Ï†zt,m)

âˆ— If xt,m,n == 1
Â· Draw wt,m,n âˆ¼Mult(Ï†B)

2117



CLBs,(r)
# of words with background switchers assigned as r and oc-
curring in messages with leader switchers s.

CLBs,(Â·)
# of words occurring in messages whose leader switchers are
s, i.e.,

âˆ‘
râˆˆ{0,1} C

LB
s,(r).

NB(r)
# of words occurring in message (t, m) and with background
switchers assigned as r.

NB(Â·)
# of words in message (t, m), i.e., NB(Â·) =âˆ‘

râˆˆ{0,1}N
B
(r).

CT Wk,(v)

# of words indexing v in vocabulary, sampled as topic (non-
background) words, and occurring in messages assigned topic
k.

CT Wk,(Â·)

# of words assigned as topic (non-background) word and
occurring in messages assigned topics k, i.e., CT Wk,(Â·) =âˆ‘V

v=1 C
T W
k,(v).

NW(v)
# of words indexing v in vocabulary that occur in message
(t, m) and are assigned as topic (non-background) word.

NW(Â·)
# of words assigned as topic (non-background) words and oc-
curring in message (t, m), i.e., NW(Â·) =

âˆ‘V
v=1 N

W
(v).

CT Ri,(j)
# of messages sampled as followers and assigned topic j,
whose parents are assigned topic i.

CT Ri,(Â·)
# of messages sampled as followers whose parents are as-
signed topic i, i.e., CT Ri,(Â·) =

âˆ‘K
j=1 C

T R
i,(j).

I(Â·) An indicator function, whose value is 1 when its argumentinside () is true, and 0 otherwise.
NCT(j)

# of messages that are children of message (t, m), sampled
as followers and assigned topic j.

NCT(Â·)
# of message (t, m)â€™s children sampled as followers, i.e.,
NCT(Â·) =

âˆ‘K
j=1 N

CT
(j)

CT Tt,(k)
# of messages on conversation tree t sampled as leaders and
assigned topic k.

CT Tt,(Â·)
# of messages on conversation tree t sampled as leaders, i.e.,
CT Tt,(Â·) =

âˆ‘K
k=1 C

T T
t,(k)

CBW(v)
# of words indexing v in vocabulary and assigned as back-
ground (non-topic) words

CBW(Â·)
# of words assigned as background (non-topic) words, i.e.,
CBW(Â·) =

âˆ‘V
v=1 C

BW
(v)

Table 1: The notations of symbols in the sampling
formulas (1) and (2). (t,m): message m on con-
versation tree t.

3.3 Inference for Parameters

We use collapsed Gibbs Sampling (Griffiths,
2002) to carry out posterior inference for param-
eter learning. The hidden multinomial variables,
i.e., message-level variables (y and z) and word-
level variables (x) are sampled in turn, conditioned
on a complete assignment of all other hidden vari-
ables. Due to the space limitation, we leave out
the details of derivation but give the core formulas
in the sampling steps.

We first define the notations of all variables
needed by the formulation of Gibbs sampling,
which are described in Table 1. In particular, the
various C variables refer to counts excluding the
message m on conversation tree t.

For each message m on a tree t, we sample
the leader switcher yt,m and topic assignment zt,m
according to the following conditional probability

distribution:

p(yt,m = s, zt,m = k|yÂ¬(t,m), zÂ¬(t,m),w,x, l,Î˜)

âˆ Î“(C
LB
s,(Â·) + 2Î´)

Î“(CLBs,(Â·) +N
B
(Â·) + 2Î´)

âˆ
râˆˆ{0,1}

Î“(CLBs,(r) +N
B
(r) + Î´)

Î“(CLBs,(r) + Î´)

Â· Î“(C
TW
k,(Â·) + V Î²)

Î“(CTWk,(Â·) +N
W
(Â·) + V Î²)

Vâˆ
v=1

Î“(CTWk,(v) +N
W
(v) + Î²)

Î“(CTWk,(v) + Î²)

Â·g(s, k, t,m)
(1)

where g(s, k, t,m) takes different forms depend-
ing on the value of s:

g(0, k, t,m) =
Î“(CTRzt,p(m),(Â·) +KÎ³)

Î“(CTRzt,p(m),(Â·) + I(zt,p(m) 6= k) +KÎ³)

Â· Î“(C
TR
k,(Â·) +KÎ³)

Î“(CTRk,(Â·) + I(zt,p(m) = k) +N
CT
(Â·) +KÎ³)

Â·
Kâˆ
j=1

Î“(CTRk,(j) +N
CT
(j) + I(zt,p(m) = j = k) + Î³)

Î“(CTRk,(j) + Î³)

Â·
Î“(CTRzt,p(m),(k) + I(zt,p(m) 6= k) + Î³)

Î“(CTRzt,p(m),(k)
+ Î³)

Â· (1âˆ’ lt,m)

and

g(1, k, t,m) =
CTTt,(k) + Î±

CTTt,(Â·) +KÎ±
Â· lt,m

For each word n in m on t, the sampling for-
mula of its background switcher is given as the
following:

p(xt,m,n = r|xÂ¬(t,m,n),y, z,w, l,Î˜)

âˆ C
LB
yt,m,(r)

+ Î´

CLByt,m,(Â·) + 2Î´
Â· h(r, t,m, n) (2)

where

h(r, t,m, n) =

ï£±ï£´ï£²ï£´ï£³
CT Wzt,m,(wt,m,n)

+Î²

CT W
zt,m,(Â·)+V Î²

if r = 0

CBW(wt,m,n)
+Î²

CBW(Â·) +V Î²
if r = 1

4 Data Collection and Experiment Setup

To evaluate our LeadLDA model, we conducted
experiments on real-world microblog dataset col-
lected from Sina Weibo that has the same 140-
character limitation and shares the similar mar-
ket penetration as Twitter (Rapoza, 2011). For
the hyper-parameters of LeadLDA, we fixed Î± =
50/K, Î² = 0.1, following the common practice
in previous works (Griffiths and Steyvers, 2004;
Quan et al., 2015). Since there is no analogue of
Î³ and Î´ in prior works, where Î³ controls topic
dependencies of follower messages to their an-
cestors and Î´ controls the different tendencies of

2118



Month # of trees # of messages Vocab size
May 10,812 38,926 6,011
June 29,547 98,001 9,539
July 26,103 102,670 10,121

Table 2: Statistics of our three evaluation datasets

leaders and followers covering topical and non-
topic words. We tuned Î³ and Î´ by grid search on
a large development set containing around 120K
posts and obtained Î³ = 50/K, Î´ = 0.5.

Because the content of posts are often incom-
plete and informal, it is difficult to manually an-
notate topics in a large scale. Therefore, we fol-
low Yan et al. (2013) to utilize hashtags led by â€˜#â€™,
which are manual topic labels provided by users,
as ground-truth categories of microblog messages.
We collected the real-time trending hashtags on
Sina Weibo and utilized the hashtag-search API4

to crawl the posts matching the given hashtag
queries. In the end, we built a corpus containing
596,318 posts during May 1 â€“ July 31, 2014.

To examine the performance of models on var-
ious topic distributions, we split the corpus into 3
datasets, each containing messages of one month.
Similar to Yan et al. (2013), for each dataset, we
manually selected 50 frequent hashtags as topics,
e.g. #mh17, #worldcup, etc. The experiments
were conducted on the subsets of posts with the
selected hashtags. Table 2 shows the statistics of
the three subsets used in our experiments.

We preprocessed the datasets before topic ex-
traction in the following steps: 1) Use FudanNLP
toolkit (Qiu et al., 2013) for word segmentation,
stop words removal and POS tagging for Chinese
Weibo messages; 2) Generate a vocabulary for
each dataset and remove words occurring less than
5 times; 3) Remove all hashtags in texts before in-
put them to models, since the models are expected
to extract topics without knowing the hashtags,
which are ground-truth topics; 4) For LeadLDA,
we use the CRF-based leader detection model (Li
et al., 2015) to classify messages as leaders and
followers. The leader detection model was im-
plemented by using CRF++5, which was trained
on the public dataset composed of 1,300 conversa-
tion paths and achieved state-of-the-art 73.7% F1-
score of classification accuracy (Li et al., 2015).

4http://open.weibo.com/wiki/2/search/
topics

5https://taku910.github.io/crfpp/

5 Experimental Results

We evaluated topic models with two sets ofK, i.e.,
the number of topics. One isK = 50, to match the
count of hashtags following Yan et al. (2013), and
the other is K = 100, much larger than the â€œrealâ€
number of topics. We compared LeadLDA with
the following 5 state-of-the-art basedlines.

TreeLDA: Analogous to Zhao et al. (2011),
where they aggregated messages posted by the
same author, TreeLDA aggregates messages from
one conversation tree as a pseudo-document. Ad-
ditionally, it includes a background word distribu-
tion to capture non-topic words controlled by a
general Beta prior without differentiating leaders
and followers. TreeLDA can be considered as a
degeneration of LeadLDA, where topics assigned
to all messages are generated from the topic distri-
butions of the conversation trees they are on.

StructLDA: It is another variant of LeadLDA,
where topics assigned to all messages are gener-
ated based on topic transitions from their parents.
The strTM (Wang et al., 2011) utilized a similar
model to capture the topic dependencies of adja-
cent sentences in a document. Following strTM,
we add a dummy topic Tstart emitting no word to
the â€œpseudo parentsâ€ of root messages. Also, we
add the same background word distribution to cap-
ture non-topic words as TreeLDA does.

BTM: Biterm Topic Model (BTM)6 (Yan et
al., 2013) directly models topics of all word pairs
(biterms) in each post, which outperformed LDA,
Mixture of Unigrams model, and the model pro-
posed by Zhao et al. (2011) that aggregated posts
by authorship to enrich context.

SATM: A general unified model proposed by
Quan et al. (2015) that aggregates documents and
infers topics simultaneously. We implemented
SATM and examined its effectiveness specifically
on microblog data.

GMTM: To tackle word sparseness, Sridhar
et al. (2015) utilized Gaussian Mixture Model
(GMM) to cluster word embeddings generated by
a log-linear word2vec model7.

The hyper-parameters of BTM, SATM and
GMTM were set according to the best hyper-
parameters reported in their original papers. For
TreeLDA and StructLDA, the parameter settings
were kept the same as LeadLDA since they are its

6https://github.com/xiaohuiyan/BTM
7https://code.google.com/archive/p/

word2vec/

2119



variants. And the background switchers were pa-
rameterized by symmetric Beta prior on 0.5, fol-
lowing Chemudugunta et al. (2006). We ran Gibbs
samplings (in LeadLDA, TreeLDA, StructLDA,
BTM and SATM) and EM algorithm (in GMTM)
with 1,000 iterations to ensure convergence.

Topic model evaluation is inherently difficult.
In previous works, perplexity is a popular metric
to evaluate the predictive abilities of topic mod-
els given held-out dataset with unseen words (Blei
et al., 2003b). However, Chang et al. (2009)
have demonstrated that models with high perplex-
ity do not necessarily generate semantically co-
herent topics in human perception. Therefore, we
conducted objective and subjective analysis on the
coherence of produced topics.

5.1 Objective Analysis
The quality of topics is commonly measured by
coherence scores (Mimno et al., 2011), assuming
that words representing a coherent topic are likely
to co-occur within the same document. However,
due to the severe sparsity of short text posts, we
modify the calculation of commonly-used topic
coherence measure based on word co-occurrences
in messages tagged with the same hashtag, named
as hashtag-document, assuming that those mes-
sages discuss related topics8.

Specifically, we calculate the coherence score of
a topic given the topN words ranked by likelihood
as below:

C = 1
K
Â·
Kâˆ‘
k=1

Nâˆ‘
i=2

iâˆ’1âˆ‘
j=1

log
D(wki , w

k
j ) + 1

D(wkj )
, (3)

where wki represents the i-th word in topic k
ranked by p(w|k), D(wki , wkj ) refers to the count
of hashtag-documents where word wki and w

k
j co-

occur, and D(wki ) denotes the number of hashtag-
documents that contain word wki .

Table 3 shows the absolute values of C scores
for topics produced on three evaluation datasets
(May, June and July), and the top 10, 15, 20 words
of topics were selected for evaluation. Lower
scores indicate better coherence in the induced
topic.

We have the following observations:
â€¢ GMTM gave the worst coherence scores,

which may be ascribed to its heavy reliance on rel-
evant large-scale high-quality external data, with-

8We sampled posts and their corresponding hashtags in
our evaluation set and found only 1% mismatch.

N Model May June JulyK50 K100 K50 K100 K50 K100

10

TREE 27.9 30.5 24.0 23.8 23.9 26.1
STR 29.9 30.8 24.0 24.1 24.4 26.4

BTM 26.7 28.9 27.8 25.5 25.4 25.2
SATM 30.6 29.9 23.8 23.7 24.3 27.5

GMTM 40.8 40.1 44.0 44.2 41.7 40.8
LEAD 28.4 26.9 19.8 23.4 22.6 25.1

15

TREE 71.9 76.4 55.3 60.4 61.2 66.2
STR 76.4 74.1 57.6 62.2 58.1 61.1

BTM 69.6 71.4 58.5 60.3 59.1 63.0
SATM 74.3 73.0 54.8 60.4 61.2 65.3

GMTM 96.4 93.1 100.4 105.1 94.6 94.9
LEAD 67.4 65.2 52.8 57.7 55.3 57.8

20

TREE 138.8 138.6 102.0 115.0 115.8 119.7
STR 134.0 136.9 104.3 112.7 111.0 117.3

BTM 125.2 131.1 109.4 115.7 115.3 120.2
SATM 134.6 131.9 105.5 114.3 113.5 118.9

GMTM 173.5 169.0 184.7 190.9 167.4 171.2
LEAD 120.9 127.2 101.6 106.0 97.2 104.9

Table 3: Absolute values of coherence scores.
Lower is better. K50: 50 topics; K100: 100 topics;
N: # of top words ranked by topic-word probabil-
ities; TREE: TreeLDA; STR: StructLDA; LEAD:
LeadLDA.

out which the trained word embedding model
failed to capture meaningful semantic features for
words, and hence could not yield coherent topics.

â€¢ TreeLDA and StructLDA produced competi-
tive results compared to the state-of-the-art base-
line models, which indicates the effectiveness of
using conversation structures to enrich context and
thus generate topics of reasonably good quality.

â€¢ The coherence of topics generated by
LeadLDA outperformed all the baselines on the
three datasets, most of time by large margins
and was only outperformed by BTM on the May
dataset when K = 50 and N = 10. The gen-
erally higher performance of LeadLDA is due
to three reasons: 1) It effectively identifies top-
ics using the conversation tree structures, which
provide richer context information; 2) It jointly
models the topics of leaders and the topic depen-
dencies of other messages on a tree. TreeLDA
and StructLDA, each only considering one of
the factors, performed worse than LeadLDA; 3)
LeadLDA separately models the probabilities of
leaders and followers containing topical or non-
topic words while the baselines only model the
general background information regardless of the
different types of messages. This implies that
leaders and followers do have different capaci-
ties in covering key topical words or background
noise, which is useful to identify key words for
topic representation.

2120



TreeLDA StructLDA BTM SATM LeadLDA

é¦™æ¸¯å¾®åšé©¬èˆªå®¶å±è¯
å®å…¥å¢ƒå¤„å®¢æœºæ¶ˆæ¯æ›¹
æ ¼æŠ•ç»™äºŒèƒé€‰é¡¹æ•™çˆ¶
æ»‹å…»é£æœºå¤–å›½å¿ƒæƒ…å 
æ¯ç”·å­åŒèƒ

ä¹Œå…‹å…°èˆªç©ºäº²çˆ±å›½æ°‘
ç»•å¼€é£è¡Œèˆªç­é¢†ç©ºæ‰€
æœ‰é¿å¼€å®£å¸ƒç©ºåŸŸä¸œéƒ¨
ä¿„ç½—æ–¯ç»ˆäºå¿˜è®°å…¬å¸
ç»æœ›çœ‹çœ‹çè´µ

é¦™æ¸¯å…¥å¢ƒå¤„å®¶å±è¯å®
ç”·å­æŠ¤ç…§å¤–å›½æ¶ˆæ¯å 
æ¯é©¬èˆªæŠ¥é“è”ç³»ç”µå°
å®¢æœºé£æœºåŒèƒç¡®è®¤äº‹
ä»¶éœå®¶ç›´æ¥

é©¬èˆªç¥ˆç¥·å®‰æ¯ç”Ÿå‘½é€
è€…ä¸–ç•Œè‰¾æ»‹ç—…ææ€–å¹¿
å·é£æœºæ— è¾œé»˜å“€è¿œç¦»
äº‹ä»¶å‡»è½å…¬äº¤è½¦ä¸­å›½
äººå›½é™…æ„¿é€è€…çœŸçš„

ä¹Œå…‹å…°é©¬èˆªå®¢æœºå‡»è½
é£æœºå æ¯å¯¼å¼¹ä¿„ç½—æ–¯
æ¶ˆæ¯ä¹˜å®¢ä¸­å›½é©¬æ¥è¥¿
äºšé¦™æ¸¯é‡éš¾äº‹ä»¶æ­¦è£…
èˆªç­ææ€–ç›®å‰è¯å®

Hong Kong, microblog,
family, confirm,
immigration, airliner,
news, Grey Chow, vote,
second baby, choice, god
father, nourish, airplane,
foreign, feeling, crash,
man, fellowman

Ukraine, airline, dear, 
national, bypass, fly,
flight, airspace, all, avoid, 
announce, airspace,
eastern, Russia, finally,
forget, company, 
disappointed, look,
valuable

Hong Kong, immigration,
family, confirm, man,
passport, foreign, news,
crash, Malaysia Airlines, 
report, contact, broadcast
station, airliner, airplane, 
fellowman, confirm,
event, Fokâ€™s family,
directly

Malaysia Airlines, prey,
rest in peace, life, dead, 
world, AIDS, terror,
Guangzhou, airplane,
innocent, silent tribute,
keep away from, event,
shoot down, bus, Chinese, 
international, wish the
dead, really

Ukraine, Malaysia
Airlines, airliner, shoot
down, airplane, crash,
missile, Russia, news,
passenger, China, 
Malaysia, Hong Kong,
killed, event, militant,
flight, terror, current,
confirm

Figure 3: The extracted topics describing MH17 crash. Each column represents the similar topic gener-
ated by the corresponding model with the top 20 words. The 2nd row: original Chinese words; The 3rd
row: English translations.

5.2 Subjective Analysis

To evaluate the coherence of induced topics from
human perspective, we invited two annotators to
subjectively rate the quality of every topic (by dis-
playing the top 20 words) generated by different
models on a 1-5 Likert scale. A higher rating in-
dicates better quality of topics. The Flessâ€™s Kappa
of annotatorsâ€™ ratings measured for various models
on different datasets given K = 50 and 100 range
from 0.62 to 0.70, indicating substantial agree-
ments (Landis and Koch, 1977).

Table 4 shows the overall subjective ratings.
We noticed that humans preferred topics pro-
duced given K = 100 to K = 50, but coher-
ence scores gave generally better grades to mod-
els for K = 50, which matched the number of
topics in ground truth. This is because models
more or less mixed more common words when
K is larger. Coherence score calculation (Equa-
tion (3)) penalizes common words that occur in
many documents, whereas humans could some-
how â€œguessâ€ the meaning of topics based on the
rest of words thus gave relatively good ratings.
Nevertheless, annotators gave remarkably higher
ratings to LeadLDA than baselines on all datasets
regardless of K being 50 or 100, which con-
firmed that LeadLDA effectively yielded good-
quality topics.

For a detailed analysis, Figure 3 lists the top 20
words about â€œMH17 crashâ€ induced by different
models9 when K = 50. We have the following

9As shown in Table 3 and 4, the topic coherence scores
of GMTM were the worst. Hence, the topic generated by

Model May June JulyK50 K100 K50 K100 K50 K100
TREE 3.12 3.41 3.42 3.44 3.03 3.48
STR 3.05 3.45 3.38 3.48 3.08 3.53
BTM 3.04 3.26 3.40 3.37 3.15 3.57
SATM 3.08 3.43 3.30 3.55 3.09 3.54
GMTM 2.02 2.37 1.99 2.27 1.97 1.90
LEAD 3.40 3.57 3.52 3.63 3.55 3.72

Table 4: Subjective ratings of topics. The mean-
ings of K50, K100, TREE, STR and LEAD are the
same as in Table 3.

observations:
â€¢ BTM, based on word-pair co-occurrences,

mistakenly grouped â€œFokâ€™s familyâ€ (a tycoon fam-
ily in Hong Kong), which co-occurred frequently
with â€œHong Kongâ€ in other topics, into the topic of
â€œMH17 crashâ€. â€œHong Kongâ€ is relevant here as a
Hong Kong passenger died in the MH17 crash.
â€¢ The topical words generated by SATM were

mixed with words relevant to the bus explosion in
Guangzhou, since it aggregated messages accord-
ing to topic affinities based on the topics learned in
the previous step. Thus the posts about bus explo-
sion and MH17 crash, both pertaining to disasters,
were aggregated together mistakenly, which gen-
erated spurious topic results.
â€¢ Both TreeLDA and StructLDA generated

topics containing non-topic words like â€œmi-
croblogâ€ and â€œdearâ€. This means that without
distinguishing leaders and followers, it is diffi-
cult to filter out non-topic words. The topic qual-
ity of StructLDA nevertheless seems better than

GMTM is not shown due to space limitation.

2121



TreeLDA, which implies the usefulness of exploit-
ing topic dependencies of posts in conversation
structures.
â€¢ LeadLDA not only produced more semanti-

cally coherent words describing the topic, but also
revealed some important details, e.g., MH17 was
shot down by a missile.

6 Conclusion and Future Works

This paper has proposed a novel topic model by
considering the conversation tree structures of mi-
croblog posts. By rigorously comparing our pro-
posed model with a number of competitive base-
lines on real-world microblog datasets, we have
demonstrated the effectiveness of using conversa-
tion structures to help model topics embedded in
short and colloquial microblog messages.

This work has proven that detecting leaders and
followers, which are coarse-grained discourse de-
rived from conversation structures, is useful to
model microblogging topics. In the next step, we
plan to exploit fine-grained discourse structures,
e.g., dialogue acts (Ritter et al., 2010), and propose
a unified model that jointly inferring discourse
roles and topics of posts in context of conversa-
tion tree structures. Another extension is to ex-
tract topic hierarchies by integrating the conversa-
tion structures into hierarchical topic models like
HLDA (Blei et al., 2003a) to extract fine-grained
topics from microblog posts.

Acknowledgment

This work is supported by General Research
Fund of Hong Kong (417112), the Innova-
tion and Technology Fund of Hong Kong SAR
(ITP/004/16LP), Shenzhen Peacock Plan Re-
search Grant (KQCX20140521144507925) and
Innovate UK (101779). We would like to thank
Shichao Dong for his efforts on data process-
ing and anonymous reviewers for the useful com-
ments.

References
David M. Blei, Thomas L. Griffiths, Michael I. Jor-

dan, and Joshua B. Tenenbaum. 2003a. Hierarchi-
cal topic models and the nested chinese restaurant
process. In Proceedings of the 17th Annual Con-
ference on Neural Information Processing Systems,
NIPS, pages 17â€“24.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.

2003b. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993â€“1022.

Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of the 23rd Annual Conference on Neural
Information Processing Systems, NIPS, pages 288â€“
296.

Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific
aspects of documents with a probabilistic topic
model. In Proceedings of the 20th Annual Con-
ference on Neural Information Processing Systems,
NIPS, pages 241â€“248.

Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl 1):5228â€“5235.

Thomas L. Griffiths, Mark Steyvers, David M. Blei,
and Joshua B. Tenenbaum. 2004. Integrating top-
ics and syntax. In Proceedings of the 18th Annual
Conference on Neural Information Processing Sys-
tems, NIPS, pages 537â€“544.

Tom Griffiths. 2002. Gibbs sampling in the generative
model of latent dirichlet allocation.

Sanda M. Harabagiu and Andrew Hickl. 2011. Rel-
evance modeling for microblog summarization. In
Proceedings of the 5th International Conference on
Web and Social Media, ICWSM.

Thomas Hofmann. 1999. Probabilistic latent seman-
tic indexing. In In Proceedings of the 22nd Annual
International, ACM SIGIR, pages 50â€“57.

Liangjie Hong and Brian D Davison. 2010. Empirical
study of topic modeling in twitter. In Proceedings of
the first workshop on social media analytics, pages
80â€“88.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning, ICML,
pages 282â€“289.

J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159â€“174.

Jing Li, Wei Gao, Zhongyu Wei, Baolin Peng, and
Kam-Fai Wong. 2015. Using content-level struc-
tures for summarizing microblog repost trees. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, EMNLP,
pages 2168â€“2178.

Cindy Xide Lin, Bo Zhao, Qiaozhu Mei, and Jiawei
Han. 2010. PET: a statistical model for popular

2122



events tracking in social communities. In Proceed-
ings of the 16th International Conference on Knowl-
edge Discovery and Data Mining, ACM SIGKDD,
pages 929â€“938.

Rishabh Mehrotra, Scott Sanner, Wray L. Buntine, and
Lexing Xie. 2013. Improving LDA topic models for
microblogs via tweet pooling and automatic label-
ing. In Proceedings of the 36th International con-
ference on research and development in Information
Retrieval, ACM SIGIR, pages 889â€“892.

David M. Mimno, Hanna M. Wallach, Edmund M.
Talley, Miriam Leenders, and Andrew McCallum.
2011. Optimizing semantic coherence in topic mod-
els. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP, pages 262â€“272.

Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
Fudannlp: A toolkit for chinese natural language
processing. In 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL, pages
49â€“54.

Xiaojun Quan, Chunyu Kit, Yong Ge, and Sinno Jialin
Pan. 2015. Short and sparse text topic modeling via
self-aggregation. In Proceedings of the 24th Inter-
national Joint Conference on Artificial Intelligence,
IJCAI, pages 2270â€“2276.

Daniel Ramage, Susan T. Dumais, and Daniel J.
Liebling. 2010. Characterizing microblogs with
topic models. In Proceedings of the 4th Inter-
national Conference on Web and Social Media,
ICWSM.

Kenneth Rapoza. 2011. Chinaâ€™s weibos vs usâ€™s twitter:
And the winner is? Forbes (May 17, 2011).

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of the 2010 Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, NAACL, pages 172â€“180.

Vivek Kumar Rangarajan Sridhar. 2015. Unsupervised
entity linking with abstract meaning representation.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL-HLT, pages 1130â€“1139.

Xuerui Wang and Andrew McCallum. 2006. Top-
ics over time: a non-markov continuous-time model
of topical trends. In Proceedings of the 12th Inter-
national Conference on Knowledge Discovery and
Data Mining, ACM SIGKDD, pages 424â€“433.

Hongning Wang, Duo Zhang, and ChengXiang Zhai.
2011. Structural topic model for latent topical struc-
ture analysis. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics, ACL, pages 1526â€“1535.

Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010. Twitterrank: finding topic-sensitive influen-
tial twitterers. In Proceedings of the 3rd Interna-
tional Conference on Web Search and Web Data
Mining, WSDM, pages 261â€“270.

Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi
Cheng. 2013. A biterm topic model for short texts.
In Proceedings of the 22nd International World Wide
Web Conference, WWW, pages 1445â€“1456.

Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In Advances in Information Retrieval - 33rd
European Conference on IR Research, ECIR, pages
338â€“349.

2123


