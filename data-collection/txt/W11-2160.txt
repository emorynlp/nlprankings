










































Joshua 3.0: Syntax-based Machine Translation with the Thrax Grammar Extractor


Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478–484,
Edinburgh, Scotland, UK, July 30–31, 2011. c©2011 Association for Computational Linguistics

Joshua 3.0: Syntax-based Machine Translation
with the Thrax Grammar Extractor

Jonathan Weese1, Juri Ganitkevitch1, Chris Callison-Burch1, Matt Post2 and Adam Lopez1,2
1Center for Language and Speech Processing

2Human Language Technology Center of Excellence
Johns Hopkins University

Abstract

We present progress on Joshua, an open-
source decoder for hierarchical and syntax-
based machine translation. The main fo-
cus is describing Thrax, a flexible, open
source synchronous context-free grammar ex-
tractor. Thrax extracts both hierarchical (Chi-
ang, 2007) and syntax-augmented machine
translation (Zollmann and Venugopal, 2006)
grammars. It is built on Apache Hadoop for
efficient distributed performance, and can eas-
ily be extended with support for new gram-
mars, feature functions, and output formats.

1 Introduction

Joshua is an open-source1 toolkit for hierarchical
machine translation of human languages. The origi-
nal version of Joshua (Li et al., 2009) was a reim-
plementation of the Python-based Hiero machine-
translation system (Chiang, 2007); it was later ex-
tended (Li et al., 2010) to support richer formalisms,
such as SAMT (Zollmann and Venugopal, 2006).

The main focus of this paper is to describe this
past year’s work in developing Thrax (Weese, 2011),
an open-source grammar extractor for Hiero and
SAMT grammars. Grammar extraction has shown
itself to be something of a black art, with decod-
ing performance depending crucially on a variety
of features and options that are not always clearly
described in papers. This hindered direct com-
parison both between and within grammatical for-
malisms. Thrax standardizes Joshua’s grammar ex-

1http://github.com/joshua-decoder/joshua

traction procedures by providing a flexible and con-
figurable means of specifying these settings. Sec-
tion 3 presents a systematic comparison of the two
grammars using identical feature sets.

In addition, Joshua now includes a single pa-
rameterized script that implements the entire MT
pipeline, from data preparation to evaluation. This
script is built on top of a module called CachePipe.
CachePipe is a simple wrapper around shell com-
mands that uses SHA-1 hashes and explicitly-
provided lists of dependencies to determine whether
a command needs to be run, saving time both in run-
ning and debugging machine translation pipelines.

2 Thrax: grammar extraction

In modern machine translation systems such as
Joshua (Li et al., 2009) and cdec (Dyer et al., 2010),
a translation model is represented as a synchronous
context-free grammar (SCFG). Formally, an SCFG
may be considered as a tuple

(N,S, Tσ, Tτ , G)

where N is a set of nonterminal symbols of the
grammar, S ∈ N is the goal symbol, Tσ and Tτ
are the source- and target-side terminal symbol vo-
cabularies, respectively, and G is a set of production
rules of the grammar.

Each rule in G is of the form

X → 〈α, γ,∼〉

where X ∈ N is a nonterminal symbol, α is a se-
quence of symbols from N ∪ Tσ, γ is a sequence of

478



symbols from N ∪ Tτ , and ∼ is a one-to-one cor-
respondence between the nonterminal symbols of α
and γ.

The language of an SCFG is a set of ordered pairs
of strings. During decoding, the set of candidate
translations of an input sentence f is the set of all
e such that the pair (f, e) is licensed by the transla-
tion model SCFG. Each candidate e is generated by
applying a sequence of production rules (r1 . . . rn).
The cost of applying each rule is:

w(X → 〈α, γ〉) =
∏
i

φi(X → 〈α, γ〉)λi (1)

where each φi is a feature function and λi is the
weight for φi. The total translation model score of
a candidate e is the product of the rules used in its
derivation. This translation model score is then com-
bined with other features (such as a language model
score) to produce an overall score for each candidate
translation.

2.1 Hiero and SAMT
Throughout this work, we will reference two par-
ticular SCFG types known as Hiero and Syntax-
Augmented Machine Translation (SAMT).

A Hiero grammar (Chiang, 2007) is an SCFG
with only one type of nonterminal symbol, tradi-
tionally labeled X . A Hiero grammar can be ex-
tracted from a parallel corpus of word-aligned sen-
tence pairs as follows: If (f ji , e

l
k) is a sub-phrase

of the sentence pair, we say it is consistent with
the pair’s alignment if none of the words in f ji are
aligned to words outside of elk, and vice-versa. The
consistent sub-phrase may be extracted as an SCFG
rule. Furthermore, if a consistent phrase is contained
within another one, a hierarchical rule may be ex-
tracted by replacing the smaller piece with a nonter-
minal.

An SAMT grammar (Zollmann and Venugopal,
2006) is similar to a Hiero grammar, except that the
nonterminal symbol set is much larger, and its la-
bels are derived from a parse tree over either the
source or target side in the following manner. For
each rule, if the target side is spanned by one con-
stituent of the parse tree, we assign that constituent’s
label as the nonterminal symbol for the rule. Other-
wise, we assign an extended category of the form
C1 + C2, C1/C2, or C2 \C1 — indicating that the

das begrüße ich sehr .

i very much welcome this .

PRP

NP

S

RB RB

ADVP
VP

VBP DT

NP

.

Figure 1: An aligned sentence pair.

target side spans two adjacent constituents, is a C1
missing a C2 to the right, or is a C1 missing a C2
on the left, respectively. Table 1 contains a list of
Hiero and SAMT rules extracted from the training
sentence pair in Figure 1.

2.2 System overview
The following were goals in the design of Thrax:

• the ability to extract different SCFGs (such as
Hiero and SAMT), and to adjust various extrac-
tion parameters for the grammars;

• the ability to easily change and extend the fea-
ture sets for each rule

• scalability to arbitrarily large training corpora.

Thrax treats the grammar extraction and scoring
as a series of dependent Hadoop jobs. Hadoop
(Venugopal and Zollmann, 2009) is an implementa-
tion of Google’s MapReduce (Dean and Ghemawat,
2004), a framework for distributed processing of
large data sets. Hadoop jobs have two parts. In the
map step, a set of key/value pairs is mapped to a set
of intermediate key/value pairs. In the reduce step,
all intermediate values associated with an interme-
diate key are merged.

The first step in the Thrax pipeline is to extract all
the grammar rules. The map step in this job takes as
input word-aligned sentence pairs and produces a set
of ordered pairs (r, c) where r is a rule and c is the
number of times it was extracted. During the reduce
step, these rule counts are summed, so the result is
a set of rules, along with the total number of times
each rule was extracted from the entire corpus.

479



Span Hiero SAMT
[1, 3] X → 〈sehr, very much〉 ADV P → 〈sehr, very much〉
[0, 3] X → 〈X sehr, X very much〉 PRP +ADV P → 〈PRP sehr, PRP very much〉
[3, 4] X → 〈begrüße,welcome〉 V BP → 〈begrüße,welcome〉
[0, 6] X → 〈X ich sehr ., i very much X .〉 S → 〈V P ich sehr ., i very much V P .〉
[0, 6] X → 〈X ., X .〉 S → 〈S/. ., S/. .〉

Table 1: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.

Given the rules and their counts, a separate
Hadoop job is run for each feature. These jobs can
all be submitted at once and run in parallel, avoid-
ing the linear sort-and-score workflow. The output
from each feature job is the same set of pairs (r, c)
as the input, except each rule r has been annotated
with some feature score f .

After the feature jobs have been completed, we
have several copies of the grammar, each of which
has been scored with one feature. A final Hadoop
job combines all these scores to produce the final
grammar.

Some users may not have access to a Hadoop
cluster. Thrax can be run in standalone or pseudo-
distributed mode on a single machine. It can also
be used with Amazon Elastic MapReduce,2 a web
service that provides computation time on a Hadoop
cluster on-demand.

2.3 Extraction
The first step in the Thrax workflow is the extraction
of grammar rules from an input corpus. As men-
tioned above, Hiero and SAMT grammars both re-
quire a parallel corpus with word-level alignments.
SAMT additionally requires that the target side of
the corpus be parsed.

There are several parameters that can make a sig-
nificant difference in a grammar’s overall translation
performance. Each of these parameters is easily ad-
justable in Thrax by changing its value in a configu-
ration file.

• maximum rule span

• maximum span of consistent phrase pairs

• maximum number of nonterminals

• minimum number of aligned terminals in rule
2http://aws.amazon.com/elasticmapreduce/

• whether to allow adjacent nonterminals on
source side

• whether to allow unaligned words at the edges
of consistent phrase pairs

Chiang (2007) gives reasonable heuristic choices
for these parameters when extracting a Hiero gram-
mar, and Lopez (2008) confirms some of them (max-
imum rule span of 10, maximum number of source-
side symbols at 5, and maximum number of non-
terminals at 2 per rule). ?) provided comparisons
among phrase-based, hierarchical, and syntax-based
models, but did not report extensive experimentation
with the model parameterizations.

When extracting Hiero- or SAMT-style gram-
mars, the first Hadoop job in the Thrax workflow
takes in a parallel corpus and produces a set of rules.
But in fact Thrax’s extraction mechanism is more
general than that; all it requires is a function that
maps a string to a set of rules. This makes it easy
to implement new grammars and extract them using
Thrax.

2.4 Feature functions

Thrax considers feature functions of two types: first,
there are features that can be calculated by looking
at each rule in isolation. Such features do not re-
quire a Hadoop job to calculate their scores, since
we may inspect the rules in any order. (In practice,
we calculate the scores at the very last moment be-
fore outputting the final grammar.) We call these
features simple features. Thrax implements the fol-
lowing simple features:

• a binary indicator functions denoting:

– whether the rule is purely abstract (i.e.,
has no terminal symbols)

480



– the rule is purely lexical (i.e., has no non-
terminals)

– the rule is monotonic or has reordering
– the rule has adjacent nonterminals on the

source side

• counters for

– the number of unaligned words in the rule
– the number of terminals on the target side

of the rule

• a constant phrase penalty

In addition to simple features, Thrax also imple-
ments map-reduce features. These are features that
require comparing rules in a certain order. Thrax
uses Hadoop to sort the rules efficiently and calcu-
late these feature functions. Thrax implements the
following map-reduce features:

• Phrasal translation probabilities p(α|γ) and
p(γ|α), calculated with relative frequency:

p(α|γ) = C(α, γ)
C(γ)

(2)

(and vice versa), where C(·) is the number of
times a given event was extracted.

• Lexical weighting plex(α|γ,A) and
plex(γ|α,A). We calculate these weights
as given in (Koehn et al., 2003): let A be the
alignment between α and γ, so (i, j) ∈ A if
and only if the ith word of α is aligned to the
jth word of γ. Then we can define plex(γ|α) as

n∏
i=1

1

|{j : (i, j) ∈ A}|
∑

(i,j)∈A

w(γj |αi) (3)

where αi is the ith word of α, γj is the jth word
of γ, and w(y|x) is the relative frequency of
seeing word y given x.

• Rarity penalty, given by

exp(1− C(X → 〈α, γ〉)) (4)

where again C(·) is a count of the number of
times the rule was extracted.

The above features are all implemented and can
be turned on or off with a keyword in the Thrax con-
figuration file.

It is easy to extend Thrax with new feature func-
tions. For simple features, all that is needed is to im-
plement Thrax’s SIMPLEFEATURE interface defin-
ing a method that takes in a rule and calculates a
feature score. Map-reduce features are slightly more
complex: to subclass MAPREDUCEFEATURE, one
must define a mapper and reducer, but also a sort
comparator to determine in what order the rules are
compared during the reduce step.

2.5 Related work
Joshua includes a simple Hiero extractor (Schwartz
and Callison-Burch, 2010). The extractor runs as a
single Java process, which makes it difficult to ex-
tract larger grammars, since the host machine must
have enough memory to hold all of the rules at once.
Joshua’s extractor scores each rule with three feature
functions — lexical probabilities in two directions,
and one phrasal probability score p(γ|α).

The SAMT implementation of Zollmann and
Venugopal (2006) includes a several-thousand-line
Perl script to extract their rules. In addition to
phrasal and lexical probabilities, this extractor im-
plements several other features that are also de-
scribed in section 2.4.

Finally, the cdec decoder (Dyer et al., 2010) in-
cludes a grammar extractor that performs well only
when all rules can be held in memory.

Memory usage is a limitation of both the Joshua
and cdec extractors. Translation models can be very
large, and many feature scores require accumulation
of statistical data from the entire set of extracted
rules. Since it is impractical to keep the entire gram-
mar in memory, rules are usually sorted on disk and
then read sequentially. Different feature calcula-
tions may require different sort orders, leading to a
linear workflow that alternates between sorting the
grammar and calculating a feature score. To cal-
culate more feature scores, more sorts have to be
performed. This discourages the implementation of
new features. For example, Joshua’s built-in rule ex-
tractor calculates the phrasal probability p(γ|α) for
each rule but, to save time, does not calculate its ob-
vious counterpart p(α|γ), which would require an-
other sort.

481



Language pair sentences (K) words (M)
cs–en 332 4.7
de–en 279 5.5
en–cs 487 6.9
en–de 359 7.2
en–fr 682 12.5
fr–en 792 14.4

Table 2: Training data size after subsampling.

The SAMT extractor does not have a problem
with large data sets; SAMT can run on Hadoop, as
Thrax does.

The Joshua and cdec extractors only extract Hiero
grammars, and Zollmann and Venugopal’s extractor
can only extract SAMT-style grammars. They are
not designed to score arbitrary feature sets, either.
Since variation in translation models and feature sets
can have a significant effect on translation perfor-
mance, we have developed Thrax in order to make it
easy to build and test new models.

3 Experiments

We built systems for six language pairs for the WMT
2011 shared task: cz-en, en-cz, de-en, en-de, fr-en,
and en-fr.3 For each language pair, we built both
SAMT and hiero grammars.4 Table 3 contains the
results on the complete WMT 2011 test set.

To train the translation models, we used the pro-
vided Europarl and news commentary data. For cz-
en and en-cz, we also used sections of the CzEng
parallel corpus (Bojar and Žabokrtský, 2009). The
parallel data was subsampled using Joshua’s built-
in subsampler to select sentences with n-grams rel-
evant to the tuning and test set. We used SRILM
to train a 5-gram language model with Kneser-Ney
smoothing using the appropriate side of the paral-
lel data. For the English LM, we also used English
Gigaword Fourth Edition.5

Before extracting an SCFG with Thrax, we used
the provided Perl scripts to tokenize and normalize

3fr=French, cz=Czech, de=German, en=English.
4Except for fr-en and en-fr. We were unable to decode with

SAMT grammars for these language pairs due to their large size.
We have since resolved this issue and will have scores for the
final version of the paper.

5LDC2009T13

pair hiero SAMT improvement
cz-en 21.1 21.7 +0.6
en-cz 16.8 16.9 +0.1
de-en 18.9 19.5 +0.6
en-de 14.3 14.9 +0.6
fr-en 28.0 - -
en-fr 30.4 - -

Table 3: Single-reference BLEU-4 scores.

the data. We also removed any sentences longer than
50 tokens (after tokenization). For SAMT grammar
extraction, we parsed the English training data us-
ing the Berkeley Parser (Petrov et al., 2006) with the
provided Treebank-trained grammar.

We tuned the model weights against the
WMT08 test set (news-test2008) using Z-
MERT (Zaidan, 2009), an implementation of mini-
mum error-rate training included with Joshua. We
decoded the test set to produce a 300-best list of
unique translations, then chose the best candidate for
each sentence using Minimum Bayes Risk reranking
(Kumar and Byrne, 2004). Figure 2 shows an exam-
ple derivation with an SAMT grammar. To re-case
the 1-best test set output, we trained a true-case 5-
gram language model using the same LM training
data as before, and used an SCFG translation model
to translate from the lowercased to true-case output.
The translation model used rules limited to five to-
kens in length, and contained no hierarchical rules.

4 CachePipe: Cached pipeline runs

Machine translation pipelines involve the specifica-
tion and execution of many different datasets, train-
ing procedures, and pre- and post-processing tech-
niques that can have large effects on translation out-
come, and which make direct comparisons between
systems difficult. The complexity of managing these
pipelines and experimental environments has led to a
number of different experimental management sys-
tems, such as Experiment.perl,6 Joshua 2.0’s Make-
file system (Li et al., 2010), and LoonyBin (Clark
and Lavie, 2010). In addition to managing the
pipeline, these scripts employ different techniques
to avoid expensive recomputation by caching steps.

6http://www.statmt.org/moses/?n=
FactoredTraining.EMS

482



the reactor type will be operated with uranium

VBN

DT+NP

GLUE

VP

PP

der reaktortyp , das nicht angereichertwird zwar mit uran betrieben

, which is not enriched

ist .

NP

GLUE

NN

COMMA+SBAR+.

ADJP

JJ

.

S

VBN

DT+NP

GLUE

VP

PP

NP

GLUE

NN

COMMA+SBAR+.

ADJP

JJ

S

Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals
and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.

However, these approaches are based on simple but
unreliable heuristics (such as timestamps or file ex-
istence) to make the caching determination.

Our solution to the caching dependency problem
is CachePipe. CachePipe is designed with the fol-
lowing goals: (1) robust content-based dependency
checking and (2) ease of use, including minimal
editing of existing scripts. CachePipe is essentially
a wrapper around command invocations. Presented
with a command to run and a list of file dependen-
cies, it computes SHA-1 hashes of the dependencies
and of the command invocation and stores them; the
command is executed only if any of those hashes are
different from previous runs. A basic invocation in-
volves specifying (1) a name or identifier associated
with the command or step, (2) the command to run,
and (3) a list of file dependencies. For example, to
copy file a to b from a shell prompt, the following
command could be used:

cachecmd copy "cp a b" a b

The first time the command is run, the file would be
copied; afterwards, the command would be skipped
after CachePipe verified that the contents of the de-
pendencies a and b had not changed.

CachePipe is open-source software, distributed

with Joshua or available separately.7 It currently
provides both a shell script interface and a program-
matic API for Perl. It accepts a number of other
arguments and dependency types. It also serves as
the foundation of a new script in Joshua 3.0 that im-
plements the complete Joshua pipeline, from data
preparation to evaluation.

5 Future work

Thrax is currently limited to SCFG-based translation
models. A natural development would be to extract
GHKM grammars (Galley et al., 2004) or more re-
cent tree-to-tree models (Zhang et al., 2008; Liu et
al., 2009; Chiang, 2010). We also hope that Thrax
will continue to be extended with more feature func-
tions as researchers develop and contribute them.

Acknowledgements

This research was supported by in part by the Eu-
roMatrixPlus project funded by the European Com-
mission (7th Framework Programme), and by the
NSF under grant IIS-0713448. Opinions, interpre-
tations, and conclusions are the authors’ alone.

7https://github.com/joshua-decoder/
cachepipe

483



References
Ondřej Bojar and Zdeněk Žabokrtský. 2009. CzEng0.9:

Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92. in
print.

David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.

David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, Uppsala, Sweden,
July.

Jonathan H. Clark and Alon Lavie. 2010. Loony-
bin: Keeping language technologists sane through au-
tomated management of experimental (hyper) work-
flows. In Proc. LREC.

Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In OSDI.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc.
ACL 2010 System Demonstrations, pages 7–12.

Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Morristown, NJ, USA.

Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.

Yang Liu, Yajuan Lü, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
ACL, Suntec, Singapore, August.

Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. COLING, Manchester, UK,
August.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL, Sydney, Aus-
tralia, July.

Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua:
Suffix arrays and prefix trees. The Prague Bulletin of
Mathematical Linguistics, 93:157–166, January.

Mark Steedman. 1999. Alternating quantifier scope in
ccg. In Proc. ACL, Stroudsburg, PA, USA.

Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67–78.

Jonathan Weese. 2011. A systematic comparison of syn-
chronous context-free grammars for machine transla-
tion. Master’s thesis, Johns Hopkins University, May.

Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.

Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL, Columbus, Ohio, June.

Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. NAACL Workshop on Statistcal Machine Trans-
lation, New York, New York.

484


