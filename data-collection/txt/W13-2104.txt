










































Enhancing the Expression of Contrast in the SPaRKy Restaurant Corpus


Proceedings of the 14th European Workshop on Natural Language Generation, pages 30–39,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

Enhancing the Expression of Contrast
in the SPaRKy Restaurant Corpus

David M. Howcroft and Crystal Nakatsu and Michael White
Department of Linguistics
The Ohio State University

Columbus, OH 43210, USA
{howcroft,cnakatsu,mwhite}@ling.osu.edu

Abstract

We show that Nakatsu & White’s (2010)
proposed enhancements to the SPaRKy
Restaurant Corpus (SRC; Walker et al.,
2007) for better expressing contrast do in-
deed make it possible to generate better
texts, including ones that make effective
and varied use of contrastive connectives
and discourse adverbials. After first pre-
senting a validation experiment for natu-
ralness ratings of SRC texts gathered using
Amazon’s Mechanical Turk, we present
an initial experiment suggesting that such
ratings can be used to train a realization
ranker that enables higher-rated texts to be
selected when the ranker is trained on a
sample of generated restaurant recommen-
dations with the contrast enhancements
than without them. We conclude with a
discussion of possible ways of improving
the ranker in future work.

1 Introduction

To lessen the need for handcrafting in developing
generation systems, Walker et al. (2007) extended
the overgenerate-and-rank methodology (Langk-
ilde and Knight, 1998; Mellish et al., 1998; Walker
et al., 2002; Nakatsu and White, 2006) to complex
information presentation tasks involving variation
in rhetorical structure. They illustrated their ap-
proach by developing SPaRKy (Sentence Planning
with Rhetorical Knowledge), a sentence planner
for generating restaurant recommendations and
comparisons in the context of the MATCH (Mul-
timodal Access To City Help) system (Walker et
al., 2004), and showed that SPaRKY can produce
texts comparable to those of MATCH’s template-
based generator.

Despite the evident importance of expressing
contrast clearly in making comparisons among

restaurants, Nakatsu (2008) surprisingly found
that most of the examples involving contrastive
connectives in the SPaRKy Restaurant Corpus
(SRC) received low ratings by the human judges.
Even though the low ratings were not necessarily
directly attributable to the use of a contrastive con-
nective in many cases, Nakatsu conjectured that
the large proportion of low-rated examples con-
taining contrastive connectives would make it dif-
ficult to train a ranker to learn to use contrastive
connectives effectively without augmenting the
corpus with better examples of contrast. Sub-
sequently, Nakatsu and White (2010) proposed
a set of enhancements to the SRC intended to
better express contrast—including ones employ-
ing multiple connectives in the same clause that
are problematic for RST (Mann and Thompson,
1988)—and showed how they could be generated
with Discourse Combinatory Categorial Grammar
(DCCG), an extension of CCG (Steedman, 2000)
designed to enable multi-sentence grammar-based
generation. However, Nakatsu and White did not
evaluate empirically whether these contrast en-
hancements were successful.

In this paper, we show that Nakatsu &
White’s (2010) proposed SRC contrast enhance-
ments do indeed make it possible to generate bet-
ter texts: in particular, we present an initial ex-
periment that shows that the oracle best restau-
rant recommendations including the contrast en-
hancements have significantly higher human rat-
ings for naturalness than comparable texts without
these enhancements, and which suggests that even
a basic n-gram ranker trained on the enhanced
recommendations can select texts with higher rat-
ings. The paper is structured as follows. In
Section 2, we review Nakatsu & White’s pro-
posed enhancements to the SRC for better express-
ing contrast—including the use of structural con-
nectives together with discourse adverbials—and
how they can be generated with DCCG. In Sec-

30



tion 3, we first present a validation experiment
showing that naturalness ratings gathered on Ama-
zon’s Mechanical Turk (AMT) are comparable to
those for the same texts in the original SRC; then,
we present our method of generating and selecting
a sample of new restaurant recommendation texts
with and without the contrast enhancements for
rating on AMT. In Section 4, we describe how we
trained discriminative n-gram rankers using cross
validation on the gathered ratings. In Section 5,
we present the oracle and cross validation results
in terms of mean scores of the top-ranked text. In
Section 6, we analyze how the individual contrast
enhancements affected the naturalness ratings and
discuss issues that may be still hampering natu-
ralness. Finally, in Section 7, we conclude with
a summary and a discussion of possible ways of
creating improved rankers in future work.

2 Enhancing Contrast with Discourse
Combinatory Categorial Grammar

Figure 1 (Nakatsu, 2008) shows examples from
the SRC where some of the SPaRKy realizations
are clearly more natural than others. In Nakatsu’s
experiments, she found that the use of contrastive
connectives was negatively correlated with human
ratings, and that an n-gram ranker learned to dis-
prefer texts containing these connectives. In an-
alyzing these unexpected results, Nakatsu noted
two factors that appeared to hamper the natural-
ness of the contrastive connective usage. First,
consistent with Grote et al.’s (1995) observation
that however and on the other hand (unlike but and
while) signal that the clause they attach to is the
more important one, we might expect realizations
to be preferred when these connectives appear
with the more desirable of the contrasted qualities.
Such preferences do indeed appear to be present
in the SRC: for example, in Figure 1, alts 8 &
13—where the better property is ordered second—
are rated highly, while alts 7 & 11—where the
better property is ordered first—are rated poorly.
Nakatsu further observed that in human-authored
comparisons, when the second clause expresses
the lesser property, it is often qualified by only
or just; consistent with this observation, alts 7 &
11 do seem to improve with the inclusion of these
modifiers.

The second factor noted by Nakatsu that may
contribute to the awkwardness of however and on
the other hand is that both of these connectives

seem to be rather “grand” for the rather simple
contrasts in Figure 1, and may sound more natu-
ral when used with heavier arguments.

Based on these observations, Nakatsu and
White (2010) proposed a set of enhancements to
the SRC, all of which are exemplified in Figure 2.1

The enhancements include (i) optional summary
statements that give an overall assessment of each
restaurant based on the average of their property
values, thereby allowing contrasts to be expressed
over larger text spans; (ii) adverbial modifiers
only, just and merely to express a lesser value of
a given property than one mentioned earlier;2 (iii)
the modifers also and too to signal the repetition
of the same value for a given property (Strieg-
nitz, 2004); and (iv) contrastive connectives for
different properties of the same restaurant, exem-
plified here by the contrast between decent decor
and mediocre food quality for Bienvenue.

In the text plan in Figure 2, <1>–<4> cor-
respond to the propositions in the original SRC
text plan and (1’)–(2’) are the new summary-level
propositions. Following Webber et al. (2003),
Nakatsu and White (2010) take only, merely, just,
also, and too to be discourse adverbials, whose
discourse relations are allowed to cut across the
primary tree structure established by the other re-
lations in the figure. Note that in addition to go-
ing beyond RST’s limitation to tree-structured dis-
courses, the example also contains clauses em-
ploying multiple discourse connectives, where one
is a structural connective (such as however or
while) and the other is a discourse adverbial.

To realize such texts, Nakatsu & White intro-
duce Discourse Combinatory Categorial Grammar
(DCCG), an extension of CCG (Steedman, 2000)
to the discourse level. DCCG follows Discourse
Lexicalized Tree Adjoining Grammar (Webber,
2004) in providing a lexicalized treatment of struc-
tural connectives and discourse adverbials, but dif-
fers in doing so in a single CCG, rather than sep-
arate sentence-level and discourse-level grammars
whose interaction is not straightforward. As such,
DCCG requires no changes to the OpenCCG real-
izer (White, 2006b; White, 2006a; White and Ra-

1In the text, words intended to help indicate similarities
and contrasts are italicized. Note that we have added overall
and on the whole to the summary statements to better indicate
their summarizing role.

2The second value must be a less extreme one on the same
side of the scale; in principle, it could be merely poor rather
than horrible, but such low attribute values did not occur in
the corpus.

31



Strategy Alt # Rating Rank Realization
3 3 7 Sonia Rose has very good decor but Bienvenue has decent decor.
7 1 16 Sonia Rose has very good decor. On the other hand, Bienvenue has decent decor.
8 4.5 13 Bienvenue has decent decor. Sonia Rose, on the other hand, has very good decor.

C2 10 4.5 5 Bienvenue has decent decor but Sonia Rose has very good decor.
11 1 12 Sonia Rose has very good decor. However, Bienvenue has decent decor.
13 5 14 Bienvenue has decent decor. However, Sonia Rose has very good decor.
14 5 3 Sonia Rose has very good decor while Bienvenue has decent decor.
15 4 4 Bienvenue has decent decor while Sonia Rose has very good decor.
17 1 15 Bienvenue’s price is 35 dollars. Sonia Rose’s price, however, is 51 dollars. Bienvenue has decent decor.

However, Sonia Rose has very good decor.

Figure 1: Some alternative [Alt] realizations of SPaRKy sentence plans from a COMPARE-2 [C2] plan, with averaged
human ratings [Rating] (5 = highest rating) and ranks assigned by the n-gram ranker [Rank] (1 = top ranked).

tion, the SPaRKy sentence plan generator adds the
INFER relation to assertions whose relations were
not specified by the content planner.

During the sentence planning phase, SPaRKy or-
ders the clauses and combines them using randomly
selected clause-combining operations. During this
process, a clause-combining operation may insert 1
of 7 connectives according to the RST relation that
holds between two discourse units (i.e. inserting
since or because for a JUSTIFY relation; and, how-
ever, on the other hand, while, or but for a CON-
TRAST relation; or and for an INFER relation).

After each sentence plan is generated, it is real-
ized by the RealPro surface realizer and the result-
ing realization is rated by two judges on a scale of
1-5, where 5 is highly preferred. These ratings are
then averaged, producing a range of 9 possible rat-
ings from {1, 1.5, ..., 5}.

2.2 Ratings/Connectives Correlation

From the ratings of the examples in Figure 1, we
can see that some of the SPaRKy sentence plan re-
alizations seem more natural than others. Upon fur-
ther analysis, we noticed that utterances containing
many contrastive connectives seemed less preferred
than those with fewer or no contrastive connectives.

To quantify this observation, we calculated the av-
erage number of connectives (aveci) used per real-
ization with rating i, using aveci = Totalci/Nri ,
where Totalci is the total number of connectives in
realizations with rating i, and Nri is the number of
realizations with rating i.

We use Pearson’s r to calculate each correlation
(in each case, df = 7). For both COMPARE strategies
(represented in Figure 2(a) and 2(b)), we find a sig-
nificant negative correlation for the average number

of connectives used in realizations with a given rat-
ing (C2: r = �0.97, p < 0.01; and C3: r = �0.93,
p < 0.01). These correlations indicate that judges’
ratings decreased as the average frequency of the
connectives increased.

Further analysis of the individual correlations
used in the comparative strategies show that there is
a significant negative correlation for however (C2:
r = �0.91, p < 0.01; and C3: r = �0.86,
p < 0.01) and on the other hand (C2: r = �0.89,
p < 0.01; and C3: r = �0.84, p < 0.01) in both
COMPARE strategies. In addition, in COMPARE-3,
the frequencies of while and but are also signifi-
cantly and strongly negatively correlated with the
judges’ ratings (r = �0.86, p < 0.01 and r =
�0.90, p < 0.01, respectively), though there is no
such correlation between the use of these connec-
tives and their ratings in COMPARE-2.

Added together, all the contrastive connectives
show strong, significant negative correlations be-
tween their average frequencies and judges’ ratings
for both comparative strategies (C2: r = �0.93,
p < 0.01; C3:r = �0.88, p < 0.01).

Interestingly, unlike in the COMPARE strategies,
there is a positive correlation (r = 0.73, p > 0.05)
between the judges’ ratings and the average fre-
quency of all connectives used in the RECOMMEND
strategy (see Figure 2(c)). Since this strategy only
uses and, since, and because and does not utilize any
contrastive connectives, this gives further evidence
that only contrastive connectives are dispreferred.

2.3 N-gram Ranker and Features

To acertain whether these contrastive connectives
are being learned by the ranker, we re-implemented
the n-gram ranker using SVM-light (Joachims,

77

Figure 1: Some alternative (Alt) realizations of SPaRKy sentence plans from a COMPARE2 (C2) plan,
with averaged human ratings (Rating; 5 = highest rating) and ranks (Rank; 1 = top ranked) assigned by
an n-gram ranker (Nakatsu, 2008)

Generating with Discourse Combinatory Categorial Grammar / 53

contrast

ggggg
ggggg

ggggg
gg

WWWWW
WWWWW

WWWWW
WW

evidence

lll
lll

lll
LLL

LLL
L evidence

lll
lll

lll
LLL

LLL
L

nucleus:(1’)
assert-summary

(mod:good)

infer

ooo
ooo

oo
OOO

OOO
OO

nucleus:(2’)
assert-summary
(mod:mediocre)

{infer|contrast}

ooo
ooo

oo
OOO

OOO
OO

nucleus:<1>
assert-com-decor

(mod:decent)

nucleus:<3>
assert-com-food-

quality (mod:very good)

YY
nucleus:<2>

assert-com-decor
(mod:decent)

nucleus:<4>
assert-com-food-

quality (mod:mediocre)bb bb

merely

additive
merely

FIGURE 31 SPaRKy tp-tree altered with new relations and summary
statements, corresponding to Example 50.

in bold font. Lastly, the connectives also and only, which represent the
additional relations, additive and merely, respectively, are indicated
in small caps.

(50) (1’): Sonia Rose is a good restaurant.
<1>: It has decent decor and
<3>: very good food quality.
(2’): However, Bienvenue is just a mediocre restaurant.
<2>: While it also has decent decor,
<4>: it only has mediocre food quality.

7 Related Work
In terms of its discourse theoretical basis, DCCG is most closely re-
lated to D-LTAG. In general, as Webber (2006) observes, discourse
grammars vary in their theoretical style, from wholly based on de-
pendency relations (e.g. Halliday and Hasan 1976) to adherence to a
completely constituent-based model (e.g. Rhetorical Structure Theory
[RST], Mann and Thompson 1988; Linguistic Discourse Model, Polanyi
1988, Polanyi and van den Berg 1996). Dependency-based discourse
theories are advantageous because they allow discourse relations to ex-
ist between non-adjacent discourse units, lifting restrictions on which
clauses can serve as discourse arguments of a given relation. Compu-

(1’): Sonia Rose is a good restaurant overall.

<1>: It has decent decor and

<3>: very good food quality.

(2’): However, Bienvenue is just a mediocre
restaurant on the whole.

<2>: While it also has decent decor,

<4>: it only has mediocre food quality.

Figure 2: Modified SPaRKy text plan for text with new relations and summary statements intended to
enhance contrast (Nakatsu and White, 2010)

32



Generating with Discourse Combinatory Categorial Grammar / 25

ot1h, A. however, B. otoh, C. however, D.
tsot1h tshowever tsotoh tshowever

TC TC
tsCUE\⇤ tsCUE tsCUE\⇤ tsCUE

< <
tsot1h tsotoh

TC
tsnil/⇤ tsotoh

>
tsnil

TC
turnnil

FIGURE 16 A DCCG derivation of nested contrast relations

Returning now to the intrasentential conjunctions that express con-
trast, their categories remain the same as in the preceding section, ex-
cept for the addition of the requirement that they combine with clauses
having nil values for the cue feature:

(39) a. {while, but } ` se,nil\⇤se1 ,nil\⇤punc,/⇧se2 ,nil :
@e(contrast-rel ^ hArg1ie1 ^ hArg2ie2)

b. while ` se,nil/⇤se2 ,nil/⇤punc,/⇧se1 ,nil :
@e(contrast-rel ^ hArg1ie1 ^ hArg2ie2)

Since these categories do not need to look outside the sentence to find
both of their discourse arguments, they do not change the cue values
of their result categories.

To conclude this section, we address the question of whether it is a
necessary move to employ unary type-changing rules in order to handle
intersentential discourse connectives in CCG. As noted in the preceding
section, the lexicalized categories for connectives o↵ered therein suggest
that there is no problem in principle with devising a purely lexicalized
approach to discourse connectives; accordingly, the cue threading ap-
proach presented in this section appears to yield grammars with cov-
erage equivalent to purely lexicalized alternatives. Nevertheless, as we
have seen, the purely lexicalized approach leads to a proliferation of lex-
ical category ambiguity, and while lexical rules might be employed to
systematically assign the necessary lexical categories, the cue threading
approach is clearly more economical. Similar considerations led Hock-
enmaier and Steedman (2002, 2007) to make extensive use of type-
changing rules in their broad coverage grammar of English, indicating
that such rules have an important role to play in practical grammars.
Hockenmaier and Steedman further argued that the formal power of
the system is una↵ected as long as (i) only a finite number of unary
rules are employed and (ii) the rules are designed so that they cannot
recursively apply to their own output, as is the case here.

Figure 3: DCCG derivation of nested contrast re-
lations (Nakatsu and White, 2010)

26 / LiLT volume 4, issue 1 September 2010

it also has poor decor

np sCUE\np/⇧(sCUE\np) snil\npnom
>

snil\npnom
<snil

FIGURE 17 A DCCG derivation of a clause including the discourse
adverbial also.

5.3 Discourse Adverbials and Anaphora Resolution
Unlike structural connectives, which find their discourse arguments via
cue threading, discourse adverbials find one argument syntactically,
and the other through anaphora resolution. To illustrate how DCCG
accomplishes this, consider (1) from Section 2, repeated below:

(1) b1: Bienvenue is a mediocre restaurant.
h1: It has poor decor and mediocre food quality.
b3: However, Sonia Rose is a good restaurant.
h2: While it also has poor decor,
h3: it has excellent food quality.

As illustrated by the derivation of the clause for h2 in Figure 17, the pre-
verbal modifier category for also in (40c) below takes a VP category
se,CUE\np as its argument and returns a VP category as its result,
adding an additive relation to the semantics.

(40) a. also ` se,CUE/⇧se,CUE/⇤punc, :
@e(hModi(a ^ additive-rel ^ hArg1ie1))

b. also ` se,CUE\⇧se,CUE\⇤punc, :
@e(hModi(a ^ additive-rel ^ hArg1ie1))

c. also ` se,CUE\np/⇧(se,CUE\np) :
@e(hModi(a ^ additive-rel ^ hArg1ie1))

Since discourse adverbials such as also do not necessarily find their dis-
course arguments in structurally adjacent text segments, they do not
use cue threading. Instead, the cue value on discourse adverbials is left
underspecfied, as seen in all the lexical entries for also in (40). These
underspecified values then unify with the cue value of the input cat-
egory, threading any undischarged structural connectives through. In
this way, a discourse adverbial and a structural connective can appear
on the same clause (e.g. However, Bienvenue also has good decor).
In our example, the underspecified cue value of the argument cate-
gory in (40c) is unified with the nil cue value from the input category

Figure 4: DCCG derivation of a clause with
the discourse adverbial also (Nakatsu and White,
2010)

jkumar, 2009) in order to generate texts that vary
in size from single sentences to entire paragraphs.

In DCCG, the technique of cue threading is
used to allow structural connectives—including
paired ones such as on the one hand . . . on the
other hand—to project beyond the sentence level,
while allowing no more than one to be active at
a time. In this way, structural connectives can
be nested, as sketched in Figure 3, but cannot
cross. In the figure, the value of the cue feature for
each text segment (ts) is shown (where ot1h and
otoh abbreviate on the one hand and on the other
hand); these cue values can be propagated through
a derivation, allowing the discourse relations to
project, but must be discharged (to nil) in a com-
plete derivation, thereby ensuring that the intended
discourse relations are actually realized. By con-
trast, discourse adverbials introduce their relations
anaphorically and are transparent to cue thread-
ing, as sketched in Figure 4, making use of typical
adverb categories syntactically. See Nakatsu and
White (2010) for further details.

3 Crowd Sourcing Ratings

To collect human judgements from a diverse group
of speakers of US English, we used Amazon’s
Mechanical Turk service (AMT) to run two ex-
periments. In the first experiment, subjects rated
the naturalness of 174 passages used in Walker et
al.’s (2007) study. As detailed in Section 5, this
validation experiment confirmed that the judge-

ments collected on AMT correlate with those of
the raters in Walker et al.’s (2007) study. Our sec-
ond experiment collected ratings on 300 passages
realized with modifications for better contrast ex-
pression (WITHMODS) and 300 passages without
these modifications (NOMODS), both realized us-
ing OpenCCG. While this does not admit a direct
comparison to the realizations produced by Walker
et al. (2007), this controls for differences between
the generators other than the variable of interest:
the contrastive enhancements. In addition to these
materials, five passages from the SRC were seen
by all subjects to control for anomalous subject be-
havior.

3.1 Survey Format
Each survey used demographic questions to de-
termine the native speaker status of the subject.
Instructions for completing comprehension ques-
tions and rating realizations followed the demo-
graphic questions.3 Each subject saw fifteen stim-
uli, each consisting of a sample user query and the
target passage as in Figure 5. After reading the
stimulus, the subject answered a yes-or-no com-
prehension question (see §3.2). Finally the subject
rated the naturalness of the passage on a seven-
point Likert scale ranging from very unnatural to
very natural. At the survey’s conclusion, the sub-
ject could offer free-form feedback, explain their
responses, or ask questions of the researchers. The
average completion time across all experiments
was about ten minutes.

Passage selection is detailed in §3.3 and §3.4.
3.2 Quality Control
We used three strategies to filter out low-quality
responses from AMT subjects.

Comprehension Questions A template-based
yes-or-no question (exemplified in Figure 5) fol-
lowed each passage. Subjects who answered less
than 75% of these questions correctly were re-
jected and not paid, in accordance with the pro-
tocol approved by our human subjects review
board. Responses from three subjects were ex-
cluded from analysis on this basis.

Uniform Ratings When a subject gave the
same rating for all passages in a given survey (and
in disagreement with other subjects), we took this
to mean that the subject was paying attention only

3These materials, along with the generated passages
and their ratings are available at http://www.ling.
ohio-state.edu/˜mwhite/data/enlg13/.

33



Figure 5: Sample survey stimulus and comprehension question

Method # subjects excluded
Comprehension Questions 3
Uniform Answers 1
SAME5 0
Native Speaker Status 2

Table 1: Number of subjects excluded based on
quality control measures or native language.

to the comprehension questions that ensured pay-
ment. Only one subject was excluded on this ba-
sis, though they were still paid for answering the
comprehension questions correctly.

SAME5 Passages Five passages were chosen
from the original SRC realizations for which the
original ratings (from Walker et al. 2007) were
identical for both judges. The passages were se-
lected such that the first and third authors of this
paper agreed with the general valence and rela-
tive rankings of the passages. That is, we took
two unambiguously bad realizations, two unam-
biguously good realizations, and one realization
near the middle of the spectrum to represent a gold
standard for rating to compare subjects against. If
any subject’s ratings on these five passages were
clear outliers, we could remove that subject’s data
for anomalous behavior, but this measure proved
unnecessary for the subjects in the present study.

3.3 Validating AMT

Data Selection In this experiment, we sampled
174 of the 1757 realizations from the SRC rated
by subjects A and B in Walker et al.’s (2007) ex-
periment.

The SRC realizations were divided randomly
into two groups. Within one group, realizations
were labelled by subject A’s rating for that real-
ization. Subject B’s rating was used for the other
group. Taking the poles of the rating scale and its

midpoint, the realizations were further partitioned
into six sets: realizations rated 1, 3, and 5 by sub-
ject A and realizations rated 1, 3, and 5 by subject
B. This division of the data ensured that the re-
alizations used would cover the full spectrum of
ratings while being representative of the SRC rat-
ings with respect to, e.g., inter-annotator ratings
correlations.

From each of these six sets, we chose 10 COM-
PARE2, 10 COMPARE3, and 10 RECOMMEND re-
alizations,4 each of these groups representing a
different realization task in the SRC. The COM-
PARE2 and COMPARE3 tasks involved the com-
parison of two restaurants or three or more restau-
rants, respectively. In the RECOMMEND context,
the sytem had to generate a recommendation for a
single restaurant.

Subject Demographics Thirty-six subjects re-
sponded to this survey initially, but one was re-
jected based on a failure to answer the compre-
hension questions and data from another had to be
excluded for non-native speaker status. Two addi-
tional subjects were recruited to replace their data.
This resulted in a subject pool with a mean age
(std. dev.) of 34.67 (9.35) years. Twenty-four sub-
jects identified as female and twelve identified as
male. Each subject received $2.50 for the survey,
estimated to take approximately 20 minutes.

3.4 Rating OpenCCG Realizations
Data Selection We selected 15 content plans
(CPs) from the SRC where the use of the con-
trastive modifiers was licensed: five COMPARE2,
five COMPARE3, and five RECOMMEND CPs.
Each of the 112 textplans (TPs) that produced

4Except that subject A used the rating ‘5’ less than subject
B. To compensate, we used as many 5-point ratings as were
available from subject A and then filled in the remainder of
the 10 slots with realizations rated ‘4’. We mirrored these
selections in the data from subject B for consistency.

34



the SRC realizations for these CPs was then pre-
processed for realization in OpenCCG both with
contrast enhancements (WITHMODS) and without
them (NOMODS).

Both structural choices and ordering choices
are encoded in these TPs.5 Structural choices in-
clude decisions about how to group the restau-
rant properties to be expressed, such as deciding
whether to describe one restaurant in its entirety
and then the other (i.e. a serial structure) or al-
ternating between one restaurant and the other, di-
rectly contrasting particular attributes (i.e. a back-
and-forth structure). Ordering choices fixed the
order of presentation of restaurant attributes in se-
rial plans and the order of presentation of attribute
contrasts in back-and-forth plans. As discussed in
§6, there turn out to be interesting interactions be-
tween these aggregation choices and the contrast
enhancements, interactions which we did not ex-
plore directly in this experiment.

Processing each TP produced a different LF for
each possible combination of aggregation choices
and contrastive modifications, resulting in approx-
imately 41k logical forms (LFs) for the TPs WITH-
MODS and 88k LFs for the TPs with NOMODS.6

Each realization received two language model
(LM) scores, one based on the semantic classes
used during realization (LMSC) and one based on
the Gigaword corpus (LMGW ). LMSC used a tri-
gram model over modified texts based on the SRC
where specific entities (e.g. restaurant names like
Caffe Buon Gusto) were replaced with their se-
mantic class (e.g. RESTAURANT). The LM scores
were normalized by CP, such that the scores for a
given CP summed to 1 in each LM. These were
then linearly combined with weights slightly pre-
ferring the LMSC score to produce a combined
LM score for each realization.

Sampling then proceeded without replacement,
weighted by the combined LM score for each real-
ization. For the NOMODS sample, 20 realizations
were chosen this way, but, in the WITHMODS sam-
ple, a series of regular expression filters were used
to ensure adequate representation of the modifica-
tions in the surveys. These filters selected (without

5This differs from Walker et al. (2007), wherein reorder-
ings were allowed in mapping from tp-trees to sp-trees and
d-trees.

6In future work we will explore a probabilistic rather than
exhaustive mapping algorithm to produce only LFs that are
more likely to result in more fluent realizations—not unlike
the weighted aggregation done by Walker et al.’s (2007) sen-
tence plan generator.

replacement) 10 realizations such that every con-
trastive modification licensed by a particular CP
was represented, leaving 10 realizations to be se-
lected by weighted sampling without replacement.

This process resulted in 300 passages in each of
the two conditions (WITHMODS, NOMODS): 20
realizations for each of the 15 CPs. Each survey
included 5 realizations WITHMODS paired by CP
with 5 realizations with NOMODS as well as the
SAME5 realizations. As noted earlier, pairing real-
izations in this way helps to control for differences
in the variety of aggregation choices and surface
realizations used in the SRC as opposed to our
SRC-inspired grammar for OpenCCG.

Subject Demographics Sixty-eight subjects
responded to these 180 surveys initially. Subjects
were allowed to complete up to six distinct sur-
veys. One subject’s data was excluded for non-
native status and another’s was excluded on the
basis of uniform ratings (as detailed in §3.2). To
compensate for the eight surveys completed by
these subjects and ten surveys mistakenly admin-
istered in draft format, we recollected data for 18
of the 180 surveys. This resulted in a final pool of
80 subjects with an average (std. dev.) age 37.15
(13.5) years. Forty identified as female, thirty-
nine identified as male, and one identified as non-
gendered.

Because subjects in the validation study com-
pleted the survey in about 10 minutes on average
with a standard deviation of about 5 minutes, we
scaled the pay to $2.00 per survey in this experi-
ment. Since subjects could participate in this ex-
periment multiple times, they could receive up to
$12.00 for their contribution.

4 Training a Text Ranker

To perform the ranking, we trained a basic n-
gram ranker using SVMlight in preference ranking
mode.7 We used the average ratings obtained in §3
as target value.

The feature set was composed of 2 types of fea-
tures. The first feature type are the two language
model scores from §3.4, LMSC and LMGW . The
second feature type consisted of n-gram counts.
We indexed the unigrams and bigrams in each cor-
pus and used each as a feature whose value was the
number of times it appeared in a given realization.

We trained the ranker on, and extracted n-gram

7SVMlight is an implementation of support vector ma-
chines by (Joachims, 2002).

35



1

2

3

4

5

6

7

1 2 3 4 5
Average rating from Walker et al. (2007)

A
ve

ra
ge

 r
at

in
g 

fr
om

 A
M

T
 s

ub
je

ct
s

Figure 6: Average ratings from our experiment
and Walker et al. (2007), accompanied by a line
of best fit. Jitter (0.1) applied to each point mini-
mizes overlap.

features from, 3 different corpora drawn from the
data selection in §3.4. The first corpus contains
299 selections WITHMODS (1 selection was dis-
carded for only being rated once), the second cor-
pus contains 300 selections with NOMODS, and
the third corpus contains BOTH of the first two cor-
pora combined.

To train and test the ranker, we performed 15-
fold cross-validation on each corpus. Within each
training fold, we had 14 training examples, corre-
sponding to 14 CPs. Each training example con-
sisted of all of a given CP’s realizations and their
ratings. After training, the realizations for the re-
maining CP were ranked.

In order to evaluate the ranker, we used the
TopRank metric (Walker et al., 2007). For each
of the ranked CP realization sets, we extracted the
target values (i.e. the average rating given by sub-
jects) of the highest ranked realization. We then
averaged the target scores of all of the top-ranked
realizations across the 15 training folds to produce
the Top Rank metric. The oracle best score is
the score of the highest rated realization, as de-
termined by the average score assigned to that re-
alization by the subjects.

5 Results

Validation Figure 6 shows the correlation be-
tween the average ratings of our subjects on AMT
and the average ratings assigned by subjects A and
B in Walker et al. (2007). This correlation was
0.31 (p < 0.01, Kendall’s tau), while the corre-
lation between subjects A and B was only 0.28

BOTH WITHMODS NOMODS
human 6.61 (0.28) 6.46 (0.43) 6.49 (0.26)
bigram 6.00 (0.58) 5.62 (0.83) 5.51 (1.02)

Table 2: TopRank scores and standard deviations
for the oracle (human) & bigram (bigram) ranks.

(p < 0.01, Kendall’s tau). On this basis we con-
clude that using AMT workers as subjects to rate
sentences for their naturalness is at least as rea-
sonable as having two expert annotators labelling
realizations for their overall quality.

SAME5 Comparison There was no signifi-
cant difference (p = 0.16, using Welch’s t-test)
between the scores given to the SAME5 stimuli
in the two experiments,8 indicating that subjects
used the rating scale similarly in both experiments.
The mean ratings for the rest of the validation re-
alizations was 5.31 (1.43) and the mean for the
OpenCCG-based realizations in the ranking exper-
iment was 4.96 (1.51), which is significantly lower
according to Welch’s t-test (p < 0.01). This high-
lights the underlying differences between the two
generation systems, validating our choice to use
OpenCCG for both the WITHMODS and NOMODS
realizations to better examine the impact of the
contrast enhancements.

Ranking Table 2 reports the oracle results,
along with our ranker’s results, using the TopRank
metric. Most indicative of the benefit of the con-
trastive enhancements is the performance of the
oracle score for the BOTH (6.61) condition com-
pared to the NOMODS condition (6.49), which is
significantly higher according to a paired t-test
(p = 0.01).

We also found that the bigram ranker with the
averaged raw ratings was better at predicting the
top rank of the combined (BOTH) corpus (6.00 vs.
oracle-best of 6.61) than either of the other two,
and better on the WITHMODS condition (5.62)
than on the NOMODS condition (5.51). However,
a two-tailed t-test revealed that the difference was
not quite signficant between BOTH and NOMODS
at the conventional level (p = 0.06), though the
p-value did meet the 0.1 threshold sometimes em-
ployed in small-scale experiments. The perfor-
mance of the different rankers, as compared to the
oracle scores, can be seen in Figure 7.

These preliminary results with a simple ranker

8Validation experiment mean (std. dev.) 4.89 (1.79) ver-
sus 5.10 (1.75) in the ranking experiment.

36



3

4

5

6

7

human bigram
Method

To
pR

an
k

Corpus

both

withMods

noMods

Figure 7: TopRank scores for each of the rankers
with standard error bars.

are promising, motivating future work on improv-
ing the ranker in addition to enlarging the dataset.

6 Discussion

To assess the impact of the enhancement options,
we performed a linear regression between the
contrast-related patterns we used for data selec-
tion and the normalized ratings, with scikit-learn’s
implementation of the Bayesian Ridge method of
regularizing weights.9 In looking at examples,
we found that the number of discourse adverbials
appeared to be a factor, so we then added these
counts as features. The coefficients and corpus
counts appear in Table 3. The results show that
the discourse adverbials were effective some of
the time, especially when used sparingly and in
conjunction with while. The “heavier” contrastive
connectives however and on the one/other hand
were dispreferred, perhaps in part because they
ended up appearing too often with small, single-
restaurant contrasts, as there were relatively few
examples of summary statements, most of which
were somewhat disfluent due to a medial choice
for overall / on the whole.

Table 4 shows examples that illustrate both suc-
cesses and remaining issues. At the top, two pairs
of examples are given where the normalized av-
erage ratings are higher with the inclusion of just
and only, and where the rating drops off greatly
when however is used with a lesser value and no
adverbial of this kind, as expected. At the bottom,
the first example shows one instance where the use
of multiple adverbials is dispreferred. A possible

9http://scikit-learn.org/stable/
modules/linear_model.html

pattern coeff count
| disc advb | = 1 0.23 102
while 0.19 38
also has 0.13 47
has . . . too 0.12 39
has only 0.09 43
while . . . disc advb 0.09 16
contrastive . . . overall 0.07 8
has just 0.04 46
however . . . disc advb 0.03 4
but -0.03 20
, however , -0.05 10
only has -0.06 30
has merely -0.11 46
on the whole -0.14 33
just has -0.16 29
merely has -0.16 8
| disc advb | = 2 -0.18 32
. however , -0.21 64
on the other hand -0.21 40
| disc advb | >= 3 -0.27 50
overall -0.29 34
on the one hand -0.36 22

Table 3: Coefficients of linear regression between
contrast-related patterns and normalized ratings,
along with pattern counts, where disc adv is one
of just, only, merely, also, too and contrastive is
one of while, however, on the one/other hand

factor here may be that in addition to there being
several similar adverbials in a row, they all involve
long-distance antecedents, which may be difficult
to process. Finally, the last example shows a real-
ization that receives a relatively high rating despite
the use of two adverbials; note, however, that since
this passage uses a back-and-forth text plan, the
antecedents of the adverbials are all very local.10

Turning to the survey feedback, many subjects
provided insightful comments regarding the task.
The most frequent comment pointed out that our
comprehension questions sometimes precipitated
a false implicature: when asked if a restaurant had
decent decor, subjects commented that they felt
that answering “no” meant implying that it had
terrible decor. Similar problems occurred when a
restaurant had, e.g., very good decor and the sub-
jects were asked if it had good decor. Despite oc-
casional deviations from our intended exact-match
interpretation of these questions, no subjects were
excluded for scoring too low as a result of this.

10As one reviewer points out, there’s also an interaction be-
tween how attributes are aggregated and the ability to express
contrast. For example, contrasting the attributes for which a
restaurant scores highly with those for which it scores poorly
requires the aggregation of attributes with like valence, as in
“This restaurant has superb decor and very good service but
only mediocre food quality.” Our future work on aggregation
will explore this interaction as well.

37



Strategy Mods? Rating Realization
C2 Y 1.13 Da Andrea’s price is 28 dollars. Gene’s’s price is 33 dollars. Da Andrea has very good

food quality while Gene’s has just good food quality.
C2 N 0.73 Da Andrea’s price is 28 dollars. Gene’s’s price is 33 dollars. Da Andrea has very good

food quality while Gene’s has good food quality.
C2 Y 1.04 Da Andrea’s price is 28 dollars. Gene’s’s price is 33 dollars. Da Andrea has very good

food quality. However, Gene’s has only good food quality.
C2 N -0.63 Da Andrea’s price is 28 dollars. Gene’s’s price is 33 dollars. Da Andrea has very good

food quality. However, Gene’s has good food quality.
C3 Y -1.85 Daniel and Jo Jo offer exceptional value among the selected restaurants. Daniel, on the

whole, is a superb restaurant. Daniel’s price is 82 dollars. Daniel has superb decor. It has
superb service and superb food quality. Jo Jo, overall, is an excellent restaurant. Jo Jo’s
price is 59 dollars. Jo Jo just has very good decor. It just has excellent service. It has
merely excellent food quality.

C2 Y 1.12 Japonica’s price is 37 dollars while Dojo’s price is 14 dollars. Japonica has excellent food
quality while Dojo has merely decent food quality. Japonica has decent decor. Dojo has
only mediocre decor.

Table 4: Examples illustrating successful and problematic contrast enhancements

In order to elicit rankings at a variety of points
on the naturalness scale, our selection included a
number of realizations with lower quality over-
all, which subjects picked up on. For example,
one subject commented that, “Repeatedly using
the name of each restaurant over and over in sim-
ple sentences make[s] almost all of these excerpts
sound horrifyingly awkward,” while another ob-
served, “The constant [use] of more sentences, in-
stead of using conjunction words . . . makes it seem
as if the system is rambling and lost in though[t]
process.”

Several subjects also pointed out that it would
be more natural to discuss the cost of an average
meal at a restaurant than to state that a restau-
rant’s price is some particular number of dollars.
Though these domain-specific lexical preferences
are tangential to the focus of this paper, they sug-
gest that exploring options to expand the range
of realizations for more naturally expressing these
properties might be a fruitful direction for future
work.

In addition to expressing an explicit prefer-
ence for serial rather than back-and-forth text-
plans, subjects also commented that higher level
contrastive adverbials like however work better
when they are used sparingly at a high level, rein-
forcing the findings in our regressions. We also re-
ceived suggestions for future work improving the
expression of contrast: some subjects suggested
that using better and worse to make explicit com-
parisons between restaurants would improve the
naturalness, and one subject suggested explicitly
stating which restaurant is (say) the cheapest as in
White et al. (2010).

7 Conclusions and Future Work

In this paper, we have shown using ratings gath-
ered on AMT that Nakatsu & White’s (2010) pro-
posed enhancements to the SPaRKy Restaurant
Corpus (Walker et al., 2007) for better express-
ing contrast do indeed make it possible to generate
better texts, and an initial experiment suggested
that even a basic n-gram ranker can do so automat-
ically. A regression analysis further revealed that
while using a few discourse adverbials sparingly
was effective, using too many discourse adverbials
had a negative impact, with antecedent distance
potentially an important factor. In future work, we
plan to improve upon this basic n-gram ranker to
take these observations into account and validate
these initial findings on a larger dataset. In the pro-
cess we will explore the interaction between con-
trast expression and aggregation and seek to bet-
ter model the felicity conditions for “weighty” top
level adverbials such as however.

Acknowledgments

This work was supported in part by NSF grant
IIS-1143635. Special thanks to the anonymous
reviewers, the Clippers computational linguistics
discussion group at Ohio State, and to Mark Dras,
Francois Lareau, and Yasaman Motazedi at Mac-
quarie University.

References
Brigitte Grote, Nils Lenke, and Manfred Stede. 1995.

Ma(r)king concessions in English and German. In
Proc. of the Fifth European Workshop on Natural
Language Generation.

38



Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.

Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Proc. INLG-
98.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Towards a functional
theory of text organization. TEXT, 8(3):243–281.

Chris Mellish, Alistair Knott, Jon Oberlander, and
Mick O’Donnell. 1998. Experiments using stochas-
tic search for text planning. In Proc. INLG-98.

Crystal Nakatsu and Michael White. 2006. Learning
to say it well: Reranking realizations by predicted
synthesis quality. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1113–1120, Syd-
ney, Australia, July. Association for Computational
Linguistics.

Crystal Nakatsu and Michael White. 2010. Generat-
ing with discourse combinatory categorial grammar.
Linguistic Issues in Language Technology, 4(1):1–
62.

Crystal Nakatsu. 2008. Learning contrastive connec-
tives in sentence realization ranking. In Proceedings
of the 9th SIGdial Workshop on Discourse and Dia-
logue, pages 76–79, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.

Mark Steedman. 2000. The Syntactic Process. MIT
Press.

Kristina Striegnitz. 2004. Generating Anaphoric Ex-
pressions — Contextual Inference in Sentence Plan-
ning. Ph.D. thesis, University of Saalandes & Uni-
versit de Nancy.

Marilyn A. Walker, Owen C. Rambow, and Monica Ro-
gati. 2002. Training a sentence planner for spo-
ken dialogue using boosting. Computer Speech and
Language, 16:409–433.

M. A. Walker, S. J. Whittaker, A. Stent, P. Mal-
oor, J. D. Moore, M. Johnston, and G Vasireddy.
2004. Generation and evaluation of user tailored re-
sponses in multimodal dialogue. Cognitive Science,
28(5):811–840.

M. Walker, A. Stent, F. Mairesse, and Rashmi Prasad.
2007. Individual and domain adaptation in sentence
planning for dialogue. Journal of Artificial Intelli-
gence Research (JAIR), 30:413–456.

Bonnie Webber, Matthew Stone, Aravind Joshi, and
Alistair Knott. 2003. Anaphora and discourse struc-
ture. Computational Linguistics, 29(4).

Bonnie Webber. 2004. D-LTAG: Extending lex-
icalized TAG to discourse. Cognitive Science,
28(5):751–779.

Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 410–
419, Singapore, August. Association for Computa-
tional Linguistics.

Michael White, Robert A. J. Clark, and Johanna D.
Moore. 2010. Generating tailored, comparative de-
scriptions with contextually appropriate intonation.
Computational Linguistics, 36(2):159–201.

Michael White. 2006a. CCG chart realization from
disjunctive logical forms. In Proc. INLG-06.

Michael White. 2006b. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation,
4(1):39–75, June.

39


