



















































Lexical Substitution for the Medical Domain


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 610–614,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Lexical Substitution for the Medical Domain

Martin Riedl1 Michael R. Glass2 Alfio Gliozzo2
(1) FG Language Technology, CS Dept., TU Darmstadt, 64289 Darmstadt, Germany

(2) IBM T.J. Watson Research, Yorktown Heights, NY 10598, USA
riedl@cs.tu-darmstadt.de, {mrglass,gliozzo}@us.ibm.com

Abstract

In this paper we examine the lexical substitu-
tion task for the medical domain. We adapt
the current best system from the open domain,
which trains a single classifier for all instances
using delexicalized features. We show sig-
nificant improvements over a strong baseline
coming from a distributional thesaurus (DT).
Whereas in the open domain system, features
derived from WordNet show only slight im-
provements, we show that its counterpart for
the medical domain (UMLS) shows a signif-
icant additional benefit when used for feature
generation.

1 Introduction
The task of lexical substitution (McCarthy and Navigli,
2009) deals with the substitution of a target term within
a sentence with words having the same meaning. Thus,
the task divides into two subtasks:

• Identification of substitution candidates, i.e.
terms that are, for some contexts, substitutable for
a given target term.

• Ranking the substitution candidates according to
their context

Such a substitution system can help for semantic text
similarity (Bär et al., 2012), textual entailment (Dagan
et al., 2013) or plagiarism detection (Chong and Specia,
2011).

Datasets provided by McCarthy and Navigli (2009)
and Biemann (2012) offer manually annotated substi-
tutes for a given set of target words within a context
(sentence). Contrary to these two datasets in Kremer et
al. (2014) a dataset is offered where all words have are
annotated with substitutes. All the datasets are suited
for the open domain.

But a system performing lexical substitution is not
only of interest for the open domain, but also for the
medical domain. Such a system could then be applied
to medical word sense disambiguation, entailment or
question answering tasks. Here we introduce a new
dataset and adapt the lexical substitution system, pro-
vided by Szarvas et al. (2013), to the medical domain.
Additionally, we do not make use of WordNet (Miller,

1995) to provide similar terms, but rather employ a Dis-
tributional Thesaurus (DT), computed on medical texts.

2 Related Work

For the general domain, the lexical substitution task
was initiated by a Semeval-2007 Task (McCarthy and
Navigli, 2009). This task was won by an unsupervised
method (Giuliano et al., 2007), which uses WordNet for
the substitution candidate generation and then relies on
the Google Web1T n-grams (Brants and Franz, 2006)1

to rank the substitutes.
The currently best system, to our knowledge, is pro-

posed by Szarvas et al. (2013). This is a supervised ap-
proach, where a single classifier is trained using delex-
icalized features for all substitutes and can thus be ap-
plied even to previously unseen substitutes. Although
there have been many approaches for solving the task
for the general domain, only slight effort has been done
in adapting it to different domains.

3 Method

To perform lexical substitution, we follow the delex-
icalization framework of Szarvas et al. (2013). We
automatically build Distributional Thesauri (DTs) for
the medical domain and use features from the Uni-
fied Medical Language System (UMLS) ontology. The
dataset for supervised lexical substitution consists of
sentences, containing an annotated target word t. Con-
sidering the sentence being the context for the target
word, the target word might have different meanings.
Thus annotated substitute candidates sg1 . . . sgn ∈ sg ,
need to be provided for each context. The negative ex-
amples are substitute candidates that either are incor-
rect for the target word, do not fit into the context or
both. We will refer to these substitutes as false substi-
tute candidates sf1 . . . sfm ∈ sf with sf ∩ sg = ∅.

For the generation of substitute candidates we do not
use WordNet, as done in previous works (Szarvas et al.,
2013), but use only substitutes from a DT. To train a
single classifier, features that distinguishing the mean-
ing of words in different context need to be considered.
Such features could be e.g. n-grams, features from dis-
tributional semantics or features which are extracted

1http://catalog.ldc.upenn.edu/
LDC2006T13

610



relative to the target word, such as the ratio between
frequencies of the substitute candidate and the target
word. After training, we apply the algorithm to un-
seen substitute candidates and rank them according to
their positive probabilities, given by the classifier. Con-
trary to Szarvas et al. (2013), we do not use any weight-
ing in the training if a substitute has been supplied by
many annotators, as we could not observe any improve-
ments. Additionally, we use logistic regression (Fan et
al., 2008) as classifier2.

4 Resources

For the substitutes and for the generation of delexical-
ized features, we rely on DTs, the UMLS and Google
Web1T.

4.1 Distributional thesauri (DTs)

We computed two different DTs using the framework
proposed in Biemann and Riedl (2013)3.

The first DT is computed based on Medline4 ab-
stracts. This thesaurus uses the left and the right word
as context features. To include multi-word expressions,
we allow the number of tokens that form a term to be
up to the length of three.

The second DT is based on dependencies as context
features from a English Slot Grammar (ESG) parser
(McCord et al., 2012) modified to handle medical data.
The ESG parser is also capable of finding multi-word
expressions. As input data we use 3.3 GB of texts
from medical textbooks, encyclopedias and clinical ref-
erence material as well as selected journals. This DT is
also used for the generation of candidates supplied to
annotators when creating the gold standard and there-
fore is the main resource to provide substitute candi-
dates.

4.2 UMLS

The Unified Medical Language System (UMLS) is an
ontology for the medical domain. In contrast to Szarvas
et al. (2013), which uses WordNet (Miller, 1995) to
generate substitute candidates and also for generating
features, we use UMLS solely for feature generation.

4.3 Google Web1T

We use the Google Web1T to generate n-gram features
as we expect this open domain resource to have consid-
erable coverage for most specific domains as well. For
accessing the resource, we use JWeb1T5 (Giuliano et
al., 2007).

2We use a Java port of LIBLINEAR (http://www.
csie.ntu.edu.tw/˜cjlin/liblinear/) available
from http://liblinear.bwaldvogel.de/

3We use Lexicographer’s Mutual Information (LMI) (Ev-
ert, 2005) as significance measure and consider only the top
1000 (p = 1000) features per term.

4http://www.nlm.nih.gov/bsd/licensee/
2014_stats/baseline_med_filecount.html

5https://code.google.com/p/jweb1t/

5 Lexical Substitution dataset
Besides the lexical substitution data sets for the open
domain (McCarthy and Navigli, 2009; Biemann, 2012;
Kremer et al., 2014) there is no dataset available that
can be used for the medical domain. Therefore, we
constructed an annotation task for the medical domain
using a medical corpus and domain experts.

In order to provide the annotators with a clear task,
we presented a question, and a passage that contains
the correct answer to the question. We restricted this to
a subset of passages that were previously annotated as
justifying the answer to the question. This is related to
a textual entailment task, essentially the passage entails
the question with the answer substituted for the focus of
the question. We instructed the annotators to first iden-
tify the terms that were relevant for the entailment rela-
tion. For each relevant term we randomly extracted 10
terms from the ESG-based DT within the top 100 most
similar terms. Using this list of distributionally similar
terms, the annotators selected those terms that would
preserve the entailment relation if substituted. This re-
sulted in a dataset of 699 target terms with substitutes.
On average from the 10 terms 0.846 are annotated as
correct substitutes. Thus, the remaining terms can be
used as false substitute candidates.

The agreement on this task by Fleiss Kappa was
0.551 indicating “moderate agreement” (Landis and
Koch, 1977). On the metric of pairwise agreement,
as defined in the SemEval lexical substitution task, we
achieve 0.627. This number is not directly comparable
to the pairwise agreement score of 0.277 for the Se-
mEval lexical substitution task (McCarthy and Navigli,
2009) since in our task the candidates are given. How-
ever, it shows promise that subjectivity may be reduced
by casting lexical substitution into a task of maintain-
ing entailment.

6 Evaluation
For the evaluation we use a ten-fold cross validation
and report P@1 (also called Average Precision (AP) at
1) and Mean Average Precision (MAP) (Buckley and
Voorhees, 2004) scores. The P@1 score indicates how
often the first substitute of the system matches the gold
standard. The MAP score is the mean of all AP from 1
to the number of all substitutes.

• Google Web 1T:
We use the same Google n-gram features, as
used in Giuliano et al. (2007) and Szarvas et al.
(2013). These are frequencies of n-grams formed
by the substitute candidate si and the left and right
words, taken from the context sentence, normal-
ized by the frequency of the same context n-gram
with the target term t. Additionally, we add the
same features, normalized by the frequency sum
of all n-grams of the substitute candidates. An-
other feature is generated using the frequencies
where t and s are listed together using the words

611



and, or and ”,” as separator and also add the left
and right words of that phrase as context. Then we
normalize this frequency by the frequency of the
context occurring only with t.

• DT features:
To characterize if t and si have similar words
in common, and therefore are similar, we com-
pute the percentage of words their thesauri en-
tries share, considering the top n words in each
entry with n = 1, 5, 20, 50, 100, 200. During
the DT calculation we also calculate the signif-
icances between each word and its context fea-
tures (see Section 4.1). Using this information,
we compute if the words in the sentences also
occur as context features for the substitute can-
didate. A third feature group relying on DTs
is created by the overlapping context features
for the top m entries of t and si with m =
1, 5, 20, 50, 100, 1000, which are ranked regard-
ing their significance score. Whereas, the simi-
larities between the trigram-based and the ESG-
based DT are similar, the context features are dif-
ferent. Both feature types can be applied to the
two DTs. Additionally, we extract the thesaurus
entry for the target word t and generate a feature
indicating whether the substitute si is within the
top k entries with k = 1, 5, 10, 20, 100 entries6.

• Part-of-speech n-grams:
To identify the context of the word we use the
POS-tag (only the first letter) of si and t as feature
and POS-tag combinations of up to three neigh-
boring words.

• UMLS:
Considering UMLS we look up all concept unique
identifiers (CUIs) for si and t. The first two fea-
tures are the number of CUIs for si and t. The next
features compute the number of CUIs that si and t
share, starting from the minimal to the maximum
number of CUIs. Additionally, we use a feature
indicating that si and t do not share any CUI.

6.1 Substitute candidates

The candidates for the substitution are taken from the
ESG based DT. For each target term we use the gold
substitute candidates as correct instances and add all
possible substitutes for the same target term occurring
in a different context and do not have been annotated
as valid in the present context as false instances.

7 Results

Running the experiment, we get the results as shown
in Table 1. As baseline system we use the ranking of

6Whereas in Szarvas et al. (2013) only k = 100 is used,
we gained an improvement in performance when also adding
smaller values of k.

the ESG-based DT. As can be seen, the baseline is al-
ready quite high, which can be attributed to the fact
that this resource was used to generate substitutes und
thus contains all positive instances. Using the super-
vised approach, we can beat the baseline by 0.10 for
the MAP score and by 0.176 for the P@1 score, which
is a significant improvement (p < 0.0001, using a two
tailed permutation test). To get insights of the contri-

System MAP P@1
Baseline 0.6408 0.5365
ALL 0.7048 0.6366
w/o DT 0.5798 0.4835
w/o UMLS 0.6618 0.5651
w/o Ngrams 0.7009 0.6252
w/o POS 0.7027 0.6323

Table 1: Results for the evaluation using substitute can-
didates from the DT.

bution of individual feature types, we perform an abla-
tion test. We observe that the most prominent features
are coming from the two DTs as we only achieve re-
sults below the baseline, when removing DT features.
We still obtain significant improvements over the base-
line when removing other feature groups. The second
most important feature comes from the UMLS. Fea-
tures coming from the Google n-grams improve the
system only slightly. The lowest improvement is de-
rived from the part-of-speech features. This leads us
to summarize that a hybrid approach for feature gen-
eration using manually created resources (UMLS) and
unsupervised features (DTs) leads to the best result for
lexical substitution for the medical domain.

8 Analysis
For a better insight into the lexical substitution we ana-
lyzed how often we outperform the baseline, get equal
results or get decreased scores. According to Table 2 in

performance # of instances Avg. ∆ MAP
decline 180 -0.16
equal 244 0
improvements 275 0.26

Table 2: Error analysis for the task respectively to the
MAP score.

around 26% of the cases we observe a decreased MAP
score, which is on average 0.16 smaller then the scores
achieved with the baseline. On the other hand, we see
improvements in around 39% of the cases: an average
improvements of 0.26, which is much higher then the
loss. For the remaining 25% of cases we observe the
same score.

Looking inside the data, the largest error class is
caused by antonyms. A sub-class of this error are
multi-word expressions having an adjective modifier.
This problems might be solved by additional features
using the UMLS resource. An example is shown in
Figure 1.

612



Figure 1: Example sentence for the target term mild
thrombocytopenia. The system returns a wrong rank-
ing, as the adjective changes the meaning and turns the
first ranked term into an antonym.

For feature generation, we currently lookup multi-
word expressions as one term, both in the DT and the
UMLS resource and do not split them into their sin-
gle tokens. This error also suggests considering the
single words inside the multi-word expression, espe-
cially adjectives, and looking them up in a resource
(e.g. UMLS) to detect synonymy and antonymy.

Figure 2 shows the case, where the ranking is per-
formed correctly, but the precise substitute is not an-
notated as a correct one. The term nail plate might be
even more precise in the context as the manual anno-
tated term nail bed. Due to the missing annotation the

Figure 2: Example sentence for the target term nails.
Here the ranking from the system is correct, but the first
substitute from the system was not annotated as such.

baseline gets better scores then the result from the sys-
tem.

9 Conclusion

In summary, we have examined the lexical substitution
task for the medical domain and could show that a sys-
tem for open domain text data can be applied to the

medical domain. We can show that following a hybrid
approach using features from UMLS and distributional
semantics leads to the best results. In future work, we
will work on integrating DTs using other context fea-
tures, as we could see an impact of using two different
DTs. Furthermore, we want to incorporate features us-
ing n-grams computed on a corpus from the domain
and include co-occurrence features.

Acknowledgments

We thank Adam Lally, Eric Brown, Edward A. Epstein,
Chris Biemann and Faisal Chowdhury for their helpful
comments.

References
Daniel Bär, Chris Biemann, Iryna Gurevych, and

Torsten Zesch. 2012. UKP: Computing Semantic
Textual Similarity by Combining Multiple Content
Similarity Measures. In Proceedings of the 6th In-
ternational Workshop on Semantic Evaluation, held
in conjunction with the 1st Joint Conference on Lex-
ical and Computational Semantics, pages 435–440,
Montreal, Canada.

Chris Biemann and Martin Riedl. 2013. Text: Now in
2D! A Framework for Lexical Expansion with Con-
textual Similarity. Journal of Language Modelling,
1(1):55–95.

Chris Biemann. 2012. Turk bootstrap word sense in-
ventory 2.0: A large-scale resource for lexical sub-
stitution. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC’12), Istanbul, Turkey.

Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram corpus version 1. Technical report, Google Re-
search.

Chris Buckley and Ellen M. Voorhees. 2004. Re-
trieval evaluation with incomplete information. In
Proceedings of the 27th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ’04, pages 25–32,
Sheffield, United Kingdom.

Miranda Chong and Lucia Specia. 2011. Lexical gen-
eralisation for word-level matching in plagiarism de-
tection. In Recent Advances in Natural Language
Processing, pages 704–709, Hissar, Bulgaria.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio M.
Zanzotto. 2013. Recognizing Textual Entailment:
Models and Applications. Synthesis Lectures on Hu-
man Language Technologies, 6(4):1–220.

Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Institut für maschinelle Sprachverarbeitung, Univer-
sity of Stuttgart.

613



Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.

Claudio Giuliano, Alfio Gliozzo, and Carlo Strappar-
ava. 2007. Fbk-irst: Lexical substitution task ex-
ploiting domain and syntagmatic coherence. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, SemEval ’07, pages 145–148,
Prague, Czech Republic.

Gerhard Kremer, Katrin Erk, Sebastian Padó, and Ste-
fan Thater. 2014. What Substitutes Tell Us - Anal-
ysis of an ”All-Words” Lexical Substitution Corpus.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2014), pages 540–549, Gothen-
burg, Sweden.

J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159–174.

Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139–159.

Michael C. McCord, J. William Murdock, and Bran-
imir K. Boguraev. 2012. Deep Parsing in Watson.
IBM J. Res. Dev., 56(3):264–278.

George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38:39–41.

György Szarvas, Chris Biemann, and Iryna Gurevych.
2013. Supervised All-Words Lexical Substitution
using Delexicalized Features. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT 2013),
pages 1131–1141, Atlanta, GA, USA.

614


