










































Linguistically-Augmented Bulgarian-to-English Statistical Machine Translation Model


Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 119–128,
Avignon, France, April 23 - 27 2012. c©2012 Association for Computational Linguistics

Linguistically-Augmented Bulgarian-to-English Statistical Machine
Translation Model

Rui Wang
Language Technology Lab

DFKI GmbH
Saarbrücken, Germany
ruiwang@dfki.de

Petya Osenova and Kiril Simov
Linguistic Modelling Department, IICT

Bulgarian Academy of Sciences
Sofia, Bulgaria

{petya,kivs}@bultreebank.org

Abstract

In this paper, we present our linguistically-
augmented statistical machine translation
model from Bulgarian to English, which
combines a statistical machine translation
(SMT) system (as backbone) with deep lin-
guistic features (as factors). The motiva-
tion is to take advantages of the robust-
ness of the SMT system and the linguis-
tic knowledge of morphological analysis
and the hand-crafted grammar through sys-
tem combination approach. The prelimi-
nary evaluation has shown very promising
results in terms of BLEU scores (38.85) and
the manual analysis also confirms the high
quality of the translation the system deliv-
ers.

1 Introduction

In the recent years, machine translation (MT)
has achieved significant improvement in terms
of translation quality (Koehn, 2010). Both
data-driven approaches (e.g., statistical MT
(SMT)) and knowledge-based (e.g., rule-based
MT (RBMT)) have achieved comparable results
shown in the evaluation campaigns (Callison-
Burch et al., 2011). However, according to the
human evaluation, the final outputs of the MT sys-
tems are still far from satisfactory.

Fortunately, recent error analysis shows that the
two trends of the MT approaches tend to be com-
plementary to each other, in terms of the types
of the errors they made (Thurmair, 2005; Chen et
al., 2009). Roughly speaking, RBMT systems of-
ten have missing lexicon and thus lack of robust-
ness, while handling linguistic phenomena requir-
ing syntactic information better. SMT systems, on

the contrary, are in general more robust, but some-
times output ungrammatical sentences.

In fact, instead of competing with each other,
there is also a line of research trying to com-
bine the advantages of the two sides using a
hybrid framework. Although many systems
can be put under the umbrella of “hybrid” sys-
tems, there are various ways to do the combi-
nation/integration. Thurmair (2009) summarized
several different architectures of hybrid systems
using SMT and RBMT systems. Some widely
used ones are: 1) using an SMT to post-edit the
outputs of an RBMT; 2) selecting the best trans-
lations from several hypotheses coming from dif-
ferent SMT/RBMT systems; and 3) selecting the
best segments (phrases or words) from different
hypotheses.

For the language pair Bulgarian-English, there
has not been much study on it, mainly due to the
lack of resources, including corpora, preproces-
sors, etc. There was a system published by Koehn
et al. (2009), which was trained and tested on the
European Union law data, but not on other do-
mains like news. They reported a very high BLEU
score (Papineni et al., 2002) on the Bulgarian-
English translation direction (61.3), which in-
spired us to further investigate this direction.

In this paper, we focus on the Bulgarian-to-
English translation and mainly explore the ap-
proach of annotating the SMT baseline with lin-
guistic features derived from the preprocessing
and hand-crafted grammars. There are three mo-
tivations behind our approach: 1) the SMT base-
line trained on a decent amount of parallel cor-
pora outputs surprisingly good results, in terms of
both statistical evaluation metrics and preliminary
manual evaluation; 2) the augmented model gives

119



us more space for experimenting with different
linguistic features without losing the ‘basic’ ro-
bustness; 3) the MT system can profit from con-
tinued advances in the development of the deep
grammars thereby opening up further integration
possibilities.

The rest of the paper will be organized as fol-
lows: Section 2 presents our work on cleaning
the corpora and Section 3 briefly describes the
preprocessing of the data. Section 4 introduces
our factor-based SMT model which allows us
to incorporate various linguistic features into an
SMT baseline, among which those features com-
ing from the MRS are described in Section 5 in
detail. We show our experiments in Section 6 as
well as both automatic and manual evaluation of
the results. Section 7 briefly mentions some re-
lated work and then we summarize this paper in
Section 8.

2 Data Preparation

In our experiments we are using the SETIMES
parallel corpus, which is part of the OPUS parallel
corpus1. The data in the corpus was aligned auto-
matically. Thus, we first checked the consistency
of the automatic alignments. It turned out that
more than 25% of the sentence alignments were
not correct. Since SETIMES appeared to be a
noisy dataset, our effort was directed into cleaning
it as much as possible before the start of the ex-
periments. We first corrected manually more than
25.000 sentence alignments. The the rest of the
data set includes around 135,000 sentences. Al-
together the data set is about 160,000 sentences,
when the manually checked part is added. Thus,
two actions were taken:

1. Improving the tokenization of the Bulgar-
ian part. The observations from the man-
ual check of the set of 25,000 sentences
showed systematic errors in the tokenized
text. Hence, these cases have been detected
and fixed semi-automatically.

2. Correcting and removing the suspicious
alignments. Initially, the ratio of the lengths
of the English and Bulgarian sentences was
calculated in the set of the 25,000 manually
annotated sentences. As a rule, the Bulgarian

1OPUS–an open source parallel corpus,
http://opus.lingfil.uu.se/

sentences are longer than the English ones.
The ratio is 1.34. Then we calculated the ra-
tio for each pair of sentences. After this, the
optimal interval was manually determined,
such that if the ratio for a given pair of sen-
tences is within the interval, then we assume
that the pair is a good one. The interval for
these experiments is set to [0.7; 1.8]. All the
pairs with ratio outside of the interval have
been deleted. Similarly, we have cleaned
EMEA dataset.

The size of the resulting datasets are: 151,718
sentence pairs for the SETIMES dataset. Simi-
lar approach was undertaken for another dataset
from OPUS corpus - EMEA. After the cleaning
704,631 sentence pairs were selected from the
EMEA dataset. Thus, the size of the original
datasets was decreased by 10%.

3 Linguistic Preprocessing

The data in SETIMES dataset was analysed on the
following levels:

• POS tagging. POS tagging is performed by
a pipe of several modules. First we apply
SVM POS tagger which takes as an input
a tokenised text and its output is a tagged
text. The performance is near 91% accuracy.
The SVM POS tagger is implemented us-
ing SVMTool (Gimnez and Mrquez, 2004).
Then we apply a morphological lexicon and
a set of rules. The lexicon add all the pos-
sible tags for the known words. The rules
reduce the ambiguity for some of the sure
cases. The result of this step is a tagged text
with some ambiguities unresolved. The third
step is application of the GTagger (Georgiev
et al., 2012). It is trained on an ambigu-
ous data and select the most appropriate tags
from the suggested ones. The accuracy of the
whole pipeline is 97.83%. In this pipeline
SVM POS Tagger plays the role of guesser
for the GTagger.

• Lemmatization. The lemmatization mod-
ule is based on the same morphological lexi-
con. From the lexicon we extracted functions
which convert each wordform into its basic
form (as a representative of the lemma). The
functions are defined via two operations on

120



wordforms: remove and concatenate. The
rules have the following form:

if tag = Tag then {remove OldEnd; concatenate
NewEnd}

where Tag is the tag of the wordform, Old-
End is the string which has to be removed
from the end of the wordform and NewEnd
is the string which has to be concatenated to
the beginning of the word form in order to
produce the lemma. The rules are for word
forms in the lexicon. Less than 2% of the
wordforms are ambiguous in the lexicon (but
they are very rare in real texts). Similar rules
are defined for unknown words. The accu-
racy of the lemmatizer is 95.23%.

• Dependency parsing. We have trained the
MALT Parser on the dependency version of
BulTreeBank2. We did this work together
with Svetoslav Marinov who has experience
in using the MALT Parser and Johan Hall
who is involved in thedevelopment of Malt
Parser. The trained model achieves 85.6%
labeled parsing accuracy. It is integrated in
a language pipe with the POS tagger and the
lemmatizer.

After the application of the language pipeline,
the result is represented in a table form following
the CoNLL shared task format3.

4 Factor-based SMT Model

Our approach is built on top of the factor-based
SMT model proposed by Koehn and Hoang
(2007), as an extension of the traditional phrase-
based SMT framework. Instead of using only the
word form of the text, it allows the system to take
a vector of factors to represent each token, both
for the source and target languages. The vec-
tor of factors can be used for different levels of
linguistic annotations, like lemma, part-of-speech
(POS), or other linguistic features. Furthermore,
this extension actually allows us to incorporate
various kinds of features if they can be (somehow)
represented as annotations to the tokens.

The process is quite similar to supertagging
(Bangalore and Joshi, 1999), which assigns “rich
descriptions (supertags) that impose complex

2http://www.bultreebank.org/dpbtb/
3http://ufal.mff.cuni.cz/conll2009-st/

task-description.html

constraints in a local context”. In our case, all
the linguistic features (factors) associated with
each token form a supertag to that token. Singh
and Bandyopadhyay (2010) had a similar idea
of incorporating linguistic features, while they
worked on Manipuri-English bidirectional trans-
lation. Our approach is slightly different from
(Birch et al., 2007) and (Hassan et al., 2007), who
mainly used the supertags on the target language
side, English. We primarily experiment with the
source language side, Bulgarian. This potentially
huge feature space provides us with various possi-
bilities of using our linguistic resources developed
in and out of our project.

In particular, we consider the following factors
on the source language side (Bulgarian):

• WF - word form is just the original text to-
ken.

• LEMMA is the lexical invariant of the orig-
inal word form. We use the lemmatizer
described in Section 3, which operates on
the output from the POS tagging. Thus,
the 3rd person, plural, imperfect tense verb
form ‘varvyaha’ (‘walking-were’, They were
walking) is lemmatized as the 1st person,
present tense verb ‘varvya’.

• POS - part-of-speech of the word. We use
the positional POS tag set of the BulTree-
Bank, where the first letter of the tag indi-
cates the POS itself, while the next letters re-
fer to semantic and/or morphosyntactic fea-
tures, such as: Dm - where ‘D’ stands for
‘adverb’, and ‘m’ stand for ‘modal’; Ncmsi
- where ‘N’ stand for ‘noun’, ‘c’ means
‘common’, ‘m’ is ‘masculine’, ‘s’ is ‘singu-
lar’,and ‘i’ is ‘indefinite’.

• LING - other linguistic features derived from
the POS tag in the BulTreeBank tagset (see
above).

In addition to these, we can also incorporate
syntactic structure of the sentence by breaking
down the tree into dependency relations. For in-
stance, a dependency tree can be represented as
a set of triples in the form of <parent, relation,
child>. <loves, subject, John> and <loves, ob-
ject, Mary> will represent the sentence “John
loves Mary”. Consequently, three additional fac-
tors are included for both languages:

121



• DEPREL - is the dependency relation be-
tween the current word and the parent node.

• HLEMMA is the lemma of the current word’s
parent node.

• HPOS is the POS tag of the current word’s
parent node.

Here is an example of a processed sentence.
The sentence is “spored odita v elektricheskite
kompanii politicite zloupotrebyavat s dyrzhavnite
predpriyatiya.” The glosses for the words in
the Bulgarian sentence are: spored (according)
odita (audit-the) v (in) elektricheskite (electrical-
the) kompanii (companies) politicite (politicians-
the) zloupotrebyavat (abuse) s (with) dyrzhavnite
(state-the) predpriyatiya (enterprises). The trans-
lation in the original source is : “electricity au-
dits prove politicians abusing public companies.”
The result from the linguistic processing and the
addition of information about head elements are
presented in the first seven columns of Table 1.

We extend the grammatical features to have the
same size. All the information is concatenated to
the word forms in the text. In the next section we
present how we extend this format to incorporate
the MRS analysis. In the next section we will ex-
tend this example to incorporate the MRS analysis
of the sentence.

5 MRS Supertagging

Our work on Minimal Recursion Semantic anal-
ysis of Bulgarian text is inspired by the work
on MRS and RMRS (Robust Minimal Recursion
Semantic) (see (Copestake, 2003) and (Copes-
take, 2007)) and the previous work on transfer
of dependency analyses into RMRS structures de-
scribed in (Spreyer and Frank, 2005) and (Jakob
et al., 2010). In this section we present first a short
overview of MRS and RMRS. Then we discuss
the new features added on the basis of the RMRS
structures.

MRS is introduced as an underspecified se-
mantic formalism (Copestake et al., 2005). It is
used to support semantic analyses in the English
HPSG grammar ERG (Copestake and Flickinger,
2000), but also in other grammar formalisms like
LFG. The main idea is that the formalism avoids
spelling out the complete set of readings resulting
from the interaction of scope bearing operators

and quantifiers, instead providing a single under-
specified representation from which the complete
set of readings can be constructed. Here we will
present only basic definitions from (Copestake et
al., 2005). For more details the cited publication
should be consulted. An MRS structure is a tu-
ple 〈 GT , R, C 〉, where GT is the top handle,
R is a bag of EPs (elementary predicates) and C
is a bag of handle constraints, such that there is
no handle h that outscopes GT . Each elementary
predication contains exactly four components: (1)
a handle which is the label of the EP; (2) a rela-
tion; (3) a list of zero or more ordinary variable
arguments of the relation; and (4) a list of zero or
more handles corresponding to scopal arguments
of the relation (i.e., holes). RMRS is introduced
as a modification of MRS which to capture the se-
mantics resulting from the shallow analysis. Here
the following assumption is taken into account the
shallow processor does not have access to a lexi-
con. Thus it does not have access to arity of the
relations in EPs. Therefore, the representation has
to be underspecified with respect to the number
of arguments of the relations. The names of rela-
tions are constructed on the basis of the lemma for
each wordform in the text and the main argument
for the relation is specified. This main argument
could be of two types: referential index for nouns
and event for the other part of speeches.

Because in this work we are using only the
RMRS relation and the type of the main argument
as features to the translation model, we will skip
here the explanation of the full structure of RMRS
structures and how they are constructed. Thus, we
firstly do a match between the surface tokens and
the MRS elementary predicates (EPs) and then
extract the following features as extra factors:

• EP - the name of the elementary predicate,
which usually indicates an event or an entity
semantically.

• EOV indicates the current EP is either an
event or a reference variable.

Notice that we do not take all the information
provided by the MRS, e.g., we throw away the
scopal information and the other arguments of the
relations. This kind of information is not straight-
forward to be represented in such ‘tagging’-style
models, which will be tackled in the future.

This information for the example sentence is
122



WF Lemma POSex Ling DepRel HLemma HPOS EP EoV
spored spored R adjunct zloupotrebyavam VP spored r e
odita odit Nc npd prepcomp spored R odit n v

v v R mod odit Nc v r e
elektricheskite elektricheski A pd mod kompaniya Nc elekticheski a e

kompanii kompaniya Nc fpi prepcomp v R kompaniya n v
politicite politik Nc mpd subj zloupotrebyavam Vp politik n v

zloupotrebyavat zloupotrebyavam Vp tir3p root - - zloupotrebyavam v e
s s R indobj zloupotrebyavam Vp s r e

dyrzhavnite dyrzhaven A pd mod predpriyatie Nc dyrzhaven a e
predpriyatiya predpriyatie Nc npi prepcomp s R predpriyatie n v

Table 1: The sentence analysis with added head information — HLemma and HPOS.

represented for each word form in the last two
columns of Table 1.

All these factors encoded within the corpus
provide us with a rich selection of factors for dif-
ferent experiments. Some of them are presented
within the next section. The model of encoding
MRS information in the corpus as additional fea-
tures does not depend on the actual semantic anal-
ysis — MRS or RMRS, because both of them pro-
vide enough semantic information.

6 Experiments

6.1 Experiments with the Bulgarian raw
corpus

To run the experiments, we use the phrase-based
translation model provided by the open-source
statistical machine translation system, Moses4

(Koehn et al., 2007). For training the translation
model, the parallel corpora (mentioned in Sec-
tion 2) were preprocessed with the tokenizer and
lowercase converter provided by Moses. Then the
procedure is quite standard:

• We run GIZA++ (Och and Ney, 2003) for bi-
directional word alignment, and then obtain
the lexical translation table and phrase table.

• A tri-gram language model is estimated us-
ing the SRILM toolkit (Stolcke, 2002).

• Minimum error rate training (MERT) (Och,
2003) is applied to tune the weights for the
set of feature weights that maximizes the of-
ficial f-score evaluation metric on the devel-
opment set.

The rest of the parameters we use the default
setting provided by Moses.

4http://www.statmt.org/moses/

We split the corpora into the training set, the
development set and the test set. For SETIMES,
the split is 100,000/500/1,000 and for EMEA, it
is 700,000/500/1,000. For reference, we also run
tests on the JRC-Acquis corpus5. The final results
under the standard evaluation metrics are shown
in the following table in terms of BLEU (Papineni
et al., 2002):

Corpora Test Dev Final Drop
SETIMES→ SETIMES 34.69 37.82 36.49 /

EMEA→ EMEA 51.75 54.77 51.62 /
SETIMES→ EMEA 13.37 / / 61.5%

SETIMES→ JRC-Acquis 7.19 / / 79.3%
EMEA→ SETIMES 7.37 / / 85.8%

EMEA→ JRC-Acquis 9.21 / / 82.2%

Table 2: Results of the baseline SMT system
(Bulgarian-English)

As we mentioned before, the EMEA corpus
is mainly about the description of medicine us-
age, and the format is quite fixed. Therefore, it
is not surprising to see high performance on the
in-domain test (2nd row in Table 2). SETIMES,
consisting of news articles, is in a less controlled
setting. The BLEU score is lower6. The results on
the out-of-domain tests are in general much lower
with a drop of more than 60% in BLEU score (the
last column). For the JRC-Acquis corpus, in con-
trast to the in-domain scores given by Koehn et
al. (2009) (61.3), the low out-of-domain results
shows a very similar situation as EMEA. A brief
manual check of the results indicate that the out-
of-domain tests suffer severely from the missing

5http://optima.jrc.it/Acquis/
6Actually, the BLEU score itself is higher than for most

of the other language pairs http://matrix.statmt.
org/. As the datasets are different, the results are not di-
rectly comparable. Here, we just want to get a rough pic-
ture. Achieving better performance for Bulgarian-to-English
translation than for other language pairs is not the focus of
the paper.

123



lexicon, while the in-domain test for the news arti-
cles contains more interesting issues to look into.
The better translation quality also makes the sys-
tem outputs human readable.

6.2 Experiments with the
Linguistically-Augmented Bulgarian
Corpus

As we described the factor-based model in Sec-
tion 4, we also perform experiments to test the
effectiveness of different linguistic annotations.
The different configurations we considered are
shown in the first column of Table 3.

These models can be roughly grouped into
five categories: word form with linguistic fea-
tures; lemma with linguistic features; models
with dependency features; MRS elementary pred-
icates (EP) and the type of the main argument of
the predicate (EOV); and MRS features without
word forms. The setting of the system is mostly
the same as the previous experiment, except for
1) increasing the training data from 100,000 to
150,000 sentence pairs; 2) specifying the factors
during training and decoding; and 3) without do-
ing MERT7. We perform the finer-grained model
only on the SETIMES data, as the language is
more diverse (compared to the other two corpora).
The results are shown in Table 3.

The first model is served as the baseline here.
We show all the n-gram scores besides the final
BLEU, since the some of the differences are very
small. In terms of the numbers, POS seems to
be an effective factor, as Model 2 has the highest
score. Model 3 indicates that linguistic features
also improve the performance. Model 4-6 show
the necessity of including the word form as one
of the factors, in terms of BLEU scores. Model
10 shows significant decrease after incorporating
HLEMMA feature. This may be due to the data
sparsity, as we are actually aligning and translat-
ing bi-grams instead of tokens. This may also in-
dicate that increasing the number of factors does
not guarantee performance enhancement. After
replacing the HLEMMA with HPOS, the result is
close to the others (Model 8). The experiments
with features from the MRS analyses (Model 11-
16) show improvements over the baseline consis-
tently and using only the MRS features (Model

7This is mainly due to the large amount of computation
required. We will perform MERT on the better-performing
configurations in the future.

17-18) also delivers descent results. In future ex-
periments we will consider to include more fea-
ture from the MRS analyses.

So far, incorporating additional linguistic
knowledge has not shown huge improvement in
terms of statistical evaluation metrics. However,
this does not mean that the translations delivered
are the same. In order to fully evaluate the system,
manual analysis is absolutely necessary. We are
still far from drawing a conclusion at this point,
but the preliminary scores calculated already indi-
cate that the system can deliver decent translation
quality consistently.

6.3 Manual Evaluation
We manually validated the output for all the mod-
els mentioned in Table 3. The guideline in-
cludes two aspects of the quality of the transla-
tion: Grammaticality and Content. Grammati-
cality can be evaluated solely on the system out-
put and Content by comparison with the reference
translation. We use a 1-5 score for each aspect as
follows:

Grammaticality

1. The translation is not understandable.

2. The evaluator can somehow guess the mean-
ing, but cannot fully understand the whole
text.

3. The translation is understandable, but with
some efforts.

4. The translation is quite fluent with some mi-
nor mistakes or re-ordering of the words.

5. The translation is perfectly readable and
grammatical.

Content

1. The translation is totally different from the
reference.

2. About 20% of the content is translated, miss-
ing the major content/topic.

3. About 50% of the content is translated, with
some missing parts.

4. About 80% of the content is translated, miss-
ing only minor things.

5. All the content is translated.

For the missing lexicons or not-translated
Cyrillic tokens, we ask the evaluators to score 2

124



ID Model BLEU 1-gram 2-gram 3-gram 4-gram
1 WF 38.61 69.9 44.6 31.5 22.7
2 WF, POS 38.85 69.9 44.8 31.7 23.0
3 WF, LEMMA, POS, LING 38.84 69.9 44.7 31.7 23.0
4 LEMMA 37.22 68.8 43.0 30.1 21.5
5 LEMMA, POS 37.49 68.9 43.2 30.4 21.8
6 LEMMA, POS, LING 38.70 69.7 44.6 31.6 22.8
7 WF, DEPREL 36.87 68.4 42.8 29.9 21.1
8 WF, DEPREL, HPOS 36.21 67.6 42.1 29.3 20.7
9 WF, LEMMA, POS, LING, DEPREL 36.97 68.2 42.9 30.0 21.3

10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 29.57 60.8 34.9 23.0 15.7
11 WF, POS, EP 38.74 69.8 44.6 31.6 22.9
12 WF, POS, LING, EP 38.76 69.8 44.6 31.7 22.9
13 WF, EP, EOV 38.74 69.8 44.6 31.6 22.9
14 WF, POS, EP, EOV 38.74 69.8 44.6 31.6 22.9
15 WF, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9
16 WF, POS, LING, EP, EOV 38.76 69.8 44.6 31.7 22.9
17 EP, EOV 37.22 68.5 42.9 30.2 21.6
18 EP, EOV, LING 38.38 69.3 44.2 31.3 22.7

Table 3: Results of the factor-based model (Bulgarian-English, SETIMES 150,000)

for one Cyrillic token and score 1 for more than
one tokens in the output translation.

The results are shown in the following two ta-
bles, Table 4 and Table 5, respectively. The cur-
rent results from the manual validation are on the
basis of 150 sentence pairs. The numbers shown
in the tables are the number of sentences given the
corresponding scores. The ‘Total’ column sums
up the scores of all the output sentences by each
model.

The results show that linguistic and seman-
tic analyses definitely improve the quality of the
translation. Exploiting the linguistic processing
on word level — LEMMA, POS and LING — pro-
duces the best result. However, the model with
only EP and EOV features also delivers very good
results, which indicates the effectiveness of the
MRS features from the deep hand-crafted gram-
mars. Including more factors (especially the in-
formation from the dependency parsing) drops the
results because of the sparseness effect over the
dataset, which is consistent with the automatic
evaluation BLEU score. The last two rows are
shown for reference. ‘Google’ shows the results
of using the online translation service provided by
http://translate.google.com/. The
high score (very close to the reference translation)
may be because our test data are not excluded
from their training data. In future we plan to do
the same evaluation with a larger dataset.

The problem with the untranslated Cyrillic to-

kens in our view could be solved in most of the
cases by providing additional lexical information
from a Bulgarian-English lexicon. Thus, we also
evaluated the possible impact of such a lexicon if
it had been available. In order to do this, we sub-
stituted each copied Cyrillic token with its trans-
lation when there was only one possible transla-
tion. We did such substitutions for 189 sentence
pairs. Then we evaluated the result by classify-
ing the translations as acceptable or unacceptable.
The number of the acceptable translations are 140
in this case.

The manual evaluation of the translation mod-
els on a bigger scale is in progress. The current re-
sults are promising. Statistical evaluation metrics
can give us a brief overview of the system perfor-
mance, but the actual translation quality is much
more interesting to us, as in many cases, the dif-
ferent surface translations can convey exactly the
same meaning in the context.

7 Related Work

Our work is also enlightened by another line of
research, transfer-based MT models, which are
seemingly different but actually very close. In this
section, before we mention some previous work
in this research direction, we firstly introduce the
background of the development of the deep HPSG
grammars.

The MRSes are usually delivered together with
the HPSG analyses of the text. There already

125



ID Model 1 2 3 4 5 Total
1 WF 20 47 5 32 46 487
2 WF, POS 20 48 5 37 40 479
3 WF, LEMMA, POS, LING 20 47 6 34 43 483
4 LEMMA 15 34 11 46 44 520
5 LEMMA, POS 15 38 12 51 34 501
6 LEMMA, POS, LING 20 48 5 34 43 482
7 WF, DEPREL 32 48 3 29 38 443
8 WF, DEPREL, HPOS 45 41 7 23 34 410
9 WF, LEMMA, POS, LING, DEPREL 34 47 5 30 34 433

10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 101 32 0 8 9 242
11 WF, POS, EP 19 49 4 34 44 485
12 WF, POS, LING, EP 19 49 3 39 40 482
13 WF, EP, EOV 20 49 2 41 38 478
14 WF, POS, EP, EOV 19 50 3 31 47 487
15 WF, LING, EP, EOV 19 48 5 37 41 483
16 WF, POS, LING, EP, EOV 19 49 5 37 40 480
17 EP, EOV 15 41 10 44 40 503
18 EP, EOV, LING 20 49 7 38 36 471
19 GOOGLE 0 2 20 52 76 652
20 REFERENCE 0 0 5 51 94 689

Table 4: Manual evaluation of the grammaticality

exist quite extensive implemented formal HPSG
grammars for English (Copestake and Flickinger,
2000), German (Müller and Kasper, 2000), and
Japanese (Siegel, 2000; Siegel and Bender, 2002).
HPSG is the underlying theory of the interna-
tional initiative LinGO Grammar Matrix (Bender
et al., 2002). At the moment, precise and lin-
guistically motivated grammars, customized on
the base of the Grammar Matrix, have been or
are being developed for Norwegian, French, Ko-
rean, Italian, Modern Greek, Spanish, Portuguese,
Chinese, etc. There also exists a first version of
the Bulgarian Resource Grammar - BURGER. In
the research reported here, we use the linguistic
modeled knowledge from the existing English and
Bulgarian grammars. Since the Bulgarian gram-
mar has limited coverage on news data, depen-
dency parsing has been performed instead. Then,
mapping rules have been defined for the construc-
tion of RMRSes.

However, the MRS representation is still quite
close to the syntactic level, which is not fully lan-
guage independent. This requires a transfer at the
MRS level, if we want to do translation from the
source language to the target language. The trans-
fer is usually implemented in the form of rewrit-
ing rules. For instance, in the Norwegian LO-
GON project (Oepen et al., 2004), the transfer
rules were hand-written (Bond et al., 2005; Oepen

et al., 2007), which included a large amount of
manual work. Graham and van Genabith (2008)
and Graham et al. (2009) explored the automatic
rule induction approach in a transfer-based MT
setting involving two lexical functional grammars
(LFGs), which was still restricted by the perfor-
mance of both the parser and the generator. Lack
of robustness for target side generation is one of
the main issues, when various ill-formed or frag-
mented structures come out after transfer. Oepen
et al. (2007) use their generator to generate text
fragments instead of full sentences, in order to in-
crease the robustness. We want to make use of
the grammar resources while keeping the robust-
ness, therefore, we experiment with another way
of transfer involving information derived from the
grammars.

In our approach, we take an SMT system as our
‘backbone’ which robustly delivers some trans-
lation for any given input. Then, we augment
SMT with deep linguistic knowledge. In general,
what we are doing is still along the lines of previ-
ous work utilizing deep grammars, but we build a
more ‘light-weighted’ transfer model.

8 Conclusion and Future Work

In this paper, we report our work on build-
ing a linguistically-augmented statistical machine
translation model from Bulgarian to English.

126



ID Model 1 2 3 4 5 Total
1 WF 20 46 5 23 56 499
2 WF, POS 20 48 5 24 53 492
3 WF, LEMMA, POS, LING 20 47 1 24 58 503
4 LEMMA 15 32 5 33 65 551
5 LEMMA, POS 15 35 9 32 59 535
6 LEMMA, POS, LING 20 48 5 22 55 494
7 WF, DEPREL 32 49 4 14 51 453
8 WF, DEPREL, HPOS 45 41 2 21 41 422
9 WF, LEMMA, POS, LING, DEPREL 34 48 3 20 45 444

10 WF, LEMMA, POS, LING, DEPREL, HLEMMA 101 32 0 6 11 244
11 WF, POS, EP 19 49 3 20 59 501
12 WF, POS, LING, EP 19 50 2 20 59 500
13 WF, EP, EOV 19 50 4 16 61 500
14 WF, POS, EP, EOV 19 50 2 23 56 497
15 WF, LING, EP, EOV 19 48 4 18 61 504
16 WF, POS, LING, EP, EOV 19 50 3 24 54 494
17 EP, EOV 14 38 7 31 60 535
18 EP, EOV, LING 19 49 7 20 55 493
19 GOOGLE 1 0 9 42 98 686
20 REFERENCE 1 0 5 37 107 699

Table 5: Manual evaluation of the content

Based on our observations of the previous ap-
proaches on transfer-based MT models, we de-
cide to build a hybrid system by combining an
SMT system with deep linguistic resources. We
perform a preliminary evaluation on several con-
figurations of the system (with different linguis-
tic knowledge). The high BLEU score shows the
high quality of the translation delivered by the
SMT baseline; and manual analysis confirms the
consistency of the system.

There are various aspects we can improve the
ongoing project: 1) The MRSes are not fully ex-
plored yet, since we have only considered the EP
and EOV features. 2) We would like to add factors
on the target language side (English) as well. 3)
The guideline of the manual evaluation needs fur-
ther refinement for considering the missing lexi-
cons as well as how much of the content is truly
conveyed (Farreús et al., 2011). 4) We also need
more experiments to evaluate the robustness of
our approach in terms of out-domain tests.

Acknowledgements

This work was supported by the EuroMatrix-
Plus project (IST-231720) funded by the Euro-
pean Community under the Seventh Framework
Programme for Research and Technological De-
velopment. The authors would like to thank Tania
Avgustinova for fruitful discussions and her help-

ful linguistic analysis; and also to Laska Laskova,
Stanislava Kancheva and Ivaylo Radev for doing
the human evaluation of the data.

References
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertag-

ging: an approach to almost parsing supertagging: an ap-
proach to almost parsing supertagging: an approach to
almost parsing. Computational Linguistics, 25(2), June.

Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar Matrix. An open-source starter-kit
for the rapid development of cross-linguistically consis-
tent broad-coverage precision grammar. In Proceedings
of the Workshop on Grammar Engineering and Evalua-
tion at the 19th International Conference on Computa-
tional Linguistics, Taipei, Taiwan.

Alexandra Birch, Miles Osborne, and Philipp Koehn. 2007.
Ccg supertags in factored statistical machine translation.
In Proceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 9–16, Prague, Czech Republic,
June.

Francis Bond, Stephan Oepen, Melanie Siegel, Ann Copes-
take, and Dan Flickinger. 2005. Open source machine
translation with DELPH-IN. In Proceedings of the Open-
Source Machine Translation Workshop at the 10th Ma-
chine Translation Summit, pages 15 – 22, Phuket, Thai-
land, September.

Chris Callison-Burch, Philipp Koehn, Christof Monz, and
Omar F. Zaidan. 2011. Findings of the 2011 workshop
on statistical machine translation. In Proceedings of the
6th Workshop on SMT.

Yu Chen, M. Jellinghaus, A. Eisele, Yi Zhang, S. Hunsicker,
S. Theison, Ch. Federmann, and H. Uszkoreit. 2009.

127



Combining multi-engine translations with moses. In Pro-
ceedings of the 4th Workshop on SMT.

Ann Copestake and Dan Flickinger. 2000. An open source
grammar development environment and broad-coverage
english grammar using hpsg. In Proceedings of the 2nd
International Conference on Language Resources and
Evaluation, Athens, Greece.

Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan Sag.
2005. Minimal recursion semantics: An introduction.
Research on Language & Computation, 3(4):281–332.

Ann Copestake. 2003. Robust minimal recursion semantics
(working paper).

Ann Copestake. 2007. Applying robust semantics. In Pro-
ceedings of the 10th Conference of the Pacific Assocation
for Computational Linguistics (PACLING), pages 1–12.

Mireia Farreús, Marta R. Costa-jussà, and Maja Popović
Morse. 2011. Study and correlation analysis of linguis-
tic, perceptual and automatic machine translation evalu-
ations. Journal of the American Society for Information
Sciences and Technology, 63(1):174–184, October.

Georgi Georgiev, Valentin Zhikov, Petya Osenova, Kiril
Simov, and Preslav Nakov. 2012. Feature-rich part-of-
speech tagging for morphologically complex languages:
Application to bulgarian. In Proceedings of EACL 2012.
MIT Press, Cambridge, MA, USA.

Jess Gimnez and Llus Mrquez. 2004. Svmtool: A general
pos tagger generator based on support vector machines.
In Proceedings of the 4th LREC.

Yvette Graham and Josef van Genabith. 2008. Packed rules
for automatic transfer-rule induction. In Proceedings of
the European Association of Machine Translation Con-
ference (EAMT 2008), pages 57–65, Hamburg, Germany,
September.

Yvette Graham, Anton Bryl, and Josef van Genabith. 2009.
F-structure transfer-based statistical machine translation.
In Proceedings of the Lexical Functional Grammar Con-
ference, pages 317–328, Cambridge, UK. CSLI Publica-
tions, Stanford University, USA.

Hany Hassan, Khalil Sima’an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation. In
Proceedings of ACL, Prague, Czech Republic, June.

Max Jakob, Markéta Lopatková, and Valia Kordoni. 2010.
Mapping between dependency structures and composi-
tional semantic representations. In Proceedings of the
7th International Conference on Language Resources and
Evaluation (LREC 2010), pages 2491–2497.

Philipp Koehn and Hieu Hoang. 2007. Factored translation
models. In Proceedings of EMNLP.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of ACL
(demo session).

Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe. In
Proceedings of MT Summit XII.

Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, January.

Stefan Müller and Walter Kasper. 2000. HPSG analy-
sis of German. In Wolfgang Wahlster, editor, Verbmo-
bil. Foundations of Speech-to-Speech Translation, pages
238 – 253. Springer, Berlin, Germany, artificial intelli-
gence edition.

Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment models.
Computational Linguistics, 29(1).

Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL.

Stephan Oepen, Helge Dyvik, Jan Tore Lønning, Erik Vell-
dal, Dorothee Beermann, John Carroll, Dan Flickinger,
Lars Hellan, Janne Bondi Johannessen, Paul Meurer,
Torbjørn Nordgård, , and Victoria Rosén. 2004. Som å
kapp-ete med trollet? towards MRS-based norwegian to
english machine translation. In Proceedings of the 10th
International Conference on Theoretical and Method-
ological Issues in Machine Translation, Baltimore, MD.

Stephan Oepen, Erik Velldal, Jan Tore Lønning, Paul
Meurer, Victoria Rosén, and Dan Flickinger. 2007. To-
wards hybrid quality-oriented machine translation — on
linguistics and probabilities in MT. In Proceedings of the
11th Conference on Theoretical and Methodological Is-
sues in Machine Translation (TMI-07), Skovde, Sweden.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of ACL.

Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of japanese. In Proceedings of the 19th
International Conference on Computational Linguistics,
Taipei, Taiwan.

Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil. Foundations of Speech-
to-Speech Translation, pages 265 – 280. Springer, Berlin,
Germany, artificial intelligence edition.

Thoudam Doren Singh and Sivaji Bandyopadhyay. 2010.
Manipuri-english bidirectional statistical machine trans-
lation systems using morphology and dependency rela-
tions. In Proceedings of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation, pages 83–91,
Beijing, China, August.

Kathrin Spreyer and Anette Frank. 2005. Projecting RMRS
from TIGER Dependencies. In Proceedings of the HPSG
2005 Conference, pages 354–363, Lisbon, Portugal.

Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, volume 2.

Gregor Thurmair. 2005. Hybrid architectures for machine
translation systems. Language Resources and Evalua-
tion, 39(1).

Gregor Thurmair. 2009. Comparing different architectures
of hybrid machine translation systems. In Proceedings of
MT Summit XII.

128


