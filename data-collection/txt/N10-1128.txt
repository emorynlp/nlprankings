










































Context-free reordering, finite-state translation


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 858–866,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Context-free reordering, finite-state translation

Chris Dyer and Philip Resnik
UMIACS Laboratory for Computational Linguistics and Information Processing

Department of Linguistics
University of Maryland, College Park, MD 20742, USA

redpony, resnik AT umd.edu

Abstract

We describe a class of translation model in
which a set of input variants encoded as a
context-free forest is translated using a finite-
state translation model. The forest structure of
the input is well-suited to representing word
order alternatives, making it straightforward to
model translation as a two step process: (1)
tree-based source reordering and (2) phrase
transduction. By treating the reordering pro-
cess as a latent variable in a probabilistic trans-
lation model, we can learn a long-range source
reordering model without example reordered
sentences, which are problematic to construct.
The resulting model has state-of-the-art trans-
lation performance, uses linguistically moti-
vated features to effectively model long range
reordering, and is significantly smaller than a
comparable hierarchical phrase-based transla-
tion model.

1 Introduction

Translation models based on synchronous context-
free grammars (SCFGs) have become widespread in
recent years (Wu, 1997; Chiang, 2007). Compared
to phrase-based models, which can be represented as
finite-state transducers (FSTs, Kumar et al. (2006)),
one important benefit that SCFG models have is the
ability to process long range reordering patterns in
space and time that is polynomial in the length of
the displacement, whereas an FST must generally
explore a number of states that is exponential in
this length.1 As one would expect, for language

1Our interest here is the reordering made possible by varying
the arrangement of the translation units, not the local word order
differences captured inside memorized phrase pairs.

pairs with substantial structural differences (and thus
requiring long-range reordering during translation),
SCFG models have come to outperform the best FST
models (Zollmann et al., 2008).

In this paper, we explore a new way to take advan-
tage of the computational benefits of CFGs during
translation. Rather than using a single SCFG to both
reorder and translate a source sentence into the target
language, we break the translation process into a two
step pipeline where (1) the source language is re-
ordered into a target-like order, with alternatives en-
coded in a context-free forest, and (2) the reordered
source is transduced into the target language using
an FST that represents phrasal correspondences.

While multi-step decompositions of the transla-
tion problem have been proposed before (Kumar et
al., 2006), they are less practical with the rise of
SCFG models, since the context-free languages are
not closed under intersection (Hopcroft and Ullman,
1979). However, the CFLs are closed under intersec-
tion with regular languages. By restricting ourselves
to a finite-state phrase transducer and representing
reorderings of the source in a context-free forest, ex-
act inference over the composition of the two models
is possible.

The paper proceeds as follows. We first ex-
plore reordering forests and describe how to trans-
late them with an FST (§2). Since we would like our
reordering model to discriminate between good re-
orderings of the source and bad ones, we show how
to train our reordering component as a latent variable
in an end-to-end translation model (§3). We then
presents experimental results on language pairs re-
quiring small amounts and large amounts of reorder-
ing (§4). We conclude with a discussion of related

858



work (§6) and possible extensions (§7).

2 Reordering forests and translation

In this section, we describe source reordering
forests, a context-free representation of source lan-
guage word order alternatives.2 The basic idea is
that for the source sentence, f, that is to be trans-
lated, we want to create a (monolingual) context-free
grammarF that generates strings (f′) of words in the
source language that are permutations of the origi-
nal sentence. Specifically, this forest should contain
derivations that put the source words into an order
that approximates how they will be ordered in the
grammar of the target language.

For a concrete example, let us consider the task of
English-Japanese translation.3 Our input sentence
is John ate an apple. Japanese is a head-final lan-
guage, where the heads of phrases (such as the verb
in a verb phrase) typically come last, and English
is a head-initial language, where heads come first.
As a result, the usual order for a declarative sen-
tence in English is SVO (subject-verb-object), but
in Japanese, it is SOV, and the desired translation
is John-ga ringo-o [an apple] tabeta [ate]. In sum-
mary, when translating from English into Japanese,
it is usually necessary to move verbs from their po-
sition between the subject and object to the end of
the sentence.

This reordering can happen in two ways, which
we depict in Figure 1. In the derivation on the left,
a memorized phrase pair captures the movement of
the verb (Koehn et al., 2003). In the other deriva-
tion, the source is first reordered into target word
order and then translated, using smaller translation
units. In addition, we have assumed that the phrase
translations were learned from a parallel corpus that
is in the original ordering, so the reordering forest F
should include derivations of phrase-size units in the
source order as well as the target order.

2Note that forests are isomorphic to context-free grammars.
For example, what is referred to as the ‘parse forest’, and un-
derstood to encode all derivations of a sentence s under some
grammar, can also be understood as being a context-free gram-
mar itself that exactly generates s. We therefore refer to a forest
as a grammar sometimes, or vice versa, depending on which
characterization is clearer in context.

3We use English as the source language since we expect the
parse structure of English sentences will be more familiar to
many readers.

0 1

an : ε

apple : リンゴを 

John : ジョンが 

ate : 食べた 

[John-ga]

[ringo-o]

[tabeta]

23

ate : εan : ε

apple : リンゴを  食べた  
[ringo-o   tabeta]

Figure 2: A fragment of a phrase-based English-Japanese
translation model, represented as an FST. Japanese ro-
manization is given in brackets.

A minimal reordering forest that supports the
derivations depicted needs to include both an SOV
and SVO version of the source. This could be ac-
complished trivially with the following grammar:

S → John ate an apple
S → John an apple ate

However, this grammar misses the opportunity to
take advantage of the regularities in the permuted
structure. A better alternative might be:

S → John VP
VP → ate NP
VP → NP ate
NP → an apple

In this grammar, the phrases John and an apple are
fixed and only the VP contains ordering ambiguity.

2.1 Reordering forests based on source parses

Many kinds of reordering forests are possible; in
general, the best one for a particular language pair
will be one that is easiest to create given the re-
sources available in the source language. It will
also be the one that most compactly expresses the
source reorderings that are most likely to be use-
ful for translation. In this paper, we consider a
particular kind of reordering forest that is inspired
by the reordering model of Yamada and Knight
(2001).4 These are generated by taking a source lan-
guage parse tree and ‘expanding’ each node so that it

4One important difference is that our translation model is not
restricted by the structure of the source parse tree; i.e., phrases
used in transduction need not correspond to constituents in the
source reordering forest. However, if a phrase does cross a con-
stituent boundary between constituents A and B, then transla-
tions that use that phrase will have A and B adjacent.

859



ジョンが    リンゴを   食べた 
John-ga      ringo-o     tabeta

John an apple ate

ジョンが    リンゴを   食べた 
John-ga      ringo-o     tabeta

John ate an apple

John ate an appleJohn ate an applef

f'

e

Figure 1: Two possible derivations of a Japanese translation of an English source sentence.

rewrites with different permutations of its children.5

For an illustration using our example sentence, re-
fer to Figure 3 for the forest representation and Fig-
ure 4 for its isomorphic CFG representation. It is
easy to see that this forest generates the two ‘good’
order variants from Figure 1; however, the forest in-
cludes many other derivations that will probably not
lead to good translations. For this reason, it is help-
ful to associate the edges in the forest (that is, the
rules in the CFG) with weights reflecting how likely
that rule is to lead to a good translation. We discuss
how these weights can be learned automatically in
§3.

2.2 Translating reordering forests with FSTs

Having described how to construct a context-free re-
ordering forest for the source sentence, we now turn
to the problem of how to translate the source forest
into the target language using a phrase-based trans-
lation model encoded as an FST, e.g. Figure 2. The
process is quite similar to the one used when trans-
lating a source sentence with an SCFG, but with a
twist: rather than representing the translation model
as a grammar and parsing the source sentence, we
represent the source sentence as a grammar (i.e. its
reordering forest), and we use it to ‘parse’ the trans-
lation model (i.e. the FST representation of the
phrase-based model). The end result (either way!)
is a translation forest containing all possible target-
language translations of the source.

Parsing can be understood as a means of comput-
ing the intersection of an FSA and a CFG (Grune and
Jacobs, 2008). Since we are dealing with FSTs that
define binary relations over strings, not FSAs defin-
ing strings, this operation is more properly compo-
sition. However, since CFG/FSA intersection is less

5For computational tractability, we only consider all permu-
tations only when the number of children is less than 5, other-
wise we exclude permutations where a child moves more than
4 positions away from where it starts.

cumbersome to describe, we present the algorithm
in terms of intersection.

To compute the composition of a reordering for-
est, G, with an FSA, F , we will make use of a variant
of Earley’s algorithm (Earley, 1970). Let weighted
finite-state automaton F = 〈Σ, Q, q0, qfinal, δ, w〉.
Σ is a finite alphabet; Q is a set of states; q0 and
qfinal ∈ Q are start and accept states, respectively,6 δ
is the transition function Q× Σ→ 2Q, and w is the
transition cost function Q × Q → R. We use vari-
ables that refer to states in the FSA with the letters
q, r, and s. We use x to represent a variable that is
an element of Σ. Variables u and v represent costs.
X and Y are non-terminals. Lowercase Greek let-
ters are strings of terminals and non-terminals. The
function δ(q, x) returns the state(s) that are reach-
able from state q by taking a transition labeled with
x in the FSA.

Figure 5 provides the inference rules for a
top-down intersection algorithm in the form of a
weighted logic program; the three inference rules
correspond to Earley’s SCAN, PREDICT, and COM-
PLETE, respectively.

3 Reordering and translation model

As pointed out in §2.1, our reordering forests may
contain many paths, some of which when translated
will lead to good translations and others that will be
bad. We would like a model to distinguish the two.

If we had a parallel corpus of source language
sentences paired with ‘reference reorderings‘, such
a model could be learned directly as a supervised
learning task. However, creating the optimal target-
language reordering f′ for some f is a nontrivial
task.7 Instead of trying to solve this problem, we
opt to treat the reordered from of the source, f′, as a

6Other FSA definitions permit sets of start and final states.
We use the more restricted definition for simplicity and because
in our FSTs q0 = qfinal.

7For a discussion of methods for generating reference re-

860



Original parse:

Reordering forest:

S

V DT NN

VP

NPsubj

NPobj

John ate an apple

1 1

1

1

1

1

22

2

22

2

S

V DT NN

VP

NPsubj

NPobj

John ate an apple

1 1

1

2

2

2

Figure 3: Example of a reordering forest. Linearization
order of non-terminals is indicated by the index at the tail
of each edge. The isomorphic CFG is shown in Figure 4;
dashed edges correspond to reordering-specific rules.

latent variable in a probabilistic translation model.
By doing this, we only require a parallel corpus of
translations to learn the reordering model. Not only
does this make our lives easier, since ‘reference re-
orderings’ are not necessary, but it is also intuitively
satisfying because from a task perspective, we are
not concerned with values of f′, but only with pro-
ducing a good translation e.

3.1 A probabilistic translation model with a
latent reordering variable

The translation model we use is a two phase process.
First, source sentence f is reordered into a target-
like word order f′ according to a reordering model
r(f′|f). The reordered source is then transduced into
the target language according to a translation model
t(e|f′). We require that r(f′|f) can be represented by
orderings from word aligned parallel corpora, refer to Tromble
and Eisner (2009).

Original parse grammar: S→ NPsubj VP
VP→ V NPobj NPobj → DT NN

NPsubj → John V→ ate
DT→ an NN→ apple

Additional reordering grammar rules:
S→ VP NPsubj

VP→ NPobj V
NPobj → NN DT

Figure 4: Context-free grammar representation of the for-
est in Figure 3. The reordering grammar contains the
parse grammar, plus the reordering-specific rules.

Initialization:

[S′ → •S, q0, q0] : 1

Inference rules:

[X → α • xβ, q, r] : u
[X → αx • β, q, δ(r, x)] : u⊗ w(δ(r, x))

[X → α • Y β, q, r]
[Y → •γ, r, r] : u

Y
u−→ γ ∈ G

[X → α • Y β, q, s] : u [Y → γ•, s, r] : v
[X → αY • β, q, r] : u⊗ v

Goal state:
[S′ → S•, q0, qfinal]

Figure 5: Weighted logic program for computing the in-
tersection of a weighted FSA and a weighted CFG.

a recursion-free probabilistic context-free grammar,
i.e. a forest as in §2.1, and that t(e|f′) is represented
by a (cyclic) finite-state transducer, as in Figure 2.

Since the reordering forest may define multiple
derivations a from f to a particular f′, and the trans-
ducer may define multiple derivations d from f′ to
a particular translation e, we marginalize over these
nuisance variables as follows to define the probabil-
ity of a translation given the source:

p(e|f) =
∑

d

∑
f′
t(e,d|f′)

∑
a
r(f′, a|f) (1)

Crucially, since we have restricted r(f′|f) to have
the form of a weighted CFG and t(e|f′) to be an

861



FST, the quantity (1), which sums over all reorder-
ings (and derivations), can be computed in polyno-
mial time with dynamic programming composition,
as described in §2.2.

3.2 Conditional training
While it is straightforward to use expectation maxi-
mization to optimize the joint likelihood of the paral-
lel training data with a latent variable model, instead
we use a log-linear parameterization and maximize
conditional likelihood (Blunsom et al., 2008; Petrov
and Klein, 2008). This enables us to employ a rich
set of (possibly overlapping, non-independent) fea-
tures to discriminate among translations. The proba-
bility of a derivation from source to reordered source
to target is thus written in terms of model parameters
Λ = {λi} as:

p(e,d, f′, a|f; Λ) =
exp

∑
i λi ·Hi(e,d, f

′, a, f)
Z(f; Λ)

where Hi(e,d, f′, a, f) =
∑
r∈d

hi(f′, r) +
∑
s∈a

hi(f, s)

The derivation probability is globally normalized by
the partition Z(f; Λ), which is just the sum of the
numerator for all derivations of f (corresponding to
any e). The Hi (written below without their argu-
ments) are real-valued feature functions that may
be overlapping and non-independent. For compu-
tational tractability, we assume that the feature func-
tions Hi decompose with the derivations of f′ and e
in terms of local feature functions hi. We also de-
fineZ(e, f;λ) to be the sum of the numerator over all
derivations that yield the sentence pair 〈e, f〉. Rather
than training purely to optimize conditional likeli-
hood, we also make use of a spherical Gaussian prior
on the value of Λ with mean 0 and variance σ2,
which helps prevent overfitting of the model (Chen
and Rosenfeld, 1998). Our objective is thus to select
Λ minimizing:

L = − log
∏
〈e,f〉

p(e|f; Λ)− ||Λ||
2

2σ2

= −
∑
〈e,f〉

[logZ(e, f; Λ)− logZ(f; Λ)]− ||Λ||
2

2σ2

The gradient of Lwith respect to the feature weights
has a parallel form; it is the difference in feature ex-
pectations under the reference distribution and the

translation distribution with a penalty term due to
the prior:

∂L
∂λi

=
∑
〈e,f〉

Ep(d,a|e,f;Λ)[hi]− Ep(e,d,a|f;Λ)[hi]−
λi
σ2

The form of the objective and gradient are quite sim-
ilar to the traditional fully observed training scenario
for CRFs (Sha and Pereira, 2003). However, rather
than matching the feature expectations in the model
to an observable feature value, we have to sum over
the latent structure that remains after observing our
target e, which makes the form of the first summand
an expectation rather than just a feature function
value.

3.2.1 Computing the objective and gradient
The objective and gradient that were just introduced
can be computed in two steps. Given a training pair
〈e, f〉, we generate the forest of reorderings F from f
as described in §2.1. We then compose this grammar
with T , the FST representing the translation model,
which yields F ◦T , a translation forest that contains
all possible translations of f into the target language,
as described in §2.2. Running the inside algorithm
on the translation forest computes Z(f; Λ), the first
term in the objective, and the inside-outside algo-
rithm can be used to compute Ep(e,d,a|f)[hi]. Next,
to compute Z(e, f; Λ) and the first expectation in the
gradient, we need to find the subset of the transla-
tion forest F ◦ T that exactly derives the reference
translation e. To do this, we again rely on the fact
that F ◦ T is a forest and therefore itself a context-
free grammar. So, we use this grammar to parse
the target reference string e. The resulting forest,
F ◦T ◦e, contains all and only derivations that yield
the pair 〈e, f〉. Here, the inside algorithm computes
Z(e, f; Λ) and the inside-outside algorithm can be
used to compute Ep(e,d,a|f)[hi].

Once we have an objective and gradient, we can
apply any first-order numerical optimization tech-
nique.8 Although the conditional likelihood surface
of this model is non-convex (on account of the la-
tent variables), we did not find a significant initial-
ization effect. For the experiments below, we ini-
tialized Λ = 0 and set σ2 = 1. Training generally
converged in fewer than 1500 function evaluations.

8For our experiments we used L-BFGS (Liu and Nocedal,
1989).

862



4 Experimental setup

We now turn to an experimental validation of the
models we have introduced. We define three con-
ditions: a small data scenario consisting of a trans-
lation task based on the BTEC Chinese-English cor-
pus (Takezawa et al., 2002), a large data Chinese-
English condition designed to be more comparable
to conditions in a NIST MT evaluation, and a large
data Arabic-English task.

For each condition, phrase tables were extracted
as described in Koehn et al. (2003) with a maxi-
mum phrase size of 5. The parallel training data
was aligned using the Giza++ implementation of
IBM Model 4 (Och and Ney, 2003). The Chinese
text was segmented using a CRF-based word seg-
menter (Tseng et al., 2005). The Arabic text was
segmented using the technique described in Lee et
al. (2003). The Stanford parser was used to generate
source parses for all conditions, and these were then
used to generate the reordering forests as described
in §2.1.

Table 1 summarizes statistics about the cor-
pora used. The reachability statistic indicates
what percentage of sentence pairs in the train-
ing data could be regenerated using our reorder-
ing/translation model.9 To train the reordering
model, we used all of the reachable sentence pairs
from BTEC, 20% of the reachable set in the
Chinese-English condition, and all reachable sen-
tence pairs under 40 words (source) in length in the
Arabic-English condition.

Error analysis indicates that a substantial portion
of unreachable sentence pairs are due to alignment
(word or sentence) or parse errors; however, in some
cases the reordering forests did not contain an ad-
equate source reordering to produce the necessary
target. For example, in Arabic, which is a VSO lan-
guage, the treebank annotation is to place the sub-
ject NP as the ‘middle child’ between the V and the
object constituent. This can be reordered into an En-
glish SVO order using our child-permutation rules;
however, if the source VP is modified by a modal
particle, the parser makes the particle the parent of
the VP, and it is no longer possible to move the sub-
ject to the first position in the sentence. Richer re-
ordering rules are needed to address this problem.

9Only sentences that can be generated by the model can be
used in training.

Other solutions to the reachability problem include
targeting reachable oracles instead of the reference
translation (Li and Khudanpur, 2009) or making use
of alternative training criteria, such as minimum risk
training (Li and Eisner, 2009).

4.1 Features

We briefly describe the feature functions we used
in our model. These include the typical dense fea-
tures used in translation: relative phrase translation
frequencies p(e|f) and p(f |e), ‘lexically smoothed’
translation probabilities plex(e|f) and plex(f |e), and
a phrase count feature. For the reordering model, we
used a binary feature for each kind of rule used, for
example φVP→V NP(a) would fire once for each time
the rule VP → V NP was used in a derivation, a.
For the Arabic-English condition, we observed that
the parse trees tended to be quite flat, with many re-
peated non-terminal types in one rule, so we aug-
mented the non-terminal types with an index indi-
cating where they were located in the original parse
tree. This resulted in a total of 6.7k features for
IWSLT, 18k features for the large Chinese-English
condition, and 516k features for Arabic-English.10

A target language model was not used during the
training of the source reordering model, but it was
used during the translation experiments (see below).

4.2 Qualitative assessment of reordering model

Before looking at the translation results, we exam-
ine what the model learns during training. Figure 6
lists the 10 most highly weighted reordering features
learned by the BTEC model (above) and shows an
example reordering using this model (below), with
the most English-like reordering indicated with a
star.11 Keep in mind, we expect these features to
reflect what the best English-like order of the input
should be. All are almost surprisingly intuitive, but
this is not terribly surprising since Chinese and En-
glish have very similar large-scale structures (both
are head initial, both have adjectives and quanti-
fiers that precede nouns). However, we see two en-
tries in the list (starred) that correspond to an En-

10The large number of features in the Arabic system was due
to the relative flatness of the Arabic parse trees.

11The italicized symbols in the English gloss are functional
elements with no precise translation. Q is an interrogative parti-
cle, and DE marks a variety of attributive roles and is used here
as the head of a relative clause.

863



Table 1: Corpus statistics

Condition Sentences Source words Target words Reachability
BTEC 44k 0.33M 0.36M 81%

Chinese-English 400k 9.4M 10.9M 25%
Arabic-English 120k 3.3M 3.6M 66%

glish word order that is ungrammatical in Chinese:
PP modifiers in Chinese typically precede the VPs
they modify, and CPs (relative clauses) also typi-
cally precede the nouns they modify. In English, the
reverse is true, and we see that the model has indeed
learned to prefer this ordering. It was not necessary
that this be the case: since our model makes use
of phrases memorized from a non-reordered training
set, it could hav relied on those for all its reordering.
Yet these results provide evidence that it is learning
large-scale reordering successfully.

Feature λ note
VP→ VE NP 0.995
VP→ VV VP 0.939 modal + VP
VP→ VV NP 0.895
VP→ VP PP∗ 0.803 PP modifier of VP

VP→ VV NP IP 0.763
PP→ P NP 0.753

IP→ NP VP PU 0.728 PU = punctuation
VP→ VC NP 0.598
NP→ DP NP 0.538

NP→ NP CP∗ 0.537 rel. clauses follow

我   能     赶上           去   西尔顿  饭店    的   巴士 吗  ?
 I    CAN  CATCH  [NP[CP GO   HILTON  HOTEL  DE]   BUS]   Q   ? 

I  CAN  CATCH  [NP BUS [CP  GO  HILTON  HOTEL  DE]]  Q  ? 
I  CAN  CATCH  [NP BUS [CP  DE  GO  HILTON  HOTEL]]  Q  ? 
I  CAN  CATCH  [NP BUS [CP  GO  HOTEL  HILTON  DE]]  Q  ? 

I  CAN  CATCH  [NP BUS [CP  DE  GO  HOTEL  HILTON]]  Q  ? 
I  CATCH  [NP BUS [CP  GO  HILTON  HOTEL  DE]]  CAN  Q  ? 

Input:

5-best reordering:

(Can I catch a bus that goes to the Hilton Hotel ?)

Figure 6: (Above) The 10 most highly-weighted features
in a Chinese-English reordering model. (Below) Exam-
ple reordering of a Chinese sentence (with English gloss,
translation, and partial syntactic information).

5 Translation experiments

We now consider how to apply this model to a trans-
lation task. The training we described in §3.2 is
suboptimal for state-of-the-art translation systems,
since (1) it optimizes likelihood rather than an MT
metric and (2) it does not include a language model.
We describe how we addressed these problems here,
and then present our results in the three conditions
defined above.

5.1 Training for Viterbi decoding
A language model was incorporated using cube
pruning (Huang and Chiang, 2007), using a 200-
best limit at each node during LM integration. To
improve the ability of the phrase model to match
reordered phrases, we extracted the 1-best reorder-
ing of the training data under the learned reordering
model and generated the phrase translation model so
that it contained phrases from both the original order
and the 1-best reordering.

To be competitive with other state-of-the-art sys-
tems, we would like to use Och’s minimum error
training algorithm for training; however, we can-
not tune the model as described with it, since it has
far too many features. To address this, we con-
verted the coefficients on the reordering features into
a single reordering feature which then had a coef-
ficient assigned to it. This technique is similar to
what is done with logarithmic opinion pools, only
the learned model is not a probability distribution
(Smith et al., 2005). Once we collapsed the reorder-
ing weights into a single feature, we used the tech-
niques described by Kumar et al. (2009) to optimize
the feature weights to maximize corpus BLEU on a
held-out development set.

5.2 Translation results
Scores on a held-out test set are reported in Table 2
using case-insensitive BLEU with 4 reference trans-
lations (16 for BTEC) using the original definition
of the brevity penalty. We report the results of our

864



model along with three baseline conditions, one with
no-reordering at all (mono), the performance of a
phrase-based translation model with distance-based
distortion, the performance of our implementation of
a hierarchical phrase-based translation model (Chi-
ang, 2007), and then our model.

Table 2: Translation results (BLEU)

Condition Mono PB Hiero Forest
BTEC 47.4 51.8 52.4 54.1

Chinese-Eng. 29.0 30.9 32.1 32.4
Arabic-Eng. 41.2 45.8 46.6 44.9

6 Related work

A variety of translation processes can be formalized
as the composition of a finite-state representation of
input (typically just a sentence, but often a more
complex structure, like a word lattice) with an SCFG
(Wu, 1997; Chiang, 2007; Zollmann and Venugopal,
2006). Like these, our work uses parsing algorithms
to perform the composition operation. But this is the
first time that the input to a finite-state transducer has
a context-free structure.12 Although not described
in terms of operations over formal languages, the
model of Yamada and Knight (2001) can be under-
stood as an instance of our class of models with a
specific input forest and phrases restricted to match
syntactic constituents.

In terms of formal similarity, Mi et al. (2008) use
forests as input to a tree-to-string transducer pro-
cess, but the forests are used to recover from 1-
best parsing errors (as such, all derivations yield
the same source string). Iglesias et al. (2009) use
a SCFG-based translation model, but implement it
using FSTs, although they use non-regular exten-
sions that make FSTs equivalent to recursive tran-
sition networks. Galley and Manning (2008) use
a context-free reordering model to score a phrase-
based (exponential) search space.

Syntax-based preprocessing approaches that have
relied on hand-written rules to restructure source
trees for particular translation tasks have been quite
widely used (Collins et al., 2005; Wang et al., 2007;
Xu et al., 2009; Chang et al., 2009). Discrimina-
tively trained reordering models have been exten-
sively explored. A widely used approach has been to

12Satta (submitted) discusses the theoretical possibility of
this sort of model but provides no experimental results.

use a classifier to predict the orientation of phrases
during decoding (Zens and Ney, 2006; Chang et al.,
2009). These classifiers must be trained indepen-
dently from the translation model using training ex-
amples extracted from the training data. A more am-
bitious approach is described by Tromble and Eisner
(2009), who build a global reordering model that is
learned automatically from reordered training data.

The latent variable discriminative training ap-
proach we describe is similar to the one originally
proposed by Blunsom et al. (2008).

7 Discussion and conclusion

We have described a new model of translation that
takes advantage of the strengths of context-free
modeling, but splits reordering and phrase transduc-
tion into two separate models. This lets the context-
free part handle what it does well, mid-to-long range
reordering, and lets the finite-state part handle lo-
cal phrasal correspondences. We have further shown
that the reordering component can be trained effec-
tively as a latent variable in a discriminative transla-
tion model using only conventional parallel training
data.

This model holds considerable promise for fu-
ture improvement. Not only does it already achieve
quite reasonable performance (performing particu-
larly well in Chinese-English, where mid-range re-
ordering is often required), but we have only begun
to scratch the surface in terms of the kinds of fea-
tures that can be included to predict reordering, as
well as the kinds of reordering forests used. Fur-
thermore, by reintroducing the concept of a cascade
of transducers into the context-free model space, it
should be possible to develop new and more effec-
tive rescoring mechanisms. Finally, unlike SCFG
and phrase-based models, our model does not im-
pose any distortion limits.

Acknowledgements

The authors gratefully acknowledge partial support from
the GALE program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001. Any
opinions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do not
necessarily reflect the views of the sponsors. Thanks
to Hendra Setiawan, Vlad Eidelman, Zhifei Li, Chris
Callison-Burch, Brian Dillon and the anonymous review-
ers for insightful comments.

865



References
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-

inative latent variable model for statistical machine
translation. In Proceedings of ACL-HLT.

P.-C. Chang, D. Jurafsky, and C. D. Manning. 2009. Dis-
ambiguating “DE” for Chinese-English machine trans-
lation,. In Proc. WMT.

S. F. Chen and R. Rosenfeld. 1998. A Gaussian prior
for smoothing maximum entropy models. Technical
Report TR-10-98, Computer Science Group, Harvard
University.

D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.

M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of ACL 2005.

J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13(2):94–102.

M. Galley and C. D. Manning. 2008. A simple and ef-
fective hierarchical phrase reordering model. In Proc.
EMNLP.

D. Grune and C. J. H. Jacobs. 2008. Parsing as intersec-
tion. In D. Gries and F. B. Schneider, editors, Parsing
Techniques, pages 425–442. Springer, New York.

J. E. Hopcroft and J. D. Ullman. 1979. Introduc-
tion to Automata Theory, Languages and Computa-
tion. Addison-Wesley.

L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL.

G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In Proc. NAACL.

P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of NAACL, pages 48–54.

S. Kumar, Y. Deng, and W. Byrne. 2006. A weighted fi-
nite state transducer translation template model for sta-
tistical machine translation. Journal of Natural Lan-
guage Engineering, 12(1):35–75.

S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. ACL.

Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In Proc. ACL.

Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. EMNLP.

Z. Li and S. Khudanpur. 2009. Efficient extraction of
oracle-best translations from hypergraphs. In Proc.
NAACL.

D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming B, 45(3):503–528.

H. Mi, L. Huang, and Q. Liu. 2008. Forest-based transla-
tion. In Proceedings of ACL-08: HLT, pages 192–199,
Columbus, Ohio, June. Association for Computational
Linguistics.

F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.

S. Petrov and D. Klein. 2008. Discriminative log-linear
grammars with latent variables. In Advances in Neu-
ral Information Processing Systems 20 (NIPS), pages
1153–1160.

G. Satta. submitted. Translation algorithms by means of
language intersection.

F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL,
pages 213–220.

A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In Proc.
ACL.

T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proceedings of LREC 2002,
pages 147–152, Las Palmas, Spain.

R. Tromble and J. Eisner. 2009. Learning linear or-
der problems for better translation. In Proceedings of
EMNLP 2009.

H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter. In Fourth SIGHAN Workshop on Chinese Lan-
guage Processing.

C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proc. EMNLP.

D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–404.

P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In Proc. NAACL, pages 245–253.

K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proc. ACL.

R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In Proc. of
the Workshop on SMT.

A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In Proc.
of the Workshop on SMT.

A. Zollmann, A. Venugopal, F. Och, and J. Ponte. 2008.
A systematic comparison of phrase-based, hierarchical
and syntax-augmented statistical MT. In Proc. Coling.

866


