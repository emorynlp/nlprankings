



















































Recurrent Neural Network Grammars


Proceedings of NAACL-HLT 2016, pages 199–209,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Recurrent Neural Network Grammars
Chris Dyer♠ Adhiguna Kuncoro♠ Miguel Ballesteros♦♠ Noah A. Smith♥
♠School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA

♦NLP Group, Pompeu Fabra University, Barcelona, Spain
♥Computer Science & Engineering, University of Washington, Seattle, WA, USA

{cdyer,akuncoro}@cs.cmu.edu, miguel.ballesteros@upf.edu, nasmith@cs.washington.edu

Abstract

We introduce recurrent neural network gram-
mars, probabilistic models of sentences with
explicit phrase structure. We explain efficient
inference procedures that allow application to
both parsing and language modeling. Experi-
ments show that they provide better parsing in
English than any single previously published
supervised generative model and better lan-
guage modeling than state-of-the-art sequen-
tial RNNs in English and Chinese.

1 Introduction

Sequential recurrent neural networks (RNNs) are
remarkably effective models of natural language.
In the last few years, language model results that
substantially improve over long-established state-of-
the-art baselines have been obtained using RNNs
(Zaremba et al., 2015; Mikolov et al., 2010) as well
as in various conditional language modeling tasks
such as machine translation (Bahdanau et al., 2015),
image caption generation (Xu et al., 2015), and dia-
logue generation (Wen et al., 2015). Despite these
impressive results, sequential models are a priori
inappropriate models of natural language, since re-
lationships among words are largely organized in
terms of latent nested structures rather than sequen-
tial surface order (Chomsky, 1957).

In this paper, we introduce recurrent neural net-
work grammars (RNNGs; §2), a new generative
probabilistic model of sentences that explicitly mod-
els nested, hierarchical relationships among words
and phrases. RNNGs operate via a recursive syntac-
tic process reminiscent of probabilistic context-free
grammar generation, but decisions are parameter-
ized using RNNs that condition on the entire syntac-
tic derivation history, greatly relaxing context-free
independence assumptions.

The foundation of this work is a top-down vari-
ant of transition-based parsing (§3). We give two
variants of the algorithm, one for parsing (given an
observed sentence, transform it into a tree), and one
for generation. While several transition-based neu-
ral models of syntactic generation exist (Hender-
son, 2003, 2004; Emami and Jelinek, 2005; Titov
and Henderson, 2007; Buys and Blunsom, 2015b),
these have relied on structure building operations
based on parsing actions in shift-reduce and left-
corner parsers which operate in a largely bottom-
up fashion. While this construction is appealing be-
cause inference is relatively straightforward, it lim-
its the use of top-down grammar information, which
is helpful for generation (Roark, 2001).1 RNNGs
maintain the algorithmic convenience of transition-
based parsing but incorporate top-down (i.e., root-
to-terminal) syntactic information (§4).

The top-down transition set that RNNGs are
based on lends itself to discriminative modeling as
well, where sequences of transitions are modeled
conditional on the full input sentence along with the
incrementally constructed syntactic structures. Sim-
ilar to previously published discriminative bottom-
up transition-based parsers (Henderson, 2004; Sagae
and Lavie, 2005; Zhang and Clark, 2011, inter alia),
greedy prediction with our model yields a linear-
time deterministic parser (provided an upper bound
on the number of actions taken between process-
ing subsequent terminal symbols is imposed); how-
ever, our algorithm generates arbitrary tree struc-
tures directly, without the binarization required by
shift-reduce parsers. The discriminative model also
lets us use ancestor sampling to obtain samples of
parse trees for sentences, and this is used to solve

1The left-corner parsers used by Henderson (2003, 2004)
incorporate limited top-down information, but a complete path
from the root of the tree to a terminal is not generally present
when a terminal is generated. Refer to Henderson (2003, Fig.
1) for an example.

199



a second practical challenge with RNNGs: approx-
imating the marginal likelihood and MAP tree of a
sentence under the generative model. We present a
simple importance sampling algorithm which uses
samples from the discriminative parser to solve in-
ference problems in the generative model (§5).

Experiments show that RNNGs are effective for
both language modeling and parsing (§6). Our gen-
erative model obtains (i) the best-known parsing re-
sults using a single supervised generative model and
(ii) better perplexities in language modeling than
state-of-the-art sequential LSTM language models.
Surprisingly—although in line with previous pars-
ing results showing the effectiveness of genera-
tive models (Henderson, 2004; Johnson, 2001)—
parsing with the generative model obtains signifi-
cantly better results than parsing with the discrim-
inative model.

2 RNN Grammars

Formally, an RNNG is a triple (N,Σ,Θ) consisting
of a finite set of nonterminal symbols (N ), a finite
set of terminal symbols (Σ) such that N ∩ Σ = ∅,
and a collection of neural network parameters Θ. It
does not explicitly define rules since these are im-
plicitly characterized by Θ. The algorithm that the
grammar uses to generate trees and strings in the lan-
guage is characterized in terms of a transition-based
algorithm, which is outlined in the next section. In
the section after that, the semantics of the param-
eters that are used to turn this into a stochastic al-
gorithm that generates pairs of trees and strings are
discussed.

3 Top-down Parsing and Generation

RNNGs are based on a top-down generation algo-
rithm that relies on a stack data structure of par-
tially completed syntactic constituents. To empha-
size the similarity of our algorithm to more familiar
bottom-up shift-reduce recognition algorithms, we
first present the parsing (rather than generation) ver-
sion of our algorithm (§3.1) and then present modi-
fications to turn it into a generator (§3.2).
3.1 Parser Transitions

The parsing algorithm transforms a sequence of
words x into a parse tree y using two data structures

(a stack and an input buffer). As with the bottom-
up algorithm of Sagae and Lavie (2005), our algo-
rithm begins with the stack (S) empty and the com-
plete sequence of words in the input buffer (B). The
buffer contains unprocessed terminal symbols, and
the stack contains terminal symbols, “open” nonter-
minal symbols, and completed constituents. At each
timestep, one of the following three classes of op-
erations (Fig. 1) is selected by a classifier, based on
the current contents on the stack and buffer:

• NT(X) introduces an “open nonterminal” X onto
the top of the stack. Open nonterminals are
written as a nonterminal symbol preceded by an
open parenthesis, e.g., “(VP”, and they represent
a nonterminal whose child nodes have not yet
been fully constructed. Open nonterminals are
“closed” to form complete constituents by subse-
quent REDUCE operations.
• SHIFT removes the terminal symbol x from the

front of the input buffer, and pushes it onto the
top of the stack.
• REDUCE repeatedly pops completed subtrees or

terminal symbols from the stack until an open
nonterminal is encountered, and then this open
NT is popped and used as the label of a new con-
stituent that has the popped subtrees as its chil-
dren. This new completed constituent is pushed
onto the stack as a single composite item. A single
REDUCE operation can thus create constituents
with an unbounded number of children.

The parsing algorithm terminates when there is a
single completed constituent on the stack and the
buffer is empty. Fig. 2 shows an example parse
using our transition set. Note that in this paper
we do not model preterminal symbols (i.e., part-of-
speech tags) and our examples therefore do not in-
clude them.2

Our transition set is closely related to the op-
erations used in Earley’s algorithm which likewise
introduces nonterminals symbols with its PREDICT

2Preterminal symbols are, from the parsing algorithm’s
point of view, just another kind of nonterminal symbol that re-
quires no special handling. However, leaving them out reduces
the number of transitions by O(n) and also reduces the number
of action types, both of which reduce the runtime. Furthermore,
standard parsing evaluation scores do not depend on preterminal
prediction accuracy.

200



operation and later COMPLETEs them after consum-
ing terminal symbols one at a time using SCAN
(Earley, 1970). It is likewise closely related to the
“linearized” parse trees proposed by Vinyals et al.
(2015) and to the top-down, left-to-right decompo-
sitions of trees used in previous generative parsing
and language modeling work (Roark, 2001, 2004;
Charniak, 2010).

A further connection is to LL(∗) parsing which
uses an unbounded lookahead (compactly repre-
sented by a DFA) to distinguish between parse alter-
natives in a top-down parser (Parr and Fisher, 2011);
however, our parser uses an RNN encoding of the
lookahead rather than a DFA.

Constraints on parser transitions. To guarantee
that only well-formed phrase-structure trees are pro-
duced by the parser, we impose the following con-
straints on the transitions that can be applied at each
step which are a function of the parser state (B,S, n)
where n is the number of open nonterminals on the
stack:

• The NT(X) operation can only be applied if B is
not empty and n < 100.3

• The SHIFT operation can only be applied if B is
not empty and n ≥ 1.
• The REDUCE operation can only be applied if the

top of the stack is not an open nonterminal sym-
bol.
• The REDUCE operation can only be applied if n ≥

2 or if the buffer is empty.

To designate the set of valid parser transitions, we
write AD(B,S, n).
3.2 Generator Transitions

The parsing algorithm that maps from sequences
of words to parse trees can be adapted with mi-
nor changes to produce an algorithm that stochas-
tically generates trees and terminal symbols. Two
changes are required: (i) there is no input buffer of

3Since our parser allows unary nonterminal productions,
there are an infinite number of valid trees for finite-length sen-
tences. The n < 100 constraint prevents the classifier from
misbehaving and generating excessively large numbers of non-
terminals. Similar constraints have been proposed to deal with
the analogous problem in bottom-up shift-reduce parsers (Sagae
and Lavie, 2005).

unprocessed words, rather there is an output buffer
(T ), and (ii) instead of a SHIFT operation there are
GEN(x) operations which generate terminal symbol
x ∈ Σ and add it to the top of the stack and the out-
put buffer. At each timestep an action is stochasti-
cally selected according to a conditional distribution
that depends on the current contents of B and T .
The algorithm terminates when a single completed
constituent remains on the stack. Fig. 4 shows an
example generation sequence.

Constraints on generator transitions. The gen-
eration algorithm also requires slightly modified
constraints. These are:

• The GEN(x) operation can only be applied if n ≥
1.
• The REDUCE operation can only be applied if the

top of the stack is not an open nonterminal symbol
and n ≥ 1.

To designate the set of valid generator transitions,
we write AG(T, S, n).

This transition set generates trees using nearly the
same structure building actions and stack configura-
tions as the “top-down PDA” construction proposed
by Abney et al. (1999), albeit without the restriction
that the trees be in Chomsky normal form.

3.3 Transition Sequences from Trees

Any parse tree can be converted to a sequence of
transitions via a depth-first, left-to-right traversal of
a parse tree. Since there is a unique depth-first, left-
ro-right traversal of a tree, there is exactly one tran-
sition sequence of each tree. For a tree y and a
sequence of symbols x, we write a(x,y) to indi-
cate the corresponding sequence of generation tran-
sitions, and b(x,y) to indicate the parser transitions.

3.4 Runtime Analysis

A detailed analysis of the algorithmic properties of
our top-down parser is beyond the scope of this pa-
per; however, we briefly state several facts. As-
suming the availability of constant time push and
pop operations, the runtime is linear in the number
of the nodes in the parse tree that is generated by
the parser/generator (intuitively, this is true since al-
though an individual REDUCE operation may require

201



Stackt Buffert Open NTst Action Stackt+1 Buffert+1 Open NTst+1
S B n NT(X) S | (X B n+ 1
S x | B n SHIFT S | x B n
S | (X | τ1 | . . . | τ` B n REDUCE S | (X τ1 . . . τ`) B n− 1

Figure 1: Parser transitions showing the stack, buffer, and open nonterminal count before and after each action type. S represents
the stack, which contains open nonterminals and completed subtrees; B represents the buffer of unprocessed terminal symbols; x

is a terminal symbol, X is a nonterminal symbol, and each τ is a completed subtree. The top of the stack is to the right, and the

buffer is consumed from left to right. Elements on the stack and buffer are delimited by a vertical bar ( | ).

Input: The hungry cat meows .
Stack Buffer Action

0 The | hungry | cat |meows | . NT(S)
1 (S The | hungry | cat |meows | . NT(NP)
2 (S | (NP The | hungry | cat |meows | . SHIFT
3 (S | (NP |The hungry | cat |meows | . SHIFT
4 (S | (NP |The | hungry cat |meows | . SHIFT
5 (S | (NP |The | hungry | cat meows | . REDUCE
6 (S | (NP The hungry cat) meows | . NT(VP)
7 (S | (NP The hungry cat) | (VP meows | . SHIFT
8 (S | (NP The hungry cat) | (VP meows . REDUCE
9 (S | (NP The hungry cat) | (VP meows) . SHIFT

10 (S | (NP The hungry cat) | (VP meows) | . REDUCE
11 (S (NP The hungry cat) (VP meows) .)

Figure 2: Top-down parsing example.

Stackt Termst Open NTst Action Stackt+1 Termst+1 Open NTst+1
S T n NT(X) S | (X T n+ 1
S T n GEN(x) S | x T | x n
S | (X | τ1 | . . . | τ` T n REDUCE S | (X τ1 . . . τ`) T n− 1

Figure 3: Generator transitions. Symbols defined as in Fig. 1 with the addition of T representing the history of generated terminals.

Stack Terminals Action
0 NT(S)
1 (S NT(NP)
2 (S | (NP GEN(The)
3 (S | (NP |The The GEN(hungry)
4 (S | (NP |The | hungry The | hungry GEN(cat)
5 (S | (NP |The | hungry | cat The | hungry | cat REDUCE
6 (S | (NP The hungry cat) The | hungry | cat NT(VP)
7 (S | (NP The hungry cat) | (VP The | hungry | cat GEN(meows)
8 (S | (NP The hungry cat) | (VP meows The | hungry | cat |meows REDUCE
9 (S | (NP The hungry cat) | (VP meows) The | hungry | cat |meows GEN(.)

10 (S | (NP The hungry cat) | (VP meows) | . The | hungry | cat |meows | . REDUCE
11 (S (NP The hungry cat) (VP meows) .) The | hungry | cat |meows | .

Figure 4: Joint generation of a parse tree and sentence.

202



applying a number of pops that is linear in the num-
ber of input symbols, the total number of pop opera-
tions across an entire parse/generation run will also
be linear). Since there is no way to bound the num-
ber of output nodes in a parse tree as a function of
the number of input words, stating the runtime com-
plexity of the parsing algorithm as a function of the
input size requires further assumptions. Assuming
our fixed constraint on maximum depth, it is linear.

3.5 Comparison to Other Models

Our generation algorithm algorithm differs from
previous stack-based parsing/generation algorithms
in two ways. First, it constructs rooted tree struc-
tures top down (rather than bottom up), and sec-
ond, the transition operators are capable of directly
generating arbitrary tree structures rather than, e.g.,
assuming binarized trees, as is the case in much
prior work that has used transition-based algorithms
to produce phrase-structure trees (Sagae and Lavie,
2005; Zhang and Clark, 2011; Zhu et al., 2013).

4 Generative Model

RNNGs use the generator transition set just pre-
sented to define a joint distribution on syntax trees
(y) and words (x). This distribution is defined as a
sequence model over generator transitions that is pa-
rameterized using a continuous space embedding of
the algorithm state at each time step (ut); i.e.,

p(x,y) =
|a(x,y)|∏

t=1

p(at | a<t)

=
|a(x,y)|∏

t=1

exp r>atut + bat∑
a′∈AG(Tt,St,nt) exp r

>
a′ut + ba′

,

and where action-specific embeddings ra and bias
vector b are parameters in Θ.

The representation of the algorithm state at time
t, ut, is computed by combining the representation
of the generator’s three data structures: the output
buffer (Tt), represented by an embedding ot, the
stack (St), represented by an embedding st, and the
history of actions (a<t) taken by the generator, rep-
resented by an embedding ht,

ut = tanh (W[ot; st; ht] + c) ,

where W and c are parameters. Refer to Figure 5
for an illustration of the architecture.

The output buffer, stack, and history are se-
quences that grow unboundedly, and to obtain rep-
resentations of them we use recurrent neural net-
works to “encode” their contents (Cho et al., 2014).
Since the output buffer and history of actions are
only appended to and only contain symbols from a
finite alphabet, it is straightforward to apply a stan-
dard RNN encoding architecture. The stack (S) is
more complicated for two reasons. First, the ele-
ments of the stack are more complicated objects than
symbols from a discrete alphabet: open nontermi-
nals, terminals, and full trees, are all present on the
stack. Second, it is manipulated using both push and
pop operations. To efficiently obtain representations
of S under push and pop operations, we use stack
LSTMs (Dyer et al., 2015).

4.1 Syntactic Composition Function

When a REDUCE operation is executed, the parser
pops a sequence of completed subtrees and/or to-
kens (together with their vector embeddings) from
the stack and makes them children of the most recent
open nonterminal on the stack, “completing” the
constituent. To compute an embedding of this new
subtree, we use a composition function based on
bidirectional LSTMs, which is illustrated in Fig. 6.

The first vector read by the LSTM in both the for-
ward and reverse directions is an embedding of the
label on the constituent being constructed (in the fig-
ure, NP). This is followed by the embeddings of the
child subtrees (or tokens) in forward or reverse or-
der. Intuitively, this order serves to “notify” each
LSTM what sort of head it should be looking for as it
processes the child node embeddings. The final state
of the forward and reverse LSTMs are concatenated,
passed through an affine transformation and a tanh
nonlinearity to become the subtree embedding.4 Be-
cause each of the child node embeddings (u, v, w in
Fig. 6) is computed similarly (if it corresponds to an

4We found the many previously proposed syntactic compo-
sition functions inadequate for our purposes. First, we must
contend with an unbounded number of children, and many
previously proposed functions are limited to binary branching
nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those
that could deal with n-ary nodes made poor use of nonterminal
information (Tai et al., 2015), which is crucial for our task.

203



The hungry cat

NP (VP(S

RE
DU

CE
GE

N
NT

(N
P)

NT
(VP

)

…

cat hungry The
a<t

p(at)

ut
Ttz }| {Stz }| {

Figure 5: Neural architecture for defining a distribution over at given representations of the stack (St), output buffer (Tt) and
history of actions (a<t). Details of the composition architecture of the NP, the action history LSTM, and the other elements of the

stack are not shown. This architecture corresponds to the generator state at line 7 of Figure 4.

NP

u v w

NP u v w NP

x
x

Figure 6: Syntactic composition function based on bidirec-
tional LSTMs that is executed during a REDUCE operation; the

network on the right models the structure on the left.

internal node), this composition function is a kind of
recursive neural network.

4.2 Word Generation
To reduce the size of AG(S, T, n), word genera-
tion is broken into two parts. First, the decision to
generate is made (by predicting GEN as an action),
and then choosing the word, conditional on the cur-
rent parser state. To further reduce the computa-
tional complexity of modeling the generation of a
word, we use a class-factored softmax (Baltescu and
Blunsom, 2015; Goodman, 2001). By using

√|Σ|
classes for a vocabulary of size |Σ|, this prediction
step runs in time O(

√|Σ|) rather than the O(|Σ|) of
the full-vocabulary softmax. To obtain clusters, we
use the greedy agglomerative clustering algorithm
of Brown et al. (1992).

4.3 Training
The parameters in the model are learned to maxi-
mize the likelihood of a corpus of trees.

4.4 Discriminative Parsing Model

A discriminative parsing model can be obtained by
replacing the embedding of Tt at each time step with
an embedding of the input buffer Bt. To train this
model, the conditional likelihood of each sequence
of actions given the input string is maximized.5

5 Inference via Importance Sampling

Our generative model p(x,y) defines a joint dis-
tribution on trees (y) and sequences of words (x).
To evaluate this as a language model, it is neces-
sary to compute the marginal probability p(x) =∑

y′∈Y(x) p(x,y
′). And, to evaluate the model as

a parser, we need to be able to find the MAP parse
tree, i.e., the tree y ∈ Y(x) that maximizes p(x,y).
However, because of the unbounded dependencies
across the sequence of parsing actions in our model,
exactly solving either of these inference problems
is intractable. To obtain estimates of these, we use
a variant of importance sampling (Doucet and Jo-
hansen, 2011).

Our importance sampling algorithm uses a condi-
tional proposal distribution q(y | x) with the fol-
lowing properties: (i) p(x,y) > 0 =⇒ q(y |
x) > 0; (ii) samples y ∼ q(y | x) can be ob-
tained efficiently; and (iii) the conditional probabil-
ities q(y | x) of these samples are known. While
many such distributions are available, the discrim-

5For the discriminative parser, the POS tags are processed
similarly as in (Dyer et al., 2015); they are predicted for English
with the Stanford Tagger (Toutanova et al., 2003) and Chinese
with Marmot (Mueller et al., 2013).

204



inatively trained variant of our parser (§4.4) ful-
fills these requirements: sequences of actions can
be sampled using a simple ancestral sampling ap-
proach, and, since parse trees and action sequences
exist in a one-to-one relationship, the product of the
action probabilities is the conditional probability of
the parse tree under q. We therefore use our discrim-
inative parser as our proposal distribution.

Importance sampling uses importance weights,
which we define as w(x,y) = p(x,y)/q(y | x), to
compute this estimate. Under this definition, we can
derive the estimator as follows:

p(x) =
∑

y∈Y(x)
p(x,y) =

∑
y∈Y(x)

q(y | x)w(x,y)

= Eq(y|x)w(x,y).

We now replace this expectation with its Monte
Carlo estimate as follows, using N samples from q:

y(i) ∼ q(y | x) for i ∈ {1, 2, . . . , N}

Eq(y|x)w(x,y)
MC≈ 1

N

N∑
i=1

w(x,y(i))

To obtain an estimate of the MAP tree ŷ, we choose
the sampled tree with the highest probability under
the joint model p(x,y).

6 Experiments

We present results of our two models both on parsing
(discriminative and generative) and as a language
model (generative only) in English and Chinese.

Data. For English, §2–21 of the Penn Treebank
are used as training corpus for both, with §24 held
out as validation, and §23 used for evaluation. Sin-
gleton words in the training corpus with unknown
word classes using the the Berkeley parser’s map-
ping rules.6 Orthographic case distinctions are pre-
served, and numbers (beyond singletons) are not
normalized. For Chinese, we use the Penn Chinese
Treebank Version 5.1 (CTB) (Xue et al., 2005).7 For

6http://github.com/slavpetrov/
berkeleyparser

7§001–270 and 440–1151 for training, §301–325 develop-
ment data, and §271–300 for evaluation.

the Chinese experiments, we use a single unknown
word class. Corpus statistics are given in Table 1.8

Table 1: Corpus statistics.
PTB-train PTB-test CTB-train CTB-test

Sequences 39,831 2,416 50,734 348
Tokens 950,012 56,684 1,184,532 8,008
Types 23,815 6,823 31,358 1,637
UNK-Types 49 42 1 1

Model and training parameters. For the dis-
criminative model, we used hidden dimensions of
128 and 2-layer LSTMs (larger numbers of dimen-
sions reduced validation set performance). For the
generative model, we used 256 dimensions and 2-
layer LSTMs. For both models, we tuned the
dropout rate to maximize validation set likelihood,
obtaining optimal rates of 0.2 (discriminative) and
0.3 (generative). For the sequential LSTM baseline
for the language model, we also found an optimal
dropout rate of 0.3. For training we used stochas-
tic gradient descent with a learning rate of 0.1. All
parameters were initialized according to recommen-
dations given by Glorot and Bengio (2010).

English parsing results. Table 2 (last two rows)
gives the performance of our parser on Section 23,
as well as the performance of several representa-
tive models. For the discriminative model, we used
a greedy decoding rule as opposed to beam search
in some shift-reduce baselines. For the generative
model, we obtained 100 independent samples from
a flattened distribution of the discriminative parser
(by exponentiating each probability by α = 0.8 and
renormalizing) and reranked them according to the
generative model.9

Chinese parsing results. Chinese parsing results
were obtained with the same methodology as in En-
glish and show the same pattern (Table 6).

Language model results. We report held-out per-
word perplexities of three language models, both se-
quential and syntactic. Log probabilities are normal-
ized by the number of words (excluding the stop

8This preprocessing scheme is more similar to what is stan-
dard in parsing than what is standard in language modeling.
However, since our model is both a parser and a language
model, we opted for the parser normalization.

9The value α = 0.8 was chosen based on the diversity of
the samples generated on the development set.

205



Table 2: Parsing results on PTB §23 (D=discriminative,
G=generative, S=semisupervised).

Model type F1
Henderson (2004) D 89.4
Socher et al. (2013a) D 90.4
Zhu et al. (2013) D 90.4
Vinyals et al. (2015) – WSJ only D 90.5
Petrov and Klein (2007) G 90.1
Bod (2003) G 90.7
Shindo et al. (2012) – single G 91.1
Shindo et al. (2012) – ensemble G 92.4
Zhu et al. (2013) S 91.3
McClosky et al. (2006) S 92.1
Vinyals et al. (2015) – single S 92.5
Vinyals et al. (2015) – ensemble S 92.8
Discriminative, q(y | x) D 89.8
Generative, p̂(y | x) G 92.4

Table 3: Parsing results on CTB 5.1.
Model type F1
Zhu et al. (2013) D 82.6
Wang et al. (2015) D 83.2
Huang and Harper (2009) D 84.2
Charniak (2000) G 80.8
Bikel (2004) G 80.6
Petrov and Klein (2007) G 83.3
Zhu et al. (2013) S 85.6
Wang and Xue (2014) S 86.3
Wang et al. (2015) S 86.6
Discriminative, q(y | x) D 80.7
Generative, p̂(y | x) G 82.7

symbol), inverted, and exponentiated to yield the
perplexity. Results are summarized in Table 4.

7 Discussion

It is clear from our experiments that the proposed
generative model is quite effective both as a parser
and as a language model. This is the result of
(i) relaxing conventional independence assumptions
(e.g., context-freeness) and (ii) inferring continu-
ous representations of symbols alongside non-linear
models of their syntactic relationships. The most
significant question that remains is why the dis-
criminative model—which has more information
available to it than the generative model—performs

Table 4: Language model perplexity results.

Model test ppl (PTB) test ppl (CTB)
IKN 5-gram 169.3 255.2
LSTM LM 113.4 207.3
RNNG 102.4 171.9

worse than the generative model. This pattern has
been observed before in neural parsing by Hender-
son (2004), who hypothesized that larger, unstruc-
tured conditioning contexts are harder to learn from,
and provide opportunities to overfit. Our discrimi-
native model conditions on the entire history, stack,
and buffer, while our generative model only ac-
cesses the history and stack. The fully discrimina-
tive model of Vinyals et al. (2015) was able to obtain
results similar to those of our generative model (al-
beit using much larger training sets obtained through
semisupervision) but similar results to those of our
discriminative parser using the same data. In light of
their results, we believe Henderson’s hypothesis is
correct, and that generative models should be con-
sidered as a more statistically efficient method for
learning neural networks from small data.

8 Related Work

Our language model combines work from two mod-
eling traditions: (i) recurrent neural network lan-
guage models and (ii) syntactic language model-
ing. Recurrent neural network language models
use RNNs to compute representations of an un-
bounded history of words in a left-to-right language
model (Zaremba et al., 2015; Mikolov et al., 2010;
Elman, 1990). Syntactic language models jointly
generate a syntactic structure and a sequence of
words (Baker, 1979; Jelinek and Lafferty, 1991).
There is an extensive literature here, but one strand
of work has emphasized a bottom-up generation of
the tree, using variants of shift-reduce parser ac-
tions to define the probability space (Chelba and
Jelinek, 2000; Emami and Jelinek, 2005). The
neural-network–based model of Henderson (2004)
is particularly similar to ours in using an unbounded
history in a neural network architecture to param-
eterize generative parsing based on a left-corner
model. Dependency-only language models have
also been explored (Titov and Henderson, 2007;

206



Buys and Blunsom, 2015a,b). Modeling generation
top-down as a rooted branching process that recur-
sively rewrites nonterminals has been explored by
Charniak (2000) and Roark (2001). Of particular
note is the work of Charniak (2010), which uses ran-
dom forests and hand-engineered features over the
entire syntactic derivation history to make decisions
over the next action to take.

The neural networks we use to model sentences
are structured according to the syntax of the sen-
tence being generated. Syntactically structured neu-
ral architectures have been explored in a num-
ber of applications, including discriminative pars-
ing (Socher et al., 2013a; Kiperwasser and Gold-
berg, 2016), sentiment analysis (Tai et al., 2015;
Socher et al., 2013b), and sentence representa-
tion (Socher et al., 2011; Bowman et al., 2006).
However, these models have been, without excep-
tion, discriminative; this is the first work to use syn-
tactically structured neural models to generate lan-
guage. Earlier work has demonstrated that sequen-
tial RNNs have the capacity to recognize context-
free (and beyond) languages (Sun et al., 1998;
Siegelmann and Sontag, 1995). In contrast, our
work may be understood as a way of incorporating a
context-free inductive bias into the model structure.

9 Outlook

RNNGs can be combined with a particle filter infer-
ence scheme (rather than the importance sampling
method based on a discriminative parser, §5) to pro-
duce a left-to-right marginalization algorithm that
runs in expected linear time. Thus, they could be
used in applications that require language models.

A second possibility is to replace the sequential
generation architectures found in many neural net-
work transduction problems that produce sentences
conditioned on some input. Previous work in ma-
chine translation has showed that conditional syn-
tactic models can function quite well without the
computationally expensive marginalization process
at decoding time (Galley et al., 2006; Gimpel and
Smith, 2014).

A third consideration regarding how RNNGs, hu-
man sentence processing takes place in a left-to-
right, incremental order. While an RNNG is not a
processing model (it is a grammar), the fact that it is

left-to-right opens up several possibilities for devel-
oping new sentence processing models based on an
explicit grammars, similar to the processing model
of Charniak (2010).

Finally, although we considered only the super-
vised learning scenario, RNNGs are joint models
that could be trained without trees, for example, us-
ing expectation maximization.

10 Conclusion

We introduced recurrent neural network grammars,
a probabilistic model of phrase-structure trees that
can be trained generatively and used as a language
model or a parser, and a corresponding discrimina-
tive model that can be used as a parser. Apart from
out-of-vocabulary preprocessing, the approach re-
quires no feature design or transformations to tree-
bank data. The generative model outperforms ev-
ery previously published parser built on a single su-
pervised generative model in English, and a bit be-
hind the best-reported generative model in Chinese.
As language models, RNNGs outperform the best
single-sentence language models.

Acknowledgments

We thank Brendan O’Connor, Swabha
Swayamdipta, and Brian Roark for feedback
on drafts of this paper, and Jan Buys, Phil Blunsom,
and Yue Zhang for help with data preparation.
This work was sponsored in part by the Defense
Advanced Research Projects Agency (DARPA)
Information Innovation Office (I2O) under the
Low Resource Languages for Emergent Incidents
(LORELEI) program issued by DARPA/I2O under
Contract No. HR0011-15-C-0114; it was also sup-
ported in part by Contract No. W911NF-15-1-0543
with the DARPA and the Army Research Office
(ARO). Approved for public release, distribution
unlimited. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. Miguel Ballesteros was supported by the
European Commission under the contract numbers
FP7-ICT-610411 (project MULTISENSOR) and
H2020-RIA-645012 (project KRISTINA).

207



References
Steven Abney, David McAllester, and Fernando Pereira.

1999. Relating probabilistic grammars and automata.
In Proc. ACL.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine transation by jointly learn-
ing to align and translate. In Proc. ICLR.

James K. Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society of
America, 65(S1):S132–S132.

Paul Baltescu and Phil Blunsom. 2015. Pragmatic neural
modelling in machine translation. In Proc. NAACL.

Dan Bikel. 2004. On the parameter space of generative
lexicalized statistical parsing models. Ph.D. thesis,
University of Pennsylvania.

Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proc. EACL.

Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi,
Raghav Gupta, Christopher D. Manning, and Christo-
pher Potts. 2006. A fast unified model for parsing and
sentence understanding. CoRR, abs/1603.06021.

Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.

Jan Buys and Phil Blunsom. 2015a. A Bayesian model
for generative transition-based dependency parsing.
CoRR, abs/1506.04334.

Jan Buys and Phil Blunsom. 2015b. Generative incre-
mental dependency parsing with neural networks. In
Proc. ACL.

Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL.

Eugene Charniak. 2010. Top-down nearly-context-
sensitive parsing. In Proc. EMNLP.

Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4).

Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning phrase represen-
tations using rnn encoder–decoder for statistical ma-
chine translation. In Proc. EMNLP.

Noam Chomsky. 1957. Syntactic Structures. Mouton,
The Hague/Paris.

Arnaud Doucet and Adam M. Johansen. 2011. A tutorial
on particle filtering and smoothing: Fifteen years later.
In Handbook of Nonlinear Filtering. Oxford.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-based

dependency parsing with stack long short-term mem-
ory. In Proc. ACL.

Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94–102.

Jeffrey L. Elman. 1990. Finding structure in time. Cog-
nitive Science, 14:179–211.

Ahmad Emami and Frederick Jelinek. 2005. A neural
syntactic language model. Machine Learning, 60:195–
227.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL.

Kevin Gimpel and Noah A. Smith. 2014. Phrase de-
pendency machine translation with quasi-synchronous
tree-to-tree features. Computational Linguistics,
40(2).

Xavier Glorot and Yoshua Bengio. 2010. Understanding
the difficulty of training deep feedforward neural net-
works. In Proc. ICML.

Joshua Goodman. 2001. Classes for fast maximum en-
tropy training. CoRR, cs.CL/0108006.

James Henderson. 2003. Inducing history representations
for broad coverage statistical parsing. In Proc. NAACL.

James Henderson. 2004. Discriminative training of a neu-
ral network statistical parser. In Proc. ACL.

Zhongqiang Huang and Mary Harper. 2009. Self-training
PCFG grammars with latent annotations across lan-
guages. In Proc. EMNLP.

Frederick Jelinek and John D. Lafferty. 1991. Compu-
tation of the probability of initial substring generation
by stochastic context-free grammars. Computational
Linguistics, 17(3):315–323.

Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proc. ACL.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Easy-
first dependency parsing with hierarchical tree
LSTMs. ArXiv:1603.00375.

David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proc.
NAACL.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proc. Inter-
speech.

Thomas Mueller, Helmut Schmid, and Hinrich Schütze.
2013. Efficient higher-order CRFs for morphologi-
cal tagging. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-

208



ing, pages 322–332. Association for Computational
Linguistics, Seattle, Washington, USA. URL http:
//www.aclweb.org/anthology/D13-1032.

Terence Parr and Kathleen Fisher. 2011. LL(*): The
foundation of the ANTLR parser generator. In Proc.
PLDI.

Slav Petrov and Dan Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. NAACL.

Brian Roark. 2001. Probabilistic top-down parsing and
language modeling. Computational Linguistics, 27(2).

Brian Roark. 2004. Robust garden path parsing. JNLE,
10(1):1–24.

Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proc. IWPT.

Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Proc.
ACL.

Hava T. Siegelmann and Eduardo D. Sontag. 1995. On
the computational power of neural nets. Journal of
Computer and System Sciences, 50.

Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with compositional
vectors. In Proc. ACL.

Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Proc. NIPS.

Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng, and
Christopher Potts. 2013b. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proc. EMNLP.

Guo-Zheng Sun, C. Lee Giles, and Hsing-Hen Chen.
1998. The neural network pushdown automaton: Ar-
chitecture, dynamics and training. In Adaptive Pro-
cessing of Sequences and Data Structures, volume
1387 of Lecture Notes in Computer Science, pages
296–345.

Kai Sheng Tai, Richard Socher, and Christopher D. Man-
ning. 2015. Improved semantic representations from
tree-structured long short-term memory networks. In
Proc. ACL.

Ivan Titov and James Henderson. 2007. A latent variable
model for generative dependency parsing. In Proc.
IWPT.

Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
NAACL.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Grammar
as a foreign language. In Proc. ICLR.

Zhiguo Wang, Haitao Mi, and Nianwen Xue. 2015. Fea-
ture optimization for constituent parsing via neural
networks. In Proc. ACL-IJCNLP.

Zhiguo Wang and Nianwen Xue. 2014. Joint POS tag-
ging and transition-based constituent parsing in Chi-
nese with non-local features. In Proc. ACL.

Tsung-Hsien Wen, Milica Gašić, Nikola Mrkšić, Pei-Hao
Su, David Vandyke, and Steve Young. 2015. Semanti-
cally conditioned LSTM-based natural language gen-
eration for spoken dialogue systems. In Proc. EMNLP.

Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhutdinov, Richard S.
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention. In Proc. ICML.

Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer.
2005. The Penn Chinese TreeBank: Phrase structure
annotation of a large corpus. Nat. Lang. Eng., 11(2).

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2015. Recurrent neural network regularization. In
Proc. ICLR.

Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Computational Linguistics, 37(1).

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and
Jingbo Zhu. 2013. Fast and accurate shift-reduce con-
stituent parsing. In Proc. ACL.

209


