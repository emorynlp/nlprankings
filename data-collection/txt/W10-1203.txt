










































Query-based Text Normalization Selection Models for Enhanced Retrieval Accuracy


Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 19–26,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Query-based Text Normalization Selection Models for Enhanced Retrieval
Accuracy

Si-Chi Chin Rhonda DeCook W. Nick Street David Eichmann
The University of Iowa

Iowa City, USA.
{si-chi-chin, rhonda-decook, nick-street, david-eichmann}@uiowa.edu

Abstract

Text normalization transforms words into a
base form so that terms from common equiv-
alent classes match. Traditionally, informa-
tion retrieval systems employ stemming tech-
niques to remove derivational affixes. Deplu-
ralization, the transformation of plurals into
singular forms, is also used as a low-level text
normalization technique to preserve more pre-
cise lexical semantics of text.

Experiment results suggest that the choice of
text normalization technique should be made
individually on each topic to enhance informa-
tion retrieval accuracy. This paper proposes a
hybrid approach, constructing a query-based
selection model to select the appropriate text
normalization technique (stemming, deplural-
ization, or not doing any text normalization).
The selection model utilized ambiguity prop-
erties extracted from queries to train a com-
posite of Support Vector Regression (SVR)
models to predict a text normalization tech-
nique that yields the highest Mean Average
Precision (MAP). Based on our study, such
a selection model holds promise in improving
retrieval accuracy.

1 Introduction

Stemming removes derivational affixes of terms
therefore allowing terms from common equivalence
classes to be clustered. However, stemming also in-
troduces noise by mapping words of different con-
cepts or meanings into one base form, thus impeding
word-sense disambiguation. Depluralization, the
conversion of plural word forms to singular form,
preserves more precise semantics of text than stem-
ming (Krovetz, 2000).

Empirical research has demonstrated the ambiva-
lent effect of stemming on text retrieval perfor-
mance. Hull (1996) conducted a comprehensive
case study on the effects of four stemmer tech-
niques and the removal of plural “s” 1 on retrieval
performance. Hull suggested that the adoption of
stemming is beneficial but plural removal is as well
competitive when the size of documents is small.
Prior research (Manning and Schtze, 1999; Mc-
Namee et al., 2008) indicated that traditional stem-
ming, though still benefiting some queries, would
not necessarily enhance the average retrieval perfor-
mance. In addition, stemming was considered one of
the technique failures undermining retrieval perfor-
mance in the TREC 2004 Robust Track (Voorhees,
2006). Prior research also noted the semantic differ-
ences between plurals and singulars. Riloff (1995)
indicated that plural and singular nouns are distinct
because plural nouns usually pertain to the “general
types of incidents,” while singular nouns often per-
tain to “a specific incident.”

Nevertheless, prior research has not closely ex-
amined the effect of the change of the semantics
caused by different level of text normalization tech-
niques. In our work, we conducted extensive exper-
iments on the TREC 2004 Robust track collection to
evaluate the effect of stemming and depluralization
on information retrieval. In addition, we quantify
the ambiguity of a query, extracting five ambiguity
properties from queries. The extracted ambiguity
properties are used to construct query-based selec-
tion model, a composite of multiple Support Vector

1In our work, we not only removed the plural “s” or “es”
but also changed irregular plural forms such as “children” to its
singular form “child”.

19



Regression models, to determine the most appropri-
ate text normalization technique for a given query.
To our knowledge, our work is the first study to con-
struct a query-based selection model, using ambigu-
ity properties extracted from provided queries to se-
lect an optimal text normalization technique for each
query.

The remainder of this paper is organized as fol-
lows. In section 2 we describe our experimental
setups and dataset. Section 3 describes and ana-
lyzes experiment results of different text normaliza-
tion techniques on the dataset. We discuss five am-
biguity properties and validate each property in sec-
tion 4. In section 5 we describe the framework and
the prediction results of the proposed query-based
selection model. Finally, we summarize our findings
and discuss future work in section 6.

2 Experiment Setup

The experiment utilizes the queries and relevance
judgment results from the TREC 2004 Robust
Track to evaluate the effect of three text normal-
ization techniques – raw text, depluralized text, and
stemmed text. The TREC 2004 Robust Track used a
document set of approximately 528,000 documents
comprising 1,904 MB of text. In total, 249 query
topics were used in TREC Robust 2004.

Figure 1 illustrates the setup of the experiment.
The collection is parsed with a SAX parser and
stored in a Postgres database. Lucene is then used to
generate three indices: indices of raw text, deplural-
ized text, and stemmed text. The Postgres database
stores each document of the collection, the query
topics of the TREC 2004 Robust Track, and results
of experiments. The ambiguity properties for each
query is also computed in the Postgres system. We
query Lucene indices to obtain the top 1,000 relevant
results and compute Mean Average Precision (MAP)
with the trec eval program to evaluate performance.
We use R to analyze performance scores, generate
descriptive charts, conduct non-parametric statisti-
cal tests, and perform a paired t-test. We use Weka
(Hall et al., 2009) to construct query-based selection
model that incorporates multiple Support Vector Re-
gression (SVR) models.

2.1 Query Models

The TREC 2004 Robust Track provides 249 query
topics; each includes a title, a short description,

and a narrative (usually one-paragraph). We se-
lected three basic query models as a modest base-
line to demonstrate the effect of different text nor-
malization techniques. Our future work will exploit
other ranking models such as BM25 and LMIR. The
three query models used in the experiment are: (1)
boolean search with the title words of topics con-
catenated with logical AND (e.g. hydrogen AND
fuel AND automobiles); (2) boolean search with the
title words of topics concatenated with logical OR
(e.g. hydrogen OR fuel OR automobiles); (3) co-
sine similarity with the title words of topics. Lucene
MoreLikeThis (MLT) class supports both boolean
and cosine similarity query methods for the exper-
iment. Figure 2 shows how query topics are pro-
cessed before interrogating the indices. Original
queries are first depluralized or stemmed, further
processed according to each query model, and fi-
nally run against the depluralized and stemmed in-
dices. The experiment runs unprocessed raw queries
against the index of raw text, depluralized queries
against the index of depluralized text, and stemmed
queries against the index of stemmed text.

3 Experiment Results

Table 3 and Figure 3 summarize the results for the
full set of topics. Each row in Table 3 represents a
query model combined with a given text normaliza-
tion technique as described in section 2.1.

For each query model and text normalization
technique, we present the MAP value computed
across all relevant topics. We also provide the p-
value for comparing MAP between each normaliza-
tion technique and the baseline (i.e. non-normalized
(raw) queries). The p-value is generated from the
pairwise Wilcoxon signed rank sum test. Figure 3
describes the distribution of MAP across the three
text normalization techniques and three query mod-
els. The distributions are skewed and many out-
liers are observed. In general, boolean OR and MLT
query models perform similarly and stemming has
the highest median MAP value across all three query
models. The results from Table 3 for the combined
topic set show that depluralization and stemming
perform significantly better than the raw baseline.
However, the performance difference between de-
pluralization and stemming is not significant except
for the AND boolean query model. In general, the
differences of MAP among three text normalization

20



XMLParser
Postgres 
DBMS Lucene

R

Lexicon 
Analysis

Lexicon 
Analysis

Lucene

Index Files
Index Files

Index FilesXML Documents

TREC Robust Track 2004

Ambiguity 
Calculator

Trec Eval
Document

Process

DBMS

Legend

Data

Query 
Topics

Search 
Results

Performance
Score

GraphsGraphsGraphs

Wilcoxon Tests

Paired T-test

Weka 
SVM 

Regression

Ambiguity 
Measures

Text

Figure 1: Flow chart of experiment setup

Final Query

Text Normalization

Query Topic

Lexicon Analysis

Query Model Lucene MoreLikeThisOR boolean
AND 

boolean

hydrogen 
fuel 

automobile

hydrogen 
fuel 

automobil 

hydrogen fuel 
automobiles

StemmerDepluralizer

Depluralized 
Index

Stemmed 
Index

hydrogen 
AND 
fuel 
AND 

automobile

Index

hydrogen 
OR 
fuel 
OR 

automobile

hydrogen 
automobile 

^0.939 
fuel^0.631

Figure 2: Using the query “hydrogen fuel automobiles”
as an example, the depluralized query becomes “hydro-
gen fuel automobile” and the stemmed query becomes
“hydrogen fuel automobil.” Final boolean queries for de-
pluralized topic become “hydrogen AND fuel AND au-
tomobile” and “hydrogen OR fuel OR automobil.” More-
LikeThis (MLT) is the Lucene class used for cosine sim-
ilarity retrieval. A term vector score appends each word
in the topic.

techniques are within 2%.
To visualize the relative performances among

three text normalization techniques, we standardized
the three MAP values for a single topic (one from
each text normalization technique) to have mean 0
and standard deviation of 1. The result provides a 3-
value pattern emphasizing the ordering of the MAPs
across the text normalization techniques, rather than
the raw MAP values themselves. We then used
the K-medoids algorithm (Kaufman and Rousseeuw,
1990) to cluster the transformed data, applying Eu-
clidean distance as the distance measure. Figure 4
is an example of 9 constructed clusters based on the
MAP scores of the MLT query model. In a clus-
ter, a line represents the standardized MAP value
of a topic on each text normalization technique.
Given the small differences in aggregate MAP per-
formance, it is interesting to note that the clusters
demonstrate variable patterns, indicating that some
topics performed better as a depluralized query than
a stemmed query.

The cluster analysis suggests that the choice of
text normalization technique should be made indi-
vidually on each topic. As we choose an appropri-
ate text normalization technique for a given topic,
we would further enhance retrieval performance. In

21



Query Model

A
v
e

ra
g

e
 P

re
c
is

io
n

and or mlt

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

Mean Average Precision vs. Query Model
0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

1
.2

raw

deplural

stem

Figure 3: Profile plot of MAP

the next section, we address the issue of inconsis-
tent performance by constructing regression models
to predict the mean average precision of each query
from the ambiguity measures, and choose an appro-
priate normalization method based on these predic-
tions.

4 Ambiguity Properties

Research has affirmed the negative impact of query
ambiguity on an information retrieval system. As
stemming clusters terms of different concepts, it
should increase query ambiguity. To quantify the
query ambiguity potentially caused by stemming,
we compute five ambiguity properties for each
query: 1) the product of the number of senses, re-
ferred as the sense product; 2) the product of the
number of words mapped to one base form (e.g. a
stem), referred as the word product; 3) the ratio of
the sense product of depluralized query to which of
stemmed query, referred as the deplural-stem ratio;
4) the sum of the inverse document frequency for
each word in a query, referred as the idf-sum; 5) the
length of a query.

4.1 Sense Product

Sense product measures the extent of query ambigu-
ity after stemming. We first find all words mapped to
a given stem and, for each word, we then count the
number of senses found in WordNet. To compute

Figure 4: Example of relative performance similarities
among text normalization techniques. The cluster analy-
sis uses the MAP scores of the MLT query model

Combined Topic Set
Run MAP p-value
AND Raw 0.1213 N/A
AND Dep 0.1324 5.598e-06*
AND Stem 0.1550 † 1.599e-07*
OR Raw 0.1851 N/A
OR Dep 0.1922 0.03035*
OR Stem 0.2069 0.01123*
MLT Raw 0.1893 N/A
MLT Dep 0.1959 0.04837*
MLT Stem 0.2093 0.009955*

Table 1: Paired Wilcoxon signed-ranked test on Mean
Average Precision (MAP), utilizing raw query as the
baseline. Significant differences between query models
are labeled with *. Results labeled with † indicate sig-
nificant differences between depluralized queries and a
stemmed queries.

22



the number of senses for a given stem, we accumu-
late the number of senses of each word mapped to
the stem. The sense product is then the multiply-
ing of the number of senses for each stemmed query
term, computed as:

sense product =
n∏

i=1

m∑
j=1

Sj (1)

Sj denotes the number of senses for each word
mapped to a stem i. We have m words mapped to a
stem i and have n stems in a query. As the sense
product increases, the query ambiguity increases.
Figure 5 illustrates the computation of the sense
product for the query “organic soil enhancement.”
The term “organic” has the stem organ, which is a
stem for 9 different words. The accumulated num-
ber of senses for “organ” is 39. With the same ap-
proach, we obtain 7 senses for 1 “soil” and 7 senses
for “enhanc.” Therefore, multiplication 39, 7, and 7
gives us the sense product value 1911.

4.2 Word Product
Word product is an alternative measure of query am-
biguity after stemming. To compute the word prod-
uct, we multiply the number of words mapped to
each stem of a given query, which is formulated as:

word product =
n∏

i=1

Wi (2)

Wi denotes the number of words mapped to a
stem i, and n is the number of stems in a query.
We assume that the query ambiguity increases as the
word product increases. Consider the query “organic
soil enhancement” in Figure 5. We find 9 words
mapped to the stem “organ”; 3 words mapped to the
stem “soil”; 5 words mapped to the stem “enhance-
ment”. Therefore the word product for the query is
105, the product of 9, 3, and 5.

4.3 Deplural-Stem Ratio
Deplural-stem ratio is a variation of sense product. It
takes the ratio of the sense product of a depluralized
query to the stemmed query. As the deplural-stem
ratio increases, the query ambiguity after stemming
increases. In the example illustrated in Figure 5, the
deplural-stem ratio is the sense product of the deplu-
ralized query “organic soil enhancement” divided by

the sense product of the stemmed query “organ soil
enhanc”. The deplural-stem ratio is computed as:

deplural-stem ratio =

∏n
i=1

∑m
j=1 Smj∏n

i=1

∑m
j=1 Dj

(3)

4.4 Idf-sum

The idf-sum is the sum of the inverse document fre-
quency (IDF) of each word in the query. The IDF of
a given word measures the importance of the word
in the document collection. Queries with high values
of IDF are more likely to return relevant documents
from the collection. For example, the term “ZX-
Turbo,” describing a series of racing cars, has a high
IDF and occurs only once in the entire TREC 2004
Robust Track collection. Therefore, searching the
collection with the term “ZX-Turb” will return the
only relevant document in the collection and achieve
high precision and recall. The idf-sum is computed
as:

idf sum =
n∑

i=1

IDFi (4)

IDFi denotes the idf of each query term i and
n is the number of words in a query. We assume
that the query ambiguity decreases as the idf-sum in-
creases. For the query “organic soil enhancement”,
the IDF for each term is 5.97082 (organic), 5.18994
(soil), and 4.86996 (enhancement). The idf-sum of
the query is 16.0307.

4.5 Query Length

The length of the query is the number of words in a
query.

4.6 Feature Validation

We performed simple linear regression on each fea-
ture as the first step to exclude ineffectual features.
Table 2 demonstrates example results of simple lin-
ear regression from the MLT query model, using the
MAP of stemmed queries as the dependent variable.
We take the logarithm of the sense product and word
product and the square root of the deplural-stem ra-
tio (ds ratio) to mitigate skewness of the data. We
included all five ambiguity properties to construct
a query-based selection model as they demonstrate
statistical significance in prediction.

23



organic soil enhancement

organic soil enhancement

organ soil enhanc

organic

organism 

organizer

organization

organize

organ

organized

organically

soil

soiled

soiling

organs 

8

7

6

6

3

3

3

2

1

5

1

1

enhancive

enhance

enhanced

enhancement

enhancer

2

2

1

1

1

39 7 7

39x7x7=1911
1911

5.97082 5.18994 4.86996

16.0307

token

n number of senses 

n IDF

Legend

5

3

9

n number of words

105
9x3x5=105

Figure 5: Example of ambiguity indicators on the query
“organic soil enhancement”

Figure 6 demonstrates the distribution of ambigu-
ity properties against the actual best text normaliza-
tion technique. It is noted that stemming is the actual
best method when a query has lower sense product,
or lower word product, or a higher idf-sum. It im-
plies that stemming is less likely to be the actual best
method as a query is ambiguous. The results demon-
strate the potential of utilizing ambiguity measures
to select the actual best text normalization technique.

5 Query-based Selection Model

The cluster analysis in Section 3 suggests that the
choice of text normalization technique should be
made individually on each topic. The retrieval per-
formance would be enhanced as we choose an ap-
propriate text normalization technique for a given
topic. Given the five ambiguity properties described
in Section 4, we constructed Support Vector Regres-
sion (SVR) (Smola and Schlkopf, 2004) models to
choose between stemming, depluralization, and not
doing any text normalization for different queries.
Regression models aim to discover the relationship
between two random variables x and y. In our work,
independent variable x is a vector of the five prop-
erties described in section 4: x = (sense product,

word product, deplural-stem ratio, Idf-sum, length),
and dependent variable y is the MAP score of a
given topic. SVR has been successfully applied
for many time series and function estimation prob-
lems. We utilized training data to construct multiple
SVR models for each of nine combinations of query
models (AND, OR, and MLT) and text normaliza-
tion techniques (raw, depluralized, and stemmed
queries). For example, the regression model for an
MLT query model using stemmed queries is:

Map MLT stem = 0.0853
− 0.0849 ∗ length
+ 0.6286 ∗ sense prod
+ 0.0171 ∗ word prod
− 0.0774 ∗ gap ds
+ 0.4189 ∗ idf sum

For a given query model, MLT, for example, we
utilized training data to construct three SVR models
each to predict the MAP scores of raw queries, de-
pluralized queries, and stemmed queries in the test
set. We then compared the predicted MAP score of a
query and selected the text normalization technique
with the highest predicted score. Figure 5 illustrates
our experiment framework on the query-based se-
lection model. We used five-fold cross-validation to
evaluate the performance of the selection model. For
each iteration (fold) we used the 4 out of the 5 parti-
tions as training data, constructing SVR models and
using the remaining fifth partition for testing. We ac-
cumulated all testing results and computed one over-
all MAP score for evaluation. Table 3 shows the re-
sults of the five-fold cross-validation performed on
249 query topics provided by the TREC 2004 Ro-
bust Track. We utilized a paired t-test to determine
the performance difference between the query-based
selection model (hybrid model) and other three text
normalization techniques. The results in Table 3
shows that the query-based selection model attained
the highest MAP score and achieved significant im-
provement.

6 Conclusion and Future work

This paper evaluates the performance of stemming
and depluralization on the TREC 2004 Robust track
collection. We assume that the depluralization, as

24



Feature Coefficient R-square P-value
length -0.06811 0.07864 5.768e-05*
log(sense prod) -0.034692 0.1482 1.819e-08*
log(word prod) -0.04557 0.09426 9.78e-06*
sqrt(ds ratio) -0.021657 0.03738 0.006088*
idf sum 0.008498 0.04165 0.003747*

Table 2: Results of simple linear regression on the MAP of stemmed queries in MLT query model.

dep raw stem

0
2

4
6

8
10

Sense Product(log) vs. Actual Best Method

Actual Best Lexical Analysis

lo
g(
se
ns
e_
pr
od
)

dep raw stem

0
1

2
3

4
5

Word Product(log) vs. Actual Best Method

Actual Best Lexical Analysis

lo
g(
w
or
d_
pr
od
)

dep raw stem

5
10

15
20

25
30

35

IDF-sum vs. Actual Best Method

Actual Best Lexical Analysis
id
f_
su
m

Figure 6: Boxplots of the distribution of three ambiguity properties in each actual best text normalization technique

2

3

4

1

5

Divide all query 
topics into five 

groups

Training

MAP score from 
raw queries

MAP score from 
depluralized queries

MAP score from 
stemmed queries

2
3
4
5

SVR for 
raw queries

2
3
4
5

SVR for 
depluralized 

queries

2
3
4
5

SVR for 
stemmed 
queries

Building Support 
Vector Regression 

(SVR) models

Testing and 
predicting Mean 

Average Precision 
(MAP)

1

1

1

Select the text 
normalization 
technique with 

highest predicted 
MAP score

Figure 7: Five-fold cross validation on query-based selection model (hybrid model)

25



Raw Dep Stem Hybrid
AND MAP 0.1213 0.1324 0.1550 0.2094

p-value <2.2e-16* <2.2e-16* <2.2e-16*
OR MAP 0.1851 0.1922 0.2069 0.2131

p-value 1.286e-05* 0.0003815* 0.09
MLT MAP 0.1893 0.1959 0.2093 0.2132

p-value 3.979e-05* 0.000939* 0.09677

Table 3: Paired T-test was performed to examine the differences of each text normalization techniques (raw, deplu-
ralizer, and stemmer) and query-based selection model (hybrid model). Significant differences between models are
labeled with *.

a low-level text-normalization technique, introduces
less ambiguity than stemming and preserves more
precise semantics of text. The experimental re-
sults demonstrate variable patterns, indicating that
some topics performed better as a depluralized query
than as a stemmed query. From Figure 4 in Sec-
tion 3, we conclude that the choice of text nor-
malization technique should be made individually
on each topic. An effective query-based selection
model would enhance information retrieval perfor-
mance. The query-based selection model utilizes
Support Vector Regression (SVR) models to predict
the mean average precision (MAP) of each query
from the ambiguity measures, and to choose an ap-
propriate normalization technique based on these
predictions. The selection is lightweight, requiring
only analysis of the topic title itself against infor-
mation readily available regarding the corpus (e.g,
term idf values). We extracted 5 measures to quan-
tify the ambiguity of a query: 1) sense product; 2)
word product; 3) deplural-stem ratio; 4) idf-sum;
5) length of a query. The constructed query-based
selection model demonstrate positive results on en-
hanced performance. The experiments reported here
show that, even when the improvement is modest
(1%), the selection model competes well with tra-
ditional approaches. To improve the model, future
work may first explore and introduce more powerful
features to the models, considering properties such
as part of speech of text. Second, future work may
explore the effect of noise and outliers in the data
to improve the accuracy of the model. Finally, ad-
ditional data mining techniques may be adopted in
future work to further improve the prediction.

References
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-

mann, and I.H. Witten. 2009. The WEKA Data Min-
ing Software: An Update. SIGKDD Explorations,
1:10–18.

David A. Hull. 1996. Stemming algorithms: A case
study for detailed evaluation. Journal of the American
Society for Information Science, 47(1):70–84.

L. Kaufman and P.J. Rousseeuw. 1990. Finding groups
in data. An introduction to cluster analysis. Wiley,
New York.

Robert Krovetz. 2000. Viewing morphology as an in-
ference process. Artificial Intelligence, 118(1-2):277–
294, April.

Christopher D Manning and Hinrich Schtze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press, Cambridge, Mass.

Paul McNamee, Charles Nicholas, and James Mayfield.
2008. Don’t have a stemmer?: be un+concern+ed.
In Proceedings of the 31st International ACM SIGIR
Conference on Research and Development in Informa-
tion retrieval, pages 813–814, Singapore, Singapore.
ACM.

Ellen Riloff. 1995. Little words can make a big differ-
ence for text classification. In Proceedings of the 18th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 130–
136, Seattle, Washington, United States. ACM.

Alex J. Smola and Bernhard Schlkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199–222.

E. M. Voorhees. 2006. The TREC 2005 robust track.
In ACM SIGIR Forum, volume 40, pages 41–48. ACM
New York, NY, USA.

26


