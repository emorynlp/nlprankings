



















































Sarcastic or Not: Word Embeddings to Predict the Literal or Sarcastic Meaning of Words


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1003–1012,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Sarcastic or Not: Word Embeddings to Predict the Literal or Sarcastic
Meaning of Words

Debanjan Ghosh§ and Weiwei Guo† and Smaranda Muresan‡
§School of Communication and Information, Rutgers University, NJ, USA

†Department of Computer Science, Columbia University, NY, USA
‡Center for Computational Learning Systems, Columbia University, NY, USA

debanjan.ghosh@rutgers.edu, weiwei@cs.columbia.edu, smara@ccls.columbia.edu

Abstract

Sarcasm is generally characterized as a
figure of speech that involves the substi-
tution of a literal by a figurative mean-
ing, which is usually the opposite of the
original literal meaning. We re-frame the
sarcasm detection task as a type of word
sense disambiguation problem, where the
sense of a word is either literal or sar-
castic. We call this the Literal/Sarcastic
Sense Disambiguation (LSSD) task. We
address two issues: 1) how to collect a set
of target words that can have either literal
or sarcastic meanings depending on con-
text; and 2) given an utterance and a target
word, how to automatically detect whether
the target word is used in the literal or the
sarcastic sense. For the latter, we investi-
gate several distributional semantics meth-
ods and show that a Support Vector Ma-
chines (SVM) classifier with a modified
kernel using word embeddings achieves a
7-10% F1 improvement over a strong lex-
ical baseline.

1 Introduction

Recognizing sarcasm is important for understand-
ing people’s actual sentiments and beliefs. For
example, failing to recognize the following mes-
sage as being sarcastic “I love that I have to go
back to the emergency room”, will lead a senti-
ment and opinion analysis system to infer that the
author’s sentiment is positive towards the event
of “going to the emergency room”. Current ap-
proaches have framed the sarcasm detection task
as predicting whether a full utterance is sarcastic
or not (Davidov et al., 2010; González-Ibáñez et
al., 2011; Riloff et al., 2013; Liebrecht et al., 2013;
Maynard and Greenwood, 2014).

We propose a re-framing of sarcasm detection
as a type of word sense disambiguation problem:

given an utterance and a target word, identify
whether the sense of the target word is literal or
sarcastic. We call this the Literal/Sarcastic Sense
Disambiguation (LSSD) task. In the above utter-
ance, the word “love” is used in a sarcastic, non-
literal sense (the author’s intended meaning be-
ing most likely the opposite of the original literal
meaning - a negative sentiment, such as “hate”).
Two key challenges need to be addressed: 1) how
to collect a set of target words that can have a lit-
eral or a sarcastic sense, depending on context;
and 2) given an utterance containing a target word,
how can we determine whether the target word is
used in its literal sense (e.g., “I love to take a nice
stroll in the park every morning”), or in a sarcastic
sense (e.g., “I love going to the dentist.”).

To address the first challenge, we need to iden-
tify a set of words from sarcastic utterances, which
have a figurative/sarcastic sense (e.g., “love” in the
utterance “I love going to the dentist”). We pro-
pose a crowdsourcing task where Turkers in Ama-
zon Mechanical Turk (MTurk) platform are given
sarcastic utterances (tweets labeled with #sarcasm
or #sarcastic hashtags) and are asked to re-phrase
those messages so that they convey the author’s in-
tended meaning (“I love going to the dentist” can
be rephrased as “I hate going to the dentist” or
“I don’t like going to the dentist”). 1 Given this
parallel dataset, we use unsupervised alignment
techniques to identify semantically opposite words
(e.g., “love” ↔ “hate”, “brilliant” ↔ “stupid”,
“never”↔ “always”). The words from these pairs
that appear in the original sarcastic utterances are
then considered as our collection of target words
(e.g., “love”, “brilliant”, “never”) that can have
both a sarcastic and a literal sense depending on
the context (Section 2).

To address the second challenge, we compare
several distributional semantics methods generally
used in word sense disambiguation tasks (Sec-

1utterances and messages are used interchangeably.

1003



Target Sense Utterance
S . . . starting off the new year great

!!!!! sick in bed . . .
great L . . . you don’t need a record label to

have great music . . .
Lsent . . . i’m in love with this song great

job justin . . .
S yay something to be proud of 3rd

poorest in the NATION . . .
proud L im filipino with dark brown eye

and forever true and proud . . .
Lsent but i’m proud of all the beliebers

AROUND THE WORLD . . .

Table 1: Examples of Targets and their Senses

tion 3). We show that using word embeddings in
a modified SVM kernel achieves the best results
(Section 4). To collect training and test datasets
for each of the target words, we use Twitter mes-
sages that contain those words. For the sarcas-
tic sense (S), we use tweets that contain the target
word and are labeled with the #sarcasm or #sar-
castic hashtags. For the literal sense (L), we col-
lect tweets that contain the target word and are
not labeled with the #sarcastic or #sarcasm hash-
tags. Table 1 shows examples of two targets words
(“great” and “proud”) and their sarcastic sense (S)
and literal sense (L). In addition, for the literal
sense, we also consider a special case, where the
tweets are labeled with either positive or nega-
tive hashtags (e.g., #happy, #sad) as proposed by
Gonzalez et al. (2011). We denote these senti-
ment tweets as Lsent (Table 1). Gonzalez et al.
(2011) argue that it is harder to distinguish sar-
castic from non-sarcastic messages where the non-
sarcastic messages contain sentiment. Our results
support this argument (97% F1 measure for the
best result for S vs. L, compared to 84% F1 for
the best result for S vs. Lsent; Section 4).2

2 Collection of Target Words

To collect a set of target words that can have either
literal or sarcastic meaning depending on context,
we propose a two step approach: 1) a crowdsourc-
ing task to collect a parallel dataset of sarcastic
utterances and their re-phrasings that convey the
authors’ intended meaning; and 2) unsupervised
alignment techniques to detect semantically oppo-
site words/phrases.

Crowdsourcing Task. Given a sarcastic mes-
sage (SM), Turkers were asked to re-phrase the

2The datasets used in the experiments is available at
https://github.com/debanjanghosh/sarcasm wsd.

message so that the new message is likely to ex-
press the author’s intended meaning (IM). Exam-
ples of an original sarcastic message (1) and three
messages generated by the Turkers (2) is given be-
low:

(1) [SM] I am so happy that I am going
back to the emergency room.

(2) a. [IM1] I don’t like that I have to go to
the emergency room again.

b. [IM2] I am so upset I have to return to
the emergency room.

c. [IM3] I’m so unhappy that I am going
back to the emergency room.

From the above examples, we can see that align-
ing the sarcastic message (SM) to the re-phrasings
containing the author’s intended meaning gener-
ated by the Turkers (IM1, IM2, IM3) will al-
low us to detect that “happy” can be aligned to
“don’t like”, “upset”, and “unhappy”. Based on
this alignment, “happy” will be considered as a
target word for the LSSD task.

We used 1,000 sarcastic messages collected
from Twitter using the #sarcasm and #sarcastic
hashtags. The Turkers were provided with de-
tailed instructions of the task including a defini-
tion of sarcasm, the task description, and multi-
ple examples. In addition, for messages that con-
tain one or more sentences and where sarcasm is
related to only a part of the message, the Turk-
ers were instructed to consider the entire message
in their rephrasing. This emphasis was added to
avoid high asymmetry in the length between the
original sarcastic message and the rephrasing of
the intended meaning. For each original sarcas-
tic message (SM), we asked five Turkers to do the
rephrasing task. Each HIT contains 1 sarcastic
message, and Turkers were paid 5 cents for each
HIT. To ensure a high quality level, only quali-
fied workers were allowed to perform the task (i.e.,
more than 90% approval rate and at least 500 ap-
proved HITs). In this way, we obtained a dataset
of 5,000 SM-IM pairs.

Unsupervised Techniques to Detect Semanti-
cally Opposite Words/Phrases. We use two
methods for unsupervised alignment. First, we
use the co-training algorithm for paraphrase detec-
tion developed by Barzilay and McKeown (2001).
This algorithm is used for two specific reasons.
First, our dataset is similar in nature to the parallel

1004



monolingual dataset used in Barzilay and McK-
eown (2001), and thus lexical and contextual in-
formation from tweets can be used to extract the
candidate targets words for LSSD. For instance,
we can align the [SM] and [IM3] (from the above
examples), where except for the words happy and
unhappy, the majority of the words in the two
messages are anchor words and thus happy and
unhappy can be extracted as paraphrases via co-
training. To model contextual information, such
as part of speech tagging for the co-training algo-
rithm, we used Tweet NLP (Gimpel et al., 2011).
Second, Bannard and Callison-Burch (2005) no-
ticed that the co-training method proposed by-
Barzilay and McKeown (2001) requires identical
bounding substrings and has bias towards single
words while extracting paraphrases. This apparent
limitation, however, is advantageous to us because
we are specifically interested in extracting target
words. Co-training resulted in 367 extracted pairs
of paraphrases.

We also considered a statistical machine transla-
tion (SMT) alignment method - IBM Model 4 with
HMM alignment implemented in Giza++ (Och
and Ney, 2000). We used Moses software(Koehn
et al., 2007) to extract lexical translations by align-
ing the dataset of 5,000 SM-IM pairs. From
the set of 367 extracted paraphrases using Barzi-
lay and McKeown (2001)’s approach, we selected
only those paraphrases where the lexical transla-
tion scores φ (resulted after running Moses) are
≥ 0.8. After filtering via translation scores and
manual inspection, we obtained a set of 80 seman-
tically opposite paraphrases. Given this set of se-
mantically opposite words, the words that appear
in the sarcastic messages were consider our target
words for LSSD (70 target words after lemmatiza-
tion). They range from verbs, such as “love” and
“like”, adjectives, such as “brilliant”, “genius”,
and adverbs, such as “really”.

3 Literal/Sarcastic Sense Disambiguation

Our Literal/Sarcastic Sense Disambiguation
(LSSD) task is formulated as follows: given a
candidate utterance (i.e., a tweet) that contains a
target word t, identify whether the sense of t is
sarcastic (S) or literal (L). In order to be able to
solve this problem we need training and test data
for each target word that consists of utterances
where the target word is used either in the literal
sense or the sarcastic sense.

love(26802), like(14995), great(14495), good(11624),
really(9825), right(6771), fun(6603), best(6182),
better(5960), glad(5748), yeah(5504), nice(4443),
awesome(4196), excited(4027), always(3807),
happy(3098), cool(2705), amazing(1952), fa-
vorite(1883), perfect(1792), wonderful(1749), won-
der(1476), lovely(1424), super(1390), fantastic(1369),
joy(1176), cute(1007), beautiful(981), sweet(800),
hot(729), proud(703), shocked(645), interested(624),
brilliant(576), genius(481), attractive(449), mature(427)

Table 2: Target words and # of training instances
per sense

3.1 Data Collection

To collect training and test datasets for each of the
target words, we use Twitter messages that contain
those words. For the sarcastic sense (S), we use
tweets that contain the target word and are labeled
with the #sarcasm or #sarcastic hashtag. For the
literal sense (L), we collect tweets that contain the
target word and are not labeled with the #sarcastic
or #sarcasm hashtags. In addition, for the literal
sense we also consider a special case, where the
tweets are labeled with either positive or negative
sentiment hashtags (e.g., #happy, #sad). Thus, we
consider two LSSD tasks: S vs. L and S vs. Lsent,
and aim to collect a balanced dataset for each tar-
get word.

For the 70 target words (see Section 2), we col-
lected a total of 2,542,249 tweets via Twitter API .
We considered a setup where 80% of data is used
for training, 10% for development, and 10% for
test. We empirically set the number of minimum
training instances for each sense of the target word
to 400 without any upper restriction. This resulted
in 37 target words to be used in the LSSD exper-
iments. Table 2 shows all the target words and
their corresponding number of training instances
for each sense (S and L/Lsent). The size of train-
ing data ranges from 26,802 for the target word
“love” to 427 for the word “mature”. As we will
see in the results sections, however, the size of
the training data is not always the key factor in
the LSSD task, especially for the methods that use
word embeddings.

3.2 Learning Approaches

We consider two classical approaches used in
word sense disambiguation tasks: 1) distributional
approaches where each sense of a target word is
represented as a context vector derived from the
training data; and 2) classification approaches (S

1005



vs. L; S vs. Lsent) for each target word.

3.2.1 Distributional Approaches
The Distributional Hypothesis in linguistics is de-
rived from the semantic theory of language usage,
i.e., words that are used and occur in the same
contexts tend to purport similar meanings (Harris,
1954). Distributional semantic models (DSMs)
use vectors that represent the contexts (e.g., co-
occurring words) in which target words appear in
a corpus, as proxies for meaning representations.
Geometric techniques such as cosine similarity are
then applied to these vectors to measure the simi-
larity in meaning of corresponding words.

The DSMs are a natural approach to model our
LSSD task. For each target word t we build two
context-vectors that will represent the two senses
of the target word t using the training data: one for
the sarcastic sense S using the sarcastic training
data for t (~vs) and one for the literal sense L using
the literal sense training data for t (~vl).3 Given
a test message u containing a target word t, we
first represent the target word as a vector ~vu using
all the context words inside u. To predict whether
t is used in a literal or sarcastic sense in the test
message u we simply apply geometric techniques
(e.g., cosine similarity) between ~vu and the two
sense vectors ~vs and ~vl, choosing the one with the
maximum score.

To create the two sense vectors ~vs and ~vl for
each of the target words t, we use the posi-
tive pointwise mutual information model (PPMI)
(Church and Hanks, 1990). Based on t’s con-
text words ck in a window of 10 words, we sep-
arately computed PPMI for sarcastic and literal
senses using t’s training data. The size of the con-
text widow used in DSMs is generally between 5
and 10, and in our experiments we used a win-
dow of 10 words since tweets often include mean-
ingful words/tokens at the end of the tweets (e.g.,
interjections, such as “yay”, “ohh”; upper-case
words, such as, “GREAT”; novel hashtags, such
as “#notreally”, “#lolol”; emoticons, such as “:(”).
We sorted the context words based on the PPMI
scores and for each target word t we selected a
maximum of 1,000 context words per sense to ap-
proximate the two senses of the target word (i.e.,
the vectors ~vs and ~vl for each target word t consist
of a maximum of 1,000 words). Table 3 shows
some target words and their corresponding con-

3In the remaining of this section we will only mention L
and not Lsent for clarity and brevity.

Targets Senses Context Vector
S ignored, being, waking, work, sick,

#not
love L please, follow, ♥, her, :)

Lsent happy, family, blessed, cute, birth-
day

S work, tomorrow, homework, friday,
sleep

fun L hope, join, girl, game, friend
Lsent #friends, #family, weekend, amaz-

ing, #christmas
S working, snow, waking, studying,

sick
joy L yesterday, sweet, special, prayer,

laughter
Lsent wishing, warmth, love, christmas,

peace

Table 3: Target words and their context words

text words that were selected based on high PPMI
scores.

To predict whether t is used in a literal or sar-
castic sense in the test message u we simply apply
the cosine similarity to the ~vu (vector representa-
tion of the target word t in the test message u) and
the two sense vectors ~vs and ~vl of t, choosing the
one with the maximum score. All vector elements
are given by the tf-idf values of the corresponding
words. This approach, denoted as the “PPMI base-
line”, is the baseline for our DSM experiments.

Context Vectors with Word Embedding: The
above method considers that the context vectors
~vs and ~vl of each target word t contain the co-
occurring words selected by their PPMI values.
We enhance the representation of context vectors
to represent each word in the context vector by
its word embedding. We experiment with three
different methods of obtaining word embeddings:
Weighted Textual Matrix Factorization (WTMF)
(Guo and Diab, 2012b); word2vec that imple-
ments the skip-gram and continuous bag-of-words
models (CBOW) of Mikolov et al. (2013a), and
GloVe (Pennington et al., 2014), a log-bilinear re-
gression model based upon global word-word co-
occurrence count in the training corpora.

After removing the tweets that are used as test
sets, we build the three word embedding mod-
els in an unsupervised fashion with the remaining
2,482,763 tweets from our original data collection
(Section 3.1). In each of the three models, each
word w is represented by its d-dimensional vec-
tor ~w of real numbers, where d=100 for all of the
embedding algorithms in our experiments. For the
size of the embedding vectors, it is common to use

1006



100 or 300 dimensions, with larger dimensions for
larger datasets. Our current dataset is smaller than
the ones used in other applications of word embed-
dings (e.g., Pennington et al. (2014) have used bil-
lion tweets to create word embedding) so we opted
for 100 dimensional vectors. Below are the short
descriptions of the three word embedding models:

• Weighted Textual Matrix Factorization
(WTMF): Low-dimensional vectors have
been used in WSD tasks, since they are
computationally efficient and provide better
generalization than surface words. A dimen-
sion reduction method is Weighted Textual
Matrix Factorization (WTMF) (Guo and
Diab, 2012b), which is designed specifically
for short texts, and has been successfully
applied in WSD tasks (Guo and Diab,
2012a). WTMF models unobserved words,
thus providing more robust embeddings for
short texts such as tweets.

• word2vec Representation: We use both the
Skip-gram model and the Continuous Bag-
of-Words (CBOW) model (Mikolov et al.,
2013a; Mikolov et al., 2013c) as imple-
mented in the word2vec gensim python li-
brary. 4 Given a window size of n words
around a word w, the skip-gram model pre-
dicts the neighboring words given the current
word. In contrast, the CBOW model predicts
the current word w, given the neighboring
words in the window. We considered a con-
text window of 10 words.

• GloVe Representation: GloVe (Pennington et
al., 2014) is a word embedding model that
is based upon weighted least-square model
trained on global word-word co-occurrence
counts instead of the local context used by
word2vec.

Here, the LSSD task is similar to the baseline:
to predict whether the target word t in the test mes-
sage u is used in a literal or sarcastic sense, we
simply use a similarity measure between the ~vu
(vector representation of the target word t in the
test message u) and the two sense vectors ~vs and
~vl of t, choosing the one with the maximum score.
The difference from the baseline is twofold: First,
all vectors elements are word embeddings (i.e.,

4https://radimrehurek.com/gensim/models/word2vec.html

100-d vectors). Second, we use the maximum-
valued matrix-element (MVME) algorithm intro-
duced by Islam and Inkpen (2008), which has been
shown to be particularly useful for computing the
similarity of short texts. We modify this algorithm
to use word embeddings (MVMEwe). The idea
behind the MVME algorithm is that it finds a one-
to-one “word alignment” between two utterances
(i.e., sentences) based on the pairwise word sim-
ilarity. Only the aligned words contribute to the
overall similarity score.

Algorithm 1 MVMEwe
1: procedure MVMEwe(vs,vu)
2: vswords ← vs.elements()
3: vuwords ← vu.elements()
4: M [vswords .size(), vuwords .size()]← 0
5: for k ← 0, vswords .size() do
6: ck ← vswords [k]
7: ~ck ← getEmbedding(ck)
8: for j ← 0, vuwords .size() do
9: wj ← vuwords [j]

10: ~wj ← getEmbedding(wj)
11: M [k][j]← cosine(~ck, ~wj)
12: end for
13: end for
14: while True do
15: repeat
16: max← getMax(M)
17: Sim← Sim+max
18: rm, cm ← getRowCol(M,max)
19: . Remove rm row and cm column from M
20: remove(M, rm, cm)
21: until max > 0 Or M.size() > 0
22: end while
23: Return Sim
24: end procedure
25:

26: procedure GETEMBEDDING(word)
27: Return wemodel[word]
28: end procedure
29: procedure GETROWCOL(M,max)
30: row, col←M.indexOf(max)
31: Return row, col
32: end procedure

Algorithm 1 presents the pseudocode of
our modified algorithm for word embeddings,
MVMEwe. Let the total similarity between ~vs
and ~vu be Sim. For each context word ck from
~vs and each word wj from ~vu, we compute a ma-
trix where the value of the matrix element Mjk

1007



denotes the cosine similarity between the embed-
ded vectors ~ck and ~wj [lines 5 -13]. Next, we first
select the matrix cell that has the highest similarity
value in M (max) and add this to the Sim score
[lines 16-17]. Let the rm and cm be the row and
the column of the cell containingmax (maximum-
valued matrix element), respectively. Next, we re-
move all the matrix elements of the rm-th row and
the cm-th column from M [line 20]. We repeat
this procedure until we have traversed through all
the rows and columns of M or max = 0 [line 21].

3.2.2 Classification Approaches
The second approach for our LSSD task is to treat
it as a binary classification task to identify the sar-
castic or literal sense of a target word t. We have
two classification tasks: S vs. L and S vs. Lsent
for each of the 37 target words. We use the lib-
SVM toolkit (Chang and Lin, 2011). Development
data is used for tuning parameters.

SVM Baseline: The SVM baseline for LSSD
tasks uses n-grams and lexicon-based binary-
valued features that are commonly used in exist-
ing state-of-the-art sarcasm detection approaches
(González-Ibáñez et al., 2011; Tchokni et al.,
2014). They are derived from i) bag-of-words
(BoW) representations of words, ii) LIWC dic-
tionary (Pennebaker et al., 2001), and iii) a list
of interjections (e.g., “ah”, “oh”, “yeah”), punc-
tuations (e.g., “!”, “?”), and emoticons collected
from Wikipedia. CMU Tweet Tokenizer is em-
ployed for tokenization. 5 We kept unigrams
unchanged when all the characters are upper-
case (e.g., “NEVER” in “A shooting in Oakland?
That NEVER happens! #sarcasm”) but otherwise
words are converted to lower case. We also change
all numbers to a generic number token “22”. To
avoid any bias during experiments, we removed
the target words from the tweets as well as any
hashtag used to determine the sense of the tweet
(e.g., #sarcasm, #sarcastic, #happy, #sad).

SVM with MVMEwe Kernel: We propose a
new kernel kernelwe to compute the semantic
similarity between two tweets ur and us using the
MVMEwe method introduced for the DSM ap-
proach, and the three types of word embeddings
(WTMF, word2vec, and GloVe). The similarity
measure in the kernel is similar to the algorithm
MVMEwe described in Algorithm 1, but instead

5http://www.ark.cs.cmu.edu/TweetNLP/

of measuring the similarity between the sense vec-
tors of t (~vs, ~vl) and the vector representation of t
in test message ( ~vu), now we measure the similar-
ity between two tweets ur and us. For each k-th
index word wk in ur and l-th index word wl in
us we compute the cosine similarity between the
embedded vectors of the words and fill up a sim-
ilarity matrix M . We select the matrix cell that
has the highest similarity, add this similarity score
to the total similarity Sim, remove the row and
column from M that has highest similarity score,
and repeat the procedure (similar to Algorithm 1).
We noticed that MVMEwe algorithm carefully
chooses the best candidate word wl in us for the
wk word in ur since wl is the most similar word to
wk. The algorithm continues the same procedure
for all the remaining words in ur and us. The fi-
nal Sim is used as the kernel similarity between
ur and us. We augment this kernel kernelwe into
libSVM and during evaluation we run supervised
LSSD classification for each target word t sepa-
rately.

4 Results and Discussions

Tables 4 and 5 show the results for the LSSD
experiments using distributional approaches and
classification-based approaches, respectively. For
brevity, we only report the average Precision (P),
Recall (R), and F1 scores with their standard
deviation (SD) (given by ‘±’), and the targets
with maximum/minimum F1 scores. w2vsg and
w2vcbow represent the skip-gram and CBOW mod-
els implemented in word2vec, respectively.

Table 4 presents the results of distributional
approaches (Section 3.2.1). We observe that
the word embedding methods have better perfor-
mance than the PPMI baseline for both S vs. L
and S vs. Lsent disambiguation tasks. Also,
the average P/R/F1 scores for S vs. L are much
higher than for S vs. Lsent. Since all tweets with
Lsent sense were collected using sentiment hash-
tags (González-Ibáñez et al., 2011), they might be
lexically more similar to the S tweets than the L
tweets are and thus identifying the sense of a tar-
get word t between S vs. Lsent is a harder task. In
Table 4 we also observe that the average F1 scores
between WTMF, w2vsg, w2vcbow, and GloVe are
comparable and between 84%-86%, with w2vsg
and w2vcbow achieving slightly higher F1.

Table 5 outlines the LSSD experiments us-

1008



Expr. Senses Avg. P Avg. R Avg. F1 Max. F1(Target) Min. F1(Target)
S 73.5 ± 3.6 84.6 ± 6.0 78.5 ± 3.2 83.9(mature) 68.8(wonder)
L 83.1 ± 5.0 70.6 ± 5.5 76.1 ± 3.4 82.7(love) 68.3(nice)

PPMIbl S 67.8 ± 7.0 76.2 ± 13.6 70.4 ± 7.6 81.8(joy) 43.8(like)
Lsent 74.2 ± 7.1 62.7 ± 12.8 66.9 ± 6.6 78.6(joy) 47.1(interested)

S 83.0 ± 3.4 87.2 ± 5.4 84.9 ± 2.4 91.4(mature) 78.7(wonder)
L 87.5 ± 4.4 82.7 ± 4.5 84.9 ± 2.2 90.5(mature) 80.6(nice)

WTMF S 67.4 ± 5.5 86.5 ± 5.1 75.6 ± 3.9 84.4(joy) 65.8(interested)
Lsent 82.1 ± 5.8 58.9 ± 9.7 68.1 ± 7.2 81.5(joy) 50.0(genius)

S 83.7 ± 3.6 85.6 ± 5.6 84.5 ± 2.8 90.6(joy) 78.8(sweet)
L 86.3 ± 4.6 84.0 ± 4.3 85.0 ± 2.5 89.6(joy) 79.2(like)

GloVe S 70.7 ± 5.1 84.3 ± 5.0 76.8 ± 3.9 85.4(joy) 67.1(interested)
Lsent 80.7 ± 5.4 64.7 ± 8.5 71.5 ± 6.1 84.0(joy) 54.7(hot)

S 84.9 ± 3.3 87.0 ± 4.8 85.8 ± 2.6 90.9(mature) 80.7(like)
L 87.5 ± 4.1 85.1 ± 4.0 86.2 ± 2.5 90.7(mature) 80.2(like)

w2vsg S 70.8 ± 4.8 85.7 ± 5.1 77.4 ± 4.0 86.7(joy) 68.1(interested)
Lsent 82.2 ± 5.7 64.3 ± 7.8 71.9 ± 5.9 85.4(joy) 57.4(interested)

S 84.9 ± 3.2 86.7 ± 4.7 85.6 ± 2.5 90.9(mature) 80.7(like)
L 87.3 ± 4.0 85.1 ± 3.8 86.1 ± 2.4 90.7(mature) 80.2(like)

w2vcbow S 70.7 ± 4.8 85.8 ± 5.0 77.4 ± 4.0 86.4(joy) 68.6(attractive)
Lsent 82.0 ± 5.6 64.0 ± 7.7 71.7 ± 5.8 85.0(joy) 58.7(interested)

Table 4: Evaluation of distributional approaches (PMI and word embedding) for LSSD experiments

Expr. Senses Avg. P Avg. R Avg. F1 Max. F1(Target) Min. F1(Target)
S 87.0 ± 3.3 85.6 ± 3.1 86.3 ± 2.7 91.7(yeah) 75.4(sweet)
L 85.9 ± 2.8 87.1 ± 3.6 86.5 ± 2.8 91.8(yeah) 76.1(sweet)

SV Mbl S 77.3 ± 4.6 78.2 ± 4.2 77.7 ± 3.8 85.5(love) 68.6(brilliant)
Lsent 77.8 ± 3.7 76.7 ± 6.4 77.1 ± 4.7 85.8(love) 64.6(attractive)

S 94.1 ± 2.2 94.6 ± 1.8 94.3 ± 1.8 97.3(brilliant) 88.3(joy)
L 94.6 ± 1.8 94.0 ± 2.3 94.3 ± 1.9 97.2(mature) 87.9(joy)

kernelWTMF S 79.0 ± 4.6 78.8 ± 4.4 78.8 ± 3.8 84.8(mature) 61.0(genius)
Lsent 78.8 ± 3.7 78.9 ± 4.9 78.8 ± 3.6 85.4(mature) 63.5(genius)

S 95.7 ± 1.6 97.4 ± 1.7 96.5 ± 1.1 99.1(mature) 92.9(glad)
L 97.4 ± 1.6 95.6 ± 1.7 96.5 ± 1.2 99.1(mature) 92.7(interested)

kernelGloV e S 79.5 ± 3.5 83.1 ± 3.0 81.2 ± 2.8 86.9(joy) 74.2(attractive)
Lsent 82.2 ± 3.0 78.3 ± 4.4 80.2 ± 3.4 86.6(joy) 69.2(attractive)

S 96.6 ± 1.1 98.5 ± 0.6 97.5 ± 0.4 99.2(cute) 93.8(interested)
L 98.5 ± 0.7 96.5 ± 1.2 97.5 ± 0.5 99.2(cute) 93.5(interested)

kernelw2vsg S 81.9 ± 3.8 88.1 ± 3.2 84.8 ± 3.0 88.8(love) 74.2(genius)
Lsent 87.0 ± 3.2 80.2 ± 4.7 83.4 ± 3.5 88.8(love) 73.3(genius)

S 96.4 ± 1.0 98.2 ± 1.1 97.3 ± 0.6 99.1(mature) 93.8(interested)
L 98.2 ± 1.1 96.3 ± 1.1 97.2 ± 0.7 99.1(mature) 93.5(interested)

kernelw2vcbow S 81.7 ± 3.8 88.6 ± 2.9 84.9 ± 2.8 89.5(love) 74.8(genius)
Lsent 87.4 ± 2.9 79.9 ± 4.8 83.4 ± 3.4 89.2(love) 74.4(genius)

Table 5: Evaluation of classification approaches (SVMbl and kernelwe) for LSSD experiments

ing the classification approaches (Section 3.2.2):
SVM baseline (SVMbl) and SVM using the
kernelwe with word embeddings (kernelWTMF ,
kernelGloV e, kernelw2vsg , and kernelw2vcbow ).
The classification approaches give better perfor-
mance compared to the distributional approaches.
The SVMbl is around 7-8 % higher than the
PPMIbl and comparable with the word embed-
dings used in distributional approaches (Table 4).
In addition, our new SVM kernel method using
word embeddings shows significantly better re-
sults when compared to the SVMbl (and distri-
butional approaches). For instance, for the S vs.
L task, the average F1 is 96-97%, which is more
than 10% higher than SVMbl. Similarly, for S

vs. Lsent task, F1 scores reported by the kernel
using word2vec embeddings are in the range of
83%-84% compared to 77% given by the SVMbl,
showing an absolute increase of 7%. As stated ear-
lier, MVME algorithm aligns similar word pairs
found in its inputs and this performs well for short
texts (i.e., tweets). Thus, the MVME algorithm
combined with word embedding in kernelwe re-
sults in very high F1. Among the word embedding
models, word2vec models give marginally better
results compared to GloVe and WTMF, and GloVe
outperforms marginally WTMF. Similar to Table
4, here, the average F1 scores for S vs. L task are
higher than the S vs. Lsent results.

In terms of the best and worst performing tar-

1009



gets, SVMbl prefers targets with more training
data (e.g., “yeah”, “love” vs. “sweet”, “attractive”;
see Table 2). In contrast, word embedding mod-
els for “joy” and “mature”, two targets with com-
paratively low number of training instances have
achieved very high F1 using both distributional
and classification approaches (Table 4 and 5). This
can be explained by the fact that for words, such as
“joy”, “mature”, “cute”, and “brilliant”, the con-
texts of their literal and sarcastic sense are quite
different, and DSMs and word embeddings are
able to capture the difference. For example, ob-
serve in the Table 3, negative sentiment words,
i.e., “sick”, “working”, “snow” are the context
words for targets “joy” and “love”, where as posi-
tive sentiment words, such as, “blessed”, “family”,
“christmas”, and “peace” are the context words for
L or Lsent senses. Overall, out of 37 targets, only
5 targets (“mature”, “joy”, “cute”, “love”, and
“yeah”) achieved “maximum” F1 scores in vari-
ous experimental settings (Tables 4 and 5) whereas
targets such as “interested”, “genius”, and “attrac-
tive” achieved low F1 scores.

In terms of variance in results, SVM results
show low SD (0-4%). For distributional ap-
proaches, SD is slightly higher (5-8%) for several
cases.

5 Related Work

Two lines of research are directly relevant to our
work: sarcasm detection in Twitter and applica-
tion of distributional semantics, such as word em-
bedding techniques to various NLP tasks. In con-
trast to current research on sarcasm and irony de-
tection (Davidov et al., 2010; Riloff et al., 2013;
Liebrecht et al., 2013; Maynard and Greenwood,
2014), we have introduced a reframing of this task
as a type of word sense disambiguation problem,
where the sense of a word is sarcastic or literal.
Our SVM baseline uses the lexical features pro-
posed in previous research on sarcasm detection
(e.g., LIWC lexicon, interjections, pragmatic fea-
tures) (Liebrecht et al., 2013; González-Ibáñez et
al., 2011; Reyes et al., 2013). Our analysis of tar-
get words where the sarcastic sense is the opposite
of the literal sense is related to the idea of “pos-
itive sentiment toward a negative situation” pro-
posed by Riloff et al. (2013) and recently studied
by Joshi et al. (2015). In our approach, we chose
distributional semantic approaches that learn con-
textual information of targets effectively from a

large corpus containing both literal and sarcastic
uses of words and show that word embedding are
highly accurate in predicting the sarcastic or lit-
eral sense of a word (Tables 4 and 5). This ap-
proach has the potential to capture more nuanced
cases of sarcasm, beyond “positive sentiment to-
wards a negative situation” (e.g., one of our target
words was “shocked” which is negative). How-
ever, our current framing is still inherently limited
to cases where sarcasm is characterized as a figure
of speech where the author means the opposite of
what she says, due to our approach of selecting the
target words.

Low-dimensional text representation, such as
WTMF, have been successful in WSD disam-
biguation research and in computing similarity be-
tween short texts (Guo and Diab, 2012a; Guo and
Diab, 2012b). word2vec and GloVe representa-
tions have provided state-of-the-art results on var-
ious word similarity and analogy detection task
(Mikolov et al., 2013c; Mikolov et al., 2013b;
Pennington et al., 2014). Word embedding based
models are also used for other NLP tasks such as
dependency parsing, semantic role labeling, POS
tagging, NER, question-answering (Bansal et al.,
2014; Collobert et al., 2011; Weston et al., 2015)
and our work on LSSD is a novel application of
word embeddings.

6 Conclusion and Future Work

We proposed a reframing of the sarcasm detec-
tion task as a type of word sense disambiguation
problem, where the sense of a word is its sarcas-
tic or literal sense. Using a crowdsourcing exper-
iment and unsupervised methods for detecting se-
mantically opposite phrases, we collected a set of
target words to be used in the LSSD task. We
compared several distributional semantics meth-
ods, and showed that using word embeddings in
a modified SVM kernel achieves the best results
(an increase of 10% F1 and 8% F1 for S vs. L
and S vs. Lsent disambiguation task, respectively,
against a SVM baseline). While the SVM base-
line preferred larger amounts of training data (best
performance achieved on the targets words with
higher number of training examples), the methods
using word embeddings seem to perform well on
target words where there might be an inherent dif-
ference in the contextual sarcastic and literal use of
a target word, even if the training data was smaller.

We want to investigate further the nature and

1010



size of training data useful for the LSSD task.
For example, to test the effect of larger training
dataset, we utilized pre-trained word vectors from
GloVe (trained with 2 Billion tweets, using 100 di-
mensions).6 For S vs. L disambiguation, the av-
erage F1 was 88.9%, which is 7% lower than the
result using GloVe on our training set of tweets
(much smaller) designed for the LSSD task. This
shows the training data utilized to create word em-
bedding models in GloVe probably do not contain
enough sarcastic tweets.

Regarding the size of the training data, recall
that the unsupervised alignment approach had ex-
tracted 70 target words (Section 2), although we
have used 37 target words as we did not have
enough training data for the remaining targets.
Thus, we plan to collect more training data for
these targets as well as more target words (espe-
cially for the S vs. Lsent task). In addition, we
plan to improve our unsupervised methods for de-
tecting semantically opposite meaning (e.g., us-
ing the IM-IM dataset in addition to the SM-IM
dataset).

One common criticism of research based on use
of hashtags as gold labels is that the training ut-
terances could be noisy. In other words, tweets
might be sarcastic but not have #sarcasm or #sar-
castic hashtags. We did a small manual validation
on a dataset of 180 tweets from the Lsent class us-
ing 3 annotators (we asked them to say whether
the tweet is sarcastic or not). For cases where all
3 coders agree none of them were considered sar-
castic, while when only 2 coders agree 1 tweet out
of 180 was considered sarcastic. In future, we plan
to perform additional experiments to study the is-
sue of noisy data. We hope that the release of our
datasets will stimulate other studies related to the
sarcasm detection problem, including addressing
the issue of noisy data.

We also plan to study the effect of hyper-
parameters in designing the DSMs. Recently,
Levy et al. (2015) have argued that parameter set-
tings have a large impact on the success of word
embedding models. We want to follow their ex-
periments to study whether parameter tuning in
PMI based disambiguation can improve its perfor-
mance.

6Downloaded from http://nlp.stanford.edu/projects/glove/

Acknowledgements

This paper is based on work supported by the
DARPA-DEFT program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. The authors thank the
anonymous reviewers for helpful comments.

References
Colin Bannard and Chris Callison-Burch. 2005. Para-

phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 597–604. As-
sociation for Computational Linguistics.

Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.

Regina Barzilay and Kathleen R McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, pages 50–57. Asso-
ciation for Computational Linguistics.

Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ’10.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42–47. Association for Computa-
tional Linguistics.

1011



Roberto González-Ibáñez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: A closer look. In ACL (Short Papers), pages
581–586. Association for Computational Linguis-
tics.

Weiwei Guo and Mona Diab. 2012a. Learning the
latent semantics of a concept from its definition.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2, pages 140–144. Association for
Computational Linguistics.

Weiwei Guo and Mona Diab. 2012b. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
864–872. Association for Computational Linguis-
tics.

Zellig S Harris. 1954. Distributional structure. Word,
10:146–162.

Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.

Aditya Joshi, Vinita Sharma, and Pushpak Bhat-
tacharyya. 2015. Harnessing context incongruity
for sarcasm detection. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), pages 757–762, Beijing,
China, July. Association for Computational Linguis-
tics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

CC Liebrecht, FA Kunneman, and APJ van den Bosch.
2013. The perfect solution for detecting sarcasm in
tweets# not.

Diana Maynard and Mark A Greenwood. 2014. Who
cares about sarcastic tweets? investigating the im-
pact of sarcasm on sentiment analysis. In Proceed-
ings of LREC.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013c. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models.

James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates, 71:2001.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.

Antonio Reyes, Paolo Rosso, and Tony Veale. 2013.
A multidimensional approach for detecting irony
in twitter. Language resources and evaluation,
47(1):239–268.

Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive senti-
ment and negative situation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 704–714. Association for
Computational Linguistics.

Simo Tchokni, Diarmuid O Séaghdha, and Daniele
Quercia. 2014. Emoticons and phrases: Status sym-
bols in social media. In Eighth International AAAI
Conference on Weblogs and Social Media.

Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete ques-
tion answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698.

1012


