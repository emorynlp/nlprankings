




































Understanding and Improving Hidden Representations for Neural Machine Translation


Proceedings of NAACL-HLT 2019, pages 466â€“477
Minneapolis, Minnesota, June 2 - June 7, 2019. cÂ©2019 Association for Computational Linguistics

466

Understanding and Improving Hidden Representation
for Neural Machine Translation

Guanlin Liï¿½, Lemao LiuÎ», Xintong Liâˆ, Conghui Zhuï¿½ âˆ— , Tiejun Zhaoï¿½, Shuming ShiÎ»
ï¿½Harbin Institute of Technology, Î» Tencent AI Lab

âˆ The Chinese University of Hong Kong
{epsilonlee.green, znculee}@gmail.com, {chzhu, tjzhao}@hit.edu.cn,

{redmondliu, shumingshi}@tencent.com

Abstract

Multilayer architectures are currently the gold
standard for large-scale neural machine trans-
lation. Existing works have explored some
methods for understanding the hidden repre-
sentations, however, they have not sought to
improve the translation quality rationally ac-
cording to their understanding. Towards un-
derstanding for performance improvement, we
first artificially construct a sequence of nested
relative tasks and measure the feature gen-
eralization ability of the learned hidden rep-
resentation over these tasks. Based on our
understanding, we then propose to regularize
the layer-wise representations with all tree-
induced tasks. To overcome the computa-
tional bottleneck resulting from the large num-
ber of regularization terms, we design efficient
approximation methods by selecting a few
coarse-to-fine tasks for regularization. Exten-
sive experiments on two widely-used datasets
demonstrate the proposed methods only lead
to small extra overheads in training but no ad-
ditional overheads in testing, and achieve con-
sistent improvements (up to +1.3 BLEU) com-
pared to the state-of-the-art translation model.

1 Introduction

Neural machine translation (NMT) has witnessed
great successes in recent years (Bahdanau et al.,
2014; Wu et al., 2016). Current state-of-the-art
(SOTA) NMT models are mainly constructed by a
stacked neural architecture consisting of multiple
hidden layers from bottom-up, where a classifier is
built upon the topmost layer to solve the target task
of translation (Gehring et al., 2017; Vaswani et al.,
2017). Most works tend to focus on the transla-
tion performance of the classifier defined on the
topmost layer, however, they do not deeply under-
stand the learned representations of hidden layers.
Shi et al. (2016) and Belinkov et al. (2017) attempt

âˆ— Conghui Zhu is the corresponding author.

root

0 1

00 01 10 11

000 001 010 011 110 111

x, yğ‘¡<

yğ‘¡

is â€¦

1100 11010100 01110000 0001
apple â€¦ cola â€¦

rice â€¦ cat â€¦

dog â€¦ wolf â€¦

run â€¦

of â€¦ in â€¦

1 1

2

3

4

2

3

4

layer depth task

HR

Structural HR

Figure 1: The structural hierarchical regularization
framework. On the left is a 4-layer NMT decoder; on
the right is a hierarchical clustering tree and the tree-
induced relative tasks at every tree depth.

to understand the hidden representations through
the lens of a few linguistic tasks, while Ding et al.
(2017) and Strobelt et al. (2018) propose appeal-
ing visualization approaches to understand NMT
models including the representation of hidden lay-
ers. However, employing the analyses to moti-
vate new methods for better translation, the ulti-
mate goal of understanding NMT, is not achieved
in these works.

In our paper, we aim at understanding the hid-
den representation of NMT from an alternative
viewpoint, and particularly we propose simple yet
effective methods to improve the translation per-
formance based on our understanding. We start
from a fundamental question: what are the char-
acteristics of the hidden representation for bet-
ter translation modeling? Inspired by the lessons
from transfer learning (Yosinski et al., 2014), we
propose to empirically verify the argument: good
hidden representation for a target task should be
able to generalize well across any similar tasks.
Unlike Shi et al. (2016) and Belinkov et al. (2017)
who employ one or two linguistic tasks involv-
ing human annotated data to evaluate the feature
generalization ability of the hidden representation,



467

which might make understanding bias to a specific
task, we instead construct a nested sequence of
many relative tasks with entailment structure in-
duced by a hierarchical clustering tree over the
output label space (target vocabulary). Each task is
defined as predicting the cluster of the next token
according to a given source sentence and its trans-
lation prefix. Similar to Yu et al. (2018), Zamir
et al. (2018) and Belinkov et al. (2017), we mea-
sure the feature generalization ability of the hidden
representation regarding each task. Our observa-
tions are (Â§2):

â€¢ The hidden representations learned by NMT
indeed has decent feature generalization abil-
ity for the tree-induced relative tasks com-
pared to the randomly initialized NMT model
and a strong baseline with lexical features.

â€¢ The hidden representations from the higher
layers generalize better across tasks than
those from the lower layers. And more simi-
lar tasks have closer performances.

Based on the above findings, we decide to regu-
larize and improve the hidden representations of
NMT for better predictive performances regarding
those relative tasks, in hope of achieving improved
performance in terms of the target translation task.
One natural solution is to feed all relative tasks to
every hidden layer of the NMT decoder under the
framework of multi-task learning. This may make
the full coverage of the potential regularization ef-
fect. Unfortunately, this vanilla method is ineffi-
cient in training because there are more than one
hundred task-layer combinations. 1 Based on the
second finding, to approximate the vanilla method,
we instead feed a single relative task to each hid-
den layer as a regularization auxiliary in a coarse-
to-fine manner (Â§3.1). Furthermore, we design an-
other regularization criterion to encourage predic-
tive decision consistency between a pair of adja-
cent hidden layers, which leads to better approxi-
mated regularization effect (Â§3.2). Our method is
simple to implement and efficient for training and
testing. Figure 1 illustrates the representation reg-
ularization framework.

To summarize, our contributions are as follows:

â€¢ We propose an approach to understand hid-
den representation of multilayer NMT by

1There are about 22 tasks that we have constructed and 6
layers in SOTA NMT models (Vaswani et al., 2017).

measuring their feature generalization ability
across relative tasks constructed by a hierar-
chical clustering tree.

â€¢ We propose two simple yet effective meth-
ods to regularize the hidden representation.
These two methods serve as trade-offs be-
tween regularization coverage and efficiency
with respect to the tree-induced tasks.

â€¢ We conduct experiments on two widely used
datasets and obtain consistent improvements
(up to +1.3 BLEU) over the current SOTA
Transformer (Vaswani et al., 2017) model.

2 Understanding Hidden Representation

In this section, we first introduce some back-
ground knowledge and notations of the multilayer
NMT model. Then, we present a simple approach
to better understand hidden representation through
transfer learning. By analyzing the feature gener-
alization ability, we draw some constructive con-
clusions which are used for designing regulariza-
tion methods in Section 3.

2.1 Background and Notations
Suppose x = ã€ˆx1, Â· Â· Â· , x|x|ã€‰ is a source sentence,
i.e. a sequence of source tokens, and a target
sentence y = ã€ˆy1, Â· Â· Â· , y|y|ã€‰ is a translation of x,
where each yt in y belongs to Y , the target vocabu-
lary. A translation model minimizes the following
chain-rule factorized negative log-likelihood loss:

`mle = âˆ’ logP (y | x; Î¸)

= âˆ’
âˆ‘
t

logP (yt | x, y<t; Î¸),
(1)

where Î¸ denotes the overall parameter of the trans-
lation model. According to Eq.(1), an alterna-
tive view of the translation problem can be cast to
token-level stepwise classification (DaumeÌ et al.,
2009): predict the target token yt given a context
consisting of x and y<t = ã€ˆy1, Â· Â· Â· , ytâˆ’1ã€‰ corre-
sponding to each factor P (yt | x, y<t; Î¸).

The SOTA multilayer NMT models parameter-
ize P (yt | x, y<t; Î¸) via powerful multilayer en-
coder and stacked layers of feature transforma-
tions ã€ˆh1, Â· Â· Â· , hLã€‰ at the decoder side:

P (yt | x, y<t; Î¸) = P (yt | x, y<t, hL; Î¸), (2)

where hl(x, y<t) = Ï†
l
(
x, y<t;h

lâˆ’1; Î¸
)

is the lth

hidden layer recursively defined by Ï†l on hlâˆ’1.



468

We also use hl(x, y<t) to represent the output hid-
den representation of layer l for a specific context.
Note that, Ï†l bears several types of instantiation
and is an active area of research (Wu et al., 2016;
Gehring et al., 2017; Vaswani et al., 2017).

2.2 Feature Generalization Ability of Hidden
Representations

Inspired by feature transfer learning (Yosinski
et al., 2014), we attempt to understand hidden rep-
resentations of NMT by evaluating their general-
ization abilities across any tasks that are related to
translation. There are some researchers who study
hidden representations of NMT by using linguistic
tasks such as morphology, named entity, part-or-
speech or syntax (Shi et al., 2016; Belinkov et al.,
2017, 2018). They typically rely on human anno-
tated resources to train a model for each linguis-
tic task, so their methods can not be used for lan-
guages which lack human annotations. Moreover,
their considered tasks are too few to have a good
coverage over task space for measuring transfer-
ability (Yu et al., 2018; Zamir et al., 2018), and
their understanding results may bias to a specific
task. As a result, to evaluate the feature gener-
alization ability of hidden representation, we arti-
ficially construct plenty of relative tasks which do
not employ any human annotation. This makes our
evaluation approach more general.

Definition of the relative tasks Suppose Yk de-
notes any partition (or clustering) regarding the
output label space (target vocabulary) Y . That is,
Yk is a set of subsets Yki âŠ‚ Y where i = 1...|Yk|,
such that âˆ€i, j,Yki âˆ© Ykj = âˆ… and âˆªiYki = Y . We
define the following relative task: given a context
ã€ˆx, y<tã€‰, predict the subset or the cluster to which
the tth token yt belongs in Yk, denoted as Yk(yt).
To simplify notation, we regard Yk both as a rela-
tive task and as a partition.

It is clear that the above type of tasks are sim-
ilar to the task of translation according to the de-
scription in Section Â§2.1. Furthermore, different k
represents different relative task and thus we ac-
tually obtain a great many relative tasks in total.
However, it is impossible to evaluate the hidden
representation on all those tasks; moreover, due to
relationship between tokens (Hu et al., 2016) in
Y , not all partitions are reasonable. As a conse-
quence, motivated by the analysis of VC Dimen-
sion (Vapnik, 1995), we construct a sequence of

nested partitions with an entailment structure: 2

Y1 ï¿½ Â· Â· Â· ï¿½ YK . The benefit is that a spectrum
of task hardness can be constructed due to the in-
creased partition or task cardinalities.

As a matter of fact, we instantiate the above
nested partitions through brown clustering (Brown
et al., 1992; Stratos et al., 2014) over Y to get a hi-
erarchical clustering tree and then consider each
tree depth along the tree as a partition represent-
ing a relative task Yk (as shown in Figure 1). In
the following experiments, we run brown cluster-
ing algorithm over a Châ‡’En dataset (Â§4) and con-
struct a tree of English with depth 21. Without loss
of generality, we regard the task Y22 at a virtual
22 depth of the tree as equivalent to the transla-
tion task Y . Actually, Y and Y22 have the same
cardinality but are different in definition. 3

Evaluating generalization We use multi-class
logistic regression to fit the layer-wise hidden
representation learned by a well-trained 6-layer
Transformer (Vaswani et al., 2017) over each
training instance ã€ˆx, y<tã€‰. Specifically, given a
context ã€ˆx, y<tã€‰, for each task Yk and a hidden
representation hl(x, y<t) of this context, which is
fixed as constant, we predict the cluster Yk(yt) ac-
cording to the following probability:

P
(
Yk(yt) | hl(x, y<t); Î¸lYk

)
, (3)

where Î¸lYk is the parameter of the logistic regres-
sion model for task Yk at lth layer. The difference
between Eq.(3) and Eq.(2) is that the former is the
linear model parameterized by Î¸lYk while the latter
is the NMT model parameterized by Î¸.

Since there are L = 6 layers in Transformerâ€™s
decoder and K = 22 relative tasks, we have more
than one hundred such linear models defined with
Eq. (3) in total. Therefore, it is costly to train
them independently. Since the loss for each linear
model is convex, joint training leads to exactly the
same optimum as independent training and thus
we employ mini-batch stochastic gradient descent
to minimize the joint loss as follows:

âˆ’
âˆ‘
k

âˆ‘
l

âˆ‘
t

logP
(
Yk(yt) | hl(x, y<t); Î¸lYk

)
.

(4)

2Here what we mean entailment relation (ï¿½) between two
partitions Yk and Yk+1 is: âˆ€Yk+1i , âˆƒ!Y

k
j , s.t.Yk+1i âŠ† Y

k
j .

3 Please refer to Appendix A for detailed preprocessing of
the tree to get nested partitions.



469

Figure 2: The transfer learning performances.

After training, we fix each Î¸lYk and then measure
the feature generalization ability of each hl by val-
idating on the task Yk regarding a heldout dataset,
following Yu et al. (2018). For validation, we
report accuracy on a heldout dataset through the
strategy of maximum a posteriori (MAP). 4

Analysis To figure out how good the learned
hidden representations are, we consider two base-
lines to extract features regarding each context
ã€ˆx, y<tã€‰ to train logistic regression models for
comparison. For the first baseline, the features of
the context are the hidden representations from the
last layer of a randomly initialized Transformer;
for the second, the features are derived by lexi-
cal feature templates, which include the source-
side bag-of-words (BOW) features and target-side
BOW features indexed by relative positions of ytâ€™s
previous with up to m (Markov length) tokens. 5

As shown in Figure 2, the lexical baseline deliv-
ers comparable accuracies for fine-grained tasks
with respect to well learned Transformerâ€™s first
layer, thanks to its discriminant ability with abun-
dant lexical features. For example, its accuracy
reaches about 26% for the task with cardinality
|Y21|. The random baseline performs worse for
tasks with cardinality |Y8|, which indicates that
random representations in NMT have limited gen-
eralization abilities to fine-grained tasks as ex-
pected. The well-trained low-layer hidden repre-
sentations yield much higher accuracies than the
random baseline and are even better than the lexi-
cal baseline. This shows that the hidden represen-
tations from a well-trained NMT have good gener-
alization abilities across relative tasks. In addition,
as the layer goes up, the performance of hidden
representations increase significantly over differ-

4The accuracy is measured by whether
argmaxzâˆˆYk P (z | hl(x, y<t); Î¸

l
Yk )) = Y

k(yt).
5Please refer to Appendix B for more details are.

ent relative tasks, which clearly demonstrates that
more complex neural architecture leads to stronger
expressibility. This provides a quantitative evi-
dence to support the statement in Bengio et al.
(2009), Goodfellow et al. (2016).

3 Structural Hierarchical Regularization

In this section, we propose two simple methods,
which respect the above findings, to enhance the
hidden representations in NMT such that they gen-
eralize well across those relative tasks.

3.1 Hierarchical Regularization

A natural method to improve feature generaliza-
tion of hidden representation is to jointly train the
target task with all relative tasks for all hidden
layers, which we call full-coverage method. As
mentioned in Section Â§2.2, this method will lead
to training more than one hundred tasks (K Ã— L)
in total, where K denotes the depth of the hierar-
chical clustering tree (aka. the number of tasks)
and L the number of hidden layers. Unfortu-
nately, since each task involves a softmax opera-
tion which may be the computation bottleneck for
the task Yk with large cardinality, this method is
inefficient for training.

As a solution to approximate the potential regu-
larization effect of the full-coverage method, we
confine each hidden layer to engage in a single
relative task. Motivated by the observation that
representations from higher layers have better ex-
pressibility than lower layers, as claimed in Â§2.2,
we instead employ a coarse-to-fine strategy to se-
lect one task for each layer: finer-grained tasks for
higher layers while coarser-grained task for lower
layers. Specifically, suppose 1 â‰¤ s(l) â‰¤ K is
the selected index regarding task Ys(l) for the lth
layer, then it subjects to s(l) < s(l + 1) for each
l. In addition, to encourage the diversity among
the selected L tasks, we require s(l + 1)âˆ’ s(l) to
be large enough for all l. Formally, the loss of the
hierarchical regularization (HR) method is:

`hr = âˆ’
âˆ‘
l

âˆ‘
t

logP (Ys(l)(yt) | x, y<t, hl; Î¸, Î¸lYs(l)), (5)

where P (Ys(l)(yt) | xj , y
j
<t, h

l; Î¸, Î¸lYs(l)) is simi-
lar to Eq. 3 except that it treats the parameters Î¸ in
NMT as parameters besides Î¸lYs(l) . Compared to
Eq. 4, it includes fewer terms for summation.



470

1 2 3 4 5 6 7 8ğ‘
ğ‘™+1

PROJ ğ‘ğ‘™+1

ğ‘ğ‘™

1-3 4-6 7-8

1 2 3

KL âˆ™ || âˆ™

0.35

0.45

0.05

0.20

0.1

0.2

0.32

0.40

0.28

0.24

0.11
0.1

0.05
0.15

Figure 3: The conceptual graph of the KL consistency
loss in SHR. Here, the multinomial probability vector
pl is calculated through P (Â· | x, y<t, hl; Î¸, Î¸lYs(l)).

3.2 Structural Hierarchical Regularization

The HR method is very simple and computation-
ally efficient, however, using one task to regular-
ize a layer may not be a good approximation of
the full-coverage method, since HR method might
lead to inconsistent decisions for two different lay-
ers, which is formalized through the following en-
tailment structure as introduced in Section 2.2:

arg max
zâˆˆYs(l1)

P (z | x, y<t, hl1 ; Î¸, Î¸
l1
Ys(l1)) âŠƒ

arg max
zâˆˆYs(l2)

P (z | x, y<t, hl2 ; Î¸, Î¸
l2
Ys(l2)), (6)

where s(l) is the selected task for the lth layer by
HR, 1 â‰¤ l1 < l2 â‰¤ L and P (z|x, y<t, hl; Î¸, Î¸lYs(l))
is similar to Eq. (3) for the task Ys(l) and lth layer
except that it does not treat the NMT parameters
Î¸ as constant. However, it always occurs on the
training data that Ys(l1)(yt) âŠ‚ Ys(l2)(yt).

To alleviate this inconsistency issue for bet-
ter approximating the full-coverage method, we
leverage the above structural property by adding
another regularization term. Firstly, we project
the distribution P (Â·|x, y<t, hl; Î¸, Î¸lYs(l)) into the
domain of Ys(lâˆ’1). Then we calculate KL di-
vergence between the projected distribution and
P (Â·|x, y<t, hlâˆ’1; Î¸, Î¸lâˆ’1Ys(lâˆ’1)). Figure 3 illustrates
the idea. Since it is inefficient to consider all pairs
of l1 and l2, so we instead consider the consistency
between all adjacent layers. Formally, we obtain

Method # Param. SpeedZhâ‡’En Enâ‡’De Train Test
Baseline 90.2M 88.0M 1.82

1.33
FHR +60.9M +63.2M 0.72
HR +3.2M +3.1M 1.65

SHR +3.2M +3.1M 1.40

Table 1: Train and test efficiency comparison, mea-
sured by steps/s (second) and sentences/s respectively.

the following loss function:

`shr = `hr +
1

Lâˆ’ 1
âˆ‘
l

KL
(
P (Â·|x, y<t, hl; Î¸,

Î¸lYs(l)) || PROJ
[
P (Â·|x, y<t, hl+1; Î¸, Î¸l+1Ys(l+1))

])
,

(7)

where PROJ is the projection defined in Figure 3,
and other notations are defined as before.

We call the above regularization as structural hi-
erarchical regularization (SHR) since it takes ad-
vantage of the structure of the tree. In our ex-
periments, we add HR (Eq.(5)) and SHR (Eq.(7))
losses respectively into the negative log-likelihood
regarding Eq. (1) for training all parameters Î¸ and
Î¸lYs(l) . One of our advantage is that we only use
Î¸ for testing and thus our testing is as efficient as
that for the baseline NMT model.

4 Experiments

4.1 Setup
We conduct experiments on two widely-used cor-
pora. We choose from the LDC corpora about
1.8M sentence pairs for Zhâ‡’En translation with
word-level vocabulary of 30k for both languages.
We use the WMT14 Enâ‡’De task which consists
4.5M sentence pairs and the vocabulary is built by
joint BPE with 32k merging operations. Besides
the baseline, we also conduct experiments on 3
regularization variants:

â€¢ Baseline: the Transformer base model pro-
posed in Vaswani et al. (2017).

â€¢ FHR: fine-grained HR based Transformer,
which adopts the original label space as task
for all selected layers for regularization. This
variant is used to demonstrate that low layers
which are weak in expressibility can mess up
hard tasks which are unsuitable to learn.

â€¢ HR and SHR: as proposed in Section 3.



471

Method MT02 MT03 MT04 MT05 MT06 MT08 Avg.
Zhao et al. (2018) N/A 44.98 45.51 43.93 43.95 33.33 42.34

Baseline 46.08 44.09 46.50 44.45 45.26 37.10 43.48
FHR 45.46 43.56 47.51 44.00 45.45 37.22 43.58
HR 46.28 44.04 47.80 44.56 45.56 38.17 44.08

SHR 47.05 44.80 48.15 45.55 46.30 39.02 44.78

Table 2: BLEU comparison on the LDC dataset.

Choice of relative tasks Based on the heuris-
tics in Section 3.1, we first choose the task with
the largest cardinality from the hierarchical clus-
tering tree without the virtual depth, because this
task is most related to translation (close cardinal-
ities). Then we balance task diversity through a
5 times cardinality difference between tasks from
the previous chosen task. As a result, we can ob-
tain 4 tasks with s(l) = 5, 8, 11, 20 for the Zhâ‡’En
task and s(l) = 5, 7, 10, 21 for the Enâ‡’De task,
where l = 2, 3, 4, 5 of the 6-layer decoder. 6

4.2 Efficiency Comparison

Table 1 summarizes the total number of param-
eters for the baseline and 3 regularization vari-
ants. As in Eq. (5), HR introduces extra param-
eters compared with the baseline. Besides, calcu-
lating the second term in Eq. (7) requires modest
overheads. Therefore, training our SHR is slower
than training the baseline. Although the proposed
HR and SHR introduce extra parameters during
training, they do not involve them during testing
and thus testing is as efficient as the baseline.

4.3 Translation Quality on Zhâ‡’En Dataset
Table 2 shows the evaluation results of the base-
line and 3 regularization variants on the Zhâ‡’En
dataset. Since there are no recent work report-
ing Transformerâ€™s performance on this dataset, we
choose a recurrent SOTA model to show that our
baseline is already better than it, which is a com-
mon knowledge that Transformer can outperform
recurrent NMT models. Our HR method sur-
passes the baseline 0.6 BLEU point, while the
SHR method can improve upon HR by about a
further 0.8 point, namely about 1.4 points over the
baseline. Interestingly, the FHR method only per-
forms on par with baseline, which indicates that
forcing low layers to learn fine-grained tasks will
not lead to beneficial intermediate representations
since they struggle to learn a well-structured rep-

6Please refer to Appendix C for detailed information.

resentation space. This matches the finding in Sec-
tion 2: low layers may not be expressible enough
to perform well on tasks with large cardinalities.

4.4 Analyses on Zhâ‡’En Dataset
In the following, we conduct several quantita-
tive experiments to demonstrate the advantages of
our proposed two regularization methods over the
baseline. Note that, since we need to guarantee
that the decoded sequence has the same length
with the reference for one-by-one token compar-
ison, the following experiments are all conducted
with teacher forcing and greedy decoding.

4.4.1 Better Feature Generalization Ability
In the same manner as Section 2, we learn soft-
max weights for all relative tasks by fixing model
weights learned by HR and SHR methods. Fig-
ure 4(a), (b) show the âˆ† feature generalization
ability (absolute accuracy difference) of HR and
SHR over baseline. Since layer 1 is not selected
as the regularized layer, no significant gap is ob-
served. However, since layer 1 is close to the
loss directly imposed on layer 2, improvements
about 5% and 8% are obtained. Since in the base-
line, layer 5, 6 are already close or with the ul-
timate fine-grained loss, HR method shows very
small gain. But our SHR method can still improve
about 4% absolute points. Except for layer 1, it
is also evident to see larger gaps (more than 20%)
at lower layers than higher layers due to the fact
that lower layers, which are distant from the top-
most loss in the baseline, require more supervision
signals to shape their latent representation space.

4.4.2 Improved Decision Consistency
We measure decision consistency for a specific
layer and decision consistency between a pair of
layers using two metrics. The first metric is mea-
sured by conditional accuracy, which is the possi-
bilities of the classifier parameterized by Î¸lYk cor-
rectly predicting Yk(yt) if the classifier parame-
terized by Î¸lYkâ€² correctly predicts Y

kâ€²(yt) for any



472

(a) HR vs Baseline (b) SHR vs Baseline

(c) HR vs Baseline (d) SHR vs Baseline

Figure 4: (a), (b) are the âˆ† feature generalization ability (absolute accuracy difference) of HR and SHR method
compared to baseline; (c), (d) are the conditional absolute accuracy difference of HR and SHR over baseline.

kâ€² < k. The second metric is measured by the
counts of consistent decision pairs between any
pair of regularized layers as defined in Eq. (6).

Figure 4(c), (d) shows the absolute conditional
accuracy difference of our HR and SHR over base-
line. In accordance with the observations in pre-
vious subsection, except for layer 1, other layers
show significant gains (HR more than 7%, SHR
more than 10%) over baseline. Decision consis-
tency for each layer proves the well-shaped layer-
wise representation and potentially paves the way
for better inter-layer decision consistency.

(a) (b) (c)

Figure 5: The consistency correlation between different
regularized layers (lyr.2 to lyr.6) of the baseline and our
two regularization methods.

Figure 5 illustrates the consistency counts be-
tween any regularized layer pairs, including those
without KL-based regularization. Deeper color
represents more consistency counts. It is evident

that the baseline has a very poor consistency be-
tween any layers. Our HR method is almost 2
times better, and the SHR obtains further improve-
ment. A better decision consistency can couple the
decision between relative tasks, so that by reach-
ing a high accuracy on easier tasks can benefit
the harder ones. Another interesting observation
is that non-adjacent layers without KL loss also
obtain significant improvements on decision con-
sistency, because the KL term is actually transitive
between layers where the predictive distributions
are in accordance with the tree structure.

Figure 6: Development set accuracy among the base-
line and our proposed regularization variants over dif-
ferent word frequency bins.



473

4.4.3 Promoted Low-Frequency Word
Performance

In this subsection, we clarify that the coarse-to-
fine regularized representations can also benefit
low-frequency words. We divide the vocabulary
into ten equally-sized bins, and summarize token
accuracy for each bin over the development set.
As shown in Figure 6, the x-axis represents the
frequency spectra, that is, we sort the bins by
word frequency from rank 1 (the most frequent
words) to 10 (the rare words). We can see that
both HR and SHR methods demonstrate a gradu-
ally increased gap over the baseline as the word
frequency decreases, which means our methods
become better for less frequent word bins. How-
ever the gap shrinks at the 10th bin. This may be
the fact that for those words that appears with less
than 50 counts, both methods are helpless.

For baseline, it is hard to train well-shaped hid-
den representations for low-frequent words; in ad-
dition, due to the distance between the loss and the
low layers, it is also hard to train weights due to the
unstable gradient signal. By adding our regular-
ization terms, every level of the multilayer decoder
will receive supervision signals directly and lower
layers will receive coarser grained thus higher fre-
quency signals to shape their representations.

4.5 Translation Quality on Enâ‡’De Dataset
Table 3 shows the evaluation results of the baseline
and the 3 regularization variants on the Enâ‡’De
dataset. Notice that we use the base model while
Chen et al. (2018) and Ott et al. (2018) use big
models. The FHR method still does not show
significant improvement over the baseline (less
than 0.2 BLEU point), which verifies the hypoth-
esis that we make by analyzing the Zhâ‡’En re-
sults. Our HR method is already stronger than
Chen et al. (2018) which uses a multilayer RNN
as decoder. Compared to the current state-of-the-
art in Ott et al. (2018) who utilize huge batch
size and over 100 GPUs on the Transformer big
model, our SHR method can be on par with them.
This comparison indicates that better regularized
hidden representations can be potentially power-
ful than increasing model capacity when using the
same optimization method.

5 Related Work

Since the dawn of NMT, many works have pro-
posed for understanding what has been encoded

Method MT13 MT14
Chen et al. (2018) N/A 28.49
Ott et al. (2018) 26.70 29.30

Baseline 25.99 27.75
FHR 26.10 27.91
HR 26.48 28.87

SHR 26.64 29.18

Table 3: BLEU comparison on the WMT14 dataset.
Here MT13 and MT14 denote newstest2013 and new-
stest2014, which are used as development and test set
respectively.

in the learned hidden representations. Shi et al.
(2016) are the first to investigate source syntax
encoded in source hidden representations. Sim-
ilarly, Belinkov et al. (2017) and Belinkov et al.
(2018) give detailed analyses of both encoder and
decoderâ€™s learned knowledge about part-of-speech
and semantic tags at different layers. Unlike those
works that employ one or two linguistic tasks, we
instead construct plenty of artificial tasks without
any human annotations to analyze the hidden rep-
resentations. This makes our approach more gen-
eral and may potentially lead to less biased con-
clusions.

Based on our understanding of the hidden rep-
resentations, we further develop simple methods
to improve NMT through representation regular-
ization. Many works regularize NMT with lexical
knowledge such as BOW (Weng et al., 2017) and
morphology (Niehues and Cho, 2017; Zaremoodi
et al., 2018), or syntactic knowledge (Kiperwasser
and Ballesteros, 2018; Eriguchi et al., 2017). One
significant difference is that we take into account
the structure among plenty of artificial tasks and
design a well motivated regularization term to en-
courage the structural consistency of tasks, which
further improves NMT performance. In addition,
our coarse-to-fine way to select tasks for regular-
ization is also inspired by recent works using a
coarse-to-fine mechanism for learning better word
embeddings in NMT (Zhang et al., 2018) and pre-
dicting intermediate solutions for semantic pars-
ing (Dong and Lapata, 2018).

6 Conclusion

In this work, we present a simple approach for bet-
ter understanding NMT learned layer-wise repre-
sentations with transfer learning over plenty of ar-
tificially constructed relative tasks. This approach



474

is general as it requires no human annotated data,
only demanding target monolingual corpus. Based
on our understanding, we propose two efficient yet
effective methods for representation regularization
which further pushes forward the SOTA NMT per-
formances. In the future, we want to dig deeply
into the subspace regularities of the learned repre-
sentations for more fine-grained understanding.

Acknowledgements

The authors would like to first thank all the
anonymous reviewers for their critical suggestion
and valuable experimental advice. The authors
would also like to thank Yong Jiang for discus-
sion; Shangchen Zhou for better figure design;
Haozhe Xie, Chaoqun Duan, Xin Li, Ziyi Dou and
Mengzhou Xia for proof reading. Tiejun Zhao is
supported by National Key RD Program of China
Project 2017YFB1002102.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan
Sajjad, and James Glass. 2017. What do neural ma-
chine translation models learn about morphology?
arXiv preprint arXiv:1704.03471.

Yonatan Belinkov, LluÄ±Ìs MaÌ€rquez, Hassan Sajjad,
Nadir Durrani, Fahim Dalvi, and James Glass. 2018.
Evaluating layers of representation in neural ma-
chine translation on part-of-speech and semantic
tagging tasks. arXiv preprint arXiv:1801.07772.

Yoshua Bengio et al. 2009. Learning deep architec-
tures for ai. Foundations and trends RÂ© in Machine
Learning, 2(1):1â€“127.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467â€“479.

Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin
Johnson, Wolfgang Macherey, George Foster, Llion
Jones, Niki Parmar, Mike Schuster, Zhifeng Chen,
et al. 2018. The best of both worlds: Combining re-
cent advances in neural machine translation. arXiv
preprint arXiv:1804.09849.

Hal DaumeÌ, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine learn-
ing, 75(3):297â€“325.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Åukasz Kaiser. 2018. Univer-
sal transformers. arXiv preprint arXiv:1807.03819.

Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Visualizing and understanding neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1150â€“
1159. Association for Computational Linguistics.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. arXiv preprint
arXiv:1805.04793.

Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun
Cho. 2017. Learning to parse and translate im-
proves neural machine translation. arXiv preprint
arXiv:1702.03525.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N Dauphin. 2017. Convolu-
tional sequence to sequence learning. arXiv preprint
arXiv:1705.03122.

Ian Goodfellow, Yoshua Bengio, Aaron Courville, and
Yoshua Bengio. 2016. Deep learning, volume 1.
MIT press Cambridge.

Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng,
Zicheng Liao, and Greg Mori. 2016. Learning struc-
tured inference neural networks with label relations.
In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 2960â€“
2968.

StanisÅ‚aw Jastrzebski, Devansh Arpit, Nicolas Ballas,
Vikas Verma, Tong Che, and Yoshua Bengio. 2017.
Residual connections encourage iterative inference.
arXiv preprint arXiv:1710.04773.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Eliyahu Kiperwasser and Miguel Ballesteros. 2018.
Scheduled multi-task learning: From syntax to
translation. arXiv preprint arXiv:1804.08915.

Marc T Law, Jake Snell, Amir-massoud Farahmand,
Raquel Urtasun, and Richard S Zemel. 2018. Di-
mensionality reduction for representing the knowl-
edge of probabilistic models.

Jan Niehues and Eunah Cho. 2017. Exploiting
linguistic resources for neural machine transla-
tion using multi-task learning. arXiv preprint
arXiv:1708.00993.

Myle Ott, Sergey Edunov, David Grangier, and
Michael Auli. 2018. Scaling neural machine trans-
lation. In Proceedings of the Third Conference on
Machine Translation, pages 1â€“9, Belgium, Brussels.
Association for Computational Linguistics.



475

Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1526â€“
1534.

Karl Stratos, Do-kyum Kim, Michael Collins, and
Daniel J Hsu. 2014. A spectral algorithm for learn-
ing class-based n-gram models of natural language.
In UAI, pages 762â€“771. Citeseer.

H. Strobelt, S. Gehrmann, M. Behrisch, A. Perer,
H. Pfister, and A. M. Rush. 2018. Seq2Seq-Vis:
A Visual Debugging Tool for Sequence-to-Sequence
Models. ArXiv e-prints.

Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin, Heidel-
berg.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998â€“6008.

Rongxiang Weng, Shujian Huang, Zaixiang Zheng,
Xinyu Dai, and Jiajun Chen. 2017. Neural machine
translation with word predictions. arXiv preprint
arXiv:1708.01771.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Googleâ€™s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Advances in neural information
processing systems, pages 3320â€“3328.

Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni
Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,
and Bowen Zhou. 2018. Diverse few-shot text clas-
sification with multiple metrics. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 1206â€“1215. Association for Computa-
tional Linguistics.

Amir R Zamir, Alexander Sax, , William B Shen,
Leonidas Guibas, Jitendra Malik, and Silvio
Savarese. 2018. Taskonomy: Disentangling task
transfer learning. In 2018 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).
IEEE.

Poorya Zaremoodi, Wray Buntine, and Gholamreza
Haffari. 2018. Adaptive knowledge sharing in
multi-task learning: Improving low-resource neural
machine translation. In Proceedings of the 56th An-
nual Meeting of the Association for Computational

Linguistics (Volume 2: Short Papers), volume 2,
pages 656â€“661.

Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, and En-
hong Chen. 2018. Coarse-to-fine learning for neural
machine translation. In CCF International Confer-
ence on Natural Language Processing and Chinese
Computing, pages 316â€“328. Springer.

Yang Zhao, Jiajun Zhang, Zhongjun He, Chengqing
Zong, and Hua Wu. 2018. Addressing troublesome
words in neural machine translation. In Proceedings
of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 391â€“400. As-
sociation for Computational Linguistics.

A Hierarchical Clustering Tree Induced
Relative Tasks

In this appendix, we demonstrate a more detailed
introduction to the partition of Y , the hierarchi-
cal clustering tree constructed by Y and the tree-
induced relative tasks, which are introduced in
Section 2.2, through an example.

Suppose our vocabulary Y only consists of a
few words, that is, Y = {cat, dog, run, jump, is}.
Any partition of Y denoted as Yk is a set of sub-
sets of Y . As shown in Figure 7, this partition is
actually { { cat, dog }, { jump, run }, { is } }.

cat

dog
run

jump

is

Figure 7: One example partition of the vocabulary Y .

Then suppose the constructed hierarchical clus-
tering tree looks like the tree in Figure 8(a). In
this tree, not all the leaves are at the same depth,
so we left-branches the leaves with {cat, dog} at
depth 1 to stretch to depth 2. Then by adding a
virtual level at depth 3, we can construct a relative
task with the same cardinality as the fine-grained
translation task Y , as shown in Figure 9(b).

B Lexical Feature-based Baseline

In this appendix, we describe the lexical feature-
based baseline that we use in Section 2.2 to com-
pare with the layer-wise representations learned
by Transformer. There are two types of feature
template: a) the source-side bag-of-word (BOW)
features; and b) the target-side order-aware BOW
features. Specifically, given a context ã€ˆx, y<tã€‰, we
extract features according to the above templates:



476

{cat, dog}

{run, jump}
Left branching

{is}

{cat, dog} {run, jump} {is}

(a)

3

2

1

depthtask

{cat} {dog} {run} {jump} {is}

{cat, dog} {run, jump} {is}

{run, jump, is}{cat, dog}

{cat, dog, run, jump, is}

={ }

={ }

={ }

(b)

Figure 8: (a) The original hierarchical clustering tree,
with left branching to have all leaves at the same tree
depth; (b) The hierarchical clustering tree with depth 2,
and a virtual task is constructed at depth 3.

â€¢ the BOW representation of the source sen-
tence x: that is, if the source vocabulary is X ,
the source BOW feature vector has length of
|X |, with each entry the appearances of that
token in x.

â€¢ the order-aware BOW representation within
k-Markov dependency chain: that is, we ex-
tract feature from ytâˆ’k:tâˆ’1 by considering
both the token identity and the relative dis-
tance of that token to the predicted one yt.
This feature template constructs a feature
vector of k Ã— |Y| entries (Y the target vocab-
ulary), with each entry set to 1 when appears
in the chain, otherwise 0.

â€¢ the order-unaware BOW representation out-
side the k-Markov dependency chain, that
is, we extract feature from the y0:tâˆ’kâˆ’1 with
the same philosophy of the source-side BOW
feature and obtain a feature vector of size Y .

So in total, the feature vector extracted from the
context ã€ˆx, y<tã€‰ has a size (|X |+ k Ã— |Y|), which
will be around 300k if the vocabulary size is
around 30k and the Markov order k = 10.

C Detailed Experiment Information

In this appendix, we describe the detailed experi-
ment information: the construction of the hierar-

(a) Task cardinalities for English (Zhâ‡’En).

(b) Task cardinalities for German (Enâ‡’De).

Figure 9: Task cardinalities.

chical clustering tree (Brown et al., 1992; Stratos
et al., 2014), the model configuration, and the
training details.

We construct two hierarchical clustering trees
for the two target languages, English (Zhâ‡’En)
and German (Enâ‡’De) in our experiments, with
Percy Liangâ€™s Brown Clustering algorithm imple-
mentation. 7 In both languages, we set the num-
ber of clusters to 5000 (a hyperparameter in the
algorithm, c = 5000), that is, we will obtain trees
with 5000 leaves. We set c to 5000 since the total
vocabulary of the two languages are around 30k,
and 5000 clusters will get a token coverage of 6
(30k/5000=6) for each leave if the clusters are bal-
anced. By using left-branching introduced in Ap-
pendix A, we can finally get two trees with every
depths as a relative task. The statistics of each rel-
ative taskâ€™s cardinality for each tree are shown in
Figure 9(a) and (b) respectively. For selecting the
depths in a coarse-to-fine manner, we follow the
heuristics mentioned in Section 3.1, that is, we se-
lect depths which are diverse enough so as to have
better coverage of all the relative tasks. Specifi-
cally, we follow a quotient between two adjacent
selected tasks of 5, and select from the task which
has the cardinality of 5000, then we select tasks
of cardinalities around 1000, 200, 40 respectively.

7https://github.com/percyliang/brown-cluster.



477

Method MT02 MT03 MT04 MT05 MT06 MT08 Avg.
Baseline 46.08 44.09 46.50 44.45 45.26 37.10 43.48

L2 46.67 44.50 47.35 45.02 46.20 38.43 44.30
L3 46.40 44.65 46.90 45.02 45.95 37.92 44.08
L4 46.35 44.30 46.97 45.10 46.06 37.31 43.95
L5 46.29 44.57 46.97 44.75 45.45 37.74 43.89
HR 46.28 44.04 47.80 44.56 45.56 38.17 44.08

SHR 47.05 44.80 48.15 45.55 46.30 39.02 44.78

Table 4: BLEU comparison on the LDC dataset with independently regularized layers.

For the Zhâ‡’En dataset, we select tasks at depths
5, 8, 11, 20 with cardinalities 32, 208, 955, 5000;
for the Enâ‡’De dataset, we select tasks at depths
5, 7, 10, 21 with cardinalities 32, 127, 878, 5000.

The model configuration strictly follows that of
the base model in Vaswani et al. (2017) with word
embedding size of 512, hidden size of 512, feed-
forward projection size of 2048, layer and head
number of 6 and 8. The dropout rates of the em-
bedding, attention and residual block are all set to
0.1. All architectural settings are in accordance
with the base model of Vaswani et al. (2017).

We use Adam (Kingma and Ba, 2014) as the
default optimizer with an initial learning rate of
0.0001. Training batch size is set to 8192 Ã— 4
tokens per batch, that is, we use data parallelism
with 4 P40 or M40 GPUs and 8192 tokens per
GPU. We train the Zhâ‡’En models from scratch
for about 240k iterations about 2 weeks on 4 M40
GPUs for both the baseline and the 3 regulariza-
tion variants. For the Enâ‡’De models, we first
attempt to train all the methods from scratch up
to 200k iterations, but do not see significant im-
provement on BLEU score (around 0.6 points on
test). So we use the pretrained baseline to initial-
ize our proposed methods, and further train them
for about 200k iterations, which results in the re-
ported improvement in Section 4.5.

D Experiments of Independent Layer
Regularization on Zhâ‡’En

As suggested by the reviewers, we conduct inde-
pendent layer regularization by imposing a rela-
tive task with cardinality of 5000 on layer 2 to
5 with the six layer Transformer. The perfor-
mances are demonstrated in Table 4. It seems
that independent layer regularization can as well
brings about descent improvements over the base-
line. And regularizing layer 2 of the decoder per-
forms better than our HR method. It is surpris-

ing that lower layers are more urgent to be regu-
larized than higher layers. This phenomenon may
raise the question that how on earth the interme-
diate layer representations help with final predic-
tion. One hypothesis may be drawn from our pa-
per is that: in baseline, the coherence among the
layer-wise representations may be weak so that
some non-linear transformations from lower layer
may not lead to essential predictive power of the
final layer representation. And by using KL diver-
gence to externally constrain their decision con-
sistency may take better advantage of lower lay-
ers. As one of the reviewer pointed out, Uni-
versal Transformer (Dehghani et al., 2018) with
base modelâ€™s hyperparameters achieves 28.90 on
WMT14 Enâ‡’De newstest14, with 1/6 enc-dec pa-
rameters (not considering embeddings). Its archi-
tectural inductive bias is motivated from iterative
refinement of the layer-wise representation so the
decoder at each time step builds an RNN like rea-
soning process to refine upon previous layerâ€™s rep-
resentation. We think this might be a more effec-
tive inductive bias in ResNet architecture which
is adopted by Transformer, since Jastrzebski et al.
(2017) provides evidence that ResNet does itera-
tive representation inference (residual as refined
quantity). Future directions may include relating
the dimension reduced representations (Law et al.,
2018) to the coarse-to-fine structural bias, or ex-
periments on Universal Transformer architecture
to probe its learned representations.


