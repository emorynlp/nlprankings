



















































Hashtag Recommendation Using Dirichlet Process Mixture Models Incorporating Types of Hashtags


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 401–410,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Hashtag Recommendation Using Dirichlet Process Mixture Models
Incorporating Types of Hashtags

Yeyun Gong, Qi Zhang, Xuanjing Huang
Shanghai Key Laboratory of Data Science

School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{12110240006, qz, xjhuang}@fudan.edu.cn

Abstract

In recent years, the task of recommending
hashtags for microblogs has been given
increasing attention. Various methods
have been proposed to study the problem
from different aspects. However, most
of the recent studies have not considered
the differences in the types or uses of
hashtags. In this paper, we introduce
a novel nonparametric Bayesian method
for this task. Based on the Dirichlet
Process Mixture Models (DPMM), we
incorporate the type of hashtag as a hid-
den variable. The results of experiments
on the data collected from a real world
microblogging service demonstrate that
the proposed method outperforms state-
of-the-art methods that do not consider
these aspects. By taking these aspects into
consideration, the relative improvement of
the proposed method over the state-of-the-
art methods is around 12.2% in F1- score.

1 Introduction

Hashtags are used to mark keywords or topics in a
microblog. Over the past few years, social media
services have become some of the most important
communication channels for people. According
to the statistic reported by the Pew Research
Centers Internet & American Life Project in Aug
5, 2013, about 72% of adult internet users are
also members of at least one social networking
site. Hence, microblogs have also been widely
used as data sources for public opinion analy-
ses (Bermingham and Smeaton, 2010; Jiang et
al., 2011), prediction (Asur and Huberman, 2010;
Bollen et al., 2011), reputation management (Pang
and Lee, 2008; Otsuka et al., 2012), and many
other applications (Sakaki et al., 2010; Becker et
al., 2010; Guy et al., 2010; Guy et al., 2013).

In addition to the limited number of characters
in the content, microblogs also contain a form
of metadata tag (hashtag), which is a string of
characters preceded by the symbol (#). Hashtags
are used to mark the keywords or topics of
a microblog. They can occur anywhere in a
microblog, at the beginning, middle, or end.
Hashtags have been proven to be useful for many
applications, including microblog retrieval (Efron,
2010), query expansion (A.Bandyopadhyay et al.,
2011), and sentiment analysis (Davidov et al.,
2010; Wang et al., 2011). However, only a
small percentages of microblogs contain hashtags
provided by their authors. Hence, the task of rec-
ommending hashtags for microblogs has become
an important research topic and has received con-
siderable attention in recent years. Existing works
have studied discriminative models (Ohkura et
al., 2006; Heymann et al., 2008) and generative
models (Blei and Jordan, 2003; Krestel et al.,
2009; Ding et al., 2013; Godin et al., 2013) based
on the textual information of a single microblog.

Since microblog users are free to develop
and use their own hashtags, they may select
hashtags for different purposes. Based on an
analysis of the hashtags crawled from a real
online service, we observe that hashtags are used
for events, conferences, conversation, disasters,
memes, recall, quotes, and so on. To illustrate it
let us take the following examples:

Example 1:#Apple iOS 9 includes music fea-
ture, new security and support for older iPhones.

Example 2:#BREAKING: Missing cyclist Na-
talie Donoghue has been found alive after she
went missing in the Hunter Valley.

We can see that the hashtag #Apple iOS 9 used
in the example summarize the main topics of
the corresponding microblog. While, the aim of
hashtag #BREAKING in the example 2 is used as
a label of the microblog. The different uses greatly

401



impact the strategy of hashtag recommendation.
However, there has been relatively few studies
which take this issue into consideration.

In this paper, we propose a novel nonpara-
metric Bayesian method to perform this problem.
Inspired by the methods proposed by Liu et
al. (2012), we assume that the hashtags and
textual content in the corresponding microblog are
parallel descriptions of the same thing in different
languages. We adapt a translation model with
topic distribution to achieve this task. Because
of the ability of Dirichlet Process Mixture Models
(DPMM) (Antoniak and others, 1974; Ferguson,
1983) to handle an unbounded number of topics,
the proposed method is extended from them.
Based on the different uses of hashtags, we
incorporate the type of hashtag into the DPMM as
a hidden variable.

The main contributions of this work can be
summarized as follows:

• Through analyzing the microblogs, we pro-
pose the problem of influences of types of
hashtags.

• We adopt a nonparametric Bayesian method
to perform the hash tag recommendation task,
which also takes the types of hashtags into
consideration.

• Experimental results on the dataset we
construct from a real microblogging service
show that the proposed method can achieve
significantly better performance than the
state-of-the-arts methods.

2 The Proposed Method

In this section, we first give some brief
descriptions about the Dirichlet process
(DP) and Dirichlet Process Mixture Models
(DPMM). Then, we detail the proposed hashtag
recommendation method.

2.1 Preliminaries

2.1.1 Dirichlet Process
The Dirichlet process (DP) is a distribution over
distributions. A DP, denoted by G ∼ DP (α,H),
is parameterized by a base measure H , and a
concentration parameter α. After a discussion
of basic definitions, we present two different
perspectives on the Dirichlet process.

A perspective on the Dirichlet process is stick-
breaking construction. The stick-breaking con-
struction considers a probability mass function
{βk}∞k=1 on a countably infinite set, where the
discrete probabilities are defined as follows:

vk|α ∼ Beta(1, α)

βk = vk
k−1∏
l=1

(1− vl).
(1)

The kth weight is a random proportion vk of the
remaining stick after the previous(k − 1) weights
have been defined. This stick-breaking construc-
tion is generally denoted by β ∼GEM(α) (GEM
stands for Griffiths, Engen and McCloskey). A
random draw G ∼ DP (α,H) can be expressed
as:

G =
∞∑
k=1

βkδθk θk|α,H ∼ H, (2)

where δθ is a probability measure concentrated at
θ.

A second perspective on the Dirichlet process
is provided by the Pólya urn scheme (Blackwell
and MacQueen, 1973). It refers to draws from
G. Let θ1, θ2, ... represent a sequence of inde-
pendent and identically distributed (i.i.d.) random
variables distributed according to G. Blackwell
and MacQueen (1973) showed that the conditional
distributions of θi given θ1, ..., θi−1 have the
following form:

θi|θ1, ..., θi−1, α,H

∼
i−1∑
j=1

j

i− 1 + αδθj +
α

i− 1 + αH.
(3)

Eq.(3) shows that θi has positive probability of
being equal to one of the previous draws. We use
φ1, ..., φK to represent the distinct values taken on
by θ1, ..., θi−1, and Eq.(3) can be re-expressed as:

θi|θ1, ..., θi−1, α,H

∼
K∑
k=1

mk
i− 1 + αδθφk +

α

i− 1 + αH,
(4)

where mk is the number of values θi′ = φk for
1 ≤ i′ < i.
2.1.2 Dirichlet Process Mixture Models
In nonparametric Bayesian statistics, DPs are
commonly used as prior distributions for mixture

402



models with an unknown number of components.
Let F (θi) denotes the distribution of the observa-
tion xi given θi. We can get the observation xi as
follows:

θi|G ∼ G
xi|θi ∼ F (θi).

Given G ∼ DP (α,H), each observation xi from
an exchangeable data set x is generated by first
choosing a parameter θi ∼ G, and then sampling
xi ∼ F (θi). This model is referred to as a
Dirichlet process mixture model. This process
is often described by a set z of independently
sampled variables zi ∼ Mult(β) indicating the
component of the mixture G(θ) associated with
each data point xi ∼ F (θzi). Then we can get:

zi|β ∼Mult(β)
xi|{θk}∞k=1, zi ∼ F (θzi).

2.2 DPMM Based Hashtag Recommendation
2.2.1 The Generation Process
Let D represent the number of microblogs in the
given corpus. A microblog contains a bag of
words denoted by wd = {wd1 , wd2 , ..., wdNd},
where Nd is the total number of terms in the
microblog. A word is defined as an item from
a vocabulary with W distinct words indexed
by w = {w1, w2, ..., wW }. Each microblog
may have a number of hashtags denoted by
hd = {hd1 , hd2 , ..., hdMd}. Md is the number of
hashtags of microblog d. Each hashtag is from the
vocabulary with V distinct hashtags indexed by
h = {h1, h2, ..., hV }. Given an unlabeled data set,
the task of hashtag recommendation is to discover
a list of hashtags for each microblog.

In standard LDA, each document is viewed as a
mixture of topics, and each topic has probabilities
to generate words. A LDA is a generalization of a
finite mixture model. Since DP is the extension
of finite mixture models to the nonparametric
setting, the appropriate tool for nonparametric
topic models is HDP. However, both LDA and
HDP are normally suitable for long documents.
For microblogs, which have limited number of
words, a single microblog is most likely to talk
about a single topic. Hence, in this work, we
regard that each microblog associates with only
one topic. The set of documents are viewed as a
mixture of infinite topics. And we use DPs as prior
distributions for the mixture of infinite topics.

The main assumptions of our model are as
follows. When user u publishes a microblog, he
will first generate the content and then generate
the hashtags. When constructing the content,
he will select a topic based on the topic distri-
bution. Then he will choose a bag of words
one by one from the word distribution of the
topic or from the background words that captures
white noise. Hashtags will be chosen according
to the following two situations. In the first
situation, hashtags summarize the corresponding
microblogs. Hashtags of a microblog can be
generated from the content through the topic-
specific alignment probability between words and
hashtags. In the second situation, hashtag is used
as a label of the microblog. We recommend the
hashtags using the words in the microblog, which
is based on the frequency of words regarded as this
type of hashtag.

Let π be the probability of choosing a topic
word or a background word, and we use yd =
{ydn}Ndn=1 to indicate a word to be a topic word
or background word. θ denotes the topic distribu-
tion, and φk represents the word distribution for
topic k. φB represents the word distribution for
background words. We use xdm to represent the
type of hashtag hdm , and use zd to represent the
topic of document d. Then each hashtag hdm is
annotated according to the translation possibility
P (hdm |wd, zd, xdm , ϕxdm ), where ϕxdm is the
probability alignment table between words and
hashtags. The generation process is as Algo-
rithm 1.

Figure 1(a) shows the graphical representation
of the generation process in Algorithm 1. Fig-
ure 1(b) is the graphical model which does not take
the types of hashtags into consideration, where
ϕ∗ ∈ {ϕ1, ϕ2}. If ϕ∗ = ϕ1, the model is just
considering the first situation. when ϕ∗ = ϕ2,
only the second type of hashtag will be considered.

2.2.2 Learning

We use collapsed Gibbs sampling (Griffiths and
Steyvers, 2004) to obtain samples of hidden
variables assignment and to estimate the model
parameters from these samples.

The sampling probability of being a
topic/background word for the nth word in the
microblog d can be calculated by the following

403



wdn

zd

θ

α

hdm

ydn π σ

φz

φB

βw

βw

ϕ∗

βh

Md Nd

D

∝W
∝

(a) CNHR

wdn

zd

θ

α

hdm

ydn π σ

φz

φB

βw

βw

xdmλ

η

ϕx

βh

Md Nd

D

∝W
∝

(b) NHR*

Figure 1: The graphical representation of the proposed model. Shaded circles are observations or
constants. Unshaded ones are hidden variables. CNHR represents the proposed hashtag recommendation
method. NHR* represents the model which does not take the types of hashtags into consideration.

Algorithm 1 The generation process of CNHR
Draw π ∼ Beta(σ), λ ∼ Beta(η)
Draw background word distribution φB ∼
Dir(βw)
Draw θ|α ∼ GEM(α)
for each microblog d = 1, 2, ..., D do

Draw zd ∼Mul(θ)
Draw word distribution φzd ∼ Dir(βw)
for each word n = 1, ..., Nd do

Draw ydn ∼ Ber(π)
if ydn = 0 then

Draw a word wdn from the background-
word distribution wdn ∼Mul(φB)

else
Draw a word wdn from the topic-word
distribution wdn ∼Mul(φzd)

end if
end for
for each hashtag m = 1, ...,Md do

Draw xdm ∼ Ber(λ)
Draw ϕxdm ∼ Dir(βh)
Draw a hashtag hdm ∼
P (hdm |wd, zd, xdm , ϕxdm )

end for
end for

equation:

p(ydn |w,h, z,y¬dn , σ, βw)

∝ N¬n,p + σ
N¬n,(.) + 2σ

· N
wdn
¬n,l + β

w

N
(.)
¬n,l + βwW

,
(5)

where l = B when p = 0 and l = zd when
p = 1, N¬n,p is a count of words that are assigned
to background words and any topic respectively,
N
wdn
¬n,B is the number of word wdn assigned to

background words, Nwdn¬n,zd is the number of word
wdn that are assigned to topic zd. All counters are
calculated with the current word wdn excluded.

We sample zd for the microblog d using the
following equation:

p(zd|w,h, z¬d,y,x, α, βw, βh) ∝ p(zd|z¬d, α)
· p(wd|z,w¬d,y, βw) · p(hd|z,wd,y,x, βh).

(6)
We can also represent p(zd|z¬d, α) with CRP as

described in the previous section. Since z1, z2, ...
is a sequence of i.i.d random variables, they are
exchangeable. Let us consider the dth variable zd
is the last observation, we can get the following
expression:

p(zd|z¬d, α) ∼
K∑
k

Nk¬d
N

(.)
¬d − 1 + α

δ(zd, k) +
α

N
(.)
¬d − 1 + α

δ(zd, k̄),

(7)
where k is an exist topic and k̄ is a new topic,
Nk¬d is the number of microblogs assigned with
topic k, N (.)¬d is the total number of microblogs,
α is concentration parameter. All counters are

404



calculated with the current microblog d excluded.
If zd equals an exist topic zd = k, then we can

calculate p(wd|z,w¬d,y, βw) by:
p(wd|z,w¬d,y, βw) =∫
φk
f̃(wd|φk)

∏
zj=k,j 6=d f̃(wj|φk)h(φk)dφk∫

φk

∏
zj=k,j 6=d f̃(wj|φk)h(φk)dφk

,

(8)
where f̃(wd|φk) =

∏
1≤n≤Nd,ydn=1 f(wdn |φk).

Nd is the number of words in microblog d.
f(wdn |φk) is the density of word wdn given topic
k. wd are the words in microblog d. h(φk) is the
density of base measure H .

If zd is a new topic zd = k̄, then we can
calculate p(wd|zd = k̄,w¬d,y, βw) by:
p(wd|zd = k̄,w¬d,y, βw) = p(wd|βw) =∫
φk̄

p̃(wd|φk̄)h(φk̄)dφk̄,
(9)

where p̃(wd|φk̄) =
∏

1≤n≤Nd,ydn=1 p(wdn |φk̄).
We can calculate the probabilities of generating

hashtags from two situations as follows:

p(hd|z,wd,y,x, βh) =
∏Md
m=1

∑
n∈Ñd

Mk,¬dwdn,hdm
+βh

Mk,¬d
wdn

,(.)
+βhV

xdm = 1∏Md
m=1

∑
n∈Ñd,wdn=hdm

Mk,¬dwdn,2
+βh

Mk,¬d
wdn

,(.)
+2βh

xdm = 2,

(10)
where Ñd represent the index set of topic
words(y = 1) in the microblog d, Mk,¬dwdn ,hdm
is the number of occurrences that word wdn is
translated to hashtag hdm given topic k, M

k,¬d
wdn ,(.)

is the total number of occurrences that word wdn
is under topic k, Mk,¬dwdn ,2 is the number of word
wdn recommended as the second type of hashtag
given topic k. All counters with ¬d are calculated
with the current microblog wd excluded.

We sample the index variable xdm for mth
hashtag in the microblog d as follows:

p(xdm |z,wd,y,x¬dm ,hd, βh)

∝
Ñd∑
n=1

ϕ
xdm
hdm ,zd,wdn

N¬dmxdm + η

N¬dm(.) + 2η
,

(11)

where N¬dmxdm is the number of hashtags that is
generated by the type xdm , N

¬dm
(.) is total number

of hashtags, the counters with ¬dm are calculated
with the current hashtag excluded.

After enough sampling iterations to burn in the
Markov chain, ϕ1 and ϕ2 are estimated as follows:

ϕ1h,k,w =
Mkw,h + β

h

Mkw,(.) + β
hV

, ϕ2h,k,w =
Mkw,2 + β

h

Mkw,(.) + 2β
h
,

(12)
The potential size of the probability alignment

ϕ1 between hashtag and word is W · V ·K. The
data sparsity may pose a more serious problem in
estimating ϕ1 than the topic-free word alignment
case. We use interpolation smoothing technique
for ϕ1. In this paper, we employ smoothing as
follows:

ϕ1∗h,k,w = γϕ
1
h,k,w + (1− γ)P (h|w), (13)

where ϕ1∗h,k,w is the smoothed topical alignment
probabilities, ϕ1h,k,w is the original topical align-
ment probabilities, P (h|w) is topic-free word
alignment probability. In this work, we obtain
P (h|w) by exploring IBM model-1 (Brown et al.,
1993). γ is trade-off of two probabilities ranging
from 0 to 1. When γ = 0, ϕ1∗h,k,w reduces to topic-
free word alignment probability, and when γ = 1,
there will be no smoothing in ϕ1∗h,k,w.

2.2.3 Hashtag Recommendation
Suppose given an unlabeled dataset, we firstly
discover the topic and determine topic/background
words for each microblog. The collapsed Gibbs
sampling is also applied for inference. The pro-
cess is almost same as previous section described
the model learning. The different is that there
are no hashtags in the unlabeled dataset. Hence,
when sampling zd for the microblog d, we use the
following equation:

p(zd|w, z¬d,y, α, βw)
∝ p(zd|z¬d, α) · p(wd|z,w¬d,y, βw).

(14)

Since there are no differences between the
word alignments with each hashtags for a new
topic in the unlabeled dataset, after the hidden
variables of topic/background words and the topic
of each microblog become stable, we only need
to estimate the distribution of topics exist in
the training dataset. Then we can estimate the
distribution of topics for the microblog d in the
unlabeled data by:

χdk =
p(k)p(wd1 |k)p(wd2 |k)...p(wdNd |k)

Z
,

(15)

405



where p(wdn |k) = N
wdn
k +β

N
(.)
k +Wβ

and Nwdnk is a count

of words wdn that are assigned to topic k in the
corpus. And p(k) = NkN(.)+α is regarded as a prior
for topic distribution, where Z is the normalized
factor. With topic distribution χ and topic-specific
word alignment table ϕ∗, we can rank hashtags for
the microblog d in the unlabeled data through the
following equation:

p(hdm |wd, χd, ϕ∗) ∝
K∑

zd=1

Nd∑
n=1

C∑
x=1

p(zd|χd) · p(wdn |wd) · p(xdm)

· p(hdm |wdn , zd, xdm , ϕxdm∗),
(16)

where C is the number of hashtag types.
p(wdn |wd) is the weight of the word wdn in the
microblog content wd, which can be estimated
by the IDF score of the word, p(xdm) is the
probability of hashtag belong to the type xdm , we
can estimate it with Eq.(11). Based on the ranking
scores, we can suggest the top-ranked hashtags
for each microblog.

3 Experiments

3.1 Data Collection

We use a dataset collected from Sina Weibo1,
which provides the Twitter-like service and is one
of the most popular one in China, to evaluate the
proposed approach and alternative methods. The
original data set contains 282.2 million microblogs
posted by around 1.1 million users. These
microblogs were obtained by starting from a set
of seed users and their follower/followee relations.
We extract the microblogs posted with hashtags
between Jan. 2012 and July 2013. Finally,
1,118,792 microblogs posted are selected for this
work. The unique number of hashtags in the
corpus is 305,227. We randomly select 100K as
training data, 10K as development data, and 10K
as test set. The hashtags marked in the original
microblogs are considered as the golden standards.

3.2 Experiment Configurations

We use precision (P ), recall (R), and F1-score
(F1) to evaluate the performance. Precision is cal-
culated based on the percentage of “hashtags truly
assigned” among “hashtags assigned by system”.
Recall is calculated based on the “hashtags truly

1http://www.weibo.com

assigned” among “hashtags manually assigned”.
F1 is the harmonic mean of precision and recall.
We do 500 iterations of Gibbs sampling to train
the model. For optimizing the hyperparmeters of
the proposed method and alternative methods, we
use development data set to do it. In this work, the
scale parameter α is set to Gamma(5, 0.5). The
other settings of hyperparameters are as follows:
βw = 0.1, βh = 0.1, η = 0.01, and σ = 0.01.
The smoothing factor γ in Eq.(13) is set to 0.8.
For estimating the translation probability without
topical information, we use GIZA++ 1.07 (Och
and Ney, 2003) to do it.

Since hashtag recommendation task can also be
modeled as a classification problem, we compare
the proposed model with the following alternative
methods:

• Naive Bayes (NB): We formulate hashtag
recommendation as a binary classification
task and apply NB to model the posterior
probability of each hashtag given a mi-
croblog.

• Support Vector Machine (SVM): Similar to
Naive Bayes, each hashtag can be regarded as
one label and we use SVM to classify these
microblogs.

• Translation model (IBM-1): IBM model 1
is directly applied to obtain the alignment
probability between the word and the hash-
tag (Liu et al., 2011).

• Topical translation model (TTM): Ding et
al. (2013) proposed the TTM for hashtag
extraction. We implemented and extended
their method for evaluating on the corpus
constructed in this work. The number of
topics in TTM is set to 20, and α is set to
50/K. The hyperparameters used in TTM
are also selected based on the development
data set.

3.3 Experimental Results

Table 1 shows the comparisons of the proposed
method with the state-of-the-art methods on the
constructed evaluation dataset. “CNHR” denotes
the method proposed in this paper. “NHR1”
is a degenerate variation of CNHR, in which
we consider all the hashtags are generated from
distribution ϕ1. “NHR2” is a model in which
we consider all the hashtags are generated from

406



0

0.1

0.2

0.3

0.4

0.5

0.6

1 2 3 4 5

P
re

ci
si

o
n

Number of Recommended Hashtags

NB

TTM

IBM1

SVM

NHR1

NHR2

CNHR

0.2

0.3

0.4

0.5

0.6

0.7

0.8

1 2 3 4 5

R
ec

a
ll

Number of Recommended Hashtags

NB

TTM

IBM1

SVM

NHR1

NHR2

CNHR

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

1 2 3 4 5

F
1

Number of Recommended Hashtags

NB

TTM

IBM1

SVM

NHR1

NHR2

CNHR

Figure 2: Precision, Recall and F1 with recommended Hashtags range from 1 to 5

Table 1: Evaluation results of different methods on
the evaluation collection.

Methods Precision Recall F1
NB 0.230 0.203 0.215
SVM 0.426 0.376 0.399
IBM1 0.279 0.246 0.261
TTM 0.445 0.393 0.417
NHR1 0.448 0.395 0.419
NHR2 0.293 0.258 0.274
CNHR 0.502 0.442 0.470

distribution ϕ2. From the results, we can observe
that discriminative methods achieve worse results
than generative methods. We think that the large
number of hashtags is one of the main reasons of
the low performances.

From the results shown in Table 1, we also
observe that the proposed method can achieve sig-
nificantly better performance than existing meth-
ods. The relative improvement of proposed CNHR
over TTM is around 12.7% in F1. And we can
see that the performances of TTM are similar as
the results of NHR1. Since TTM and NHR1 are
similar with each other except that TTM is based
on LDA and NHR1 is adapted from DPMM. The
results demonstrate the advantage of using DPMM
over LDA. It does not need prior knowledge
about number of topics. Comparing the results of
the method CNHR with the methods NHR1 and
NHR2 which do not take the types of hashtags
into consideration, we can see that the proposed
method benefits a lot from incorporating the types
of hashtags.

Figure 2 shows the Precision, Recall, and F1
curves of NB, IBM1, SVM, TTM, NHR1, NHR2
and CNHR on the test data. Each point of a curve

Table 2: The influence of the number of topics
K of TTM.

K Precision Recall F1
10 0.441 0.389 0.413
20 0.445 0.393 0.417
30 0.432 0.381 0.405
40 0.413 0.364 0.387
50 0.391 0.345 0.367

represents the extraction of a different number of
hashtags ranging from 1 to 5 respectively. In
curves, the curve that is the highest of the graph in-
dicates the best performance. Based on the results,
we can observe that the performance of CNHR is
the highest in all the curves. This indicates that the
proposed method was significantly better than the
other methods.

In TTM, the number of topics K is also crucial
factor. Table 2 shows the impact of the number
of topics. From the table, we can observe that
TTM obtains the best performance when K is
set to 20. And performance decreases with more
number of topics. We think that data sparsity may
be one of the main reasons. With much more topic
number, the data sparsity problem will be more
serious when estimating topic-specific translation
probability. We compare our method with the best
performance of TTM.

From the description of the proposed model,
we can know that there is a smooth parameter
γ in the proposed method CNHR. To evaluate
the impact of it, Figure 3 shows the influence of
the translation probability smoothing parameter γ.
When γ is set to 0.0, it means that the topical
information is omitted. Comparing the results
of γ = 0.0 and other values, we can observe
that the topical information can benefit this task.

407



Figure 3: The influence of the smoothing
parameter γ of CNHR.

0

0.1

0.2

0.3

0.4

0.5

0.6

Precision Recall F1

γ=0

γ=0.2

γ=0.4

γ=0.6

γ=0.8

γ=1

When γ is set to 1.0, it represents the method
without smoothing. The results indicate that it is
necessary to address the sparsity problem through
smoothing.

4 Related Works

Due to the usefulness of tag recommendation,
many methods have been proposed from different
perspectives (Heymann et al., 2008; Krestel et
al., 2009; Rendle et al., 2009; Liu et al., 2012;
Ding et al., 2013). Heymann et al. (Heymann
et al., 2008) investigated the tag recommen-
dation problem using the data collected from
social bookmarking system. They introduced an
entropy-based metric to capture the generality of a
particular tag. In (Song et al., 2008), a Poisson
Mixture Model based method is introduced to
achieve the tag recommendation task. Krestel
et al. (Krestel et al., 2009) introduced a Latent
Dirichlet Allocation to elicit a shared topical
structure from the collaborative tagging effort of
multiple users for recommending tags. Ding et
al. (2013) proposed to use translation process to
model this task.

Based on the the observation that similar web
pages tend to have the same tags, Lu et al. (2009)
proposed a method taking both tag information
and page content into account to achieve the task.
They extended the translation based method and
introduced a topic-specific translation model to
process the various meanings of words in different
topics. In (Tariq et al., 2013), discriminative-
term-weights were used to establish topic-term
relationships, of which users’ perception were

learned to suggest suitable hashtags for users.
To handle the vocabulary problem in keyphrase
extraction task, Liu et al. proposed a topical
word trigger model, which treated the keyphrase
extraction problem as a translation process with
latent topics (Liu et al., 2012).

Most of the works mentioned above are based
on textual information. Besides these methods,
personalized methods for different recommenda-
tion tasks have also been paid lots of atten-
tions (Liang et al., 2007; Shepitsen et al., 2008;
Garg and Weber, 2008; Li et al., 2010; Liang
et al., 2010; Rendle and Schmidt-Thieme, 2010;
Huang et al., 2012). Shepitsen et al. (2008)
proposed to use hierarchical agglomerative clus-
tering to take into account personalized navigation
context in cluster selection. In (Garg and Weber,
2008), the problem of personalized, interactive
tag recommendation was also studied based on
the statistics of the tags co-occurrence. Liang et
al. (2010) proposed to the multiple relationships
among users, items and tags to find the semantic
meaning of each tag for each user individually
and used this information for personalized item
recommendation.

From the brief descriptions given above, we
can observe that most of the previous works
on hashtag suggestion did not take the types
of hashtags into consideration. In this work,
we propose to incorporate it into the generative
methods.

5 Conclusions

In this paper, we study the problem of hashtag
recommendation for microblogs. Since exist-
ing translation model based methods for this
task regard all the hashtags generated from the
same distribution, we propose a novel method
which incorporates different type of hashtags have
different distribution into the topical translation
model for hashtag recommendation task. To
evaluate the proposed method, we also construct
a dataset from real world microblogging services.
The results of experiments on the constructed
dataset demonstrate that the proposed method
outperforms state-of-the-art methods that do not
consider these aspects.

6 Acknowledgement

The authors wish to thank the anonymous review-
ers for their helpful comments. This work was par-

408



tially funded by National Natural Science Founda-
tion of China (No. 61473092 and 61472088), the
National High Technology Research and Develop-
ment Program of China (No. 2015AA011802),
and Shanghai Science and Technology Develop-
ment Funds (13dz226020013511504300).

References
A.Bandyopadhyay, M. Mitra, and P. Majumder.

2011. Query expansion for microblog retrieval.
In Proceedings of The Twentieth Text REtrieval
Conference, TREC 2011.

Charles E Antoniak et al. 1974. Mixtures of
dirichlet processes with applications to bayesian
nonparametric problems. The annals of statistics,
2(6):1152–1174.

S. Asur and B.A. Huberman. 2010. Predicting the
future with social media. In WI-IAT’10, volume 1,
pages 492–499.

Hila Becker, Mor Naaman, and Luis Gravano. 2010.
Learning similarity metrics for event identification
in social media. In Proceedings of WSDM ’10.

Adam Bermingham and Alan F. Smeaton. 2010.
Classifying sentiment in microblogs: is brevity an
advantage? In Proceedings of CIKM’10.

David Blackwell and James B MacQueen. 1973.
Ferguson distributions via pólya urn schemes. The
annals of statistics, pages 353–355.

D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of SIGIR, pages 127–134.

Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 – 8.

Peter F Brown, Vincent J Della Pietra, Stephen
A Della Pietra, and Robert L Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263–311.

Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of COLING ’10.

Zhuoye Ding, Xipeng Qiu, Qi Zhang, and Xuanjing
Huang. 2013. Learning topical translation model
for microblog hashtag suggestion. In Proceedings
of IJCAI 2013.

Miles Efron. 2010. Hashtag retrieval in a
microblogging environment. In Proceedings of
SIGIR ’10.

Thomas S Ferguson. 1983. Bayesian density
estimation by mixtures of normal distributions.
Recent advances in statistics, 24:287–302.

Nikhil Garg and Ingmar Weber. 2008. Personalized,
interactive tag recommendation for flickr. In
Proceedings of RecSys ’08.

Fréderic Godin, Viktor Slavkovikj, Wesley De Neve,
Benjamin Schrauwen, and Rik Van de Walle.
2013. Using topic models for twitter hashtag
recommendation. In Proceedings of the 22Nd
International Conference on World Wide Web
Companion, WWW ’13 Companion, pages 593–
596, Republic and Canton of Geneva, Switzerland.
International World Wide Web Conferences Steering
Committee.

T. L. Griffiths and M. Steyvers. 2004. Finding
scientific topics. Proceedings of the National
Academy of Sciences.

Ido Guy, Naama Zwerdling, Inbal Ronen, David
Carmel, and Erel Uziel. 2010. Social media
recommendation based on people and tags. In
Proceedings of SIGIR ’10.

Ido Guy, Uri Avraham, David Carmel, Sigalit Ur,
Michal Jacovi, and Inbal Ronen. 2013. Mining
expertise and interests from social media. In
Proceedings of WWW ’13.

Paul Heymann, Daniel Ramage, and Hector Garcia-
Molina. 2008. Social tag prediction. In
Proceedings of SIGIR ’08.

Wenyi Huang, Saurabh Kataria, Cornelia Caragea,
Prasenjit Mitra, C Lee Giles, and Lior Rokach.
2012. Recommending citations: translating papers
into references. In Proceedings of the 21st
ACM international conference on Information and
knowledge management, pages 1910–1914. ACM.

Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter
sentiment classification. In Proceedings of ACL
2011, Portland, Oregon, USA.

Ralf Krestel, Peter Fankhauser, and Wolfgang Nejdl.
2009. Latent dirichlet allocation for tag recommen-
dation. In Proceedings of RecSys ’09.

Lihong Li, Wei Chu, John Langford, and Robert E
Schapire. 2010. A contextual-bandit approach
to personalized news article recommendation. In
Proceedings of the 19th international conference on
World wide web, pages 661–670. ACM.

Ting-Peng Liang, Hung-Jen Lai, and Yi-Cheng Ku.
2007. Personalized content recommendation and
user satisfaction: Theoretical synthesis and empir-
ical findings. Journal of Management Information
Systems, 23(3):45–70.

Huizhi Liang, Yue Xu, Yuefeng Li, Richi Nayak,
and Xiaohui Tao. 2010. Connecting users and
items with weighted tags for personalized item
recommendations. In Proceedings of the 21st ACM
conference on Hypertext and hypermedia, pages 51–
60. ACM.

409



Zhiyuan Liu, Xinxiong Chen, and Maosong Sun.
2011. A simple word trigger method for social
tag suggestion. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1577–1588. Association for
Computational Linguistics.

Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012.
Topical word trigger model for keyphrase extraction.
In Proceedings of COLING.

Yu-Ta Lu, Shoou-I Yu, Tsung-Chieh Chang, and Jane
Yung-jen Hsu. 2009. A content-based method to
enhance tag recommendation. In Proceedings of
IJCAI’09.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Tsutomu Ohkura, Yoji Kiyota, and Hiroshi Nakagawa.
2006. Browsing system for weblog articles based
on automated folksonomy. Workshop on the
Weblogging Ecosystem Aggregation Analysis and
Dynamics at WWW.

Takanobu Otsuka, Takuya Yoshimura, and Takayuki
Ito. 2012. Evaluation of the reputation network
using realistic distance between facebook data. In
Proceedings of WI-IAT ’12.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135, January.

Steffen Rendle and Lars Schmidt-Thieme. 2010. Pair-
wise interaction tensor factorization for personalized
tag recommendation. In Proceedings of the third
ACM international conference on Web search and
data mining, pages 81–90. ACM.

Steffen Rendle, Leandro Balby Marinho, Alexandros
Nanopoulos, and Lars Schmidt-Thieme. 2009.
Learning optimal ranking with tensor factorization
for tag recommendation. In Proceedings of KDD
’09.

Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of WWW ’10.

Andriy Shepitsen, Jonathan Gemmell, Bamshad
Mobasher, and Robin Burke. 2008. Personalized
recommendation in social tagging systems using
hierarchical clustering. In Proceedings of the 2008
ACM Conference on Recommender Systems, RecSys
’08, pages 259–266, New York, NY, USA. ACM.

Yang Song, Ziming Zhuang, Huajing Li, Qiankun
Zhao, Jia Li, Wang-Chien Lee, and C. Lee Giles.
2008. Real-time automatic tag recommendation. In
Proceedings of SIGIR ’08.

Amara Tariq, Asim Karim, Fernando Gomez, and
Hassan Foroosh. 2013. Exploiting topical
perceptions over multi-lingual text for hashtag
suggestion on twitter. In FLAIRS Conference.

Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming
Zhou, and Ming Zhang. 2011. Topic sentiment
analysis in twitter: a graph-based hashtag sentiment
classification approach. In Proceedings of CIKM
’11.

410


