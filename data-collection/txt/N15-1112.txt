



















































Socially-Informed Timeline Generation for Complex Events


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1055–1065,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Socially-Informed Timeline Generation for Complex Events

Lu Wang Claire Cardie Galen Marchetti
Department of Computer Science

Cornell University
Ithaca, NY 14853

{luwang, cardie}@cs.cornell.edu gjm97@cornell.edu

Abstract

Existing timeline generation systems for com-
plex events consider only information from
traditional media, ignoring the rich social con-
text provided by user-generated content that
reveals representative public interests or in-
sightful opinions. We instead aim to gen-
erate socially-informed timelines that con-
tain both news article summaries and selected
user comments. We present an optimization
framework designed to balance topical cohe-
sion between the article and comment sum-
maries along with their informativeness and
coverage of the event. Automatic evaluations
on real-world datasets that cover four com-
plex events show that our system produces
more informative timelines than state-of-the-
art systems. In human evaluation, the asso-
ciated comment summaries are furthermore
rated more insightful than editor’s picks and
comments ranked highly by users.

1 Introduction

Social media sites on the Internet provide increas-
ingly more, and increasingly popular, means for
people to voice their opinions on trending events.
Traditional news media — the New York Times and
CNN, for example — now provide online mecha-
nisms that allow and encourage readers to share re-
actions, opinions, and personal experiences relevant
to a news story. For complex emerging events, in
particular, user comments can provide relevant, in-
teresting and insightful information beyond the facts
reported in the news. But their large volume and
tremendous variation in quality make it impossible

* Comment A: The “Crimean 
Parliament”, headed by an ethnic 
Russian separatist who was 
elected leader of parliament 
AFTER pro-Russian armed forces 
occupied the parliamentary 
chambers, has voted for Crimea to 
be annexed into Russia…

* Comment B: Does the West and 
US have a policy at all? The 
Obama administration has 
warned of “increasingly harsh 
sanctions”, but it is unlikely that 
Europe will comply…

* Comment C: Sanctions are 
effective and if done in unison 
with the EU…

- Crimeans vote in a referendum to rejoin 
Russia or return to its status under the 1992 
constitution.

March 16th, 2014

- The Crimean parliament officially 
declared independence and requested full 
accession to the Russian Federation.
- Obama declared sanctions on Russian 
officials considered responsible for the crisis.
- The leader of the pro-Russian 
organization “Youth Unity” was arrested.

- President Obama warned Vladimir Putin 
that further provocations by Russia could 
isolate and diminish its influence.
- One pro-Russian soldier was killed in the 
Simferopol incident.

March 17th, 2014

March 18th, 2014

… summaries for other dates …

Figure 1: A snippet of the event timeline on Ukraine Cri-
sis is displayed on the left. On the right, we display a set
of representative comments addressing the article sum-
mary of March 17th. Comment A (underlined) brings a
perspective on “Crimean parliament passes declaration of
independence” (the article sentence is also underlined on
the left). Comments B and C focus on Obama’s sanctions
on Ukrainian and Russian officials. Sentences linked by
edges belong to the same event thread, which is centered
on the entities with the same color.

for readers to efficiently digest the user-generated
content, much less integrate it with reported facts
from the dozens or hundreds of news reports pro-
duced on the event each day.

In this work, we present a socially-informed time-
line generation system that jointly generates a news
article summary and a user comment summary for
each day of an ongoing complex event. A sample
(gold standard) timeline snippet for Ukraine Crisis
is shown in Figure 1. The event timeline is on the
left; the comment summary for March 17th is on the
right.

1055



While generating timelines from news articles and
summarizing user comments have been studied as
separate problems (Yan et al., 2011; Ma et al., 2012),
their joint summarization for timeline generation
raises new challenges. Firstly, there should be a tight
connection between the article and comment por-
tion of the timeline. By definition, users comment
on socially relevant events. So the important part of
articles and insightful comments should both cover
these events. Moreover, good reading experience re-
quires that the article summary and comment sum-
mary demonstrate evident connectivity. For exam-
ple, Comment C in Figure 1 (“Sanctions are effec-
tive and if done in unison with the EU”) is obscure
without knowing the context that “sanctions are im-
posed by U.S”. Simply combining the outputs from
a timeline generation system and a comment sum-
marization system may lead to timelines that lack
cohesion. On the other hand, articles and comments
are from intrinsically different genres of text: arti-
cles emphasize facts and are written in a professional
style; comments reflect opinions in a less formal
way. Thus, it could be difficult to recognize the con-
nections between articles and comments. Finally, it
is also challenging to enforce continuity in timelines
with many entities and events.

To address the challenges mentioned above, we
formulate the timeline generation task as an opti-
mization problem, where we maximize topic cohe-
sion between the article and comment summaries
while preserving their ability to reflect important
concepts and subevents, adequate coverage of men-
tioned topics, and continuity of the timeline as it is
updated with new material each day. We design a
novel alternating optimizing algorithm that allows
the generation of a high quality article summary and
comment summary via mutual reinforcement. We
demonstrate the effectiveness of our algorithm on
four disparate complex event datasets collected over
months from the New York Times, CNN, and BBC.
Automatic evaluation using ROUGE (Lin and Hovy,
2003) and gold standard timelines indicates that our
system can effectively leverage user comments to
outperform state-of-the-art approaches on timeline
generation. In a human evaluation via Amazon Me-
chanical Turk, the comment summaries generated
by our method were selected as the best in terms
of informativeness and insightfulness in 66.7% and

51.7% of the evaluations (vs. 26.7% and 30.0% for
randomly selected editor’s-picks).

Especially, our optimization framework relies on
two scoring functions that estimate the importance
of including individual article sentences and user
comments in the timeline. Based on the observa-
tion that entities or events frequently discussed in
the user comments can help with identify summary-
worthy content, we show that the scoring functions
can be learned jointly by utilizing graph-based reg-
ularization. Experiments show that our joint learn-
ing model outperforms state-of-the-art ranking algo-
rithms and other joint learning based methods when
evaluated on sentence ranking and comment rank-
ing. For example, we achieve an NDCG@3 of
0.88 on the Ukraine crisis dataset, compared to 0.77
from Yang et al. (2011) which also conducts joint
learning between articles and social context using
factor graphs.

Finally, to encourage continuity in the gener-
ated timeline, we propose an entity-centered event
threading algorithm. Human evaluation demon-
strates that users who read timelines with event
threads write more informative answers than users
who do not see the threads while answering the same
questions. This implies that our system constructed
threads can help users better navigate the timelines
and collect relevant information in a short time.

For the rest of the paper, we first describe data
collection (Section 2). We then introduce the
joint learning model for importance prediction (Sec-
tion 3). The full timeline generation system is pre-
sented in Section 4, which is followed by evaluations
(Section 5). Related work and conclusion are in Sec-
tions 6 and 7.

2 Data Collection and Preprocessing

We crawled news articles from New York Times
(NYT), CNN, and BBC on four trending events: the
missing Malaysia Airlines Flight MH370 (MH370),
the political unrest in Ukraine (Ukraine), the Israel-
Gaza conflict (Israel-Gaza), and the NSA surveil-
lance leaks (NSA). For each event, we select a set
of key words (usually entities’ name), which are
used to filter out irrelevant articles. We collect com-
ments for NYT articles through NYT community
API, and comments for CNN articles via Disqus

1056



API. 1 NYT comments come with information on
whether a comment is an editor’s-pick. The statis-
tics on the four datasets are displayed in Table 1.2

Time Span # Articles # Comments
MH370 03/08 - 06/30 955 406,646
Ukraine 03/08 - 06/30 3,779 646,961
Israel-Gaza 07/20 - 09/30 909 322,244
NSA 03/23 - 06/30 145 60,481

Table 1: Statistics on the four event datasets.

We extract parse trees, dependency trees, and
coreference resolution results of articles and com-
ments with Stanford CoreNLP (Manning et al.,
2014). Sentences in articles are labeled with times-
tamps using SUTime (Chang and Manning, 2012).

We also collect all articles with comments from
NYT in 2013 (henceforth NYT2013) to form a
training set for learning importance scoring func-
tions on articles sentences and comments (see Sec-
tion 3). NYT2013 contains 3, 863 articles and
833, 032 comments.

3 Joint Learning for Importance Scoring

We first introduce a joint learning method that uses
graph-based regularization to simultaneously learn
two functions — a SENTENCE scorer and a COM-
MENT scorer — that predict the importance of in-
cluding an individual news article sentence or a par-
ticular user comment in the timeline.

We train the model on the aforementioned
NYT2013 dataset, where 20% of the articles and
their comments are reserved for parameter tuning.
Formally, the training data consists of a set of ar-
ticles D = {di}|D|−1i=0 . Each article di contains a
set of sentences xsdi = {xsdi ,j}

|sdi |−1
j=0 and a set of

associated comments xcdi = {xcdi ,k}
|cdi |−1
k=0 , where

|sdi | and |cdi | are the numbers of sentences and com-
ments for di. For simplicity, we use xs or xc to de-
note a sentence or a comment wherever there is no
ambiguity.

In addition, each article has a human-written ab-
stract. We use the ROUGE-2 (Lin and Hovy, 2003)
score of each sentence computed against the associ-
ated abstract as its gold-standard importance score.

1BBC comment volume is low, so we do not collect it.
2The datasets are available at http://www.cs.

cornell.edu/˜luwang/data.html.

Each comment is assigned a gold-standard value of
1.0 if it is an editor’s pick, or 0.0 otherwise.

The SENTENCE and COMMENT scorers rely on
two classifiers, each designed to handle the special
characteristics of news and user comments, respec-
tively; and a graph-based regularizing constraint that
encourages similarity between selected sentences
and comments. We describe each component below.

Article SENTENCE Importance. Each sentence
xs in a news article is represented as a k-dimensional
feature vector xs ∈ Rk, with a gold-standard label
ys. We denote the training set as a feature matrix
X̃s, with a label vector Ỹs. To produce the SEN-
TENCE scoring function fs(xs) = xs · ws, we use
ridge regression to learn a vector ws that minimizes
||X̃sws − Ỹs||22 + βs · ||ws||22. Features used in the
model are listed in Table 2. We also impose the
following position-based regularizing constraint to
encode the fact that the first sentence in a news arti-
cle usually conveys the most essential information:
λs ·

∑
di

∑
xsdi ,j

,j 6=0 ||(xsdi ,0 − xsdi ,j) ·ws − (ysdi ,0 −
ysdi ,j)||22 , where xsdi ,j is the j-th sentence in doc-
ument di. Term (xsdi ,0 − xsdi ,j) · ws measures the
difference in predicted scores between the first sen-
tence and any other sentence. This value is expected
be close to the true difference. We further construct
X̃′s to contain all difference vectors (xsdi ,0 − xsdi ,j),
with Ỹ′s as label difference vector. The objective
function to minimize becomes

Js(ws) =

||X̃sws − Ỹs||22 + λs · ||X̃′sws − Ỹ′s||22 + βs · ||ws||22
(1)

User COMMENT Importance. Similarly, each
comment xc is represented as an l−dimensional fea-
ture vector xc ∈ Rl, with label yc. Comments in the
training data are denoted with a feature matrix X̃c
with a label vector Ỹc. Likewise, we learn fc(xc) =
xc ·wc by minimizing ||X̃cwc − Ỹc||22 + βc · ||wc||22.
Features are listed in Table 3. We apply a pairwise
preference-based regularizing constraint (Joachims,
2002) to incorporate a bias toward editor’s picks:
λc ·
∑
di

∑
xcdi ,j

∈Edi ,xcdi ,k /∈Edi
||(xcdi ,j−xcdi ,k) ·wc−

1||22 , where Edi are the editor’s picks for di. Term
(xcdi ,j − xcdi ,k) · wc enforces the separation of ed-
itor’s picks from regular comments. We further
construct X̃′c to contain all the pairwise differences

1057



(xcdi ,j − xcdi ,k). Ỹ′c is a vector of same size as X̃′c
with each element as 1. Thus, the objective function
to minimize is:

Jc(wc) =

||X̃cwc − Ỹc||22 + λc · ||X̃′cwc − Ỹ′c||22 + βc · ||wc||22
(2)

Graph-Based Regularization. The regularizing
constraint is based on two mutually reinforcing hy-
potheses: (1) the importance of a sentence depends
partially on the availability of sufficient insightful
comments that touch on topics in the sentence; (2)
the importance of a comment depends partially on
whether it addresses notable events reported in the
sentences. For example, we want our model to bias
ws to predict a high score for a sentence with high
similarity to numerous insightful comments.

We first create a bipartite graph from sentences
and comments on the same articles, where edge
weights are based on the content similarity between
a sentence and a comment (TF-IDF similarity is
used). Let R̃ be an N ×M adjacency matrix, where
N and M are the numbers of sentences and com-
ments. Rsc is the similarity between sentence xs and
comment xc. We normalize R̃ by Q̃ = D̃−

1
2 R̃D̃′

− 12 ,
where D̃ and D̃′ are diagonal matrices: D̃ ∈ RN×N ,
Di,i =

∑M
j=1Ri,j ; D̃

′ ∈ RM×M , D′j,j =
∑N
i=1Ri,j .

The interplay between the two types of data is en-
coded in the following regularizing constraint:

Js,c(ws,wc) =

λsc ·
∑
di

∑
xs∈xsdi ,xc∈xcdi

Qxs,xc · (xs ·ws − xc ·wc)2

(3)

Full Objective Function. Thus, the full objective
function consists of the three parts discussed above:

J(ws,wc) = Js(ws) + Jc(wc) + Js,c(ws,wc) (4)

Furthermore, using the following notation,

X̃ =
[
X̃s 0
0 X̃c

]
Ỹ =

[
Ỹs
Ỹc

]
X̃′ =

[
X̃′s 0
0 X̃′c

]
Ỹ′ =

[
Ỹ′s
Ỹ′c

]

β̃ =
[
βsIk 0
0 βcIl

]
λ̃ =

[
λsI|X′s| 0

0 λcI|X′c|

]

L̃ =
[
λscI|Xs| −λscQ̃
−λscQ̃T λscI|Xc|

]
w =

[
ws
wc

]

we can show a closed form solution to Equation 4
as follows:

ŵ =

(X̃TL̃X̃ + X̃TX̃ + X̃′Tλ̃X̃′ + β̃)−1(X̃TỸ + X̃′Tλ̃Ỹ′)
(5)

Basic Features Social Features
- num of words - avg/sum frequency of
- absolute/relative position words appearing in comment
- overlaps with headline - avg/sum frequency of
- avg/sum TF-IDF scores dependency relations
- num of NEs appearing in comment

Table 2: Features used for sentence importance scoring.

Basic Features Readability Features
- num of words - Flesch-Kincaid Readability
- num of sentences - Gunning-Fog Readability
- avg num of words Discourse Features

per sentence - num/proportion of connectives
- num of NEs - num/proportion of hedge words
- num/proportion of Article Features

capitalized words - TF/TF-IDF simi with article
- avg/sum TF-IDF - TF/TF-IDF simi with comments
- contains URL - JS/KL divergence (div) with article
- user rating (pos/neg) - JS/KL div with comments
Sentiment Features
- num /proportion of positive/negative/neutral words (MPQA
(Wilson et al., 2005), General Inquirer (Stone et al., 1966))
- num /proportion of sentiment words

Table 3: Features used for comment importance scoring.

4 Timeline Generation

Now we present an optimization framework for
timeline generation. Formally, for each day, our sys-
tem takes as input a set of sentences Vs and a set of
comments Vc to be summarized, and the (automati-
cally generated) timeline T (represented as threads)
for days prior to the current day. It then identifies
a subset S ⊆ Vs as the article summary and a subset
C ⊆ Vc as the comment summary by maximizing the
following function:

Z(S,C; T ) = Squal(S; T )+Cqual(C)+δX (S,C) (6)

where Squal(S; T ) measures the quality of the article
summary S in the context of the historical timeline
represented as event threads T ; Cqual(C) computes
the quality of the comment summary C; and X (S,C)
estimates the connectivity between S and C.

We solve this maximization problem using an al-
ternating optimization algorithm which is outlined

1058



in Section 4.4. In general, we alternately search
for a better article summary S with hill climbing
search and a better comment summary C with Ford-
Fulkerson algorithm until convergence.

In the rest of this section, we first describe an
entity-centered event threading algorithm to con-
struct event threads T which are used to boost article
timeline continuity. Then we explain how to com-
pute Squal(S; T ) and Cqual(C) in Section 4.2, fol-
lowed by X (S,C) in Section 4.3.

4.1 Entity-Centered Event Threading

We present an event threading process where each
thread connects sequential events centered on a
set of relevant entities. For instance, the following
thread connects events about Obama’s action
towards the annexation of Crimea by Russia:
Day 1: Obama declared sanctions on Russian officials.
Day 2: President Obama warned Russian.
Day 3: Obama urges Russian to move back its troops.
Day 4: Obama condemns Russian aggression in Ukraine.

We first collect relation extractions as (entity, re-
lation, entity) triples from OLLIE (Mausam et al.,
2012), a dependency relation based open informa-
tion extraction system. We retain extractions with
confidence scores higher than 0.5. We further de-
sign syntactic patterns based on Fader et al. (2011)
to identify relations expressed as a combination of a
verb and nouns. Each relation contains at least one
event-related word (Ritter et al., 2012).

The entity-centered event threading algorithm
works as follows: on the first day, each sentence in
the summary becomes an individual cluster; there-
after, each sentence in the current day’s article sum-
mary either gets attached to an existing thread or
starts a new thread. The updated threads then be-
come the input to next day’s summary generation
process. On day n, we have a set of threads T = {τ :
s1, s2, · · · , sn−1} constructed from previous n − 1
days, where si represents the set of sentences at-
tached to thread τ from day i. The cohesion between
a new sentence s ∈ S and a thread τ is denoted as
cohn(s, τ). s is attached to τ̂ if there exists τ̂ =
maxτ∈T cohn(s, τ) and cohn(s, τ̂) > 0.0. Otherwise,
s becomes a new thread. We define cohn(s, τ) =
minsi∈τ,si 6=∅ tfsimi(si, s), where tfsimi(si, s) mea-
sures the TF similarity between si and s. We con-

sider unigrams/bigrams/trigrams generated from the
entities of our event extractions.

4.2 Summary Quality Measurement
Recall that we learned two separate importance scor-
ing functions for sentences and comments, which
will be denoted here as imps(s) and impc(c). With
an article summary S and threads T = {τi}, the ar-
ticle summary quality function Squal(S; T ) has the
following form:

Squal(S; T ) =
∑
s∈S imp(s)

+θcov
∑
s′∈Vs min(

∑
s∈S tfidf(s, s

′), α
∑
ŝ∈Vs tfidf(ŝ, s

′))

+ θcont
∑
τ∈T maxsk∈S cohn(sk, τ)

tfidf(·, ·) is the TF-IDF similarity function.
Squal(S; T ) captures three desired qualities of an ar-
ticle summary: importance (first item), coverage
(second item), and the continuity of the current sum-
mary to previously generated summaries. The cov-
erage function has been used to encourage summary
diversity and reduce redundancy (Lin and Bilmes,
2011; Wang et al., 2014). The continuity function
considers how well article summary S can be at-
tached to each event thread, thus favors summaries
that can be connected to multiple threads.

Parameters θcov and α are tuned on multi-
document summarization dataset DUC 2003 (Over
and Yen, 2003). Experiments show that system per-
formance peaks and is stable for θcont ∈ [1.0, 5.0].
We thus fix θcont to 1.0. We discard sentences with
more than 80% of content words covered by histor-
ical summaries. We use BASIC to denote a system
that only optimizes on importance and coverage (i.e.
first two items in Squal(S; T )). The system optimiz-
ing Squal(S; T ) is henceforth called THREAD.

The comment summary quality function simply
takes the form Cqual(C) =

∑
c∈C impc(c).

4.3 Connectivity Measurement
We encode two objectives in the connectivity func-
tion X (S,C): (1) encouraging topical cohesion (i.e.
connectivity) between article summary and com-
ment summary; and (2) favoring comments that
cover diversified events.

Let conn(s, c) measure content similarity between
a sentence s ∈ S and a comment c ∈ C. Connectivity
between article summary S and comment summary
C is computed as follows. We build a bipartite graph
G between S and C with edge weight as conn(s, c).

1059



We then find an edge setM, the best matching of G.
X (S,C) is defined as the sum over edge weights in
M, i.e. X (S,C) = ∑e∈M weight(e). An example is
illustrated in Figure 2.

C0: The “Crimean Parliament”, 
headed by an ethnic Russian 
separatist, has voted for Crimea to 
be annexed into Russia…

S0: The Crimean 
parliament officially 
declared independence 
and requested full 
accession to the 
Russian Federation.

Article Summary Comment Summary

C1: The Obama administration has 
warned of "increasingly harsh 
sanctions", but it is unlikely that 
Europe will comply…

C2: Sanctions are effective and if 
done in unison with the EU…

S1: Obama declared 
sanctions on Russian 
officials considered 
responsible for the crisis.

0.8

0.1
0.1

0.3
0.8

0.5

Figure 2: An example on computing the connectivity
between an article summary (left) and a comment sum-
mary (right) via best matching in bipartite graph. Num-
ber on each edge indicates the content similarity between
a sentence and a comment. Solid lines are edges in the
best matching graph. For this example, the connectivity
X (S,C) is 0.8 + 0.8 = 1.6.

We consider two options for conn(s, c). One is
lexical similarity which is based on TF-IDF vec-
tors. Another is semantic similarity. Let Rs =
{(as, rs, bs)} and Rc = {(ac, rc, bc)} be the sets of
dependency relations in s and c. conn(s, c) is calcu-
lated as:∑

(as,rs,bs)∈Rs max(ac,rc,bc)∈Rc
rs=rc

simi(as, ac)× simi(bs, bc)
where simi(·, ·) is a word similarity function.
We experiment with shortest path based similar-
ity defined on WordNet (Miller, 1995) and Co-
sine similarity with word vectors trained on Google
news (Mikolov et al., 2013). Systems using the three
metrics that optimize Z(S,C; T ) are henceforth
called THREAD+OPTTFIDF, THREAD+OPTWordNet
and THREAD+OPTWordVec.

4.4 An Alternating Optimization Algorithm
To maximize the full objective function Z(S,C; T ),
we design a novel alternating optimization algorithm
(Alg. 1) where we alternately find better S and C.

We initialize S0 by a greedy algorithm (Lin and
Bilmes, 2011) with respect to Squal(S; T ). Notice
that Squal(S; T ) is a submodular function, so that
the greedy solution is a 1 − 1/e approximation to
the optimal solution of Squal(S; T ). Fixing S0, we
model the problem of finding C0 that maximizes
Cqual(C) + δX (S0, C) as a maximum-weight bipar-

tite graph matching problem. This problem can be
reduced to a maximum network flow problem, and
then be solved by Ford-Fulkerson algorithm (de-
tails are discussed in (Kleinberg and Tardos, 2005)).
Thereafter, for each iteration, we alternately find a
better St with regard to Squal(S; T ) + δX (S,Ct−1)
using hill climbing, and an exact solution Ct to
Cqual(C)+δX (St, C) with Ford-Fulkerson algorithm.
Iteration stops when the increase of Z(S,C) is below
threshold � (set to 0.01). System performance is sta-
ble when we vary δ ∈ [1.0, 10.0], so we set δ = 1.0.

Input : sentences Vs, comments Vc, threads T , δ,
threshold �, functions Z(S,C; T ),
Squal(S; T ), Cqual(C), X (S,C)

Output: article summary S, comment summary C
/* Initialize S and C by greedy algorithm

and Ford-Fulkerson algorithm */
S0 ←maxS Squal(S; T );
C0 ← maxC Cqual(C) + δX (S0, C);
t← 1;
∆Z ←∞;
while ∆Z > � do

/* Step 1: Hill climbing algorithm */
St ← maxS Squal(S; T ) + δX (S,Ct−1);
/* Step 2: Ford-Fulkerson algorithm */
Ct ← maxC Cqual(C) + δX (St, C);
∆Z = Z(St, Ct; T )−Z(St−1, Ct−1; T );
t← t+ 1;

end
Algorithm 1: Generate article summary and com-
ment summary for a given day via alternating opti-
mization .

Algorithm 1 is guaranteed to find a solution at
least as good as S0 and C0. It progresses only if Step
1 finds St that improves upon Z(St−1, Ct−1; T ), and
Step 2 finds Ct where Z(St, Ct; T ) ≥ Z(St, Ct−1; T ).

5 Experimental Results

5.1 Evaluation of SENTENCE and COMMENT
Importance Scorers

We test importance scorers (Section 3) on single
document sentence ranking and comment ranking.

For both tasks, we compare with two previ-
ous systems on joint ranking and summarization of
news articles and tweets. Yang et al. (2011) em-
ploy supervised learning based on factor graphs to
model content similarity between the two types of
data. We use the same features for this model.
Gao et al. (2012) summarize by including the
complementary information between articles and

1060



tweets, which is estimated by an unsupervised topic
model.3 We also consider two state-of-the-art
rankers: RankBoost (Freund et al., 2003) and Lamb-
daMART (Burges, 2010). Finally, we use a position
baseline that ranks sentences based on their position
in the article, and a rating baseline that ranks com-
ments based on positive user ratings.

We evaluate using normalized discounted cumu-
lative gain at top 3 returned results (NDCG@3).
Sentences are considered relevant if they have
ROUGE-2 scores larger than 0.0 (computed against
human abstracts), and comments are considered rel-
evant if they are editor’s picks.4 Figure 3 demon-
strates that our joint learning model uniformly out-
performs all the other comparisons for both rank-
ing tasks. In general, supervised learning based ap-
proaches (e.g. our method, Yang et al. (2011), Rank-
Boost, and LambdaMART) produce better results
than unsupervised method (e.g. Gao et al. (2012)).5

Figure 3: Evaluation of sentence and comment ranking
on the four datasets by using normalized discounted cu-
mulative gain at top 3 returned results (NDCG@3). Our
joint learning based approach uniformly outperforms all
the other comparisons.

5.2 Leveraging User Comments
In this section, we test if our system can leverage
comments to produce better article-based summaries
for event timelines. We collect gold-standard time-
lines for each of the four events from the corre-
sponding Wikipedia page(s), NYT topic page, or
BBC news page.

We consider two existing timeline creation sys-
tems that only utilize news articles, and a timeline
generated from single-article human abstracts: (1)
CHIEU AND LEE (2004) select sentences with high

3We thank Zi Yang and Peng Li for providing the code.
4We experiment with all articles for sentence ranking, and

NYT comments (with editor’s picks) for comment ranking.
5Similar results are obtained with mean reciprocal rank.

“interestingness” and “burstiness” using a likelihood
ratio test to compare word distributions of sentences
with articles in neighboring days. (2) YAN ET AL.
(2011) design an evolutionary summarization sys-
tem that selects sentences based on on coverage, co-
herence, and diversity. (3) We construct a timeline
from the human ABSTRACTs provided with each ar-
ticle: we sort them chronologically according to arti-
cle timestamps and add abstract sentences into each
daily summary until reaching the word limit.

We test on five variations of our system. The
first two systems generate article summaries
with no comment information by optimizing
Squal(S; T ) using a greedy algorithm: BASIC ignores
event threading; THREAD considers the threads.
THREAD+OPTTFIDF, THREAD+OPTWordNet and
THREAD+OPTWordVec (see Section 4.3) leverage
user comments to generate article summaries as
well as comment summaries based on alternating
optimization of Equation 3. Although comment
summaries are generated, they are not used in the
evaluation.

For all systems, we generate daily article sum-
maries of at most 100 words, and select 5 com-
ments for the corresponding comment summary. We
employ ROUGE (Lin and Hovy, 2003) to automat-
ically evaluate the content coverage (in terms of
ngrams) of the article-based timelines vs. gold-
standard timelines. ROUGE-2 (measures bigram
overlap) and ROUGE-SU4 (measures unigram and
skip-bigrams separated by up to four words) scores
are reported in Table 4. As can be seen, under the al-
ternating optimization framework, our systems, em-
ploying both articles and comments, consistently
yield better ROUGE scores than the three baseline
systems and our systems that do not leverage com-
ments. Though constructed from single-article ab-
stracts, baseline ABSTRACT is found to contain re-
dundant information and thus limited in content cov-
erage. This is due to the fact that different media
tend to report on the same important events.

5.3 Evaluating Socially-Informed Timelines

We evaluate the full article+comment-based time-
lines on Amazon Mechanical Turk. Turkers are
presented with a timeline consisting of five con-
secutive days’ article summaries and four vari-
ations of the accompanying comment summary:

1061



MH370 Ukraine Israel-Gaza NSA
R-2 R-SU4 R-2 R-SU4 R-2 R-SU4 R-2 R-SU4

CHIEU AND LEE 6.43 10.89 4.64 8.87 3.38 7.32 6.14 9.73
YAN ET AL. 6.37 10.35 4.57 8.67 2.39 5.78 3.99 7.73
ABSTRACT 6.16 10.62 3.85 8.40 2.21 5.42 7.03 8.65
- Greedy Algorithm
BASIC 6.59 9.80 5.31 9.23 3.15 6.20 3.81 7.58
THREAD 6.55 10.86 5.73 9.75 3.16 6.16 6.29 10.09
- Alternating Optimization (leveraging comments)
THREAD+OPTTFIDF 8.74 11.63 9.10 12.59 3.78 6.45 8.07 10.31
THREAD+OPTWordNet 8.73 11.87 8.67 12.10 4.11 6.64 8.63 11.12
THREAD+OPTWordVec 9.29 11.63 9.16 12.72 3.75 6.38 8.29 10.36

Table 4: ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4)
scores (multiplied by 100) for different timeline gener-
ation approaches on four event datasets. Systems that
statistically significantly outperform the three baselines
(p < 0.05, paired t−test) are in italics. Numbers in bold
are the highest score for each column.

RANDOMly selected comments, USER’S-PICKS
(ranked by positive user ratings), randomly selected
EDITOR’S-PICKS and timelines produced by the
THREAD+OPTWordVec version of OUR SYSTEM. We
also include one noisy comment summary (i.e. irrel-
evant to the question) to avoid spam. We display two
comments per day for each system.6

Turkers are asked to rank the comment summary
variations according to informativeness and insight-
fulness. For informativeness, we ask the Turkers
to judge based only on knowledge displayed in the
timeline, and to rate each comment summary based
on how much relevant information they learn from it.
For insightfulness, Turkers are required to focus on
insights and valuable opinions. They are requested
to leave a short explanation of their ranking.

15 five-day periods are randomly selected. We so-
licit four distinct Turkers located in the U.S. to eval-
uate each set of timelines. An inter-rater agreement
of Krippendorff’s α of 0.63 is achieved for infor-
mativeness ranking and α is 0.50 for insightfulness
ranking.

Table 5 shows the percentage of times a partic-
ular method is selected as producing the best com-
ment portion of the timeline, as well as the micro-
average rank of each method, for both informative-
ness and insightfulness. Our system is selected as
the best in 66.7% of the evaluations for informative-
ness and 51.7% for insightfulness. In both cases, we
statistically significantly outperform (p < 0.05 us-
ing a Wilcoxon signed-rank test) the editor’s-picks

6For our system, we select the two comments with highest
importance scores from the comment summary.

Informativeness Insightfulness
% Best Avg Rank % Best Avg Rank

Random 1.7% 3.67 3.3% 3.58
User’s-picks 5.0% 2.83 15.0% 2.55
Editor’s-picks 26.7% 2.05 30.0% 2.22
Our system 66.7% 1.45 51.7% 1.65

Table 5: Human evaluation results on the comment por-
tion of socially-informed timelines. Boldface indicates
statistical significance vs. other results in the same col-
umn using a Wilcoxon signed-rank test (p < 0.05). On
average, the output from our system is ranked higher than
all other alternatives.

and user’s-picks. Turkers’ explanations indicate that
they prefer our comment summaries mainly because
they are “very informative and insightful to what
was happening”, and “show the sharpness of the
commenter”. Turkers sometimes think the sum-
maries randomly selected from editor’s-picks “lack
connection”, and characterize user’s-picks as “the
information was somewhat limited”.

Figure 4 shows part of the timeline generated by
our system for the Ukraine crisis.

Article Summary Comment Summary
2014-03-17 Obama administra-
tion froze the U.S. assets of seven
Russian officials, while similar
sanctions were imposed on four
Ukrainian officials. . . .

Theodore Roosevelt said that the
worst possible thing you can do in
diplomacy is “soft hitting”. That
is what the US and the EU are
doing in these timid “sanctions”
against people without any over-
seas accounts. . .

2014-03-18 Ukraine does not rec-
ognize a treaty signed in Moscow
on Tuesday making its Crimean
peninsula a part of Russia. . .

Though there were many in
Crimea who supported annexa-
tion, there were certainly some
who did not. what about those
people?. . .

2014-03-19 The head of NATO
warned on Wednesday that Rus-
sian President Vladimir Putin
may not stop with the annexation
of Crimea . . .

If you look at a real map , Crimea
is an island and has always been
more connected to Russia than to
Ukraine. . .

2014-03-20 The United States on
Thursday expanded its sanctions
on Russians. . .

The US and EU should follow up
economic sanctions with concrete
steps to strengthen NATO. . .

Figure 4: A snippet of timeline generated by our sys-
tem THREAD+OPTWordVec for the Ukraine crisis. Due to
space limitations, we only display partial summaries.

5.4 Human Evaluation of Event Threading
Here we evaluate on the utility of event threads for
high-level information access guidance: can event
threads allow users to easily locate and absorb in-
formation with a specific interest in mind?

We first sample a 10-day timeline for each dataset
from those produced by the THREAD+OPTWordVec

1062



variation of our system. We designed one ques-
tion for each timeline. Sample questions are: “de-
scribe the activities for searching for the missing
flight MH370”, and “describe the attitude and action
of Russian Government on Eastern Ukraine”. We re-
cruited 10 undergraduate and graduate students who
are native speakers of English. Each student first
read one question and its corresponding timeline for
5 minutes. The timeline was then removed, and the
student wrote down an answer for the question. We
asked each student to answer the question for each
of four timelines (one for each event dataset). Two
timelines are displayed with threads, and two with-
out threads. We presented threads by adding a thread
number in front of each sentence.

We then used Amazon Mechanical Turk to evalu-
ate the informativeness of students’ answers. Turk-
ers were asked to read all 10 answers for the same
question, with five answers based on timelines with
threads and five others based on timelines without
threads. After that, they rated each answer with an
informativeness score on a 1-to-5 rating scale (1 as
“not relevant to the query”, and 5 as “very informa-
tive”). We also added two quality control questions.
Table 6 shows that the average rating for answers
written after reading timelines with threads is 3.29
(43% are rated≥ 4), higher than the 2.58 for the time-
lines with no thread exhibited (30% are rated ≥ 4).
Answer Type Avg ± STD Rated 5 (%) Rated 4 (%)
No Thread 2.58 ± 1.20 7% 23%
With Threads 3.29 ± 1.28 17% 26%

Table 6: Human evaluation on the informativeness of
answers written after reading timelines with threads vs.
with no thread. Answers written with access to threads
are rated higher (3.29) than the ones with no thread
(2.58).

6 Related Work

There is a growing interest in generating article
summaries informed by social context. Existing
work focuses on learning users’ interests from com-
ments and incorporates the learned information into
a news article summarization system (Hu et al.,
2008). Zhao et al. (2013) instead estimate word dis-
tributions from tweets, and bias a Page Rank algo-
rithm to give higher restart probability to sentences
with similar distributions. Generating tweet+article
summaries has been recently investigated in Yang et

al. (2011). They propose a factor graph to allow sen-
tences and tweets to mutually reinforce each other.
Gao et al. (2012) exploit a co-ranking model to iden-
tify sentence-tweet pairs with complementary infor-
mation estimated from a topic model. These efforts
handle a small number of documents and tweets,
while we target a larger scale of data.

In terms of timeline summarization, the Chieu
and Lee (2004) system ranks sentences according
to “burstiness” and “interestingness” estimated by a
likelihood ratio test. Yan et al. (2011) explore an op-
timization framework that maximizes the relevance,
coverage, diversity, and coherence of the timeline.
Neither system has leveraged the social context. Our
event threading algorithm is also inspired by work
on topic detection and tracking (TDT) (Allan et al.,
1998), where efforts are made for document-level
link detection and topic tracking. Similarly, Nalla-
pati et al. (2004) investigate event threading for ar-
ticles, where they predict linkage based on causal
and temporal dependencies. Shahaf et al. (2012) in-
stead seek for connecting articles into one coherent
graph. To the best of our knowledge, we are the first
to study sentence-level event threading.

7 Conclusion

We presented a socially-informed timeline gener-
ation system, which constructs timelines consist-
ing of article summaries and comment summaries.
An alternating optimization algorithm is designed
to maximize the connectivity between the two sets
of summaries as well as their importance and infor-
mation coverage. Automatic and human evaluations
showed that our system produced more informative
timelines than state-of-the-art systems. Our com-
ment summaries were also rated as very insightful.

Acknowledgments

We thank John Hessel, Lillian Lee, Moontae Lee,
David Lutz, Karthik Raman, Vikram Rao, Yiye
Ruan, Xanda Schofield, Adith Swaminathan, Chen-
hao Tan, Bishan Yang, other members of Cornell
NLP group, and the NAACL reviewers for valuable
suggestions and advice on various aspects of this
work. This work was supported in part by DARPA
DEFT Grant FA8750-13-2-0015.

1063



References

J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic detection and tracking pi-
lot study: Final report. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 194–218, Lansdowne, VA,
USA, February. 007.

Christopher J. C. Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An Overview. Technical
report, Microsoft Research.

Angel X. Chang and Christopher Manning. 2012.
Sutime: A library for recognizing and normaliz-
ing time expressions. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).

Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ’04, pages 425–432, New York,
NY, USA. ACM.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’11, pages 1535–1545, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. J. Mach. Learn. Res., 4:933–
969, December.

Wei Gao, Peng Li, and Kareem Darwish. 2012. Joint
topic modeling for event summarization across news
and social media streams. In Proceedings of the
21st ACM International Conference on Information
and Knowledge Management, CIKM ’12, pages 1173–
1182, New York, NY, USA. ACM.

Meishan Hu, Aixin Sun, and Ee-Peng Lim. 2008.
Comments-oriented document summarization: Under-
standing documents with readers’ feedback. In Pro-
ceedings of the 31st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ’08, pages 291–298, New York,
NY, USA. ACM.

Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-

edge Discovery and Data Mining, KDD ’02, pages
133–142, New York, NY, USA. ACM.

Jon Kleinberg and Eva Tardos. 2005. Algorithm De-
sign. Addison-Wesley Longman Publishing Co., Inc.,
Boston, MA, USA.

Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ’11, pages 510–520,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, pages 71–78.

Zongyang Ma, Aixin Sun, Quan Yuan, and Gao Cong.
2012. Topic-driven reader comments summarization.
In Proceedings of the 21st ACM International Con-
ference on Information and Knowledge Management,
CIKM ’12, pages 265–274, New York, NY, USA.
ACM.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language process-
ing toolkit. In Proceedings of 52nd Annual Meeting of
the Association for Computational Linguistics: System
Demonstrations, pages 55–60, Baltimore, Maryland.
Association for Computational Linguistics.

Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ’12, pages 523–
534, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

George A. Miller. 1995. Wordnet: A lexical database for
english. Commun. ACM, 38(11):39–41, November.

Ramesh Nallapati, Ao Feng, Fuchun Peng, and James Al-
lan. 2004. Event threading within news topics. In
Proceedings of the Thirteenth ACM International Con-
ference on Information and Knowledge Management,
CIKM ’04, pages 446–453, New York, NY, USA.
ACM.

P. Over and J. Yen. 2003. An introduction to DUC 2003:
Intrinsic evaluation of generic news text summariza-
tion systems.

1064



Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter.
In Proceedings of the 18th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, KDD ’12, pages 1104–1112, New York, NY,
USA. ACM.

Dafna Shahaf, Carlos Guestrin, and Eric Horvitz. 2012.
Trains of thought: Generating information maps. In
Proceedings of the 21st International Conference on
World Wide Web, WWW ’12, pages 899–908, New
York, NY, USA. ACM.

Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press, Cambridge, MA.

Lu Wang, Hema Raghavan, Claire Cardie, and Vittorio
Castelli. 2014. Query-focused opinion summarization
for user-generated content. In Proceedings of COL-
ING 2014, the 25th International Conference on Com-
putational Linguistics: Technical Papers, pages 1660–
1669, Dublin, Ireland, August. Dublin City University
and Association for Computational Linguistics.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ’05,
pages 347–354, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011. Evolution-
ary timeline summarization: A balanced optimization
framework via iterative substitution. In Proceedings of
the 34th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, SI-
GIR ’11, pages 745–754, New York, NY, USA. ACM.

Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and
Juanzi Li. 2011. Social context summarization. In
Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ’11, pages 255–264, New York,
NY, USA. ACM.

Xin Wayne Zhao, Yanwei Guo, Rui Yan, Yulan He, and
Xiaoming Li. 2013. Timeline generation with so-
cial attention. In Proceedings of the 36th Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, SIGIR ’13, pages
1061–1064, New York, NY, USA. ACM.

1065


