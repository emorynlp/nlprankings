










































CIST System Report for ACL MultiLing 2013 - Track 1: Multilingual Multi-document Summarization


Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 39–44,
Sofia, Bulgaria, August 9 2013. c©2013 Association for Computational Linguistics

CIST System Report for ACL MultiLing 2013 

-- Track 1: Multilingual Multi-document Summarization 

 

 

Lei Li, Wei Heng, Jia Yu, Yu Liu, Shuhong Wan 

Center for Intelligence Science and Technology (CIST),  

School of Computer Science and Technology, 

Beijing University of Posts and Telecommunications (BUPT), China 

leili@bupt.edu.cn 

 

  

 

Abstract 

This report provides a description of the meth-

ods applied in CIST system participating ACL 

MultiLing 2013. Summarization is based on 

sentence extraction. hLDA topic model is 
adopted for multilingual multi-document mod-

eling. Various features are combined to evalu-

ate and extract candidate summary sentences. 

1 Introduction 

CIST system has participated Track 1: Multilin-

gual Multi-document Summarization in ACL 

MultiLing 2013 workshop. It could deal with all 

ten languages: Arabic, Chinese, Czech, English, 

French, Greek, Hebrew, Hindi, Romanian and 

Spanish. It summarizes every topic containing 10 

texts and generates a summary in plain text, 

UTF8 encoding, less than 250 words.  

2 System Design 

There have been many researches about multi-

document summarization, (Wan et al., 2006; He 

et al., 2008; Flore et al., 2008; Bellemare et al., 

2008; Conroy and Schlesinger, 2008; Zheng and 

Takenobu, 2009; Louis and Nenkova, 2009; 

Long et al., 2009; Lin and Chen, 2009; Gong et 

al., 2010; Darling, 2010; Kumar et al., 2010; 

Genest and Lapalme, 2010; Jin et al., 2010; Ken-

nedy et al., 2010; Zhang et al., 2011), but less 

about multilingual multi-document summariza-

tion (Leuski et al., 2003; Liu et al., 2011; Conroy 

et al., 2011; Hmida and Favre, 2011; Das and 

Srihari, 2011; Steinberger et al., 2011; Saggion, 

2011; El-Haj et al., 2011).  

This system must be applicable for unlimited 

topics, we couldn’t use topic knowledge. Differ-

ent topic has different language styles, so we use 

sentence as the processing unit and summariza-

tion method based on sentence extraction. It must 

also be available for different languages, we 

couldn’t use much specific knowledge for all 

languages except one or two we understand. We 

refer to a statistical method, hLDA (hierarchical 

Latent Dirichlet Allocation (LDA)). 

LDA has been widely applied. (Arora and 

Balaraman, 2008; Krestel et al., 2009). Some 

improvements have been made. (Griffiths et al., 

2005; Blei and Lafferty, 2006; Wang and Blei, 

2009). One is to relax its assumption that topic 

number is known and fixed. Teh et al. (2006) 

provided an elegant solution. Blei et al. (2010) 

extended it to exploit the hierarchical tree struc-

ture of topics, hDLA, which is unsupervised 

method in which topic number could grow with 

the data set automatically. There’s no relations 

between topics in LDA (Blei, 2003), but hLDA 

could organize topics into a hierarchy, in which 

higher level topics are more abstractive. This 

could achieve a deeper semantic model similar 

with human mind and is especially helpful for 

summarization. Celikyilmaz (2010) provided a 

multi-document summarization method based on 

hLDA with competitive results. However, it has 

the disadvantage of relying on ideal summaries. 

To avoid this, the innovation of our work is 

completely dependent on data and hierarchy to 

extract candidate summary sentences. 

Figure 1 and 2 show the framework for ten 

languages. Since Chinese Hanzi is different from 

other languages, we treat it with special pro-

cessing. But the main modules are the same. The 

kernel one is constructing an hLDA model
1
. It’s 

language independent.  

                                                 
1 http://www.cs.princeton.edu/~blei/topicmodeling.html 

39



 
Figure 1: framework for nine languages (no Chinese) 

 

 
Figure 2: framework for Chinese 

3 Text Pre-processing 

There are some unified pre-processing steps for 

all languages and a special step for Chinese.  

3.1 Merging Documents 

We treat multi-document together, so we firstly 

combine them into a big text. As to Chinese, we 

combine and delete empty lines. As to other nine 

languages, we do this when we split sentences. 

3.2 Splitting Sentences 

We split sentences to get the processing unit. 

There are two lines of title and date ending with 

no punctuation mark. We add a full stop our-

selves to avoid them being connected with the 

first sentence. For Chinese, we split sentences 

according to ending punctuation marks, while for 

other nine languages, the full stop “.” could have 

other functions. We adopt machine learning 

method
2
. After some experiments, we choose 

Support Vector Machine model for English and 

French, Naïve Bayes model for other 7 languages. 

                                                 
2 https://code.google.com/p/splitta/ 

3.3 Removing Stop Words 

We add ICTCLAS
3
 word segmentation to Chi-

nese to make all languages have the same word 

separator. Then we could obtain words easily, 

among which are some stop words. We construct 

stop lists. For English and Chinese, the stop list 

contains punctuation marks and some functional 

words, while for other languages, it contains 

punctuation marks, which could unified the 

whole process easily although generally we do 

not treat punctuation marks as words. At the 

same time, all capitalized characters are changed 

to lower case.  

3.4 Generating Input File for hLDA 

We build a dictionary for remaining words, 

which are sorted according to frequency. The 

more frequent words are located before the less 

frequent ones. This is a mapping from word to a 

number varying from 1 to dictionary size. Finally 

we generate an input file for hLDA, in which 

each line represents a sentence, in the following 

form: 

[number of words in the sentence] [word-

NumberA]:[local frequencyA] [word-

NumberB]:[local frequencyB]... 

Figure 3 shows an example. As we can see 

that now it’s language independent. 

 
Figure 3: hLDA input file 

4 hLDA Topic Modeling 

Given a collection of sentences in the input file, 

we wish to discover common usage patterns or 

topics and organize them into a hierarchy. Each 

node is associated with a topic, which is a distri-

bution across words. A sentence is generated by 

choosing a path from the root to a leaf, repeated-

ly sampling topics along that path, and sampling 

the words from the selected topics. Sentences 

sharing the same path should be similar to each 

other because they share the same sub-topics. All 

sentences share the topic distribution associated 

with the root node.  

As to this system, we set hierarchy depth to 3, 

because we have found out in former experi-

ments that 2 is too simple, and 4 or bigger is too 

complex for the unit of sentence. 

                                                 
3 http://www.nlpir.org/download/ICTCLAS2012-SDK-

0101.rar 

40



4.1 Hierarchy Evaluation 

In order to make sure that a hierarchy is good, 

we need to evaluate its performance. The best 

method is human reading, but it’s too laborious 

to browse all topics and all languages. In fact, we 

could not understand all ten languages at all. So 

we build another simpler and faster evaluation 

method based on numbers. According to former 

empirical analysis, if a hierarchy has more than 4 

paths and the sentence numbers for all paths ap-

pear in balanced order from bigger to smaller, 

and the sentences in bigger paths could occupy 

70-85% in all sentences, then we could possibly 

infer that this hierarchy is good. 

4.2 Parameter Setting 

When facing a new corpus, we could hardly set 

the parameters automatically either by human or 

machine. There is a choice of sampling. We tried 

it for all languages with 100000 iterations. But 

the results are poor, even in the worst case each 

sentence is set to a single path.  Thus we give up 

sampling and try to set the parameters by human. 

We begin with Chinese because it seems to be 

the most difficult case. We randomly choose two 

topics for original testing and set some parame-

ters according to former experience. Then we 

evaluate the result using method in 4.1. If it’s not 

good, we go on to adjust the settings until we 

obtain a satisfactory result. The satisfied settings 

are then used originally for the whole corpus. 

Table 1 shows the details. 

Parameter Setting 

ETA 1.2   0.5 0.05 

GAM 1.0   1.0 

GEM_MEAN 0.5 

GEM_SCALE 100 

SCALING_SHAPE 1.0 

SCALING_SCALING 0.5 

SAMPLE_ETA 0 

SAMPLE_GAM 0 

Table 1: Original parameter settings 

Language Topic 

English M006 

Hebrew M001 M006 

Romanian M002 

Spanish M003 

Chinese M004 M006 

Table 2: original bad result 

After running the whole corpus, we evaluate 

the results again. We found out that for most cas-

es, the hierarchy is good, but there are some cas-

es not so good, as shown in Table 2. So one set 

of parameter settings could not deal with all lan-

guages and topics successfully. The reason may 

be that different language and different topic 

must have different inherent features. 

4.3 Parameter Adjustment 

We analyze the bad results and try to adjust the 

settings. For instance, in English M006, there are 

only two paths indicating that the tree is too clus-

tered. Parameter ETA should be reduced to sepa-

rate more sub-topics. But too small ETA may 

lead to hLDA failure without level assignment 

result in limited iterations. So we also adjust 

GEM to get closer to the prior explanation of 

corpus. In some case, the numbers are assigned 

too much to the former big paths, then we should 

adjust SCALING parameters to separate some 

numbers to the smaller paths. For the bad cases 

in Table 2, we finally use the settings in Table 3. 

Parameter Setting 

ETA 5.2  0.005  0.0005 

GAM 1.0   1.0 

GEM_MEAN 0.35 

GEM_SCALE 100 

SCALING_SHAPE 2.0 

SCALING_SCALING 1.0 

SAMPLE_ETA 0 

SAMPLE_GAM 0 

Table 3: Adjusted parameter settings 

Figure 4 shows an example of the modeling 

result of M004 in English.  

 
Figure 4: hLDA result example 

5 Summary Generation 

5.1 Sentence Evaluation 

In the hLDA result, sentences are clustered into 

sub-topics in a hierarchical tree. A sub-topic is 

more important if it contains more sentences. 

Trivial sub-topics containing only one or two 

sentences could be neglected. Final summary 

41



should cover those most important sub-topics 

with their most representative sentences. We 

evaluate the sentence importance in a sub-topic 

considering three features. 

1) Sentence coverage, which means that how 

much a sentence could contain words appearing 

in more sentences for a sub-topic. We consider 

sentence coverage of each word in one sentence. 

The sentence weight is calculated as eq.(1). 

||

)(||

1

s

n

wnum

S

s

i

is

tf




       
(1) 

Where wi is the ith word in sentence s, nums(wi) 

is the number of sentences that wi covers, | s | is 

the number of words in the sentence, and n is the 

total number of all sentences. 

2) Word Abstractive level. hLDA constructs a 

hierarchy by positioning all sentences on a three-

level tree. Level 0 is the most abstractive one, 

level 2 is the most specific one, and level 1 is 

between them. We evaluate the sentence abstrac-

tive feature as eq.(2). 

 (2) 

Where num(W0), num(W1), num(W2) are 

numbers of level 0, 1 and 2 words respectively in 

the sentence. There are three parameters: a, b and 

c, which are used to control the weights for 

words in different levels. Although we hope the 

summary to be as abstractive as possible, there is 

really some specific information we also want. 

For instance, earthquake news needs specific 

information about death toll and money lost.  

3) Named entity. We consider the number of 

named entities in one sentence. This time we on-

ly have time to use Stanford’s named entity 

recognition toolkit
4
, which could identify English 

person, address and institutional names. If one 

sentence contains more entities, then it has a high 

priority to be chosen as candidate summary sen-

tence. Let Sn be the number of named entity cat-

egories in one sentence. For example, if one sen-

tence has only person names, then Sn is 1; else if 

it also has address information, then Sn is 2; else 

if it contains all three categories, then Sn is 3. 

At last, we calculate sentence score S as eq. (3, 

4), where d, e and f are feature weights: 

English:        (3) 

Others:                       (4) 

After experiments, we set {a, b, c, d, e, f} to 

{0.3, 1, 0.3, 2, 1, 0.05} for English, {a, b, c, d, e} 

                                                 
4 http://nlp.stanford.edu/software/CRF-NER.shtml 

to {1, 0.75, 0.25, 2, 1} for Chinese without M004 

and M006, and {0.3, 1, 0.3, 2, 1} for others. 

5.2 Summary Generation 

We extract 30 candidate sentences with high S 

ordered by S from bigger to smaller and check 

them one by one. We use 30 sentences to make 

sure that when a candidate sentence is not good 

to be in a final summary, we could have enough 

other alternative sentences with less S. Then we 

generate the final summary as Figure 5. 

 
Figure 5: 250-summary generation flow chart 

6 Evaluations 

We’ve got only the automatic evaluation result. 

CIST could get best performance in some lan-

guage, such as Hindi in ROUGE, and in some 

topics, such as Arabic M104, English and Roma-

nia M005, Czech M007, Spanish M103 etc. in N-

gram graph methods: AutoSummENG, MeMoG 

and NPowER. CIST could also get nearly worst 

performance in some cases, such as French and 

Hebrew. In other cases it gets middle perfor-

mance. But Chinese result looks very strange to 

us; we think that it needs more special discussion. 

7 Conclusion and Future Work 

hLDA is a language independent model. It could 

work well sometimes, but not stable enough. Fu-

ture work will focus on parameter adjustment, 

modeling result evaluation, sentence evaluation 

and good summary generation. 

 

Acknowledgments 

We get support from NSFC 61202247, 71231002, 

Fundamental Research Funds for Central Uni-

versities 2013RC0304 and Beijing Science and 

Technology Information Institute. 

42



 

References  

Abdullah Bawakid and Mourad Oussalah, 2008.  A 

Semantic Summarization System: University of 

Birmingham at TAC 2008. TAC 2008 Pro-

ceedings. 

Alistair Kennedy, Terry Copeck, Diana Inkpen and 

Stan Szpakowicz, 2010.Entropy-based Sentence 

Selection with Roget’s Thesaurus. TAC 2010 

Proceedings. 

Annie Louis and Ani Nenkova, 2009. Predicting 

Summary Quality using Limited Human Input. 

TAC 2009 Proceedings. 

Anton Leuski, Chin-Yewlin, Liang Zhou, Ulrich 

Germann, Franz Josef Och, and Eduard Hovy,2003. 

Cross-Lingual C*ST*RD: English Access to 

Hindi Information. ACM Transactions on 

Asian Language Information Processing, 
2(3):245–269. 

Arora Rachit, and Balaraman Ravindran, 2008. La-

tent dirichlet allocation based multi-document 

summarization.  Proceedings of the second 

workshop on Analytics for noisy unstructured 

text data. ACM, 2008. 

Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A 

hybrid hierarchical model for multi-document 

summarization. Proceedings of the 48th An-

nual Meeting of the Association for Computa-

tional Linguistics, 815–824, Uppsala, Sweden, 
11-16 July 2010. 

Blei D. and Lafferty J., 2006. Dynamic topic mod-

els. In International Conference on Machine 

Learning (2006). ACM, New York, NY, 
USA:113–120. 

Blei D., Griffiths T. and Jordan M., 2010. The nested 

Chinese restaurant process and Bayesian 

nonparametric inference of topic hierarchies. 

J. ACM 57, 2 (2010):1–30. 

Chin-Yew Lin and Eduard Hovy, 2002. Automated 

Multi-document Summarization in NeATS. 

Proceedings of HLT 2002, Second Interna-

tional Conference on Human Language Tech-

nology Research. 

Chong Long, Minlie Huang and  Xiaoyan Zhu, 2009. 

Tsinghua University at TAC 2009: Summariz-

ing Multi-documents by Information Distance. 

TAC 2009 Proceedings. 

D. M. Blei, A. Ng, and M. Jordan. 2003.Latent di-

richlet allocation, Jrnl. Machine Learning Re-

search, 3:993-1022, 2003b. 

Feng Jin, Minlie Huang and Xiaoyan Zhu, 2010.The 

THU Summarization Systems at TAC 2010. 

TAC 2010 Proceedings. 

Firas Hmida and Benoit Favre, 2011. LIF at TAC 

Multiling: Towards a Truly Language Inde-

pendent Summarizer. TAC 2011 Proceedings. 

Griffiths T., Steyvers M., Blei D. and Tenenbaum J., 

2005. Integrating topics and syntax. Advances 

in Neural Information Processing Systems 17. 
L. K. Saul, Y. Weiss, and L. Bottou, eds. MIT 

Press, Cambridge, MA,2005:537–544. 

Hongyan Liu, Ping’an Liu, Wei Heng and Lei Li, 

2011.The CIST Summarization System at TAC 

2011. TAC 2011 Proceedings. 

Horacio Saggion, 2011. Using SUMMA for Lan-

guage Independent Summarization at TAC 

2011. TAC 2011 Proceedings. 

John M. Conroy and Judith D. Schlesinger, 2008. 

CLASSY and TAC 2008 Metrics. TAC 2008 

Proceedings. 

John M. Conroy, Judith D. Schlesinger and Dianne P. 

O’Leary, 2006. Topic-Focused Multi-document 

Summarization Using an Approximate Oracle 

Score. Proceedings of the COLING/ACL 2006 

Main Conference Poster Sessions: 152–159. 

John M. Conroy, Judith D. Schlesinger and Jeff Kubi-

na, 2011.CLASSY 2011 at TAC: Guided and 

Multi-lingual Summaries and Evaluation Met-

rics. TAC 2011  Proceedings. 

Jorge García Flores, Laurent Gillard and Olivier Fer-

ret, 2008. Bag-of-senses versus bag-of-words: 

comparing semantic and lexical approaches 

on sentence extraction. TAC 2008 Proceed-

ings. 

Josef Steinberger, Mijail Kabadjov, Ralf Steinberger, 

Hristo Tanev, Marco Turchi and Vanni Zavarel-

la,2011. JRC’s Participation at TAC 2011: 

Guided and Multilingual Summarization Tasks. 

TAC 2011 Proceedings. 

Judith D. Schlesinger, Dianne P. O’Leary and John M. 

Conroy, 2008. Arabic/English Multi-document 

Summarization with CLASSY—The Past and 

the Future. CICLing 2008 Proceedings: 568–
581. 

Krestel Ralf, Peter Fankhauser and Wolfgang Nejdl, 

2009. Latent dirichlet allocation for tag rec-

ommendation. Proceedings of the third ACM 

43



conference on Recommender systems. ACM, 

2009. 

Mahmoud El-Haj, Udo Kruschwitz and Chris Fox, 

2011. University of Essex at the TAC 2011 

Multilingual Summarisation Pilot. TAC 2011 

Proceedings. 

Niraj Kumar, Kannan Srinathan and Vasudeva Varma, 

2010. An Effective Approach for AESOP and 

Guided Summarization Task. TAC 2010  Pro-

ceedings. 

Pierre-Etienne Genest and Guy Lapalme, 2010.Text 

Generation for Abstractive Summarization. 

TAC 2010  Proceedings. 

Pradipto Das and Rohini Srihari, 2011.Global and 

Local Models for Multi-Document Summari-

zation. TAC 2011  Proceedings. 

Renxian Zhang, You Ouyang and Wenjie Li, 

2011.Guided Summarization with Aspect 

Recognition. TAC 2011  Proceedings. 

Shih-Hsiang Lin and Berlin Chen, 2009. THE NTNU 

SUMMARIZATION SYSTEM AT TAC 2009. 

TAC 2009  Proceedings. 

Shu Gong, Youli Qu and Shengfeng Tian, 2009. 

Summarization using Wikipedia. TAC 2010 

Proceedings. 

Sylvain Bellemare, Sabine Bergler and Ren é Witte, 

2008. ERSS at TAC 2008. TAC 2008 Proceed-

ings. 

Teh Y., Jordan M., Beal M. and Blei D., 2006. Hier-

archical Dirichlet processes. J. Am. Stat. As-

soc. 101, 476(2006):1566–1581. 

Tingting He, Jinguang Chen, Zhuoming Gui, and 

Fang Li, 2008. CCNU at TAC 2008 ：
Proceeding on Using Semantic Method for 

Automated Summarization Yield. TAC 2008 

Proceedings. 

Wang C. and Blei D., 2009. Decoupling sparsity 

and smoothness in the discrete hierarchical 

Dirichlet process. Advances in Neural Infor-

mation Processing Systems 22. Y. Bengio, D. 
Schuurmans, J. Lafferty, C. 

William M. Darling, 2010. Multi-Document Sum-

marization from First Principles. TAC 2010 

Proceedings. 

Xiaojun Wan, Jianwu Yang and Jianguo Xiao, 2006.  

Using Cross-Document Random Walks for 

Topic-Focused Multi-Document Summariza-

tion. WI 2006 Main Conference Proceedings. 

Yuanrong Zheng and Tokunaga Takenobu, 2009. The 

TITech Summarization System at TAC-2009. 

TAC 2009  Proceedings. 

 

 

 

 

44


