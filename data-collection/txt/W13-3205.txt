










































Transducing Sentences to Syntactic Feature Vectors: an Alternative Way to "Parse"?


Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 40–49,
Sofia, Bulgaria, August 9 2013. c©2013 Association for Computational Linguistics

Transducing Sentences to Syntactic Feature Vectors:
an Alternative Way to “Parse”?

Fabio Massimo Zanzotto
University of Rome “Tor Vergata”

Via del Politecnico 1
00133 Roma, Italy

fabio.massimo.zanzotto@uniroma2.it

Lorenzo Dell’Arciprete
University of Rome “Tor Vergata”

Via del Politecnico 1
00133 Roma, Italy

lorenzo.dellarciprete@gmail.com

Abstract

Classification and learning algorithms use
syntactic structures as proxies between
source sentences and feature vectors. In
this paper, we explore an alternative path
to use syntax in feature spaces: the Dis-
tributed Representation “Parsers” (DRP).
The core of the idea is straightforward:
DRPs directly obtain syntactic feature vec-
tors from sentences without explicitly pro-
ducing symbolic syntactic interpretations.
Results show that DRPs produce feature
spaces significantly better than those ob-
tained by existing methods in the same
conditions and competitive with those ob-
tained by existing methods with lexical in-
formation.

1 Introduction

Syntactic processing is widely considered an im-
portant activity in natural language understand-
ing (Chomsky, 1957). Research in natural lan-
guage processing (NLP) exploits this hypothesis
in models and systems. Syntactic features improve
performance in high level tasks such as question
answering (Zhang and Lee, 2003), semantic role
labeling (Gildea and Jurafsky, 2002; Pradhan et
al., 2005; Moschitti et al., 2008; Collobert et al.,
2011), paraphrase detection (Socher et al., 2011),
and textual entailment recognition (MacCartney et
al., 2006; Wang and Neumann, 2007; Zanzotto et
al., 2009).

Classification and learning algorithms are key
components in the above models and in current
NLP systems, but these algorithms cannot directly
use syntactic structures. The relevant parts of
phrase structure trees or dependency graphs are
explicitly or implicitly stored in feature vectors.

To fully exploit syntax in learning classi-
fiers, kernel machines (Cristianini and Shawe-
Taylor, 2000) use graph similarity algorithms
(e.g., (Collins and Duffy, 2002) for trees) as struc-
tural kernels (Gärtner, 2003). These structural ker-
nels allow to exploit high-dimensional spaces of
syntactic tree fragments by concealing their com-
plexity. These feature spaces, although hidden,
still exist. Then, even in kernel machines, sym-
bolic syntactic structures act only as proxies be-
tween the source sentences and the syntactic fea-
ture vectors.

In this paper, we explore an alternative way
to use syntax in feature spaces: the Distributed
Representation Parsers (DRP). The core of the
idea is straightforward: DRPs directly bridge
the gap between sentences and syntactic feature
spaces. DRPs act as syntactic parsers and fea-
ture extractors at the same time. We leverage on
the distributed trees recently introduced by Zan-
zotto&Dell’Arciprete (2012) and on multiple lin-
ear regression models. Distributed trees are small
vectors that encode the large vectors of the syn-
tactic tree fragments underlying the tree kernels
(Collins and Duffy, 2002). These vectors effec-
tively represent the original vectors and lead to
performances in NLP tasks similar to tree kernels.
Multiple linear regression allows to learn linear
DRPs from training data. We experiment with the
Penn Treebank data set (Marcus et al., 1993). Re-
sults show that DRPs produce distributed trees sig-
nificantly better than those obtained by existing
methods, in the same non-lexicalized conditions,
and competitive with those obtained by existing
methods with lexical information. Finally, DRPs
are extremely faster than existing methods.

The rest of the paper is organized as fol-
lows. First, we present the background of our

40



idea (Sec. 2). Second, we fully describe our
model (Sec. 3). Then, we report on the experi-
ments (Sec. 4). Finally, we draw some conclusions
and outline future work (Sec. 5)

2 Background

Classification and learning algorithms for NLP
tasks treat syntactic structures t as vectors in fea-
ture spaces ~t ∈ Rm. Each feature generally rep-
resents a substructure τi. In simple weighting
schemes, feature values are 1 if τi is a substruc-
ture of t and 0 otherwise. Different weighting
schemes are used and possible. Then, learning al-
gorithms exploit these feature vectors in different
ways. Decision tree learners (Quinlan, 1993) elect
the most representative feature at each iteration,
whereas kernel machines (Cristianini and Shawe-
Taylor, 2000) exploit similarity between pairs of
instances, s(t1, t2). This similarity is generally
measured as the dot product between the two vec-
tors, i.e. s(t1, t2) = ~t1 · ~t2.

The use of syntactic features changed when tree
kernels (Collins and Duffy, 2002) appeared. Tree
kernels gave the possibility to fully exploit feature
spaces of tree fragments. Until then, learning al-
gorithms could not treat these huge spaces. It is
infeasible to explicitly represent that kind of fea-
ture vectors and to directly compute similarities
through dot products. Tree kernels (Collins and
Duffy, 2002), by computing similarities between
two trees with tree comparison algorithms, exactly
determine dot products of vectors in these target
spaces. After their introduction, different tree ker-
nels have been proposed (e.g., (Vishwanathan and
Smola, 2002; Culotta and Sorensen, 2004; Mos-
chitti, 2006)). Their use spread in many NLP
tasks (e.g., (Zhou et al., 2007; Wang and Neu-
mann, 2007; Moschitti et al., 2008; Zanzotto et
al., 2009; Zhang and Li, 2009)) and in other areas
like biology (Vert, 2002; Hashimoto et al., 2008)
and computer security (Düssel et al., 2008; Rieck
and Laskov, 2007; Bockermann et al., 2009).

Tree kernels have played a very important role
in promoting the use of syntactic information in
learning classifiers, but this method obfuscated the
fact that syntactic trees are ultimately used as vec-
tors in learning algorithms. To work with the
idea of directly obtaining rich syntactic feature
vectors from sentences, we need some techniques
to make these high-dimensional vectors again ex-
plicit, through smaller but expressive vectors.

A solution to the above problem stems from
the recently revitalized research in Distributed
Representations (DR) (Hinton et al., 1986; Ben-
gio, 2009; Collobert et al., 2011; Socher et al.,
2011; Zanzotto and Dell’Arciprete, 2012). Dis-
tributed Representations, studied in opposition to
symbolic representations (Rumelhart and Mcclel-
land, 1986), are methods for encoding data struc-
tures such as trees into vectors, matrices, or high-
order tensors. The targets of these representa-
tions are generally propositions, i.e., flat tree struc-
tures. The Holographic Reduced Representations
(HRR), proposed by Plate (1994), produce nearly
orthogonal vectors for different structures by com-
bining circular convolution and randomly gener-
ated vectors for basic components (as in (Ander-
son, 1973; Murdock, 1983)).

Building on HRRs, Distributed Trees (DT) have
been proposed to encode deeper trees in low di-
mensional vectors (Zanzotto and Dell’Arciprete,
2012). DTs approximate the feature space of tree
fragments defined for the tree kernels (Collins and
Duffy, 2002) and guarantee similar performances
of classifiers in NLP tasks such as question classi-
fication and textual entailment recognition. Thus,
Distributed Trees are good representations of syn-
tactic trees, that we can use in our definition of
distributed representation parsers (DRPs).

3 Distributed Representation Parsers

In this section, first, we sketch the idea of Dis-
tributed Representation “Parsers” (DRPs). Then,
we review the distributed trees as a way to repre-
sent trees in low dimensional vectors. Finally, we
describe how to build DRPs by mixing a function
that encodes sentences in vectors and a linear re-
gressor that can be induced from training data.

3.1 The Idea

The approach to using syntax in learning algo-
rithms generally follows two steps: first, parse
sentences s with a symbolic parser (e.g., (Collins,
2003; Charniak, 2000; Nivre et al., 2007)) and
produce symbolic trees t; second, use an en-
coder to build syntactic feature vectors. Fig-
ure 1 sketches this idea when the final vectors
are the distributed trees

;

t ∈ Rd (Zanzotto and
Dell’Arciprete, 2012)1. In this case, the last step

1To represent a distributed tree for a tree t, we use the

notation
;

t to stress that this small vector is an approximation
of the original high-dimensional vector ~t in the space of tree

41



s

“We booked the flight”
→

Symbolic
Parser
(SP)

→

t

S
PPP

���
NP

PRP

We

VP
aa!!

V

booked

NP
QQ��

DT

the

NN

flight

→
Distributed

Tree
Encoder

(DT)

→

;

t ∈ Rd












0.0002435

0.00232

.

.

.
−0.007325













Distributed Representation Parser (DRP)
;

s ∈ Rd

D →












−0.0017245

0.0743869

.

.

.
0.0538474













→ P

Figure 1: “Parsing” with distributed structures in perspective

is the Distributed Tree Encoder (DT).
Our proposal is to build a Distributed Represen-

tation “Parser” (DRP) that directly maps sentences
s into the final vectors. We choose the distributed
trees

;

t as these reduced vectors fully represent the
syntactic trees. A DRP acts as follows (see Fig-
ure 1): first, a function D encodes sentence s into
a distributed vector

;

s ∈ Rd; second, a function
P transforms the input vector

;

s into a distributed

tree
;

t . This second step is a vector to vector trans-
formation and, in a wide sense, “parses” the input
sentence.

Given an input sentence s, a DRP is then a
function defined as follows:

;

t = DRP (s) = P (D(s)) (1)

In this paper, we design some functions D and we
propose a linear function P , designed to be a re-
gressor that can be induced from training data. In
this study, we use a space with d dimensions for

both sentences
;

s and distributed trees
;

t , but, in
general, these spaces can be of different size.

3.2 Syntactic Trees as Distributed Vectors

We here report on the distributed trees2 (Zan-
zotto and Dell’Arciprete, 2012) to describe how
these vectors represent syntactic trees and how the
dot product between two distributed trees approxi-
mates the tree kernel defined by Collins and Duffy
(2002).

fragments.
2For the experiments, we used the implemen-

tation of the distributed tree encoder available at
http://code.google.com/p/distributed-tree-kernels/

Given a tree t, the corresponding distributed tree
;

t is defined as follows:

DT (t) =
∑

τi∈S(t)

ωi
;

τ i (2)

where S(t) is the set of the subtrees τi of t,
;

τ i
is the small vector corresponding to tree fragment
τi and ωi is the weight of subtree τi in the final
feature space. As in (Collins and Duffy, 2002), the
set S(t) contains tree fragments τ such that the
root of τ is any non-terminal node in t and, if τ
contains node n, it must contain all the siblings of
n in t (see, for example, Slex(t) in Figure 2). The
weight ωi is defined as:

ωi =







√
λ|τi|−1 if |τi| > 1 and λ 6= 0

1 if |τi| = 1
0 if λ = 0

(3)

where |τi| is the number of non-terminal nodes of
tree fragment τi and λ is the traditional parame-
ter used to penalize large subtrees. For λ = 0,
ωi has a value 1 for productions and 0 other-
wise. If different tree fragments are associated
to nearly orthonormal vectors, the dot product
;

t1 ·
;

t2 approximates the tree kernel (Zanzotto and
Dell’Arciprete, 2012).

A key feature of the distributed tree fragments
;

τ is that these vectors are built compositionally
from a set N of nearly orthonormal random vec-
tors

;

n , associated to node labels n. Given a sub-
tree τ , the related vector is obtained as:

;

τ =
;

n1 ⊠
;

n2 ⊠ . . . ⊠
;

nk

42



Sno lex(t) = {
S
QQ��

NP VP
,

VP
ZZ��

V NP
,

NP

PRP
,

S
ZZ��

NP

PRP

VP ,

S
HH��

NP VP
ZZ��

V NP

,

VP
HH��

V NP
cc##

DT NN

, . . . }

Slex(t) = Sno lex(t) ∪ {

S
ZZ��

NP

PRP

We

VP
,

VP
aa!!

V

booked

NP
cc##

DT NN

,

VP
aa!!

V

booked

NP
cc##

DT

the

NN
,

VP
aa!!

V

booked

NP
QQ��

DT NN

flight

, . . . }

Figure 2: Subtrees of the tree t in Figure 1

where node vectors
;

n i are ordered according to a
depth-first visit of subtree τ and ⊠ is a vector com-
position operation, specifically the shuffled circu-
lar convolution3 . This function guarantees that
two different subtrees have nearly orthonormal
vectors (see (Zanzotto and Dell’Arciprete, 2012)
for more details). For example, the fifth tree τ5 of
set Sno lex(t) in Figure 2 is:

;

τ 5 =
;

S ⊠ (
;

NP ⊠ (
;

V P ⊠ (
;

V ⊠
;

NP )))

We experiment with two tree fragment sets:
the non-lexicalized set Sno lex(t), where tree frag-
ments do not contain words, and the lexicalized
set Slex(t), including all the tree fragments. An
example is given in Figure 2.

3.3 The Model

To build a DRP, we need to define the encoder
D and the transformer P . In the following, we
present a non-lexicalized and a lexicalized model
for the encoder D and we describe how we can
learn the transformer P by means of a linear re-
gression model.

3.3.1 Sentence Encoders

Establishing good models to encode input sen-
tences into vectors is the most difficult challenge.
The models should consider the kind of informa-
tion that can lead to a correct syntactic interpre-
tation. Only in this way, the distributed repre-
sentation parser can act as a vector transforming
module. Unlike in models such as (Socher et al.,
2011), we want our encoder to represent the whole
sentence as a fixed size vector. We propose a non-
lexicalized model and a lexicalized model.

3The shuffled circular convolution ⊠ is defined as ~a⊠~b =
s1(~a)⊗ s2(~b) where ⊗ is the circular convolution and s1 and
s2 are two different random permutations of vector elements.

Non-lexicalized model The non-lexicalized
model relies only on the pos-tags of the sentences
s: s = p1 . . . pn where pi is the pos-tag associated
with the i-th token of the sentence. In the follow-
ing we discuss how to encode this information in
a Rd space. The basic model D1(s) is the one that
considers the bag-of-postags, that is:

D1(s) =
∑

i

;

p i (4)

where
;

p i ∈ N is the vector for label pi, taken
from the set of nearly orthonomal random vectors
N . It is basically in line with the bag-of-word
model used in random indexing (Sahlgren, 2005).
Due to the commutative property of the sum and
since vectors in N are nearly orthonormal: (1)
two sentences with the same set of pos-tags have
the same vector; and, (2) the dot product between
two vectors, D1(s1) and D1(s2), representing sen-
tences s1 and s2, approximately counts how many
pos-tags the two sentences have in common. The
vector for the sentence in Figure 1 is then:

D1(s) =
;

PRP +
;

V +
;

DT +
;

NN

The general non-lexicalized model that takes
into account all n-grams of pos-tags, up to length
j, is then the following:

Dj(s) = Dj−1(s) +
∑

i

;

p i ⊠ . . . ⊠
;

p i+j−1

where ⊠ is again the shuffled circular convolution.
An n-gram pi . . . pi+j−1 of pos-tags is represented

as
;

p i ⊠ . . . ⊠
;

p i+j−1. Given the properties of
the shuffled circular convolution, an n-gram of
pos-tags is associated to a versor, as it composes
j versors, and two different n-grams have nearly
orthogonal vectors. For example, vector D3(s)
for the sentence in Figure 1 is:

43



D3(s) =
;

PRP +
;

V +
;

DT +
;

NN +
;

PRP ⊠
;

V +
;

V ⊠
;

DT +
;

DT ⊠
;

NN +
;

PRP ⊠
;

V ⊠
;

DT +
;

V ⊠
;

DT ⊠
;

NN

Lexicalized model Including lexical informa-
tion is the hardest part of the overall model, as
it makes vectors denser in information. Here
we propose an initial model that is basically as
the non-lexicalized model, but includes a vector
representing the words in the unigrams. The
equation representing sentences as unigrams is:

Dlex1 (s) =
∑

i

;

p i ⊠
;

wi

Vector
;

wi represents word wi and is taken from the
set N of nearly orthonormal random vectors. This
guarantees that Dlex1 (s) is not lossy. Given a pair
word-postag (w, p), it is possible to know if the
sentence contains this pair, as Dlex1 (s)×

;

p⊠
;

w ≈ 1
if (w, p) is in sentence s and Dlex1 (s)×

;

p ⊠
;

w ≈ 0
otherwise. Other vectors for representing words,
e.g., distributional vectors or those obtained as
look-up tables in deep learning architectures (Col-
lobert and Weston, 2008), do not guarantee this
possibility.

The general equation for the lexicalized version
of the sentence encoder follows:

Dlexj (s) = D
lex
j−1(s) +

∑

i

;

p i ⊠ . . . ⊠
;

p i+j−1

This model is only an initial proposal in order
to take into account lexical information.

3.3.2 Learning Transformers with Linear
Regression

The transformer P of the DRP (see Equation 1)
can be seen as a linear regressor:

;

t = P
;

s (5)

where P is a square matrix. This latter can be esti-
mated having training sets (T,S) of oracle vectors

and sentence input vectors (
;

t i,
;

s i) for sentences
si. Interpreting these sets as matrices, we need to
solve a linear set of equations, i.e.: T = PS.

An approximate solution can be computed us-
ing Principal Component Analysis and Partial
Least Square Regression4. This method relies on

4An implementation of this method is available within the
R statistical package (Mevik and Wehrens, 2007).

Moore-Penrose pseudo-inversion (Penrose, 1955).
Pseudo-inverse matrices S+ are obtained using
singular value decomposition (SVD). Matrices
have the property SS+ = I. Using the itera-
tive method for computing SVD (Golub and Ka-
han, 1965), we can obtain different approxima-
tions S+(k) of S

+ considering k singular values. Fi-
nal approximations of DRP s are then: P(k) =
TS

+
(k).

Matrices P are estimated by pseudo-inverting
matrices representing input vectors for sentences
S. Given the different input representations for
sentences, we can then estimate different DRPs:
DRP1 = TS

+
1 , DRP2 = TS

+
2 , and so on. We

need to estimate the best k in a separate parameter
estimation set.

4 Experiments

We evaluated three issues for assessing DRP mod-
els: the performance of DRPs in reproducing or-
acle distributed trees (Sec. 4.2); the quality of the
topology of the vector spaces of distributed trees
induced by DRPs (Sec. 4.3); and the computation
run time of DRPs (Sec. 4.4). Section 4.1 describes
the experimental set-up.

4.1 Experimental Set-up

Data We derived the data sets from the Wall
Street Journal (WSJ) portion of the English Penn
Treebank data set (Marcus et al., 1993), using
a standard data split for training (sections 2-21
PTtrain with 39,832 trees) and for testing (section
23 PT23 with 2,416 trees). We used section 24
PT24 with 1,346 trees for parameter estimation.

We produced the final data sets of distributed
trees with three different λ values: λ=0, λ=0.2,
and λ=0.4. For each λ, we have two ver-
sions of the data sets: a non-lexicalized version
(no lex), where syntactic trees are considered
without words, and a lexicalized version (lex),
where words are considered. Oracle trees t are
transformed into oracle distributed trees

;

o using
the Distributed Tree Encoder DT (see Figure 1).
We experimented with two sizes of the distributed
trees space Rd: 4096 and 8192.

We have designed the data sets to determine
how DRPs behave with λ values relevant for
syntax-sensitive NLP tasks. Both tree kernels and
distributed tree kernels have the best performances
in tasks such as question classification, seman-
tic role labeling, or textual entailment recognition

44



with λ values in the range 0–0.4.

System Comparison We compared the DRPs
against the existing way of producing distributed
trees (based on the recent paper described in
(Zanzotto and Dell’Arciprete, 2012)): distributed
trees are obtained using the output of a sym-
bolic parser (SP) that is then transformed into a
distributed tree using the DT with the appropri-
ate λ. We refer to this chain as the Distributed
Symbolic Parser (DSP ). The DSP is then the
chain DSP (s) = DT (SP (s)) (see Figure 1).
As for the symbolic parser, we used Bikel’s ver-
sion (Bikel, 2004) of Collins’ head-driven statisti-
cal parser (Collins, 2003). For a correct compar-
ison, we used the Bikel’s parser with oracle part-
of-speech tags. We experimented with two ver-
sions: (1) a lexicalized method DSPlex, i.e., the
natural setting of the Collins/Bikel parser, and (2)
a fully non-lexicalized version DSPno lex that ex-
ploits only part-of-speech tags. We obtained this
last version by removing words in input sentences
and leaving only part-of-speech tags. We trained
these DSP s on PTtrain.

Parameter estimation DRPs have two basic pa-
rameters: (1) parameter k of the pseudo-inverse,
that is, the number of considered eigenvectors (see
Section 3.3.2) and (2) the maximum length j of the
n-grams considered by the encoder Dj (see Sec-
tion 3.3.1). We performed the parameter estima-
tion on the datasets derived from section PT24 by
maximizing a pseudo f-measure. Section 4.2 re-
ports both the definition of the measure and the
results of the parameter estimation.

4.2 Parsing Performance

The first issue to explore is whether DRP s are
actually good “distributed syntactic parsers”. We
compare DRP s against the distributed symbolic
parsers by evaluating how well these “distributed
syntactic parsers” reproduce oracle distributed
trees.

Method A good DRP should produce dis-
tributed trees that are similar to oracle distributed
trees. To capture this, we use the cosine similarity
between the system and the oracle vectors:

cos(
;

t ,
;

o ) =

;

t ·;o
||
;

t ||||;o ||
where

;

t is the system’s distributed tree and
;

o

is the oracle distributed tree. We compute these

dim Model λ = 0 λ = 0.2 λ = 0.4

4096

DRP1 0.6285 0.5697 0.542
DRP2 0.8011 0.7311 0.631
DRP3 0.8276‡ 0.7552 ‡ 0.6506‡

DRP4 0.8171 0.744 0.6419
DRP5 0.8045 0.7342 0.631
DSPno lex 0.654 0.5884 0.4835
DSPlex 0.815 0.7813 0.7121

8192
DRP3 0.8335‡ 0.7605‡ 0.6558‡

DSPno lex 0.6584 0.5924 0.4873
DSPlex 0.8157 0.7815 0.7123

Table 1: Average similarity on PT23 of the
DRPs (with different j) and the DSP on the non-
lexicalized data sets with different λs and with the
two dimensions of the distributed tree space (4096
and 8192). ‡ indicates significant difference wrt.
DSPno lex (p << .005 computed with the Stu-
dent’s t test)

Model λ = 0 λ = 0.2 λ = 0.4
DRP3 0.7192 0.6406 0.0646
DSPlex 0.9073 0.8564 0.6459

Table 2: Average similarity on PT23 of the DRP3
and the DSPlex on the lexicalized data sets with
different λs on the distributed tree space with 4096
dimensions

the cosine similarity at the sentence-based (i.e.,
vector-based) granularity. Results report average
values.

Estimated parameters We estimated parame-
ters k and j by training the different DRP s on
the PTtrain set and by maximizing the similarity
of the DRP s on PT24. The best pair of param-
eters is j=3 and k=3000. For completeness, we
report also the best k values for the five different
j we experimented with: k = 47 for j=1 (the lin-
early independent vectors representing pos-tags),
k = 1300 for j=2, k = 3000 for j=3, k = 4000
for j=4, and k = 4000 for j=5. For comparison,
some resulting tables report results for the differ-
ent values of j.

Results Table 1 reports the results of the first set
of experiments on the non-lexicalized data sets.
The first block of rows (seven rows) reports the av-
erage cosine similarity of the different methods on
the distributed tree spaces with 4096 dimensions.
The second block (the last three rows) reports the
performance on the space with 8192 dimensions.
The average cosine similarity is computed on the
PT23 set. Although we already selected j=3 as
the best parameterization (i.e. DRP3), the first

45



Output Model λ = 0 λ = 0.2 λ = 0.4

No lex
DRP3 0.9490 0.9465 0.9408
DSPno lex 0.9033 0.9001 0.8932
DSPlex 0.9627 0.9610 0.9566

Lex
DRP3 0.9642 0.9599 0.0025
DSPlex 0.9845 0.9817 0.9451

Table 3: Average Spearman’s Correlation: dim
4096 between the oracle’s vector space and the
systems’ vector spaces (100 trials on lists of 1000
sentence pairs).

five rows of the first block report the results of the
DRPs for five values of j. This gives an idea of
how the different DRPs behave. The last two rows
of this block report the results of the two DSPs.

We can observe some important facts. First,
DRP s exploiting 2-grams, 3-grams, 4-grams, and
5-grams of part-of-speech tags behave signifi-
cantly better than the 1-grams for all the values
of λ. Distributed representation parsers need in-
puts that keep trace of sequences of pos-tags of
sentences. But these sequences tend to confuse
the model when too long. As expected, DRP3
behaves better than all the other DRPs. Second,
DRP3 behaves significantly better than the com-
parable traditional parsing chain DSPno lex that
uses only part-of-speech tags and no lexical in-
formation. This happens for all the values of λ.
Third, DRP3 behaves similarly to DSPlex for
λ=0. Both parsers use oracle pos tags to emit sen-
tence interpretations but DSPlex also exploits lex-
ical information that DRP3 does not access. For
λ=0.2 and λ=0.4, the more informed DSPlex be-
haves significantly better than DRP3. But DRP3
still behaves significantly better than the compa-
rable DSPno lex. All these observations are valid
also for the results obtained for 8192 dimensions.

Table 2 reports the results of the second set of
experiments on the lexicalized data sets performed
on a 4192-dimension space. The first row reports
the average cosine similarity of DRP3 trained on
the lexicalized model and the second row reports
the results of DSPlex. In this case, DRP3 is not
behaving well with respect to DSPlex. The addi-
tional problem DRP3 has is that it has to repro-
duce input words in the output. This greatly com-
plicates the work of the distributed representation
parser. But, as we report in the next section, this
preliminary result may be still satisfactory for λ=0
and λ=0.2.

Figure 3: Topology of the resulting spaces derived
with the three different methods: similarities be-
tween sentences

4.3 Kernel-based Performance

This experiment investigates how DRP s preserve
the topology of the oracle vector space. This cor-
relation is an important quality factor of a dis-
tributed tree space. When using distributed tree
vectors in learning classifiers, whether

;

oi ·
;

oj in

the oracle’s vector space is similar to
;

ti ·
;

tj in
the DRP’s vector space is more important than

whether
;

oi is similar to
;

ti (see Figure 3). Sen-
tences that are close using the oracle syntactic in-
terpretations should also be close using DRP vec-
tors. The topology of the vector space is more rel-
evant than the actual quality of the vectors. The
experiment on the parsing quality in the previous
section does not properly investigate this property,
as the performance of DRPs could be not sufficient
to preserve distances among sentences.

Method We evaluate the coherence of the topol-
ogy of two distributed tree spaces by measuring
the Spearman’s correlation between two lists of
pairs of sentences (si, sj), ranked according to the
similarity between the two sentences. If the two
lists of pairs are highly correlated, the topology
of the two spaces is similar. The different meth-
ods and, thus, the different distributed tree spaces
are compared against the oracle vector space (see
Figure 3). Then, the first list always represents the
oracle vector space and ranks pairs (si, sj) accord-

ing to
;

o i ·
;

o j . The second list instead represents
the space obtained with a DSP or a DRP. Thus, it

is respectively ranked with
;

ẗi ·
;

ẗj or
;

ti ·
;

tj . In this
way, we can comparatively evaluate the quality of
the distributed tree vectors of our DRP s with re-
spect to the other methods. We report average and
standard deviation of the Spearman’s correlation
on 100 runs over lists of 1000 pairs. We used the
testing set PT23 for extracting vectors.

46



Figure 4: Running time with respect to the sen-
tence length (dimension = 4092)

Results Table 3 reports results both on the non-
lexicalized and on the lexicalized data set. For
the non-lexicalized data set we report three meth-
ods (DRP3, DSPno lex, and DSPlex) and for the
lexicalized dataset we report two methods (DRP3
and DSPlex). Columns represent different values
of λ. Experiments are carried out on the 4096-
dimension space. For the non-lexicalized data set,
distributed representation parsers behave signifi-
cantly better than DSPno lex for all the values of
λ. The upper-bound of DSPlex is not so far. For
the harder lexicalized data set, the difference be-
tween DRP3 and DSPlex is smaller than the one
based on the parsing performance. Thus, we have
more evidence of the fact that we are in a good
track. DRP s can substitute the DSP in generating
vector spaces of distributed trees that adequately
approximate the space defined by an oracle.

4.4 Running Time

In this last experiment, we compared the running
time of the DRP with respect to the DSP . The
analysis has been done on a dual-core processor
and both systems are implemented in the same
programming language, i.e. Java. Figure 4 plots
the running time of the DRP , the SP , and the
full DSP = DT ◦ SP . The x-axis represents the
sentence length in words and the y-axis represents
the running time in milliseconds. The distance be-
tween SP and DSP shrinks as the plot is in a log-
arithmic scale. Figure 5 reports the average co-
sine similarity of DRP , DSPlex, and DSPno lex,
with respect to the sentence length, on the non-
lexicalized data set with λ=0.4.

We observe that DRP becomes extremely con-
venient for sentences larger than 10 words (see
Fig. 4) and the average cosine similarity difference
between the different methods is nearly constant
for the different sentence lengths (see Fig. 5). This
test already makes DRPs very appealing methods
for real time applications. But, if we consider that

Figure 5: Average similarity with λ=0.4 with re-
spect to the sentence length (dimension = 4092)

DRPs can run completely on Graphical Processing
Units (GPUs), as dealing only with matrix prod-
ucts, fast-Fourier transforms, and random genera-
tors, we can better appreciate the potentials of the
proposed methods.

5 Conclusions and Future Work

We presented Distributed Representation Parsers
(DRP) as a novel path to use syntactic structures
in feature spaces. We have shown that these
”parsers” can be learnt using training data and that
DRPs are competitive with respect to traditional
methods of using syntax in feature spaces.

This novel path to use syntactic structures in
feature spaces opens interesting and unexplored
possibilities. First, DRPs tackle the issue of com-
putational efficiency of structural kernel methods
(Rieck et al., 2010; Shin et al., 2011) from another
perspective. DRPs could reduce structural kernel
computations to extremely efficient dot products.
Second, the tight integration of parsing and feature
vector generation lowers the computational cost of
producing distributed representations from trees,
as circular convolution is not applied on-line.

Finally, DRPs can contribute to treat syntax in
deep learning models in a uniform way. Deep
learning models (Bengio, 2009) are completely
based on distributed representations. But when
applied to natural language processing tasks (e.g.,
(Collobert et al., 2011; Socher et al., 2011)), syn-
tactic structures are not represented in the neural
networks in a distributed way. Syntactic informa-
tion is generally used by exploiting symbolic parse
trees, and this information positively impacts per-
formances on final applications, e.g., in paraphrase
detection (Socher et al., 2011) and in semantic role
labeling (Collobert et al., 2011). Building on the
results presented here, an interesting line of re-
search is then the integration of distributed repre-
sentation parsers and deep learning models.

47



References

James A. Anderson. 1973. A theory for the recognition
of items from short memorized lists. Psychological
Review, 80(6):417 – 438.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and Trends in Machine Learning,
2(1):1–127.

Daniel M. Bikel. 2004. Intricacies of collins’ parsing
model. Comput. Linguist., 30:479–511, December.

Christian Bockermann, Martin Apel, and Michael
Meier. 2009. Learning sql for database intrusion de-
tection using context-sensitive modelling. In Detec-
tion of Intrusions andMalware & Vulnerability As-
sessment (DIMVA), pages 196–205.

Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. of the 1st NAACL, pages
132–139, Seattle, Washington.

Naom Chomsky. 1957. Aspect of Syntax Theory. MIT
Press, Cambridge, Massachussetts.

Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of ACL02.

Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Comput. Linguist.,
29(4):589–637.

R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.

Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Uni-
versity Press, March.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ’04, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Patrick Düssel, Christian Gehl, Pavel Laskov, and Kon-
rad Rieck. 2008. Incorporation of application layer
protocol syntax into anomaly detection. In Proceed-
ings of the 4th International Conference on Infor-
mation Systems Security, ICISS ’08, pages 188–202,
Berlin, Heidelberg. Springer-Verlag.

Thomas Gärtner. 2003. A survey of kernels for struc-
tured data. SIGKDD Explorations.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245–288.

Gene Golub and William Kahan. 1965. Calculat-
ing the singular values and pseudo-inverse of a ma-
trix. Journal of the Society for Industrial and Ap-
plied Mathematics, Series B: Numerical Analysis,
2(2):205–224.

Kosuke Hashimoto, Ichigaku Takigawa, Motoki Shiga,
Minoru Kanehisa, and Hiroshi Mamitsuka. 2008.
Mining significant tree patterns in carbohydrate
sugar chains. Bioinformatics, 24:i167–i173, Au-
gust.

G. E. Hinton, J. L. McClelland, and D. E. Rumel-
hart. 1986. Distributed representations. In D. E.
Rumelhart and J. L. McClelland, editors, Paral-
lel Distributed Processing: Explorations in the Mi-
crostructure of Cognition. Volume 1: Foundations.
MIT Press, Cambridge, MA.

Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 41–48, New York City,
USA, June. Association for Computational Linguis-
tics.

M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19:313–330.

Bjrn-Helge Mevik and Ron Wehrens. 2007. The
pls package: Principal component and partial least
squares regression in r. Journal of Statistical Soft-
ware, 18(2):1–24, 1.

Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34(2):193–224.

Alessandro Moschitti. 2006. Efficient Convolution
Kernels for Dependency and Constituent Syntactic
Trees. In Proceedings of The 17th European Con-
ference on Machine Learning, Berlin, Germany.

Bennet B. Murdock. 1983. A distributed memory
model for serial-order information. Psychological
Review, 90(4):316 – 338.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Glsen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.

Roger Penrose. 1955. A generalized inverse for matri-
ces. In Proc. Cambridge Philosophical Society.

T. A. Plate. 1994. Distributed Representations and
Nested Compositional Structure. Ph.D. thesis.

48



Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role labeling using different syntactic views.
In ACL ’05: Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 581–588. Association for Computational Lin-
guistics, Morristown, NJ, USA.

J. Quinlan. 1993. C4:5:programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo.

Konrad Rieck and Pavel Laskov. 2007. Language
models for detection of unknown attacks in network
traffic. Journal in Computer Virology, 2:243–256.
10.1007/s11416-006-0030-0.

Konrad Rieck, Tammo Krueger, Ulf Brefeld, and
Klaus-Robert Müller. 2010. Approximate tree ker-
nels. J. Mach. Learn. Res., 11:555–580, March.

David E. Rumelhart and James L. Mcclelland. 1986.
Parallel Distributed Processing: Explorations in the
Microstructure of Cognition : Foundations (Parallel
Distributed Processing). MIT Press, August.

Magnus Sahlgren. 2005. An introduction to random
indexing. In Proceedings of the Methods and Appli-
cations of Semantic Indexing Workshop at the 7th In-
ternational Conference on Terminology and Knowl-
edge Engineering TKE, Copenhagen, Denmark.

Kilho Shin, Marco Cuturi, and Tetsuji Kuboyama.
2011. Mapping kernels for trees. In Lise Getoor
and Tobias Scheffer, editors, Proceedings of the
28th International Conference on Machine Learning
(ICML-11), ICML ’11, pages 961–968, New York,
NY, USA, June. ACM.

Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems 24.

Jean-Philippe Vert. 2002. A tree kernel to anal-
yse phylogenetic profiles. Bioinformatics, 18(suppl
1):S276–S284, July.

S. V. N. Vishwanathan and Alexander J. Smola. 2002.
Fast kernels for string and tree matching. In Suzanna
Becker, Sebastian Thrun, and Klaus Obermayer, ed-
itors, NIPS, pages 569–576. MIT Press.

Rui Wang and Günter Neumann. 2007. Recognizing
textual entailment using sentence similarity based on
dependency tree skeletons. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 36–41, Prague, June. Associa-
tion for Computational Linguistics.

F.M. Zanzotto and L. Dell’Arciprete. 2012. Dis-
tributed tree kernels. In Proceedings of Interna-
tional Conference on Machine Learning, pages 193–
200.

Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learning
approach to textual entailment recognition. NATU-
RAL LANGUAGE ENGINEERING, 15-04:551–582.

Dell Zhang and Wee Sun Lee. 2003. Question classi-
fication using support vector machines. In Proceed-
ings of the 26th annual international ACM SIGIR
conference on Research and development in infor-
maion retrieval, SIGIR ’03, pages 26–32, New York,
NY, USA. ACM.

Min Zhang and Haizhou Li. 2009. Tree kernel-based
SVM with structured syntactic knowledge for BTG-
based phrase reordering. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 698–707, Singapore, Au-
gust. Association for Computational Linguistics.

GuoDong Zhou, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse
tree information. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 728–736.

49


