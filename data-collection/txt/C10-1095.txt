Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 842–850,

Beijing, August 2010

842

Co-STAR: A Co-training Style Algorithm for Hyponymy Relation

Acquisition from Structured and Unstructured Text

Jong-Hoon Oh, Ichiro Yamada, Kentaro Torisawa, and Stijn De Saeger

Language Infrastructure Group, MASTAR Project,

National Institute of Information and Communications Technology (NICT)

{rovellia,iyamada,torisawa,stijn}@nict.go.jp

Abstract

This paper proposes a co-training style
algorithm called Co-STAR that acquires
hyponymy relations simultaneously from
structured and unstructured text.
In Co-
STAR, two independent processes for hy-
ponymy relation acquisition – one han-
dling structured text and the other han-
dling unstructured text – collaborate by re-
peatedly exchanging the knowledge they
acquired about hyponymy relations. Un-
like conventional co-training, the two pro-
cesses in Co-STAR are applied to dif-
ferent source texts and training data.
We show the effectiveness of this al-
gorithm through experiments on large-
scale hyponymy-relation acquisition from
Japanese Wikipedia and Web texts. We
also show that Co-STAR is robust against
noisy training data.
Introduction

terms,

1
Acquiring semantic knowledge, especially se-
mantic relations between lexical
is re-
garded as a crucial step in developing high-level
natural language applications. This paper pro-
poses Co-STAR (a Co-training STyle Algorithm
for hyponymy Relation acquisition from struc-
tured and unstructured text).
Similar to co-
training (Blum and Mitchell, 1998),
two hy-
ponymy relation extractors in Co-STAR, one for
structured and the other for unstructured text, it-
eratively collaborate to boost each other’s perfor-
mance.

Many algorithms have been developed to auto-
matically acquire semantic relations from struc-
tured and unstructured text. Because term pairs
are encoded in structured and unstructured text in
different styles, different kinds of evidence have
been used for semantic relation acquisition:

Evidence from unstructured text:

lexico-
syntactic patterns and distributional similar-
ity (Ando et al., 2004; Hearst, 1992; Pantel
et al., 2009; Snow et al., 2006; De Saeger et
al., 2009; Van Durme and Pasca, 2008);

Evidence from structured text: topic hierarchy,
layout structure of documents, and HTML
tags (Oh et al., 2009; Ravi and Pasca, 2008;
Sumida and Torisawa, 2008; Shinzato and
Torisawa, 2004).

Recently, researchers have used both structured
and unstructured text for semantic-relation acqui-
sition, with the aim of exploiting such different
kinds of evidence at the same time. They ei-
ther tried to improve semantic relation acquisition
by putting the different evidence together into a
single classiﬁer (Pennacchiotti and Pantel, 2009)
or to improve the coverage of semantic relations
by combining and ranking the semantic relations
obtained from two source texts (Talukdar et al.,
2008).

In this paper we propose an algorithm called
Co-STAR. The main contributions of this work
can be summarized as follows.

• Co-STAR is a semi-supervised learning
method composed of two parallel and iter-
ative processes over structured and unstruc-
tured text. It was inspired by bilingual co-
training, which is a framework for hyponymy
relation acquisition from source texts in two
languages (Oh et al., 2009). Like bilingual
co-training, two processes in Co-STAR op-
erate independently on structured text and
unstructured text. These two processes are
trained in a supervised manner with their
initial training data and then each of them
tries to enlarge the existing training data of
the other by iteratively exchanging what they

843

have learned (more precisely, by transfer-
ring reliable classiﬁcation results on com-
mon instances to one another) (see Section
4 for comparison Co-STAR and bilingual
co-training). Unlike the ensemble semantic
framework (Pennacchiotti and Pantel, 2009),
Co-STAR does not have a single “master”
classiﬁer or ranker to integrate the differ-
ent evidence found in structured and unstruc-
tured text. We experimentally show that, at
least in our setting, Co-STAR works better
than a single “master” classiﬁer.

• Common relation instances found in both
structured and unstructured text act as a
communication channel between the two ac-
quisition processes. Each process in Co-
STAR classiﬁes common relation instances
and then transfers its high-conﬁdence classi-
ﬁcation results to training data of the other
process (as shown in Fig. 1), in order to im-
prove classiﬁcation results of the other pro-
cess. Moreover, the efﬁciency of this ex-
change can be boosted by increasing the
“bandwidth” of this channel. For this pur-
pose each separate acquisition process auto-
matically generates a set of relation instances
that are likely to be negative. In our experi-
ments, we show that the above idea proved
highly effective.

• Finally, the acquisition algorithm we propose
is robust against noisy training data. We
show this by training one classiﬁer in Co-
STAR with manually labeled data and train-
ing the other with automatically generated
but noisy training data. We found that Co-
STAR performs well in this setting. This is-
sue is discussed in Section 6.

This paper is organized as follows. Sections 2
and 3 precisely describe our algorithm. Section 4
describes related work. Sections 5 and 6 describe
our experiments and present their results. Conclu-
sions are drawn in Section 7.

2 Co-STAR
Co-STAR consists of two processes that simul-
taneously but independently extract and classify

Figure 1: Concept of Co-STAR.

hyponymy relation instances from structured and
unstructured text. The core of Co-STAR is the
collaboration between the two processes, which
continually exchange and compare their acquired
knowledge on hyponymy relations. This collabo-
ration is made possible through common instances
shared by both processes. These common in-
stances are classiﬁed separately by each process,
but high-conﬁdence classiﬁcation results by one
process can be transferred as new training data to
the other.

2.1 Common Instances
Let S and U represent a source (i.e.
corpus)
of structured and unstructured text, respectively.
In this paper, we use the hierarchical layout of
Wikipedia articles and the Wikipedia category
system as structured text S (see Section 3.1), and
a corpus of ordinary Web pages as unstructured
text U. Let XS and XU denote a set of hyponymy
relation candidates extracted from S and U, re-
spectively. XS is extracted from the hierarchi-
cal layout of Wikipedia articles (Oh et al., 2009)
and XU is extracted by lexico-syntactic patterns
for hyponymy relations (i.e., hyponym such as hy-
ponymy) (Ando et al., 2004) (see Section 3 for a
detailed explanation)

We deﬁne two types of common instances,
called “genuine” common instances (G) and “vir-
tual” common instances (V ). The set of common
instances is denoted by Y = G ∪ V . Genuine
common instances are hyponymy relation candi-
dates found in both S and U (G = XS ∩ XU ). On

Structured	  Texts	

…..	

Training	

Further	  Enlarged	  
Training	  Data	  
for	  Structured	  Texts	

Classiﬁer	

Training	

Enlarged	  	  
Training	  Data	  
for	  Structured	  Text	

Classiﬁer	

Training	

Training	  Data	  
for	  Structured	  Texts	

Common	  
instances	
Transferring	  

reliable	  

classiﬁca0on	  

results	  of	  
classiﬁers	

Transferring	  

reliable	  

classiﬁca0on	  

results	  of	  
classiﬁers	

Unstructured	  Texts	

…..	

Training	

Further	  Enlarged	  
Training	  Data	  
for	  Unstructured	  Texts	

Classiﬁer	

Training	

Enlarged	  	  
Training	  Data	  
for	  Unstructured	  Texts	
Classiﬁer	

Training	

Training	  Data	  
For	  Unstructured	  Texts	

I
t
e
r
a
0
o
n
	

844

the other hand, term pairs are obtained as virtual
common instances when:

and U) are more likely to hold a hyponymy re-
lation than virtual common instances.

• 1) they are extracted as hyponymy relation

candidates in either S or U and;

• 2) they do not seem to be a hyponymy rela-

tion in the other text

In summary, genuine and virtual common in-
stances can be used as different ground for collab-
oration as well as broader collaboration channel
between the two processes than genuine common
instances used alone.

The ﬁrst condition corresponds to XS ⊕ XU .
Term pairs satisfying the second condition are de-
ﬁned as RS and RU , where RS ∩ XS = φ and
RU ∩ XU = φ.
RS contains term pairs that are found in the
Wikipedia category system but neither term ap-
pears as ancestor of the other1. For example, (nu-
trition,protein) and (viruses,viral disease), respec-
tively, hold a category-article relation, where nu-
trition is not ancestor of viruses and vice versa in
the Wikipedia category system. Here, term pairs,
such as (nutrition, viruses) and (viral disease, nu-
trition), can be ones in RS.

RU is a set of term pairs extracted from U

when:

• they are not hyponymy relation candidates in

XU and;

• they regularly co-occur in the same sentence
as arguments of the same verb (e.g., A cause
B or A is made by B);

As a result, term pairs in RU are thought as hold-
ing some other semantic relations (e.g., A and B
in “A cause B” may hold a cause/effect relation)
than hyponymy relation. Finally, virtual common
instances are deﬁned as:

• V = (XS ⊕ XU ) ∩ (RS ∪ RU )
The virtual common instances, from the view-
point of either S or U, are unlikely to hold a hy-
ponymy relation even if they are extracted as hy-
ponymy relation candidates in the other text. Thus
many virtual common instances would be a nega-
tive example for hyponymy relation acquisition.
On the other hand, genuine common instances
(hyponymy relation candidates found in both S

1A term pair often holds a hyponymy relation if one term
in the term pair is a parent of the other in the Wikipedia cat-
egory system (Suchanek et al., 2007).

2.2 Algorithm

We assume that classiﬁer c assigns class label
cl ∈ {yes, no} (“yes” (hyponymy relation) or
“no” (not a hyponymy relation)) to instances in
x ∈ X with conﬁdence value r ∈ R+, a non-
negative real number. We denote the classiﬁca-
tion result by classiﬁer c as c(x) = (x, cl, r). We
used support vector machines (SVMs) in our ex-
periments and the absolute value of the distance
between a sample and the hyperplane determined
by the SVMs as conﬁdence value r.

U )
S and L0
S and cn
U )

1: Input: Common instances (Y = G ∪ V ) and
the initial training data (L0
2: Output: Two classiﬁers (cn
3: i = 0
4: repeat
5:
6:
7:
8:
9:

S := LEARN (Li
ci
S)
U := LEARN (Li
ci
U )
S ∪ Li
CRi
U}
S ∪ Li
CRi
U}
for each (y, clS, rS) ∈ T opN (CRi
S) and
(y, clU , rU ) ∈ CRi

S(y)|y ∈ Y , y /∈ Li
U (y)|y ∈ Y , y /∈ Li

S := {ci
U := {ci

U do

10:

11:
12:
13:
14:

15:

U

L(i+1)

:= L(i+1)

if (rS > α and rU < β)
or (rS > α and clS = clU ) then
∪ {(y, clS)}

U
end if
end for
for each (y, clU , rU ) ∈ T opN (CRi
(y, clS, rS) ∈ CRi

S do

if (rU > α and rS < β)
or (rU > α and clS = clU ) then
∪ {(y, clU )}

:= L(i+1)

L(i+1)

S

16:
17:
18:
19:
20: until stop condition is met

S
end if
end for
i = i + 1

Figure 2: Co-STAR algorithm

U ) and

845

The Co-STAR algorithm is given in Fig. 2. The
algorithm is interpreted as an iterative procedure
1) to train classiﬁers (ci
U , ci
S) with the existing
training data (Li
S and Li
U ) and 2) to select new
training instances from the common instances to
be added to existing training data. These are re-
peated until stop condition is met.

S and ci

S and L0

In the initial stage, two classiﬁers c0

S and c0
U
are trained with manually prepared labeled in-
stances (or training data) L0
U , respec-
tively. The learning procedure is denoted by
c = LEARN (L) in lines 5–6, where c is a re-
sulting classiﬁer. Then ci
U are applied
to classify common instances in Y (lines 7–8).
We denote CRi
S as a set of the classiﬁcation re-
sults of ci
S for common instances, which are not
included in the current training data Li
U .
S ∪ Li
Lines 9–13 describe a way of selecting instances
in CRi
S to be added to the existing training data
S acts as a teacher
in U. During the selection, ci
and ci
S) is a set of
S(y) = (y, clS, rS), whose rS is the top-N high-
ci
S. (In our experiments, N = 900.) The
est in CRi
teacher instructs his student the class label of y if
the teacher can decide the class label of y with a
certain level of conﬁdence (rS > α) and the stu-
dent satisﬁes one of the following two conditions:

U as a student. T opN (CRi

• the student agrees with the teacher on class

label of y (clS = clU ) or

• the student’s conﬁdence in classifying y is

low (rU < β)

U

rU < β enables the teacher to instruct his student
in spite of their disagreement over a class label.
If one of the two conditions is satisﬁed, (y, clS)
is added to existing labeled instances L(i+1)
. The
roles are reversed in lines 14–18, so that ci
U be-
comes the teacher and ci

S the student.

The iteration stops if the change in the differ-
ence between the two classiﬁers is stable enough.
The stability is estimated by d(ci
U ) in Eq. (1),
where σi represents the change in the average
difference between the conﬁdence values of the
two classiﬁers in classifying common instances.
We terminate the iteration if d(ci
U ) is smaller
than 0.001 in three consecutive rounds (Wang and

S, ci

S, ci

Zhou, 2007).

d(ci

S, ci

U ) = |σi − σ(i−1)|/|σ(i−1)|

(1)

3 Hyponymy Relation Acquisition
In this section we explain how each process ex-
tracts hyponymy relations from its respective text
source either Wikipedia or Web pages. Each pro-
cess extracts hyponymy relation candidates (de-
noted by (hyper,hypo) in this section). Because
there are many non-hyponymy relations in these
candidates2, we classify hyponymy relation can-
didates into correct hyponymy relation or not. We
used SVMs (Vapnik, 1995) for the classiﬁcation
in this paper.

3.1 Acquisition from Wikipedia

(a) Layout structure

(b) Tree structure

Figure 3: Example borrowed from Oh et al.
(2009): Layout and tree structures of Wikipedia
article TIGER

We follow the method in Oh et al. (2009) for
acquiring hyponymy relations from the Japanese
Wikipedia. Every article is transformed into a tree
structure as shown in Fig. 3, based on the items in
its hierarchical layout including title, (sub)section
headings, and list items. Candidate relations are
extracted from this tree structure by regarding a
node as a hypernym candidate and all of its subor-
dinate nodes as potential hyponyms of the hyper-
nym candidate (e.g., (TIGER, TAXONOMY) and
(TIGER, SIBERIAN TIGER) from Fig. 3). We ob-
tained 1.9× 107 Japanese hyponymy relation can-
didates from Wikipedia.

2Only 25–30% of candidates was true hyponymy relation

in our experiments.

Tiger

Range

Taxonomy

Subspecies

Bengal tiger

Malayan tiger

Siberian tiger

846

Feature from Wikipedia
(“WikiFeature”)

Type
Lexical
Structure

Feature from Web texts
(“WebFeature”)

Infobox
Lexical
Pattern

Description
Morphemes and POS of hyper and hypo; hyper and hypo themselves
Distance between hyper and hypo in a tree structure;
Lexical patterns for article or section names, where listed items often appear;
Frequently used section headings in Wikipedia (e.g., “Reference”);
Layout item type (e.g., section or list); Tree node type (e.g., root or leaf);
Parent and children nodes of hyper and hypo
Attribute type and its value obtained from Wikipedia infoboxes
Morphemes and POS of hyper and hypo; hyper and hypo themselves
Lexico-syntactic patterns applied to hyper and hypo;
PMI score between pattern and hyponymy relation candidate (hyper,hypo)
PMI score between hyper and hypo

Collocation
Noun Class Noun classes relevant to hyper and hypo

Table 1: Feature sets (WikiFeature and WebFeature): hyper and hypo represent hypernym and hyponym
parts of hyponymy relation candidates, respectively.

As features for classiﬁcation we used lex-
ical, structure, and infobox information from
Wikipedia (WikiFeature), as shown in Table 1.
Because they are the same feature sets as those
used in Oh et al. (2009), here we just give a brief
overview of the feature sets. Lexical features3
are used to recognize the lexical evidence for
hyponymy relations encoded in hyper and hypo.
For example, the common head morpheme tiger
in (TIGER, BENGAL TIGER) can be used as the
lexical evidence. Such information is provided
along with the words/morphemes and the parts of
speech of hyper and hypo, which can be multi-
word/morpheme nouns.

Structure features provide evidence found in
layout or tree structures for hyponymy relations.
For example, hyponymy relations (TIGER, BEN-
GAL TIGER) and (TIGER,MALAYAN TIGER) can
be obtained from tree structure “(root node, chil-
dren nodes of Subspecies)” in Fig 3.

3.2 Acquisition from Web Texts
As the target for hyponymy relation acquisition
from the Web, we used 5 × 107 pages from
the TSUBAKI corpus (Shinzato et al., 2008),
a 108 page Japanese Web corpus that was de-
pendency parsed with KNP (Kurohashi-Nagao
Parser) (Kurohashi and Kawahara, 2005). Hy-
ponymy relation candidates are extracted from the
corpus based on the lexico-syntactic patterns such
as “hypo nado hyper (hyper such as hypo)” and
“hypo to iu hyper (hyper called hypo)” (Ando

3MeCab

(http://mecab.sourceforge.net/)

was used to provide the lexical features.

et al., 2004). We extracted 6 × 106 Japanese
hyponymy relation candidates from the Japanese
Web texts. Features (WebFeature) used for classi-
ﬁcation are summarized in Table 1. Similar to the
hyponymy relation acquisition from Wikipedia,
lexical features are used to recognize the lexical
evidence for hyponymy relations.

Lexico-syntactic patterns for hyponymy rela-
tion show different coverage and accuracy in hy-
ponymy relation acquisition (Ando et al., 2004).
Further if multiple lexico-syntactic patterns sup-
port acquisition of hyponymy relation candidates,
these candidates are more likely to be actual hy-
ponymy relations. The pattern feature of hy-
ponymy relation candidates is used for these ev-
idence.

We use PMI (point-wise mutual information)
of hyponymy relation candidate (hyper, hypo) as
a collocation feature (Pantel and Ravichandran,
2004), where we assume that hyper and hypo in
candidates would frequently co-occur in the same
sentence if they hold a hyponymy relation.

Semantic noun classes have been regarded as
useful information in semantic relation acquisi-
tion (De Saeger et al., 2009). EM-based clus-
tering (Kazama and Torisawa, 2008) is used for
obtaining 500 semantic noun classes4 from 5 ×
105 nouns (including single-word and multi-word
ones) and their 4× 108 dependency relations with
5 × 105 verbs and other nouns in our target Web
4Because EM clustering provides a probability distri-
bution over noun class nc, we obtain discrete classes of
each noun n with a probability threshold p(nc|n) ≥
0.2 (De Saeger et al., 2009).

847

Co-training
(Blum and Mitchell, 1998)
Same
Split by human decision

Instance space
Feature space
Common instances Genuine-common

(or All unlabeled) instances

Bilingual co-training
(Oh et al., 2009)
Different
Split by languages
Genuine-common
instances (Translatable)

Co-STAR
(Proposed method)
Almost different
Split by source texts
Genuine-common and
virtual-common instances

Table 2: Differences among co-training, bilingual co-training, and Co-STAR

corpus. For example, noun class C311 includes
biological or chemical substances such as tatou
(polysaccharide) and yuukikagoubutsu (organic
compounds). Noun classes (i.e., C311) relevant to
hyper and hypo, respectively, are used as a noun
class feature.

4 Related Work
There are two frameworks, which are most rele-
vant to our work – bilingual co-training and en-
semble semantics.

The main difference between bilingual co-
training and Co-STAR lies in an instance space.
In bilingual co-training, instances are in different
spaces divided by languages while, in Co-STAR,
many instances are in different spaces divided by
their source texts. Table 2 shows differences be-
tween co-training, bilingual co-training and Co-
STAR.

Ensemble semantics is a relation acquisition
framework, where semantic relation candidates
are extracted from multiple sources and a single
ranker ranks or classiﬁes the candidates in the ﬁ-
nal step (Pennacchiotti and Pantel, 2009). In en-
semble semantics, one ranker is in charge of rank-
ing all candidates extracted from multiple sources;
while one classiﬁer classiﬁes candidates extracted
from one source in Co-STAR.

5 Experiments
We used the July version of Japanese Wikipedia
(jawiki-20090701) as structured text. We ran-
domly selected 24,000 hyponymy relation candi-
dates from those identiﬁed in Wikipedia and man-
ually checked them. 20,000 of these samples were
used as training data for our initial classiﬁer, the
rest was equally divided into development and test
data for Wikipedia. They are called “WikiSet.”
As unstructured text, we used 5 × 107 Japanese
Web pages in the TSUBAKI corpus (Shinzato et

al., 2008). Here, we manually checked 9,500
hyponymy relation candidates selected randomly
from Web texts. 7,500 of these were used as train-
ing data. The rest was split into development and
test data. We named this data “WebSet”.

In both classiﬁers, the development data was
used to select the optimal parameters, and the test
data was used to evaluate our system. We used
TinySVM (TinySVM, 2002) with a polynomial
kernel of degree 2 as a classiﬁer. α (the threshold
value indicating high conﬁdence), β (the thresh-
old value indicating low conﬁdence), and T opN
(the maximum number of training instances to be
added to the existing training data in each iter-
ation) were selected through experiments on the
development set. The combination of α = 1,
β = 0.3, and T opN=900 showed the best perfor-
mance and was used in the following experiments.
Evaluation was done by precision (P ), recall (R),
and F-measure (F ).

5.1 Results
We compare six systems. Three of these, B1–B3,
show the effect of different feature sets (“Wik-
iFeature” and “WebFeature” in Table 1) and dif-
ferent training data. We trained two separate clas-
siﬁers in B1 and B2, while we integrated feature
sets and training data for training a single classi-
ﬁer in B3. The classiﬁers in these three systems
are trained with manually prepared training data
(“WikiSet” and “WebSet”). For the purpose of our
experiment, we consider B3 as the closest possible
approximation of the ensemble semantics frame-
work (Pennacchiotti and Pantel, 2009).

• B1 consists of two completely independent
classiﬁers. Both S and U classiﬁers are
trained and tested on their own feature and
data sets (respectively “WikiSet + WikiFea-
ture” and “WebSet + WebFeature”).

848

• B2 is the same as B1, except that both clas-
siﬁers are trained with all available training
data — WikiSet and WebSet are combined
(27,500 training instances in total). However,
each classiﬁer only uses its own feature set
(WikiFeature or WebFeature)5.
• B3 adds a master classiﬁer to B1. This third
classiﬁer is trained on the complete 27,500
training instances (same as B2) using all
available features from Table 1, including
each instance’s SVM scores obtained from
the two B1 classiﬁers6. The verdict of the
master classiﬁer is considered to be the ﬁnal
classiﬁcation result.

The other three systems, BICO, Co-B, and Co-
STAR (our proposed method), are for compari-
son between bilingual co-training (Oh et al., 2009)
(BICO) and variants of Co-STAR (Co-B and Co-
STAR). Especially, we prepared Co-B and Co-
STAR to show the effect of different conﬁgura-
tions of common instances on the Co-STAR al-
gorithm. We use both B1 and B2 as the initial
classiﬁers of Co-B and Co-STAR. We notate Co-
B and Co-STAR without ‘∗’ when B1 is used as
their initial classiﬁer and those with ‘∗’ when B2
is used.

• BICO implements the bilingual co-training
in which
algorithm of (Oh et al., 2009),
two processes collaboratively acquire hy-
ponymy relations in two different languages.
For BICO, we prepared 20,000 English and
20,000 Japanese training samples (Japanese
ones are the same as training data in the
WikiSet) by hand.
• Co-B is a variant of Co-STAR that uses only
the genuine-common instances as common
instances (67,000 instances)7, to demonstrate

5Note that training instances from WebSet (or WikiSet)
can have WikiFeature (or WebFeature) if they also appear
in Wikipedia (or Web corpus). But they can always have
lexical feature, the common feature set between WikiFeature
and WebFeature.

6SVM scores are assigned to the instances in training data

in a 10-fold cross validation manner.

7Co-B can

conventional
co-
as
training (Blum and Mitchell, 1998)
in the sense that
two classiﬁers collaborate through actual common instances.

considered

be

the effectiveness of the virtual common in-
stances.
• Co-STAR is our proposed method, which
uses both genuine-common and virtual-
common instances (643,000 instances in to-
tal).

WebSet

F
73.5
75.9
76.8

R
65.2
69.6
72.0

P
P
87.8
84.3
87.4
83.4
82.2
86.1
N/A N/A N/A 84.5
89.7
86.2
89.6
85.5
88.0
85.9
83.3
87.6

63.5
69.9
76.0
80.7

73.2
77.0
80.6
82.0

WikiSet

R
74.7
79.5
77.7
81.8
74.1
76.5
81.8
81.8

F
80.7
83.2
81.7
83.1
81.2
82.5
84.8
84.6

B1
B2
B3
BICO
Co-B
Co-B∗
Co-STAR
Co-STAR∗

Table 3: Comparison of different systems

Table 3 summarizes the result. Features for
common instances in Co-B and Co-STAR are pre-
pared in the same way as training data in B2, so
that both classiﬁers can classify the common in-
stances with their trained feature sets.

Comparison between B1–B3 shows that B2 and
B3 outperform B1 in F-measure. More train-
ing data used in B2–B3 (27,500 instances for
both WebSet and WikiSet) results in higher per-
formance than that of B1 (7,500 and 20,000 in-
stances used separately). We think that the lexical
features, assigned regardless of source text to in-
stances in B2–B3, are mainly responsible for the
performance gain over B1, as they are the least
domain-dependent type of features. B2–B3 are
composed of different number of classiﬁers, each
of which is trained with different feature sets and
training instances. Despite this difference, B2 and
B3 showed similar performance in F-measure.

Co-STAR outperformed the algorithm similar
to the ensemble semantics framework (B3), al-
though we admit that a more extensive com-
parison is desirable. Further Co-STAR outper-
formed BICO. While the manual cost for build-
ing the initial training data used in Co-STAR
and BICO is hard to quantify, Co-STAR achieves
better performance with fewer training data in
total (27,500 instances) than BICO (40,000 in-
stances). The difference in performance between
Co-B and Co-STAR shows the effectiveness of

849

the automatically generated virtual-common in-
stances. From these comparison, we can see that
virtual-common instances coupled with genuine-
common instances can be leveraged to enable
more effective collaboration between the two clas-
siﬁers in Co-STAR.

As a result, our proposed method outperforms
the others in F-measure by 1.4–8.5%. We ob-
tained 4.3 × 105 hyponymy relations from Web
texts and 4.6 × 106 ones from Wikipedia8.
6 Co-STAR with Automatically

Generated Training Data

For Co-STAR, we need two sets of manually pre-
pared training data, one for structured text and the
other for unstructured text. As in any other su-
pervised system, the cost of preparing the training
data is an important issue. We therefore investi-
gated whether Co-STAR can be trained for a lower
cost by generating more of its training data auto-
matically.

We automatically built training data for Web
texts by using deﬁnition sentences9 and category
names in the Wikipedia articles, while we stuck to
manually prepared training data for Wikipedia. To
obtain hypernyms from Wikipedia article names,
we used deﬁnition-speciﬁc lexico-syntactic pat-
terns such as “hyponym is hypernym” and “hy-
ponym is a type of hypernym” (Kazama and Tori-
sawa, 2007; Sumida and Torisawa, 2008). Then,
we extracted hyponymy relations consisting of
pairs of Wikipedia category names and their mem-
ber articles when the Wikipedia category name
and the hypernym obtained from the deﬁnition
of the Wikipedia article shared the same head
word. Next, we selected a subset of the extracted
hyponymy relations that are also hyponymy re-
lation candidates in Web texts, as positive in-
stances for hyponymy relation acquisition from
Web text. We obtained around 15,000 positive in-
stances in this way. Negative instances were cho-
sen from virtual-common instances, which also
originated from the Wikipedia category system
and hyponymy relation candidates in Web texts

8We obtained them with 90% precision by setting the
SVM score threshold to 0.23 for Web texts and 0.1 for
Wikipedia.

9The ﬁrst sentences of Wikipedia articles.

(around 293,000 instances).

The automatically built training data was noisy
and its size was much bigger than manually pre-
pared training data in WebSet. Thus 7,500 in-
stances as training data (the same number of man-
ually built training data in WebSet) were ran-
domly chosen from the positive and negative in-
stances with a positive:negative ratio of 1:410.

WebSet

WikiSet

P
81.0
80.0
82.0
82.2
79.2

R
47.6
55.4
33.7
60.8
69.6

F
60.0
65.5
47.8
69.9
74.1

B1
B2
B3
Co-STAR
Co-STAR∗
Table 4: Results with automatically generated
training data

R
74.7
79.5
75.6
80.7
81.8

F
80.7
83.1
81.0
83.8
84.4

P
87.8
87.1
87.1
87.3
87.0

With the automatically built training data for
Web texts and manually prepared training data for
Wikipedia, we evaluated B1–B3 and Co-STAR,
which are the same systems in Table 3. The results
in Table 4 are encouraging. Co-STAR was robust
even when faced with noisy training data. Further
Co-STAR showed better performance than B1–
B3, although its performance in Table 4 dropped a
bit compared to Table 3. This result shows that we
can reduce the cost of manually preparing training
data for Co-STAR with only small loss of the per-
formance.

7 Conclusion
This paper proposed Co-STAR, an algorithm for
hyponymy relation acquisition from structured
and unstructured text. In Co-STAR, two indepen-
dent processes of hyponymy relation acquisition
from structured texts and unstructured texts, col-
laborate in an iterative manner through common
instances. To improve this collaboration, we in-
troduced virtual-common instances.

Through a series of experiments, we showed
that Co-STAR outperforms baseline systems and
virtual-common instances can be leveraged to
achieve better performance. We also showed that
Co-STAR is robust against noisy training data,
which requires less human effort to prepare it.

10We select the ratio by testing different ratio from 1:2 to

1:5 with our development data in WebSet and B1.

850

Shinzato, Keiji and Kentaro Torisawa. 2004. Ex-
tracting hyponyms of prespeciﬁed hypernyms from
itemizations and headings in web documents.
In
Proceedings of COLING ’04, pages 938–944.

Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008. Tsubaki: An open search engine infrastruc-
ture for developing new information access. In Pro-
ceedings of IJCNLP ’08, pages 189–196.

Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng.
2006. Semantic taxonomy induction from heteroge-
nous evidence.
In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 801–808.

Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proc. of WWW ’07, pages 697–706.

Sumida, Asuka and Kentaro Torisawa. 2008. Hack-
ing Wikipedia for hyponymy relation acquisition.
In Proc. of the Third International Joint Conference
on Natural Language Processing (IJCNLP), pages
883–888, January.

Talukdar, Partha Pratim, Joseph Reisinger, Marius
Pasca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proc. of EMNLP08, pages 582–590.

TinySVM. 2002. http://chasen.org/˜taku/

software/TinySVM.

Van Durme, Benjamin and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction.
In Proc. of AAAI08, pages
1243–1248.

Vapnik, Vladimir N. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.

Wang, Wei and Zhi-Hua Zhou. 2007. Analyzing co-
training style algorithms. In ECML ’07: Proceed-
ings of the 18th European conference on Machine
Learning, pages 454–465.

References
Ando, Maya, Satoshi Sekine, and Shun Ishiza. 2004.
Automatic extraction of hyponyms from Japanese
newspaper using lexico-syntactic patterns. In Proc.
of LREC ’04.

Blum, Avrim and Tom Mitchell.

1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT’ 98: Proceedings of the eleventh annual con-
ference on Computational learning theory, pages
92–100.

De Saeger, Stijn, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proc. of ICDM 2009, pages 764–769.

Hearst, Marti A. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics,
pages 539–545.

Kazama, Jun’ichi and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proc. of Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 698–707.

Kazama, Jun’ichi and Kentaro Torisawa. 2008.

In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations.
In
Proceedings of ACL-08: HLT, pages 407–415.

Kurohashi, Sadao and Daisuke Kawahara. 2005. KNP

(Kurohashi-Nagao Parser) 2.0 users manual.

Oh, Jong-Hoon, Kiyotaka Uchimoto, and Kentaro
Torisawa. 2009. Bilingual co-training for mono-
lingual hyponymy-relation acquisition. In Proc. of
ACL-09: IJCNLP, pages 432–440.

Pantel, Patrick and Deepak Ravichandran. 2004. Au-
In Proc. of

tomatically labeling semantic classes.
HLT-NAACL ’04, pages 321–328.

Pantel, Patrick, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP ’09, pages 938–947.

Pennacchiotti, Marco and Patrick Pantel. 2009. En-
tity extraction via ensemble semantics. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 238–247.

Ravi, Sujith and Marius Pasca. 2008. Using structured
In CIKM-

text for large-scale attribute extraction.
08, pages 1183–1192.

