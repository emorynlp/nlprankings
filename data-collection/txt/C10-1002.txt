Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 10–18,

Beijing, August 2010

10

Identifying Multi-word Expressions by

Leveraging Morphological and Syntactic Idiosyncrasy

Hassan Al-Haj

Language Technologies Institute

Carnegie Mellon University

Shuly Wintner

Department of Computer Science

University of Haifa

hhaj@cs.cmu.edu

shuly@cs.haifa.ac.il

Abstract

Multi-word expressions constitute a sig-
niﬁcant portion of the lexicon of every
natural language, and handling them cor-
rectly is mandatory for various NLP appli-
cations. Yet such entities are notoriously
hard to deﬁne, and are consequently miss-
ing from standard lexicons and dictionar-
ies. Multi-word expressions exhibit id-
iosyncratic behavior on various levels: or-
thographic, morphological, syntactic and
semantic.
In this work we take advan-
tage of the morphological and syntactic
idiosyncrasy of Hebrew noun compounds
and employ it to extract such expressions
from text corpora. We show that relying
on linguistic information dramatically im-
proves the accuracy of compound extrac-
tion, reducing over one third of the errors
compared with the best baseline.

Introduction

1
Multi-word expressions (MWEs) are notoriously
hard to deﬁne. They span a range of constructions,
from completely frozen, semantically opaque id-
iomatic expressions, to frequent but morpholog-
ically productive and semantically compositional
collocations. Various linguistic processes (ortho-
graphic, morphological, syntactic, semantic, and
cognitive) apply to MWEs in idiosyncratic ways.
Notably, MWEs blur the distinction between the
lexicon and the grammar, since they often have
some properties of words and some of phrases.

In this work we deﬁne MWEs as expressions
whose linguistic properties (morphological, syn-
tactic or semantic) are not directly derived from
the properties of their word constituents. This is
a functional deﬁnition, driven by a practical mo-
tivation: any natural language processing (NLP)

application that cares about morphology, syntax
or semantics must consequently store MWEs in
the lexicon.

MWEs are numerous and constitute a signif-
icant portion of the lexicon of any natural lan-
guage. They are a heterogeneous class of con-
structions with diverse sets of characteristics.
Morphologically, some MWEs allow some of
their constituents to freely inﬂect while restricting
(or even preventing) the inﬂection of other con-
stituents. MWEs may allow constituents to un-
dergo non-standard morphological inﬂections that
they would not undergo in isolation. Some MWEs
contain words that never occur outside the context
of the MWE. Syntactically, some MWEs appear
in one rigid pattern (and a ﬁxed order), while oth-
ers permit various syntactic transformations. Se-
mantically, the compositionality of MWEs (i.e.,
the degree to which the meaning of the whole ex-
pression results from combining the meanings of
its individual words when they occur in isolation)
is gradual.

These morphological, syntactic and semantic
idiosyncrasies make MWEs a challenge for NLP
applications (Sag et al., 2002). They are even
more challenging in languages with complex mor-
phology, because of the unique interaction of mor-
phological and orthographic processes with the
lexical speciﬁcation of MWEs (Oﬂazer et al.,
2004; Alegria et al., 2004).

Because the idiosyncratic features of MWEs
cannot be predicted on the basis of their com-
ponent words, they must be stored in the lexi-
con of NLP applications. Handling MWEs cor-
rectly is beneﬁcial for a variety of applications,
including information retrieval, building ontolo-
gies, text alignment, and machine translation. Au-
tomatic identiﬁcation and corpus-based extraction
of MWEs is thus crucial for such (and several
other) applications.

11

In this work we describe an approach that lever-
ages the morphological and syntactic idiosyncrasy
of a certain class of Hebrew1 MWEs, namely
noun compounds, to help identify such expres-
sions in texts. While the main contribution of
this work is a system that can distinguish be-
tween MWE and non-MWE instances of a partic-
ular construction in Hebrew, thereby facilitating
faster and more accurate integration of MWEs in
a large-coverage lexicon of the language, we be-
lieve that it carries added value to anyone inter-
ested in MWEs. The technique that we propose
here should be applicable in principle to any lan-
guage in which MWEs exhibit linguistically id-
iosyncratic behavior.

We describe the properties of Hebrew noun-
noun constructions in Section 2, and specify the
irregularities exhibited by compounds. Section 3
presents the experimental setup and the main re-
sults. Compared with the best (collocation-based)
baseline, our approach reduces over 30% of the
errors, yielding accuracy of over 80%. We dis-
cuss related work in Section 4 and conclude with
suggestions for future research.

2 Hebrew noun-noun constructions
We focus on Hebrew noun-noun constructions;
these are extremely frequent constructions, and
while many of them are fully compositional, oth-
ers, called noun compounds (or just compounds)
here, are clearly MWEs. We ﬁrst discuss the gen-
eral construction and then describe the peculiar,
idiosyncratic properties of compounds.

2.1 The general case
Hebrew nouns inﬂect for number (singular and
plural) and, when the noun denotes an animate en-
tity, for gender (masculine and feminine). In ad-
dition, nouns come in three states: indeﬁnite, def-
inite and a construct state that is used in genitive
constructions. Table 1 demonstrates the paradigm.
A noun-noun construction (henceforth NNC)
consists of a construct-state noun, called head
here,
the modi-
ﬁer (Borer, 1988; Borer, 1996; Glinert, 1989).

followed by a noun phrase,

1To facilitate readability we use a transliteration of He-
brew using Roman characters; the letters used, in Hebrew
lexicographic order, are abgdhwzxTiklmns‘pcqrˇst.

State
indeﬁnite
deﬁnite
construct

F/Pl
M/Sg F/Sg M/Pl
ildim
ild
ildwt
hildim hildwt
hild
ild
ildi
ildwt

ildh
hildh
ildt

Table 1: The noun paradigm, demonstrated on ild
“child”

The semantic relation between the two is usually,
but not always, related to possession (Levi, 1976).
Construct-state nouns only occur in the context of
NNC, and can never occur in isolation. When a
NNC is deﬁnite, the deﬁnite article is expressed
on its modiﬁer (Wintner, 2000).

h‘itwn
the-journal

hw‘dh
the-committee

In the examples below, we explicitly indicate
construct-state nouns by the morpheme ‘.CONST’
in the gloss; and deﬁnite nouns are indicated by
the morpheme ‘the-’. We provide both a literal
and a non-literal meaning of the MWE examples.
Expressions that have a literal, but not the ex-
pected MWE meaning, are preceded by ‘#’.
Example 1 (Noun-noun constructions)
hxlTt
decision.CONST
“the committee decision”
‘wrk
editor.CONST
“the journal editor”
‘wrk
din
editor.CONST
law
“law editor” =⇒ lawyer
xwlim
bti
houses.CONST
patients
“patient houses” =⇒ hospitals
2.2 Noun compounds: Linguistic properties
While many of the NNCs are free, compositional
combinations of words, some are not; we use the
term noun compounds for the latter group. Com-
pounds typically (but not necessarily) have non-
compositional meaning; presumably due to their
opaque, more lexical meaning, they also differ
from other NNCs in their morphological and syn-
tactic behavior. Some of these distinctive prop-
erties are listed below, to motivate the methodol-
ogy that we propose in Section 3 to distinguish
between compounds and non-MWE NNCs.

12

2.2.1 Limited inﬂection

When a NNC consists of two nouns, the sec-
ond can typically occur in either singular or plural
form. Compounds often limit the possibilities to
only one of those.
Example 2 (No plural form of the modiﬁer)
‘wrki
editors-.CONST
“the journals’ editors”

h‘itwnim
the-journals

hdin
the-law

‘wrki
editors.CONST
“the law editors” =⇒ the lawyers
#wrki
editors.CONST
Example 3 (No singular form of the modiﬁer)
kiwwn
direction.CONST
“the wind’s direction”

hrwx
the-wind

hdinim
the-laws

kiwwn
direction.CONST
“the winds’ direction”

hrwxwt
the-winds

h-rwxwt
the-winds

ˇswˇsnt
lily.CONST
“lily of the winds” =⇒ compass rose
#ˇswˇsnt
lily.CONST

h-rwx
the-wind

2.2.2 Limited syntactic variation

Since NNCs typically denote genitive (posses-
sive) constructions, they can be paraphrased by a
construction that uses the genitive preposition ˇsl
“of” (or, in some cases, other prepositions). These
syntactic variants are often restricted in the case of
compounds.
Example 4 (Limited paraphrasing)
h‘wrk
the-editor
“the journal editor”

h‘itwn
the-journal

ˇsl
of

ˇsl
of

hdin
the-law

#h‘wrk
the-editor
Example 5 (Limited paraphrasing)
m‘il
cmr
coat.CONST wool
“wool coat”

from-wool

m‘il mcmr
coat
“wool coat”
cmr
wool.CONST
“steel wool” =⇒ steel wool
#cmr mpldh
wool
2.2.3 Limited syntactic modiﬁcation

from-steel

pldh
steel

h’itwn
the-journal-m

NNCs typically allow adjectival modiﬁcation
of either of their constituents. Since compounds
tend to be more semantically opaque, it is of-
ten only possible to modify the entire compound,
but not any of the constituents.
In the follow-
ing example, note that ‘wrkt “editor” is feminine,
whereas ‘itwn “journal” is masculine; adjectives
must agree on gender with the noun they modify.
Example 6 (Limited adjectival modiﬁcation)
’wrkt
editor-f.CONST
“the journal editor”
‘wrkt
editor-f.CONST
“the new editor of the journal”
‘wrkt
editor-f.CONST
“the editor of the new journal”
‘wrkt
hdin
editor-f.CONST
the-law-m the-new-f
“the new law editor” =⇒ the new lawyer
#‘wrkt
editor-f.CONST
2.2.4 Limited coordination

h‘itwn
the-journal-m the-new-m

h‘itwn
the-journal-m the-new-f

hdin
the-law-m the-new-m

hxdˇsh

hxdˇsh

hxdˇs

hxdˇs

Two NNCs that share a common head can be
conjoined using the coordinating conjunction w
“and”. This possibility is often blocked in the case
of compounds.
Example 7 (Limited coordination)
mwsdwt
institutions.CONST
“education and health institutions”
bti
houses.CONST
“book houses” =⇒ schools

wbriawt
and-health

xinwk
education

spr
book

13

xwlim
patients

bti
houses.CONST
“patient houses” =⇒ hospitals
wxwlim
#bti
and-patients
houses.CONST

spr
book

Identiﬁcation of noun compounds

3
In this section we describe a system that identi-
ﬁes noun compounds in Hebrew text, and extracts
them in order to extend the lexicon. We capitalize
on the morphological and syntactic irregularities
of noun compounds described in Section 2.2.

Given a large monolingual corpus,

the text
is ﬁrst morphologically analyzed and disam-
biguated. Then, all NNCs (candidate noun com-
pounds) are extracted from the morphologically
disambiguated text.
For each candidate noun
compound we deﬁne a set of features (Section 3.3)
based on the idiosyncratic morphological and syn-
tactic properties deﬁned in Section 2.2. These
features inform a support vector machine classi-
ﬁer which is then used to identify the noun com-
pounds in the set of NNCs with high accuracy
(Section 3.5).

3.1 Resources
We use (a subset of) the Corpus of Contempo-
rary Hebrew (Itai and Wintner, 2008) which con-
sists of four sub-corpora: The Knesset corpus
contains the Israeli parliament proceedings from
2004-2005; the Haaretz corpus contains articles
from the Haaretz newspaper from 1991; The-
Marker corpus contains ﬁnancial articles from the
TheMarker newspaper from 2002; and the Arutz
7 corpus contains newswire articles from 2001-
2006. Corpora sizes are listed in Table 2.

Corpus
Knesset
Harretz
The Marker
Arutz 7
Total

Number of tokens
12,742,879
463,085
684,801
7,714,309
21,605,074

Table 2: Corpus data

The entire corpus was morphologically ana-
lyzed (Yona and Wintner, 2008; Itai and Wintner,

2008) and POS-tagged (Bar-haim et al., 2008);
note that no syntactic parser is available for He-
brew. From the morphologically disambiguated
corpus, we extract all bi-grams in which the ﬁrst
token is a noun in the construct state and the sec-
ond token is a noun that is not in the construct
state, i.e., all two-word NNC candidates.

3.2 Annotation
For training and evaluation, we select the NNCs
that occur at least 100 times in the corpus, yield-
ing 1060 NNCs. These NNCs were annotated
by three annotators, who were asked to classify
them to the following four groups: compounds
(+); non-compounds (–); unsure (0); and errors of
the morphological processor (i.e., the candidate is
not a NNC at all). Table 3 lists the number of can-
didates in each class.

Annotator

1
2
3

+
314
335
400

–
332
403
630

0
238
179
16

err
176
143
14

Table 3: NNC classiﬁcation by annotator

We adopt a conservative approach in combin-
ing the three annotations. First, we eliminate 204
NNCs that were tagged as errors by at least one
annotator. For the remaining NNCs, a candidate is
considered a compound or a non-compound only
if all three annotators agree on its classiﬁcation.
This reduces the annotated data to 463 instances,
of which 205 are compounds and 258 are clear
cases of non-compound NNCs.2

3.3 Linguistically-motivated features
We deﬁne a set of features based on the idiosyn-
cratic properties of noun compounds deﬁned in
Section 2.2. For each candidate NNC, we com-
pute counts which reﬂect the likelihood of it ex-
hibiting one of the linguistic properties.

Refer back to Section 2.2. We focus on the
property of limited inﬂection (Section 2.2.1), and
deﬁne features 1–8 to reﬂect it. To reﬂect limited
syntactic variation (Section 2.2.2) we deﬁne fea-
tures 9–10. Feature 11 addresses the phenomenon

2This annotated corpus is freely available for download.

14

of limited coordination (Section 2.2.4). To reﬂect
limited syntactic modiﬁcation (Section 2.2.3) we
deﬁne feature 12. .

For each NNC candidate N1 N2, the following

features are deﬁned:

1. The number of occurrences of the NNC in

which both constituents are in singular.

2. The number of occurrences of the NNC in
which N1 is in singular and N2 is in plural.

3. The number of occurrences of the NNC in
which N1 is in plural and N2 is in singular.

4. The number of occurrences of the NNC in

which both constituents are in plural.

5. The number of occurrences of N1 in plural

outside the expression.

6. The number of occurrences of N1 in singular

outside the expression.

7. The number of occurrences of N2 in plural

outside the expression.

8. The number of occurrences of N2 in singular

outside the expression.

9. The number of occurrences of N1 ˇsl N2 “N1

of N2” in the corpus.

10. The number of occurrences of N1 m N2 “N1

from N2” in the corpus.

11. The number of occurrences of N1 N2 w N3
“N1 N2 and N3” in the corpus, where N3 is
an indeﬁnite, non-construct-state noun.

12. The number of occurrences of N1 N2 Adj in
the corpus, where the adjective Adj agrees
with N2 on both gender and number, while
disagreeing with N1 on at least one of these
attributes.

We also deﬁne four features that represent known
collocation measures (Evert and Krenn, 2001):
Point-wise mutual information (PMI); T-Score;
log-likelihood; and the raw frequency of N1 N2
in the corpus.3

3A detailed description of these measures is given by
Manning and Sch¨utze (1999, Chapter 5); see also http:
//www.collocations.de/, where several other asso-
ciation measures are discussed as well.

3.4 Training and evaluation
For each NNC in the annotated set of Section 3.2
we create a vector of the 16 features described in
Section 3.3 (12 linguistically-motivated features
plus four collocation measures). We obtain a list
of 463 instances, of which 205 are positive ex-
amples (noun compounds) and 258 are negative.
We use this set for training and evaluation of a
two class soft margin SVM classiﬁer (Chang and
Lin, 2001) with a radial basis function kernel. We
experiment below with different combinations of
features, where for each combination we use 10-
fold cross-validation over the 463 NNcs to evalu-
ate the classiﬁer. We report Precision, Recall, F-
score and Accuracy (averaged over the 10 folds).

3.5 Results
The results of the different classiﬁers that we
trained are given in Table 4. The ﬁrst four rows
of the table show the performance of classiﬁers
trained using each of the four different colloca-
tion measure features alone. Both PMI and Log-
likelihood outperform the other collocation mea-
sures, with an F-score of 60, which we consider
our baseline. We also report the performance of
two combinations of collocation measures, which
yield small improvement. The best combinations
provide accuracy of about 70% and F-score of 63.
The remaining rows report results using the
linguistically-motivated features (LMF) of Sec-
tion 3.3. These features alone yield accuracy of
77.75% and an F-score of 76. Adding also Log-
likelihood improves F-score by 1.16 and accuracy
by 1.29%. Finally, using Log-likelihood with a
subset of the LMF consisting of features 1-2, 4-
6, 9-10 and 12 (see below) yields the best re-
sults, namely accuracy of over 80% and F-score
of 78.85, reﬂecting a reduction of over one third
in classiﬁcation error rate compared with the base-
line.

3.6 Optimizing feature combination
We search for the combination of linguistically-
motivated features that would yield the best per-
formance. Training a classiﬁer on all possible
feature combinations is clearly infeasible.
In-
stead, we follow a more efﬁcient greedy approach,
whereby we start with the best collocation mea-

15

Features

Accuracy

PMI
Frequency
T-Score
Log-likelihood
T-score+Log-likelihood
PMI+Log-likelihood
LMF
LMF+PMI
LMF+Log-likelihood
Log-likelihood+LMF[1-2,4-6,9-10,12]

67.17
60.47
61.98
69.33
70.62
69.97
77.75
77.32
79.04
80.77

Precision Recall
56.09
32.19
42.92
51.21
56.09
58.53
81.46
81.95
81.95
80.97

64.97
60.00
59.86
71.42
71.42
68.96
71.98
71.18
73.68
76.85

F-score
60.20
41.90
50.00
59.65
62.84
63.32
76.43
76.19
77.59
78.85

Table 4: Results: 10-Fold accuracy, precision, recall, and F-score for classiﬁers trained using different
combinations of features. LMF stands for linguistically-motivated features

sure, Log-likelihood, and add other features one at
a time, in the order in which they are listed in Sec-
tion 3.3. After adding each feature the classiﬁer is
retrained; the feature is retained in the feature set
only if adding it improves the 10-fold F-score of
the current feature set.

Table 5 lists the results of this experiment. For
each feature set the difference in the 10-fold F-
score compared to the previous feature set is listed
in parentheses. The results show that the best fea-
ture combination improves the F-score by 1.26,
compared with using all features. This experi-
ments shows that features 3, 7, 8 and 11 turn out
not to be useful, and the classiﬁer is more accurate
without them. We also tried this approach with
PMI as the starting feature, with very similar re-
sults.

Feature set

F-score

Log-likelihood
Log-likelihood,1
Log-likelihood,1-2
Log-likelihood,1-3
Log-likelihood,1-2,4
Log-likelihood,1-2,4-5
Log-likelihood,1-2,4-6
Log-likelihood,1-2,4-7
Log-likelihood,1-2,4-6,8
Log-likelihood,1-2,4-6,9
Log-likelihood,1-2,4-6,9-10
Log-likelihood,1-2,4-6,9-11
Log-likelihood,1-2,4-6,9-10,12

59.65
60.34
65.42
64.87
66.66
70.00
74.37
73.78
73.58
78.72
78.83
77.37
78.85

(+0.68)
(+5.08)
(-0.54)
(+1.78)
(+3.33)
(+4.37)
(–0.58)
(–0.79)
(+4.35)
(+0.10)
(–1.46)
(+0.02)

Table 5: Optimizing the set of linguistically-
motivated features

4 Related work

There has been a growing awareness in the re-
search community of the problems that MWEs
pose, both in linguistics and in NLP (Villavicencio
et al., 2005). Recent works address the deﬁnition,
lexical representation and computational process-
ing of MWEs, as well as algorithms for extracting
them from data.

Focusing on acquisition of MWEs, early ap-
proaches concentrated on their collocational be-
havior (Church and Hanks, 1989). Pecina (2008)
compares 55 different association measures in
ranking German Adj-N and PP-Verb colloca-
tion candidates. This work shows that combin-
ing different collocation measures using standard
statistical-classiﬁcation methods (such as Linear
Logistic Regression and Neural Networks) gives
a signiﬁcant improvement over using a single col-
location measure. Our results show that this is
indeed the case, but the contribution of colloca-
tion methods is limited, and more information is
needed in order to distinguish frequent colloca-
tions from bona ﬁde MWEs.

Other works show that adding linguistic infor-
mation to collocation measures can improve iden-
tiﬁcation accuracy. Several approaches rely on the
semantic opacity of MWEs; but very few seman-
tic resources are available for Hebrew (the He-
brew WordNet (Ordan and Wintner, 2007), the
only lexical semantic resource for this language,
is small and too limited).
Instead, we capital-

16

ize on the morphological and syntactic irregular-
ities that MWEs exhibit, using computational re-
sources that are more readily-available.

Ramisch et al. (2008) evaluate a number of
association measures on the task of identifying
English Verb-Particle Constructions and German
Adjective-Noun pairs. They show that adding
linguistic information (mostly POS and POS-
sequence patterns) to the association measure
yields a signiﬁcant improvement in performance
over using pure frequency. We follow this line
of research by deﬁning a number of syntactic pat-
terns as a source of linguistic information. In ad-
dition, our linguistic features are much more spe-
ciﬁc to the phenomenon we are interested in, and
the syntactic patterns are enriched by morpholog-
ical information pertaining to the idiosyncrasy of
MWEs; we believe that this explains the improved
performance compared to the baseline.

Several works address the lexical ﬁxedness or
syntactic ﬁxedness of (certain types of) MWEs in
order to extract them from texts. An expression
is considered lexically ﬁxed if replacing any of its
constituents by a semantically (and syntactically)
similar word generally results in an invalid or lit-
eral expression. Syntactically ﬁxed expressions
prohibit (or restrict) syntactic variation.

For example, Van de Cruys and Villada Moir´on
(2007) use lexical ﬁxedness to extract Dutch Verb-
Noun idiomatic combinations (VNICs). Bannard
(2007) uses syntactic ﬁxedness to identify En-
glish VNICs. Another work uses both the syn-
tactic and the lexical ﬁxedness of VNICs in or-
der to distinguish them from non-idiomatic ones,
and eventually to extract them from corpora (Fa-
zly and Stevenson, 2006). While these approaches
are in line with ours, they require lexical seman-
tic resources (e.g., a database that determines se-
mantic similarity among words) and syntactic re-
sources (parsers) that are unavailable for Hebrew
(and many other languages). Our approach only
requires morphological processing, which is more
readily-available for several languages.

Another unique feature of our work is that
it computationally addresses Hebrew (and, more
generally, Semitic) MWEs for the ﬁrst
time.
Berman and Ravid (1986) deﬁne the dictionary
degree of noun compounds in Hebrew as their

closeness to a single word from a grammatical
point of view, as judged by the manner in which
they are grasped by language speakers. A group
of 120 Hebrew speakers were asked to assign a
dictionary degree (from 1 to 5) to a list of 30
noun compounds. An analysis of the question-
naire results revealed that language speaker share
a common dictionary, where the highest degree of
agreement was achieved on the ends of the dictio-
nary degree spectrum. Another conclusion is that
both the pragmatic uses of the noun compound
and the semantic relation between its constituents
deﬁne the dictionary degree of the compound. Not
having access to semantic and pragmatic knowl-
edge, we are trying to approximate it using mor-
phology.

Attia (2005) proposes methods to process
ﬁxed, semi-ﬁxed, and syntactically-ﬂexible Ara-
bic MWEs (adopting the classiﬁcation and the ter-
minology of Sag et al. (2002)). Fabri (2009) pro-
vides an overview of the different types of com-
pounds (14 in total) in present-day Maltese, fo-
cusing on one type of compounds consisting of an
adjective followed by a noun. He also provides
morphological, syntactic, and semantic properties
of this group which distinguishes them from other
non-compound constructions. Automatic identiﬁ-
cation of MWEs is not addressed in either of these
works.

5 Conclusions and future work
We described a system that can identify Hebrew
noun compounds with high accuracy, distinguish-
ing them from non-idiomatic noun-noun construc-
tions. The methodology we advocate is based on
careful examination of the linguistic peculiarities
of the construction, followed by corpus-based ap-
proximation of these properties via a general ma-
chine learning algorithm that is fed with features
based on the linguistic properties. While our ap-
plication is limited to a particular construction in
a particular language, we are conﬁdent that it can
be equally well applied to other constructions and
other languages, as long as the targeted MWEs
exhibit a consistent set of irregular features (es-
pecially in the morphology).

This work can be extended in various direc-
tions. Addressing other constructions is relatively

17

easy, and requires only a theoretical linguistic in-
vestigation of the construction. We are currently
interested in extending the system to cope also
with Adjective-Noun, Noun-Adjective and Verb-
Preposition constructions in Hebrew.

The accuracy of MWE acquisition systems can
be further improved by combining our morpho-
logical and syntactic features with semantically
informed features such as translational entropy
computed from a parallel corpus (Villada Moir´on
and Tiedemann, 2006), or features that can cap-
ture the local linguistic context of the expression
using latent semantic analysis (Katz and Gies-
brecht, 2006). We are currently working on the
former direction (Tsvetkov and Wintner, 2010b),
utilizing a small Hebrew-English parallel corpus
(Tsvetkov and Wintner, 2010a).

Finally, we are interested in evaluating the
methodology proposed in this paper to other lan-
guages with complex morphology, in particular to
Arabic. We leave this direction to future research.

Acknowledgments

research was
SCIENCE

supported by THE IS-
This
RAEL
FOUNDATION (grants
No. 137/06, 1269/07). We are grateful to Alon
Itai for his continuous help and advice throughout
the course of this project, and to Bracha Nir for
very useful comments. We also wish to thank
Yulia Tsvetkov and Gily Chen for their annotation
work.

References
Alegria,

I˜naki, Olatz Ansa, Xabier Artola, Nerea
Ezeiza, Koldo Gojenola, and Ruben Urizar. 2004.
Representation and treatment of multiword expres-
sions in Basque. In Tanaka, Takaaki, Aline Villavi-
cencio, Francis Bond, and Anna Korhonen, editors,
Second ACL Workshop on Multiword Expressions:
Integrating Processing, pages 48–55, Barcelona,
Spain, July. Association for Computational Linguis-
tics.

2005.

Attia, Mohammed A.

Accommodat-
ing multiword expressions in an lfg grammar.
The ParGram Meeting, Japan September 2005,
September. Mohammed A. Attia The Univer-
sity of Manchester School of Informatics mo-
hammed.attia@postgrad.manchester.ac.uk.

Bannard, Colin. 2007. A measure of syntactic ﬂexibil-
ity for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop on
A Broader Perspective on Multiword Expressions,
pages 1–8. Association for Computational Linguis-
tics.

Bar-haim, Roy, Khalil Sima’an, and Yoad Winter.
2008. Part-of-speech tagging of Modern Hebrew
text. Natural Language Engineering, 14(2):223–
251.

Berman, Ruth A. and Dorit Ravid. 1986. Lexicaliza-
tion of noun compounds. Hebrew Linguistics, 24:5–
22. In Hebrew.

Borer, Hagit. 1988. On the morphological parallelism
between compounds and constructs. In Booij, Geert
and Jaap van Marle, editors, Yearbook of Morphol-
ogy 1, pages 45–65. Foris publications, Dordrecht,
Holland.

Borer, Hagit.

1996.

The construct

in review.
In Lecarme, Jacqueline, Jean Lowenstamm, and
Ur Shlonsky, editors, Studies in Afroasiatic Gram-
mar, pages 30–61. Holland Academic Graphics,
The Hague.

Chang, Chih-Chung and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.

Church, Kenneth. W. and Patrick Hanks. 1989. Word
association norms, mutual information and lexicog-
raphy (rev). Computational Linguistics, 19(1):22–
29.

Evert, Stefan and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
pages 188–195, Morristown, NJ, USA. Association
for Computational Linguistics.

Fabri, Ray. 2009. Compounding and adjective-noun
compounds in Maltese.
In Comrie, Bernard, Ray
Fabri, Elizabeth Hume, Manwel Mifsud, Thomas
Stolz, and Martine Vanhove, editors, Introducing
Maltese Linguistics, volume 113 of Studies in Lan-
guage Companion Series. John Benjamins.

Fazly, Afsaneh and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations.
In Proceedings of the 11th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL), pages
337–344.

Glinert, Lewis. 1989. The Grammar of Modern He-

brew. Cambridge University Press, Cambridge.

18

Itai, Alon and Shuly Wintner. 2008. Language re-
sources for Hebrew. Language Resources and Eval-
uation, 42:75–98, March.

and Evaluation (LREC’10), pages 3389–3392. Eu-
ropean Language Resources Association (ELRA),
May.

Tsvetkov, Yulia and Shuly Wintner.

2010b. Ex-
traction of multi-word expressions from small par-
allel corpora.
In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING 2010), August.

Van de Cruys, Tim and Bego˜na Villada Moir´on. 2007.
Semantics-based multiword expression extraction.
In Proceedings of
the Workshop on A Broader
Perspective on Multiword Expressions, pages 25–
32, Prague, Czech Republic, June. Association for
Computational Linguistics.

Villada Moir´on, Bego˜na and J¨org Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word alignment. In Proceedings of the EACL 2006
Workshop on Multi-word-expressions in a multilin-
gual context. Association for Computational Lin-
guistics.

Villavicencio, Aline, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005.
Introduction to the
special issue on multiword expressions: Having a
crack at a hard nut. Computer Speech & Language,
19(4):365–377.

Wintner, Shuly. 2000. Deﬁniteness in the Hebrew

noun phrase. Journal of Linguistics, 36:319–363.

Yona, Shlomo and Shuly Wintner. 2008. A ﬁnite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering, 14(2):173–190, April.

Katz, Graham and Eugenie Giesbrecht. 2006. Au-
tomatic identiﬁcation of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, pages 12–19, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.

Levi, Judith N. 1976. A semantic analysis of Hebrew
compound nominals.
In Cole, Peter, editor, Stud-
ies in Modern Hebrew Syntax and Semantics, num-
ber 32 in North-Holland Linguistic Series, pages 9–
55. North-Holland, Amsterdam.

Manning, Christopher D. and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing. The MIT Press, Cambridge, Mass.

Oﬂazer, Kemal,

¨Ozlem C¸ etino˘glu, and Bilge Say.
2004. Integrating morphology with multi-word ex-
pression processing in Turkish. In Tanaka, Takaaki,
Aline Villavicencio, Francis Bond, and Anna Ko-
rhonen, editors, Second ACL Workshop on Multi-
word Expressions:
Integrating Processing, pages
64–71, Barcelona, Spain, July. Association for
Computational Linguistics.

Ordan, Noam and Shuly Wintner.

2007. Hebrew
WordNet: a test case of aligning lexical databases
across languages. International Journal of Transla-
tion, special issue on Lexical Resources for Machine
Translation, 19(1).

Pecina, Pavel. 2008. A machine learning approach
to multiword expression extraction. In Proceedings
of the LREC Workshop Towards a Shared Task for
Multiword Expressions.

Ramisch, Carlos, Paulo Schreiner, Marco Idiart, and
Alline Villavicencio. 2008. An evaluation of meth-
ods for the extraction of multiword expressions.
In Proceedings of the LREC Workshop Towards a
Shared Task for Multiword Expressions.

Sag,

Ivan, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the Third International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING 2002), pages 1–15, Mexico City,
Mexico.

Tsvetkov, Yulia and Shuly Wintner. 2010a. Automatic
acquisition of parallel corpora from websites with
dynamic content.
In Proceedings of the Seventh
conference on International Language Resources

