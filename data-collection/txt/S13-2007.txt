










































SemEval-2013 Task 5: Evaluating Phrasal Semantics


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 39–47, Atlanta, Georgia, June 14-15, 2013. c©2013 Association for Computational Linguistics

SemEval-2013 Task 5: Evaluating Phrasal Semantics

Ioannis Korkontzelos
National Centre for Text Mining

School of Computer Science
University of Manchester, UK

ioannis.korkontzelos@man.ac.uk

Torsten Zesch
UKP Lab, CompSci Dept.

Technische Universität Darmstadt
Germany

zesch@ukp.informatik.tu-darmstadt.de

Fabio Massimo Zanzotto
Department of Enterprise Engineering

University of Rome “Tor Vergata”
Italy

zanzotto@info.uniroma2.it

Chris Biemann
FG Language Technology, CompSci Dept.

Technische Universität Darmstadt
Germany

biem@cs.tu-darmstadt.de

Abstract

This paper describes the SemEval-2013 Task
5: “Evaluating Phrasal Semantics”. Its first
subtask is about computing the semantic simi-
larity of words and compositional phrases of
minimal length. The second one addresses
deciding the compositionality of phrases in a
given context. The paper discusses the impor-
tance and background of these subtasks and
their structure. In succession, it introduces the
systems that participated and discusses evalu-
ation results.

1 Introduction

Numerous past tasks have focused on leveraging the
meaning of word types or words in context. Exam-
ples of the former are noun categorization and the
TOEFL test, examples of the latter are word sense
disambiguation, metonymy resolution, and lexical
substitution. As these tasks have enjoyed a lot suc-
cess, a natural progression is the pursuit of models
that can perform similar tasks taking into account
multiword expressions and complex compositional
structure. In this paper, we present two subtasks de-
signed to evaluate such phrasal models:

a. Semantic similarity of words and compositional
phrases

b. Evaluating the compositionality of phrases in
context

For example, the first subtask addresses computing
how similar the word “valuation” is to the compo-
sitional sequence “price assessment”, while the sec-
ond subtask addresses deciding whether the phrase
”piece of cake” is used literally or figuratively in the
sentence “Labour was a piece of cake!”.

The aim of these subtasks is two-fold. Firstly,
considering that there is a spread interest lately in
phrasal semantics in its various guises, they provide
an opportunity to draw together approaches to nu-
merous related problems under a common evalua-
tion set. It is intended that after the competition,
the evaluation setting and the datasets will comprise
an on-going benchmark for the evaluation of these
phrasal models.

Secondly, the subtasks attempt to bridge the
gap between established lexical semantics and full-
blown linguistic inference. Thus, we anticipate that
they will stimulate an increased interest around the
general issue of phrasal semantics. We use the no-
tion of phrasal semantics here as opposed to lexi-
cal compounds or compositional semantics. Bridg-
ing the gap between lexical semantics and linguis-
tic inference could provoke novel approaches to cer-
tain established tasks, such as lexical entailment and
paraphrase identification. In addition, it could ul-

39



timately lead to improvements in a wide range of
applications in natural language processing, such
as document retrieval, clustering and classification,
question answering, query expansion, synonym ex-
traction, relation extraction, automatic translation,
or textual advertisement matching in search engines,
all of which depend on phrasal semantics.

The remainder of this paper is structured as fol-
lows: Section 2 presents details about the data
sources and the variety of sources applicable to the
task. Section 3 discusses the first subtask, which
is about semantic similarity of words and compo-
sitional phrases. In subsection 3.1 the subtask is
described in detail together with some information
about its background. Subsection 3.2 discusses the
data creation process and subsection 3.3 discusses
the participating systems and their results. Section 4
introduces the second subtask, which is about eval-
uating the compositionality of phrases in context.
Subsection 4.1 explains the data creation process for
this subtask. In subsection 4.2 the evaluation statis-
tics of participating systems are presented. Section
5 is a discussion about the conclusions of the entire
task. Finally, in section 6 we summarize this presen-
tation and discuss briefly our vision about challenges
in distributional semantics.

2 Data Sources & Methodology

Data instances of both subtasks are drawn from the
large-scale, freely available WaCky corpora (Baroni
et al., 2009). The resource contains corpora in 4 lan-
guages: English, French, German and Italian. The
English corpus, ukWaC, consists of 2 billion words
and was constructed by crawling to the .uk domain
of the web and using medium-frequency words from
the BNC as seeds. The corpus is part-of-speech
(PoS) tagged and lemmatized using the TreeTagger
(Schmid, 1994). The French corpus, frWaC, con-
tains 1.6 billion word corpus and was constructed
by web-crawling the .fr domain and using medium-
frequency words from the Le Monde Diplomatique
corpus and basic French vocabulary lists as seeds.
The corpus was PoS tagged and lemmatized with
the TreeTagger. The French corpus, deWaC, con-
sists of 1.7 billion word corpus and was constructed
by crawling the .de domain and using medium-
frequency words from the SudDeutsche Zeitung cor-

pus and basic German vocabulary lists as seeds. The
corpus was PoS tagged and lemmatized with the
TreeTagger. The Italian corpus, itWaC, is a 2 billion
word corpus constructed from the .it domain of the
web using medium-frequency words from the Re-
pubblica corpus and basic Italian vocabulary lists as
seeds. The corpus was PoS tagged with the Tree-
Tagger, and lemmatized using the Morph-it! lexicon
(Zanchetta and Baroni, 2005). Several versions of
the WaCky corpora, with various extra annotations
or modifications are also available1.

We ensured that data instances occur frequently
enough in the WaCky corpora, so that participat-
ing systems could gather statistics for building dis-
tributional vectors or other uses. As the evalua-
tion data only contains very small annotated sam-
ples from freely available web documents, and the
original source is provided, we could provide them
without violating copyrights.

The size of the WaCky corpora is suitable for
training reliable distributional models. Sentences
are already lemmatized and part-of-speech tagged.
Participating approaches making use of distribu-
tional methods, part-of-speech tags or lemmas, were
strongly encouraged to use these corpora and their
shared preprocessing, to ensure the highest possi-
ble comparability of results. Additionally, this had
the potential to considerably reduce the workload of
participants. For the first subtask, data were pro-
vided in English, German and Italian and for the sec-
ond subtask in English and German.

The range of methods applicable to both subtasks
was deliberately not limited to any specific branch of
methods, such as distributional or vector models of
semantic compositionality. We believe that the sub-
tasks can be tackled from different directions and we
expect a great deal of the scientific benefit to lie in
the comparison of very different approaches, as well
as how these approaches can be combined. An ex-
ception to this rule is the fact that participants in the
first subtask were not allowed to use directly defini-
tions extracted from dictionaries or lexicons. Since
the subtask is considered fundamental and its data
were created from online knowledge resources, sys-
tems using the same tools to address it would be of
limited use. However, participants were allowed to

1WaCky website: wacky.sslmit.unibo.it

40



use other information residing in dictionaries, such
as Wordnet synsets or synset relations.

Participating systems were allowed to attempt one
or both subtasks, in one or all of the languages sup-
ported. However, it was expected that systems per-
forming well at the first basic subtask would pro-
vide a good starting point for dealing with the sec-
ond subtask, which is considered harder. Moreover,
language-independent models were of special inter-
est.

3 Subtask 5a: Semantic Similarity of
Words and Compositional Phrases

The aim of this subtask is to evaluate the compo-
nent of a semantic model that computes the simi-
larity between word sequences of different length.
Participating systems are asked to estimate the se-
mantic similarity of a word and a short sequence of
two words. For example, they should be able to fig-
ure out that contact and close interaction are similar
whereas megalomania and great madness are not.

This subtask addresses a core problem, since sat-
isfactory performance in computing the similarity of
full sentences depends on similarity computations
on shorter sequences.

3.1 Background and Description

This subtask is based on the assumption that we
first need a basic set of functions to compose the
meaning of two words, in order to construct more
complex models that compositionally determine the
meaning of sentences, as a second step. For compo-
sitional distributional semantics, the need for these
basic functions is discussed in Mitchell and Lapata
(2008). Since then, many models have been pro-
posed for addressing the task (Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010; Guevara, 2010),
but still comparative analysis is in general based on
comparing sequences that consist of two words.

As in Zanzotto et al. (2010), this subtask proposes
to compare the similarity of a 2-word sequence and
a single word. This is important as it is the basic
step to analyse models that can compare any word
sequences of different length.

The development and testing set for this subtask
were built based on the idea described in Zanzotto
et al. (2010). Dictionaries were used as sources of

contact/[kon-takt]

1. the act or state of touching;
a touching or meeting, as of
two things or people.

2. close interaction

3. an acquaintance, colleague,
or relative through whom a
person can gain access to
information, favors, influ-
ential people, and the like.

Figure 1: The definition of contact in a sample dictionary

positive training examples. Dictionaries are natural
repositories of equivalences between words under
definition and sequences of words used for defining
them. Figure 1 presents the definition of the word
contact, from which the pair (contact, close interac-
tion) can be extracted. Such equivalences extracted
from dictionaries can be seen as natural and unbi-
ased data instances. This idea opens numerous op-
portunities:

• Since definitions in dictionaries are syntacti-
cally rich, we are able to create examples for
different syntactic relations.

• We have the opportunity to extract positive ex-
amples for languages for which dictionaries
with sufficient entries are available.

Negative examples were generated by matching
words under definition with randomly chosen defin-
ing sequences. In the following subsection, we pro-
vide details about the application of this idea to build
the development and testing set for subtask 5a.

3.2 Data Creation
Data for this subtask were provided in English, Ger-
man and Italian. Pairs of words under definitions and
defining sequences were extracted from the English,
German and Italian part of Wiktionary, respectively.
In particular, for each language, all Wiktionary en-
tries were downloaded and part-of-speech tagged us-
ing the Genia tagger (Tsuruoka et al., 2005). In
succession, definitions that start with noun phrases

41



Language Train set Test set Total

English 5,861 3,907 9,768
German 1,516 1,010 2,526
Italian 1,275 850 2,125

German - no names 1,101 733 1,834

Table 1: Quantitative characteristics of the datasets

were kept, only. For the purpose of extracting word
and sequence pairs for this subtask, we consider as
noun phrases, sequences that consist of adjectives
or noun and end with a noun. In cases where the
extracted noun phrase was longer than two words,
the right-most two sequences were kept, since in
most cases noun phrases are governed by their right-
most component. Subsequently, we discarded in-
stances whose words occur too infrequently in the
WaCky corpora (Baroni et al., 2009) of each lan-
guage. WaCky corpora are available freely and are
large enough for participating systems to extract dis-
tributional statistics. Taking the numbers of ex-
tracted instances into account, we set the frequency
thresholds at 10 occurrences for English and 5 for
German and Italian.

Data instances extracted following this process
were then checked by a computational linguist. Can-
didate pairs in which the definition sequence was not
judged to be a precise and adequate definition of the
word under definition were discarded. These cases
were very limited and mostly account for shortcom-
ings of the very simple pattern used for extraction.
For example, the pair (standard, transmission vehi-
cle) coming from the definition of “standard” as “A
manual transmission vehicle” was discarded. Simi-
larly in German, the pair (Fremde (Eng. stranger),
weibliche Person (Eng. female person)) was dis-
carded. “Fremde”, which is of female grammat-
ical genre, was defined as “weibliche Person, die
man nicht kennt (Eng. female person, one does not
know)”. In Italian, the pair (paese (Eng. land, coun-
try, region), grande estensione (Eng. large tract))
was discarded, since the original definition was
“grande estensione di terreno abitato e generalmente
coltivato (Eng. large tract of land inhabited and cul-
tivated in general)”.

The final data sets were divided into training and

held-out testing sets, according to a 60% and 40%
ratio, respectively. The first three rows of table 1
present the numbers of the train and test sets for the
three languages chosen. It was identified that a fair
percentage of the German instances (approximately
27%) refer to the definitions of first names or family
names. This is probably a flaw of the German part of
Wiktionary. In addition, the pattern used for extrac-
tion happens to apply to the definitions of names.
Name instances were discarded from the German
data set to produce the data set described in the last
row of table 1.

The training set was released approximately 3
months earlier than the test data. Instances in both
set ware annotated as positive or negative. Test set
annotations were not released to the participants, but
were used for evaluation, only.

3.3 Results
Participating systems were evaluated on their ability
to predict correctly whether the components of each
test instance, i.e. word-sequence pair, are semanti-
cally similar or distinct. Participants were allowed
to use or ignore the training data, i.e. the systems
could be supervised or unsupervised. Unsupervised
systems were allowed to use the training data for de-
velopment and parameter tuning. Since this is a core
task, participating systems were not be able to use
dictionaries or other prefabricated lists. Instead, they
were allowed to use distributional similarity models,
selectional preferences, measures of semantic simi-
larity etc.

Participating system responses were scored in
terms of standard information retrieval measures:
accuracy (A), precision (P), recall (R) and F1 score
(Radev et al., 2003). Systems were encouraged to
submit at most 3 solutions for each language, but
submissions for fewer languages were accepted.

Five research teams participated. Ten system runs
were submitted for English, one for German (on data
set: German - no names) and one for Italian. Table 2
illustrates the results of the evaluation process. The
teams of (HsH) (Wartena, 2013), CLaC (Siblini and
Kosseim, 2013), UMCC DLSI-(EPS) (Dávila et al.,
2013), and ITNLP, the Harbin Institute of Technol-
ogy, approached the task in a supervised way, while
MELODI (Van de Cruys et al., 2013) participated
with two unsupervised approaches. Interestingly,

42



Language Rank Participant Id run Id A R P rej. R rej. P F1

1 HsH 1 .803 .752 .837 .854 .775 .792
3 CLaC 3 .794 .707 .856 .881 .750 .774
2 CLaC 2 .794 .695 .867 .893 .745 .771
4 CLaC 1 .788 .638 .910 .937 .721 .750

English 5 MELODI lvw .748 .614 .838 .882 .695 .709
6 UMCC DLSI-(EPS) 1 .724 .613 .787 .834 .683 .689
7 ITNLP 3 .703 .501 .840 .904 .645 .628
8 MELODI dm .689 .481 .825 .898 .634 .608
9 ITNLP 1 .663 .392 .857 .934 .606 .538
10 ITNLP 2 .659 .427 .797 .891 .609 .556

German 1 HsH 1 .825 .765 .870 .885 .790 .814

Italian 1 UMCC DLSI-(EPS) 1 .675 .576 .718 .774 .646 .640

Table 2: Task 5a: Evaluation results. A, P, R, rej. and F1 stand for accuracy, precision, recall, rejection and F1 score,
respectively.

these approaches performed better than some super-
vised ones for this experiment. Below, we sum-
marise the properties of participating systems.

(HsH) (Wartena, 2013) used distributed similarity
and especially random indexing to compute similar-
ities between words and possible definitions, under
the hypothesis that a word and its definition are dis-
tributionally more similar than a word and an arbi-
trary definition. Considering all open-class words,
context vectors over the entire WaCky corpus were
computed for the word under definition, the defining
sequence, its component words separately, the ad-
dition and multiplication of the vectors of the com-
ponent words and a general context vector. Then,
various similarity measures were computed on the
vectors, including an innovative length-normalised
version of Jensen-Shannon divergence. The similar-
ity values are used to train a Support Vector Machine
(SVM) classifier (Cortes and Vapnik, 1995).

The first approach (run 1) of CLaC (Siblini and
Kosseim, 2013) is based on a weighted semantic
network to measure semantic relatedness between
the word and the components of the phrase. A
PART classifier is used to generate a partial decision
trained on the semantic relatedness information of
the labelled training set. The second approach uses
a supervised distributional method based on words
frequently occurring in the Web1TB corpus to cal-
culate relatedness. A JRip classifier is used to gen-

erate rules trained on the semantic relatedness infor-
mation of the training set. This approach was used
in conjunction with the first one as a backup method
(run 2). In addition, features generated by both ap-
proaches were used to train the JRIP classifier col-
lectively (run 3).

The first approach of MELODI (Van de Cruys
et al., 2013), called lvw, uses a dependency-based
vector space model computed over the ukWaC cor-
pus, in combination with Latent Vector Weighting
(Van de Cruys et al., 2011). The system computes
the similarity between the first noun and the head
noun of the second phrase, which was weighted ac-
cording to the semantics of the modifier. The second
approach, called dm, used a dependency-based vec-
tor space model, but, unlike the first approach, disre-
garded the modifier in the defining sequence. Since
both systems are unsupervised, the training data was
used to train a similarity threshold parameter, only.

UMCC DLSI-(EPS) (Dávila et al., 2013) locates
the synsets of words in data instances and computes
the semantic distances between each synset of the
word under definition and each synsets of the defin-
ing sequence words. In succession, a classifier is
trained using features based on distance and Word-
Net relations.

The first attempt of ITNLP (run 1) consisted of an
SVM classifier trained on semantic similarity com-
putations between the word under definition and

43



the defining sequence in each instance. Their sec-
ond attempt also uses an SVM, however trained on
WordNet-based similarities. The third attempt of
ITNLP is a combination of the previous two; it com-
bines their features to train an SVM classifier.

4 Subtask 5b: Semantic Compositionality
in Context

An interesting sub-problem of semantic composi-
tionality is to decide whether a target phrase is used
in its literal or figurative meaning in a given con-
text. For example “big picture” might be used lit-
erally as in Click here for a bigger picture or figura-
tively as in To solve this problem, you have to look at
the bigger picture. Another example is “old school”
which can also be used literally or figuratively: He
will go down in history as one of the old school, a
true gentlemen. vs. During the 1970’s the hall of the
old school was converted into the library.

Being able to detect whether a phrase is used lit-
erally or figuratively is e.g. especially important for
information retrieval, where figuratively used words
should be treated separately to avoid false positives.
For example, the example sentence He will go down
in history as one of the old school, a true gentle-
men. should probably not be retrieved for the query
“school”. Rather, the insights generated from sub-
task 5a could be utilized to retrieve sentences using
a similar phrase such as “gentleman-like behavior”.
The task may also be of interest to the related re-
search fields of metaphor detection and idiom iden-
tification.

There were no restrictions regarding the array of
methods, and the kind of resources that could be
employed for this task. In particular, participants
were allowed to make use of pre-fabricated lists of
phrases annotated with their probability of being
used figuratively from publicly available sources, or
to produce these lists from corpora. Assessing how
well the phrase suits its context might be tackled
using e.g. measures of semantic relatedness as well
as distributional models learned from the underlying
corpus.

Participants of this subtask were provided with
real usage examples of target phrases. For each us-
age example, the task is to make a binary decision
whether the target phrase is used literally or figu-

ratively in this context. Systems were tested in two
different disciplines: a known phrases task where all
target phrases in the test set were contained in the
training, and an unknown phrases setting, where all
target phrases in the test set were unseen.

4.1 Data Creation

The first step in creating the corpus was to compile
a list of phrases that can be used either literally or
metaphorically. Thus, we created an initial list of
several thousand English idioms from Wiktionary by
listing all entries under the category ENGLISH ID-
IOMS using the JWKTL Wiktionary API (Zesch et
al., 2008). We manually filtered the list removing
most idioms that are very unlikely to be ever used
literally (anymore), e.g. to knock on heaven’s door.
For each of the resulting list of phrases, we extracted
usage contexts from the ukWaC corpus (Baroni et
al., 2009). Each usage context contains 5 sentences,
where the sentence with the target phrase appears in
a randomized position. Due to segmentation errors,
some usage contexts actually might contain less than
5 sentences, but we manually filtered all usage con-
texts where the remaining context was insufficient.
This was done in the final cleaning step where we
also manually removed (near) duplicates, obvious
spam, encoding problems etc.

The target phrases in context were annotated for
figurative, literal, both or impossible to tell usage,
using the CrowdFlower2 crowdsourcing annotation
platform. We used about 8% of items as “gold”
items for quality assurance, and had each example
annotated by three crowdworkers. The task was
comparably easy for crowdworkers, who reached
90%-94% pairwise agreement, and 95% success on
the gold items. About 5% of items with low agree-
ment and marked as impossible were removed. Ta-
ble 3 summarizes the quantitative characteristics of
all datasets resulting from this process. We took care
in sampling the data as to keep similar distributions
across the training, development and testing parts.

4.2 Results

Training and development datasets were made avail-
able in advance, test data was provided during the
evaluation period without labels. System perfor-

2www.crowdflower.com

44



Task Dataset # Phrases # Items Items per phrase # Liter. # Figur. # Both

known
train 10 1,424 68–188 702 719 3
dev 10 358 17–47 176 181 1
test 10 594 28–78 294 299 1

unseen
train 31 1,114 4–75 458 653 3
dev 9 342 4–74 141 200 1
test 15 518 8–73 198 319 1

Table 3: Quantitative characteristics of the datasets

Rank System Run Accuracy

1 IIRG 3 .779
2 UNAL 2 .754
3 UNAL 1 .722
5 IIRG 1 .530
4 Baseline MFC - .503
6 IIRG 2 .502

Table 4: Task 5b: Evaluation results for the known
phrases setting

Rank System Run Accuracy

1 UNAL 1 .668
2 UNAL 2 .645
3 Baseline MFC - .616
4 CLaC 1 .550

Table 5: Task 5b: Evaluation results for the unseen
phrases setting

mance was measured in accuracy. Since all partic-
ipants provided classifications for all test items, the
accuracy score is equivalent to precision/recall/F1.
Participants were allowed to enter up to three dif-
ferent runs for evaluation. We also provide baseline
accuracy scores, which are obtained by always as-
signing the most frequent class (figurative).

Table 4 provides the evaluation results for the
known phrases task, while Table 5 ranks participants
for the unseen phrases task. As expected, the un-
seen phrases setting is much harder than the known
phrases setting, as for unseen phrases it is not possi-
ble to learn lexicalised contextual clues. In both set-
tings, the winning entries were able to beat the MFC
baseline. While performance in the known phrases

setting is close to 80% and thus acceptable, the gen-
eral task of recognizing the literal or figurative use of
unseen phrases remains very challenging, with only
a small improvement over the baseline. We refer to
the system descriptions for more details on the tech-
niques used for this subtask: UNAL (Jimenez et al.,
2013), IIRG (Byrne et al., 2013) and CLaC (Siblini
and Kosseim, 2013).

5 Task Conclusions

In this section, we further discuss the findings and
conclusion of the evaluation challenge in the task of
“Phrasal Semantics”.

Looking at the results of both subtasks, one ob-
serves that the maximum performance achieved is
higher for the first than the second subtask. For
this comparison to be fair, trivial baselines should be
taken into account. A system randomly assigning an
output value would be on average 50% correct in the
first subtask, since the numbers of positive and neg-
ative instances in the testing set are equal. Similarly,
a system assigning the most frequent class, i.e. the
figurative use of any phrase, would be 50.3% and
61.6% accurate in the second subtask for seen and
unseen test instances, respectively. It should also be
noted that the testing instances in the first subtask
are unseen in the respective training set. As a result,
in terms of baselines, the second subtask on unseen
data (Table 5) should be considered easier than the
first subtask (Table 2). However, the best perform-
ing systems achieved much higher accuracy in the
first than in the second subtask. This contradiction
confirms our conception that the first subtask is less
complex than the second.

In the first subtask, it is evident that no method
performs much better or much worse than the others.

45



Although the participating systems have employed a
wide variety of approaches and tools, the difference
between the best and worst accuracy achieved is
relatively limited, in particular approximately 14%.
Even more interestingly, unsupervised approaches
performed better than some supervised ones. This
observation suggests that no “golden recipe” has
been identified so far for this task. Thus, probably
different processing tools take advantage of different
sources of information. It is a matter of future re-
search to identify these sources and the correspond-
ing tools, and then develop hybrid methods of im-
proved performance.

In the second subtask, the results of evaluation
on known phrases are much higher than on unseen
phrases. This was expected, as for unseen phrases it
is not possible to learn lexicalised contextual clues.
Thus, the second subtask has succeeded in identify-
ing the complexity threshold up to which the cur-
rent state-of-the-art can address the computational
problem. Further than this threshold, i.e. for unseen
phrases, current systems have not yet succeeded in
addressing it. In conclusion, the difficulty in eval-
uating the compositionality of previously unseen
phrases in context highlights the overall complexity
of the second subtask.

6 Summary and Future Work

In this paper we have presented the 5th task of Se-
mEval 2013, “Evaluating Phrasal Semantics”, which
consists of two subtasks: (1) semantic similarity of
words and compositional phrases, and (2) compo-
sitionality of phrases in context. The former sub-
task, which focussed on the first step of composing
the meaning of phrases of any length, is less com-
plex than the latter subtask, which considers the ef-
fect of context to the semantics of a phrase. The
paper presents details about the background and im-
portance of these subtasks, the data creation process,
the systems that took part in the evaluation and their
results.

In the future, we expect evaluation challenges on
phrasal semantics to progress towards two direc-
tions: (a) the synthesis of semantics of sequences
longer than two words, and (b) aiming to improve
the performance of systems that determine the com-
positionality of previously unseen phrases in con-

text. The evaluation results of the first task sug-
gest that state-of-the-art systems can compose the
semantics of two word sequences with a promising
level of success. However, this task should be seen
as the first step towards composing the semantics
of sentence-long sequences. As far as subtask 5b
is concerned, the accuracy achieved by the partici-
pating systems on unseen testing data was low, only
slightly better than the most frequent class baseline,
which assigns the figurative use to all test phrases.
Thus, the subtask cannot be considered well ad-
dressed by the state-of-the-art and further progress
should be sought.

Acknowledgements

The work relevant to subtask 5a described in this pa-
per is funded by the European Community’s Seventh
Framework Program (FP7/2007-2013) under grant
agreement no. 318736 (OSSMETER).

We would like to thank Tristan Miller for help-
ing with the subtleties of English idiomatic ex-
pressions, and Eugenie Giesbrecht for support
in the organization of subtask 5b. This work
has been supported by the Volkswagen Founda-
tion as part of the Lichtenberg-Professorship Pro-
gram under grant No. I/82806, and by the Hes-
sian research excellence program Landes-Offensive
zur Entwicklung Wissenschaftlich-ökonomischer
Exzellenz (LOEWE) as part of the research center
Digital Humanities.

46



References

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA. Association for Compu-
tational Linguistics.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.

Lorna Byrne, Caroline Fenlon, and John Dunnion. 2013.
IIRG: A naive approach to evaluating phrasal seman-
tics. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), At-
lanta, Georgia, USA.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273–297.

Héctor Dávila, Antonio Fernández Orquı́n, Alexander
Chávez, Yoan Gutiérrez, Armando Collazo, José I.
Abreu, Andrés Montoyo, and Rafael Muñoz. 2013.
UMCC DLSI-(EPS): Paraphrases detection based on
semantic distance. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Atlanta, Georgia, USA.

Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
pages 33–37, Uppsala, Sweden. Association for Com-
putational Linguistics.

Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. UNAL: Discriminating between literal
and figurative phrasal usage using distributional statis-
tics and POS tags. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Atlanta, Georgia, USA.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244, Columbus, Ohio. As-
sociation for Computational Linguistics.

Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.

Dragomir R. Radev, Simone Teufel, Horacio Saggion,
Wai Lam, John Blitzer, Hong Qi, Arda Çelebi, Danyu
Liu, and Elliott Drabek. 2003. Evaluation challenges
in large-scale document summarization. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, ACL ’03, pages

375–382, Morristown, NJ, USA. Association for Com-
putational Linguistics.

Helmut Schmid. 1994. Probabilistic Part-of-Speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.

Reda Siblini and Leila Kosseim. 2013. CLaC: Semantic
relatedness of words and phrases. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012), Atlanta, Georgia, USA.

Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun’ichi Tsujii. 2005. Developing a robust Part-
of-Speech tagger for biomedical text. In Panayiotis
Bozanis and Elias N. Houstis, editors, Advances in In-
formatics, volume 3746, chapter 36, pages 382–392.
Springer Berlin Heidelberg, Berlin, Heidelberg.

Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 1012–1022, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Tim Van de Cruys, Stergos Afantenos, and Philippe
Muller. 2013. MELODI: Semantic similarity of words
and compositional phrases using latent vector weight-
ing. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), At-
lanta, Georgia, USA.

Christian Wartena. 2013. HsH: Estimating semantic sim-
ilarity of words and short phrases with frequency nor-
malized distance measures. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), Atlanta, Georgia, USA.

Eros Zanchetta and Marco Baroni. 2005. Morph-it!: A
free corpus-based morphological resource for the ital-
ian language. Corpus Linguistics 2005, 1(1).

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING).

Torsten Zesch, Christof Müller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
Wikipedia and Wiktionary. Proceedings of the Confer-
ence on Language Resources and Evaluation (LREC),
15:60.

47


