










































Better Evaluation for Grammatical Error Correction


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568–572,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Better Evaluation for Grammatical Error Correction

Daniel Dahlmeier1 and Hwee Tou Ng1,2
1NUS Graduate School for Integrative Sciences and Engineering

2Department of Computer Science, National University of Singapore
{danielhe,nght}@comp.nus.edu.sg

Abstract

We present a novel method for evaluating
grammatical error correction. The core of
our method, which we call MaxMatch (M2),
is an algorithm for efficiently computing the
sequence of phrase-level edits between a
source sentence and a system hypothesis that
achieves the highest overlap with the gold-
standard annotation. This optimal edit se-
quence is subsequently scored using F1 mea-
sure. We test our M2 scorer on the Helping
Our Own (HOO) shared task data and show
that our method results in more accurate eval-
uation for grammatical error correction.

1 Introduction

Progress in natural language processing (NLP) re-
search is driven and measured by automatic eval-
uation methods. Automatic evaluation allows fast
and inexpensive feedback during development, and
objective and reproducible evaluation during testing
time. Grammatical error correction is an important
NLP task with useful applications for second lan-
guage learning. Evaluation for error correction is
typically done by computing F1 measure between
a set of proposed system edits and a set of human-
annotated gold-standard edits (Leacock et al., 2010).

Unfortunately, evaluation is complicated by the
fact that the set of edit operations for a given system
hypothesis is ambiguous. This is due to two reasons.
First, the set of edits that transforms one string into
another is not necessarily unique, even at the token
level. Second, edits can consist of longer phrases
which introduce additional ambiguity. To see how

this can affect evaluation, consider the following
source sentence and system hypothesis from the re-
cent Helping Our Own (HOO) shared task (Dale and
Kilgarriff, 2011) on grammatical error correction:

Source : Our baseline system feeds word
into PB-SMT pipeline.

Hypot. : Our baseline system feeds a word
into PB-SMT pipeline.

The HOO evaluation script extracts the system edit
(� → a), i.e., inserting the article a. Unfortunately,
the gold-standard annotation instead contains the ed-
its (word → {a word, words}). Although the ex-
tracted system edit results in the same corrected sen-
tence as the first gold-standard edit option, the sys-
tem hypothesis was considered to be invalid.

In this work, we propose a method, called Max-
Match (M2), to overcome this problem. The key idea
is that if there are multiple possible ways to arrive
at the same correction, the system should be eval-
uated according to the set of edits that matches the
gold-standard as often as possible. To this end, we
propose an algorithm for efficiently computing the
set of phrase-level edits with the maximum overlap
with the gold standard. The edits are subsequently
scored using F1 measure. We test our method in the
context of the HOO shared task and show that our
method results in a more accurate evaluation for er-
ror correction.

The remainder of this paper is organized as fol-
lows: Section 2 describes the proposed method; Sec-
tion 3 presents experimental results; Section 4 dis-
cusses some details of grammar correction evalua-
tion; and Section 5 concludes the paper.

568



2 Method

We begin by establishing some notation. We con-
sider a set of source sentences S = {s1, . . . , sn} to-
gether with a set of hypotheses H = {h1, . . . ,hn}
generated by an error correction system. Let G =
{g1, . . . ,gn} be the set of gold standard annota-
tions for the same sentences. Each annotation gi =
{g1i , . . . , gri } is a set of edits. An edit is a triple
(a, b, C), consisting of:
• start and end (token-) offsets a and b with re-

spect to a source sentence,

• a correction C. For gold-standard edits, C is a
set containing one or more possible corrections.
For system edits, C is a single correction.

Evaluation of the system output involves the follow-
ing two steps:

1. Extracting a set of system edits ei for each
source-hypothesis pair (si,hi).

2. Evaluating the system edits for the complete
test set with respect to the gold standard G.

The remainder of this section describes a method for
solving these two steps. We start by describing how
to construct an edit lattice from a source-hypothesis
pair. Then, we show that finding the optimal se-
quence of edits is equivalent to solving a shortest
path search through the lattice. Finally, we describe
how to evaluate the edits using F1 measure.

2.1 Edit lattice
We start from the well-established Levenshtein dis-
tance (Levenshtein, 1966), which is defined as the
minimum number of insertions, deletions, and sub-
stitutions needed to transform one string into an-
other. The Levenshtein distance between a source
sentence si = s1i , . . . , s

k
i and a hypothesis hi =

h1i , . . . , h
l
i can be efficiently computed using a two

dimensional matrix that is filled using a classic dy-
namic programming algorithm. We assume that
both si and hi have been tokenized. The matrix for
the example from Section 1 is shown in Figure 1. By
performing a simple breadth-first search, similar to
the Viterbi algorithm, we can extract the lattice of
all shortest paths that lead from the top-left corner
to the bottom-right corner of the Levenshtein ma-
trix. Each vertex in the lattice corresponds to a cell

Our baseline system feeds a word into PB-SMT pipeline .

0 1 2 3 4 5 6 7 8 9 10

Our 1 0 1 2 3 4 5 6 7 8 9

baseline 2 1 0 1 2 3 4 5 6 7 8

system 3 2 1 0 1 2 3 4 5 6 7

feeds 4 3 2 1 0 1 2 3 4 5 6

word 5 4 3 2 1 1 1 2 3 4 5

into 6 5 4 3 2 2 2 1 2 3 4

PB-SMT 7 6 5 4 3 3 3 2 1 2 3

pipeline 8 7 6 5 4 4 4 3 2 1 2

. 9 8 7 6 5 5 5 4 3 2 1

Figure 1: The Levenshtein matrix and the shortest path
for a source sentence “Our baseline system feeds word
into PB-SMT pipeline .” and a hypothesis “Our baseline
system feeds a word into PB-SMT pipeline .”

in the Levenshtein matrix, and each edge in the lat-
tice corresponds to an atomic edit operation: insert-
ing a token, deleting a token, substituting a token,
or leaving a token unchanged. Each path through
the lattice corresponds to a shortest sequence of ed-
its that transform si into hi. We assign a unit cost to
each edge in the lattice.

We have seen that annotators can use longer
phrases and that phrases can include un-
changed words from the context, e.g., the
gold edit from the example in Section 1 is
(4, 5,word, {a word, words}). However, it seems
unrealistic to allow an arbitrary number of un-
changed words in an edit. In particular, we want to
avoid very large edits that cover complete sentences.
Therefore, we limit the number of unchanged words
by a parameter u. To allow for phrase-level edits,
we add transitive edges to the lattice as long as the
number of unchanged words in the newly added edit
is not greater than u and the edit changes at least one
word. Let e1 = (a1, b1, C1) and e2 = (a2, b2, C2)
be two edits corresponding to adjacent edges in the
lattice, with the first end offset b1 being equal to the
second start offset a2. We can combine them into a
new edit e3 = (a1, b2, C1 + C2), where C1 + C2 is
the concatenation of strings C1 and C2. The cost of
a transitive edge is the sum of the costs of its parts.
The lattice extracted from the example sentence is
shown in Figure 2.

2.2 Finding maximally matching edit sequence

Our goal is to find the sequence of edits ei with
the maximum overlap with the gold standard. Let
L = (V,E) be the edit lattice graph from the last
section. We change the cost of each edge whose cor-

569



0,0 1,1Our (1) 2,2baseline (1)

3,3
system (1)

4,5feeds/feeds a (2)

4,4

feeds (1) 5,6
word (1)ε/a (1)

word/a word (-45)

6,7

into (1)

7,8PB-SMT (1) 8,9pipeline (1) 9,10. (1)

system feeds/system feeds a (3)
feeds word/feeds a word (3)

word into/a word into (3)

Figure 2: The edit lattice for “Our baseline system feeds (�→ a) word into PB-SMT pipeline .” Edge costs are shown
in parentheses. The edge from (4,4) to (5,6) matches the gold annotation and carries a negative cost.

responding edit has a match in the gold standard to
−(u + 1) × |E|. An edit e matches a gold edit g
iff they have the same offsets and e’s correction is
included in g:

match(e, g)⇔ e.a = g.a ∧ e.b = g.b ∧ e.C ∈ g.C
(1)

Then, we perform a single-source shortest path
search with negative edge weights from the start to
the end vertex1. This can be done efficiently, for ex-
ample with the Bellman-Ford algorithm (Cormen et
al., 2001). As the lattice is acyclic, the algorithm is
guaranteed to terminate and return a shortest path.
Theorem 1. The set of edits corresponding to the
shortest path has the maximum overlap with the gold
standard annotation.

Proof. Let e = e1, . . . , ek be the edit sequence cor-
responding to the shortest path and let p be the num-
ber of matched edits. Assume that there exists an-
other edit sequence e′ with higher total edge weights
but p′ > p matching edits. Then we have

p(−(u+ 1)|E|) + q ≤ p′(−(u+ 1)|E|) + q′(2)
⇔ (q − q′) ≤ (p′ − p)(−(u+ 1)|E|),

where q and q′ denote the combined cost of all non-
matching edits in the two paths, respectively. Be-
cause p′ − p ≥ 1, the right hand side is at most
−(u + 1)|E|. Because q and q′ are positive and
bounded by (u+ 1)|E|, the left hand side cannot be
smaller than or equal to −(u+ 1)|E|. This is a con-
tradiction. Therefore there cannot exist such an edit
sequence e′, and e is the sequence with the maxi-
mum overlap with the gold-standard annotation.

1To break ties between non-matching edges, we add a small
cost ζ � 1 to all non-matching edges, thus favoring paths that
use fewer edges, everything else being equal.

2.3 Evaluating edits
What is left to do is to evaluate the set of edits
with respect to the gold standard. This is done by
computing precision, recall, and F1 measure (van
Rijsbergen, 1979) between the set of system edits
{e1, . . . , en} and the set of gold edits {g1, . . . ,gn}
for all sentences

P =

∑n
i=1 |ei ∩ gi|∑n

i=1 |ei|
(3)

R =

∑n
i=1 |ei ∩ gi|∑n

i=1 |gi|
(4)

F1 = 2×
P ×R
P +R

, (5)

where we define the intersection between ei and gi
as

ei ∩ gi = {e ∈ ei | ∃ g ∈ gi(match(e, g))}. (6)

3 Experiments and Results

We experimentally test our M2 method in the con-
text of the HOO shared task. The HOO test data2

consists of text fragments from NLP papers to-
gether with manually-created gold-standard correc-
tions (see (Dale and Kilgarriff, 2011) for details).
We test our method by re-scoring the best runs of
the participating teams3 in the HOO shared task with
our M2 scorer and comparing the scores with the of-
ficial HOO scorer, which simply uses GNU wdiff4

to extract system edits. We obtain each system’s
output and segment it at the sentence level accord-
ing to the gold standard sentence segmentation. The

2Available at http://groups.google.com/group/hoo-nlp/ after
registration.

3Except one team that did not submit any plain text output.
4http://www.gnu.org/s/wdiff/

570



M2 scorer . . . should basic translational unit be (word→ a word) . . .
HOO scorer . . . should basic translational unit be *(�→ a) word . . .

M2 scorer . . . development set similar (with→ to) (�→ the) test set . . .
HOO scorer . . . development set similar *(with→ to the) test set . . .

M2 scorer (�→ The) *(Xinhua portion of→ xinhua portion of) the English Gigaword3 . . .
HOO scorer *(Xinhua→ The xinhua) portion of the English Gigaword3 . . .

Table 2: Examples of different edits extracted by the M2 scorer and the official HOO scorer. Edits that do not match
the gold-standard annotation are marked with an asterisk (*).

Team HOO scorer M2 scorer
P R F1 P R F1

JU (0) 10.39 3.78 5.54 12.30 4.45 6.53
LI (8) 20.86 3.22 5.57 21.12 3.22 5.58
NU (0) 29.10 7.38 11.77 31.09 7.85 12.54
UI (1) 50.72 13.34 21.12 54.61 14.57 23.00
UT (1) 5.01 4.07 4.49 5.72 4.45 5.01

Table 1: Results for participants in the HOO shared task.
The run of the system is shown in parentheses.

source sentences, system hypotheses, and correc-
tions are tokenized using the Penn Treebank stan-
dard (Marcus et al., 1993). The character edit offsets
are automatically converted to token offsets. We set
the parameter u to 2, allowing up to two unchanged
words per edit. The results are shown in Table 1.
Note that the M2 scorer and the HOO scorer adhere
to the same score definition and only differ in the
way the system edits are computed. We can see that
the M2 scorer results in higher scores than the offi-
cial scorer for all systems, showing that the official
scorer missed some valid edits. For example, the
M2 scorer finds 155 valid edits for the UI system
compared to 141 found by the official scorer, and 83
valid edits for the NU system, compared to 78 by
the official scorer. We manually inspect the output
of the scorers and find that the M2 scorer indeed ex-
tracts the correct edits matching the gold standard
where possible. Examples are shown in Table 2.

4 Discussion

The evaluation framework proposed in this work dif-
fers slightly from the one in the HOO shared task.

Sentence-by-sentence. We compute the edits
between source-hypothesis sentence pairs, while
the HOO scorer computes edits at the document
level. As the HOO data comes in a sentence-
segmented format, both approaches are equivalent,
while sentence-by-sentence is easier to work with.

Token-level offsets. In our work, the start and
end of an edit are given as token offsets, while the
HOO data uses character offsets. Character offsets
make the evaluation procedure very brittle as a small
change, e.g., an additional whitespace character, will
affect all subsequent edits. Character offsets also in-
troduce ambiguities in the annotation, e.g., whether
a comma is part of the preceding token.

Alternative scoring. The HOO shared task de-
fines three different scores: detection, recognition,
and correction. Effectively, all three scores are F1
measures and only differ in the conditions on when
an edit is counted as valid. Additionally, each score
is reported under a “with bonus” alternative, where
a system receives rewards for missed optional ed-
its. The F1 measure defined in Section 2.3 is equiv-
alent to correction without bonus. Our method can
be used to compute detection and recognition scores
and scores with bonus as well.

5 Conclusion

We have presented a novel method, called Max-
Match (M2), for evaluating grammatical error cor-
rection. Our method computes the sequence of
phrase-level edits that achieves the highest over-
lap with the gold-standard annotation. Experi-
ments on the HOO data show that our method
overcomes deficiencies in the current evaluation
method. The M2 scorer is available for download
at http://nlp.comp.nus.edu.sg/software/.

Acknowledgments

We thank Chang Liu for comments on an earlier
draft. This research is supported by the Singa-
pore National Research Foundation under its Inter-
national Research Centre @ Singapore Funding Ini-
tiative and administered by the IDM Programme Of-
fice.

571



References
T. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein.

2001. Introduction to Algorithms. MIT Press, Cam-
bridge, MA.

R. Dale and A. Kilgarriff. 2011. Helping Our Own: The
HOO 2011 pilot shared task. In Proceedings of the
2011 European Workshop on Natural Language Gen-
eration.

C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault,
2010. Automated Grammatical Error Detection for
Language Learners, chapter 5. Morgan and Claypool
Publishers.

V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707–710.

M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.

C. J. van Rijsbergen. 1979. Information Retrieval. But-
terworth, 2nd edition.

572


