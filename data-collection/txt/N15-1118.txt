



















































Injecting Logical Background Knowledge into Embeddings for Relation Extraction


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1119–1129,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Injecting Logical Background Knowledge into
Embeddings for Relation Extraction

Tim Rocktäschel
University College London

London, UK

Sameer Singh
University of Washington

Seattle, WA

Sebastian Riedel
University College London

London, UK

Abstract

Matrix factorization approaches to relation
extraction provide several attractive features:
they support distant supervision, handle open
schemas, and leverage unlabeled data. Unfortu-
nately, these methods share a shortcoming with
all other distantly supervised approaches: they
cannot learn to extract target relations with-
out existing data in the knowledge base, and
likewise, these models are inaccurate for rela-
tions with sparse data. Rule-based extractors,
on the other hand, can be easily extended to
novel relations and improved for existing but
inaccurate relations, through first-order formu-
lae that capture auxiliary domain knowledge.
However, usually a large set of such formulae
is necessary to achieve generalization.

In this paper, we introduce a paradigm
for learning low-dimensional embeddings of
entity-pairs and relations that combine the ad-
vantages of matrix factorization with first-order
logic domain knowledge. We introduce simple
approaches for estimating such embeddings,
as well as a novel training algorithm to jointly
optimize over factual and first-order logic in-
formation. Our results show that this method
is able to learn accurate extractors with little or
no distant supervision alignments, while at the
same time generalizing to textual patterns that
do not appear in the formulae.

1 Introduction

Relation extraction, the task of identifying rela-
tions between named entities, is a crucial compo-
nent for information extraction. A recent successful
approach (Riedel et al., 2013) relies on two ideas:

(a) unifying traditional canonical relations, such as
those of the Freebase schema, with OpenIE surface
form patterns in a universal schema, and (b) complet-
ing a knowledge base of such a schema using matrix
factorization. This approach has several attractive
properties. First, for canonical relations it effectively
performs distant supervision (Bunescu and Mooney,
2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann
et al., 2011; Surdeanu et al., 2012) and hence re-
quires no textual annotations. Second, in the spirit of
OpenIE, a universal schema can use textual patterns
as novel relations and thus increases the coverage
of traditional schemas (Riedel et al., 2013; Fan et
al., 2014). Third, matrix factorization learns better
embeddings for entity-pairs for which only surface
form patterns are observed, and these can also lead
to better extractions of canonical relations.

Unfortunately, populating a universal schema
knowledge base using matrix factorization suffers
from a problem all distantly-supervised techniques
share: you can only reliably learn relations that ap-
pear frequently enough in the knowledge base. In par-
ticular, for relations that do not appear in the knowl-
edge base or for which no facts are known we cannot
learn a predictor at all. One way to overcome this
problem is to incorporate additional domain knowl-
edge, either specified manually or bootstrapped from
auxiliary sources. In fact, domain knowledge en-
coded as simple logic formulae over patterns and
relations has been used in practice to directly specify
relation extractors (Reiss et al., 2008; Chiticariu et al.,
2013; Akbik et al., 2014). However, these extractors
can be brittle and obtain poor recall, since they are
unable to generalize to textual patterns that are not

1119



found in given formulae. Hence, there is a need for
learning extractors that are able to combine logical
knowledge with benefits of factorization techniques
to facilitate precise extractions and generalization to
novel relations.

In this paper, we propose a paradigm for learning
universal schema extractors by combining matrix fac-
torization based relation extraction with additional in-
formation in the form of first-order logic knowledge.
Our contributions are threefold: (i) We introduce sim-
ple baselines that enforce logic constraints through
deterministic inference before and after matrix factor-
ization (§3.1). (ii) We propose a novel joint training
algorithm that learns vector embeddings of relations
and entity-pairs using both distant supervision and
first-order logic formulae such that the factorization
captures these formulae (§3.2). (iii) We present an
empirical evaluation using automatically mined rules
that demonstrates the benefits of incorporating logi-
cal knowledge in relation extraction, in particular that
joint factorization of distant and logic supervision is
efficient, accurate, and robust to noise (§5).

2 Matrix Factorization and Logic

In this section we provide background on matrix fac-
torization for universal schema relation extraction,
and describe its connections to first-order logic.

2.1 Notation

In order to later unify observed facts and logical back-
ground knowledge, we first represent given factual
data in terms of first-order logic. We have a set E
of constants that refer to entities, and a set of predi-
catesR that refer to relations between these entities.
In the following we will focus on binary relations
in a universal schema that contains both structured
relations from one (or more) knowledge bases, and
surface-form relations. Further, with P ⊆ E × E we
denote the domain over entity-pairs of interest.

In function-free first-order logic a term is de-
fined as a constant or a variable, and the most
basic form of a formula is an atom such as
professorAt(x, y) that applies a predicate to a
pair of terms. More complex formulae such as ∀x, y :
professorAt(x, y)⇒ employeeAt(x, y) can
be constructed by combining atoms with logical con-
nectives (such as ¬ and ∧) and quantifiers (∃x, ∀x).

The simplest form of first-order formulae are
ground atoms: predicates applied to constants, such
as directorOf(NOLAN,INTERSTELLAR). A pos-
sible world is a set of ground atoms. Ground lit-
erals are either ground atoms or negated ground
atoms such as ¬bornIn(NOLAN,BERLIN), and
correspond to positive or negative facts. Training
data for distant supervision can now be viewed as
a knowledge base of such ground literals. Our goal
is to extend the class of formulae from such facts to
rules such as the first-order formula above.

2.2 Matrix Factorization with Ground Atoms
Given the notation presented above, matrix factor-
ization can now be seen as a learning task in which
low-dimensional embeddings are estimated for all
constant pairs in P and predicates (relations) in R,
given a collection of ground atoms (facts) as supervi-
sion. We represent constant-pairs as rows and predi-
cates as columns of a |P| × |R| binary matrix, and
each atom in the training data represents an observed
cell in this matrix. As introduced in Riedel et al.
(2013), we seek to find a low-rank factorization into
a |P|×k matrix of embeddings of constant-pairs and
a k × |R| matrix of predicate embeddings such that
they approximate the observed matrix.

More precisely, let v(·) denote the mapping from
constant-pairs and predicates to their corresponding
embedding. That is, vrm is the embedding for predi-
cate rm, and v(ei,ej) is the embedding for the pair
of constants (ei, ej). Let w be a possible world
(i.e. a set of ground atoms), and V be the set of
all entity-pair and relation embeddings. Further, let
π

ei,ej
m = σ(vrm · vei,ej ) where σ is the sigmoid func-

tion and vrm · v(ei,ej) denotes the vector dot-product
between the embeddings of relation rm and entity-
pair (ei, ej). We define the conditional probability of
a possible world w given embeddings V as

p(w|V) =
∏

rm(ei,ej)∈w
π

ei,ej
m

∏
rm(ei,ej)/∈w

(
1− πei,ejm

)
.

The embeddings can be estimated by maximizing the
likelihood of a set of observed ground atoms with `2
regularization (Collins et al., 2001), optimized using
stochastic gradient descent. In summary, with atomic
formulae (i.e. factual knowledge) we learn entity-pair
and relation embeddings that reconstruct known facts
and are able to generalize to unknown facts.

1120



KB
∀x, y : co-founder-of(x, y)⇒ company/founders(y, x)
∀x, y : review-by(x, y)⇒ author/works written(y, x)
∀x, y : daughter-of(x, y)⇒ person/parents(x, y)

|R|

|P|

k

|P| |R|
k

|R|

|P|

Evidence Sparse Training Matrix Low-rank Logic Embeddings Completed Matrix

Facts

First-order
Formulae

Figure 1: Injecting Logic into Matrix Factorization: Given a sparse binary matrix consisting of observed facts over
entity-pairs P and predicates/relationsR, matrix factorization is used to learn k-dimensional relation and entity-pair
embeddings that approximate the observed matrix. In this paper we use additional first-order logic formulae over
entities and relations to learn the embeddings such that the predictions (completed matrix) also satisfy these formulae.

3 Injecting Logic Into Factorization

Matrix factorization is capable of learning complex
dependencies between relations, but requires ob-
served facts as training signal. However, often we
either do not have this signal because the relations of
interest do not have pre-existing facts, or this signal
is noisy due to alignment errors or mismatches when
linking knowledge base entities to mentions in text.

To overcome this problem we investigate the use
of first-order logic background knowledge (e.g. im-
plications) to aid relation extraction. One option is
to rely on a fully symbolic approach that exclusively
uses first-order logic (Bos and Markert, 2005; Baader
et al., 2007; Bos, 2008). In this case incorporating
additional background knowledge is trivial. However,
it is difficult to generalize and deal with noise and
uncertainty in language when relying only on manual
rules. In contrast, matrix factorization methods can
overcome these shortcomings, but it is not clear how
they can be combined with logic formulae.

In this section, we propose to inject formulae into
the embeddings of relations and entity-pairs, i.e., esti-
mate the embeddings such that predictions based on
them conform to given logic formulae (see Figure 1
for an overview). We refer to such embeddings as
low-rank logic embeddings. Akin to matrix factoriza-
tion, inference of a fact at test time still amounts to
an efficient dot product of the corresponding relation
and entity-pair embeddings, and logical inference is
not needed. We present two techniques for inject-
ing logical background knowledge, pre-factorization

inference (§3.1) and joint optimization (§3.2), and
demonstrate in subsequent sections that they gen-
eralize better than direct logical inference, even if
such inference is performed on the predictions of the
matrix factorization model.

3.1 Pre-Factorization Inference
Background knowledge in form of first-order formu-
lae can be seen as hints that can be used to generate
additional training data (Abu-Mostafa, 1990). For
pre-factorization inference we first perform logical
inference on the training data and add inferred facts
as additional training data. For example, for a for-
mula F = ∀x, y : rs(x, y)⇒ rt(x, y), we add an
additional observed cell rt(x, y) for any (x, y) for
which rs(x, y) is observed in the distant supervision
training data. This is repeated until no further facts
can be inferred. Subsequently, we run matrix factor-
ization on the extended set of observed cells.

The intuition is that the additional training data
generated by the formulae provide evidence of the
logical dependencies between relations to the matrix
factorization model, while at the same time allowing
the factorization to generalize to unobserved facts
and to deal with ambiguity and noise in the data. No
further logical inference is performed during or after
training of the factorization model as we expect that
the learned embeddings encode the given formulae.

3.2 Joint Optimization
One drawback of pre-factorization inference is that
the formulae are enforced only on observed atoms,

1121



i.e., first-order dependencies on predicted facts are
ignored. Instead we would like to include a loss term
for the logical formulae directly in the matrix factor-
ization objective, thus jointly optimizing embeddings
to reconstruct factual training data as well as obeying
to first-order logical background knowledge.

3.2.1 Training Objective

Here we first present a learning objective that uni-
fies ground atoms (facts) and logical background
knowledge by treating both as logic formulae (atomic
or complex), and define a loss function over this gen-
eral representation. We then define the loss function
for ground atoms and simple implications, along with
a brief sketch of how the loss can be defined for arbi-
trarily complex logic formulae.

As introduced in §2.1, let R be the set of all
relations/predicates and P be the set of all entity-
pairs/constants. Furthermore, let F be a training set
of logic formulae F , and L a loss function. The train-
ing objective (omitting `2 regularization on v(·) for
simplicity) is

min
V

∑
F∈F
L([F ]) (1)

where V is the set of all relation and entity-pair em-
beddings, and [F ] is the marginal probability p(w|V)
that the formula F is true under the model. In this pa-
per we use the logistic loss: L([F ]) := − log([F ]).
The objective thus prefers embeddings that assign
formulae a high marginal probability.

To optimize this function we need the marginal
probabilities [F ], and the gradients of the losses
L([F ]) for every F ∈ F with respect to entity-
pair and relation embeddings, i.e., ∂L([F ])/∂vrm
and ∂L([F ])/∂v(ei,ej). Below we discuss how these
quantities can be computed or approximated for arbi-
trary first-order logic formulae, with details provided
for ground atoms and implications.

Ground Atoms Due to the conditional indepen-
dence of ground atoms in the distribution p(w|V),
the marginal probability of a ground atom F =
rm(ei, ej) is [F ] = πei,ejm = σ(vrm · vei,ej ). Hence
when only ground atoms (or literals) are used, objec-
tive (1) reduces to the standard log-likelihood loss.
The gradients of the loss for the entity-pair embed-

ding v(ei,ej) and relation embedding vrm are

∂[F ]/∂v(ei,ej) = [F ](1− [F ])vrm (2)
∂[F ]/∂vrm = [F ](1− [F ])v(ei,ej) (3)

∂L([F ])/∂v(ei,ej) = −[F ]−1∂[F ]/∂v(ei,ej) (4)
∂L([F ])/∂vrm = −[F ]−1∂[F ]/∂vrm . (5)

First-order Logic Crucially, and in contrast to the
log-likelihood loss for matrix factorization, we can in-
ject more expressive logic formulae than just ground
atoms. We briefly outline how to recursively compute
the probability of the formula [F ] and the gradients of
the loss L([F ]) for any first-order formula F . Again,
note that the probabilities of ground atoms in our
model are independent conditioned on embeddings.
This means that for any two formulae A and B, the
marginal probability of [A ∧ B] can be computed as
[A][B] (known as product t-norm), provided both for-
mula concern non-overlapping sets of ground atoms.
In combination with [¬A] := 1 − [A] and the [ ]
operator as defined for ground atoms earlier, we can
compute the probability of any propositional formula
recursively, e.g.,

[A ∨ B] = [A] + [B]− [A][B]
[A ⇒ B] = [A]([B]− 1) + 1

[A ∧ ¬B ⇒ C] = ([A](1− [B]))([C]− 1) + 1.
Note that for statements [F ] ∈ {0, 1}, we directly
recover logical semantics. First-order formulae in
finite domains can be embedded through explicit
grounding. For universal quantification we can get
[∀x, y : F(x, y)] = [∧x,y F(x, y)]. If we again as-
sume non-overlapping ground atoms in each of the
arguments of the conjunction, we can simplify this to∏

x,y[F(x, y)]. When arguments do overlap we can
think of this simplification as an approximation.

Since [F(x, y)] is defined recursively, we can
back-propagate the training signal through the
structure of [F ] to compute ∂[F(x, y)]/∂vrm and
∂[F(x, y)]/∂vei,ej for any nested formula.
Implications A particularly useful family of for-
mulae for relation extraction are universally quanti-
fied first-order formula over a knowledge base such
as F = ∀x, y : rs(x, y) ⇒ rt(x, y). Assuming
a finite domain, such a formula can be unrolled
into a conjunction of propositional statements of
the form Fij = rs(ei, ej) ⇒ rt(ei, ej), one for

1122



each entity-pair (ei, ej) in the domain. Specifically,
[F ] = ∏(ei,ej)∈P [Fij ], and therefore L([F ]) =∑

(ei,ej)∈P L([Fij ]). The gradients are derived as:

[Fij ] = [rs(ei, ej)] ([rt(ei, ej)]− 1) + 1 (6)
∂L([Fij ])
∂vrs

= −[Fij ]−1 ([rt(ei, ej)]− 1) ∂[rs(ei, ej)]
∂vrs

∂L([Fij ])
∂vrt

= −[Fij ]−1[rs(ei, ej)]∂[rt(ei, ej)]
∂vrt

(7)

∂L([Fij ])
∂vei,ej

= −[Fij ]−1 ([rt(ei, ej)]− 1) ∂[rs(ei, ej)]
∂vei,ej

− [Fij ]−1[rs(ei, ej)]∂[rt(ei, ej)]
∂vei,ej

. (8)

Following such a derivation, one can obtain gradients
for other first-order logic formulae as well.

3.2.2 Learning
We learn the embeddings by minimizing Eq. 1

with `2-regularization using AdaGrad (Duchi et al.,
2011). Since we have no negative training facts, we
follow Riedel et al. (2013) by sampling unobserved
facts that we assume to be false. Specifically, in ev-
ery epoch and for every true training fact rm(ei, ej)
we sample an (ep, eq) such that rm(ep, eq) is unob-
served. Subsequently, we perform two kinds of up-
dates: F = rm(ei, ej) and F = ¬ rm(ep, eq). For
every non-atomic first-order formula in F we iterate
over all entity-pairs for which at least one atom in the
formula is observed (in addition to as many sampled
entity-pairs for which none of the atoms have been
observed) and add corresponding grounded propo-
sitional formulae to the training objective. At test
time, predicting a score for any unobserved state-
ment rm(ei, ej) is done efficiently by calculating
[rm(ei, ej)]. Note that this does not involve any ex-
plicit logical inference, instead we expect that the
predictions from the learned embeddings already re-
spect the provided formulae.

4 Experimental Setup

There are two orthogonal question when evaluat-
ing the effectiveness of low-rank logic embeddings:
a) does injection of logic formulae into the embed-
dings of entity-pairs and relations provide any bene-
fits, and b) where do the background formulae come
from? The latter is a well-studied problem (Hipp et
al., 2000; Schoenmackers et al., 2010; Völker and

Niepert, 2011). In this paper we focus the evaluation
on the ability of various approaches to benefit from
formulae that we directly extract from the training
data using a simple method.

Distant Supervision Evaluation We follow the
procedure as used in Riedel et al. (2013) for eval-
uating knowledge base completion of Freebase (Bol-
lacker et al., 2008) with textual data from the NY-
Times corpus (Sandhaus, 2008). The training matrix
consists of 4 111 columns, representing 151 Free-
base relations and 3 960 textual patterns, 41 913 rows
(entity-pairs) and 118 781 training facts of which
7 293 belong to Freebase relations. The entity-pairs
are divided into train and test, and we hide all Free-
base relations for the test pairs from training. Our pri-
mary evaluation measure is average and (weighted)
mean average precision, MAP and wMAP respec-
tively (see Riedel et al. (2013) for details).

Formulae Extraction and Annotation We use a
simple technique for extracting formulae from the
matrix factorization model. We first run matrix fac-
torization over the complete training data to learn
accurate relation and entity-pair embeddings. After
training, we iterate over all pairs of relations (rs, rt)
where rt is a Freebase relation. For every relation-
pair we iterate over all training atoms rs(ei, ej), eval-
uate the score [rs(ei, ej) ⇒ rt(ei, ej)] as described
in §3.2.1, and calculate the average to arrive at a
score for the formula. Finally, we rank all formulae
by their score and manually filter the top 100 for-
mulae, which resulted in 36 annotated high-quality
formulae (see Table 1 for examples). Note that our
formula extraction approach does not observe the re-
lations for test entity-pairs. All models used in our
experiments have access to these formulae, except
for the matrix factorization baseline.

Methods Our proposed methods for injecting logic
into relation embeddings are pre-factorization infer-
ence (Pre; §3.1) which performs regular matrix fac-
torization after propagating the logic formulae in a
deterministic manner, and joint optimization (Joint;
§3.2) which maximizes an objective that combines
terms from factual and first-order logic knowledge.
Additionally, we use the following three baselines.
The matrix factorization (MF; §2.2) model uses only
ground atoms to learn relation and entity-pair embed-

1123



Formula Score

∀x, y : #2-unit-of-#1(x, y)⇒ org/parent/child(x, y) 0.97
∀x, y : #2-city-of-#1(x, y)⇒ location/containedby(x, y) 0.97
∀x, y : #2-minister-#1(x, y)⇒ person/nationality(x, y) 0.97
∀x, y : #2-executive-#1(x, y)⇒ person/company(x, y) 0.96
∀x, y : #2-co-founder-of-#1(x, y)⇒ company/founders(y, x) 0.96

Table 1: Sample Extracted Formulae: Top implica-
tions of textual patterns to five different Freebase relations.
These implications were extracted from the matrix factor-
ization model and manually annotated. The premises of
these implications are dependency paths, but we present a
simplified version to make them more readable.

dings (i.e. it has no access to any formulae). Further-
more, we consider pure logical inference (Inf). Our
final approach, post-factorization inference (Post),
first runs matrix factorization and then performs logi-
cal inference on the known and predicted facts. Post-
inference is computationally expensive, since for all
premises of formulae we have to iterate over all
rows (entity-pairs) in the matrix to assess whether
the premise is true or not.

Parameters For every matrix factorization based
method we use k = 100 as the dimension for the em-
beddings, λ = 0.01 as parameter of `2-regularization
and α = 0.1 as initial learning rate for AdaGrad,
which we run for 200 epochs.

Complexity Each AdaGrad update is defined over
a single cell of the matrix, and thus training data can
be streamed one ground atom at a time. For matrix
factorization, each AdaGrad epoch touches all the
observed atoms once, and as many sampled negative
atoms. With given formulae, it additionally revisits
all the observed atoms that appear as an atom in the
formula (and as many sampled negative atoms), and
thus more general formulae will be more expensive.
However the updates over atoms are performed inde-
pendently and thus not all the data needs to be stored
in memory. All presented models take less than 15
minutes to train on a 2.8 GHz Intel Core i7 machine.

5 Results and Discussion

To evaluate the utility of injecting logic formulae
into embeddings, we present a comparison on a va-
riety of benchmarks. First, in §5.1 we study the
scenario of learning extractors for relations for which
we do not have any Freebase alignments, evaluating
how the approaches are able to generalize only from

Relation # MF Inf Post Pre Joint

person/company 102 0.07 0.03 0.15 0.31 0.35
location/containedby 72 0.03 0.06 0.14 0.22 0.31
author/works written 27 0.02 0.05 0.18 0.31 0.27
person/nationality 25 0.01 0.19 0.09 0.15 0.19
parent/child 19 0.01 0.01 0.48 0.66 0.75
person/place of birth 18 0.01 0.43 0.40 0.56 0.59
person/place of death 18 0.01 0.24 0.23 0.27 0.23
neighborhood/neighborhood of 11 0.00 0.00 0.60 0.63 0.65
person/parents 6 0.00 0.17 0.19 0.37 0.65
company/founders 4 0.00 0.25 0.13 0.37 0.77
film/directed by 2 0.00 0.50 0.50 0.36 0.51
film/produced by 1 0.00 1.00 1.00 1.00 1.00

MAP 0.01 0.23 0.34 0.43 0.52
Weighted MAP 0.03 0.10 0.21 0.33 0.38

Table 2: Zero-shot Relation Learning: Average and
(weighted) mean average precisions with relations that do
not appear in any of the annotated formulae omitted from
the evaluation. The difference between “Pre” and “Joint”
is significant according to the sign-test (p < 0.05).

logic formulae and textual patterns. In §5.2 we then
describe an experiment where the amount of Free-
base alignments is varied in order to assess the effect
of combining distant supervision and background
knowledge on the accuracy of predictions. Although
the methods presented in this paper target relations
with insufficient alignments, we also provide a com-
parison on the complete distant supervision dataset
in §5.3. We conclude in §5.4 with a brief analysis of
the reasoning capacity of the learned embeddings.

5.1 Zero-shot Relation Learning

We start with the scenario of learning extractors for
relations that do not appear in the knowledge base
schema, i.e., those that do not have any textual align-
ments. Such a scenario occurs in practice when a new
relation needs to be added to a knowledge base for
which there are no facts that connect the new relation
to existing relations or surface patterns. For accurate
extractions of such relations, the model needs to rely
primarily on background domain knowledge to iden-
tify relevant textual alignments, but at the same time
it also needs to utilize correlations between textual
patterns for generalization. To simulate this setup,
we remove all alignments between all entity-pairs
and Freebase relations from the distant supervision
data, use the extracted logic formulae (§4) as back-
ground knowledge, and evaluate on the ability of the
different methods to recover the lost alignments.

Table 2 provides detailed results. Unsurprisingly,

1124



 0

 0.2

 0.4

 0.6

 0  0.1  0.2  0.3  0.4  0.5

w
M

AP

Fraction of Freebase training facts

MF
Joint

Pre
Post

Inf

Figure 2: Relations with Few Distant Labels:
Weighted mean average precisions of the various methods
as the fraction of Freebase training facts is varied. For
0% Freebase training facts we get the zero-shot relation
learning results presented in Table 2.

matrix factorization (MF) performs poorly since em-
beddings cannot be learned for the Freebase relations
without any observed cells. Scores higher than zero
for matrix factorization are caused by random pre-
dictions. Logical inference (Inf) is limited by the
number of known facts that appear as premise in one
of the implications. Although post-factorization in-
ference (Post) is able to achieve a large improvement
over logical inference, explicitly injecting logic for-
mulae into the embeddings (i.e. learning low-rank
logic embeddings) using pre-factorization inference
(Pre) or joint optimization (Joint) gives superior re-
sults. Last, we observe that joint optimization is able
to best combine logic formulae and textual patterns
for accurate, zero-shot learning of relation extractors.

5.2 Relations with Few Distant Labels

In this section we study the scenario of learning rela-
tions that have only a few distant supervision align-
ments, in particular, we observe the behavior of the
various methods as the amount of distant supervi-
sion is varied. We run all methods on training data
that contains different fractions of Freebase training
facts (and therefore different degrees of relation/text
pattern alignment), but keep all textual patterns in
addition to the set of extracted formulae.

Figure 2 summarizes the results. The performance
of pure logical inference does not depend on the
amount of distant supervision data, since it does not
take advantage of the correlations in the data. Ma-

 0.2

 0.4

 0.6

 0.8

 1

 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

Pr
ec

is
io

n

Recall

Joint
MF

Mintz09
Yao11

Surdeanu12
Riedel13-F

Figure 3: Comparison on Complete Data: Aver-
aged precision/recall curve demonstrating that the “Joint”
method outperforms existing factorization approaches
(“MF” and “Riedel13-F”). The formulae used by our ap-
proach have been extracted only from the training data.

trix factorization ignores logic formulae, and thus
is the baseline performance when only using distant
supervision. For the factorization based methods,
only a small fraction (15%) of the training data is
needed to achieve around 0.50 wMAP performance,
thus demonstrating that they are efficiently exploiting
correlations and generalizing to unobserved facts.

Pre-factorization inference, however, does not out-
perform post-factorization inference, and is on par
with matrix factorization for most of the curve. This
suggests that it is not an effective way of injecting
logic into embeddings when ground facts are also
available. In contrast, joint optimization leads to
low-rank logic embeddings that outperform all other
methods in the 0 to 30% Freebase training data inter-
val. Beyond 30% there seem to be sufficient Freebase
facts for matrix factorization to encode these formu-
lae, thus yielding diminishing returns.

5.3 Comparison on Complete Data

Although the focus of this paper is injection of logical
knowledge for relations without sufficient alignments
to the knowledge base, we also present an evaluation
on the complete distant supervision data as used by
Riedel et al. (2013). Compared to the Riedel et al.’s
“F” model, our matrix factorization implementation
(“MF”) achieves a lower wMAP (64% vs 68%) and
a higher MAP (66% vs 64%). We attribute this dif-
ference to the different loss function (logistic loss in
our case vs. ranking loss). We show the PR curve

1125



in Figure 3, demonstrating that joint optimization
provides benefits over the existing factorization and
distant supervision techniques even on the complete
dataset, and obtains 66% wMAP and 69% MAP. This
improvement over the matrix factorization model can
be explained by reinforcement of high-quality anno-
tated formulae via the joint model.

5.4 Analysis of Asymmetry in the Predictions

Since the injected formulae are of the form ∀x, y :
rs(x, y)⇒ rt(x, y), it is worthwhile to study the ex-
tent to which these rules are captured, and which
approaches are in fact capturing the asymmetric na-
ture of the implication. To this end, we compute the
probabilities that the formulae and their inverse hold,
averaged over all annotated formulae and cells. The
degree to which rs⇒ rt is captured is quite high for
all models (0.94, 0.96, and 0.97 for matrix factoriza-
tion, pre-factorization inference, and joint optimiza-
tion respectively). On the other hand, the probability
of rt⇒ rs is also relatively high for matrix factoriza-
tion and pre-factorization inference (0.81 and 0.83
respectively), suggesting that these methods are pri-
marily capturing symmetric similarity between rela-
tions. Joint optimization, however, produces much
more asymmetric predictions (probability of rt⇒ rs
is 0.49), demonstrating that it is appropriate for en-
coding logic in the embeddings.

6 Related Work

Embeddings for Knowledge Base Completion
Embedding predicates and constants (or pairs of con-
stants) based on factual knowledge for knowledge
base completion has for instance been investigated by
Bordes et al. (2011), Nickel et al. (2012), Socher et
al. (2013), Riedel et al. (2013) and Fan et al. (2014).
Our work goes further in that we learn embeddings
that follow not only factual but also first-order logic
knowledge, and the ideas presented in this paper can
be incorporated with any embedding-based method
that uses a per-atom loss.

Logical Inference A common alternative that di-
rectly incorporates first-order logic knowledge is to
perform logical inference (Bos and Markert, 2005;
Baader et al., 2007; Bos, 2008), however such purely
symbolic approaches cannot deal with the uncertainty
inherent to natural language, and generalize poorly.

Probabilistic Inference To ameliorate some of
the drawbacks of symbolic logical inference, proba-
bilistic logic based approaches have been proposed
(Schoenmackers et al., 2008; Garrette et al., 2011;
Beltagy et al., 2013; Beltagy et al., 2014). Since log-
ical connections between relations are modeled ex-
plicitly, such approaches are generally hard to scale.
Specifically, approaches based on Markov Logic Net-
works (MLNs) (Richardson and Domingos, 2006)
encode logical knowledge in dense, loopy graphical
models, making structure learning, parameter estima-
tion, and inference hard for the scale of our data. In
contrast, in our model the logical knowledge is cap-
tured directly in the embeddings, leading to efficient
inference. Furthermore, as our model is based on
matrix factorization, we have a natural way to deal
with linguistic ambiguities and label errors.

Weakly Supervised Learning Our work is also in-
spired by weakly supervised approaches (Ganchev et
al., 2010) that use structural constraints as a source of
indirect supervision, and have been used for several
NLP tasks (Chang et al., 2007; Mann and McCallum,
2008; Druck et al., 2009; Singh et al., 2010). Carlson
et al. (2010) in particular is similar since they use
common sense constraints to jointly train multiple
information extractors. In this work, however, we are
training a matrix factorization model, and allowing
for arbitrarily complex logic formulae.

Combining Symbolic and Distributed Represen-
tations There have been a number of recent ap-
proaches that combine distributed representations
with symbolic knowledge. Grefenstette (2013) de-
scribes an isomorphism between first-order logic
and tensor calculus, using full-rank matrices to ex-
actly memorize facts. Based on this isomorphism,
Rocktäschel et al. (2014) combine logic with ma-
trix factorization for learning low-dimensional em-
beddings that approximately satisfy given formulae
and generalize to unobserved facts on toy data. Our
work extends this workshop paper by proposing a
simpler formalism without tensor-based logical con-
nectives, presenting results on a real-world task, and
demonstrating the utility of this approach for learning
relations with few textual alignments.

Chang et al. (2014) use Freebase entity types as
hard constraints in a tensor factorization objective for
universal schema relation extraction. In contrast, our

1126



approach is imposing soft constraints that are formu-
lated as universally quantified first-order formula.

de Lacalle and Lapata (2013) combine first-order
logic knowledge with a topic model to improve sur-
face pattern clustering for relation extraction. Since
these formulae only specify which relations can be
clustered and which not, they do not capture the va-
riety of dependencies embeddings can model, such
as asymmetry. Lewis and Steedman (2013) use dis-
tributed representations to cluster predicates before
logical inference. Again, this approach is not as pow-
erful as factorizing the relations, as it makes symme-
try assumptions for the predicates.

Several studies have investigated the use of sym-
bolic representations (such as dependency trees)
to guide the composition of distributed representa-
tions (Clark and Pulman, 2007; Mitchell and Lapata,
2008; Coecke et al., 2010; Hermann and Blunsom,
2013). Instead we are using symbolic representa-
tions (first-order logic) as prior domain knowledge to
directly learn better embeddings.

Combining symbolic information with neural net-
works has also been an active area of research. Towell
and Shavlik (1994) introduce Knowledge-Based Arti-
ficial Neural Networks whose topology is isomorphic
to a knowledge base of facts and inference formulae.
There, facts are input units, intermediate conclusions
hidden units, and final conclusions (inferred facts)
output units. Unlike our work, there is no latent rep-
resentation of predicates and constants. Hölldobler
et al. (1999) and Hitzler et al. (2004) prove that for
every logic program theoretically there exists a recur-
rent neural network that approximates the semantics
of that program. Finally, Bowman (2014) recently
demonstrated that a neural tensor network can accu-
rately learn natural logic reasoning.

7 Conclusions

Inspired by the benefits of logical background knowl-
edge that can lead to precise extractors, and of distant
supervision based matrix factorization that can utilize
dependencies between textual patterns to generalize,
in this paper we introduced a novel training paradigm
for learning embeddings that combine matrix factor-
ization with logic formulae. Along with a determin-
istic approach to enforce the formulae a priori, we
propose a joint objective that rewards predictions that

satisfy given logical knowledge, thus learning embed-
dings that do not require logical inference at test time.
Experiments show that the proposed approaches are
able to learn extractors for relations with little to no
observed textual alignments, while at the same time
benefiting more common relations. The source code
of the methods presented in this paper and the anno-
tated formulae used for evaluation are available at
github.com/uclmr/low-rank-logic.

This research has thrown up many questions in
need of further investigation. As opposed to our ap-
proach that modifies both relation and entity-pair
embeddings, further work needs to explore train-
ing methods that only modify relation embeddings
in order to encode logical dependencies explicitly,
and thus avoid memorization. Although we ob-
tain significant gains by using implications, our ap-
proach facilitates the use of arbitrary formulae; it
would be worthwhile to pursue this direction by fol-
lowing the steps outlined in §3.2.1. Furthermore,
we are interested in combining relation extraction
with models that learn entity type representations
(e.g. tensor factorization or neural models) to al-
low for expressive logical statements such as ∀x, y :
nationality(x, y)⇒ country(y). Since such
common sense formulae are often not directly ob-
served in distant supervision, they can go a long way
in fixing common extraction errors. Finally, we will
investigate methods to automatically mine common-
sense knowledge for injection into embeddings from
additional resources such as Probase (Wu et al., 2012)
or directly from text using a semantic parser (Zettle-
moyer and Collins, 2005).

Acknowledgments

The authors want to thank Mathias Niepert for
proposing pre-factorization inference as alternative
to joint optimization. We thank Edward Grefenstette,
Luke Zettlemoyer, and Guillaume Bouchard for com-
ments on the manuscript, and the reviewers for very
helpful feedback. This work was supported in part
by Microsoft Research through its PhD Scholarship
Programme and in part by the TerraSwarm Research
Center, one of six centers supported by the STAR-
net phase of the Focus Center Research Program
(FCRP) a Semiconductor Research Corporation pro-
gram sponsored by MARCO and DARPA.

1127



References
Yaser S Abu-Mostafa. 1990. Learning from hints in

neural networks. Journal of complexity, 6(2):192–198.
Alan Akbik, Thilo Michael, and Christoph Boden. 2014.

Exploratory relation extraction in large text corpora. In
International Conference on Computational Linguistics
(COLING), pages 2087–2096.

Franz Baader, Bernhard Ganter, Baris Sertkaya, and Ulrike
Sattler. 2007. Completing description logic knowledge
bases using formal concept analysis. In IJCAI, pages
230–235.

Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013. Mon-
tague meets markov: Deep semantics with probabilis-
tic logical form. In 2nd Joint Conference on Lexical
and Computational Semantics: Proceeding of the Main
Conference and the Shared Task, Atlanta, pages 11–21.

Islam Beltagy, Katrin Erk, and Raymond J. Mooney. 2014.
Probabilistic soft logic for semantic textual similarity.
In Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL-14), Bal-
timore, MD.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of the 2008 ACM SIGMOD
international conference on Management of data, pages
1247–1250. ACM.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In AAAI.

Johan Bos and Katja Markert. 2005. Recognising textual
entailment with logical inference. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, pages
628–635. Association for Computational Linguistics.

Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, Research in Computational Semantics,
pages 277–286. College Publications.

Samuel R Bowman. 2014. Can recursive neural tensor
networks learn logical reasoning? In ICLR’14.

Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to Extract Relations from the Web using Mini-
mal Supervision. In Annual Meeting of the Association
for Computational Linguistics (ACL), Prague, Czech
Republic, June.

Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In International conference on Web search
and data mining (WSDM).

Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven learn-
ing. In Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 280–287.

Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christo-
pher Meek. 2014. Typed tensor decomposition of
knowledge bases for relation extraction.

Laura Chiticariu, Yunyao Li, and Frederick R. Reiss.
2013. Rule-based information extraction is dead! long
live rule-based information extraction systems! In
Empirical Methods in Natural Language Processing
(EMNLP), pages 827–832.

Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
AAAI Spring Symposium: Quantum Interaction, pages
52–55.

Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a composi-
tional distributional model of meaning. CoRR,
abs/1003.4394.

Michael Collins, Sanjoy Dasgupta, and Robert E Schapire.
2001. A generalization of principal components anal-
ysis to the exponential family. In Advances in neural
information processing systems, pages 617–624.

Oier Lopez de Lacalle and Mirella Lapata. 2013. Unsu-
pervised relation extraction with general domain knowl-
edge. In EMNLP, pages 415–425.

Gregory Druck, Gideon Mann, and Andrew McCallum.
2009. Semi-supervised learning of dependency parsers
using generalized expectation criteria. In Joint Confer-
ence of the Annual Meeting of the ACL and the Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine Learn-
ing Research, 12:2121–2159.

Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu,
Thomas Fang Zheng, and Edward Y Chang. 2014.
Distant supervision for relation extraction with matrix
completion. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics,
volume 1, pages 839–849.

Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine Learn-
ing Research (JMLR), July.

Dan Garrette, Katrin Erk, and Raymond Mooney. 2011.
Integrating logical representations with probabilistic
information using markov logic. In Proceedings of
the Ninth International Conference on Computational
Semantics, pages 105–114. Association for Computa-
tional Linguistics.

1128



Edward Grefenstette. 2013. Towards a formal distribu-
tional semantics: Simulating logical calculi with ten-
sors. In Proceedings of the Second Joint Conference
on Lexical and Computational Semantics, pages 1–10.

Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compositional
semantics. In Proc. of ACL, pages 894–904.

Jochen Hipp, Ulrich Güntzer, and Gholamreza
Nakhaeizadeh. 2000. Algorithms for association rule
mininga general survey and comparison. ACM sigkdd
explorations newsletter, 2(1):58–64.

Pascal Hitzler, Steffen Hölldobler, and Anthony Karel
Seda. 2004. Logic programs and connectionist net-
works. Journal of Applied Logic, 2(3):245–272.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).

Steffen Hölldobler, Yvonne Kalinke, and Hans-Peter Störr.
1999. Approximating the semantics of logic programs
by recurrent neural networks. Appl. Intell., 11(1):45–
58.

Mike Lewis and Mark Steedman. 2013. Combined distri-
butional and logical semantics. In Transactions of the
Association for Computational Linguistics, volume 1,
pages 179–192.

Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 870–878.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Association for Computational
Linguistics (ACL).

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proc. of ACL,
pages 236–244.

Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel.
2012. Factorizing yago: scalable machine learning for
linked data. In Proc. of WWW, pages 271–280.

Frederick Reiss, Sriram Raghavan, Rajasekar Krishna-
murthy, Huaiyu Zhu, and Shivakumar Vaithyanathan.
2008. An algebraic approach to rule-based informa-
tion extraction. In International Conference on Data
Engineering (ICDE), pages 933–942, April.

Matthew Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62(1-2):107–136.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Proceed-
ings of NAACL-HLT, pages 74–84.

Tim Rocktäschel, Matko Bosnjak, Sameer Singh, and
Sebastian Riedel. 2014. Low-Dimensional Embed-
dings of Logic. In ACL Workshop on Semantic Parsing
(SP’14).

Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium.

Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld.
2008. Scaling textual inference to the web. In
Empirical Methods in Natural Language Processing
(EMNLP).

Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order horn clauses
from web text. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).

Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies (NAACL HLT).

Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with neural
tensor networks for knowledge base completion. In
NIPS, pages 926–934.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D. Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Empirical
Methods in Natural Language Processing (EMNLP).

Geoffrey G Towell and Jude W Shavlik. 1994.
Knowledge-based artificial neural networks. Artificial
intelligence, 70(1):119–165.

Johanna Völker and Mathias Niepert. 2011. Statistical
schema induction. In The Semantic Web: Research and
Applications, pages 124–138. Springer.

Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q
Zhu. 2012. Probase: A probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management of
Data, pages 481–492. ACM.

Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’11), July.

Luke S Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Uncertainty in Artifical Intelligence (UAI).

1129


