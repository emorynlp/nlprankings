



















































Chinese Native Language Identification


Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

Chinese Native Language Identification

Shervin Malmasi
Centre for Language Technology

Macquarie University
Sydney, NSW, Australia

shervin.malmasi@mq.edu.au

Mark Dras
Centre for Language Technology

Macquarie University
Sydney, NSW, Australia

mark.dras@mq.edu.au

Abstract
We present the first application of Na-
tive Language Identification (NLI) to non-
English data. Motivated by theories of lan-
guage transfer, NLI is the task of iden-
tifying a writer’s native language (L1)
based on their writings in a second lan-
guage (the L2). An NLI system was ap-
plied to Chinese learner texts using topic-
independent syntactic models to assess
their accuracy. We find that models using
part-of-speech tags, context-free grammar
production rules and function words are
highly effective, achieving a maximum ac-
curacy of 71% . Interestingly, we also find
that when applied to equivalent English
data, the model performance is almost
identical. This finding suggests a sys-
tematic pattern of cross-linguistic transfer
may exist, where the degree of transfer is
independent of the L1 and L2.

1 Introduction
Native Language Identification (NLI) is the task of
identifying an author’s native language (L1) based
on their writings in a second language (the L2).
NLI works by identifying language use patterns
that are common to groups of speakers that share
the same native language. This process is under-
pinned by the presupposition that an author’s L1
will dispose them towards particular language pro-
duction patterns in their L2, as influenced by their
mother tongue. This relates to Cross-Linguistic
Influence (CLI), a key topic in the field of Second
Language Acquisition (SLA) that analyzes trans-
fer effects from the L1 on later learned languages
(Ortega, 2009).

While NLI has applications in security, most re-
search has a strong linguistic motivation relating to
language teaching and learning. Rising numbers
of language learners have led to an increasing need

for language learning resources, which has in turn
fuelled much of the language acquisition research
of the past decade. In this context, by identify-
ing L1-specific language usage and error patterns,
NLI can be used to better understand SLA and de-
velop teaching methods, instructions and learner
feedback that is specific to their mother tongue.

However, all of the NLI research to date has fo-
cused exclusively on English L2 data. To this end
there is a need to apply NLI to other languages,
not only to gauge their applicability but also to aid
in teaching research for other emerging languages.

Interest in learning Chinese is rapidly growing,
leading to increased research in Teaching Chinese
as a Second Language (TCSL) and the develop-
ment of related resources such as learner corpora
(Chen et al., 2010). The application of these tools
and scientific methods like NLI can greatly assist
researchers in creating effective teaching practices
and is an area of active research.

The aim of this research is to evaluate the cross-
language applicability of NLI techniques by ap-
plying them to Chinese learner texts, evaluating
their efficacy and comparing the results with their
English equivalents.

To the best of our knowledge this is the first
reported application of NLI to non-English data
and we believe this is an important step in gain-
ing deeper insights about the technique.

2 Related Work
NLI is a fairly recent, but rapidly growing area of
research. While some research was conducted in
the early 2000s, the most significant work has only
appeared in the last few years (Wong and Dras,
2009; Wong and Dras, 2011; Swanson and Char-
niak, 2012; Tetreault et al., 2012; Bykh and Meur-
ers, 2012).

Most studies approach NLI as a multi-class su-
pervised classification task. In this experimental
design, the L1 metadata are used as class labels

95



and the individual writings are used as training and
testing data. Using lexical and syntactic features
of increasing sophistication, researchers have ob-
tained good results under this paradigm. While a
detailed exposition of NLI has been omitted here
due to space constraints, a concise review can be
found in Bykh and Meurers (2012).

2.1 NLI 2013 Shared Task
This increased interest brought unprecedented
level of research focus and momentum, resulting
in the first NLI shared task being held in 2013.1

The shared task aimed to facilitate the comparison
of results by providing a large NLI-specific dataset
and evaluation procedure, to enable direct compar-
ison of results achieved through different methods.
Overall, the event was considered a success, draw-
ing 29 entrants and experts from not only Compu-
tational Linguistics, but also SLA. The best teams
achieved accuracies of greater than 80% on this
11-class classification task. A detailed summary
of the results is presented in Tetreault et al. (2013).

3 Data
Growing interest has led to the recent develop-
ment of the Chinese Learner Corpus (Wang et al.,
2012), the first large-scale corpus of learner texts
comprised of essays written by university stu-
dents. Learners from 59 countries are represented
and proficiency levels have been sampled repre-
sentatively across beginners, intermediate and ad-
vanced learners. However, texts by native speak-
ers of other Asian countries are disproportionately
represented, likely due to geographical proximity.

For this work we extracted 3.75 million tokens
of text from the CLC in the form of individual
sentences.2 Following the methodology of Brooke
and Hirst (2011), we combine the sentences from
the same L1 to form texts of 600 tokens on aver-
age, creating a set of documents suitable for NLI3.

We choose the top 11 languages, shown in Ta-
ble 1, to use in our experiments. This is due to
two considerations. First, while many L1s are rep-
resented in the corpus, most have relatively few
texts. Choosing the top 11 classes allows us to

1Organised by the Educational Testing Service and co-
located with the eighth instalment of the Building Ed-
ucational Applications Workshop at NAACL/HLT 2013.
sites.google.com/site/nlisharedtask2013/

2Full texts are not made available, only individual sen-
tences with the relevant metadata (proficiency/nationality).

3Pending permission from the CLC corpus authors, we
will attempt to release the Chinese NLI dataset publicly.

Language Size Language Size
Filipino FIL 415 Indonesian IND 402
Thai THA 400 Laotian LAO 366
Burmese MYA 349 Korean∗ KOR 330
Khmer KHM 294 Vietnamese VIE 267
Japanese∗ JAP 180 Spanish∗ SPA 112
Mongolian MON 101

Table 1: Our data, broken down by language and
the number of texts in each class. Languages over-
lapping with the TOEFL11 corpus marked with ∗.

balance having a large number of classes, and also
maximizes the amount of data used. Secondly, this
is the same number of classes used in the NLI 2013
shared task, enabling us to draw cross-language
comparisons with the shared task results.

4 Experimental Setup
We also follow the supervised classification ap-
proach described in §2. We devise and run exper-
iments using several models that capture different
types of linguistic information. For each model,
features are extracted from the texts and a clas-
sifier is trained to predict the L1 labels using the
features. As our data is not topic-balanced, we
avoid using topic-dependent lexical features such
as character or word n-grams.

Each experiment is run with two feature repre-
sentations: binary (presence/absence of a feature)
and normalized frequencies, where feature values
are normalized to text length using the l2-norm.

4.1 Parser
The Stanford CoreNLP4 suite of NLP tools and
the provided Chinese models are used to tokenize,
PoS tag and parse the unsegmented corpus texts.

4.2 Classifier
We use Support Vector Machines for classifica-
tion. Specifically, we use the LIBLINEAR SVM
package (Fan et al., 2008) as it is well-suited to
text classification tasks with large numbers of fea-
tures and texts. We use the L2-regularized L2-loss
support vector classification (dual) solver.

4.3 Evaluation
The same evaluation metrics and standards used in
the NLI2013 Shared Task are used: we report clas-
sification accuracy under 10-fold cross-validation.
We also use the same number of classes as the
shared task to facilitate comparative analyses.

4http://nlp.stanford.edu/software/corenlp.shtml

96



Feature Accuracy (%)
Binary Frequency

Random Baseline 9.09 9.09
PoS unigrams 20.12 35.32
Part-of-Speech bigrams 32.83 54.24
Part-of-Speech trigrams 47.24 55.60

Function Words 43.93 51.91

Production Rules 36.14 49.80
All features 61.75 70.61

Table 2: Chinese Native Language Identification
accuracy (%) for all of our models.

5 Experiments and Results
5.1 Part-of-Speech tag n-grams

Our first experiment assesses the utility of the
syntactic information captured by part-of-speech
(PoS) tags for Chinese NLI. The PoS tags for each
text are predicted and n-grams of size 1–3 are ex-
tracted from the tags. These n-grams capture (very
local) syntactic patterns of language use and are
used as classification features.

The results for these three features, and our
other models are shown in Table 2. The trigram
frequencies give the best accuracy of 55.60%, sug-
gesting that there exist group-specific patterns of
Chinese word order and category choice which
provide a highly discriminative cue about the L1.

5.2 Function Words

As opposed to content words, function words are
topic-independent grammatical words that indi-
cate the relations between other words. They
include determiners, conjunctions and auxiliary
verbs. Distributions of English function words
have been found to be useful in studies of author-
ship attribution and NLI. Unlike PoS tags, this
model analyzes the author’s specific word choices.

We compiled a list of 449 Chinese function
words5 to be used as features in this model. As
shown in Table 2, the function word frequency
features provide the best accuracy of 51.91%,
significantly higher than the random baseline.
This again suggests the presence of L1-specific
grammatical and lexical choice patterns that can
help distinguish the L1, potentially due to cross-
linguistic transfer. Such lexical transfer effects

5The function word list was compiled from Chinese lan-
guage teaching resources. The complete list can be accessed
at http://comp.mq.edu.au/˜madras/research/
data/chinese-fw.txt

ROOT

IP

PU

。

VP

VP

IP

VP

VV

上瘾

QP

CLP

M

点

CD

一

VE

有

PP

NP

NP

NN

游戏

DP

DT

这个

P

对

NP

PN

我

IP → NP VP PU VP → PP VP
NP → DP NP PP → P NP
Figure 1: A constituent parse tree for a sentence
from the corpus along with some of the context-
free grammar production rules extracted from it.

have been previously noted by researchers and
linguists (Odlin, 1989). These effects are medi-
ated not only by cognates and similarities in word
forms, but also word semantics and meanings.

5.3 Context-free Grammar Production Rules

In the next experiment we investigate the differ-
ences in the distribution of the context-free gram-
mar production rules used by the learners. To do
this, constituent parses for all sentences are ob-
tained and the production rules, excluding lexical-
izations, are extracted. Figure 1 shows a sample
tree and rules. These context-free phrase structure
rules capture the overall structure of grammatical
constructions and are used as classification fea-
tures in this experiment.

As seen in Table 2, the model achieves an accu-
racy of 49.80%. This supports the hypothesis that
the syntactic substructures contain characteristic
constructions specific to L1 groups and that these
syntactic cues strongly signal the writer’s L1.

5.4 Combining All Features

Finally, we assess the redundancy of the informa-
tion captured by our models by combining them
all into one vector space to create a single clas-
sifier. From Table 2 we see that for each feature
representation, the combined feature results are
higher than the single best feature, with a max-

97



imum accuracy of 70.61%. This demonstrates
that for at least some of the features, the informa-
tion they capture is orthogonal and complemen-
tary, and combining them can improve results.

6 Discussion
A key finding here is that NLI models can be suc-
cessfully applied to non-English data. This is an
important step for furthering NLI research as the
field is still relatively young and many fundamen-
tal questions have yet to be answered.

All of the tested models are effective, and they
appear to be complementary as combining them
improves overall accuracy. We also note the differ-
ence in the efficacy of the feature representations
and see a clear preference for frequency-based fea-
ture values. Others have found that binary features
are the most effective for English NLI (Brooke and
Hirst, 2012), but our results indicate frequency in-
formation is more informative in this task. The
combination of both feature types has also been
reported to be effective (Malmasi et al., 2013).

To see how these models perform across lan-
guages, we also compare the results against the
TOEFL11 corpus used in the NLI2013 shared
task. We perform the same experiments on that
dataset using the English CoreNLP models, Penn
Treebank PoS tagset and a set of 400 English func-
tion words. Figure 2 shows the results side by side.

Remarkably, we see that the model results
closely mirror each other across corpora. This is a
highly interesting finding from our study that mer-
its further investigation. There is a systematic pat-
tern occurring across data from learners of com-
pletely different L1-L2 pairs. This suggests that
manifestations of CLI via surface phenomena oc-
cur at the same levels and patternings regardless
of the L2. Cross-language studies can help re-
searchers in linguistics and cognitive science to
better understand the SLA process and language
transfer effects. They can enhance our understand-
ing of how language is processed in the brain in
ways that are not possible by just studying mono-
linguals or single L1-L2 pairs, thereby providing
us with important insights that increase our knowl-
edge and understanding of the human language
faculty.

One limitation of this work is the lack of sim-
ilar amounts of training data for each language.
However, many of the early and influential NLI
studies (e.g. Koppel et al. (2005), Tsur and Rap-
poport (2007)) were performed under similar cir-

PoS-1 PoS-2 PoS-3 FW PR
0

20

40

60

A
cc

ur
ac

y
(%

)

Chinese English

Figure 2: Comparing feature performance on the
Chinese Learner Corpus and English TOEFL11
corpora. PoS-1/2/3: PoS uni/bi/trigrams, FW:
Function Words, PR: Production Rules

cumstances. This issue was noted at the time, but
did not deter researchers as corpora with similar
issues were used for many years. Non-English
NLI is also at a similar state where the extant cor-
pora are not optimal for the task, but no other al-
ternatives exist for conducting this research.

Finally, there are also a number of way to fur-
ther develop this work. Firstly, the experimental
scope could be expanded to use even more lin-
guistically sophisticated features such as depen-
dency parses. Model accuracy could potentially
be improved by using the metadata to develop
proficiency-segregated models. Classifier ensem-
bles could also help in increasing the accuracy.

7 Conclusion
In this work we have presented the first application
of NLI to non-English data. Using the Chinese
Learner Corpus, we compare models based on
PoS tags, function words and context-free gram-
mar production rules and find that they all yield
high classification accuracies.

Comparing the models against an equivalent
English learner corpus we find that the accura-
cies are almost identical across both L2s, suggest-
ing a systematic pattern of cross-linguistic transfer
where the degree of transfer is independent of the
L1 and L2. Further research with other L2 learner
corpora is needed to investigate this phenomena.

Acknowledgments
We wish to thank Associate Professor Maolin
Wang for providing access to the CLC corpus, and
Zhendong Zhao for his assistance. We also thank
the reviewers for their constructive feedback.

98



References
Julian Brooke and Graeme Hirst. 2011. Na-

tive language detection with ‘cheap’ learner cor-
pora. In Conference of Learner Corpus Research
(LCR2011), Louvain-la-Neuve, Belgium. Presses
universitaires de Louvain.

Julian Brooke and Graeme Hirst. 2012. Robust, Lex-
icalized Native Language Identification. In Pro-
ceedings of COLING 2012, pages 391–408, Mum-
bai, India, December. The COLING 2012 Organiz-
ing Committee.

Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams – In-
vestigating Abstraction and Domain Dependence.
In Proceedings of COLING 2012, pages 425–440,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.

Jianguo Chen, Chuang Wang, and Jinfa Cai. 2010.
Teaching and learning Chinese: Issues and perspec-
tives. IAP.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.

Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Automatically determining an anonymous au-
thor’s native language. In Intelligence and Security
Informatics, volume 3495 of LNCS, pages 209–217.
Springer-Verlag.

Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. Nli shared task 2013: Mq submission.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 124–133, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.

Terence Odlin. 1989. Language Transfer: Cross-
linguistic Influence in Language Learning. Cam-
bridge University Press, Cambridge, UK.

Lourdes Ortega. 2009. Understanding Second Lan-
guage Acquisition. Hodder Education, Oxford, UK.

Benjamin Swanson and Eugene Charniak. 2012.
Native Language Detection with Tree Substitution
Grammars. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 193–197, Jeju
Island, Korea, July. Association for Computational
Linguistics.

Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in na-
tive language identification. In Proceedings of COL-
ING 2012, pages 2585–2602, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.

Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 48–57, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.

Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computat.
Language Acquisition, pages 9–16.

Maolin Wang, Qi Gong, Jie Kuang, and Ziyu Xiong.
2012. The development of a chinese learner corpus.
In Speech Database and Assessments (Oriental CO-
COSDA), 2012 International Conference on, pages
1–6. IEEE.

Sze-Meng Jojo Wong and Mark Dras. 2009. Con-
trastive Analysis and Native Language Identifica-
tion. In Proceedings of the Australasian Language
Technology Association Workshop 2009, pages 53–
61, Sydney, Australia, December.

Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing Parse Structures for Native Language Identifi-
cation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1600–1610, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.

99


