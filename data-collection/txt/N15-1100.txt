



















































Word Embedding-based Antonym Detection using Thesauri and Distributional Information


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 984–989,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Word Embedding-based Antonym Detection using Thesauri and
Distributional Information

Masataka Ono, Makoto Miwa, Yutaka Sasaki
Department of Advanced Science and Technology

Toyota Technological Institute
2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan

{sd12412, makoto-miwa, yutaka.sasaki}@toyota-ti.ac.jp

Abstract

This paper proposes a novel approach to train
word embeddings to capture antonyms. Word
embeddings have shown to capture synonyms
and analogies. Such word embeddings, how-
ever, cannot capture antonyms since they de-
pend on the distributional hypothesis. Our
approach utilizes supervised synonym and
antonym information from thesauri, as well
as distributional information from large-scale
unlabelled text data. The evaluation results on
the GRE antonym question task show that our
model outperforms the state-of-the-art sys-
tems and it can answer the antonym questions
in the F-score of 89%.

1 Introduction

Word embeddings have shown to capture synonyms
and analogies (Mikolov et al., 2013b; Mnih and
Kavukcuoglu, 2013; Pennington et al., 2014). Word
embeddings have also been effectively employed
in several tasks such as named entity recogni-
tion (Turian et al., 2010; Guo et al., 2014), adjectival
scales (Kim and de Marneffe, 2013) and text classi-
fication (Le and Mikolov, 2014). Such embeddings
trained based on distributional hypothesis (Harris,
1954), however, often fail to recognize antonyms
since antonymous words, e.g. strong and weak, oc-
cur in similar contexts. Recent studies focuses on
learning word embeddings for specific tasks, such
as sentiment analysis (Tang et al., 2014) and de-
pendency parsing (Bansal et al., 2014; Chen et al.,
2014). These motivate a new approach to learn word
embeddings to capture antonyms.

Recent studies on antonym detection have shown
that thesauri information are useful in distinguishing
antonyms from synonyms. The state-of-the-art sys-
tems achieved over 80% in F-score on GRE antonym
tests. Yih et al. (2012) proposed a Polarity Induc-
ing Latent Semantic Analysis (PILSA) that incor-
porated polarity information in two thesauri in con-
structing a matrix for latent semantic analysis. They
additionally used context vectors to cover the out-of-
vocabulary words; however, they did not use word
embeddings. Recently, Zhang et al. (2014) pro-
posed a Bayesian Probabilistic Tensor Factorization
(BPTF) model to combine thesauri information and
existing word embeddings. They showed that the
usefulness of word embeddings but they used pre-
trained word embeddings.

In this paper, we propose a novel approach
to construct word embeddings that can capture
antonyms. Unlike the previous approaches, our ap-
proach directly trains word embeddings to represent
antonyms. We propose two models: a Word Em-
bedding on Thesauri information (WE-T) model and
a Word Embeddings on Thesauri and Distributional
information (WE-TD) model. The WE-T model re-
ceives supervised information from synonym and
antonym pairs in thesauri and infers the relations
of the other word pairs in the thesauri from the su-
pervised information. The WE-TD model incorpo-
rates corpus-based contextual information (distribu-
tional information) into the WE-T model, which en-
ables the calculation of the similarities among in-
vocabulary and out-of-vocabulary words.

984



co-occurrence

Relations in Thesauri Relations inferred by WE-T

Distributional information

Relations inferred

by WE-TD

disperse

garner

scatter

nucleate

disperse

garner

scatter
synonym

antonym

disperse

garner

scatterWE-T

(§ 2.1)

WE-TD

(§ 2.2)
disperse

garner

scatter

nucleate

Figure 1: Overview of our approach. When we use the thesauri directly, disperse and garner are known to be antony-
mous and disperse and scatter are known to be synonymous, but the remaining relations are unknown. WE-T infers
indirect relations among words in thesauri. Furthermore, WE-TD incorporates distributional information, and the
relatedness among in-vocabulary and out-of-vocabulary words (nucleate here) are obtained.

2 Word embeddings for antonyms

This section explains how we train word embed-
dings from synonym and antonym pairs in thesauri.
We then explain how to incorporate distributional in-
formation to cover out-of-vocabulary words. Fig-
ure 1 illustrates the overview of our approach.

2.1 Word embeddings using thesauri
information

We first introduce a model to train word embeddings
using thesauri information alone, which is called the
WE-T model. We embed vectors to words in the-
sauri and train vectors to represent synonym and
antonym pairs in the thesauri. More concretely, we
train the vectors by maximizing the following objec-
tive function:∑

w∈V

∑
s∈Sw

log σ(sim(w, s))

+α
∑
w∈V

∑
a∈Aw

log σ(−sim(w, a))
(1)

V is the vocabulary in thesauri. Sw is a set of syn-
onyms of a word w, and Aw is a set of antonyms of
a word w. σ(x) is the sigmoid function 1

1+e−x . α is
a parameter to balance the effects of synonyms and
antonyms. sim(w1, w2) is a scoring function that
measures a similarity between two vectors embed-
ded to the corresponding words w1 and w2. We use
the following asymmetric function for the scoring

function:

sim(w1, w2) = vw1 · vw2 + bw1 (2)
vw is a vector embedded to a word w and bw is a
scalar bias term corresponding to w. This similarity
score ranges from minus infinity to plus infinity and
the sigmoid function in Equation (1) scales the score
into the [0, 1] range.

The first term of Equation (1) denotes the sum of
the similarities between synonym pairs. The second
term of Equation (1) denotes the sum of the dissimi-
larities between antonym pairs. By maximizing this
objective, synonym and antonym pairs are tuned to
have high and low similarity scores respectively, and
indirect antonym pairs, e.g., synonym of antonym,
will also have low similarity scores since the em-
beddings of the words in the pairs will be dissimi-
lar. We use AdaGrad (Duchi et al., 2011) to maxi-
mize this objective function. AdaGrad is an online
learning method using a gradient-based update with
automatically-determined learning rate.

2.2 Word embeddings using thesauri and
distributional information

Now we explain a model to incorporate corpus-
based distributional information into the WE-T
model, which is called the WE-TD model.

We hereby introduce Skip-Gram with Negative
Sampling (SGNS) (Mikolov et al., 2013a), which
the WE-TD model bases on. Levy and Goldberg
(2014) shows the objective function for SGNS can

985



be rewritten as follows.∑
w∈V

∑
c∈V

{#(w, c) log σ(sim(w, c))

+ k#(w)P0(c) log σ(−sim(w, c))}
(3)

The first term represents the co-occurrence pairs
within a context window of C words preceding and
following target words. #(w, c) stands for the num-
ber of appearances of a target word w and its con-
text c. The second term represents the negative sam-
pling. k is a number of negatively sampled words for
each target word. #p(w) is the number of appear-
ances of w as a target word, and its negative context
c is sampled from a modified unigram distribution
P0 (Mikolov et al., 2013a). We employ the subsam-
pling (Mikolov et al., 2013a), which discards words
according to the probability of P (w) = 1 −

√
t

p(w) .

p(w) is the proportion of occurrences of a word w
in the corpus, and t is a threshold to control the dis-
card. When we use a large-scale corpus directly, the
effects of rare words are dominated by the effects of
frequent words. Subsampling alleviates this prob-
lem by discarding frequent words more often than
rare words.

To incorporate the distributional information into
the WE-T model, we propose the following objec-
tive function, which simply adds this objective func-
tion to Equation 1 with an weight β:

β{
∑
w∈V

∑
s∈Sw

log σ(sim(w, s))

+α
∑
w∈V

∑
a∈Aw

log σ(−sim(w, a))}

+
∑
w∈V

∑
c∈V

{#(w, c) log σ(sim(w, c))

+k#(w)P0(c) log σ(−sim(w, c))}

(4)

This function can be further arranged as∑
w∈V

∑
c∈V

{Aw,c log σ(sim(w, c))

+ Bw,c log σ(−sim(w, c))}
(5)

Here, the coefficients Aw,c and Bw,c are sums of cor-
responding coefficients in Equation 4. These terms
can be pre-calculated by using the number of ap-
pearances of contextual word pairs, unigram distri-
butions, and synonym and antonym pairs in thesauri.

The objective is maximized by using AdaGrad.
We skip some updates according to the coefficients
Aw,c and Bw,c to speed up the computation; we
ignore the terms with extremely small coefficients
(< 10−5) and we sample the terms according to the
coefficients when the coefficients are less than 1.

3 Experiments

3.1 Evaluation settings

This section explains the task setting, resource for
training, parameter settings, and evaluation metrics.

3.1.1 GRE antonym question task
We evaluate our models and compare them with

other existing models using GRE antonym ques-
tion dataset originally provided by Mohammad et
al. (2008). This dataset is widely used to evaluate
the performance of antonym detection. Each ques-
tion has a target word and five candidate words, and
the system has to choose the most contrasting word
to the target word from the candidate words (Mo-
hammad et al., 2013). All the words in the questions
are single-token words. This dataset consists of two
parts, development and test, and they have 162 and
950 questions, respectively. Since the test part con-
tains 160 development data set, We will also report
results on 790 (950-160) questions following Mo-
hammad et al. (2013).

In evaluating our models on the questions, we first
calculated similarities between a target word and its
candidate words. The similarities were calculated
by averaging asymmetric similarity scores using the
similarity function in Equation 2. We then chose a
word which had the lowest similarity among them.
When the model did not contain any words in a ques-
tion, the question was left unanswered.

3.1.2 Resource for training
For supervised dataset, we used synonym and

antonym pairs in two thesauri: WordNet (Miller,
1995) and Roget (Kipfer, 2009). These pairs were
provided by Zhang et al. (2014)1. There were 52,760
entries (words), each of which had 11.7 synonyms
on average, and 21,319 entries, each of which had
6.5 antonyms on average.

1https://github.com/iceboal/
word-representations-bptf

986



Dev. Set Test Set (950) Test Set (790)
Prec. Rec. F Prec. Rec. F Prec. Rec. F

Encarta lookup† 0.65 0.61 0.63 0.61 0.56 0.59 — — —
WordNet & Roget lookup¶ 1.00 0.49 0.66 0.98 0.45 0.62 0.98 0.45 0.61
WE-T 0.92 0.71 0.80 0.90 0.72 0.80 0.90 0.72 0.80
WordNet + Affix heuristics

0.79 0.66 0.72 — — — 0.77 0.63 0.69
+ Adjacent category annotation§

WE-D 0.09 0.08 0.09 0.08 0.07 0.07 0.07 0.07 0.07
Encarta PILSA

0.88 0.87 0.87 0.81 0.80 0.81 — — —
+ S2Net + Embedding†

WordNet & Roget BPTF‡ 0.88 0.88 0.88 0.82 0.82 0.82 — — —
WE-TD 0.92 0.91 0.91 0.90 0.88 0.89 0.89 0.87 0.88

Table 1: Results on the GRE antonym question task. † is from Yih et al. (2012), ‡ is from Zhang et al. (2014), and §

is from Mohammad et al. (2013). ¶ slightly differs from the result in Zhang et al. (2014) since thesauri can contain
multiple candidates as antonyms and the answer is randomly selected for the candidates.

Error Type Description
# Example

Errors Target Gold Predicted

Contrasting
Predicted answer is contrasting,

7
reticence loquaciousness storm

but not antonym. dussuade exhort extol

Degree
Both answers are antonyms, but gold

3 postulate verify reject
has a higher degree of contrast.

Incorrect gold Gold answer is incorrect. 2 flinch extol advance
Wrong Gold and predicted answers are

1 hapless fortunate happy
expansion both in the expanded thesauri.
Incorrect Predicted answer is not contrasting. 1 sessile obile ceasing

Total 14 — — —

Table 2: Error types by WE-TD on the development set.

We obtained raw texts from Wikipedia on Novem-
ber 2013 for unsupervised dataset. We lowercased
all words in the text.

3.1.3 Parameter settings

The parameters were tuned using the development
part of the dataset. In training the WE-T model, the
dimension of embeddings was set to 300, the num-
ber of iteration of AdaGrad was set to 20, and the
initial learning rate of AdaGrad was set to 0.03. α
in Equation 1 were set to 3.2, according to the pro-
portion of the numbers of synonym and antonym
pairs in the thesauri. In addition to these parameters,
when we trained the WE-TD model, we added the
top 100,000 frequent words appearing in Wikipedia
into the vocabulary. The parameter β was set to 100,

the number of negative sampling k was set as 5, the
context window size C was set to 5, the threshold
for subsampling2 was set to 10−8.

3.1.4 Evaluation metrics

We used the F-score as a primary evaluation met-
ric following Zhang et al. (2014). The F-score is the
harmonic mean of precision and recall. Precision is
the proportion of correctly answered questions over
answered questions. Recall is the proportion of cor-
rectly answered questions over the questions.

2This small threshold is because this was used to balance the
effects of supervised and unsupervised information.

987



3.2 Results

Table 1 shows the results of our models on the GRE
antonym question task. This table also shows the
results of previous systems (Yih et al., 2012; Zhang
et al., 2014; Mohammad et al., 2013) and models
trained on Wikipedia without thesauri (WE-D) for
the comparison.

The low performance of WE-D illuminates the
problem of distributional hypothesis. Word em-
beddings trained by using distributional information
could not distinguish antonyms from synonyms.

Our WE-T model achieved higher performance
than the baselines that only look up thesauri. In
the thesauri information we used, the synonyms and
antonyms have already been extended for the origi-
nal thesauri by some rules such as ignoring part of
speech (Zhang et al., 2014). This extension con-
tributes to the larger coverage than the original syn-
onym and antonym pairs in the thesauri. This im-
provement shows that our model not only captures
the information of synonyms and antonyms pro-
vided by the supervised information but also infers
the relations of other word pairs more effectively
than the rule-based extension.

Our WE-TD model achieved the highest score
among the models that use both thesauri and distri-
butional information. Furthermore, our model has
small differences in the results on the development
and test parts compared to the other models.

3.3 Error Analysis

We analyzed the 14 errors on the development set,
and summarized the result in Table 2.

Half of the errors (i.e., seven errors) were caused
in the case that the predicted word is contrasting to
some extent but not antonym (“Contrasting”). This
might be caused by some kind of semantic drift. In
order to predict these gold answers correctly, con-
straints of the words, such as part of speech and se-
lectional preferences, need to be used. For example,
“venerate” usually takes “person” as its object, while
“magnify” takes “god.” Three of the errors were
caused by the degree of contrast of the gold and the
predicted answers (“Degree”). The predicted word
can be regarded as an antonym but the gold answer
is more appropriate. This is because our model does
not consider the degree of antonymy, which is out of

our focus. One of the questions in the errors had an
incorrect gold answer (“Incorrect gold”). We found
that in one case both gold and predicted answers are
in the expanded antonym dictionary (“Wrong expan-
sion”). In expanding dictionary entries, the gold and
predicted answers were both included in the word
list of an antonym entries. In one case, the predicted
answer was simply wrong (“Incorrect”).

4 Conclusions

This paper proposed a novel approach that trains
word embeddings to capture antonyms. We pro-
posed two models: WE-T and WE-TD models. WE-
T trains word embeddings on thesauri information,
and WE-TD incorporates distributional information
into the WE-T model. The evaluation on the GRE
antonym question task shows that WE-T can achieve
a higher performance over the thesauri lookup base-
lines and, by incorporating distributional informa-
tion, WE-TD showed 89% in F-score, which out-
performed the conventional state-of-the-art perfor-
mances. As future work, we plan to extend our ap-
proaches to obtain word embeddings for other se-
mantic relations (Gao et al., 2014).

References

Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.
Tailoring continuous word representations for depen-
dency parsing. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 809–815, Balti-
more, Maryland, June. Association for Computational
Linguistics.

Wenliang Chen, Yue Zhang, and Min Zhang. 2014. Fea-
ture embedding for dependency parsing. In Proceed-
ings of COLING 2014, the 25th International Confer-
ence on Computational Linguistics: Technical Papers,
pages 816–826, Dublin, Ireland, August. Dublin City
University and Association for Computational Lin-
guistics.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning
Research, 12:2121–2159, July.

Bin Gao, Jiang Bian, and Tie-Yan Liu. 2014. Wordrep:
A benchmark for research on learning word represen-
tations. ICML 2014 Workshop on Knowledge-Powered
Deep Learning for Text Mining.

988



Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu.
2014. Revisiting embedding features for simple semi-
supervised learning. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 110–120, Doha, Qatar,
October. Association for Computational Linguistics.

Zellig S Harris. 1954. Distributional structure. Word,
10(23):146–162.

Joo-Kyung Kim and Marie-Catherine de Marneffe. 2013.
Deriving adjectival scales from continuous space word
representations. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1625–1630, Seattle, Washington, USA,
October. Association for Computational Linguistics.

Barbara Ann Kipfer. 2009. Roget’s 21st Century The-
saurus. Philip Lief Group, third edition edition.

Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Tony Jebara
and Eric P. Xing, editors, Proceedings of the 31st In-
ternational Conference on Machine Learning (ICML-
14), pages 1188–1196. JMLR Workshop and Confer-
ence Proceedings.

Omer Levy and Yoav Goldberg. 2014. Neural word em-
bedding as implicit matrix factorization. In Z. Ghahra-
mani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q.
Weinberger, editors, Advances in Neural Information
Processing Systems 27, pages 2177–2185. Curran As-
sociates, Inc.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositionality.
In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K.Q. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 26, pages 3111–
3119. Curran Associates, Inc.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 746–751, Atlanta, Georgia, June.
Association for Computational Linguistics.

George A. Miller. 1995. Wordnet: A lexical database for
english. Commun. ACM, 38(11):39–41, November.

Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive es-
timation. In Z. Ghahramani, M. Welling, C. Cortes,
N.D. Lawrence, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems 26,
pages 2265–2273. Curran Associates, Inc.

Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word-pair antonymy. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 982–991, Honolulu,

Hawaii, October. Association for Computational Lin-
guistics.

Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics, 39(3):555–590, Septem-
ber.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1532–1543, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin. 2014. Learning sentiment-specific
word embedding for twitter sentiment classification.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 1555–1565, Baltimore, Mary-
land, June. Association for Computational Linguistics.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, ACL ’10, pages 384–394, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012.
Polarity inducing latent semantic analysis. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1212–
1222, Jeju Island, Korea, July. Association for Com-
putational Linguistics.

Jingwei Zhang, Jeremy Salwen, Michael Glass, and Al-
fio Gliozzo. 2014. Word semantic representations us-
ing bayesian probabilistic tensor factorization. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
1522–1531. Association for Computational Linguis-
tics.

989


