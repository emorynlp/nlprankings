Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 761–769,

Beijing, August 2010

761

Controlling Listening-oriented Dialogue using Partially Observable

Markov Decision Processes

Toyomi Meguro†, Ryuichiro Higashinaka‡, Yasuhiro Minami†, Kohji Dohsaka†

†NTT Communication Science Laboratories, NTT Corporation

meguro@cslab.kecl.ntt.co.jp

‡NTT Cyber Space Laboratories, NTT Corporation
higashinaka.ryuichiro@lab.ntt.co.jp
{minami,dohsaka}@cslab.kecl.ntt.co.jp
Abstract

This paper investigates how to automat-
ically create a dialogue control compo-
nent of a listening agent to reduce the cur-
rent high cost of manually creating such
components. We collected a large number
of listening-oriented dialogues with their
user satisfaction ratings and used them to
create a dialogue control component using
partially observable Markov decision pro-
cesses (POMDPs), which can learn a pol-
icy to satisfy users by automatically ﬁnd-
ing a reasonable reward function. A com-
parison between our POMDP-based com-
ponent and other similarly motivated sys-
tems using human subjects revealed that
POMDPs can satisfactorily produce a dia-
logue control component that can achieve
reasonable subjective assessment.

Introduction

1
Although task-oriented dialogue systems have
been actively researched (Hirshman, 1989; Fer-
guson et al., 1996; Nakano et al., 1999; Walker
et al., 2002), recently non-task-oriented functions
are starting to attract attention, and systems with-
out a speciﬁc task that deal with more casual di-
alogues, such as chats, are being actively investi-
gated from their social and entertainment aspects
(Bickmore and Cassell, 2001; Higashinaka et al.,
2008; Higuchi et al., 2008).

In the same vein, we have been working on
listening-oriented dialogues in which one conver-
sational participant attentively listens to the other
(hereafter, listening-oriented dialogue). Our aim
is to build listening agents that can implement
such a listening process so that users can satisfy
their desire to speak and be heard. Figure 1 shows

an excerpt from a typical listening-oriented dia-
logue. In the literature, dialogue control compo-
nents for less (or non-) task-oriented dialogue sys-
tems, such as listening agents, have typically used
hand-crafted rules for dialogue control, which
can be problematic because completely covering
all dialogue states by hand-crafted rules is difﬁ-
cult when the dialogue has fewer task restrictions
(Wallace, 2004; Isomura et al., 2009).

To solve this problem, this paper aims to auto-
matically build a dialogue control component of a
listening agent using partially observable Markov
decision processes (POMDPs). POMDPs, which
make it possible to learn a policy that can max-
imize the averaged reward in partially observable
environments (Pineau et al., 2003), have been suc-
cessfully adopted in task-oriented dialogue sys-
tems for learning a dialogue control module from
data (Williams and Young, 2007). However, no
work has attempted to use POMDPs for less (or
non-) task-oriented dialogue systems, such as lis-
tening agents, because user goals are not as well-
deﬁned as task-oriented ones, complicating the
ﬁnding of a reasonable reward function.

We apply POMDPs to listening-oriented dia-
logues by having the system learn a policy that si-
multaneously maximizes how well users feel that
they are being listened to (hereafter, user satis-
faction) and how smoothly the system generates
dialogues (hereafter, smoothness). This formu-
lation is new; no work has considered both user
satisfaction and smoothness using POMDPs. We
collected a large amount of listening-oriented di-
alogues and annotated them with dialogue acts
and also obtained subjective evaluation results for
them. From them, we calculated the rewards and
learned the POMDP policies. We evaluated the
dialogue-act tag sequences of our POMDPs using
human subjects.

762

Utterance

S: Good evening.

The topic is “food,” nice to
meet you.

L: Nice to meet you, too.
S: I had curry for dinner.

Do you like curry?

L: Yes, I do.
S: Really?
Me, too.

Dialogue act
GREETING
GREETING

GREETING
S-DISC (sub: fact)
QUESTION (sub: pref)
SYMPATHY
REPEAT
SYMPATHY

L: Do you usually go out to eat? QUESTION (sub: habit)
S: No, I always cook at home.

S-DISC (sub: habit)
S-DISC (sub: habit)

I don’t use any special spices,
but I sometimes cook noodles
using soup and curry.

L: That sounds good!

S-DISC (sub: pref (pos-
itive))

Figure 1: Excerpt of a typical listening-oriented
dialogue. Dialogue topic is “food.” Dialogue acts
corresponding to utterances are shown in paren-
theses (See Table 1 for meanings): S-DISC stands
for SELF-DISCLOSURE; PREF for PREFERENCE;
S for speaker; and L for listener. The dialogue
was translated from Japanese by the authors.

The next section introduces related work. Sec-
tion 3 describes our approach. Section 4 de-
scribes our collection of listening-oriented dia-
logues. This is followed in Section 5 by an evalua-
tion experiment that compared our POMDP-based
dialogue control with other similarly motivated
systems. The last section summarizes the main
points and mentions future work.

2 Related work
With increased attention on social dialogues and
senior peer counseling, work continues to emerge
on listening-oriented dialogues. One early work
is (Maatman et al., 2005), which showed that vir-
tual agents can give users the sense of being heard
using such gestures as nodding and head shak-
ing. Recently, Meguro et al.
(2009a) analyzed
the characteristics of listening-oriented dialogues.
They compared listening-oriented dialogues and
casual conversations between humans, revealing
that the two types of dialogues have signiﬁcantly
different ﬂows and that listeners actively ques-
tion with frequently inserted self-disclosures; the
speaker utterances were mostly concerned with
self-disclosure.

Shitaoka et al.

(2010) also investigated the
functions of listening agents, focusing on their
response generation components. Their system
takes the conﬁdence score of speech recognition

into account and changes the system response ac-
cordingly; it repeats the user utterance or makes
an empathic utterance for high-conﬁdence user ut-
terances and makes a backchannel when the con-
ﬁdence is low. The system’s empathic utterances
can be “I’m happy” or “That’s too bad,” depend-
ing on whether a positive or negative expression
is included in the user utterances. Their system’s
response generation only uses the speech recogni-
tion conﬁdence and the polarity of user utterances
as cues to choose its actions. Currently, it does
not consider the utterance content or the user in-
tention.

In order for listening agents to achieve high
smoothness, a switching mechanism between the
“active listening mode,” in which the system is
a listener, and the “topic presenting mode,” in
which the system is a speaker, has been proposed
(Yokoyama et al., 2010; Kobayashi et al., 2010).
Here, the system uses a heuristic function to main-
tain a high user interest level and to keep the sys-
tem in an active listening mode. Dialogue con-
trol is done by hand-crafted rules. Our motivation
bears some similarity to theirs in that we want to
build a listening agent that gives users a sense of
being heard; however, we want to automatically
make such an agent from dialogue data.

POMDPs have been introduced for robot action
control (Pineau et al., 2003). Here, the system
learns to make suitable movements for complet-
ing a certain task. Over the years, POMDPs have
been actively studied for applications to spoken
dialogue systems. Williams et al.
(2007) suc-
cessfully used a POMDP for dialogue control in a
ticket-buying domain in which the objective was
to ﬁx the departure and arrival places for tickets.
Recent work on POMDPs indicates that it is pos-
sible to train a dialogue control module in task-
oriented dialogues when the user goal is obvious.
In contrast, in this paper, we aim to verify whether
POMDPs can be applied to less task-oriented di-
alogues (i.e., listening-oriented dialogues) where
user goals are not as obvious.

In a recent study, Minami et al.

(2009) ap-
plied POMDPs to non-task-oriented man-machine
interaction. Their system learned suitable ac-
tion control of agents that can act smoothly by
obtaining rewards from the statistics of artiﬁ-
cially generated data. Our work is different be-
cause we use real human-human dialogue data to

763

train POMDPs for dialogue control in listening-
oriented dialogues.

3 Approach
A typical dialogue system has utterance under-
standing, dialogue control, and utterance gen-
eration modules. The utterance understanding
module comprehends user natural-language utter-
ances, whose output (i.e., a user dialogue act) is
passed to the dialogue control module. The dia-
logue control module chooses the best system di-
alogue act at every dialogue point using the user
dialogue act as input. The utterance generation
module generates natural-language utterances and
says them to users by realizing the system dia-
logue acts as surface forms.

This paper focuses on the dialogue control
module of a listening agent. Since a listening-
oriented dialogue has a characteristic conversation
ﬂow (Meguro et al., 2009a), focusing on this mod-
ule is crucial because it deals with the dialogue
ﬂow. Our objective is to train from data a dialogue
control module that achieves a smooth dialogue
ﬂow that makes users feel that they are being lis-
tened to attentively.

3.1 Dialogue control using POMDPs
The purpose of our dialogue control is to simulta-
neously create situations in which users feel lis-
tened to (i.e., user satisfaction) and to generate
smooth action sequences (i.e., smoothness). To
do this, we automatically and statistically train
the reward and the policy of the POMDP using a
large amount of listening-oriented dialogue data.
POMDP is a reinforcement learning framework
that can learn a policy to select an action sequence
that maximizes average future rewards. Setting a
reward is crucial in POMDPs.

For our purpose, we introduce two different re-
wards: one for user satisfaction and the other for
smoothness. Before creating a POMDP structure,
we used the dynamic Bayesian network (DBN)
structure (Fig. 2) to obtain the statistical structure
of the data and the two rewards.

The random values in the DBN are as follows:
so and sa are the dialogue state and action state,
o is a speaker observation, a is a listener action,
and d is a random variable for an evaluation score
that indicates the degree of the user being listened
to. This evaluation score can be obtained by ques-

d

r

s

‘
o

'o

s

‘
a

'a

s

o

o

s

a

a

s

‘
o

'o

s

‘
a

'a

s

o

o

s

a

a

DBN structure

POMDP structure

Figure 2: DBN and POMDP structures employed
in this paper. Note that a in the POMDP is isolated
from other states because it is decided by a learned
policy.

tionnaires, and the variable is used for calculat-
ing a user satisfaction reward for the POMDP.
The DBN arcs in Fig. 2 deﬁne the emission and
transition probabilities. Pr(o(cid:2)|s(cid:2)o) is the emission
probability of o(cid:2) given s(cid:2)o. Pr(d|so) is the emis-
sion probability of d given so. Pr(s(cid:2)o|so, a) is a
transition probability from so to s(cid:2)o given a. The
DBN is trained using the EM algorithm. Using
the obtained variables, we calculate the two re-
ward functions as follows:
(1) Reward for user satisfaction This reward is
obtained from the d variable by

r1((so, ∗), a) =

d × Pr(d|so, a),

max(cid:2)d=min

where * is arbitrary sa and min and max are min-
imum and maximum evaluation scores.
(2) Reward for smoothness For smoothness,
we maximize the action predictive probability
given the history of actions and observations. The
probability is calculated from listening-oriented
dialogue data. sa is introduced for estimating the
predictive probability of action a and for selecting
a to maximize the predictive probability.

We set Pr(a|sa) = 1 when a = sa so that sa
corresponds one-on-one with a. Then, if at = sa
at time t is given, we obtain
Pr(at|o1, a1, . . . , at−1, ot)

Pr(at|s(cid:2)a) Pr(s(cid:2)a|o1, a1, . . . , at−1, ot)

=(cid:2)s(cid:2)a

= Pr(sa|o1, a1, . . . , ot−1, at−1, ot)

Consequently, maximizing the predictive proba-
bility of a equals maximizing that of sa.
If we

764

set 1.0 to reward r2((∗, sa), a) when sa = a, the
POMDP will generate actions that maximize their
predictive probabilities. We believe that this re-
ward should increase the smoothness of a system
action sequence since the sequence is generated
according to the statistics of human-human dia-
logues.
Converting a DBN into a POMDP The DBN
is converted into a POMDP (Fig. 2), while main-
taining the transition and output probabilities. We
convert d to r as described above.

The system is in a partially observed state.
Since the state is not known exactly, we use a dis-
tribution called “belief state” bt with which we ob-
tain the average reward that will be gained in the
future at time t by:

Vt =

bτ +t((so, sa))r((so, sa), aτ +t),

∞(cid:2)τ =0

γ τ(cid:2)s

where τ is a discount factor; namely, the future
reward is decreased by τ. A policy is learned by
value iteration so that the action that maximizes
Vt can be chosen. We deﬁne r((so, sa), a) as fol-
lows:
r((so, sa), a) = r1((so, ∗), a) + r2((∗, sa), a).
By balancing these two rewards, we can choose
an action that satisﬁes both user satisfaction and
smoothness.

4 Data collection
We collected listening-oriented dialogues using
human subjects who consisted of ten listeners
(ﬁve males and ﬁve females) and 37 speakers (18
males and 19 females). The listeners and speak-
ers ranged from 20 to 60 years old and were all
native Japanese speakers. Listeners and speakers
were matched to form a listener-speaker pair and
communicated over the Internet with our chat in-
terface. They used only text; they were not al-
lowed to use voice, video, or facial expressions.
The speakers chose their own listener and freely
participated in dialogues from 7:00 pm to mid-
night for a period of 15 days. One conversation
was restricted to about ten minutes. The subjects
talked about a topic chosen by the speaker. There
were 20 predeﬁned topics: money, sports, TV and
radio, news, fashion, pets, movies, music, house-
work and childcare, family, health, work, hob-
bies, food, human relationships, reading, shop-
ping, beauty aids, travel, and miscellaneous. The

listeners were instructed to make it easy for the
speakers to say what the speakers wanted to say.
We collected 1260 listening-oriented dialogues.

4.1 Dialogue-act annotation
We labeled the collected dialogues using the
dialogue-act tag set shown in Table 1. We made
these tags by selecting, extending, and modifying
those from previous studies that concerned human
listening behaviors in some way (Meguro et al.,
2009a; Jurafsky et al., 1997; Ivey and Ivey, 2002).
In our tag set, only question and self-disclosure
tags have sub-category tags. Two annotators (not
the authors) labeled each sentence of our collected
dialogues using these 32 tags. In dialogue-act an-
notation, since there can be several sentences in
one utterance, one annotator ﬁrst split the utter-
ances into sentences, and then both annotators la-
beled each sentence with a single dialogue act.

4.2 Obtaining evaluation scores
POMDPs need evaluation scores (i.e., d) for dia-
logue acts (i.e., a) for training a reward function.
Therefore, we asked a third-party participant, who
was neither a listener nor a speaker in our dialogue
data collection, to evaluate the user satisfaction
levels of the collected dialogues. She rated each
dialogue in terms of how she would have felt “be-
ing heard” after the dialogue if she had been the
speaker of the dialogue in question. She provided
ratings on the 7-point Likert scale for each dia-
logue. Since she rated the whole dialogue with a
single rating, we set the evaluation score of each
action within a dialogue using the evaluation score
for that dialogue.

We used a third-person’s evaluation and not the
original person’s to avoid the fact that the eval-
uative criterion is too different between humans;
identical evaluation scores from two people do
not necessarily reﬂect identical user satisfaction
levels. We highly valued the reliability and con-
sistency of the third-person scores. This way, at
least, we can train a policy that maximizes its av-
erage reward function for the rater, which we need
to verify ﬁrst before considering adaptation to two
or more individuals.

5 Experiment
5.1 Experimental setup
The experiment followed three steps.

765

GREETING

INFORMATION

SELF-
DISCLOSURE
sub: fact
sub: experience
sub: habit
sub: preference

(positive)

sub: preference

(negative)

sub: preference

(neutral)

sub: desire
sub: plan
sub: other
ACKNOWLEDGM-
ENT
QUESTION
sub: information
sub: fact
sub: experience
sub: habit
sub: preference
sub: desire
sub: plan

sub: other
SYMPATHY

Greeting and conﬁrmation of dialogue
theme.
e.g., Hello. Let’s talk about
lunch.
Delivery of objective information. e.g.,
My friend recommended a restaurant.
Disclosure of preferences and feelings.

e.g., I live in Tokyo.
e.g., I had a hamburger for lunch.
e.g., I always go out for dinner.
e.g., I like hamburgers.

e.g., I don’t really like hamburgers.

e.g., Its taste is near my homemade

taste.

e.g., I want to try it.
e.g., I’m going there next week.

Encourage the conversational partner to
speak. e.g., Well. Aha.
Utterances that expect answers.
e.g., Please tell me how to cook it.
e.g., What kind of curry?
e.g., What did you have for dinner?
e.g., Did you cook it yourself?
e.g., Do you like it?
e.g., Don’t you want to eat rice?
e.g., What are you going to have for

dinner?

Sympathetic utterances and praises.
e.g., Me, too.

NON-SYMPATHY Negative utterances. e.g., Not really.
CONFIRMATION Conﬁrm what the conversation partner

PROPOSAL

REPEAT
PARAPHRASE
APPROVAL

THANKS
APOLOGY
FILLER

said. e.g., Really?
Encourage the partner to act. e.g., Try
it.
Repeat the partner’s utterance.
Paraphrase the partner’s utterance.
Broach or show goodwill toward the
partner. e.g., Absolutely!
Express thanks e.g., Thank you.
Express regret e.g., I’m sorry.
Filler between utterances. e.g., Uh. Let
me see.
Express affection. e.g., Ha-ha.
Other utterances.

ADMIRATION
OTHER
Table 1: Deﬁnition and example of dialogue acts

In the ﬁrst step, we created our POMDP sys-
tem using our approach (See Section 3.1). We
also made ﬁve other systems for comparison that
we describe in Section 5.2. Each system outputs
dialogue-act tag sequences for evaluation. The
dialogue theme was “food” because it was the
most frequent theme and accounted for 20% of
our data (See Table 2 for the statistics); we trained
our POMDP using the “food” dialogues. We re-
stricted the dialogue topic to verify that our ap-
proach at least works with a small set. Since there
is no established measure for automatically eval-
uating a dialogue-act tag sequence, we evaluated

# dialogues
# words
# utterances per dialogue
# dialogues per listener
# dialogues per speaker
# dialogue acts
inter-annotator agreement

All Food (subset of All)
250
94867
29.1
25
6.8
13376
0.55

1260
479881
28.2
126
34
67801
0.57

Table 2: Statistics of collected dialogues and
dialogue-act annotation.
Inter-annotator agree-
ment means agreement of dialogue-act annotation
using Cohen’s κ.

our dialogue control module using human subjec-
tive evaluations. However, this is very difﬁcult to
do because dialogue control modules only output
dialogue acts, not natural language utterances.

In the second step, we recruited participants
who created natural
language utterances from
dialogue-act tag sequences. In their creating dia-
logues, we provided them with situations to stim-
ulate their imaginations. Table 3 shows the situ-
ations, which were deemed common in everyday
Japanese life; we let the participants create utter-
ances that ﬁt the situations. These situations were
necessary because, without restrictions, the evalu-
ation scores could be inﬂuenced by dialogue con-
tent rather than by dialogue ﬂow.

For this dialogue-imagining exercise, we re-
cruited 16 participants (eight males and eight fe-
males) who ranged from 19 to 39 years old. Each
participant made twelve dialogues using two situ-
ations. For assigning the situations, we ﬁrst cre-
ated four conditions: (1) a student and living with
family, (2) working and living with family, (3) a
student and living alone, and (4) working and liv-
ing alone. Then the participants were categorized
into one of these conditions on the basis of their
actual lifestyle and assigned two of the situations
matching the condition.

For each situation, each participant created six
imaginary dialogues from the six dialogue-act se-
quences output by the six systems: our POMDP
and the other ﬁve systems for comparison. This
process produced such dialogues as shown in
Figs. 5 and 6. The dialogue in Fig. 5 was made
from a dialogue-act tag sequence of a human-
human conversation using No. 1 of Table 3. The
dialogue in Fig. 6 was made from the sequence of
our POMDP using No. 2 of Table 3.

In the third step, we additionally recruited three
judges (one male and two females) to evalu-

766

ate the imagined 192 (16 × 2 × 6) dialogues.
The judges were neither the participants who
made dialogues nor those who rated the collected
listening-oriented dialogues. Six dialogues made
from one situation were randomly shown to the
judges one-by-one, who then ﬁlled out question-
naires to indicate their user satisfaction levels by
answering this question on a 7-point Likert scale:
“If you had been the speaker, would you have felt
that you were listened to?”

5.2 Systems for comparison
We created our POMDP-based dialogue control
and ﬁve other systems for comparison.
POMDP We learned a policy based on our ap-
proach. We used “food” dialogues (See Section
4), and the evaluation scores were those described
in Section 4.2. This system used the policy to
generate sequences of dialogue-act tags by sim-
ulation; user observations were generated based
on emission probability, and system actions were
generated based on the policy.

In this paper, the total number of observations
and actions was 33 because we have 32 dialogue-
act tags (See Table 1) plus a “skip” tag. In learning
the policy, an observation and an action must indi-
vidually take turns, but our data can include mul-
tiple dialogue-act tags in one utterance. There-
fore, if there is more than one dialogue-act tag
in one utterance, a “skip” is inserted between the
tags. The state numbers for So and Sa were 16
and 33, respectively. In this experiment, we set 10
to r2((∗, sa), a).
EvenPOMDP We arranged a POMDP using
only the smoothness reward (hereafter, Even-
POMDP) by creating a POMDP system with a
ﬁxed evaluation score; hence user satisfaction
is not incorporated in the reward. When using
ﬁxed (even) evaluation scores for all dialogues,
the effect of the user satisfaction reward is de-
nied, and the system only generates highly fre-
quent sequences. We have EvenPOMDP to clarify
whether user satisfaction is necessary. The other
conditions are identical as in the POMDP system.
HMM We modeled our dialogue-act
tag se-
quences using a Speaker HMM (SHMM) (Me-
guro et al., 2009a), which has been utilized to
model two-party listening-oriented dialogues. In a
SHMM, half the states emit listener dialogue acts,

1

2

Listener：

GREETI N G

3

Listener：

QUESTI O N

Speaker：

GREETI N G

4

Speaker：
S-DISC

Listener：
S-DISC

or

5

SYMPATH Y

Figure 3: Structure of rule-based system

and the other half emit speaker dialogue acts. All
states are connected to each other. We modeled
the “food” dialogues using an SHMM, and made
the model generate the most probable dialogue-act
tag sequences. More speciﬁcally, ﬁrst, a dialogue-
act tag was generated randomly based on the ini-
tial state.
If the state was that of a listener, we
generated a maximum likelihood action and the
state was randomly transited based on the transi-
tion probability. If the state was that of a speaker,
we randomly generated an action based on the
emission probability and the state was transited
using the maximum likelihood transition proba-
bility.

system This
Rule-based
system creates
dialogue-act
tag sequences using hand-crafted
rules that are based on the ﬁndings in (Meguro et
al., 2009a) and are realized as shown in Fig. 3.
A sequence begins at state 1(cid:3) in Fig. 3, and one
dialogue act is generated at each state. At state
3(cid:3), a sub-category tag under QUESTION is chosen
randomly, and at state 4(cid:3), a matched sub-category
tag under SELF-DISCLOSURE is chosen. At
the listener’s SELF-DISCLOSURE or
state 5(cid:3),
SYMPATHY is generated randomly.

Human dialogue sequence This system created
dialogue-act tag sequences by randomly choosing
dialogues between humans from the collected data
and used their annotated tag sequences.

Random This system simply created dialogue-
act tag sequences at random.

5.3 Experimental results
Figure 4 shows the average subjective evaluation
scores. Except between HMM and EvenPOMDP,
there was a signiﬁcant difference (p<0.01) be-
tween all systems in a non-parametric multiple
comparison test (Steel-Dwass test). The dialogues
shown in Figs. 5 and 6 were generated by the sys-
tems. The dialogue in Fig. 5 was made from hu-
man dialogue sequences, and the one in Fig. 6 was
made from POMDP.

767

With whom What day
weekday
weekend
weekday

family
family

co-workers

What time

What

around 6:00 pm
grilled salmon
around 7:00 pm potato and meat
boiled seaweed

at noon

...

...

...

Where
home
home
...

lunch box

weekday

at noon

hamburger

school cafeteria

...
friend

Who made

mother
mother
myself

...
N/A

1
2
3
...
32

Table 3: Dialogue situations relating to everyday Japanese life

We qualitatively analyzed the dialogues of each
system and observed the following characteristics:
POMDP At a dialogue’s beginning, the system
greets several times and shifts to a different phase
in which listeners ask questions and self-disclose
to encourage speakers to reciprocate.
Rule-based The output of this system seems
very natural and easy to read. The dialogue-act
tags followed reasonable rules, making it easier
for the participants to create natural utterances
from them.
Human conversation The dialogues between
humans were obviously natural before they were
changed to tags from the natural-language ut-
terances. However, human dialogues have ran-
domness, which makes it difﬁcult for the partic-
ipants to create natural-language utterances from
the tags. Hence, the evaluation score for this sys-
tem was lower than the “Rule-based.”
HMM, EvenPOMDP Since these systems con-
tinually output the same action tags, their output
was very unnatural. For example, greetings never
stopped because GREETING is most frequently
followed by GREETING in the data. These sys-
tems have no mechanism to stop this loop.

POMDP successfully avoided such continua-
tion because its actions have more varied rewards.
For example, GREETING is repeated in Even-
POMDP because its smoothness reward is high;
however, in POMDP, although the smoothness re-
ward remains high, its user satisfaction reward is
not that high. This is because greetings appear
in all dialogues and their user satisfaction reward
converges to the average. Therefore, such actions
as greetings do not get repeated in POMDP. In
POMDP, some states have high user satisfaction
rewards, and the POMDP policy generated actions
to move to such states.
Random Since this system has more variety
of tags than HMM,
its evaluation scores out-
performed HMM, but were outperformed by
POMDP, which learned statistically from the data.

Rule-based

6.07

Human 
dialogue

5.22 POMDP

（Proposed）

3.76

Random

2.67

HMM
1.17

Even

POMDP 

1.16

7

6

5

4

3

2

1

0

 

s
e
r
o
c
S
n
o
i
t
a
u
a
v
E
d
e
g
a
r
e
v
A

l

 

System scores.

Figure 4:
Except between
POMDP and EvenPOMDP, signiﬁcant differences
exist among all systems (p<0.01).

From our qualitative analysis, we found that
POMDP can generate more satisfying sequences
than HMM/EvenPOMDP because it does not fall
into the loop of frequent dialogue-act tag se-
quences. This suggests the usefulness of incor-
porating two kinds of rewards into the policy and
that our approach for setting a reward is promis-
ing.

However, with the proposed POMDP, unnatural
sequences remain; for example, the system sud-
denly output THANKS, as shown in Fig. 6. The
number of states may have been too small. We
plan to investigate what caused this in the future.
In our qualitative analysis, we observed that
randomness in dialogues might hold a clue for
improving evaluation scores.
Therefore, we
measured the perplexity of each system output
using dialogue-act trigrams and obtained 72.8
for “Random,” 27.4 for “Human dialogue,” 7.4
for “POMDP,” 3.2 for “HMM,” 2.5 for “Even-
POMDP,” and 1.7 for “Rule-based.”

The perplexity of the human dialogues is less
than that of the random system, but humans also
exhibit a certain degree of freedom. On the other
hand, POMDP’s perplexity is less than the human
dialogues; they still have some freedom, which
probably led to their reasonable evaluation scores.
Considering that HMM and EvenPOMDP, which
continually output the same dialogue acts, had low

768

Utterance

S: Hello.
L: Nice to meet you
S: I had dinner at home today.
Do you like grilled salmon?

L: Yes, I think so.

I sometimes want
fancy meal.

to have a

L: Do you usually do your own

S: Deluxe.
Me too.

cooking?

S: No, I don’t.

I always buy my meals at the
convenience store.

L: I like the lunch boxes of conve-

nience stores

Dialogue act
GREETING
GREETING
S-DISC (sub: fact)
QUESTION, PREF
SYMPATHY
S-DISC (sub: de-
sire)
REPEAT
SYMPATHY
QUESTION
habit)
S-DISC, HABIT
S-DISC
habit)
S-DISC (sub: pref
(positive))

(sub:

(sub:

Figure 5: Excerpt of listening-oriented dialogue
that participant imagined from tag sequences of
human conversations. Dialogue was translated
from Japanese by the authors.

Utterance

L: Nice to meet you.

Where and who did you have
dinner with today?

S: I had “niku-jaga” (meat and
beef) with my family at home.

L: Oh.
S: I think it is normal to eat with

your family at home.

L: Thanks.

Do you have any brothers or sis-
ters?
Soon, my brother and his wife
will visit my home.

S: I see.
L: I want to use expensive meat in

my “niku-jaga.”
Oh.
Please give me your recipe.

S: My friends claim that my
“niku-jaga” is as good as a
restaurant’s.

L: I’d love to try it

Dialogue act
GREETING
QUESTION
fact)
S-DISC (sub: fact)

(sub:

ADMIRATION
S-DISC (sub: pref
(neutral))
THANKS
QUESTION
fact)
S-DISC (sub: plan)

(sub:

SYMPATHY
S-DISC (sub: de-
sire)
ADMIRATION
QUESTION
information)
INFORMATION

(sub:

S-DISC (sub: de-
sire)

Figure 6: Excerpt of a listening-oriented dialogue
made from tag sequences of POMDP

evaluation scores, we conclude that randomness is
necessary in non-task-oriented dialogues and that
some randomness can be included with our ap-
proach. We do not discuss “Rule-based” here be-
cause its tag sequence was meant to have small
perplexity.

6 Conclusion and Future work
This paper investigated the possibility of automat-
ically building a dialogue control module from di-

alogue data to create automated listening agents.
With a POMDP as a learning framework,
a dialogue control module was learned from
the listening-oriented dialogues we collected and
compared with ﬁve different systems.
Our
POMDP system showed higher performance in
subjective evaluations than other statistically mo-
tivated systems, such as an HMM-based one, that
work by selecting the most likely subsequent ac-
tion in the dialogue data. When we investigated
the output sequences of our POMDP system, the
system frequently chose to self-disclose and ques-
tion, which corresponds to human listener be-
havior, as revealed in the literature (Meguro et
al., 2009a). This suggests that learning dialogue
control by POMDPs is achievable for listening-
oriented dialogues.

The main contribution of this paper is that
we successfully showed that POMDPs can be
used to train dialogue control policies for less
task-oriented dialogue systems, such as listening
agents, where the user goals are not as clear as
task-oriented ones. We also revealed that the re-
ward function can be learned effectively by our
formulation that simultaneously maximizes user
satisfaction and smoothness. Finding an appro-
priate reward function is a real challenge for less
task-oriented dialogue systems; this work has pre-
sented the ﬁrst workable solution.

Much work still remains. Even though we
conducted an evaluation experiment by simula-
tion (i.e, ofﬂine evaluation), human dialogues ob-
viously do not necessarily proceed as in simula-
tions. Therefore, we plan to evaluate our sys-
tem using online evaluation, which also forces us
to implement utterance understanding and gener-
ation modules. We also want to incorporate the
idea of topic shift into our policy learning because
we observed in our data that listeners frequently
change topics to keep speakers motivated. We are
also considering adapting the system behavior to
users. Speciﬁcally, we want to investigate dia-
logue control that adapts to the personality traits
of users because it has been found that the ﬂow
of listening-oriented dialogues differs depending
on the personality traits of users (Meguro et al.,
2009b). Finally, although we only dealt with text,
we also want to extend our approach to speech and
other modalities, such as gestures and facial ex-
pressions.

769

References
Bickmore, Timothy and Justine Cassell. 2001. Rela-
tional agents: a model and implementation of build-
ing user trust.
In Proc. SIGCHI conference on
human factors in computing systems (CHI), pages
396–403.

Ferguson, George, James F. Allen, and Brad Miller.
1996. TRAINS-95: towards a mixed-initiative plan-
ning assistant. In Proc. Third Artiﬁcial Intelligence
Planning Systems Conference (AIPS), pages 70–77.

Higashinaka, Ryuichiro, Kohji Dohsaka, and Hideki
Isozaki. 2008. Effects of self-disclosure and em-
pathy in human-computer dialogue. In Proc. IEEE
Workshop on Spoken Language Technology (SLT),
pages 108–112.

Higuchi, Shinsuke, Rafal Rzepka, and Kenji Araki.
2008. A casual conversation system using modal-
ity and word associations retrieved from the web.
In Proc. 2008 conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
382–390.

Hirshman, Lynette. 1989. Overview of the DARPA
speech and natural language workshop.
In Proc.
DARPA Speech and Natural Language Workshop
1989, pages 1–2.

Isomura, Naoki, Fujio Toriumi, and Kenichiro Ishii.
2009. Evaluation method of non-task-oriented di-
alogue system using HMM. IEICE Transactions on
Information and Systems, J92-D(4):542–551.

Ivey, Allen E. and Mary Bradford Ivey. 2002.

In-
tentional Interviewing and Counseling: Facilitat-
ing Client Development in a Multicultural Society.
Brooks/Cole Publishing Company.

Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-
asca, 1997. Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual.

Kobayashi, Yuka, Daisuke Yamamoto, Toshiyuki
Koga, Sachie Yokoyama, and Miwako Doi. 2010.
Design targeting voice interface robot capable of
active listening.
In Proc. 5th ACM/IEEE inter-
national conference on Human-robot interaction
(HRI), pages 161–162,

Maatman, R. M., Jonathan Gratch, and Stacy Marsella.
2005. Natural behavior of a listening agent. Lecture
Notes in Computer Science, 3661:25–36.

Meguro, Toyomi, Ryuichiro Higashinaka, Kohji
Dohsaka, Yasuhiro Minami, and Hideki Isozaki.
2009a. Analysis of listening-oriented dialogue for
building listening agents. In Proc. 10th Annual SIG-
DIAL Meeting on Discourse and Dialogue (SIG-
DIAL), pages 124–127.

Meguro, Toyomi, Ryuichiro Higashinaka, Kohji
Dohsaka, Yasuhiro Minami, and Hideki Isozaki.
2009b. Effects of personality traits on listening-
oriented dialogue. In Proc. International Workshop
on Spoken Dialogue Systems Technology (IWSDS),
pages 104–107.

Minami, Yasuhiro, Akira Mori, Toyomi Meguro,
Ryuichiro Higashinaka, Kohji Dohsaka, and Eisaku
Maeda.
2009. Dialogue control algorithm for
ambient intelligence based on partially observable
markov decision processes. In Proc. International
Workshop on Spoken Dialogue Systems Technology
(IWSDS), pages 254–263.

Nakano, Mikio, Noboru Miyazaki, Jun ichi Hirasawa,
Kohji Dohsaka, and Takeshi Kawabata. 1999. Un-
derstanding unsegmented user utterances in real-
time spoken dialogue systems.
In Proc. 37th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 200–207.

Pineau, Joelle., Geoff. Gordon, and Sebastian Thrun.
2003. Point-based value iteration: An anytime al-
gorithm for POMDPs. In Proc. International Joint
Conference on Artiﬁcial Intelligence (IJCAI), pages
1025–1032.

Shitaoka, Kazuya, Ryoko Tokuhisa, Takayoshi
Yoshimura, Hiroyuki Hoshino,
and Narimasa
Watanabe. 2010. Active listening system for dia-
logue robot. In JSAI SIG-SLUD Technical Report,
volume 58, pages 61–66. (in Japanese).

Walker, Marilyn, Alex Rudnicky, John Aberdeen, Eliz-
abeth Owen Bratt, Rashmi Prasad, Salim Roukos,
Greg S, and Seneff Dave Stallard. 2002. DARPA
communicator evaluation: progress from 2000 to
2001. In Proc. International Conference on Spoken
Language Processing (ICSLP), pages 273–276.

Wallace, Richard S. 2004. The Anatomy of A.L.I.C.E.

A.L.I.C.E. Artiﬁcial Intelligence Foundation, Inc.

Williams, Jason D. and Steve Young. 2007. Par-
tially observable markov decision processes for spo-
ken dialog systems. Computer Speech & Language,
21(2):393–422.

Yokoyama,

Sachie, Daisuke Yamamoto, Yuka
Kobayashi, and Miwako Doi. 2010. Development
of dialogue interface for elderly people –switching
the topic presenting mode and the attentive listening
In IPSJ SIG Technical
mode to keep chatting–.
Report, volume 2010-SLP-80, pages 1–6.
(in
Japanese).

