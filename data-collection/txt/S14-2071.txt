



















































LyS: Porting a Twitter Sentiment Analysis Approach from Spanish to English


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 411–415,
Dublin, Ireland, August 23-24, 2014.

LyS: Porting a Twitter Sentiment Analysis Approach
from Spanish to English

David Vilares, Miguel Hermo, Miguel A. Alonso, Carlos Gómez-Rodrı́guez, Yerai Doval

Grupo LyS, Departamento de Computación, Facultade de Informática

Universidade da Coruña, Campus de A Coruña

15071 A Coruña, Spain

{david.vilares, miguel.hermo, miguel.alonso, carlos.gomez, yerai.doval}@udc.es

Abstract

This paper proposes an approach to solve

message- and phrase-level polarity classi-

fication in Twitter, derived from an exist-

ing system designed for Spanish. As a

first step, an ad-hoc preprocessing is per-

formed. We then identify lexical, psycho-

logical and semantic features in order to

capture different dimensions of the human

language which are helpful to detect sen-

timent. These features are used to feed a

supervised classifier after applying an in-

formation gain filter, to discriminate irrel-

evant features. The system is evaluated on

the SemEval 2014 task 9: Sentiment Anal-

ysis in Twitter. Our approach worked com-

petitively both in message- and phrase-

level tasks. The results confirm the robust-

ness of the approach, which performed

well on different domains involving short

informal texts.

1 Introduction

Millions of opinions, conversations or just trivia

are published each day in Twitter by users of dif-

ferent cultures, countries and ages. This provides

an effective way to poll how people praise, com-

plain or discuss about virtually any topic. Compre-

hending and analysing all this information has be-

come a new challenge for organisations and com-

panies, which aim to find out a way to make quick

and more effective decisions for their business. In

particular, identifying the perception of the public

with respect to an event, a service or an entity are

some of their main goals in a short term. In this

respect, sentiment analysis, and more specifically

polarity classification, is playing an important role

This work is licensed under a Creative Commons Attribu-
tion 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

in order to automatically analyse subjective infor-

mation in texts.

This paper describes our participation at Sem-

Eval 2014 task 9: Sentiment Analysis in Twit-

ter. Specifically, two subtasks were presented:

(A) contextual polarity disambiguation and (B)

message polarity classification. The first sub-

task consists on determining the polarity of words

or phrases extracted from short informal texts,

the scope of extracts being provided by the Se-

mEval organisation. Subtask B focusses on clas-

sifying the content of the whole message. In

both cases, three possible sentiments are consid-

ered: positive, negative and neutral (which in-

volves mixed and non-opinionated instances). Al-

though the training set only contains tweets, the

test set also includes short informal texts from

other domains, in order to measure cross-domain

portability. You can test the model for subtask B

at miopia.grupolys.org.

2 SemEval 2014-Task 9: Sentiment

Analysis in Twitter

Our contribution is a reduced version of a Span-

ish sentiment classification system (Vilares et al.,

2013a; Vilares et al., 2013b) that participated in

TASS 2013 (Villena-Román et al., 2014), achiev-

ing the 5th place on the global sentiment classifi-

cation task and the 1st place on topic classification

on tweets. In this section we describe how we have

ported to English this system originally designed

for Spanish. Tasks A and B are addressed from

the same perspective, which is described below.

2.1 Preprocessing

We implement a naive preprocessing algorithm

which seeks to normalise some of the most com-

mon ungrammatical elements. It is intended for

Twitter, but many of the issues addressed would

also be valid in other domains:

411



• Replacement of frequent abbreviations The
list of the most frequent ones was extracted

from the training set, taking the Penn Tree-

bank (Marcus et al., 1993) as our dictionary.

A term is considered ungrammatical if it does

not appear in our dictionary. We then carry

out a manual review to distinguish between

unknown words and abbreviations, providing

a correction in the latter case. For example,

‘c’mon’ becomes ‘come on’ and ‘Sat’ is re-

placed by ‘Saturday’.

• Emoticon normalisation: We employ the
emoticon collection published in (Agarwal et

al., 2011). Each emoticon is replaced with

one of these five labels: strong positive (ESP),

positive (EP), neutral (ENEU), negative (EN)

or strong negative (ESN).

• Laughs : Multiple forms used in social media
to reflect laughs (e.g. ‘hhahahha’, ‘HHEHE-

HEH’) are preprocessed in a homogeneous

way to obtain a pattern of the form ‘hxhx’

where x ∈ {a, e, i, o, u}.
• URL normalisation: External links are re-
placed by the string ‘url’.

• Hashtags (‘#’) and usernames (‘@’): If the
hashtag appears at the end or beginning of

the tweet, we remove the hashtag. Based

on other participant approaches at SemEval

2013 (Nakov et al., 2013), we realized maybe

this is not the best option, although we be-

lieve hashtags will not be useful in most of

cases, since they refer to very specific events.

Otherwise, only the ‘#’ is removed, hypothe-

sising the hashtag is used to emphasise a term

(e.g. ‘Matthew #Mcconaughey has won the

Oscar’).

2.2 Feature Extraction

Our approach only takes into account information

extracted from the text, without considering any

kind of meta-data. Extracted features combine

lexical, psychological and semantic knowledge in

order to build a linguistic model able to analyse

tweets, but also other kinds of messages. These

features can be divided into two types: corpus-

extracted features and lexicon-extracted features.

All of them take the total number of occurrences

of the respective feature as the weighting factor to

then feed the supervised classifier.

2.2.1 Corpus-extracted features

Given a corpus, we use it to extract the following

set of features:

• Word forms: A model based on this type of
features is our baseline. Each single word is

considered as a feature in order to feed the

supervised classifier. This often becomes a

simple and acceptable start point which ob-

tains a decent performance.

• Part-of-speech (PoS) information: some
coarse-grained PoS-tags such as adjective or

adverb are usually good indicators of subjec-

tive texts while some fine-grained PoS tags

such as third person personal pronoun pro-

vide evidence of non-opinionated messages

(Pak and Paroubek, 2010).

2.2.2 Lexicon-extracted features

We also consider information obtained from exter-

nal lexicons in order to capture linguistic informa-

tion that can not be extracted from a training cor-

pus by means of bag-of-words and PoS-tag mod-

els. We rely on two manually-build lexicons:

• Pennebaker et al. (2001) psychometric dictio-
naries. Linguistic Inquiry and Word Count1

(LIWC) is a software which includes a seman-

tic dictionary to measure how people use dif-

ferent kinds of words over a wide number of

texts. It categorises terms into psychometric

properties, which correspond to different di-

mensions of the human language. The dictio-

nary relates terms with psychological prop-

erties (e.g. anger or anxiety), but also with

topics (e.g. family, friends, religion) or even

morphological features (e.g. future time, past

time or exclamations).

• Hu and Liu (2004) opinion lexicon. It is a col-
lection of positive and negative words. Many

of the occurrences are misspelled, since they

often come from web environments.

2.2.3 Syntactic features

We also parsed the tweets using MaltParser (Nivre

et al., 2007) in order to obtain dependency triplets

of the form (wi, arcij, wj), where wi is the head
word wj , the dependent one and arcij the exist-
ing syntactic relation between them. We tried to

incorporate generalised dependency triplets (Joshi

1http://www.liwc.net/

412



and Penstein-Rosé, 2009), following an enriched

perspective presented in Vilares et al. (2014). A

generalisation consists on backing off the words

to more abstracted terms. For example, a valid de-

pendency triplet for the phrase ‘awesome villain’

is (villain, modifier, awesome), which could be

generalised into (anger, modifier, assent) by means

of psychometric properties. However, experimen-

tal results over the development corpus using these

features decreased performance with respect to

our best model, probably due to the small size of

the training corpus, since dependency triplets tend

to suffer from sparsity, so a larger training corpus

is needed to exploit them in a proper way (Vilares

et al., 2014).

2.3 Feature Selection

For a machine learning approach, sparsity could

be an issue. In particular, due to the size of the cor-

pus, many of the terms extracted from the training

set only appear a few times in it. This makes it

impossible to properly learn the polarity of many

tokens. Thus, we carry out a filtering step before

feeding our classifier. In particular, we rely on

the information gain (IG) method to then rank the

most relevant features. Information gain measures

the relevance of an attribute with respect to a class.

It takes values between 0 and 1, where a higher

value implies a higher relevance. Table 1 shows

the top five relevant features based on their infor-

mation gain for our best model. The top features

for task A were very similar. Our official runs only

consider features with an IG greater than zero.

IG Feature Category

0.140 positive emotion Pennebaker et al. (2001)
0.137 #positive-words Hu and Liu (2004)
0.126 affect Pennebaker et al. (2001)
0.089 #negative-words Hu and Liu (2004)
0.083 negative emotion Pennebaker et al. (2001)

Table 1: Most relevant features for task B. ‘#’ must

be read this table as ‘the number of’and not as a

hashtag.

2.4 Classifier

We have trained our runs with a SVMLibLINEAR

classifier (Fan et al., 2008) taking the implementa-

tion provided in WEKA (Hall et al., 2009). The

selection was motivated by the acceptable results

that some of the participants in SemEval 2013, e.g.

Becker et al. (2013), obtained using this imple-

mentation. We configured the multi-class support

vector machine by Crammer and Singer (2002) as

the SVMtype. Since the corpus was unbalanced,

we tuned the weights for the classes using the de-

velopment corpus: 1 for the positive class, 2 for

negative and 0.5 for neutral. The rest of parame-

ters were set to default values.

3 Experimental Results

The SemEval 2014 organisation provides a stan-

dard training corpus for both tasks A an B. For task

A, each tweet is marked with a list of the words

and phrases to analyse, and for each one its senti-

ment label is provided. In addition, a development

corpus was released for tuning the system parame-

ters. The training and the development corpus can

be used jointly (constrained runs) to train mod-

els that are then evaluated over the test corpus.2

Some participants used external annotated corpora

(unconstrained runs) to build their models. With

respect to the test corpus, it contains texts from

tweets but also from LiveJournal texts, which we

are abbreviating as LJ, and SMS messages.

Table 2 contains the statistics of the corpora we

used. Sharing data is a violation of Twitter’s terms

of service, so we had to download them. Unfortu-

nately, some of the tweets were no longer available

for several reasons, e.g., user or a tweet does not

exist anymore or the privacy settings of a user have

changed. As a result, the size of our training and

development corpora may be different from those

of other participant’s corpora.

Task Set Positive Negative Neutral

Train 4,917 2,591 385
A Dev 555 365 45

Test 6,354 3,771 556

Train 3,063 1,202 3,935
B Dev 493 290 633

Test 3,506 1,541 3,940

Table 2: SemEval 2014 corpus statistics.

3.1 Evaluation Metrics

F-measure is the official score to measure how sys-

tems behave on each class. In order to rank partic-

ipants, the SemEval 2014 organisation proposed

the averaged F-measure of positive and negative

tweets.

2We followed this angle.

413



3.2 Performance on Sets

Tables 3 and 4 show performance on the test set

of different combinations of the proposed features.

Table 5 shows the performance of our run on task

A. The results over the corresponding sets for task

B are illustrated in Table 6. They are significant

lower than in task A. This suggests that when a

message involves more than one of two tokens, a

lexical approach is not enough. Improving perfor-

mance should involve taking into account context

and linguistic phenomena that appear in sentences

to build a model based on the composition of lin-

guistic information.

Model LJ SMS
Twitter Twitter Twitter
2013 2014 Sarcasm

WPLT
82.21 82.32 84.82 81.69 71.19

(no IG)
WPL 83.55 81.04 84.85 80.64 68.79
WPLT* 83.96 81.46 85.63 79.93 71.98
WP 78.53 80.97 80.34 73.35 74.18
P 75.70 78.74 73.58 65.75 71.82
W 61.58 65.45 64.56 59.16 62.93
L 66.04 64.11 62.96 53.81 61.26
T 47.07 51.37 71.82 43.64 49.37

Table 3: Performance on the test set for task A.

The model marked with a *was our official run. W

stands for features obtained from a bag-of-words

approach, L from Hu and Liu (2004), P from Pen-

nebaker et al. (2001) and T for fine-grained PoS-

tags. They can be combined, e.g., a model named

WP use both words and psychometric properties.

Model LJ SMS
Twitter Twitter Twitter
2013 2014 Sarcasm

WPLT* 69.79 60.45 66.92 64.92 42.40
WPL 70.19 61.41 66.71 64.51 45.72
WP 66.84 60.22 65.29 63.90 45.90
WPLT

66.38 57.01 61.96 62.84 43.71
(no IG)
W 65.12 56.00 62.87 62.64 48.75
P 63.42 54.80 60.05 57.66 54.20
T 45.99 35.85 46.53 45.99 48.58
L 57.53 45.14 48.80 44.48 49.14

Table 4: Performance on the test set for task B.

4 Conclusions

This papers describes the participation of the LyS

Research Group (http://www.grupolys.

org) at the SemEval 2014 task 9: Sentiment Anal-

ysis in Twitter, with a system that attained com-

petitive performance both in message and phrase-

Test set Positive Negative Neutral

DEV 86.30 81.60 4.30
TWITTER 2013

88.70 81.90 17.60
(full)

TWITTER 2013
88.81 82.57 20.75

(progress subset)
LJ 84.34 83.56 13.84
SMS 80.31 82.56 7.10
TWITTER 2014 89.02 70.82 4.44
TWITTER SARCASM 85.71 57.63 28.57

Table 5: Performance on different sets for our

model on task A. The model evaluated on the de-

velopment set was only built using the training set.

Test set Positive Negative Neutral

DEV 69.80 60.40 66.70
TWITTER 2013

72.50 64.30 72.30
(full)

TWITTER 2013
71.92 61.92 71.22

(progress subset)
LJ 71.94 67.65 66.23
SMS 63.83 57.06 73.76
TWITTER 2014 74.26 55.58 66.76
TWITTER SARCASM 55.17 29.63 51.61

Table 6: Performance on different sets for our

model on task B.

Test set Task A Task B

LiveJournal 2014 4 / 27 13 / 50
SMS 2013 12 / 27 19 / 50
Twitter 2013 9 / 27 10 / 50
Twitter 2014 11 / 27 18 / 50
Twitter 2014 Sarcasm 10 / 27 33 / 50

Table 7: Position of our submission on each cor-

pus and task, according to results provided by the

organization on April 22, 2014.

level tasks, as can be observed in Table 7. This

system is a reduced version of a sentiment classifi-

cation model for Spanish texts that performed well

in the TASS 2013 (Villena et al., 2013). The offi-

cial results show how our approach works com-

petitively both on tasks A and B without needing

large and automatically-built resources. The ap-

proach is based on a bag-of-words that includes

word-forms and PoS-tags. We also extract psy-

chometric and sentiment information from exter-

nal lexicons. In order to reduce sparsity problems,

we firstly apply an information gain filter to select

only the most relevant features. Experiments on

the development set showed a significant improve-

ment on the same model with respect to skipping

it on subtask B.

414



Acknowledgements

Research reported in this paper has been partially

funded by Ministerio de Economı́a y Competitivi-

dad and FEDER (Grant TIN2010-18552-C03-02)

and by Xunta de Galicia (Grant CN2012/008).

References

Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of Twitter data. In Proceedings of the Work-
shop on Languages in SocialMedia, LSM ’11, pages
30–38, Stroudsburg, PA, USA. ACL.

Lee Becker, George Erhart, David Skiba, and Valen-
tine Matula. 2013. AVAYA: Sentiment Analysis on
Twitter with Self-Training and Polarity Lexicon Ex-
pansion. Atlanta, Georgia, USA, page 333.

Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265–292.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. The Journal
of Machine Learning Research, 9:1871–1874.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10–18, November.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Manesh Joshi and Carolyn Penstein-Rosé. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, ACLShort ’09, pages 313–316,
Suntec, Singapore.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.

Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. pages 312–320, June.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.

A. Pak and P. Paroubek. 2010. Twitter as a Corpus for
Sentiment Analysis and Opinion Mining. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC’10),
pages 1320–1326, Valletta, Malta, May. European
Language Resources Association (ELRA).

J.W. Pennebaker, M.E. Francis, and R.J. Booth. 2001.
Linguistic inquiry and word count: LIWC 2001.
Mahway: Lawrence Erlbaum Associates, 71.

David Vilares, Miguel A. Alonso, and Carlos Gómez-
Rodrı́guez. 2013a. LyS at TASS 2013: Analysing
Spanish tweets by means of dependency pars-
ing, semantic-oriented lexicons and psychometric
word-properties. In Alberto Dı́az Esteban, Iñaki
Alegrı́a Loinaz, and Julio Villena Román, editors,
XXIX Congreso de la Sociedad Española de Proce-
samiento de Lenguaje Natural (SEPLN 2013). TASS
2013 - Workshop on Sentiment Analysis at SEPLN
2013, pages 179–186, Madrid, Spain, September.

David Vilares, Miguel A. Alonso, and Carlos Gómez-
Rodrı́guez. 2013b. Supervised polarity classifica-
tion of Spanish tweets based on linguistic knowl-
edge. In DocEng’13. Proceedings of the 13th ACM
Symposium on Document Engineering, pages 169–
172, Florence, Italy, September. ACM.

David Vilares, Miguel A. Alonso, and Carlos Gómez-
Rodrı́guez. 2014. On the usefulness of lexical
and syntactic processing in polarity classification of
Twitter messages. Journal of the Association for In-
formation Science Science and Technology, to ap-
pear.

Julio Villena-Román, Janine Garcı́a-Morera, Cristina
Moreno-Garcı́a, Sara Lana-Serrano, and José Carlos
González-Cristóbal. 2014. TASS 2013 — a sec-
ond step in reputation analysis in Spanish. Proce-
samiento del Lenguaje Natural, 52:37–44, March.

415


