



















































Discourse Planning with an N-gram Model of Relations


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1973–1977,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Discourse Planning with an N-gram Model of Relations

Or Biran
Columbia University

orb@cs.columbia.edu

Kathleen McKeown
Columbia University

kathy@cs.columbia.edu

Abstract

While it has been established that transi-
tions between discourse relations are im-
portant for coherence, such information
has not so far been used to aid in lan-
guage generation. We introduce an ap-
proach to discourse planning for concept-
to-text generation systems which simul-
taneously determines the order of mes-
sages and the discourse relations between
them. This approach makes it straightfor-
ward to use statistical transition models,
such as n-gram models of discourse re-
lations learned from an annotated corpus.
We show that using such a model signif-
icantly improves the quality of the gener-
ated text as judged by humans.

1 Introduction

Discourse planning is a subtask of Natural Lan-
guage Generation (NLG), concerned with deter-
mining the ordering of messages in a document
and the discourse relations that hold among them
(Reiter and Dale, 2000). Early approaches to dis-
course planning used manually written rules, often
based on schemas (McKeown, 1985) or on Rhetor-
ical Structure Theory (RST) (Mann and Thomp-
son, 1987; Hovy, 1993; Power, 2000). In the
past decade, various statistical approaches have
emerged (Duboue and McKeown, 2001; Dimitro-
manolaki and Androutsopoulos, 2003; Soricut and
Marcu, 2006; Konstas and Lapata, 2013). Other
relevant statistical approaches to content ordering
can also be found in the summarization literature
(Barzilay et al., 2001; Lapata, 2003; Bollegala
et al., 2005). These approaches overwhelmingly
focus on determining the best order of messages
using semantic conent, while discourse relations
are in most cases either determined by manually-
written derivation rules or completely ignored.

Meanwhile, researchers working on discourse
relation disambiguation have observed that the se-
quence of discourse relations itself, independently
of content, helps in disambiguating adjacent re-
lations (Wellner et al., 2006; Pitler et al., 2008).
Sequential discourse information has been used
successfully in discourse parsing (Ghosh et al.,
2011; Feng and Hirst, 2014), and discourse struc-
ture was shown to be as important for text co-
herence as entity-based content structure (Lin et
al., 2011; Feng et al., 2014). Surprisingly, so
far, discourse sequential information from exist-
ing discourse-annotated corpora, such as the Penn
Discourse Treebank (PDTB) (Prasad et al., 2008)
has not been used in generation.

In this paper, we present an NLG framework
that generates texts from existing semantic web
ontologies. We use an n-gram model of discourse
relations to perform discourse planning for these
stories. Through a crowd-sourced human evalua-
tion, we show that the ordering of our documents
and the choice of discourse relations is signifi-
cantly better when using this model.

2 Generation Framework

In concept-to-text generation pipelines, discourse
planning typically occurs after the content selec-
tion stage. The input, therefore, is an unordered
set of messages that are not yet realized: instead
of being represented as text, the messages have a
structured semantic representation.

In this paper, we generate comparison stories,
describing and comparing two similar entities,
from an RDF ontology. The RDF semantic rep-
resentation is commonly used in semantic web re-
sources and free ontologies. An RDF message
(called a triple) has three parts: a subject, a pred-
icate and an object. For each story, we consider
any triple whose subject is one of the participating
entities as a potential message to be generated. We
do only minimal processing on these messages:

1973



where two triples have the same subject and pred-
icate but different objects, we merge them into a
single message with multiple objects; and where
two triples have the same subject and object but
different predicates, we merge them into a single
message with multiple predicates.

Next, we build the set of potential discourse re-
lations between all messages. We use the PDTB
class-level relations, of which there are four: ex-
pansion, comparison, contingency and temporal.
Each is an abstraction of a family of more specific
relations, such as cause, concession, etc. We do
not differentiate between explicit and implicit re-
lations, and treat entrel as a type of expansion.

Potential discourse relations are implied in the
semantics of the triples: messages that contain the
same predicate and object may have an expansion
relation among them (e.g. “John has a ball. Mary
also has a ball”). Messages that contain the same
predicate but different subjects and objects may
have a comparison relation (e.g. “John likes ap-
ples but Mary likes oranges”).

Specific predicate pairs will also have specific
potential relations among them - for example,
“birth place” and “residence” have a temporal re-
lation (when applied to the same subject). The
same is true for contingency relations (e.g., “city”
and “country” for the same subject - if the sub-
ject is in a city, it implies which country it is in).
We manually annotated the 59 predicate pairs that
had potential temporal and contingency relations,
as well as 8 pairs with special potential compari-
son relations (e.g., “birth place” and “residence” if
the subject is the same but the object is not).

Once the potential relations are identified, we
have a directed multigraph where each vertex is
a message and each edge is a potential relation.
There can be multiple edges between any two ver-
tices, since messages may have more than one po-
tential relation among them.

Once the graph is ready, we perform content se-
lection. Given a desired number of messages to
generate, we choose the set of messages that max-
imizes the number of edges in the resulting sub-
graph (thus ensuring that the selected messages
are discourse-coherent). If there are multiple such
sets, we choose one at random.

The task we are focused on in this paper is dis-
course planning, which in our formulation is the
task of finding a Hamiltonian path through the se-
lected subgraph, thus simultaneously selecting the

order of the messages (nodes) as well as the rela-
tions (edges) that connect them. Our approach for
choosing the best path is discussed in the next sec-
tion. For the remainder of this section, we describe
our simple implementations of the next stages of
generation: sentence planning and realization.

For each of the four discourse relations we use,
we selected a few explicit connectives from the
PDTB that are often used to convey them. We
specifically chose connectives that apply to the en-
tire range of class-level relations (e.g., for compar-
ison we chose “while” - since it applies to both
contrast and concession in the PDTB, but not “in
contrast” which applies only to the former). We
also chose only those connectives which have the
structure [ARG1 connective ARG2] or [ARG1.
connective, ARG2]. During realization, we arbi-
trarily choose a connective to realize the relation.

Since the ordering and relations between mes-
sages is determined by the discourse plan, sen-
tence planning falls naturally out of it: sentence
breaks occur where the connective pattern creates
them, or where there is no relation between adja-
cent messages.

To realize the messages themselves, we follow
a single pattern: “the [predicate(s)] of [subject]
(is/are) [object(s)]”. Simple rules are used to plu-
ralize the predicate when there are multiple objects
and to create lists of multiples objects or predi-
cates where needed.

These basic solutions for the various stages of
NLG produce texts that are rich enough to be ac-
ceptable for human readers, but which have rel-
atively little variation in grammatical and lexical
quality. This crucial combination allows us to per-
form a human study to specifically evaluate the
discourse planning component.

3 Discourse Planning

As explained in the previous section, we formu-
late the discourse planning task as finding a path
through a multigraph of potential relations be-
tween messages. One major component of what
makes a good path is the sequence of content:
some content is more central and should appear
earlier, for example; and some predicates and ob-
jects are semantically related and should appear
near one another. In this paper we focus on a com-
ponent that has so far been neglected in generation
- the sequence of discourse relations - while try-
ing to minimize the effect that content semantics

1974



have on the evaluation (other than the semantics
implicit in the relations). In order to quantify the
likelihood of a sequence of relations, we build an
n-gram model from a discourse-annotated corpus.

An n-gram model measures the transitional
probabilities for sequences of the units that the n-
grams are composed of. In this case, the units are
discourse relations. The probability of a particu-
lar sequence of relations of length n + 1 given an
existing subsequence of length n is computed as
a fraction of the number of times it appears in the
corpus and the number of times the subsequence
appears in the corpus, i.e.

P (ri|ri−n, ..., ri−1) = C(ri−n, ..., ri−1, ri)
C(ri−n, ..., ri−1)

Where C(s) is the number of times sequence s ap-
pears in the corpus. Using this model to generate
a discourse plan given a potential relation multi-
graph is a stochastic process: at each stage, we
choose the next relation edge out of the last cho-
sen message vertex (the first vertex is chosen at
random) based on the selected sequence of rela-
tion edges and the probabilities for the next rela-
tion in the model. Once a vertex is added to the
path edges leading to it can no longer be selected.

4 Evaluation

One method for evaluating a discourse plan in-
dependently of content is to produce pairs of
generated short text documents, each containing
the same content, but with different ordering and
discourse relations (as dictated by the discourse
plan). The only obvious way to decide which text
is better is to have human judges make that de-
cision. It is important to minimize the effects of
other qualities of the texts (differences in content,
word choice, grammatical style, etc.) as much as
possible, so that the judgement is based only on
the differences in order and discourse.

We used DBPedia (Auer et al., 2007) - an RDF
ontology extracted from Wikipedia - to generate
content. Each document generated was a compar-
ison story of two entities in a single category (i.e.,
the messages in the stories were selected, as de-
scribed in Section 2, from the set of triples where
one of the entities was the subject). In order to
experiment with different domains, we used four
different categories: Office Holder (i.e., a per-
son holding office such as a President or a Judge);
River; Television Show; and Military Unit. The

The birth place of Allen J. Ellender is Montegut,
Louisiana, while the death place of Allen J. El-
lender is Maryland. The birth place of Robert E.
Quinn is Phoenix, Rhode Island. Subsequently, the
death place of Robert E. Quinn is Rhode Island.

The birth place of Allen J. Ellender is Montegut,
Louisiana. In comparison, the birth place of Robert
E. Quinn is Phoenix, Rhode Island. The death
place of Robert E. Quinn is Rhode Island, but the
death place of Allen J. Ellender is Maryland.

Figure 1: Sample pair of comparison stories

entity pairs from each category were chosen at ran-
dom but were required to have at least 8 predicates
and 3 objects in common, so that they were some-
what semantically related.

To ensure that human judges can easily tell
the differences between the stories on a sentential
level, we limited the size of each story to 4 mes-
sages. For each pair of stories, everything but the
discourse plan (i.e. the content selection, the real-
ization of messages and the lexical choice of con-
nectives) was identical. Figure 1 shows an exam-
ple pair of stories from the Office Holder category.

4.1 Experiments

We conducted two crowd sourced experiments on
the CrowdFlower platform. Each question con-
sisted of two short stories that are completely iden-
tical in content, but each generated with a different
discourse planner. The human judge was asked to
decide which of the stories has a better flow (or
whether they are equally good), and then to give
each of the stories a score from 1 to 5, paying spe-
cific attention to the ordering of the prepositions
and the relations between them. The stories were
presented in a random order and were not given
labels, to avoid bias. We generated 125 pairs of
stories from each category - a total of 500 - for
each experiment.

Each question was presented to three judges.
In each experiment, there was complete disagree-
ment among the three annotators in approximately
15% of the questions, and those were discarded.
In approximately 20% there was complete agree-
ment, and in the rest of the questions there were
two judges who agreed and one who disagreed.
We also computed inter-annotator agreement us-
ing Cohen’s Kappa for 217 pairs of judges who

1975



Quality comparison Avg. score
Base Equal Pdtb Base Pdtb

Of. Holder 27.4% 30.2% 42.5% 3.67 3.76
TV Show 34.3% 25.7% 40% 3.79 3.8
Mil. Unit 32.3% 23.2% 44.4% 3.69 3.84
River 39.2% 23.5% 37.3% 3.71 3.72
Total 34% 25% 41% 3.72 3.78

Table 1: Results for the comparison between the
PDTB n-gram model and the baseline

Quality comparison Avg. score
Pdtb Equal Wiki Pdtb Wiki

Of. Holder 33.6% 14.5% 51.8% 3.51 3.65
TV Show 43.2% 8.1% 48.6% 3.62 3.65
Mil. Unit 40.4% 14.4% 45.2% 3.65 3.67
River 41.1% 11.2% 47.7% 3.68 3.7
Total 39.6% 12% 48.4% 3.61 3.67

Table 2: Results for the comparison between the
Wikipedia model and the PDTB model

both answered at least 10 of the same questions.
The average kappa value was 0.5, suggesting rea-
sonable agreement.

In the first experiment, we compared stories
generated by a planner using an n-gram model ex-
tracted from the PDTB with stories generated by
a baseline planner, where all edges have identical
probabilities. The results are shown in Table 1.

In the second experiment, we used a PDTB
shallow discourse parser we developed (Biran and
McKeown, 2015) to create a discourse-annotated
version of the English Wikipedia. We then com-
pared stories generated by a planner using an n-
gram model extracted from the parsed Wikipedia
corpus with those generated by a planner using the
PDTB model. The results are shown in Table 2.

The total results in both tables are statistically
significant (p < 0.05).

4.2 Discussion

The results in Table 1 show that the judges signif-
icantly preferred the stories created by the n-gram
model-based planner to those created by the base-
line planner, both in terms of the three-way deci-
sion and in terms of the numeric score. This is true
for the total set as well as every specific topic, ex-
cept for River. This may be because the predicates
in the River category are much more cohesive than
in other categories: virtually all predicates related
to rivers describe an aspect of the location of the
river. That fact may make it easier for a random
planner to produce a story that seems coherent.
Note, however, that while the judges preferred the
baseline story more often in the River questions,

the average score is higher for the model, which
suggests that when the baseline was better it was
only mildly so, while when the model was better
is was significantly so.

The results in Table 2 show that the Wikipedia-
based model produces better results than the
PDTB-based model. We hypothesize that it is for
two reasons. First, Wikipedia contains definitional
texts and is closer in style and content to the stories
we produce than the PDTB, which contains WSJ
articles. Temporal relations constitute about 10%
of both corpora, but contingency and comparison
relations each make up almost 20% of the PDTB,
while in Wikipedia they span only 10% and 12%
of the corpus, respectively, making the share of ex-
pansion relations much larger. Second, since the
PDTB is small, higher-order n-grams are sparsely
found, which can add noise to the model. The
Wikipedia corpus is significantly larger and does
not suffer from this problem.

The differences in average scores seen in the ex-
periments are relatively small. That is expected,
since we have eliminated the content coherence
factor, which is known to be significant. In addi-
tion, while judges were specifically asked to fo-
cus on the order of messages and relations be-
tween them, there is inevitably some noise due to
accidental lexical or syntactic mismatches, order-
ing that is awkward content-wise, and other side-
effects of the generation framework we employed.

5 Conclusion and Future Work

We introduced an approach to discourse planning
that relies on a potential discourse multigraph, al-
lowing for an n-gram model of relations to drive
the discourse plan and efficiently determine both
the ordering and the relations between messages.

We conducted two experiments, comparing sto-
ries generated with different discourse planners.
The first shows that an n-gram model-based plan-
ner significantly outperforms the random baseline.
The second suggests that using an n-gram model
derived from a corpus that is larger and closer
in style and content, though less accurately anno-
tated, can further improve results.

In future work, we intend to combine this
discourse-based view of coherence with a content-
based view to create a unified statistical discourse
planner. In addition, we will explore additional
stochastic models of discourse that look at other,
non-sequential collocational information.

1976



References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens

Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: a nucleus for a web of open data.
In Proceedings of the 6th international The seman-
tic web and 2nd Asian conference on Asian semantic
web conference, ISWC’07/ASWC’07, pages 722–
735, Berlin, Heidelberg. Springer-Verlag.

Regina Barzilay, Noemie Elhadad, and Kathleen R.
McKeown. 2001. Sentence ordering in multidocu-
ment summarization. In Proceedings of the First In-
ternational Conference on Human Language Tech-
nology Research, HLT ’01, pages 1–7, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Or Biran and Kathleen McKeown. 2015. Pdtb dis-
course parsing as a tagging task: The two taggers
approach. In Proceedings of the 16th Annual SIG-
dial Meeting on Discourse and Dialogue, SIGDIAL
2015, Prague, Czech Republic.

Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2005. A machine learning approach
to sentence ordering for multidocument summa-
rization and its evaluation. In Natural Lan-
guage Processing–IJCNLP 2005, pages 624–635.
Springer.

Aggeliki Dimitromanolaki and Ion Androutsopoulos.
2003. Learning to order facts for discourse plan-
ning in natural language generation. arXiv preprint
cs/0306062.

Pablo A. Duboue and Kathleen R. McKeown. 2001.
Empirically estimating order constraints for content
planning in generation. In Proceedings of 39th An-
nual Meeting of the Association for Computational
Linguistics, pages 172–179, Toulouse, France, July.
Association for Computational Linguistics.

Vanessa Wei Feng and Graeme Hirst. 2014. A Linear-
Time Bottom-Up Discourse Parser with Constraints
and Post-Editing. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, pages 511–521, Baltimore, Maryland,
June. Association for Computational Linguistics.

Vanessa Wei Feng, Ziheng Lin, Graeme Hirst, and Sin-
gapore Press Holdings. 2014. The impact of deep
hierarchical discourse structures in the evaluation of
text coherence. In Proceedings of the 25th Interna-
tional Conference on Computational Linguistics.

Sucheta Ghosh, Richard Johansson, Giuseppe Ric-
cardi, and Sara Tonelli. 2011. Shallow discourse
parsing with conditional random fields. In Proceed-
ings of 5th International Joint Conference on Natu-
ral Language Processing, pages 1071–1079.

Eduard H. Hovy. 1993. Automated discourse genera-
tion using discourse structure relations. Artif. Intell.,
63(1-2):341–385, October.

Ioannis Konstas and Mirella Lapata. 2013. Induc-
ing document plans for concept-to-text generation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1503–1514, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.

Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, ACL ’03,
pages 545–552, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 997–1006. Association for Computational
Linguistics.

William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.

Kathleen R. McKeown. 1985. Discourse strategies
for generating natural-language text. Artif. Intell.,
27(1):1–41, September.

Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind K Joshi. 2008.
Easily identifiable discourse relations.

Richard Power. 2000. Planning texts by constraint
satisfaction. In Proceedings of the 18th Conference
on Computational Linguistics - Volume 2, COLING
’00, pages 642–648, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of LREC.

Ehud Reiter and Robert Dale. 2000. Building natu-
ral language generation systems, volume 33. Cam-
bridge university press.

Radu Soricut and Daniel Marcu. 2006. Discourse
generation using utility-trained coherence models.
In Proceedings of the COLING/ACL on Main Con-
ference Poster Sessions, COLING-ACL ’06, pages
803–810, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Ben Wellner, James Pustejovsky, Catherine Havasi,
Anna Rumshisky, and Roser Saurı́. 2006. Clas-
sification of discourse coherence relations: An ex-
ploratory study using multiple knowledge sources.
In Proceedings of the 7th SIGdial Workshop on Dis-
course and Dialogue, SigDIAL ’06, pages 117–125,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

1977


