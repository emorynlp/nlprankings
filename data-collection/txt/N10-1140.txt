










































Accurate Non-Hierarchical Phrase-Based Translation


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Accurate Non-Hierarchical Phrase-Based Translation

Michel Galley and Christopher D. Manning

Computer Science Department

Stanford University

Stanford, CA 94305

{mgalley,manning}@cs.stanford.edu

Abstract

A principal weakness of conventional (i.e.,

non-hierarchical) phrase-based statistical machine

translation is that it can only exploit continuous

phrases. In this paper, we extend phrase-based

decoding to allow both source and target phrasal

discontinuities, which provide better generalization

on unseen data and yield significant improvements

to a standard phrase-based system (Moses).

More interestingly, our discontinuous phrase-

based system also outperforms a state-of-the-art

hierarchical system (Joshua) by a very significant

margin (+1.03 BLEU on average on five Chinese-

English NIST test sets), even though both Joshua

and our system support discontinuous phrases.

Since the key difference between these two systems

is that ours is not hierarchical—i.e., our system

uses a string-based decoder instead of CKY, and it

imposes no hard hierarchical reordering constraints

during training and decoding—this paper sets

out to challenge the commonly held belief that

the tree-based parameterization of systems such

as Hiero and Joshua is crucial to their good

performance against Moses.

1 Introduction

Phrase-based machine translation models (Och and

Ney, 2004) advanced the state of the art by extend-

ing the basic translation unit from words to phrases.

By conditioning translations on more than a sin-

gle word, a statistical machine translation (SMT)

system benefits from the larger context of a phrase

pair to properly handle multi-word units and lo-

cal reorderings. Experimentally, it was found that

longer phrases yield better MT output (Koehn et al.,

2003). However, while it is computationally feasi-

ble at training time to extract phrase pairs of nearly

unbounded size (Zhang and Vogel, 2005; Callison-

Burch et al., 2005), phrase pairs applicable at test

time tend to be fairly short. Indeed, data sparsity

often forces conventional phrase-based systems to

segment test sentences into small phrases, and there-

fore to translate dependent words (e.g., the French

ne . . . pas) separately instead of jointly.

We present a solution to this sparsity problem by

going beyond using only continuous phrases, and

instead define our translation unit as any subset of

words of a sentence, i.e., a discontinuous phrase.

We generalize conventional multi-beam string-based

decoding (Koehn, 2004) to allow variable-size dis-

continuities in both source and target phrases. Since

each sentence pair can be more flexibly decomposed

into translation units, it is possible to exploit the rich

context of longer (possibly discontinuous) phrases

to improve translation quality. Our decoder provides

two extensions to Moses (Koehn et al., 2007): (a) to

cope with source gaps, we follow (Lopez, 2007) to

efficiently find all discontinuous phrases in the train-

ing data that also appear in the input sentence; (b) to

enable target discontinuities, we augment transla-

tion hypotheses to not only record the current par-

tial translation, but also a set of subphrases that may

be appended to the partial translation at some later

stages of decoding. With these enhancements, our

best discontinuous system outperforms Moses with

lexicalized reordering by 0.77 BLEU and 1.53 TER

points on average.

We also show that our approach compares favor-

ably to binary synchronous context-free grammar

(2-SCFG) systems such as Hiero (Chiang, 2007),

even though 2-SCFG systems also allow phrasal dis-

continuities. Part of this difference may be due to a

difference of expressiveness, since 2-SCFG models

impose hard hierarchical constraints that our mod-

els do not impose. Recent work (Wellington et

al., 2006; Søgaard and Kuhn, 2009; Søgaard and

966



ai

ak am

bj

bl bn

ai akbj

bl bnam

source: ai ckbj dl

bm apdn cttarget:

(iii)(ii)(i)

Figure 1: 2-SCFG systems such as Hiero are unable to in-

dependently generate translation units a, b, c, and d with

the following types of alignments: (i) inside-out (Wu,

1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009);

(iii) “bonbon” (Simard et al., 2005). Standard phrase-

based decoders cope with (i), but not (ii) and (iii). Our

phrase-based decoder handles all three cases.

Wu, 2009) has questioned the empirical adequacy of

2-SCFG systems, which are unable to perform any

of the transformations shown in Fig. 1. For instance,

using manually-aligned bitexts for 12 European lan-

guages pairs, Søgaard and Kuhn found that inside-

out and cross-serial discontinuous translation units

(DTU) account for 1.6% (Danish-English) to 18.6%

(French-English) of all translation units. The em-

pirical adequacy of 2-SCFG models would presum-

ably be lower with automatically-aligned texts and if

the study also included non-European languages. In

contrast, phrase-based systems can properly handle

inside-out alignments when used with a reasonably

large distortion limit, and all configurations in Fig. 1

are accounted for in our system. In our experiments,

we show that our discontinuous phrase-based sys-

tem outperforms Joshua (Li et al., 2009), a reimple-

mentation of Hiero, by 1.03 BLEU points and 1.19

TER points on average. A final compelling advan-

tage of our decoder is that it preserves the compu-

tational efficiency of Moses (i.e., time complexity is

linear when a distortion limit is used), while SCFG

decoders have a running time that is at least cubic

(Huang et al., 2005).

2 Discontinuous Phrase Extraction

In this section, we introduce the extraction of dis-

continuous phrases for phrase-based MT. We will

describe a decoder that can handle such phrases

in the next section. Formally, we define the dis-

continuous phrase-based translation problem as fol-

lows. We are given a source sentence f = fJ1 =
f1, . . . , fj, . . . , fJ , which is to be translated into a

target sentence e = eI1 = e1, . . . , ei, . . . , fI . Un-
like (Och and Ney, 2004), in this work, a sentence

pair may be segmented into phrases that are not con-

Hiero:

This work:

ne veux plus X  

je ne veux plus X 

do not want X anymore

I do not want X anymore

veux

ne ... plus

je ne ... plus

ne veux plus

je ne veux plus

veux ... jouer

do ... want

not ... anymore

I ... not ... anymore

do not want ... anymore

I do not want ... anymore

do ... want to play

je

ne

veux

plus

jouer

I

do

not

want

to

play

anymore

Figure 2: Due to hierarchical constraints, Hiero only ex-

tracts two discontinuous phrases from the alignment on

the left, but our system extracts 11 (only 6 are shown).

tinuous, so each phrase is characterized by a cover-

age set, i.e., a set of word indices. Assuming that

the sentence pair (f , e) is decomposed into K dis-
continuous phrases, we use s = (s1, . . . , sK) and
t = (t1, . . . , tK) to respectively represent the de-
composition of the source and target sentence into

K word subsets that are complementary and non-

overlapping. A pair of coverage sets (sk, tk) is said
to be consistent with the word alignment A if the

following condition holds:

∀(i, j) ∈ A : i ∈ sk ←→ j ∈ tk (1)

For continuous phrases, finding all phrase pairs

that satisfy this condition can be done in O(nm3)
time (Och and Ney, 2004), where n is the length of

the sentence and m is the maximum phrase length.

The set of discontinuous phrases is exponential in

the maximum span length, so phrase extraction must

be tailored to a specific text (e.g., a given test sen-

tence) for relatively large m values. Lopez (2007)

presents an efficient solution using suffix arrays for

finding all discontinuous phrases of the training data

that are relevant to a given test sentence or test set.

A complete overview of this technique is beyond

the scope of this paper, though we will mention

that it solves a phrase collocation problem by effi-

ciently identifying collocated continuous phrases of

the training data that also happen to be collocated in

the test sentence. While this technique was primar-

ily designed for extracting hierarchical phrases for

Hiero (Chiang, 2007), it can readily be applied to

the problem of finding all discontinuous phrases for

our phrase-based system. Indeed, the suffix-array

technique gives us for each input sentence a list of

relevant source coverage sets. For each such sk, we

can easily enumerate each tk satisfying Eq. 1. The

967



!!"! !!# $% & ' ( )* +, -.!!

he said are to this

one access

make arrangements

he said are ... for this

visit

arrangements ... made

he said

oo-------

score = -1.3

are

for this | made

ooooo--oo

score = -4.8

arrangements

made

oo-----oo

score = -3.2

made

for this

ooooo--oo

score = -6.1

for this

ooooo--oo

score = -7.2

visit

ooooooooo

score = -8.5

source:

translation

options

(subset):

state

expansions:

* *

Figure 3: A particular decoder search path for the input shown at the top. Note that this example contains a cross-serial

DTU (which interleaves arrangements ... made with are ... for this), a structure Hiero can’t handle.

key difference between Hiero-style extraction and

our work is that Eq. 1 is the only constraint.1 Since

our decoder doesn’t impose hierarchical constraints,

we exploit all discontinuous phrase pairs consis-

tent with the word alignment, which often includes

sound translations not captured by Hiero (e.g., ne . . .

plus translating to not . . . anymore in Fig. 2).

3 Decoder

The core engine of our phrase-based system, Phrasal

(Cer et al., 2010), is a multi-stack decoder similar to

Moses (Koehn, 2004), which we extended to sup-

port variable-size gaps in the source and the target.

In Moses, partial translation hypotheses are arranged

into different stacks according to the total number of

input words they cover. At every translation step,

stacks are pruned using partial translation cost and a

lower bound on the estimated future cost. Pruning

is implemented using both threshold and histogram

pruning, and Moses allows for hypothesis recombi-

nation between hypotheses that are indistinguishable

according to the underlying models.

The key difference between Moses and our sys-

tem is that, in order to account for target disconti-

nuities, phrases that contains gaps in the target are

appended to a partial translation hypothesis in mul-

tiple steps. Specifically, each translation hypothesis

in our decoder is not only represented as a transla-

tion prefix and a coverage set as in Moses, but it also

contains a set of isolated phrases (shown in italic in

Fig. 3) that must be added to the translation at some

later time. For instance, the figure shows how the

1In order to keep the number of phrases manageable, we

additionally require that each (maximal) contiguous substring

of sk and tk be connected with at least one word alignment.

Beam search algorithm.

1 create initial hypothesis H∅; add it to S
g
0

2 for j = 0 to J
3 if j > 0 then
4 for n = 1 to N
5 for each Hnew in consolidate(H

c
jn)

6 add Hnew to S
g
j

7 if j < J then

8 for n = 1 to N
9 Hold := H

g
jn

10 u := first uncovered source word of Hold
11 for m = u to u + distortionLimit
12 for each (sk, tk) in translation options(m)
13 if source sk does not overlap Hold then

14 Hnew :=combine(Hold, sk, tk)
15 add Hnew to S

c
j+l, where l = |sk|

16 return arg max(SgJ )

Table 1: Discontinuous phrase-based MT.

phrase pair (\ú, arrangements ... made) is be-
ing added to a partial translation. The prefix (ar-

rangements) is immediately appended to form the

hypothesis (he said arrangements), and the isolated

phrase (made) is stored for later use.

A beam search algorithm for discontinuous

phrase-based MT is shown in Table 1. Pruning is

done implicitly in the table to avoid cluttering the

pseudo-code. The algorithm handles 2J + 1 stacks
S

g
0
, S

g
1
, . . . , S

g
J and S

c
1, . . . , S

c
J , where each stack

may contain up to N hypotheses Hj1, . . . ,HjN .

The main loop of the algorithm alternates two

stages: grow (lines 7–15) and consolidate (lines 3–

6).2 The grow stage is similar to standard phrase-

2The distinction between S
g
i and S

c
i stacks ensures that the

consolidate operation does not read and write hypotheses on the

same stack. While it may seem effective to store hypotheses in

968



based MT: we take a hypothesis H
g
jn from S

g
j and

combine it with a translation option (sk, tk), which
yields a new hypothesis that is added to stack Scj+l
(where l = |sk|). The second stage, consolidate, lets
the decoder select any number of isolated phrases

(not necessarily all, and possibly zero) and append

them in any order at the end of the current trans-

lation.3 Consolidation operations are marked with

stars in the figure (for simplicity, the figure does

not display consolidations that keep hypotheses un-

changed). We limit the number of isolated phrases

to 4, which is generally enough to account for most

transformations seen in the data. Any hypothesis in

the last beam S
g
J is automatically discarded if it con-

tains any isolated phrase.

One last difference with standard decoders is

that we also handle source discontinuities. This

problem is a known instance of MT by pattern

matching (Lopez, 2007), which we already men-

tioned in the previous section. The function transla-

tion options(m) of Table 1 returns the set of options
applicable at position m using this pattern match-

ing algorithm. Since this function is invoked a large

number of times, it is important to precompute its

return values for each m prior to decoding.

4 Features

Our system incorporates the same eight baseline fea-

tures of Moses: two relative-frequency phrase trans-

lation probabilities p(e|f) and p(f |e), two lexically-
weighted phrase translation probabilities (Koehn et

al., 2003) lex(e|f) and lex(f |e), a language model
probability, word penalty, phrase penalty, and linear

distortion, and we optionally add 6 lexicalized re-

ordering features as computed in Moses.

Our computation of linear distortion is different

from the one in Moses, since we need to account

for discontinuous phrases. We found that it is

crucial to penalize discontinuous phrases that have

relatively long gaps. Hence, in our computation of

different stacks depending on the number of isolated phrases,

we have not found various implementations of this idea to work

better than the algorithm described here.
3We let isolated phrases be reordered freely, with only three

constraints: (1) the internal word order must be preserved, i.e., a

phrase may not be split or reordered. (2) isolated phrases drawn

from the same discontinuous phrase must appear in the specified

order (i.e., the phrase A ... B ... C may not yield the translation

A ... C ... B). (3) Empty gaps are forbidden.

Figure 4: Linear distortion computed using both continu-

ous and discontinuous phrase.

linear distortion, we treat continuous subphrases

of each discontinuous phrase as if they were

continuous phrases on their own. Specifically,

let s̄ = (s̄1, . . . , s̄L) be the list of L (maximal)
continuous subphrases of the K source phrases

(L ≥ K) selected for a given translation hypothesis.
Subphrases in s̄ are enumerated according to their

order in the target language, which may be different

from the source-side ordering. We then compute

the linear distortion between pair of successive

elements (s̄i, s̄i+1) as follows:

d(̄s) = s̄first
1

+
L

∑

i=2

∣

∣

∣

s̄lasti−1 + 1− s̄
first
i

∣

∣

∣

where the superscripts first and last respectively

refer to source position of the first and last word

of a given subphrase. Fig. 4 shows an example of

how distortion is computed for phrases (s1, s2, s3),
including the discontinuous phrase s2 split into three

continuous subphrases. In practice, we compute

intra-phrase (shown with thin arrows in the figure)

and inter-phrase linear distortion separately in order

to produce two distinct features, since translation

tends to improves when the intra-phrase cost has a

lower feature weight.

Finally, we add two features that are not present

in Moses. First, we penalize target discontinuities

by including a feature that is the sum of the lengths

of all target gaps. The second feature is the count

of discontinuous phrases that are in configurations

(cross-serial DTU (Søgaard and Kuhn, 2009) and

“bonbon” (Simard et al., 2005)) that can’t be han-

dled by 2-SCFG systems. The advantage of such

features is two-fold. First, similarly to hierarchi-

cal systems, they prevent many distorted reorderings

that are unlikely to correspond to quality transla-

tions. Second, it imposes soft rather than hard con-

straints, which means that the decoder is entirely

free to violate hierarchical constraints when these

violations are supported by other features.

969



5 Experimental Setup

Three systems are evaluated in this paper: Moses

(Koehn et al., 2007), Joshua (Li et al., 2009) – a

reimplementation of Hiero, and our phrase-based

system. We made our best attempts to make our sys-

tem comparable to Moses. That is, when no discon-

tinuous phrases are provided to our system, it gener-

ates an output that is almost identical to Moses (only

about 1% of translations differ on average). In both

systems, we use the default settings of Moses, i.e.,

we set the beam size to 200, the distortion limit to 6,

we limit to 20 the number of target phrases that are

loaded for each source phrase, and we use the same

default eight features of Moses. We use version 1.3

of Joshua with its default settings. Both Moses and

our system are evaluated with and without lexical-

ized reordering (Tillmann, 2004).4 We believe it

to be fair to compare Joshua against phrase-based

systems that exploit lexicalized reordering, since Hi-

ero’s hierarchical rules are also lexically sensitive.5

The language pair for our experiments is Chinese-

to-English. The training data consists of about 28

million English words and 23.3 million Chinese

words drawn from various news parallel corpora dis-

tributed by the Linguistic Data Consortium (LDC).

In order to provide experiments comparable to previ-

ous work, we used the same corpora as (Wang et al.,

2007). We performed word alignment using a cross-

EM word aligner (Liang et al., 2006). For this, we

ran two iterations of IBM Model 1 and two HMM

iterations. Finally, we generated a symmetric word

alignment from cross-EM Viterbi alignment using

the Moses grow-diag heuristic in the case Moses and

our system. In the case of Joshua, we used the grow-

diag-final heuristic since this gave better results.

In order to train a competitive baseline given our

computational resources, we built a large 5-gram

language model using the Xinhua and AFP sections

4We use Moses’ default orientations: monotone, swap, and

discontinuous. As far as this reordering model is concerned,

we treat discontinuous phrases as continuous, i.e., we simply

ignore what lies within gaps to determine phrase orientation.
5(Tillmann, 2004) learns for each phrase a tendency to ei-

ther remain monotone or to swap with other phrases. As noted

in (Lopez, 2008), Hiero can represent the same information

with hierarchical rules of the form uX, Xu, and XuX. Hi-

ero actually models lexicalized reordering patterns that (Till-

mann, 2004) does not account for, e.g., a transformation from

X1uX2v to X2u
′
v

′
X1.

of the Gigaword corpus (LDC2007T40) in addition

to the target side of the parallel data. This data rep-

resents a total of about 700 million words. We man-

ually removed documents of Gigaword that were re-

leased during periods that overlap with those of our

development and test sets. The language model was

smoothed with the modified Kneser-Ney algorithm

as implemented in SRILM (Stolcke, 2002), and we

only kept 4-grams and 5-grams that occurred at least

three times in the training data.

For tuning and testing, we use the official NIST

MT evaluation data for Chinese from 2003 to 2008

(MT03 to MT08), which all have four English ref-

erences for each input sentence. We used the 1664

sentences of MT06 for tuning and development and

all other sets for testing. Parameter tuning was

done with minimum error rate training (Och, 2003),

which was used to maximize IBM BLEU-4 (Pap-

ineni et al., 2001). Since MERT is prone to search

errors, especially with large numbers of parameters,

we ran each tuning experiment four times with dif-

ferent initial conditions. We used n-best lists of size

200. In the final evaluations, we report results using

both TER version 0.7.25 (Snover et al., 2006) and

BLEU-4 (both uncased).

6 Results

We start by comparing some translations generated

by the best configurations of Joshua, Moses, and our

phrase-based decoder, systems we will empirically

evaluate later in this section. Fig. 5 shows trans-

lations of our development set MT06, which were

selected because our system makes a crucial use of

discontinuous phrases. In the first example, the Chi-

nese input contains S ... ö, which typically trans-
lates as when. Lacking an entry for the input phraseS�}C«e:ö in its phrase table, Moses is
unable to translate this segment appropriately, and

must instead split this phrase to generate the trans-

lation when the right was deprived of, where ö is
translated into of. This is evidently a poor transla-

tion. Conversely, our system uses a discontinuous

phrase to translate S ... ö, and translates the inter-
vening words separately.

The remaining three translations all contain cross-

serial DTUs (Søgaard and Kuhn, 2009) and thus

would be difficult to generate using 2-SCFG sys-

tems. The second example motivates the idea

970



MT06 — segment 1589

Reference: Under such cir-

cumstances, when the right

of existence was deprived,

the only way remaining was

to overthrow the existing

dynasty by force and try to

replace it.

Joshua: Under such cir-

cumstances, when life be

deprived, can only resort to

violence to overthrow the

current dynasty, trying to re-

place,

Moses: Under such circum-

stances, when the right was

deprived of, can only adopt

the means of violence, in

an attempt to overthrow the

present dynasty replaced,

This work: Under such cir-

cumstances, when he was de-

prived of the right to life, it

can only resort to violence in

an attempt to overthrow the

current dynasty replaced,

in this kind case when life right was deprive when only can use violence of means( Ù Í Åµ � , S �} C « e: ö , ê ý ÇÖ ´  Kµ ...
under such circumstances , when he was deprived of the right to life , it can only resort to violence ...

MT06 — segment 1044

Reference: CCP organi-

zation ministry demands

to further enlarge strength

of supervision of leading

cadres and cadre selection

and appointment

Joshua: Department de-

mands further intensify su-

pervision over the work

of selecting and appointing

leading cadres, and inten-

sify

Moses: The central organi-

zation department, called on

leading cadres, further in-

crease the intensity of super-

vision over work of selecting

and appointing cadres.

This work: The central orga-

nization department has called

for further increase the inten-

sity of supervision of leading

cadres and the work of select-

ing and appointing cadres.

CCP request further increase to leading cadres and cadre selection appointment work of supervision intensity-Äè B Û�e  ' ù ü rè  rè 	Ô û( å\  Ñ �
the central organization department has called for further increase the intensity of supervision of leading cadres and ... ...

MT06 — segment 559

Reference: The government

will take all possible mea-

sures to prevent similar inci-

dents from happening in the

future.

Joshua: Government will

take all measures to prevent

the re-occurrence of similar

incidents in the future.

Moses: The government will

take all measures to prevent

the occurrence of similar inci-

dents in the future.

This work: The government

will take all measures to pre-

vent similar incidents from

happening again in the future.

government will take all measure to prevent future again happen similar of incidents? � � ÇÖ �� ª½ e 2b ÊÆ  Ñ� {<  ö �
the government will take all measures to prevent similar incidents from happening again in the future .

MT06 — segment 769

Reference: He also said that

the arrangements are being

made now for the visits.

Joshua: He also said that

now is making arrange-

ments for this visit.

Moses: He also said that the

current visit is to make ar-

rangements.

This work: He also said that

the current arrangements are

made for the visit.

he also said now are for this one visit make arrangementsÖ Ø � °( ( : Ù � ¿î \ú  �
he also said that the current arrangements are made for the visit .

Figure 5: Actual translations produced by Joshua, Moses, and our system. For our system, we also display phrase

alignments, including discontinuous phrase alignments. Results for these three systems here are displayed in rows 2,

4, and 8 of Table 2. The thick blue arrows represent alignments between discontinuous phrases, while red segmented

arrows align continuous phrases.

971



MT06 (tune) MT03 MT04 MT05 MT08 ALL

System Gaps LexR BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER

1 hierarchical

(Joshua)

src yes 33.55 58.04 33.25 59.73 36.03 58.92 32.03 61.11 26.30 61.30 31.70 58.21

2 src+tgt yes 33.84 58.11 33.47 59.85 36.10 58.82 32.17 61.20 26.61 61.21 31.90 58.22

3 phrase-based

(Moses)

no no 33.17 59.24 32.60 60.80 35.38 59.55 31.15 62.43 25.56 61.98 31.08 59.14

4 no yes 34.25 58.23 33.72 60.42 36.37 59.18 32.49 61.80 26.70 61.48 32.16 58.56

5
discontinuous

phrase-based

(this work)

src no 33.77 58.56 33.20 60.42 36.17 59.13 31.75 61.62 25.99 61.47 31.68 58.60

6 tgt no 33.27 58.98 32.95 60.42 35.41 59.35 31.08 62.45 25.69 61.71 31.17 58.93

7 src+tgt no 33.86 58.26 33.32 60.02 36.36 58.56 31.87 61.35 26.13 61.29 31.81 58.25

8 src+tgt yes 35.00 56.85 34.96 57.97 37.44 57.61 33.39 59.92 26.74 60.51 32.93 57.03

Improvement over hierarchical +1.16 −1.26 +1.49 −1.88 +1.34 −1.21 +1.22 −1.28 +0.13 −0.70 +1.03 −1.19
Improvement over phrase-based +0.75 −1.38 +1.24 −2.45 +1.07 −1.57 +0.90 −1.88 +0.04 −0.97 +0.77 −1.53

Number of sentences 1664 919 1788 1082 1357 6810

Table 2: Our system compared again conventional and hierarchical phrase-based MT (Moses and Joshua). using

uncased BLEUr4n4[%] and TER[%]. LexR indicates whether lexicalized reordering is enabled or not. We use ran-

domization tests (Riezler and Maxwell, 2005) to determine significance of our best results (row 8) against Joshua (row

2) and Moses (row 4): differences marked in bold are significant at the p ≤ .01 level.

that larger translation units, including discontinuous

phrases, lead to better translations. The reference in-

cludes the translation enlarge strength of supervision

of leading cadres, and our system is able to produce

a translation that is almost identical (increase the in-

tensity of supervision of leading cadres) using only

two phrases, pulling together input words that are

fairly far apart in the sentence. The third Chinese

sentence has a word order quite different from En-

glish, but our decoder flexibly reorders it in a manner

that can’t be handled with SCFG decoders to give

a word order (prevent similar events from happen-

ing) that matches the one in the reference. The last

Chinese sentence includes the topicalization word: (for), which indicates the input sentence has no
subject. One way to properly handle this translation

is to turn the sentence into a passive in English (as

in the reference), a transformation our system does,

thanks to its support for complex reorderings.

Our main results are displayed in Table 2. First,

Joshua systematically outperforms the Moses base-

line (+0.82 BLEU point and −0.92 TER point on
average), but performance of the two is about the

same when Moses incorporates lexicalized reorder-

ing. This finding is consistent with previous work

(Lopez, 2008). The results of our system displayed

in rows 5–8 demonstrate that our system consis-

tently outperforms Moses, whether they both use

lexicalized reordering or not. The performance of

our best system—i.e., with lexicalized reordering

and both source and target gaps—is significantly

better than the best Moses system (+0.77 BLEU

and −1.53 TER). While the performance of our sys-

tem without lexicalized reordering is close to that of

Joshua, our system with lexicalized reordering sig-

nificantly outperforms Joshua (p ≤ .01) in 9 out of
10 evaluations. The single experiment where our im-

provement over Hiero is insignificant (i.e., BLEU on

MT08) is mainly affected by a discrepancy of length

(our brevity penalty on MT08 is 0.92).

It is interesting to notice that our system allowing

phrasal discontinuities only on the source (row 5)

performs almost as well as the system that allows

them on both sides (row 7). For instance, while

source discontinuities improve performance by 0.7

BLEU point on MT06, further enabling target dis-

continuities only raises performance by a mere 0.09

BLEU point. This naturally raises the question of

whether our support for target gaps is ineffective,

or whether target-discontinuous phrases are some-

what superfluous to the MT task. While it is cer-

tainly difficult to either confirm or deny the latter

hypothesis, we can at least compare our handling of

target-discontinuous phrases with hierarchical sys-

tems. In one additional set of experiments, we re-

moved target-discontinuous phrases in Joshua prior

to MERT and test time. Specifically, we removed

all hierarchical phrases whose target side has the

form uXv, uXvX, XuXv, and uXvXw, and only

allowed rules whose target side has the form uX,

Xu, XuX, XXu, or uXX. After this filtering,

we found that target-discontinuous phrases in Joshua

are also not crucial to its performance, since their re-

moval only caused a drop of 0.2 BLEU point (row 1)

and almost no change in terms of TER. We speculate

that using target discontinuous phrases is more diffi-

972



1 2 3 4 5 6 7
0

5000

10000

15000

# of English words per phrase

w
o

rd
 m

a
s
s

 

 

Moses

this work

Figure 6: Phrase length histogram for MT06.

cult, since it represents a generation rather than just

a matching problem.

In this paper, we have also argued that a main

benefit of discontinuous phrases—and particularly

source-discontinuous phrases—is that the decoder is

allowed to use larger translation units than when re-

stricted to continuous phrases. This claim is con-

firmed in Fig. 6. We find that our decoder makes

effective use of the extended set of translation op-

tions at its disposal: While the Moses baseline trans-

lates MT06 with an average 1.73 words per phrase,

adding support for discontinuities increases this av-

erage to 2.16, and reduces by 43% the use of sin-

gle word phrases. On MT06, 53% of the translated

sentences produced by our best system use at least

one source-discontinuous phrase, and 9% of them

exploit one or more target-discontinuous phrases.

7 Related Work

The main goal of this paper is to show that discontin-

uous phrases can greatly improve the performance

of phrase-based systems. While some of the most

recent phrase-based systems (Chiang, 2007; Watan-

abe et al., 2006) exploit context-free decoding algo-

rithms (CKY, Earley, etc.) to cope with discontinu-

ities, our system preserves the simplicity and speed

of conventional phrase-based decoders, and in par-

ticular does not build any intermediate tree structure,

does not impose any hard reordering constraints

other than the distortion limit, and still achieves

translation performance that is superior to that of a

state-of-the-art hierarchical system.

A few previous non-hierarchical systems have

also exploited phrasal discontinuities. The most no-

table previous attempt to incorporate gaps is de-

scribed in (Simard et al., 2005). Simard et al.

presents an extension to Moses that allows gaps in

both source and target phrases, though each of their

gap symbols must span exactly one word. This fact

makes decoding simpler, since the position of all tar-

get words in a translation hypothesis is known as

soon as the hypothesis is laid down, but fixed-size

discontinuous phrases are less general and increase

sparsity. By comparison, our gaps may span any

number of words, so we have an increased ability to

flexibly match the input sentence effectively. (Crego

and Yvon, 2009) also handles gaps, though this work

is applicable to an n-gram-based SMT framework

(Mariòo et al., 2006), which is fairly different from

the phrase-based framework.

8 Conclusions

In this paper, we presented a generalization of con-

ventional phrase-based decoding to handle discon-

tinuities in both source and target phrases. Our

system significantly outperforms Moses and Joshua,

two standard implementations of conventional and

hierarchical phrase-based decoding. We found that

allowing discontinuities in the source is more use-

ful than target discontinuities in our system, though

we found that this turns out to also be the case with

the hierarchical phrases of Joshua. In future work,

we plan to extend the parameterization of phrase-

based lexicalized reordering models to be sensitive

to these discontinuities, and we will also consider

adding syntactic features to our models to penal-

ize discontinuities that are not syntactically moti-

vated (Marton and Resnik, 2008; Chiang et al.,

2009). The discontinuous phrase-based MT system

described in this work is part of Phrasal, an open-

source phrase-based system available for download

at http://nlp.stanford.edu/software/phrasal.

Acknowledgements

The authors thank three anonymous reviewers, Dan

Jurafsky, Spence Green, Steven Bethard, Daniel Cer,

Chris Callison-Burch, and Pi-Chuan Chang for their

helpful comments. This paper is based on work

funded by the Defense Advanced Research Projects

Agency through IBM. The content does not neces-

sarily reflect the views of the U.S. Government, and

no official endorsement should be inferred.

973



References

Chris Callison-Burch, Colin Bannard, and Josh

Schroeder. 2005. Scaling phrase-based statistical

machine translation to larger corpora and longer

phrases. In Proc. of ACL, pages 255–262.

Daniel Cer, Michel Galley, Dan Jurafsky, and Christo-

pher Manning. 2010. Phrasal: A statistical machine

translation toolkit for exploring new model features.

In Proc. of NAACL-HLT, Demonstration Session.

David Chiang, Kevin Knight, and Wei Wang. 2009.

11,001 new features for statistical machine translation.

In Proc. of NAACL, pages 218–226.

David Chiang. 2007. Hierarchical phrase-based transla-

tion. Computational Linguistics, 33(2):201–228.

Josep Crego and François Yvon. 2009. Gappy transla-
tion units under left-to-right SMT decoding. In Proc.

of EAMT.

Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-

chine translation as lexicalized parsing with hooks. In

Proc. of the Ninth International Workshop on Parsing

Technology, pages 65–73.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.

2003. Statistical phrase-based translation. In Proc.

of NAACL, pages 48–54.

Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,

Christopher Callison-Burch, Marcello Federico,

Nicola Bertoldi, Brooke Cowan, Wade Shen,

Christine Moran, Richard Zens, Chris Dyer, Ondrej

Bojar, Alexandra Constantin, and Evan Herbst. 2007.

Moses: Open source toolkit for statistical machine

translation. In Proc. of ACL, Demonstration Session.

Philipp Koehn. 2004. Pharaoh: a beam search decoder

for phrase-based statistical machine translation mod-

els. In Proc. of AMTA, pages 115–124.

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-

itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren

N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.

2009. Joshua: an open source toolkit for parsing-

based MT. In Proc. of WMT.

Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-

ment by agreement. In Proc. of HLT-NAACL, pages

104–111.

Adam Lopez. 2007. Hierarchical phrase-based transla-

tion with suffix arrays. In Proc. of EMNLP-CoNLL,

pages 976–985.

Adam Lopez. 2008. Tera-scale translation models via

pattern matching. In Proc. of COLING.

José B. Mariòo, Rafael E. Banchs, Josep M. Crego, Adrià

de Gispert, Patrik Lambert, José A. R. Fonollosa, and

Marta R. Costa-jussà. 2006. N-gram-based machine

translation. Computational Linguistics, 32(4):527–

549.

Yuval Marton and Philip Resnik. 2008. Soft syntactic

constraints for hierarchical phrased-based translation.

In Proc. of ACL, pages 1003–1011.

Franz Josef Och and Hermann Ney. 2004. The align-

ment template approach to statistical machine transla-

tion. Computational Linguistics, 30(4):417–449.

Franz Josef Och. 2003. Minimum error rate training for

statistical machine translation. In Proc. of ACL.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-

Jing Zhu. 2001. BLEU: a method for automatic eval-

uation of machine translation. In Proc. of ACL.

Stefan Riezler and John T. Maxwell. 2005. On some pit-

falls in automatic evaluation and significance testing

for MT. In Proc. of Workshop on Evaluation Mea-

sures, pages 57–64.

Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc

Dymetman, Eric Gaussier, Cyril Goutte, Kenji Ya-

mada, Philippe Langlais, and Arne Mauser. 2005.

Translating with non-contiguous phrases. In Proc. of

HLT-EMNLP, pages 755–762.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-

nea Micciulla, and John Makhoul. 2006. A study of

translation edit rate with targeted human annotation.

In Proc. of AMTA, pages 223–231.

Anders Søgaard and Jonas Kuhn. 2009. Empirical lower

bounds on alignment error rates in syntax-based ma-

chine translation. In Proc. of the Third Workshop on

Syntax and Structure in Statistical Translation (SSST-

3) at NAACL HLT 2009, pages 19–27.

Anders Søgaard and Dekai Wu. 2009. Empirical lower

bounds on translation unit error rate for the full class

of inversion transduction grammars. In Proc. of IWPT,

pages 33–36.

Andreas Stolcke. 2002. SRILM – an extensible language

modeling toolkit. In Proc. of ICSLP, pages 901–904.

Christoph Tillmann. 2004. A unigram orientation model

for statistical machine translation. In Proc. of HLT-

NAACL, pages 101–104.

Chao Wang, Michael Collins, and Philipp Koehn. 2007.

Chinese syntactic reordering for statistical machine

translation. In Proc. of EMNLP-CoNLL.

Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.

2006. Left-to-right target generation for hierarchical

phrase-based translation. In Proc. of ACL.

Benjamin Wellington, Sonjia Waxmonsky, and I. Dan

Melamed. 2006. Empirical lower bounds on the

complexity of translational equivalence. In Proc. of

COLING-ACL, pages 977–984.

Dekai Wu. 1997. Stochastic inversion transduction

grammars and bilingual parsing of parallel corpora.

Computational Linguistics, 23(3):377–404.

Ying Zhang and Stephan Vogel. 2005. An efficient

phrase-to-phrase alignment model for arbitrarily long

phrase and large corpora. In Proc. of EAMT.

974


