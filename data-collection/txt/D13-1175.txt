










































Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1688–1699,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Boosting Cross-Language Retrieval by Learning
Bilingual Phrase Associations from Relevance Rankings

Artem Sokolov and Laura Jehl and Felix Hieber and Stefan Riezler
Department of Computational Linguistics

Heidelberg University, 69120 Heidelberg, Germany
{sokolov,jehl,hieber,riezler}@cl.uni-heidelberg.de

Abstract

We present an approach to learning bilin-
gual n-gram correspondences from relevance
rankings of English documents for Japanese
queries. We show that directly optimizing
cross-lingual rankings rivals and complements
machine translation-based cross-language in-
formation retrieval (CLIR). We propose an ef-
ficient boosting algorithm that deals with very
large cross-product spaces of word correspon-
dences. We show in an experimental evalu-
ation on patent prior art search that our ap-
proach, and in particular a consensus-based
combination of boosting and translation-based
approaches, yields substantial improvements
in CLIR performance. Our training and test
data are made publicly available.

1 Introduction

The central problem addressed in Cross-Language
Information Retrieval (CLIR) is that of translating
or projecting a query into the language of the docu-
ment repository across which retrieval is performed.
There are two main approaches to tackle this prob-
lem: The first approach leverages the standard Sta-
tistical Machine Translation (SMT) machinery to
produce a single best translation that is used as
search query in the target language. We will hence-
forth call this the direct translation approach. This
technique is particularly useful if large amounts of
data are available in domain-specific form.

Alternative approaches avoid to solve the hard
problem of word reordering, and instead rely on
token-to-token translations that are used to project

the query terms into the target language with a
probabilistic weighting of the standard term tf-idf
scheme. Darwish and Oard (2003) termed this
method the probabilistic structured query approach.
The advantage of this technique is an implicit query
expansion effect due to the use of probability distri-
butions over term translations (Xu et al., 2001). Re-
cent research has shown that leveraging query con-
text by extracting term translation probabilities from
n-best direct translations of queries instead of using
context-free translation tables outperforms both di-
rect translation and context-free projection (Ture et
al., 2012b; Ture et al., 2012a).

While direct translation as well as probabilistic
structured query approaches use machine learning to
optimize the SMT module, retrieval is done by stan-
dard search algorithms in both approaches. For ex-
ample, Google’s CLIR approach uses their standard
proprietary search engine (Chin et al., 2008). Ture et
al. (2012b; 2012a) use standard retrieval algorithms
such as BM25 (Robertson et al., 1998). That means,
machine learning in SMT-based approaches concen-
trates on the cross-language aspect of CLIR and is
agnostic of the ultimate ranking task.

In this paper, we present a method to project
search queries into the target language that is com-
plementary to SMT-based CLIR approaches. Our
method learns a table of n-gram correspondences by
direct optimization of a ranking objective on rele-
vance rankings of English documents for Japanese
queries. Our model is similar to the approach of
Bai et al. (2010) who characterize their technique as
“Learning to rank with (a Lot of) Word Features”.
Given a set of search queries q ∈ IRQ and docu-

1688



ments d ∈ IRD, where the jth dimension of a vector
indicates the occurrence of the jth word for dictio-
naries of size Q and D, we want to learn a score
f(q,d) between a query and a given document us-
ing the model1

f(q,d) = q>Wd =

Q∑
i=1

D∑
j=1

qiWijdj .

We take a pairwise ranking approach to optimiza-
tion. That is, given labeled data in the form of a
set R of tuples (q,d+,d−), where d+ is a relevant
(or higher ranked) document and d− an irrelevant
(or lower ranked) document for query q, the goal
is to find a weight matrix W ∈ IRQ×D such that
f(q,d+) > f(q,d−) for all data tuples from R.
The scoring model learns weights for all possible
correspondences of query terms and document terms
by directly optimizing the ranking objective at hand.
Such a phrase table contains domain-specific word
associations that are useful to discern relevant from
irrelevant documents, something that is orthogonal
and complementary to standard SMT models.

The challenge of our approach can be explained
by constructing a joint feature map φ from the outer
product of the vectors q and d where

φ((i−1)D+j)(q,d) = (q⊗ d)ij = (qd>)ij . (1)

Using this feature map, we see that the score func-
tion f can be written in the standard form of a lin-
ear model that computes the inner product between
a weight vector w and a feature vector φ where
w, φ ∈ IRQ×D and

f(q,d) = 〈w, φ(q,d)〉. (2)

While various standard algorithms exist to optimize
linear models, the difficulty lies in the memory foot-
print and capacity of the word-based model. A full-
sized model includes Q × D parameters which is
easily in the billions even for moderately sized dic-
tionaries. Clearly, an efficient implementation and
remedies against overfitting are essential.

The main contribution of our paper is the pre-
sentation of algorithms that make learning a phrase

1With bold letters we denote vectors for query q and docu-
ment d. Vector components are denoted with normal font letters
and indices (e.g., qi).

table by direct rank optimization feasible, and an
experimental verification of the benefits of this ap-
proach, especially with regard to a combination
of the orthogonal information sources of ranking-
based and SMT-based CLIR approaches. Our ap-
proach builds upon a boosting framework for pair-
wise ranking (Freund et al., 2003) that allows the
model to grow incrementally, thus avoiding having
to deal with the full matrix W . Furthermore, we
present an implementation of boosting that utilizes
parallel estimation on bootstrap samples from the
training set for increased efficiency and reduced er-
ror (Breiman, 1996). Our “bagged boosting” ap-
proach allows to combine incremental feature selec-
tion, parallel training, and efficient management of
large data structures.

We show in an experimental evaluation on large-
scale retrieval on patent abstracts that our boosting
approach is comparable in MAP and improves sig-
nificantly by 13-15 PRES points over very competi-
tive translation-based CLIR systems that are trained
on 1.8 million parallel sentence pairs from Japanese-
English patent documents. Moreover, a combination
of the orthogonal information learned in ranking-
based and translation-based approaches improves
over 7 MAP points and over 15 PRES points over the
respective translation-based system in a consensus-
based voting approach following the Borda Count
technique (Aslam and Montague, 2001).

2 Related Work

Recent research in CLIR follows the two main
paradigms of direct translation and probabilistic
structured query approaches. An example for the
first approach is the work of Magdy and Jones
(2011) who presented an efficient technique to adapt
off-the-shelf SMT systems for CLIR by training
them on data pre-processed for retrieval (case fold-
ing, stopword removal, stemming). Nikoulina et al.
(2012) presented an approach to direct translation-
based CLIR where the n-best list of an SMT system
is re-ranked according to the MAP performance of
the translated queries. The probabilistic structured
query approach has seen a lot of work on context-
aware query expansion across languages, based on
various similarity statistics (Ballesteros and Croft,
1998; Gao et al., 2001; Lavrenko et al., 2002; Gao

1689



et al., 2007). At the time of writing this paper, the
most recent extension to this paradigm is Ture et
al. (2012a). In addition to projecting terms from
n-best translations, they propose a projection ex-
tracted from the hierarchical phrase- based grammar
models, and a scoring method based on multi-token
terms. Since the latter techniques achieved only
marginal improvements over the context-sensitive
query translation from n-best lists, we did not pur-
sue them in our work.

CLIR in the context of patent prior art search
was done as extrinsic evaluation at the NTCIR
PatentMT2 workshops until 2010, and has been on-
going in the CLEF-IP3 benchmarking workshops
since 2009. However, most workshop participants
did either not make use of automatic translation at
all, or they used an off-the-shelf translation tool.
This is due to the CLEF-IP data collection where
parts of patent documents are provided as man-
ual translations into three languages. In order to
evaluate CLIR in a truly cross-lingual scenario, we
created a large patent CLIR dataset where queries
and documents are Japanese and English patent ab-
stracts, respectively.

Ranking approaches to CLIR have been presented
by Guo and Gomes (2009) who use pairwise rank-
ing for patent retrieval. Their method is a classical
learning-to-rank setup where retrieval scores such as
tf-idf or BM25 are combined with domain knowl-
edge on patent class, inventor, date, location, etc.
into a dense feature vector of a few hundred fea-
tures. Methods to learn word-based translation cor-
respondences from supervised ranking signals have
been presented by Bai et al. (2010) and Chen et
al. (2010). These approaches tackle the problem of
complexity and capacity of the cross product matrix
of word correspondences from different directions.
The first proposes to learn a low rank representa-
tion of the matrix; the second deploys sparse online
learning under `1 regularization to keep the matrix
small. Both approaches are mainly evaluated in a
monolingual setting. The cross-lingual evaluation
presented in Bai et al. (2010) uses weak translation-
based baselines and non-public data such that a di-
rect comparison is not possible.

2http://research.nii.ac.jp/ntcir/ntcir/
3http://www.ifs.tuwien.ac.at/˜clef-ip/

A combination of bagging and boosting in the
context of retrieval has been presented by Pavlov et
al. (2010) and Ganjisaffar et al. (2011). This work
is done in a standard learning-to-rank setup using a
few hundred dense features trained on hundreds of
thousands of pairs. Our setup deals with billions of
sparse features (from the cross-product of the un-
restricted dictionaries) trained on millions of pairs
(sampled from a much larger space). Parallel boost-
ing where all feature weights are updated simultane-
ously has been presented by Collins et al. (2002) and
Canini et al. (2010). The first method distributes the
gradient calculation for different features among dif-
ferent compute nodes. This is not possible in our ap-
proach because we construct the cross-product ma-
trix on-the-fly. The second approach requires sub-
stantial efforts in changing the data representation
to use the MapReduce framework. Overall, one of
the goals of our work is sequential updating for im-
plicit feature selection, something that runs contrary
to parallel boosting.

3 CLIR Approaches

3.1 Direct translation approach

For direct translation, we use the SCFG decoder
cdec (Dyer et al., 2010)4 and build grammars us-
ing its implementation of the suffix array extraction
method described in Lopez (2007). Word align-
ments are built from all parallel data using mgiza5

and the Moses scripts6. SCFG models use the same
settings as described in Chiang (2007). Training
and querying of a modified Kneser-Ney smoothed 5-
gram language model are done on the English side
of the training data using KenLM (Heafield, 2011)7.
Model parameters were optimized using cdec’s im-
plementation of MERT (Och (2003)).

At retrieval time, all queries are translated
sentence-wise and subsequently re-joined to form
one query per patent. Our baseline retrieval system
uses the Okapi BM25 scores for document ranking.

4https://github.com/redpony/cdec
5http://www.kyloo.net/software/doku.php/

mgiza:overview
6http://www.statmt.org/moses/?n=Moses.

SupportTools
7http://kheafield.com/code/kenlm/

estimation/

1690



3.2 Probabilistic structured query approach
Early Probabilistic Structured Query approaches
(Xu et al., 2001; Darwish and Oard, 2003) represent
translation options by lexical, i.e., token-to-token
translation tables that are estimated using standard
word alignment techniques (Och and Ney, 2000).
Later approaches (Ture et al., 2012b; Ture et al.,
2012a) extract translation options from the decoder’s
n-best list for translating a particular query. The
central idea is to let the language model choose flu-
ent, context-aware translations for each query term
during decoding. This retains the desired query-
expansion effect of probabilistic structured models,
but it reduces query drift by filtering translations
with respect to the context of the full query.

A projection of source language query terms f ∈
F into the target language is achieved by repre-
senting each source token f by its probabilistically
weighted translations. The score of target document
E, given source language query F , is computed by
calculating the BM25 rank over projected term fre-
quency and document frequency weights as follows:

score(E|F ) =
∑
f∈F

BM25(tf(f,E), df(f)) (3)

tf(f,E) =
∑
e∈Ef

tf(e, E)p(e|f)

df(f) =
∑
e∈Ef

df(e)p(e|f)

where Ef = {e ∈ E|p(e|f) > pL} is the set of
translation options for query term f with probability
greater than pL. We also use a cumulative threshold
pC so that only the most probable options are added
until pC is reached.

Ture et al. (2012b; 2012a) achieved best retrieval
performance by interpolating between (context-free)
lexical translation probabilities plex estimated on
symmetrized word alignments, and (context-aware)
translation probabilities pnbest estimated on the n-
best list of an SMT decoder:

p(e|f) = λpnbest(e|f) + (1− λ)plex(e|f) (4)

pnbest(e|f) is estimated by calculating expectations
of term translations from k-best translations:

pnbest(e|f) =
∑n

k=1 ak(e, f)D(k, F )∑n
k=1

∑
e′ ak(e

′, f)D(k, F )

where ak(e, f) is a function indicating an alignment
of target term e to source term f in the kth derivation
of queryF , andD(k, F ) is the model score of the kth
derivation in the n-best list for query F .

We use the same hierarchical phrase-based sys-
tem that was used for direct translation to calcu-
late n-best translations for the probabilistic struc-
tured query approach. This allows us to extract
word alignments between source and target text for
F from the SCFG rules used in the derivation. The
concept of self-translation is covered by the de-
coder’s ability to use pass-through rules if words or
phrases cannot be translated.

Probabilistic structured queries that include
context-aware estimates of translation probabilities
require a preservation of sentence-wise context-
sensitivity also in retrieval. Thus, unlike the direct
translation approach, we compute weighted term
and document frequencies for each sentence s in
query F separately. The scoring (3) of a target doc-
ument for a multiple sentence query then becomes:

score(E|F ) =
∑
s in F

∑
f∈s

BM25(tf(f,E), df(f))

3.3 Direct Phrase Table Learning from
Relevance Rankings

Pairwise Ranking using Boosting The general
form of the RankBoost algorithm (Freund et al.,
2003; Collins and Koo, 2005) defines a scoring
function f(q,d) on query q and document d as a
weighted linear combination of T weak learners ht
such that f(q,d) =

∑T
t=1wtht(q,d). Weak learn-

ers can belong to an arbitrary family of functions,
but in our case they are restricted to the simplest
case of unparameterized indicator functions select-
ing components of the feature vector φ(q,d) in (1)
such that f is of the standard linear form (2). In our
experiments, these features indicate the presence of
pairs of uni- and bi-grams from the source-side vo-
cabulary of query terms and the target-side vocabu-
lary of document-terms, respectively. Furthermore,
in order to simulate the pass-through behavior of
SMT, we introduce additional features to the model
that indicate the identity of terms in source and tar-
get. All identity features have the same fixed weight
β, which is found on the development set.

For training, we are given labeled data in the form

1691



of a set R of tuples (q,d+,d−), where d+ is a rel-
evant (or higher ranked) document and d− an ir-
relevant (or lower ranked) document for query q.
RankBoost’s objective is to correctly rank query-
document pairs such that f(q,d+) > f(q,d−) for
all data tuples from R. RankBoost achieves this by
optimizing the following convex exponential loss:

Lexp =
∑

(q,d+,d−)∈R

D(q,d+,d−)ef(q,d
−)−f(q,d+),

where D(q,d+,d−) is a non-negative importance
function on pairs of documents for a given q.

We optimize Lexp in a greedy iterative fash-
ion, which closely follows an efficient algorithm of
Collins and Koo (2005) for the case of binary-valued
h. In each step, the single feature h is selected that
provides the largest decrease of Lexp, i.e., that has
the largest projection on the direction of the gradi-
ent ∇hLexp. Because of the sequential nature of
the algorithm, RankBoost implicitly performs auto-
matic feature selection and regularization (Rosset et
al., 2004), which is crucial to reduce complexity and
capacity for our application.

Parallelization and Bagging To achieve paral-
lelization we use a variant of bagging (Breiman,
1996) on top of boosting, which has been observed
to improve performance, reduce variance and is
trivial to parallelize. The procedure is described
as part of Algorithm 1: From the set of prefer-
ence pairs R, draw S equal-sized samples with
replacement and distribute to nodes. Then, us-
ing each of the samples as a training set, sep-
arate boosting models {wst , hst}, s = 1 . . . S are
trained that contain the same number of features
t = 1 . . . T . Finally the models are averaged:
f(q,d) = 1S

∑
t

∑
sw

s
th
s
t (q,d).

Algorithm The entire training procedure is out-
lined in Algorithm 1. For each possible feature h
we maintain auxiliary variables W+h and W

−
h :

W±h =
∑

(q,d+,d−):h(q,d+)−h(q,d−)=±1

D(q,d+,d−),

which are the cumulative weights of correctly and
incorrectly ranked instances by a candidate feature
h. The absolute value of ∂Lexp/∂h can be ex-
pressed as

∣∣√W+h −√W−h ∣∣ which is used as fea-
ture selection criterion (Collins and Koo, 2005).

The optimum of minimizing Lexp over w (with
fixed h) can be shown to be w = 12 ln

W+h +�Z

W−h +�Z
,

where � is a smoothing parameter to avoid prob-
lems with small W±h (Schapire and Singer, 1999),
and Z =

∑
(q,d+,d−)∈RD(q,d

+,d−). Further-
more, for each step t of the learning process, values
of D are updated to concentrate on pairs that have
not been correctly ranked so far:

Dt+1 = Dt · ewt
(
ht(q,d−)−ht(q,d+)

)
. (5)

Finally, to speed up learning, on iteration t we
recalculate W±h only for those h that cooccur
with previously selected ht and keep the rest un-
changed (Collins and Koo, 2005).

Algorithm 1: Bagged Boosting

Input: training tuplesR, max number of features
T , initial D0, smoothing param. � ' 10−5

Initialize:
fromR draw S samples with replacement and
distribute to nodes
Learn:
for all samples s = 1 . . . S in parallel do

calculate W+h ,W
−
h , Z on sample’s data

for all t = 1 . . . T do

choose ht = arg maxh
∣∣√W+h −√W−h ∣∣

and wt = 12 ln
W+h +�Z

W−h +�Z

update Dt according to (5)
update W±h for all h that cooccur with ht

end
return to master {hst , wst }, t = 1 . . . T

end
Bagging:
return scoring function
f(q,d) = 1S

∑
t

∑
sw

s
th
s
t (q,d)

Implementation Because of the total number of
features (billions) there are several obstacles for the
straight-forward implementation of Algorithm 1.

First, we cannot directly access all pairs (q,d)
containing a particular feature h needed for calcu-
lating W±h . Building an inverted index is compli-
cated as it needs to fit into memory for fast fre-

1692



quent access8. We resort to the on-the-fly creation of
the cross-product space of features, following prior
work by Grangier and Bengio (2008) and Goel et al.
(2008). That is, while processing a pair (q,d), we
update W±h for all h found for the pair.

Second, even if the explicit representation of all
features is avoided by on-the-fly feature construc-
tion, we still need to keep all W±h in addressable
RAM. To achieve that we use hash kernels (Shi et
al., 2009) and map original features into b-bit integer
hashes. The values W±h′ for new, “hashed”, features
h′ become W±h′ =

∑
h:HASH(h)=h′W

±
h . We used

the MurmurHash3 function on the UTF-8 represen-
tations of features and b = 30 (resulting in more
than 1 billion distinct hashes).

4 Model Combination by Borda Counts

SMT-based approaches to CLIR and our boosting
approach have different strengths. The SMT-based
approaches produce fluent translations that are use-
ful for matching general passages written in natu-
ral language. Both baseline SMT-based approaches
presented above are agnostic of the ultimate retrieval
task and are not specifically adapted for it. The
boosting method, on the other hand, learns domain-
specific word associations that are useful to discern
relevant from irrelevant documents. In order to com-
bine these orthogonal sources of information in a
way that democratically respects each approach we
use Borda Counts, i.e., a consensus-based voting
procedure that has been successfully employed to
aggregate ranked lists of documents for metasearch
(Aslam and Montague, 2001).

We implemented a weighted version of the Borda
Count method where each voter has a fixed amount
of voting points which she is free to distribute among
the candidates to indicate the amount of preference
she is giving to each of them. In the case of retrieval,
for each q, the candidates are the scored documents
in the retrieved subset of the whole document set.
The aggregate score fagg for two rankings f1(q,d)

8It is possible to construct separate query and document in-
verted indices and intersect them on the fly to determine the
set of documents that contains some pair of words. In practice,
however, we found the overhead of set intersection during each
feature access prohibitive.

and f2(q,d) for all (q,d) in the test set is then:

fagg(q,d) = κ
f1(q,d)∑
d f1(q,d)

+(1−κ) f2(q,d)∑
d f2(q,d)

.

In practice, the normalizations sum over the top K
retrieved documents. If a document is present only
in the top-K list of one system, its score is con-
sidered zero for the other system. The aggregated
scores fagg(q,d) are sorted in descending order and
top K scores are kept for evaluation.

Using the terminology proposed by Belkin et
al. (1995), combining several systems’ scores with
Borda Counts can be viewed as the “data fusion”
approach to IR, that merges outputs of the systems,
while the PSQ baseline is an example of the “query
combination” approach that extends the query at the
input. Both techniques were earlier found to have
similar performance in CLIR tasks based on direct
translation, with a preference for the data fusion ap-
proach (Jones and Lam-Adesina, 2002).

5 Translation and Ranking Data

5.1 Parallel Translation Data

For Japanese-to-English patent translation we used
data provided by the organizers of the NTCIR9

workshop for the JP-EN PatentMT subtask. In par-
ticular, we used the data provided for NTCIR-7 (Fu-
jii et al., 2008), consisting of 1.8 million parallel
sentence pairs from the years 1993-2002 for train-
ing. For parameter tuning we used the develop-
ment set of the NTCIR-8 test collection, consisting
of 2,000 sentence pairs. The data were extracted
from the description section of patents published
by the Japanese Patent Office (JPO) and the United
States Patent and Trademark Office (USPTO) by the
method described in Utiyama and Isahara (2007).

Japanese text was segmented using the MeCab10

toolkit. Following Feng et al. (2011), we applied
a modified version of the compound splitter de-
scribed in Koehn and Knight (2003) to katakana
terms, which are often transliterations of English
compound words. As these are usually not split by
MeCab, they can cause a large number of out-of-
vocabulary terms.

9http://research.nii.ac.jp/ntcir/ntcir/
10https://code.google.com/p/mecab/

1693



#queries #relevant #unique docs
train 107,061 1,422,253 888,127
dev 2,000 26,478 25,669
test 2,000 25,173 24,668

Table 1: Statistics of ranking data.

For the English side of the training data, we ap-
plied a modified version of the tokenizer included in
the Moses scripts. This tokenizer relies on a list of
non-breaking prefixes which mark expressions that
are usually followed by a “.” (period). We cus-
tomized the list of prefixes by adding some abbrevi-
ations like “Chem”, “FIG” or “Pat”, which are spe-
cific to patent documents.

5.2 Ranking Data from Patent Citations

Graf and Azzopardi (2008) describe a method to ex-
tract relevance judgements for patent retrieval from
patent citations. The key idea is to regard patent doc-
uments that are cited in a query patent, either by the
patent applicant, or by the patent examiner or in a
patent office’s search report, as relevant for the query
patent. Furthermore, patent documents that are re-
lated to the query patent via a patent family relation-
ship, i.e., patents granted by different patent author-
ities but related to the same invention, are regarded
as relevant. We assign three integer relevance levels
to these three categories of relationships, with high-
est relevance (3) for family patents, lower relevance
for patents cited in search reports by patent examin-
ers (2), and lowest relevance level (1) for applicants’
citations. We also include all patents which are in
the same patent family as an applicant or examiner
citation to avoid false negatives. This methodol-
ogy has been used to create patent retrieval data at
CLEF-IP11 and proved very useful to automatically
create a patent retrieval dataset for our experiments.

For the creation of our dataset, we used the
MAREC12 citation graph to extract patents in cita-
tion or family relation. Since the Japanese portion
of the MAREC corpus only contains English ab-
stracts, but not the Japanese full texts, we merged
the patent documents in the NTCIR-10 test collec-
tion described above with the Japanese (JP) section

11http://www.ifs.tuwien.ac.at/˜clef-ip/
12http://www.ifs.tuwien.ac.at/imp/marec.

shtml

of MAREC. Title, abstract, description and claims
were added to the MAREC-JP data if the docu-
ment was available in NTCIR. In order to keep par-
allel data for SMT training separate from ranking
data, we used only data from the years 2003-2005
to extract training data for ranking, and two small
datasets of 2,000 queries each from the years 2006-
2007 for development and testing. Table 1 gives an
overview over the data used for ranking. For de-
velopment and test data, we randomly added irrele-
vant documents from the NTCIR-10 collection until
we obtained two pools of 100,000 documents. The
necessary information to reproduce the exact train,
development and test data samples is downloadable
from authors’ webpage13.

The experiments reported here use only the ab-
stract of the Japanese and English patents in our
training, development and test collection.

6 Experiments

6.1 System Development

System development and evaluation in our exper-
iments was done on the ranking data described
in the previous section (see Table 1). We report
Mean Average Precision (MAP) scores, using the
trec eval (ver. 8.1) script from the TREC evalu-
ation campaign14, with a limit of top K = 1, 000 re-
trieved documents for each query. Furthermore, we
use the Patent Retrieval Evaluation Score (PRES)15

introduced by Magdy and Jones (2010). This met-
ric accounts for both precision and recall. In the
study by Magdy and Jones (2010), PRES agreed
with MAP in almost 80% of cases, and both agreed
on the ranks of the best and the worst IR system.
Both MAP and PRES scores are reported in the same
range [0, 1], and 0.01 stands for 1 MAP (PRES)
point. Statistical significance of pairwise system
comparisons was assessed using the paired random-
ization test (Noreen, 1989; Smucker et al., 2007).

For each system, optimal meta-parameter settings
were found by choosing the configuration with high-
est MAP score on the development set. These results

13http://www.cl.uni-heidelberg.de/
statnlpgroup/boostclir

14http://trec.nist.gov/trec_eval
15http://www.computing.dcu.ie/˜wmagdy/

Scripts/PRESeval.htm

1694



method MAP PRESdev test dev test
1 DT 0.2636 0.2555 0.5669 0.5681

2 PSQ lexical table 0.2520 0.2444 0.5445 0.5498
3 PSQ n-best table 0.2698 0.2659 0.5789 0.5851

Boost-1g 0.2064 1230.1982 0.5850 120.6122
Boost-2g 0.2526 30.2474 0.6900 1230.7196

Table 2: MAP and PRES scores for CLIR methods (best
configurations) on the development and test sets. Prefixed
numbers denote statistical significance of a pairwise com-
parison with the baseline indicated by the superscript. For
example, the bottom right result shows that Boost-2g is
significantly better than DT (method 1), PSQ lexical ta-
ble (method 2) and PSQ n-best table (method 3).

(together with PRES results) are shown in the sec-
ond and fourth column of Table 2.

The direct translation approach (DT) was devel-
oped in three configurations: no stopword filtering,
small stopword list (52 words) and a large stopword
list (543 words). The last configuration achieved the
highest score (MAP 0.2636).

The probabilistic structured query (PSQ) ap-
proach was developed using the lexical translation
table and the translation table estimated on the de-
coder’s n-best list, both optionally pruned with a
variable lower pL and cumulative pC threshold on
the word pair probability in the table (Section 3.2).
A further meta-parameter of PSQ was whether to use
standard or unique n-best lists. Finally, all variants
were coupled with the same stopword filters as in
the DT approach. The configurations that achieved
the highest scores were: MAP 0.2520 for PSQ with
a lexical table (pL = 0.01, pC = 0.95, no stop-
word filtering), and MAP 0.2698 for PSQ with a
translation table estimated on the n-best list (pL =
0.005, pC = 0.95, large stopword list). Interpolat-
ing between lexical and n-best tables did not im-
prove results in our experiments, thus we set λ = 1
in equation (4).

Each SMT-based system was run with 4 different
MERT optimizations, leading to variations of less
than 1 MAP point for each system. The best con-
figurations for DT and PSQ on the development set
were fixed and used for evaluation on the test set.

Training of the boosting approach (Boost) was
done in parallel on bootstrap samples from the train-
ing data. First, a query q (i.e., a Japanese abstract)
was sampled uniformly from all training queries.

method MAP PRESdev test dev test
DT + PSQ n-best 0.2778 ∗0.2726 0.5884 ∗0.5942

DT + Boost-1g 0.2778 ∗0.2728 0.6157 ∗0.6225
DT + Boost-2g 0.3309 ∗0.3300 0.7132 ∗0.7279
PSQ lexical + Boost-1g 0.2695 ∗0.2653 0.6068 ∗0.6131
PSQ lexical + Boost-2g 0.3215 ∗0.3187 0.7071 ∗0.7240
PSQ n-best + Boost-1g 0.2863 ∗0.2850 0.6309 ∗0.6402
PSQ n-best + Boost-2g 0.3439 ∗0.3416 0.7212 ∗0.7376

Table 3: MAP and PRES scores of the aggregated mod-
els on the development and test sets. Development scores
correspond to peaks in Figures 1 and 3, respectively, for
MAP and PRES; test scores are given for the κ’s deliv-
ering these peaks on the development set. Prefixed ∗ in-
dicates statistical significance of the result difference be-
tween aggregated system and the respective translation-
based system used in the aggregation.

Then we sampled independently and uniformly a
relevant document d+ (i.e., an English abstract)
from the English patents marked relevant for the
Japanese patent, and a random document d− from
the whole pool of English patent abstracts. If d−

had a relevance score greater or equal to the rele-
vance score of d+, it was resampled. The initial im-
portance weight D0 for a triplet (q,d+,d−) was set
to the positive difference in relevance scores for d+

and d−. Each bootstrap sample consisted of 10 pairs
of documents for each of 10, 000 queries, resulting
in 100, 000 training instances per sample.

The Boost approach was developed for uni-gram
and combined uni- and bi-gram versions. We ob-
served that the performance of the Boost method
continuously improved with the number of iterations
T and with the number of samples S, but saturated
at about 15-20 samples without visible over-fitting
in the tested range of T . Therefore we arbitrarily
stopped training after obtaining 5, 000 features per
sample, and used 35 samples for uni-gram version
and 65 samples for the combined bi-gram version,
resulting in models with 104K and 172K unique fea-
tures, respectively. The optimal values for the pass-
through weight β were found to be 0.3 and 0.2 for
the uni-gram and bi-gram models on the develop-
ment set. The best configuration of uni-gram and
bi-gram model achieved MAP scores of 0.2064 and
0.2526 the development set. Using stopword filters
during training did not improve the results here.

1695



0.24

0.25

0.26

0.27

0.28

0.29

0.30

0.31

0.32

0.33

0.34

0.35

 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

M
A

P

κ

DT + Boost-2g
PSQ lexical + Boost-2g
PSQ n-best + Boost-2g

DT, 0.2636
PSQ lexical, 0.2520
PSQ n-best, 0.2698

Boost-2g, 0.2526
PSQ n-best + DT

Figure 1: MAP rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.

0.24

0.25

0.26

0.27

0.28

0.29

0.30

0.31

0.32

0.33

0.34

0.35

 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

M
A

P

κ

dev, PSQ n-best + Boost-2g
test, PSQ n-best + Boost-2g

dev, PSQ n-best, 0.2698
test, PSQ n-best, 0.2659

dev, Boost-2g, 0.2526
test, Boost-2g, 0.2474

Figure 2: MAP rank aggregation for the bi-gram boosting
and the “PSQ n-best table” approach on dev and test sets.

0.54

0.56

0.58

0.60

0.62

0.64

0.66

0.68

0.70

0.72

 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1

P
R

E
S

κ

DT + Boost-2g
PSQ lexical + Boost-2g
PSQ n-best + Boost-2g

DT, 0.5669
PSQ lexical, 0.5445
PSQ n-best, 0.5789

Boost-2g, 0.6900
PSQ n-best + DT

Figure 3: PRES rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.

6.2 Testing and Model Combination

The third and the fifth columns of Table 2 give a
comparison of the MAP scores of the baseline ap-
proaches and the Boost model evaluated individu-
ally on the test set. Each score corresponds to the
best configuration found on the development set. We
see that the PSQ approach using n-best lists for pro-
jection outperforms all other methods in terms of
MAP, but loses to both Boost approaches when eval-
uated with PRES. Direct translation is about 1 MAP
point lower than PSQ n-best; Boost with combined
uni- and bi-grams is another 0.8 MAP points worse,
but is better in terms of PRES, especially for the bi-
gram version. Given the fact that the complex SMT
system behind the direct translation and PSQ ap-
proach is trained and tuned on very large in-domain
datasets, the performance of the bare phrase table
induced by the Boost method is respectable.

Our best results are obtained by a combination
of the orthogonal information sources of the SMT
and the Boost approaches. We evaluated the Borda
Count aggregation scheme on the development data
in order to find the optimal value for κ ∈ [0, 1]. The
interpolation was done for the best combined uni-
and bi-gram boosting model with the best variants of
the DT and PSQ approaches. As can be seen from
Figures 1 and 3, rank aggregation by Borda Count
outperforms both individual approaches by a large
margin. Figure 2 verifies that the results are trans-
ferable from the development set to the test set. The
best performing system combination on the develop-
ment data is also optimal on the test data.

Table 3 shows the retrieval performance of the
best baseline model (PSQ n-best) combined with
the best Boost model (bi-gram), with an impres-
sive gain of over 7 MAP points (15 PRES points)
over the best individual baseline result from Table 2.
Even when, according to the PRES measure (Fig-
ure 3), the Boost-2g system is better on its own, in-
jecting complementary information from the PSQ or
DT approach still contributes several points. Simi-
lar gains are obtained by model combination of the
DT approach with the best Boost model. However,
a combination of the SMT-based CLIR approaches
DT and PSQ barely improved results over the best
input model. In summary, aggregating rankings is
helpful for orthogonal systems, but not for systems
including similar information.

1696



6.3 Analysis

Table 4 lists some of the top-200 selected features
for the boosting approach (the most common trans-
lation of the Japanese term is put in subscript).

We see that the direct ranking approach is able
to penalize uni- and bi-gram cooccurrences that are
harmful for retrieval by assigning them a negative
weight, e.g., the pairing of解決resolution with image.
Pairs of uni- and bi-grams that are useful for re-
trieval are boosted by positive weights, e.g., the pair
圧縮compression,機machine and compressor captures an
important compound. Further examples, not shown
in the table, are matches of the same source (tar-
get) n-gram with several different target (source) n-
grams, e.g., the Japanese term 画像image is paired
not only with its main translation, but also with
dozens of related notions: video, picture, scanning,
printing, photosensitive, pixel, background etc. This
has a query expansion effect that is not possible in
systems that use one translation or a small list of n-
best translations. In addition, associations of source
n-grams with overlapping target n-grams help boost
the final score: e.g., the same term画像image is pos-
itively paired with target bi-grams as {an,original},
{original,image} and {image,for}. This has the ef-
fect of compensating for the lack of handling phrase
overlaps in an SMT decoder.

7 Conclusion

We presented a boosting approach to induce a table
of bilingual n-gram correspondences by direct pref-
erence learning on relevance rankings. This table
can be seen as a phrase table that encodes word-
based information that is orthogonal and comple-
mentary to the information in standard translation-
based CLIR approaches. We compared our boosting
approach to very competitive CLIR baselines that
use a complex SMT system trained and tuned on
large in-domain datasets. Furthermore, our patent
retrieval setup gives SMT-based approaches an ad-
vantage in that queries consist of several normal-
length sentences, as opposed to the short queries
common to web search. Despite this and despite the
tiny size (about 170K parameters) of the boosting
phrase table, compared to standard SMT phrase ta-
bles, this approach reached performance similar to
direct translation using a full SMT model in terms

t ht (uni- & bi-grams) wt
1 層layer - layer 1.29
2 データdata - data 1.13
3 回路circuit - circuit 1.13

76 でin - voltage -0.39
77 導guide,電power - conductive 1.25

81 解決resolution - image -0.25

99 変速speed - transmission 1.68
100 液晶LCD - liquid,crystal 1.73

123 力power - force 0.91
124 圧縮compression,機machine - compressor 2.83

132 ケーブルcable - cable 1.81
133 超hyper,音波sound wave - ultrasonic 3.34

169 粒子particle - particles 1.57
170 算出calculation - for,each 1.14

184 ロータrotor - rotor 2.01
185 検出detection,器vessel - detector 1.43

Table 4: Examples of the features found by boosting.

of MAP, and was significantly better in terms of
PRES. Overall, we obtained the best results by a
model combination using consensus- based voting
where the best SMT-based approach was combined
with the boosting phrase table (gaining more than 7
MAP or 15 PRES points). We attribute this to the
fact that the boosting approach augments SMT ap-
proaches with valuable information that is hard to
get in approaches that are agnostic about the rank-
ing data and the ranking task at hand.

The experimental setup presented in this paper
uses relevance links between patent abstracts as
ranking data. While this technique is useful to de-
velop patent retrieval systems, it would be interest-
ing to see if our results transfer to patent retrieval
scenarios where full patent documents are used in-
stead of only abstracts, or to standard CLIR scenar-
ios that use short search queries in retrieval.

Acknowledgements

The research presented in this paper was supported
in part by DFG grant “Cross-language Learning-to-
Rank for Patent Retrieval”. We would like to thank
Eugen Ruppert for his contribution to the ranking
data construction.

1697



References
Javed A. Aslam and Mark Montague. 2001. Models for

metasearch. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR’01), New Orleans, LA.

Bing Bai, Jason Weston, David Grangier, Ronan
Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning to
rank with (a lot of) word features. Information Re-
trieval Journal, 13(3):291–314.

Lisa Ballesteros and W. Bruce Croft. 1998. Resolving
ambiguity for cross-language retrieval. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR’98), Mel-
bourne, Australia.

Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and
Joseph A. Shaw. 1995. Combining the evidence
of multiple query representations for information re-
trieval. Inf. Process. Manage., 31(3):431–448.

Leo Breiman. 1996. Bagging predictors. Journal of Ma-
chine Learning Research, 24:123–140.

Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFad-
den, Ken Goldman, Mike Gunter, Jeremiah Harm-
sen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret
Llinares, Indraneel Mukherjee, Fernando Pereira, Josh
Redstone, Tal Shaked, and Yoram Singer. 2010.
Sibyl: A system for large scale machine learning.
In LADIS: The 4th ACM SIGOPS/SIGACT Workshop
on Large Scale Distributed Systems and Middleware,
Zurich, Switzerland.

Xi Chen, Bing Bai, Yanjun Qi, Qihang Ling, and Jaime
Carbonell. 2010. Learning preferences with millions
of parameters by enforcing sparsity. In Proceedings
of the IEEE International Conference on Data Mining
(ICDM’10), Sydney, Australia.

David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.

Jeffrey Chin, Maureen Heymans, Alexandre Kojoukhov,
Jocelyn Lin, and Hui Tan. 2008. Cross-language
information retrieval. Patent Application. US
2008/0288474 A1.

Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25–69.

Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, AdaBoost and Bregman
distances. Journal of Machine Learning Research,
48(1-3):253–285.

Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings. of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR’03), Toronto,
Canada.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, Upp-
sala, Sweden.

Minwei Feng, Christoph Schmidt, Joern Wuebker,
Stephan Peitz, Markus Freitag, and Hermann Ney.
2011. The RWTH Aachen system for NTCIR-9
PatentMT. In Proceedings of the NTCIR-9 Workshop,
Tokyo, Japan.

Yoav Freund, Ray Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning
Research, 4:933–969.

Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of NTCIR-7 Workshop Meeting, Tokyo, Japan.

Yasser Ganjisaffar, Rich Caruana, and Cristina Videira
Lopes. 2011. Bagging gradient-boosted trees for
high precision, low variance ranking models. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR’11),
Beijing, China.

Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang,
Ming Zhou, and Changning Huang. 2001. Improv-
ing query translation for cross-language information
retrieval using statistical models. In Proceedings of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR’01), New Or-
leans, LA.

Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
lingual query suggestion using query logs of different
languages. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR’07), Amsterdam, The Netherlands.

Sharad Goel, John Langford, and Alexander L. Strehl.
2008. Predictive indexing for fast search. In Advances
in Neural Information Processing Systems, Vancouver,
Canada.

Erik Graf and Leif Azzopardi. 2008. A methodology
for building a patent test collection for prior art search.
In Proceedings of the 2nd International Workshop on
Evaluating Information Access (EVIA), Tokyo, Japan.

David Grangier and Samy Bengio. 2008. A discrimi-
native kernel-based approach to rank images from text
queries. IEEE Transactions on Pattern Analysis and
Machine Intelligence (PAMI), 30(8):1371–1384.

Yunsong Guo and Carla Gomes. 2009. Ranking struc-
tured documents: A large margin based approach for

1698



patent prior art search. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI’09), Pasadena, CA.

Kenneth Heafield. 2011. KenLM: faster and smaller lan-
guage model queries. In Proceedings of the EMNLP
2011 Sixth Workshop on Statistical Machine Transla-
tion (WMT’11), Edinburgh, UK.

Gareth J.F. Jones and Adenike M. Lam-Adesina. 2002.
Combination methods for improving the reliability of
machine translation based cross-language information
retrieval. In Proceedings of the 13th Irish Interna-
tional Conference on Artificial Intelligence and Cog-
nitive Science (AICS’02), Limerick, Ireland.

Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
Conference on European Chapter of the Association
for Computational Linguistics (EACL’03), Budapest,
Hungary.

Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceed-
ings of the ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR’02), Tampere,
Finland.

Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007), Prague,
Czech Republic.

Walid Magdy and Gareth J.F. Jones. 2010. PRES: a
score metric for evaluating recall-oriented information
retrieval applications. In Proceedings of the ACM SI-
GIR conference on Research and development in in-
formation retrieval (SIGIR’10), New York, NY.

Walid Magdy and Gareth J. F. Jones. 2011. An efficient
method for using machine translation technologies in
cross-language patent search. In Proceedings of the
20th ACM Conference on Informationand Knowledge
Management (CIKM’11), Glasgow, Scotland, UK.

Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos,
and Christof Monz. 2012. Adaptation of statistical
machine translation model for cross-lingual informa-
tion retrieval in a service context. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics (EACL’12),
Avignon, France.

Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.

Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Meeting of the Association for Computational Linguis-
tics (ACL’00), Hongkong, China.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting on Association for Computational Lin-
guistics (ACL’03), Sapporo, Japan.

Dmitry Pavlov, Alexey Gorodilov, and Cliff A. Brunk.
2010. Bagboo: a scalable hybrid bagging-the-
boosting model. In Proceedings of the 19th ACM
International Conference on Information and Knowl-
edge Management (CIKM’10), Toronto, Canada.

Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Conference
(TREC-7), Gaithersburg, MD.

Saharon Rosset, Ji Zhu, and Trevor Hastie. 2004. Boost-
ing as a regularized path to a maximum margin clas-
sifier. Journal of Machine Learning Research, 5:941–
973.

Robert E. Schapire and Yoram Singer. 1999. Im-
proved boosting algorithms using confidence-rated
predictions. Journal of Machine Learning Research,
37(3):297–336.

Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alexander J. Smola, Alexander L. Strehl, and
Vishy Vishwanathan. 2009. Hash Kernels. In Pro-
ceedings of the 12th Int. Conference on Artificial In-
telligence and Statistics (AISTATS’09), Irvine, CA.

Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests for
information retrieval evaluation. In Proceedings of the
16th ACM conference on Conference on Information
and Knowledge Management (CIKM ’07), New York,
NY.

Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Combining statistical translation techniques for cross-
language information retrieval. In Proceedings of the
International Conference on Computational Linguis-
tics (COLING 2012), Bombay, India.

Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012b.
Looking inside the box: Context-sensitive translation
for cross-language information retrieval. In Proceed-
ings of the ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2012),
Portland, OR.

Masao Utiyama and Hitoshi Isahara. 2007. A Japanese-
English patent parallel corpus. In Proceedings of MT
Summit XI, Copenhagen, Denmark.

Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of the ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR’01), New York, NY.

1699


