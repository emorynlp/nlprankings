Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387,

Beijing, August 2010

1379

Syntactic Scope Resolution in Uncertainty Analysis

Lilja Øvrelid♠♣ and Erik Velldal♣ and Stephan Oepen♣

♣ University of Oslo, Department of Informatics

♠Universität Potsdam, Institut für Linguistik

ovrelid@uni-potsdam.de and erikve@ifi.uio.no and oe@ifi.uio.no

Abstract

We show how the use of syntactic struc-
ture enables the resolution of hedge scope
in a hybrid,
two-stage approach to un-
certainty analysis.
In the ﬁrst stage, a
Maximum Entropy classiﬁer, combining
surface-oriented and syntactic features,
identiﬁes cue words. With a small set of
hand-crafted rules operating over depen-
dency representations in stage two, we at-
tain the best overall result (in terms of
both combined ranks and average F1) in
the 2010 CoNLL Shared Task.

1 Background—Motivation

Recent years have witnessed an increased interest
in the analysis of various aspects of sentiment in
natural language (Pang & Lee, 2008). The sub-
task of hedge resolution deals with the analysis of
uncertainty as expressed in natural language, and
the linguistic means (so-called hedges) by which
speculation or uncertainty are expressed.
Infor-
mation of this kind is of importance for various
mining tasks which aim at extracting factual data.
Example (1),
taken from the BioScope corpus
(Vincze, Szarvas, Farkas, Móra, & Csirik, 2008),
shows a sentence where uncertainty is signaled by
the modal verb may.1

(1)

{The unknown amino acid hmayi be used by these
species}.

The topic of the Shared Task at the 2010 Con-
ference for Natural Language Learning (CoNLL)
is hedge detection in biomedical literature—in a
sense ‘zooming in’ on one particular aspect of the
broader BioNLP Shared Task in 2009 (Kim, Ohta,
Pyysalo, Kano, & Tsujii, 2009). It involves two
subtasks: Task 1 is described as learning to detect

1In examples throughout this paper, angle brackets high-
light hedge cues, and curly braces indicate the scope of a
given cue, as annotated in BioScope.

sentences containing uncertainty; the objective of
Task 2 is learning to resolve the in-sentence scope
of hedge cues (Farkas, Vincze, Mora, Csirik, &
Szarvas, 2010). The organizers further suggest:
This task falls within the scope of semantic analy-
sis of sentences exploiting syntactic patterns [...].
The utility of syntactic information within var-
ious approaches to sentiment analysis in natu-
ral language has been an issue of some debate
(Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta,
& Ariﬁn, 2006), and the potential contribution of
syntax clearly varies with the speciﬁcs of the task.
Previous work in the hedging realm has largely
been concerned with cue detection, i.e. identify-
ing uncertainty cues such as may in (1), which
are predominantly individual tokens (Medlock &
Briscoe, 2007; Kilicoglu & Bergler, 2008). There
has been little previous work aimed at actually
resolving the scope of such hedge cues, which
presumably constitutes a somewhat different and
likely more difﬁcult problem. Morante and Daele-
mans (2009) present a machine-learning approach
to this task, using token-level, lexical informa-
tion only. To this end, CoNLL 2010 enters largely
uncharted territory, and it remains to be seen (a)
whether syntactic analysis indeed is a necessary
component in approaching this task and, more
generally, (b) to what degree the speciﬁc task
setup can inform us about the strong and weak
points in current approaches and technology.

In this article, we investigate the contribution
of syntax to hedge resolution, by reﬂecting on our
experience in the CoNLL 2010 task.2 Our CoNLL
system submission ranked fourth (of 24) on Task 1
and third (of 15) on Task 2, for an overall best av-
erage result (there appears to be very limited over-
lap among top performers for the two subtasks).

2It turns out, in fact, that all the top-performing systems
in Task 2 of the CoNLLShared Task rely on syntactic informa-
tion provided by parsers, either in features for machine learn-
ing or as input to manually crafted rules (Morante, Asch, &
Daelemans, 2010; Rei & Briscoe, 2010).

1380

Sentences Hedged
Sentences

Cues Multi-Word Tokens Cue Tokens

Abstracts
Articles
Total

11871
2670
14541

2101
519
2620

2659
668
3327

Cues

364
84
448

309634
68579
378213

3056
782
3838

Table 1: Summary statistics for the Shared Task training data.

This article transcends our CoNLL system descrip-
tion (Velldal, Øvrelid, & Oepen, 2010) in several
respects, presenting updated and improved cue de-
tection results (§ 3 and § 4), focusing on the role
of syntactic information rather than on machine
learning speciﬁcs (§ 5 and § 6), providing an anal-
ysis and discussion of Task 2 errors (§ 7), and gen-
erally aiming to gauge the value of available anno-
tated data and processing tools (§ 8). We present
a hybrid, two-level approach for hedge resolution,
where a statistical classiﬁer detects cue words, and
a small set of manually crafted rules operating
over syntactic structures resolve scope. We show
how syntactic information—produced by a data-
driven dependency parser complemented with in-
formation from a ‘deep’, hand-crafted grammar—
contributes to the resolution of in-sentence scope
of hedge cues, discussing various types of syn-
tactic constructions and associated scope detec-
tion rules in considerable detail. We furthermore
present a manual error analysis, which reveals re-
maining challenges in our scope resolution rules
as well as several relevant idiosyncrasies of the
preexisting BioScope annotation.

2 Task, Data, and System Basics

Task Deﬁnition and Evaluation Metrics
Task 1 is a binary sentence classiﬁcation task:
identifying utterances as being certain or uncer-
tain. Following common practice, this subtask
is evaluated in terms of precision, recall, and
F1 for the ‘positive’ class,
In
our work, we approach Task 1 as a byproduct
of the full hedge resolution problem, labeling a
sentence as uncertain if it contains at least one
token classiﬁed as a hedge cue.
In addition to
the sentence-level evaluation for Task 1, we also
present precision, recall, and F1 for the cue-level.
Task 2 comprises two subtasks: cue detection
and scope resolution. The ofﬁcial CoNLL eval-

i.e. uncertain.

uation does not tease apart these two aspects of
the problem, however: Only an exact match of
both the cue and scope bracketing (in terms of
substring positions) will be counted as a success,
again quantiﬁed in terms of precision, recall, and
F1. Discussing our results below, we report cue
detection and scope resolution performance sepa-
rately, and further put scope results into perspec-
tive against an upper bound based on the gold-
standard cue annotation.

Besides the primary biomedical domain data,
some annotated Wikipedia data was provided
for Task 1, and participating systems are classi-
ﬁed as in-domain (using exclusively the domain-
speciﬁc data), cross-domain (combining both
types of training data), or open (utilizing addi-
tional uncertainty-related resources). In our work,
we focus on the interplay of syntax and the more
challenging Task 2; we ignored the Wikipedia
track in Task 1. Despite our using general NLP
tools (see below), our system falls into the most
restrictive, in-domain category.

Training and Evaluation Data The training
data for the CoNLL 2010 Shared Task is taken from
the BioScope corpus (Vincze et al., 2008) and
consists of 14,541 ‘sentences’ (or other root-level
utterances) from biomedical abstracts and articles
(see Table 1).3 The BioScope corpus provides
annotation for hedge cues as well as their scope.
According to the annotation guidelines (Vincze et
al., 2008), the annotation adheres to a principle
of minimalism when it comes to hedge cues, i.e.
the minimal unit expressing hedging is annotated.
The inverse is true of scope annotations, which ad-
here to a principle of maximal scope—meaning
that scope should be set to the largest syntactic

3As it was known beforehand that evaluation would draw
on full articles only, we put more emphasis on the article
subset of the training data, for example in cross validation
testing and manual diagnosis of errors.

1381

LEMMA
the

POS FEATS
DT _

HEAD DEPREL XHEAD XDEP
ID FORM
4
1 The
4
2 unknown unknown JJ degree:attributive
4
JJ degree:attributive
3 amino
NN pers:3|case:nom|num:sg|ntype:common
4 acid
5
MD mood:ind|subcat:MODAL|tense:pres|clauseType:decl 0
5 may
5
VB _
6 be
7 used
VBN subcat:V-SUBJ-OBJ|vtype:main|passive:+
6
7
IN _
8 by
9 these
DT deixis:proximal
10
10 species specie NNS num:pl|pers:3|case:obl|common:count|ntype:common 8
11 .
5

NMOD
NMOD
NMOD
SBJ
ROOT
VC
VC
LGS
NMOD
PMOD
P

SPECDET
ADJUNCT
ADJUNCT
SUBJ
ROOT
PHI
XCOMP
PHI
SPECDET
OBL-AG
PUNC

amino
acid
may
be
use
by
these

4
4
4
3
0
7
5
9
10
7
0

.

.

_

Table 2: Stacked dependency representation of example (1), with MaltParser and XLE annotations.

unit possible.

For evaluation purposes,

the task organizers
provided newly annotated biomedical articles, fol-
lowing the same general BioScope principles. The
CoNLL 2010 evaluation data comprises 5,003 ad-
ditional utterances (138,276 tokens), of which 790
are annotated as hedged. The data contains a to-
tal of 1033 cues, of which 87 are so-called multi-
word cues (i.e. cues spanning multiple tokens),
comprising 1148 cue tokens altogether.

Stacked Dependency Parsing For syntactic
analysis we employ the open-source MaltParser
(Nivre, Hall, & Nilsson, 2006), a platform for
data-driven dependency parsing. For improved
accuracy and portability across domains and gen-
res, we make our parser incorporate the pre-
dictions of a large-scale, general-purpose LFG
parser—following the work of Øvrelid, Kuhn, and
Spreyer (2009). A technique dubbed parser stack-
ing enables the data-driven parser to learn, not
only from gold standard treebank annotations, but
from the output of another parser (Nivre & Mc-
Donald, 2008). This technique has been shown to
provide signiﬁcant improvements in accuracy for
both English and German (Øvrelid et al., 2009),
and a similar setup employing an HPSG gram-
mar has been shown to increase domain indepen-
dence in data-driven dependency parsing (Zhang
& Wang, 2009). The stacked parser combines
two quite different approaches—data-driven de-
pendency parsing and ‘deep’ parsing with a hand-
crafted grammar—and thus provides us with a
broad range of different types of linguistic infor-
mation for the hedge resolution task.

MaltParser is based on a deterministic pars-
ing strategy in combination with treebank-induced
classiﬁers for predicting parse transitions. It sup-
ports a rich feature representation of the parse his-

tory in order to guide parsing and may easily be
extended to take additional features into account.
The procedure to enable the data-driven parser
to learn from the grammar-driven parser is quite
simple. We parse a treebank with the XLE plat-
form (Crouch et al., 2008) and the English gram-
mar developed within the ParGram project (Butt,
Dyvik, King, Masuichi, & Rohrer, 2002). We
then convert the LFG output to dependency struc-
tures, so that we have two parallel versions of the
treebank—one gold standard and one with LFG
annotation. We extend the gold standard treebank
with additional information from the correspond-
ing LFG analysis and train MaltParser on the en-
hanced data set.

Table 2 shows the enhanced dependency rep-
resentation of example (1) above, taken from the
training data. For each token, the parsed data con-
tains information on the word form, lemma, and
part of speech (PoS), as well as on the head and
dependency relation in columns 6 and 7. The
added XLE information resides in the F E A T S col-
umn, and in the XLE-speciﬁc head and depen-
dency columns 8 and 9. Parser outputs, which in
turn form the basis for our scope resolution rules
discussed in Section 5, also take this same form.
The parser employed in this work is trained on
the Wall Street Journal sections 2 – 24 of the Penn
Treebank (PTB), converted to dependency format
(Johansson & Nugues, 2007) and extended with
XLE features, as described above. Parsing uses the
arc-eager mode of MaltParser and an SVM with
a polynomial kernel. When tested using 10-fold
cross validation on the enhanced PTB, the parser
achieves a labeled accuracy score of 89.8.

PoS Tagging and Domain Variation Our
parser is trained on ﬁnancial news, and although
stacking with a general-purpose LFG parser is ex-

1382

pected to aid domain portability, substantial dif-
ferences in domain and genre are bound to neg-
atively affect syntactic analysis (Gildea, 2001).
MaltParser presupposes that inputs have been PoS
tagged, leaving room for variation in preprocess-
ing. On the one hand, we aim to make parser
inputs maximally similar to its training data (i.e.
the conventions established in the PTB); on the
other hand we wish to beneﬁt from specialized re-
sources for the biomedical domain.

The GENIA tagger (Tsuruoka et al., 2005) is
particularly relevant in this respect (as could be
the GENIA Treebank proper4). However, we
found that GENIA tokenization does not match the
PTB conventions in about one out of ﬁve sen-
tences (for example wrongly splitting tokens like
‘390,926’ or ‘Ca(2+)’); also in tagging proper
nouns, GENIA systematically deviates from the
PTB. Hence, we adapted an in-house tokenizer
(using cascaded ﬁnite-state rules) to the CoNLL
task, run two PoS taggers in parallel, and eclec-
tically combine annotations across the various
preprocessing components—predominantly giv-
ing precedence to GENIA lemmatization and PoS
hypotheses.

To assess the impact of improved, domain-
adapted inputs on our hedge resolution system,
we contrast two conﬁgurations: ﬁrst, running the
parser in the exact same manner as Øvrelid, Kuhn,
and Spreyer (2010), we use TreeTagger (Schmid,
1994) and its standard model for English (trained
on the PTB) for preprocessing; second, we give as
inputs to the parser our reﬁned tokenization and
merged PoS tags, as described above. When eval-
uating the two modes of preprocessing on the ar-
ticles subset of the training data, and using gold-
standard cues, our system for resolving cue scopes
(presented in § 5) achieves an F1 of 66.31 with
TreeTagger inputs, and 72.30 using our reﬁned to-
kenization and tagger combination. These results
underline the importance of domain adaptation for
accurate syntactic analysis, and in the following
we assume our hybrid in-house setup.

4Although the GENIA Treebank provides syntactic anno-
tation in a form inspired by the PTB, it does not provide func-
tion labels. Therefore, our procedure for converting from
constituency to dependency requires non-trivial adaptation
before we can investigate the effects of retraining the parser
against GENIA.

3 Stage 1: Identifying Hedge Cues

For the task of identifying hedge cues, we devel-
oped a binary maximum entropy (MaxEnt) clas-
siﬁer. The identiﬁcation of cue words is used
for (a) classifying sentences as certain/uncertain
(Task 1), and (b) providing input to the syntac-
tic rules that we later apply for resolving the in-
sentence scope of the cues (Task 2). We also re-
port evaluation scores for the sub-task of cue de-
tection in isolation.

As annotated in the training data, it is possible
for a hedge cue to span multiple tokens, e.g. as in
whether or not. The majority of the multi-word
cues in the training data are very infrequent, how-
ever, most occurring only once, and the classiﬁer
itself is not sensitive to the notion of multi-word
cues. Instead, the task of determining whether a
cue word forms part of a larger multi-word cue, is
performed in a separate post-processing step (ap-
plying a heuristic rule targeted at only the most
frequently occurring patterns of multi-word cues
in the training data).

During development, we trained cue classiﬁers
using a wide variety of feature types, both syn-
tactic and surface-oriented. In the end, however,
we found n-gram-based lexical features to have
the greatest contribution to classiﬁer performance.
Our best-performing classiﬁer so far (see ‘Final’
in Table 3) includes the following feature types:
n-grams over forms (up to 2 tokens to the right),
n-grams over base forms (up to 3 tokens left
and right), PoS (from GENIA), subcategorization
frames (from XLE), and phrase-structural coordi-
nation level (from XLE). Our CoNLL system de-
scription includes more details of the various other
feature types that we experimented with (Velldal
et al., 2010).

4 Cue Detection Evaluation

Table 3 summarizes the performance of our Max-
Ent hedge cue classiﬁer in terms of precision, re-
call and F1, computed using the ofﬁcial Shared
Task scorer script. The sentence-level scores cor-
respond to Task 1 of the Shared Task, and the cue-
level scores are based on the exact-match counts
for full hedge cues (possibly spanning multiple to-
kens).

1383

Conﬁguration

Baseline, Development
Final, Development
Final, Held-Out

Prec

Rec

Sentence Level
F1
79.20
89.00
85.33

79.45
86.78
85.06

79.25
91.39
85.61

Cue Level

Prec

Rec

77.37
90.18
81.97

71.70
79.47
76.41

F1
74.43
84.49
79.10

Table 3: Isolated evaluation of the hedge cue classiﬁer.

As the CoNLL test data was known beforehand
to consist of articles only, in 10-fold cross vali-
dation for classiﬁer development we tested exclu-
sively against the articles segment, while always
including all sentences from the abstracts in the
training set. This corresponds to the development
results in Table 3, while the held-out results are
for the ofﬁcial Shared Task evaluation data (train-
ing on all the available training data). A model
using only unigram features serves as a baseline.

5 Stage 2: Resolving Scope

Hedge scope may vary quite a lot depending on
linguistic properties of the cue in question. In our
approach to scope resolution we rely heavily on
syntactic information, taken from the dependency
structures proposed by both MaltParser and XLE,
as well as on various additional features relating
to speciﬁc syntactic constructions.

We constructed a small set of heuristic rules
which deﬁne the scope for each cue detected in
Stage 1. In developing these rules, we made use
of the information provided by the guidelines for
scope annotation in the BioScope corpus (Vincze
et al., 2008), combined with manual inspection of
the training data in order to further generalize over
the phenomena discussed by Vincze et al. (2008)
and work out interactions of constructions for var-
ious types of cues.

The rules take as input a parsed sentence which
has been further tagged with hedge cues. They
operate over the dependency structures and ad-
ditional features provided by the parser. Default
scope is set to start at the cue word and span to
the end of the sentence (modulo punctuation), and
this scope also provides the baseline for the eval-
uation of our rules. In the following, we discuss
broad classes of rules, organized by categories of
hedge cues. As there is no explicit representa-
tion of phrase or clause boundaries in our depen-

dency universe, we assume a set of functions over
dependency graphs, for example ﬁnding the left-
or rightmost (direct) dependent of a given node,
or transitively selecting left- or rightmost descen-
dants.

Coordination The dependency analysis of co-
ordination provided by our parser makes the ﬁrst
conjunct the head of the coordination. For cues
that are coordinating conjunctions (PoS tag CC),
such as or, we deﬁne the scope as spanning the
whole coordinate structure, i.e. start scope is set
to the leftmost dependent of the head of the coor-
dination, e.g., roX in (2), and end scope is set to
its rightmost dependent (conjunct), e.g., RNAs in
(2). This analysis provides us with coordinations
at various syntactic levels, such as NP and N (2),
AP and AdvP, or VP (3):

(2)

(3)

[...] the {roX genes hori RNAs} recruit the entire set
of MSL proteins [...]

[...] the binding interfaces are more often {kept hori
even reused} rather than lost in the course of
evolution.

Adjectives We distinguish between adjectives
(JJ) in attributive (N M O D) function and adjectives
in predicative (P R D) function. Attributive adjec-
tives take scope over their (nominal) head, with all
its dependents, as in (4) and (5):

(4) The {hpossiblei selenocysteine residues} are shown

in red, [...]

(5) Extensive analysis of the ﬂanks failed to show any
hallmarks of {hputativei transposons that might be
associated with this RAG1-like protein}, [...]

For adjectives in a predicative function the scope
includes the subject argument of the head verb
(the copula), as well as a (possible) clausal argu-
ment, as in (6). The scope does not, however, in-
clude expletive subjects, as in (7).

1384

(7)

(6) Therefore, {the unknown amino acid, if it is encoded
by a stop codon, is hunlikelyi to exist in the current
databases of microbial genomes}.
For example, it is quite {hlikelyi that there exists an
extremely long sequence that is entirely unique to U}.
Verbs The scope of verbal cues is a bit more
complex and depends on several factors. In our
rules, we distinguish passive usages from active
usages, raising verbs from non-raising verbs, and
the presence or absence of a subject-control em-
bedding context. The scopes of both passive and
raising verbs include the subject argument of their
head verb, as in (8) and (9), unless it is an exple-
tive pronoun, as in (10).

(8)

(9)

(10)

{Interactions determined by high-throughput methods
are generally hconsideredi to be less reliable than
those obtained by low-throughput studies} 1314 and
as a consequence [...]

{Genomes of plants and vertebrates hseemi to be free
of any recognizable Transib transposons} (Figure 1).
It has been {hsuggestedi that unstructured regions of
proteins are often involved in binding interactions,
particularly in the case of transient interactions} 77.

In the case of subject control involving a hedge
cue, speciﬁcally modals, subject arguments are in-
cluded in scopes where the controller heads a pas-
sive construction or a raising verb, as in exam-
ple (1) above, repeated here for convenience:

(11) {The unknown amino acid hmayi be used by these

species}.

In general, the end scope of verbs should ex-
tend over the minimal clause that contains the verb
in question.
In terms of dependency structures,
we deﬁne the clause boundary as comprising the
chain of descendants of a verb which is not inter-
vened by a token with a higher attachment in the
graph than the verb in question. In example (8)
for instance, the sentence-level conjunction and
marks the end of the clause following the cue con-
sidered.

Prepositions and Adverbs Cues that are tagged
as prepositions (including some complementizers)
take scope over their argument, with all its de-
scendants, (12). Adverbs take scope over their
head with all its (non-subject) syntactic descen-
dants (13).

Conﬁguration

P
S
B

Default, Gold Cues
Rules, Gold Cues
Rules, System Cues

E Rules, Gold Cues

S
B

Rules, System Cues

F1
45.21
72.31
64.77

66.73
55.75

Table 4: Evaluation of scope resolution rules.

(12) {hWhetheri the codon aligned to the inframe stop
codon is a nonsense codon or not} was neglected at
this stage.

(13) These effects are {hprobablyi mediated through the

1,25(OH)2D3 receptor}.

Multi-Word Cues
In the case of multi-word
cues, such as indicate that or either ... or, we need
to determine the head of the multi-word unit. We
then set the scope of the whole unit to the scope
of the head token.

As an illustration of rule processing, consider
our running example (11), with its syntactic anal-
ysis as shown in Table 2 above. This example
invokes a variety of syntactic properties, includ-
ing parts of speech, argumenthood, voice etc. Ini-
tially, the scope of the hedge cue is set to default
scope. Then the subject control rule is applied,
which checks the properties of the verbal argu-
ment used, going through a chain of verbal depen-
dents from the modal verb. Since it is marked as
passive in the LFG analysis, the start scope is set to
include the subject of the cue word (the leftmost
descendant in its S B J dependent).

6 Rule Evaluation

Table 4 summarizes scope resolution performance
(viewed as a an isolated subtask) for various con-
ﬁgurations, both against the articles section of the
CoNLL training data (dubbed BSP) and against the
held-out evaluation data (BSE). First of all, we note
that the ‘default scope’ baseline is quite strong:
unconditionally extending the scope of a cue to
the end of the sentence yields an F1 of 45.21.
Given gold standard cue information, our scope
rules improve on the baseline by 27 points on the
articles section of the data set, for an F1 of 72.31;
with system-assigned hedge cues, our rules still

1385

achieve an F1 of 64.77. Note that scope resolu-
tion scores based on classiﬁed cues also yield the
end-to-end system evaluation for Task 2.

The bottom rows of Table 4 show the evaluation
of scope rules on the CoNLL held-out test data. Us-
ing system cues, scope resolution on the held-out
data scores at 55.75 F1. Comparing to the result
on the (articles portion of the) training data, we
observe a substantial drop in performance (of six
points with gold-standard cues, nine points with
system cues). There are several possible explana-
tions for this effect. First of all, there may well
be a certain degree of overﬁtting of our rules to
the training data. The held-out data may contain
hedging constructions that are not covered by our
current set of scope rules, or annotation of parallel
constructions may in some cases differ in subtle
ways (see § 7 below). Moreover, scope resolution
performance is of course inﬂuenced by cue detec-
tion (see Table 3). The cue-level F1 of our sys-
tem on the held-out data set is 79.10, compared to
84.49 (using cross validation) on the training data.
This drop in cue-level performance appears to af-
fect classiﬁcation precision far more than recall.
Of course, given that our heuristics for identifying
multi-word cues were based on patterns extracted
from the training data, some loss in the cue-level
score was expected.

7 Error Analysis

To start shedding some light on the signiﬁcance
of our results, we performed a manual error anal-
ysis on the article portion of the training material
(BSP), with two of the authors (trained linguists)
working in tandem. Using gold-standard cues,
our scope resolution rules fail to exactly replicate
the target annotation in 185 (of 668) cases, corre-
sponding to 72.31 F1 in Table 4 above. Our eval-
uators reviewed and discussed these 185 cases,
classifying 156 (84 %) as genuine system errors,
22 (12 %) as likely5 annotation errors, and a re-

5In some cases, there is no doubt that annotation is er-
roneous, i.e. in violation of the available annotation guide-
lines (Vincze et al., 2008) or in conﬂict with otherwise un-
ambiguous patterns. In other cases, however, judgments are
necessarily based on generalizations made by the evaluators,
i.e. assumptions about the underlying system and syntactic
analyses implicit in the BioScope annotations. Furthermore,
selecting items for manual analysis that do not align with the

maining seven cases as involving controversial or
seemingly arbitrary decisions.

The two most frequent classes of system er-
rors pertain (a) to the recognition of phrase and
clause boundaries and (b) to not dealing success-
fully with relatively superﬁcial properties of the
text. Examples (14) and (15) illustrate the ﬁrst
class of errors, where in addition to the gold-
standard annotation we use vertical bars (‘|’) to
indicate scope predictions of our system.

(14)

[...] {the reverse complement |mR of m will be
hconsideredi to be [...]|}

(15) This |{hmighti affect the results} if there is a
systematic bias on the composition of a protein
interaction set|.

In our syntax-driven approach to scope resolution,
system errors will almost always correspond to a
failure in determining constituent boundaries, in a
very general sense. However, speciﬁcally exam-
ple (15) is indicative of a key challenge in this
task, where adverbials of condition, reason, or
contrast frequently attach within the dependency
domain of a hedge cue, yet are rarely included in
the scope annotation.

Example (16) demonstrates our second fre-
quent class of system errors. One in six items
in the BSP training data contains a sentence-ﬁnal
parenthesized element or trailing number, as for
example (2), (9), or (10) above; most of these are
bibliographic or other in-text references, which
are never included in scope annotation. Hence,
our system includes a rule to ‘back out’ from trail-
ing parentheticals;
in examples like (16), how-
ever, syntax does not make explicit the contrast
between an in-text reference vs. another type of
parenthetical.

(16) More speciﬁcally, {|the bristle and leg phenotypes are
hlikelyi to result from reduced signaling by Dl| (and
not by Ser)}.

Moving on to apparent annotation errors, the
rules for inclusion (or not) of the subject
in
the scope of verbal hedge cues and decisions
on boundaries (or internal structure) of nominals

predictions made by our scope resolution rules is likely to
bias our sample, such that our estimated proportion of 12 %
annotation errors cannot be used to project an overall error
rate.

1386

seem problematic—as illustrated in examples (17)
to (22).6

(17)

(18)

(19)

[...] and |this is also {hthoughti to be true for the full
protein interaction networks we are modeling}|.
[...] {Neur |hcani promote Ser signaling|}.
|Some of the domain pairs {hseemi to mediate a large
number of protein interactions, thus acting as reusable
connectors}|.

(20) One {|hpossiblei explanation| is functional
redundancy with the mouse Neur2 gene}.
[...] |redeﬁnition of {one of them is hfeasiblei}|.
|The {Bcl-2 family happearsi to function [...]}|.

(22)

(21)

Finally, the difﬁcult corner cases invoke non-
constituent coordination, ellipsis, or NP-initial fo-
cus adverbs—and of course interactions of the
phenomena discussed above. Without making the
syntactic structures assumed explicit, it is often
very difﬁcult to judge such items.

8 Reﬂections — Outlook

Our combination of stacked dependency parsing
and hand-crafted scope resolution rules proved
adequate for the CoNLL 2010 competition, con-
ﬁrming the central role of syntax in this task.
With a comparatively small set of rules (imple-
mented in a few hundred lines of code), con-
structed through roughly two full weeks of ef-
fort (studying BioScope annotations and develop-
ing rules), our CoNLL system achieved an end-to-
end F1 of 55.33 on Task 2.7 The two submis-
sions with better results (at 57.32 and 55.65 F1)
represent groups who have pioneered the hedge
analysis task in previous years (Morante et al.,
2010; Rei & Briscoe, 2010). Scores for other ‘in-
domain’ participants range from 52.24 to 2.15 F1.
6Like in the presentation of system errors, we include
scope predictions of our own rules here too, which we be-
lieve to be correct in these cases. Also in this class of errors,
we ﬁnd the occasional ‘uninteresting’ mismatch, for exam-
ple related to punctuation marks and inconsistencies around
parentheses.

7In § 4 and § 6 above, we report scores for a slightly im-
proved version of our system, where (after the ofﬁcial CoNLL
submission date) we eliminated a bug related to the treatment
of sentence-initial whitespace in the XML annotations. At an
end-to-end F1 of 55.75, this system would outrank the sec-
ond best performer in Task 2.

Doubtless there is room for straightforward exten-
sion: for example retraining our parser on the GE-
NIA Treebank, further improving the cue classiﬁer,
and reﬁning scope resolution rules in the light of
the error analysis above.

At

the same time, we remain mildly am-
bivalent about the long-term impact of some of
the speciﬁcs of the 2010 CoNLL task. Shared
tasks (i.e. system bake-offs) have become increas-
ingly popular in past years, and in some sub-
ﬁelds (e.g. IE, SMT, or dependency parsing) high-
visibility competitions can shape community re-
search agendas. Hence, even at this early stage, it
seems appropriate to reﬂect on the possible con-
clusions to be drawn from the 2010 hedge res-
olution task. First, we believe the harsh ‘exact
substring match’ evaluation metric underestimates
the degree to which current technology can solve
this problem; furthermore, idiosyncratic, string-
level properties (e.g. the exact treatment of punc-
tuation or parentheticals) may partly obscure the
interpretation of methods used and corresponding
system performance.

These effects are compounded by some con-
cerns about the quality of available annotation.
Even though we tried ﬁne-tuning our cross vali-
dation testing to the nature of the evaluation data
(comprising only articles), our system performs
substantially worse on the newly annotated CoNLL
test data, in both stages.8 In our view, the anno-
tation of hedge cues and scopes ideally would be
overtly related to at least some level of syntactic
annotation—as would in principle be possible for
the segment of BioScope drawing on the abstracts
of the GENIA Treebank.

Acknowledgements

We are grateful to the organizers of the 2010
CoNLL Shared Task and creators of the BioScope
resource; ﬁrst, for engaging in these kinds of com-
munity service, and second for many in-depth dis-
cussions of annotation and task details. We thank
our colleagues at the Universities of Oslo and
Potsdam for their comments and support.

8We are leaving open the possibility to further reﬁne our
system; we have therefore abstained from an error analysis
on the evaluation data so far.

1387

mars. In Proceedings of the 47th Meeting of the Associ-
ation for Computational Linguistics (pp. 37 – 40). Singa-
pore.

Øvrelid, L., Kuhn, J., & Spreyer, K.

(2010). Cross-
framework parser stacking for data-driven dependency
parsing. TAL 2010 special issue on Machine Learning
for NLP, 50(3).

Pang, B., & Lee, L.

(2008). Opinion mining and senti-
ment analysis. Foundations and Trends in Information
Retrieval, 2(1-2).

Rei, M., & Briscoe, T. (2010). Combining manual rules and
supervised learning for hedge cue and scope detection. In
Proceedings of the 14th Conference on Natural Language
Learning (pp. 56 – 63). Uppsala, Sweden.

Schmid, H. (1994). Probabilistic part-of-speech tagging us-
ing decision trees.
In International conference on new
methods in language processing (p. 44-49). Manchester,
England.

Tsuruoka, Y., Tateishi, Y., Kim, J.-D., Ohta, T., McNaught,
(2005). Developing a robust
J., Ananiadou, S., et al.
Part-of-Speech tagger for biomedical text. In Advances in
informatics (pp. 382 – 392). Berlin, Germany: Springer.
(2010). Resolving
speculation: MaxEnt cue classiﬁcation and dependency-
based scope rules. In Proceedings of the 14th Conference
on Natural Language Learning. Uppsala, Sweden.

Velldal, E., Øvrelid, L., & Oepen, S.

Vincze, V., Szarvas, G., Farkas, R., Móra, G., & Csirik, J.
(2008). The BioScope corpus: Annotation for negation,
uncertainty and their scope in biomedical texts. In Pro-
ceedings of the BioNLP 2008 Workshop. Columbus, OH,
USA.

Wilson, T., Wiebe, J., & Hwa, R. (2006). Recognizing strong
and weak opinion clauses. Computational Intelligence,
22(2), 73 – 99.

Zhang, Y., & Wang, R. (2009). Cross-domain dependency
parsing using a deep linguistic grammar. In Proceedings
of the 47th Meeting of the Association for Computational
Linguistics. Singapore.

References
Butt, M., Dyvik, H., King, T. H., Masuichi, H., & Rohrer,
C. (2002). The Parallel Grammar Project. In Proceed-
ings of COLING workshop on grammar engineering and
evaluation (pp. 1 – 7). Taipei, Taiwan.

Crouch, D., Dalrymple, M., Kaplan, R., King, T., Maxwell,
J., & Newman, P. (2008). XLE documentation. Palo Alto,
CA. (Palo Alto Research Center)

Farkas, R., Vincze, V., Mora, G., Csirik, J., & Szarvas, G.
(2010). The CoNLL 2010 Shared Task: Learning to de-
tect hedges and their scope in natural language text. In
Proceedings of the 14th Conference on Natural Language
Learning. Uppsala, Sweden.

Gildea, D.

(2001). Corpus variation and parser perfor-
mance. In Proceedings of the 2001 conference on Empir-
ical Methods in Natural Language Processing (pp. 167 –
202). Pittsburgh, PA.

Johansson, R., & Nugues, P. (2007). Extended constituent-
to-dependency conversion for English.
In J. Nivre, H.-
J. Kaalep, & M. Koit (Eds.), Proceedings of NODALIDA
2007 (p. 105-112). Tartu, Estonia.

Kilicoglu, H., & Bergler, S. (2008). Recognizing speculative
language in biomedical research articles: A linguistically
motivated perspective.
In Proceedings of the BioNLP
2008 Workshop. Columbus, OH, USA.

Kim, J.-D., Ohta, T., Pyysalo, S., Kano, Y., & Tsujii, J.
(2009). Overview of BioNLP 2009 Shared Task on event
extraction. In Proceedings of the BioNLP 2009 workshop
companion volume for shared task (pp. 1 – 9). Boulder,
CO: Association for Computational Linguistics.

Medlock, B., & Briscoe, T. (2007). Weakly supervised learn-
ing for hedge classiﬁcation in scientiﬁc literature. In Pro-
ceedings of the 45th Meeting of the Association for Com-
putational Linguistics (pp. 992 – 999). Prague, Czech Re-
public: Association for Computational Linguistics.

Morante, R., Asch, V. V., & Daelemans, W.

(2010).
Memory-based resolution of in-sentence scope of hedge
cues. In Proceedings of the 14th Conference on Natural
Language Learning (pp. 40 – 47). Uppsala, Sweden.

Morante, R., & Daelemans, W. (2009). Learning the scope
of hedge cues in biomedical texts.
In Proceedings of
the BioNLP 2009 Workshop (pp. 28 – 36). Boulder, Col-
orado.

Ng, V., Dasgupta, S., & Ariﬁn, S. M. N. (2006). Examin-
ing the role of linguistic knowledge sources in the auto-
matic identiﬁcation and classiﬁcation of reviews. In Pro-
ceedings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of the
Association for Computational Linguistics. Sydney, Aus-
tralia.

Nivre, J., Hall, J., & Nilsson, J. (2006). MaltParser: A data-
driven parser-generator for dependency parsing. In Pro-
ceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (p. 2216-2219). Genoa,
Italy.

Nivre, J., & McDonald, R. (2008, June). Integrating graph-
based and transition-based dependency parsers. In Pro-
ceedings of the 46th Meeting of the Association for Com-
putational Linguistics (pp. 950 – 958). Columbus, Ohio.
Øvrelid, L., Kuhn, J., & Spreyer, K. (2009). Improving data-
driven dependency parsing using large-scale LFG gram-

