



















































WFST-Based Grapheme-to-Phoneme Conversion: Open Source tools for Alignment, Model-Building and Decoding


Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 45–49,
Donostia–San Sebastián, July 23–25, 2012. c©2012 Association for Computational Linguistics

WFST-based Grapheme-to-Phoneme Conversion: Open Source Tools for
Alignment, Model-Building and Decoding

Josef R. Novak, Nobuaki Minematsu, Keikichi Hirose
Graduate School of Information Science and Technology

The University of Tokyo, Japan
{novakj,mine,hirose}@gavo.t.u-tokyo.ac.jp

Abstract

This paper introduces a new open source,
WFST-based toolkit for Grapheme-to-
Phoneme conversion. The toolkit is efficient,
accurate and currently supports a range of
features including EM sequence alignment
and several decoding techniques novel in
the context of G2P. Experimental results
show that a combination RNNLM system
outperforms all previous reported results on
several standard G2P test sets. Preliminary
experiments applying Lattice Minimum
Bayes-Risk decoding to G2P conversion are
also provided. The toolkit is implemented
using OpenFst.

1 Introduction

Grapheme-to-Phoneme (G2P) conversion is an im-
portant problem related to Natural Language Pro-
cessing, Speech Recognition and Spoken Dialog
Systems development. The primary goal of G2P
conversion is to accurately predict the pronunciation
of a novel input word given only the spelling. For
example, we would like to be able to predict,

PHOENIX → /f i n I k s/
given only the input spelling and a G2P model or set
of rules. This problem is straightforward for some
languages like Spanish or Italian, where pronuncia-
tion rules are consistent. For languages like English
and French however, inconsistent conventions make
the problem much more challenging.

In this paper we present a fully data-driven,
state-of-the-art, open-source toolkit for G2P conver-
sion, Phonetisaurus [1]. It includes a novel mod-
ified Expectation-Maximization (EM)-driven G2P
sequence alignment algorithm, support for joint-
sequence language models, and several decoding so-
lutions. The paper also provides preliminary in-
vestigations of the applicability of Lattice Mini-

mum Bayes-Risk (LMBR) decoding [2; 3] and N-
best rescoring with a Recurrent Neural Network
Language Model (RNNLM) [4; 5] to G2P con-
version. The Weighted Finite-State Transducer
(WFST) framework is used throughout, and the open
source implementation relies on OpenFst [6]. Ex-
perimental results are provided illustrating the speed
and accuracy of the proposed system.

The remainder of the paper is structured as fol-
lows. Section 2 provides background, Section 3 out-
lines the alignment approach, Section 4 describes
the joint-sequence LM. Section 5 describes decod-
ing approaches. Section 6 discusses preliminary ex-
periments, Section 7 provides simple usage com-
mands and Section 8 concludes the paper.

2 G2P problem outline

Grapheme-to-Phoneme conversion has been a pop-
ular research topic for many years. Many differ-
ent approaches have been proposed, but perhaps the
most popular is the joint-sequence model [6]. Most
joint-sequence modeling techniques focus on pro-
ducing an initial alignment between corresponding
grapheme and phoneme sequences, and then mod-
eling the aligned dictionary as a series of joint to-
kens. The gold standard in this area is the EM-
driven joint-sequence modeling approach described
in [6] that simultaneously infers both alignments and
subsequence chunks. Due to space constraints the
reader is referred to [6] for a detailed background of
previous research.

The G2P conversion problem is typically bro-
ken down into several sub-problems: (1) Sequence
alignment, (2) Model training and, (3) Decoding.
The goal of (1) is to align the grapheme and
phoneme sequence pairs in a training dictionary.
The goal of (2) is to produce a model able to gen-
erate new pronunciations for novel words, and the

45



goal of (3) is to find the most likely pronunciation
given the model.

3 Alignment

The proposed toolkit implements a modified WFST-
based version of the EM-driven multiple-to-multiple
alignment algorithm proposed in [7] and elaborated
in [8]. This algorithm is capable of learning natural
G-P relationships like igh→/AY/ which were not
possible with previous 1-to-1 algorithms like [9].

The proposed alignment algorithm includes three
modifications to [7]: (1) A constraint is imposed
such that only m-to-one and one-to-m arcs are
considered during training. (2) During initialization
a joint alignment lattice is constructed for each in-
put entry, and any unconnected arcs are deleted. (3)
All arcs, including deletions and insertions are ini-
tialized to and constrained to maintain a non-zero
weight.

These minor modifications appear to result in a
small but consistent improvement in terms of Word
Accuracy (WA) on G2P tasks. The Expectation and
Maximization steps for the EM training procedure
are outlined in Algorithms 2, 3. The EM algorithm

Algorithm 1: EM-driven M2One/One2M
Input: xT , yV , mX , mY , dX , dY
Output: γ, AlignedLattices

1 foreach sequence pair (xT , yV ) do
2 InitFSA(xT , yV , mX , mY , dX , dY )
3 foreach sequence pair (xT , yV ) do
4 Expectation(xT , yV , mX , mY , γ)
5 Maximization(γ)

is initialized by generating an alignment FSA for
each dictionary entry, which encodes all valid G-P
alignments, given max subsequence parameters sup-
plied by the user. Any unconnected arcs are deleted
and all remaining arcs are initialized with a non-zero
weight. In Algorithm 2 lines 2-3 compute the for-
ward and backward probabilities. Lines 4-8 com-
pute the arc posteriors and update the current model.
In Algorithm 3 lines 1-2 normalize the probability
distribution. Lines 3-6 update the alignment lattice
arc weights with the new model.

Algorithm 2: Expectation step
Input: AlignedLattices
Output: γ, total

1 foreach FSA alignment lattice F do
2 α← ShortestDistance(F )
3 β ← ShortestDistance(FR)
4 foreach state q ∈ Q[F ] do
5 foreach arc e ∈ E[q] do
6 v ← ((α[q]⊗w[e])⊗β[n[e]])�β[0];
7 γ[i[e]]← γ[i[e]]⊕ v;
8 total← total ⊕ v;

Algorithm 3: Maximization step
Input: γ, total
Output: AlignedLattices

1 foreach arc e in E[γ] do
2 γnew[i[e]]← w[e]/total; γ[i[e]]← 0;
3 foreach FSA alignment lattice F do
4 foreach state q ∈ Q[F ] do
5 foreach arc e ∈ E[q] do
6 w[e]← γnew[i[e]];

4 Joint Sequence N-gram model

The pronunciation model implemented by the
toolkit is a straightforward joint N-gram model. The
training corpus is constructed by extracting the best
alignment for each entry, e.g.:

a}x b}b a}@ c|k}k
a}x b}b a}@ f}f t}t

The training procedure is then, (1) Convert aligned
sequence pairs to sequences of aligned joint label
pairs, (g1:p1, g2:p2, ..., gn:pn); (2) Train an N-gram
model from (1); (3) Convert the N-gram model to
a WFST. Step (3) may be performed with any lan-
guage modeling toolkit. In this paper mitlm [11] is
utilized.

5 Decoding

The proposed toolkit provides varying support for
three different decoding schemes. The default de-
coder provided by the distribution simply extracts
the shortest path through the phoneme lattice created
via composition with the input word,

Hbest = ShortestPath(Projecto(w ◦M)) (1)

46



whereHbest refers to the lowest cost path, Projecto
refers to projecting the output labels, w refers to the
input word, M refers to the G2P model, and ◦ indi-
cates composition.

5.1 RNNLM N-best rescoring
Recurrent Neural Network Language Models have
recently enjoyed a resurgence in popularity in the
context of ASR applications [4]. In another re-
cent publication we investigated the applicability
of this approach to G2P conversion with joint se-
quence models by providing support for the rnnlm
toolkit [5]. The training corpus for the G2P LM
is a corpus of joint sequences, thus it can be used
without modification to train a parallel RNNLM. N-
best reranking is then accomplished with the pro-
posed toolkit by causing the decoder to output the
N-best joint G-P sequences, and employing rnnlm
to rerank the the N-best joint sequences,

HNbest =NShortestPaths(w ◦M)
Hbest =Projecto(Rescorernn(HNbest)).

(2)

In practice the rnnlm models require considerable
tuning, and somewhat more time to train, but pro-
vide a consistent WA boost. For further details on
algorithm as well as tuning for G2P see [4; 10].

5.2 Lattice Minimum Bayes-Risk decoding for
G2P

In [2] the authors note that the aim of MBR decod-
ing is to find the hypothesis that has the “least ex-
pected loss under the model”. MBR decoding was
successfully applied to Statistical Machine Trans-
lation (SMT) lattices in [2], and significantly im-
proved in [3]. Noting the similarities between G2P
conversion and SMT, we have begun work imple-
menting an integrated LMBR decoder for the pro-
posed toolkit.

Our approach closely follows that described
in [3], and the algorithm implementation is sum-
marized in Algorithm 4. The inputs are the full
phoneme lattice that results from composing the in-
put word with the G2P model and projecting output
labels, an exponential scale factor α, and N-gram
precision factors θ0−N . The θn are computed us-
ing a linear corpus BLEU [2] N-gram precision p,
and a match ratio r using the following equations,
θ0 = −1/T ; θn = 1/(NTprn−1). T is a constant

Algorithm 4: G2P Lattice MBR-Decode
Input: E ← Projecto(w ◦M), α, θ0−n

1 E ←ScaleLattice(α× E)
2 NN ←ExtractN-grams(E)
3 for n← 1 to N do
4 Φn ←MakeMapper(Nn)
5 ΨRn ←MakePathCounter(Nn)
6 Un ← Opt((E ◦ Φn) ◦ΨRn )
7 Ωn = Φn
8 for state q ∈ Q[Ωn] do
9 for arc e ∈ E[q] do

10 w[e]← θn × U(o[e])
11 P ← Projectinput(Eθ0 ◦ Ω1)
12 for n← 2 to N do
13 P ← Projectinput(P ◦ Ωn)
14 Hbest = ShortestPath(P)

which does not affect the MBR decision [2]. Line
1 applies α to the raw lattice. In effect this controls
how much we trust the raw lattice weights. After
applying α, E is normalized by pushing weights to
the final state and removing any final weights. In
line 2 all unique N-grams up to order N are ex-
tracted from the lattice. Lines 4-10 create, for each
order, a context-dependency FST (Φn) and a spe-
cial path-posterior counting WFST (ΨRn ), which are
then used to compute N-gram posteriors (Un), and
finally to create a decoder WFST (Ωn). The full
MBR decoder is then computed by first making an
unweighted copy of E , applying θ0 uniformly to all
arcs, and iteratively composing and input-projecting
with each Ωn. The MBR hypothesis is then the best
path through the result P . See [2; 3] for further
details.

6 Experimental results

Experimental evaluations were conducted utilizing
three standard G2P test sets. These included repli-
cations of the NetTalk, CMUdict, and OALD En-
glish language dictionary evaluations described in
detail in [6]. Results comparing various configu-
ration of the proposed toolkit to the joint sequence
model Sequitur [6] and an alternative discriminative
training toolkit direcTL+ [8] are described in Ta-
ble 1. Here m2m-P indicates the proposed toolkit
using the alignment algorithm from [7], m2m-fst-P

47



System NT15k CMUdict OALD
Sequitur [6] 66.20 75.47 82.51
direcTL+ [8] ∼ 75.52 83.32
m2m-P 66.39 75.08 81.20
m2m-fst-P 66.41 75.25 81.86
rnnlm-P 67.77 75.56 83.52

Table 1: Comparison of G2P WA(%) for previous sys-
tems and variations of the proposed toolkit.

indicates the alternative FST-based alignment algo-
rithm, and rnnlm-P indicates the use of RNNLM N-
best reranking.

The results show that the improved alignment al-
gorithm contributes a small but consistent improve-
ment to WA, while RNNLM reranking contributes a
further small but significant boost to WA which pro-
duces state-of-the-art results on all three test sets.

The WA gains are interesting, however a major
plus point for the toolkit is speed. Table 2 compares
training times for the proposed toolkit with previ-
ously reported results. The m2m-fst-P for system for

System NETtalk-15k CMUdict
Sequitur [6] Hours Days
direcTL+ [8] Hours Days
m2m-P 2m56s 21m58s
m2m-fst-P 1m43s 13m06s
rnnlm-P 20m 2h

Table 2: Training times for the smallest (15k entries) and
largest (112k entries) training sets.

CMUdict performs %0.27 worse than the state-of-
the-art, but requires just a tiny fraction of the train-
ing time. This turn-around time may be very impor-
tant for rapid system development. Finally, Figure. 1
plots WA versus decoding time for m2m-fst-P on the
largest test set, further illustrating the speed of the
decoder, and the impact of using larger models.

Preliminary experiments with the LMBR decoder
were also carried out using the smaller NT15k
dataset. The θn values were computed using p, r,
and T from [2] while α was tuned to 0.6. Re-
sults are described in Table 3. The system matched
the basic WA for N=6, and achieved a small im-
provement in PA over m2m-fst-P (%91.80 versus
%91.82). Tuning the loss function for the G2P task
should improve performance.

!"#

!$#

$"#

$$#

%"#

%$#

&"#

&$#

'"#

()# (!# (%# ('# )"# ))# )!# )%# )'#

!
"#
$%
&'

'(
#)
'*
%+,

-%

./'"$012%34/%+5/'-%

678$0'9%:;<=::;<%%
!"#$%&''(#)'*%>5?%@04/%A"#%BC;D:;%

*+#,-#./01#

Figure 1: Decoding speed vs. WA plot for various N-
gram orders for the CMUdict 12k/112k test/train set.
Times averaged over 5 run using ctime.

NT15k N=1 N=2 N=3 N=4 N=5 N=6
WA 28.88 65.48 66.03 66.41 66.37 66.50
PA 83.17 91.74 91.79 91.87 91.82 91.82

Table 3: LMBR decoding Word Accuracy (WA) and
Phoneme Accuracy (PA) for order N=1-6.

7 Toolkit distribution and usage

The preceding sections introduced various theoreti-
cal aspects of the toolkit as well as preliminary ex-
perimental results. The current section provides sev-
eral introductory usage commands.

The toolkit is open source and released under
the liberal BSD license. It is available for down-
load from [1], which also includes detailed com-
pilation instructions, tutorial information and addi-
tional examples. The examples that follow utilize
the NETTalk dictionary.
Align a dictionary:
$ phonetisaurus-align --input=test.dic \

--ofile=test.corpus

Train a 7-gram model with mitlm:
$ estimate-ngram -o 7 -t test.corpus \

-wl test.arpa

Convert the model to a WFSA
$ phonetisaurus-arpa2fst --input=test.arpa \

--prefix=test

Apply the default decoder
$ phonetisaurus-g2p --model=test.fst \

--input=abbreviate --nbest=3 --words
abbreviate 25.66 @ b r i v i e t

48



abbreviate 28.20 @ b i v i e t
abbreviate 29.03 x b b r i v i e t

Apply the LMBR decoder

$ phonetisaurus-g2p --model=test.fst \
--input=abbreviate --nbest=3 --words \
--mbr --order=7

abbreviate 1.50 @ b r i v i e t
abbreviate 2.62 x b r i v i e t
abbreviate 2.81 a b r i v i e t

8 Conclusion and Future work

This work introduced a new Open Source WFST-
driven G2P conversion toolkit which is both highly
accurate as well as efficient to train and test. It incor-
porates a novel modified alignment algorithm. To
our knowledge the RNNLM N-best reranking and
LMBR decoding are also novel applications in the
context of G2P.

Both the RNNLM N-best reranking and LMBR
decoding are promising but further work is required
to improve usability and performance. In particular
RNNLM training requires considerable tuning, and
we would like to automate this process. The pro-
visional LMBR decoder achieved a small improve-
ment but further work will be needed to tune the
loss function. Several known optimizations are also
planned to speed up the LMBR decoder.

Nevertheless the current release of the toolkit pro-
vides several novel G2P solutions, achieves state-of-
the-art WA on several test sets and is efficient for
both training and decoding.

References

J. Novak, et al. [1].
http://code.google.com/p/
phonetisaurus

R. Tromble and S. Kumar and F. Och and W. Macherey.
[2]. Lattice Minimum Bayes-Risk Decoding for Sta-
tistical Machine Translation, Proc. EMNLP 2007, pp.
620-629.

G. Blackwood and A. Gispert and W. Byrne. [3]. Effi-
cient path counting transducers for minimum bayes-
risk decoding of statistical machine translation lat-
tices, Proc. ACL 2010, pp. 27-32.

T. Mikolov and M. Karafiat and L. Burget and J. Čer-
nocký and S. Khundanpur. [4]. Recurrent Neural Net-
work based Language Model, Proc. InterSpeech, 2010.

T. Mikolov and S. Kombrink and D. Anoop and L. Burget
and J. Černocký. [5]. RNNLM - Recurrent Neural Net-
work Language Modeling Toolkit, ASRU 2011, demo
session.

C. Allauzen and M. Riley and J. Schalkwyk and W. Skut
and M. Mohri. [6]. OpenFST: A General and Effi-
cient Weighted Finite-State Transducer Library, Proc.
CIAA 2007, pp. 11-23.

M. Bisani and H. Ney. [6]. Joint-sequence models for
grapheme-to-phoneme conversion, Speech Communi-
cation 50, 2008, pp. 434-451.

S. Jiampojamarn and G. Kondrak and T. Sherif. [7]. Ap-
plying Many-to-Many Alignments and Hidden Markov
Models to Letter-to-Phoneme Conversion, NAACL
HLT 2007, pp. 372-379.

S. Jiampojamarn and G. Kondrak. [8]. Letter-to-
Phoneme Alignment: an Exploration, Proc. ACL
2010, pp. 780-788.

E. Ristad and P. Yianilos. [9]. Learning String Edit Dis-
tance, IEEE Trans. PRMI 1998, pp. 522-532.

J. Novak and P. Dixon and N. Minematsu and K. Hirose
and C. Hori and H. Kashioka. [10]. Improving WFST-
based G2P Conversion with Alignment Constraints
and RNNLM N-best Rescoring, Interspeech 2012 (Ac-
cepted).

B. Hsu and J. Glass. [11]. Iterative Language Model Es-
timation: Efficient Data Structure & Algorithms, Proc.
Interspeech 2008.

49


