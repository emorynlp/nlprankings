



















































Relaxed Cross-lingual Projection of Constituent Syntax


Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1192–1201,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics

Relaxed Cross-lingual Projection of Constituent Syntax

Wenbin Jiang and Qun Liu and Yajuan Lü

Key Laboratory of Intelligent Information Processing
Institute of Computing Technology

Chinese Academy of Sciences
{jiangwenbin, liuqun, lvyajuan}@ict.ac.cn

Abstract

We propose a relaxed correspondence as-
sumption for cross-lingual projection of con-
stituent syntax, which allows a supposed
constituent of the target sentence to corre-
spond to an unrestricted treelet in the source
parse. Such a relaxed assumption fundamen-
tally tolerates the syntactic non-isomorphism
between languages, and enables us to learn
the target-language-specific syntactic idiosyn-
crasy rather than a strained grammar di-
rectly projected from the source language syn-
tax. Based on this assumption, a novel con-
stituency projection method is also proposed
in order to induce a projected constituent tree-
bank from the source-parsed bilingual cor-
pus. Experiments show that, the parser trained
on the projected treebank dramatically out-
performs previous projected and unsupervised
parsers.

1 Introduction

For languages with treebanks, supervised models
give the state-of-the-art performance in dependency
parsing (McDonald and Pereira, 2006; Nivre et al.,
2006; Koo and Collins, 2010; Martins et al., 2010)
and constituent parsing (Collins, 2003; Charniak
and Johnson, 2005; Petrov et al., 2006). To break the
restriction of the treebank scale, lots of works have
been devoted to the unsupervised methods (Klein
and Manning, 2004; Bod, 2006; Seginer, 2007; Co-
hen and Smith, 2009) and the semi-supervised meth-
ods (Sarkar, 2001; Steedman et al., 2003; McClosky
et al., 2006; Koo et al., 2008) to utilize the unan-
notated text. In recent years, researchers have also

conducted many investigations on syntax projection
(Hwa et al., 2005; Ganchev et al., 2009; Smith and
Eisner, 2009; Jiang et al., 2010), in order to borrow
syntactic knowledge from another language.

Different from the bilingual parsing (Smith and
Smith, 2004; Burkett and Klein, 2008; Zhao et al.,
2009; Huang et al., 2009; Chen et al., 2010) that
improves parsing performance with bilingual con-
straints, and the bilingual grammar induction (Wu,
1997; Kuhn, 2004; Blunsom et al., 2008; Snyder et
al., 2009) that induces grammar from parallel text,
the syntax projection aims to project the syntac-
tic knowledge from one language to another. This
seems especially promising for the languages that
have bilingual corpora parallel to resource-rich lan-
guages with large treebanks. Previous works mainly
focus on dependency projection. The dependency
relationship between words in the parsed source sen-
tences can be directly projected across the word
alignment to words in the target sentences, follow-
ing the direct correspondence assumption (DCA)
(Hwa et al., 2005). Due to the syntactic non-
isomorphism between languages, DCA assumption
usually leads to conflicting or incomplete projection.
Researchers have to adopt strategies to tackle this
problem, such as designing rules to handle language
non-isomorphism (Hwa et al., 2005), and resorting
to the quasi-synchronous grammar (Smith and Eis-
ner, 2009).

For constituency projection, however, the lack of
isomorphism becomes much more serious, since a
constituent grammar describes a language in a more
detailed way. In this paper we propose a relaxed
correspondence assumption (RCA) for constituency

1192



Through

a series of

experiments

he verified

the previous hypothesis

.
IN

DT NN IN

NNS

NP

PPNP

NP

PP

PRP

NP

DT JJ NN

.

VBD NP

VP

S

PN

P NN

VV AS

LC DEG

NN

PU

[VBD]

[NP-DT-JJ-*]

[NP]

[VP][PP]

[S-PP-*-VP-*]

[S-PP-NP-VP-*]

[S]

TOP TOP

11

2

2 3

3

4

4

5

5

Figure 1: An example for constituency projection based on the RCA assumption. The projection is from English
to Chinese. A dash dot line links a projected constituent to its corresponding treelet, which is marked with gray
background; An Arabic numeral relates a directly-projected constituent to its counter-part in the source parse.

projection. It allows a supposed constituent of
the target sentence to correspond to an unrestricted
treelet in the source parse. Such a relaxed as-
sumption fundamentally tolerates the syntactic non-
isomorphism between languages, and enables us to
learn the target-language-specific syntactic idiosyn-
crasy, rather than induce a strained grammar directly
projected from the source language syntax. We also
propose a novel cross-lingual projection method for
constituent syntax based on the RCA assumption.
Given a word-aligned source-parsed bilingual cor-
pus, a PCFG grammar can be induced for the target
language by maximum likelihood estimation on the
exhaustive enumeration of candidate projected pro-
ductions, where each nonterminal in a production
is an unrestricted treelet extracted from the source
parse. The projected PCFG grammar is then used
to parse each target sentence under the guidance of
the corresponding source tree, so as to produce an
optimized projected constituent tree.

Experiments validate the effectiveness of the
RCA assumption and the constituency projection
method. We induce a projected Chinese constituent
treebank from the FBIS Chinese-English parallel
corpus with English sentences parsed by the Char-
niak parser. The Berkeley Parser trained on the pro-

jected treebank dramatically outperforms the previ-
ous projected and unsupervised parsers. This pro-
vides an promising substitute for unsupervised pars-
ing methods, to the resource-scarce languages that
have bilingual corpora parallel to resource-rich lan-
guages with human-annotated treebanks.

In the rest of this paper we first presents the RCA
assumption, and the algorithm used to determine the
corresponding treelet in the source parse for a can-
didate constituent in the target sentence. Then we
describe the induction of the projected PCFG gram-
mar and the projected constituent treebank from the
word-aligned source-parsed parallel corpus. After
giving experimental results and the comparison with
previous unsupervised and projected parsers, we fi-
nally conclude our work and point out several as-
pects to be improved in the future work.

2 Relaxed Correspondence Assumption

The DCA assumption (Hwa et al., 2005) works well
in dependency projection. A dependency grammar
describes a sentence in a compact manner where the
syntactic information is carried by the dependency
relationships between pairs of words. It is reason-
able to audaciously assume that the relationship of

1193



Algorithm 1 Treelet Extraction Algorithm.
1: Input: Tf : parse tree of source sentence f
2: e: target sentence
3: A: word alignment of e and f
4: for i, j s.t. 1 ≤ i < j ≤ |e| do ⊲ all spans
5: t← EXTTREELET(e, i, j,Tf ,A)
6: T〈i,j〉 ← PRUNETREE(t)
7: Output: treelet set T for all spans of e
8: function EXTTREELET(e, i, j, T, A)
9: if T aligns totally outside ei:j then

10: return ∅
11: if T aligns totally inside ei:j then
12: return {T · root}
13: t← {T · root} ⊲ partly aligned inside ei:j
14: for each subtree s of T do
15: t← t ∪ EXTTREELET(e, i, j, s,A)
16: return t
17: function PRUNETREE(T)
18: for each node n in T do
19: merge n’s successive empty children

20: t← T
21: while t has only one non-empty subtree do
22: t← the non-empty subtree of t
23: return t

a word pair in the source sentence also holds for
the corresponding word pair in the target sentence.
Compared with dependency grammar, constituent
grammar depicts syntax in a more complex way that
gives a sentence a hierarchically branched structure.
Therefore the lack of syntactic isomorphism for con-
stituency projection becomes much more serious, it
will be hard and inappropriate to directly project the
complex constituent structure from one language to
another.

For constituency projection, we propose a relaxed
corresponding assumption (RCA) to eliminate the
influence of syntactic non-isomorphism between the
source- and target languages. This assumption al-
lows a supposed constituent of the target sentence to
correspond to an unrestricted treelet in the source
parse. A treelet is a connected subgraph in the
source constituent tree, which covers a discontigu-
ous sequence of words of the source sentence. This
property enables a supposed constituent of the tar-
get sentence not necessarily to correspond to exactly
a constituent of the source parse, so as to funda-
mentally tolerate the syntactic non-isomorphism be-
tween languages. Figure 1 gives an example of re-

* *

DT JJ *

*

* NP

VP

S

[NP-DT-JJ-*]

TOP

DT JJ *

NP

[TOP-[S-*-*-[VP-*-[NP-DT-JJ-*]]-*]](a)

(b)

* NP

DT JJ *

*

NP *

*

S

[NP-DT-JJ-*]

TOP

DT JJ *

NP

[TOP-[S-*-[NP-[NP-DT-JJ-*]-*]-*-*]]

Figure 2: Two examples for treelet pruning. Asterisks
indicate eliminated subtrees, which are represented as
empty children of their parent nodes.

laxed correspondence.

2.1 Corresponding Treelet Extraction

According to the word alignment between the source
and target sentences, we can extract the treelet out of
the source parse for any possible constituent span of
the target sentence. Algorithm 1 shows the treelet
extraction algorithm.

Given the target sentence e, the parse tree Tf of
the source sentence f , and the word alignment A
between e and f , the algorithm extracts the corre-
sponding treelet out of Tf for each candidate span
of e (line 4-6). For a given span 〈i, j〉, its corre-
sponding treelet in Tf can be extracted by a recur-
sive top-down traversal in the tree. If all nodes in
the current subtree T align outside of source subse-
quence ei:j , the recursion stops and returns an empty
tree ∅, indicating that the subtree is eliminated from
the final treelet (line 9-10). And, if all nodes in T
align inside ei:j , the root of T is returned as the con-
cise representation of the whole subtree (line 11-12).
For the third situation, that is to say T aligns partly
inside ei:j , the recursion has to continue to investi-
gate the subtrees of T (line 14-15). The recursive
traversal finally returns a treelet t that exactly corre-

1194



sponds to the candidate constituent span 〈i, j〉 of the
source sentence.

We can find that even for a smaller span, the recur-
sive extraction procedure still starts from the root of
the source tree. This leads to a expatiatory treelet
with some redundant nodes on the top. Function
PRUNETREE takes charge of the treelet pruning (line
6). It traverses the treelet to merge the successive
empty sibling nodes (marked with asterisks) into one
(line 18-19), then conducts a top-down pruning to
delete the redundant branches until meeting a branch
with more than one non-empty subtrees (line 20-22).
Figure 2 shows the effect of the pruning operation
with two examples. The pruning operation maps the
two original treelets into the same simplified ver-
sion, that is, the pruned treelet. The branches pruned
out of the original treelet serve as the context of the
pruned treelet. The bracketed representations of the
pruned treelets, as shown above the treelet graphs,
are used as the nonterminals of the projected target
parses.

Since the overall complexity of the algorithm is
O(|e|3), it seems inefficient to collect the treelets
for all spans in the target sentence. But in fact it
runs fast on the realistic corpus in our experiments,
we assume that the function EXTTREELET doesn’t
always consume O(|e|) because of the more or less
isomorphism between two languages.

3 Projected Grammar and Treebank

This section describes how to build a projected con-
stituent treebank based on the RCA assumption. Ac-
cording to the last section, each span of the target
sentence could correspond to a treelet in the source
parse. If a span 〈i, j〉 has a corresponding treelet t,
a candidate projected constituent can be defined as a
triple 〈i, j, t〉. For an n-way partition of this span,

〈i, k1〉, 〈k1 + 1, k2〉, .., 〈kn−1 + 1, j〉

if each sub-span 〈kp−1+1, kp〉 corresponds to a can-
didate constituent 〈kp−1+1, kp, tp〉, a candidate pro-
jected production can then be defined, denoted as

〈i, j, t〉 → 〈i, k1, t1〉〈k1+1, k2, t2〉..〈kn−1+1, j, tn〉

There may be many candidate projected constituents
because of arbitrary combination, the tree projec-
tion procedure aims to find the optimum tree from

the parse forest determined by these candidate con-
stituents. Each production in the optimum tree
should satisfy this principle: the rule used in this
production appears in the whole corpus as frequently
as possible.

However, due to translation diversity and word
alignment error, the real constituent tree of the target
sentence may not be contained in the candidate pro-
jected constituents. We propose a relaxed and fault-
tolerant tree projection strategy to tackle this prob-
lem. First, based on the distribution of candidate
projected constituents over each single sentence, we
estimate the distribution over the whole corpus for
the rules used in these constituents, so as to obtain
a projected PCFG grammar. Then, using a PCFG
parser and this grammar, we parse each target sen-
tence under the guidance of the candidate projected
constituent set of the target sentence, so as to ob-
tain the optimum projected tree as far as possible.
In the following, we first describe the estimation of
the projected PCFG grammar and then show the tree
projection procedure.

3.1 Projected PCFG Grammar

From a human-annotated treebank, we can induce a
PCFG grammar by estimating the frequency of the
production rules, which are contained in the produc-
tions of the trees. But for each target sentence we
don’t know which candidate productions consist the
correct constituent tree, so we can’t estimate the fre-
quency of the production rules directly.

A reasonable hypothesis is, if a candidate pro-
jected production for a target sentence happens to be
in the correct parse of the sentence, the rule used in
this production will appear frequently in the whole
corpus. We assume that each candidate projected
production may be a part of the correct parse, but
with different probabilities. If we give each candi-
date projected production an appropriate probabil-
ity and use this probability as the appearance fre-
quency of this production in the correct parse, we
can achieve an approximation of the PCFG gram-
mar hidden in the target sentences. In this work,
we restrict the productions to be binarized to reduce
the computational complexity. It results in a bina-
rized PCFG grammar, similar to previous unsuper-
vised works.

To estimate the frequencies of the candidate pro-

1195



ductions in the correct parse of the target sentence,
we need first estimate the frequencies of the candi-
date spans, which are described as follows:

p(〈i, j〉|e) = # of trees including 〈i, j〉
# of all trees

(1)

The count of all binary trees of a target sentence e
can be calculated similar to the β value calculation
in the inside-outside algorithm. Without confusion,
we adopt the symbol β(i, j) to denote the count of
binary tree for span 〈i, j〉:

β(i, j) =





1 i = j

j−1∑

k=i

β(i, k) · β(k + 1, j) i < j
(2)

β(1, |e|) is the count of binary trees of target sen-
tence e. We also need to calculate the count of bi-
nary tree fragments that cover the nodes outside span
〈i, j〉. This is similar to the calculation of the α value
in the inside-outside algorithm. We also adopt the
symbol α(i, j) here:

α(i, j) =





1 i = 1, j = |e|

|e|∑

k=j+1

α(i, k) · β(k + 1, |e|)

+

i−1∑

k=1

α(k, j) · β(k, j − 1) else

(3)

For simplicity we omit some conditions in above for-
mulas. The count of trees containing span 〈i, j〉 is
α(i, j) · β(i, j). Equation 1 can be rewritten as

p(〈i, j〉|e) = α(i, j) · β(i, j)
β(1, |e|) (4)

On condition that 〈i, j〉 is a span in the parse of e,
the probability that 〈i, j〉 has two children 〈i, k〉 and
〈k + 1, j〉 is

p(〈i, k〉〈k + 1, j〉|〈i, j〉) = β(i, k) · β(k + 1, j)
β(i, j)

(5)

Therefore, the probability that 〈i, j〉 is a span in the
parse of e and has two children 〈i, k〉 and 〈k + 1, j〉

can be calculated as follows:

p(〈i,j〉 → 〈i, k〉〈k + 1, j〉|e)
= p(〈i, j〉|e) · p(〈i, k〉〈k + 1, j〉|〈i, j〉)

=
α(i, j) · β(i, k) · β(k + 1, j)

β(1, |e|)

(6)

Since each candidate projected span aligns to one
treelet at most, this probability is also the frequency
of the candidate projected production related to the
three spans.

The counting approach above is based on the as-
sumption that there is a uniform distribution over the
projected trees for every target sentence. The inside
and outside algorithms and the other counting for-
mulae are used to calculate the expected counts un-
der this assumption. This looks like a single iteration
of EM.

A binarized projected PCFG grammar can then be
easily induced by maximum likelihood estimation.
Due to word alignment errors, free translation, and
exhaustive enumeration of possible projected pro-
ductions, such a PCFG grammar may contain too
much noisy nonterminals and production rules. We
introduce a threshold bRULE to filter the grammar. A
production rule can be reserved only if its frequency
is larger than bRULE .

3.2 Relaxed Tree Projection

The projected PCFG grammar is used in the pro-
cedure of constituency projection. Such a gram-
mar, as a kind of global syntactic knowledge, can
attenuate the negative effect of word alignment er-
ror, free translation and syntactic non-isomorphism
for the constituency projection between each sin-
gle sentence pair. To obtain as optimal a projected
constituency tree as possible, we have to integrate
two kinds of knowledge: the local knowledge in
the candidate projected production set of the target
sentence, and the global knowledge in the projected
PCFG grammar.

The integrated projection strategy can be con-
ducted as follows. We parse each target sentence
with the projected PCFG grammar G, and use the
candidate projected production set D to guide the
PCFG parsing. The parsing procedure aims to find
an optimum projected tree, which maximizes both
the PCFG tree probability and the count of produc-
tions that also appear in the candidate projected pro-

1196



 0

 2000

 4000

 6000

 8000

 10000

 12000

 14000

 16000

 1  2  4  8  16  32  64  128  256  512 1024
 0

 10

 20

 30

 40

 50

 60

 70

 80

 90

# 
re

se
rv

ed
 r

ul
es

P
er

ce
nt

ag
e 

in
 a

ll 
ru

le
s

# selected NTs

# reserved rules
Percentage in all rules

Figure 3: Rule counts corresponding to selected nonter-
minal sets, and their frequency summation proportions to
the whole rule set.

duction set. The two optimization objectives can be
coordinated as follows:

ỹ = argmax
y

∏

d∈y
(p(d|G) · eλ·δ(d,D)) (7)

Here, d represents a production; δ is a boolean func-
tion that returns 1 if d appears in D and returns 0
otherwise; λ is a weight coefficient that needs to be
tuned to maximize the quality of the projected tree-
bank.

4 Experiments

Our work focuses on the constituency projection
from English to Chinese. The FBIS Chinese-English
parallel corpus is used to obtain a projected con-
stituent treebank. It contains 239 thousand sentence
pairs, with about 6.9/8.9 million Chinese/English
words. We parse the English sentences with the
Charniak Parser (Charniak and Johnson, 2005), and
tag the Chinese sentences with a POS tagger imple-
mented faithfully according to (Collins, 2002) and
trained on the Penn Chinese Treebank 5.0 (Xue et
al., 2005). We perform word alignment by runing
GIZA++ (Och and Ney, 2000), and then use the
alignment results for constituency projection.

Following the previous works of unsupervised
constituent parsing, we evaluate the projected parser
on the subsets of CTB 1.0 and CTB 5.0, which con-
tain no more than 10 or 40 words after the removal
of punctuation. The gold-standard POS tags are di-
rectly used for testing. The evaluation for unsu-
pervised parsing differs slightly from the standard

 10

 15

 20

 25

 30

 35

 1  2  4  8  16  32  64  128  256  512  1024

U
nl

ab
el

ed
 F

1 
(%

)

# selected NTs

Figure 4: Performance curve of the projected PCFG
grammars corresponding to different sizes of nontermi-
nal sets.

PARSEVAL metrics, it ignores the multiplicity of
brackets, brackets of span one, and the bracket la-
bels. In all experiments we report the unlabeled F1
value which is the harmonic mean of the unlabeled
precision and recall.

4.1 Projected PCFG Grammar

An initial projected PCFG grammar can be induced
from the word-aligned and source-parsed parallel
corpus according to section 3.1. Such an initial
grammar is huge and contains a large amount of
projected nonterminals and production rules, where
many of them come from free translation and word
alignment errors. We conservatively set the filtra-
tion threshold bRULE as 1.0 to discard the rules with
frequency less than one, the rule count falls dramat-
ically from 3.3 millions to 92 thousands.

Figure 3 shows the statistics of the remained pro-
duction rules. We sort the projected nonterminals
according to their frequencies and select the top 2N

(1 ≤ N ≤ 10) best ones, and then discard the rules
that fall out of the selected nonterminal set. The fre-
quency summation of the rule set corresponding to
32 best nonterminals accounts for nearly 90% of the
frequency summation of the whole rule set.

We use the developing set of CTB 1.0 (chapter
301-325) to evaluate the performance of a series of
filtered grammars. Figure 4 gives the unlabeled F1
value of each grammar on all trees in the developing
set. The filtered grammar corresponding to the set
of top 32 nonterminals achieves the highest perfor-
mance. We denote this grammar as G32 and use it

1197



 36

 37

 38

 39

 40

 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5

U
nl

ab
el

ed
 F

1 
(%

)

Weight coefficient

Figure 5: Performance curve of the Berkeley Parser
trained on 5 thousand projected trees. The weight co-
efficient λ ranges from 0 to 5.

in the following tree projection procedure.

4.2 Projected Treebank and Parser

The projected grammar G32 provides global syn-
tactic knowledge for constituency projection. Such
global knowledge and the local knowledge carried
by the candidate projected production set are inte-
grated in a linear weighted manner as in Formula
7. The weight coefficient λ is tuned to maximize
the quality of the projected treebank, which is in-
directly measured by evaluating the performance of
the parser trained on it.

We select the first 5 thousand sentence pairs from
the Chinese-English FBIS corpus, and induce a se-
ries of projected treebanks using different λ, ranging
from 0 to 5. Then we train the Berkeley Parser on
each projected treebank, and test it on the develop-
ing set of CTB 1.0. Figure 5 gives the performance
curve, which reports the unlabeled F1 values of the
projected parsers on all sentences of the developing
set. We find that the best performance is achieved
with λ between 1 and 2.5, with slight fluctuation
in this range. It can be concluded that, the pro-
jected PCFG grammar and the candidate projected
production set do represent two different kinds of
constraints, and we can effectively coordinate them
by tuning the weight coefficient. Since different λ
values in this range result in slight performance fluc-
tuation of the projected parser, we simply set it to 1
for the constituency projection on the whole FBIS
corpus.

There are more than 200 thousand projected trees

 45

 45.5

 46

 46.5

 47

 47.5

 48

 48.5

 49

 49.5

U
nl

ab
el

ed
 F

1 
(%

)

Scale of treebank
5000 10000 20000 40000 80000 160000

Figure 6: Performance curve of the Berkeley Parser
trained on different amounts of best project trees. The
scale of the selected treebank ranges from 5000 to
160000.

induced from the Chinese-English FBIS corpus. It
is a heavy burden for a parser to train on so large a
treebank. And on the other hand, the free translation
and word alignment errors result in many projected
trees of poor-quality. We design a criteria to approx-
imate the quality of the projected tree y for the target
sentence x:

Q̃(y) = |x|−1

√∏

d∈y
(p(d|G) · eλ·δ(d,D)) (8)

and use an amount of best projected trees instead of
the whole projected treebank to train the parser. Fig-
ure 6 shows the performance of the Berkeley Parser
trained on different amounts of selected trees. The
performance of the Berkeley Parser constantly im-
proves along with the increment of selected trees.
However, treebanks containing more than 40 thou-
sand projected trees can not brings significant im-
provement. The parser trained on 160 thousand trees
only achieves an F1 increment of 0.4 points over the
one trained on 40 thousand trees. This indicates that
the newly added trees do not give the parser more
information due to their projection quality, and a
larger parallel corpus may lead to better parsing per-
formance.

The Berkeley Parser trained on 160 thousand best
projected trees is used in the final test. Table 1
gives the experimental results and the comparison
with related works. This is a sparse table since the
experiments of previous researchers focused on dif-
ferent data sets. Our projected parser significantly

1198



System CTB-TEST-40 CTB1-ALL-10 CTB5-ALL-10 CTB5-ALL-40
(Klein and Manning, 2004) — 46.7 — —
(Bod, 2006) — 47.2 — —
(Seginer, 2007) — — 54.6 38.0
(Jiang et al., 2010) 40.4 — — —

our work 52.1 54.4 54.5 49.2

Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous
works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences≤ 40 words from CTB standard
test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences ≤ 10 words from CTB 1.0/CTB 5.0 after the
removal of punctuation; CTB5-ALL-40: sentences ≤ 40 words from CTB 5.0 after the removal of punctuation.

outperforms the parser of Jiang et al. (2010), where
they directly adapt the DCA assumption of (Hwa
et al., 2005) from dependency projection to con-
stituency projection and resort to a better word align-
ment and a more complicated tree projection algo-
rithm. This indicates that the RCA assumption is
more suitable for constituency projection than the
DCA assumption, and can induce a better grammar
that much more reflects the language-specific syn-
tactic idiosyncrasy of the target language.

Our projected parser also obviously surpasses ex-
isting unsupervised parsers. The parser of Seginer
(2007) performs slightly better on CTB 5.0 sen-
tences no more than 10 words, but obviously falls
behind on sentences no more than 40 words. Fig-
ure 7 shows the unlabeled F1 of our parser on
a series of subsets of CTB 5.0 with different sen-
tence length upper limits. We find that even on the
whole treebank, our parser still gives a promising
result. Compared with unsupervised parsing, con-
stituency projection can make use of the syntactic
information of another language, so that it proba-
bly induce a better grammar. Although compar-
ing a syntax projection technique to supervised or
semi-supervised techniques seems unfair, it still sug-
gests that if a resource-poor language has a bilingual
corpus parallel to a resource-rich language with a
human-annotated treebank, the constituency projec-
tion based on RCA assumption is a promising sub-
stitute for unsupervised parsing.

5 Conclusion and Future Works

This paper describes a relaxed correspondence as-
sumption (RCA) for constituency projection. Un-
der this assumption a supposed constituent in the
target sentence can correspond to an unrestricted

 45

 46

 47

 48

 49

 50

 51

 52

 53

 54

 55

 10  20  30  40  50  60  70  80  90  100

U
nl

ab
el

ed
 F

1 
(%

)

Upper limit of sentence length

+

Figure 7: Performance of the Berkeley Parser on subsets
of CTB 5.0 with different sentence length upper limits.
100+ indicates the whole treebank.

treelet in the parse of the source sentence. Different
from the direct correspondence assumption (DCA)
widely used in dependency projection, the RCA as-
sumption is more suitable for constituency projec-
tion, since it fundamentally tolerates the syntactic
non-isomorphism between the source and target lan-
guages. According to the RCA assumption we pro-
pose a novel constituency projection method. First, a
projected PCFG grammar is induced from the word-
aligned source-parsed parallel corpus. Then, the tree
projection is conducted on each sentence pair by a
PCFG parsing procedure, which integrates both the
global knowledge in the projected PCFG grammar
and the local knowledge in the set of candidate pro-
jected productions.

Experiments show that the parser trained on
the projected treebank significantly outperforms the
projected parsers based on the DCA assumption.
This validates the effectiveness of the RCA assump-
tion and the constituency projection method, and
indicates that the RCA assumption is more suit-

1199



able for constituency projection than the DCA as-
sumption. The projected parser also obviously sur-
passes the unsupervised parsers. This suggests
that if a resource-poor language has a bilingual
corpus parallel to a resource-rich language with a
human-annotated treebank, the constituency projec-
tion based on RCA assumption is an promising sub-
stitute for unsupervised methods.

Although achieving appealing results, our current
work is quite coarse and has many aspects to be im-
proved. First, the word alignment is the fundamental
precondition for projected grammar induction and
the following constituency projection, we can adopt
the better word alignment strategies to improve the
word alignment quality. Second, the PCFG grammar
is too weak due to its context free assumption, we
can adopt more complicated grammars such as TAG
(Joshi et al., 1975), in order to provide a more pow-
erful global syntactic constraints for the tree projec-
tion procedure. Third, the current tree projection
algorithm is too simple, more bilingual constraints
could lead to better projected trees. Last but not
least, the constituency projection and the unsuper-
vised parsing make use of different kinds of knowl-
edge, therefore the unsupervised methods can be in-
tegrated into the constituency projection framework
to achieve better projected grammars, treebanks, and
parsers.

Acknowledgments

The authors were supported by National Natural
Science Foundation of China Contract 90920004,
60736014 and 60873167. We are grateful to the
anonymous reviewers for their thorough reviewing
and valuable suggestions.

References

Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of the NIPS.

Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of the COLING-ACL.

David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the EMNLP.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine-grained n-best parsing and discriminative rerank-
ing. In Proceedings of the ACL.

Wenliang Chen, Jun.ichi Kazama, and Kentaro Tori-
sawa. 2010. Bitext dependency parsing with bilingual
subtree constraints. In Proceedings of the ACL.

Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
the NAACL-HLT.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the EMNLP.

Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics.

Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of the 47th ACL.

Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the EMNLP.

Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering.

Wenbin Jiang, Yajuan Lü, Yang Liu, and Qun Liu. 2010.
Effective constituent projection across languages. In
Proceedings of the COLING.

A. K. Joshi, L. S. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal Computer Systems Sci-
ence.

Dan Klein and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
ACL.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the ACL.

Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of the ACL.

Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the ACL.

André F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Mário A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of EMNLP.

David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the ACL.

Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81–88.

1200



Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudoprojec-
tive dependency parsing with support vector machines.
In Proceedings of CoNLL, pages 221–225.

Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of the ACL.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the ACL.

Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.

Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of the ACL.

David Smith and Jason Eisner. 2009. Parser adaptation
and projection with quasi-synchronous grammar fea-
tures. In Proceedings of EMNLP.

David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of the EMNLP.

Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of the ACL.

Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the EACL.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.

Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.

Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the ACL-IJCNLP.

1201


