










































DAEBAK!: Peripheral Diversity for Multilingual Word Sense Disambiguation


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 250–254, Atlanta, Georgia, June 14-15, 2013. c©2013 Association for Computational Linguistics

DAEBAK!: Peripheral Diversity for Multilingual Word Sense
Disambiguation

Steve L. Manion
University of Canterbury

Christchurch, New Zealand
steve.manion

@pg.canterbury.ac.nz

Raazesh Sainudiin
University of Canterbury

Christchurch, New Zealand
r.sainudiin

@math.canterbury.ac.nz

Abstract

We introduce Peripheral Diversity (PD) as a
knowledge-based approach to achieve multi-
lingual Word Sense Disambiguation (WSD).
PD exploits the frequency and diverse use
of word senses in semantic subgraphs de-
rived from larger sense inventories such as
BabelNet, Wikipedia, and WordNet in order
to achieve WSD. PD’s f -measure scores for
SemEval 2013 Task 12 outperform the Most
Frequent Sense (MFS) baseline for two of
the five languages: English, French, German,
Italian, and Spanish. Despite PD remain-
ing under-developed and under-explored, it
demonstrates that it is robust, competitive, and
encourages development.

1 Introduction

By reading out aloud “A minute is a minute divi-
sion of time” (Nelson, 1976), we can easily make
the distinction between the two senses of the homo-
graph minute. For a machine this is a complex task
known as Word Sense Disambiguation (WSD). Task
12 of SemEval 2013 (Navigli et al., 2013) calls for a
language-independent solution to WSD that utilises
a multilingual sense inventory.

Supervised approaches to WSD have dominated
for some time now (Màrquez et al., 2007). Homo-
graphs such as minute are effortlessly disambiguated
and more polysemous words such as bar or line
can also be disambiguated with reasonable compe-
tence (Agirre and Edmonds, 2007). However our ap-
proach is purely knowledge-based and employs se-
mantic graphs. This allows us to avoid the notorious

predicament Gale et al. (1992) name the information
bottleneck, in which supervised approaches fail to be
portable across alternative languages and domains
if the annotated corpora do not exist. Conversely,
knowledge-based approaches for WSD are usually
applicable to all words in unrestricted text (Mihal-
cea, 2007). It is this innate scalability that moti-
vates us to pursue knowledge-based approaches. Re-
gardless of whether sense inventories can maintain
knowledge-richness as they grow, their continued re-
finement by contributors is directly beneficial.

Knowledge-based approaches that employ se-
mantic graphs increasingly rival leading supervised
approaches to WSD. They can beat a Random or
LESK (Lesk, 1986) baseline (see Mihalcea (2005),
Navigli and Lapata (2007), Sinha and Mihalcea
(2007), Navigli and Lapata (2010)) and can com-
pete with or even beat the Most Frequent Sense
(MFS) baseline in certain contexts which is by no
means an easy task (see Navigli et al. (2007), Eneko
Agirre and Aitor Soroa (2009), Navigli and Ponzetto
(2012a)).

2 Methodology

PD is a framework for knowledge-based WSD ap-
proaches that employ semantic graphs. However be-
fore we can elaborate we must first cover the funda-
mental resources it is built upon.

2.1 Fundamental Resource Definitions
2.1.1 Lemma Sequences

At a glance across the text of any language, we ab-
sorb meaning and new information through its lexi-
cal composition. Depending on the length of text

250



we are reading, we could interpret it as one of many
structural subsequences of writing such as a para-
graph, excerpt, quote, verse, sentence, among many
others. LetW = (wa, ..., wb) be this subsequence of
words, which we will utilise as a sliding window for
PD. Again let W = (w1, ..., wm) be the larger body
of text of length m, such as a book, newspaper, or
corpus of text, that our sliding window of length b−a
moves through.

In SemEval Task 12 on Multilingual Word Sense
Disambiguation all words are lemmatised, which is
the process of unifying the different inflected forms
of a word so they can be analysed as a consolidated
lemma (or headword). Therefore words (or lexemes)
such as runs and ran are all mapped to their unifying
lemma run1.

To express this, let `w : W → L be a many-
to-one mapping from the sequence of words W to
the sequence of lemmas L, in which (wa, ..., wb) 7→
(`wa , ..., `wb) = (`a, ..., `b). To give an example
from the test data set2, the word sequenceW = (And,
it, ’s, nothing, that, runs, afoul, of, ethics, rules,
.) maps to the lemma sequence L = (and, it, be,
nothing, that, run, afoul, of, ethic, rule, .). In or-
der to complete this SemEval task we disambiguate
a large sequence of lemmas L = (`1, ..., `m), via our
lemma-based sliding window L = (`a, ..., `b).

2.1.2 Synsets
Each lemma `i ∈ L may refer up to k senses in

S(`i) = {si,1, si,2, ..., si,k} = S . Furthermore each
sense si,j ∈ S maps to a set of unique concepts in
the human lexicon. To clarify let us consider one
of the earliest examples of modern ambiguity taken
from Bar-Hillel’s (1960) critique of Machine Trans-
lation: W = (The, box, was, in, the, pen, .). The
sense of pen could be either a) a certain writing uten-
sil or b) an enclosure where small children can play,
therefore {senclosure, sutensil} ⊂ S(`pen) = S. Humans
can easily resolve the ambiguity between the pos-
sible senses of pen by accessing their own internal
lexicon and knowledge of the world they have built
up over time.

In the same vein, when accessing sense invento-
ries such as BabelNet, WordNet (Fellbaum, 1998),

1While all words are lemmatised, this task strictly focuses
on the WSD of noun phrases.

2This is sentence d010.s014 in the English test data set.

and Wikipedia which are discrete representations of
the human lexicon, we refer to each sense si,j ∈ S
as a synset. Depending on the sense inventory the
synset belongs to, it may contain alternative or trans-
lated lexicalisations, glosses, links to other semantic
resources, among a collection of semantically de-
fined relations to other synsets.

2.1.3 Subgraphs
PD makes use of subgraphs derived from a di-

rected graph G = (V, E) that can be crafted from
a sense inventory, such as BabelNet, WordNet, or
Wikipedia. We construct subgraphs using the Babel-
Net API which accesses BabelNet3 and Babel synset
paths4 indexed into Apache Lucene5 to ensure speed
of subgraph construction. This process is described
in Navigli and Ponzetto (2012a) and demonstrated
in Navigli and Ponzetto (2012b). Our formalisation
of subgraphs is adapted into our own notation from
the original papers of Navigli and Lapata (2007) and
Navigli and Lapata (2010). We refer the reader to
these listed sources if they desire an extensive ex-
planation of our subgraph construction as we have
built PD on top of the same code base therefore we
do not deviate from it.

For a given lemma sequence L = (`i, ..., `n) and
directed graph G = (V, E) we construct our sub-
graph GL = (VL, EL) in two steps:

1. Initialize VL :=
⋃n

i=1 S(`i) and EL := ∅.

2. For each node v ∈ VL, we perform a depth-
first search (DFS) of G, such that, every time
we encounter a node v′ ∈ VL (v′ 6= v) along a
path v, v1, ..., vk, v′ of length ≤ L in G, we add
all intermediate nodes and edges on the path
from v to v′, i.e., VL := VL ∪ {v1, ..., vk} and
EL := EL ∪ {{v, v1}, ..., {vk, v′}}.

2.2 Interpretation of Problem

For the lemmatisation of any word wi 7→ `i :
wi ∈ W, `i ∈ L, we must estimate the most ap-
propriate synset si,∗ ∈ S(`i) = {si,1, si,2, ..., si,k}.
Our system associates a PD score φ(si,j) for each

3BabelNet 1.1.1 API & Sense Inventory - http://lcl.
uniroma1.it/babelnet/download.jsp

4BabelNet 1.0.1 Paths - http://lcl.uniroma1.it/
babelnet/data/babelnet_paths.tar.bz2

5Apache Lucene - http://lucene.apache.org

251



si,j ∈ S(`i) by taking GL as input. We estimate
si,∗, the most appropriate sense for `i, by ŝi,∗ =
arg maxsi,j∈S(`i) φ(si,j). It’s worth noting here that
GL ensures the estimation of ŝi,∗ is not an indepen-
dent scoring rule, since GL embodies the context sur-
rounding `i via our sliding lemma-based window L.

2.3 Peripheral Diversity Framework
PD is built on the following two ideas that are ex-
plained in the following subsections:

1. For a subgraph derived from one lone lemma
`i, in which no other lemmas can provide con-
text, the synset si,j ∈ G`i that has the largest
and most semantically diverse set of peripheral
synset nodes is assumed to be the MFS for `i.

2. For a larger subgraph derived from a sliding
lemma window L, in which other lemmas can
provide context, the synset si,j ∈ GL that ob-
serves the largest increase in size and semantic
diversity of its peripheral synset nodes is esti-
mated to be si,∗, the most appropriate synset for
lemma `i.

Therefore PD is merely a framework that exploits
these two assumptions. Now we will go through the
process of estimating si,∗ for a given lemma `i.

2.3.1 Pairwise Semantic Dissimilarity
First, for each synset si,j ∈ S, we need to acquire

a set of its peripheral synsets. We do this by travel-
ling a depth of up to d (stopping if the path ends),
then adding the synset we reach to our set of periph-
eral synsets P≤d = {sj,1, sj,2, ..., sj,k′}.

Next for every pair of synsets v and v′ that are
not direct neighbours in P≤d such that v 6= v′,
we calculate their Pairwise Semantic Dissimilarity
(PSD) δ(v, v′) which we require for a synset’s
PD score. To generate our results for this task we
have used the complement to Cosine Similarity,
commonly known as the Cosine Distance as our
PSD measure:

δ(v, v′) =

1−
(

|O(v)∩O(v′)|√
|O(v)|

√
|O(v′)|

)
, if |O(v)||O(v′)| 6= 0

1, otherwise,

where O(v) is the outgoing (out-neighbouring)
synsets for v ∈ P≤d, and |O(v)| denotes the number
of elements in O(v).

2.3.2 Peripheral Diversity Score

Once we have PSD scores for every permitted
pairing of v and v′, we have a number of ways to
generate our φ(si,j) values. To generate our results
for this task, we chose to score synsets on the sum
of their minimum PSD values, which is expressed
formally below:
φ(si,j) =

∑
v∈P≤d(si,j)

min
v′ 6=v

v′∈P≤d(si,j)

δ(v, v′)

The idea is that this summing over the peripheral
synsets in P≤d(si,j) accounts for how frequently
synset si,j is used, then each increment in size is
weighted by a peripheral synset’s minimumal PSD
across all synsets in P≤d(si,j). Therefore periph-
eral set size and semantic diversity are rewarded
simultaneously by φ. To conclude, the final esti-
mated synset sequence for a given lemma sequence
(`1, ..., `m) based on φ is (ŝ1,∗, ŝ2,∗, ..., ŝm,∗).

2.3.3 Strategies, Parameters, & Filters

Wikipedia’s Did You Mean? We account for de-
viations and errors in spelling to ensure lemmas
have the best chance of being mapped to a synset.
Absent synsets in subgraph GL will naturally de-
grade system output. Therefore if `i 7→ ∅,
we make an HTTP call to Wikipedia’s Did you
mean? and parse the response for any alternative
spellings. For example in the test data set6 the
misspelt lemma: “feu de la rampe” is corrected to
“feux de la rampe”.

Custom Back-off Strategy As back-off strate-
gies7 have proved useful in (Navigli and Ponzetto,
2012a) and (Navigli et al., 2007), we designed our
own back-off strategy. In the event our system pro-
vides a null result, the Babel synset si,j ∈ S(`i) =
S with the most senses associated with it will be
chosen with preference to its region in BabelNet
such that WIKIWN �WN �WIKI.

6Found in sentence d001.s002.t005 in the French test
data set.

7In the event the WSD technique fails to provide an answer,
a back-off strategy provides one for the system to output.

252



Input Parameters We set our sliding window
length (b− a) to encompass 5 sentences at a time, in
which the step size is also 5 sentences. For subgraph
construction the maximum lengthL = 3. Finally we
set our peripheral search depth d = 3.

Filters For the purposes of reproducibility only
we briefly mention two filters we apply to our sub-
graphs that ship with the BabelNet API. We re-
move WordNet contributed domain relations with
the ILLEGAL POINTERS filter and apply the
SENSE SHIFTS filter. For more information on
these filters we suggest the reader consult the Ba-
belNet API documentation.

3 Results & Discussion

3.1 Results of SemEval Submission

Language DAEBAK! MFSBaseline +/-
DE German 59.10 68.60 -9.50
EN English 60.40 65.60 -5.20
ES Spanish 60.00 64.40 -4.40
FR French 53.80 50.10 +3.70
IT Italian 61.30 57.20 +4.10

Mean 58.92 61.18 -2.26

Table 1: DAEBAK! vs MFS Baseline on BabelNet

As can be seen in Table 1, the results of our single
submission were varied and competitive. The worst
result was for German in which our system fell be-
hind the MFS baseline by a margin of 9.50. Again
for French and Italian we exceeded the MFS base-
line by a margin of 3.70 and 4.10 respectively. Our
Daebak back-off strategy contributed anywhere be-
tween 1.12% (for French) to 2.70% (for Spanish) in
our results, which means our system outputs a re-
sult without the need for a back-off strategy at least
97.30% of the time. Overall our system was slightly
outperformed by the MFS baseline by a margin of
2.26. Overall PD demonstrated to be robust across
a range of European languages. With these prelimi-
nary results this surely warrants further investigation
of what can be achieved with PD.

3.2 Exploratory Results
The authors observed some inconsistencies in the
task answer keys across different languages as Ta-
ble 2 illustrates. For each Babel synset ID found in

the answer key, we record where its original source
synsets are from, be it Wikipedia (WIKI), WordNet
(WN), or both (WIKIWN).

Language WIKI WN WIKIWN
DE German 43.42% 5.02% 51.55%
EN English 10.36% 32.11% 57.53%
ES Spanish 30.65% 5.40% 63.94%
FR French 40.81% 6.55% 52.64%
IT Italian 38.80% 7.33% 53.87%

Table 2: BabelNet Answer Key Breakdown

This is not a critical observation but rather an
empirical enlightenment on the varied mechanics
of different languages and the amount of devel-
opment/translation effort that has gone into the
contributing subparts of BabelNet: Wikipedia and
WordNet. The heterogeneity of hybrid sense inven-
tories such as BabelNet creates new obstacles for
WSD, as seen in (Medelyan et al., 2013) it is diffi-
cult to create a disambiguation policy in this context.
Future work we would like to undertake would be to
investigate the heterogenous nature of BabelNet and
how this affects various WSD methods.

4 Conclusion & Future Directions

To conclude PD has demonstrated in its early stages
that it can perform well and even outperform the
MFS baselines in certain experimental contexts.
Furthermore it leaves a lot left to be explored in
terms of what this approach is capable of via adjust-
ing subgraph filters, strategies, and input parameters
across both heterogenous and homogenous semantic
graphs.

Acknowledgments

This research was completed with the help of the
Korean Foundation Graduate Studies Fellowship8.

5 Resources

The code base for this work can be found in the near
future at http://www.stevemanion.com/.

8KF Graduate Studies Fellowship - http://www.kf.
or.kr/eng/01_sks/sks_fel_sfb01.asp

253



References
Eneko Agirre and Philip Edmonds. 2007. Introduction.

Word Sense Disambiguation Algorithms and Applica-
tions, Chapter 1:1-28. Springer, New York.

Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing PageRank for Word Sense Disambiguation. In
Proceedings of the 12th Conference of the European
Chapter of the ACL, April:33–41. Association for
Computational Linguistics.

Yehoshua Bar-Hillel. 1960. The Present Status of Au-
tomatic Translation of Languages. Advances in Com-
puters, 1:91–163.

Christiane Fellbaum. 1998, ed. WordNet: An Electronic
Lexical Database., Cambridge, MA: MIT Press.

William A Gale, Kenneth W Church, David Yarowsky.
1992. A Method for Disambiguating Word Senses in a
Large Corpus. Computers and the Humanities, 26(5–
6):415–439.

Michael Lesk. 1986. Automatic Sense Disambiguation
Using Machine Readable Dictionaries: How to Tell
a Pine Cone from an Ice Cream Cone. Proceedings
of the 5th Annual International Conference on System
Documentation., 24–26. ACM.

Llus Màrquez, Gerard Escudero, David Martı́nez, Ger-
man Rigau. 2007. Supervised Corpus-Based Meth-
ods for WSD. Word Sense Disambiguation Algorithms
and Applications, Chapter 7:167-216. Springer, New
York.

Rada Mihalcea. 2005. Unsupervised Large-Vocabulary
Word Sense Disambiguation with Graph-based Algo-
rithms for Sequence Data Labeling. Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
411-418. Association for Computational Linguistics.

Rada Mihalcea. 2007. Knowledge-Based Methods
for WSD. Word Sense Disambiguation Algorithms
and Applications, Chapter 5:107–131. Springer, New
York.

Alyona Medelyan, Steve Manion, Jeen Broekstra, Anna
Divoli, Anna-lan Huang, and Ian H Witten. 2013.
Constructing a Focused Taxonomy from a Document
Collection Extended Semantic Web Conference, (Ac-
cepted, in press)

Roberto Navigli and Mirella Lapata. 2007. Graph con-
nectivity measures for unsupervised word sense dis-
ambiguation. IJCAI’07 Proceedings of the 20th In-
ternational Joint Conference on Artifical Intelligence,
1683–1688.

Roberto Navigli, Kenneth C Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: Coarse-
Grained English All-Words Task. In Proceedings of
the 4th International Workshop on Semantic Evalua-
tions, 30–35.

Roberto Navigli and Mirella Lapata. 2010. An Experi-
mental Study of Graph Connectivity for Unsupervised
Word Sense Disambiguation. IEEE transactions on
pattern analysis and machine intelligence, 32(4):678–
692.

Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217–250.

Roberto Navigli and Simone Paolo Ponzetto. 2012. Mul-
tilingual WSD with Just a Few Lines of Code: the Ba-
belNet API. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
67–72.

Roberto Navigli, David Jurgens, and Daniele Vannella.
2013. SemEval-2013 Task 12: Multilingual Word
Sense Disambiguation. Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013), in conjunction with the Second Joint Confer-
ence on Lexical and Computational Semantcis (*SEM
2013).

Frederic Nelson. 1976. Homographs American Speech,
51(3):296–297.

Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
Graph-based Word Sense Disambiguation Using Mea-
sures of Word Semantic Similarity. Proceedings of
IEEE International Conference on Semantic Comput-
ing.

254


