



















































Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1624â€“1633,
October 25-29, 2014, Doha, Qatar. cÂ©2014 Association for Computational Linguistics

Automatic Generation of Related Work Sections in Scientific Papers: 

An Optimization Approach 

 

 

 Yue Hu and Xiaojun Wan 

Institute of Computer Science and Technology 

The MOE Key Laboratory of Computational Linguistics 

Peking University, Beijing, China 

{ayue.hu,wanxiaojun}@pku.edu.cn 

 

 

  

 

Abstract 

In this paper, we investigate a challeng-

ing task of automatic related work gener-

ation. Given multiple reference papers as 

input, the task aims to generate a related 

work section for a target paper. The gen-

erated related work section can be used 

as a draft for the author to complete his 

or her final related work section. We 

propose our Automatic Related Work 

Generation system called ARWG to ad-

dress this task. It first exploits a PLSA 

model to split the sentence set of the giv-

en papers into different topic-biased parts, 

and then applies regression models to 

learn the importance of the sentences. At 

last it employs an optimization frame-

work to generate the related work section. 

Our evaluation results on a test set of 150 

target papers along with their reference 

papers show that our proposed ARWG 

system can generate related work sec-

tions with better quality. A user study is 

also performed to show ARWG can 

achieve an improvement over generic 

multi-document summarization baselines. 

1 Introduction 

The related work section is an important part of a 

paper. An author often needs to help readers to 

understand the context of his or her research 

problem and compare his or her current work 

with previous works. A related work section is 

often used for this purpose to show the differ-

ences and advantages of his or her work, com-

pared with related research works. In this study, 

we attempt to automatically generate a related 

work section for a target academic paper with its 

reference papers. This kind of related work sec-

tions can be used as a basis to reduce the authorâ€™s 

time and effort when he or she wants to complete 

his or her final related work section. 

Automatic related work section generation is a 

very challenging task. It can be considered a top-

ic-biased, multiple-document summarization 

problem. The input is a target academic paper, 

which has no related work section, along with its 

reference papers. The goal is to create a related 

work section that describes the related works and 

addresses the relationship between the target pa-

per and the reference papers. Here we assume 

that the set of reference papers has been given as 

part of the input. Existing works in the NLP and 

recommendation systems communities have al-

ready focused on the task of finding reference 

papers. For example, citation prediction (Nal-

lapati et al., 2008) aims at finding individual pa-

per citation patterns. 

Generally speaking, automatic related work 

section generation is a strikingly different prob-

lem and it is much more difficult in comparison 

with general multi-document summarization 

tasks. For example, multi-document summariza-

tion of news articles aims at synthesizing con-

tents of similar news and removing the redundant 

information contained by the different news arti-

cles. However, each scientific paper has much 

specific content to state its own work and contri-

bution. Even for the papers that investigate the 

same research topic, their contributions and con-

tents can be totally different. The related work 

section generation task needs to find the specific 

contributions of individual papers and arrange 

them into one or several paragraphs. 

In this study, we focus on the problem of au-

tomatic related work section generation and pro-

pose a novel system called ARWG to address the 

1624



problem. For the target paper, we assume that the 

abstract and introduction sections have already 

been written by the author and they can be used 

to help generate the related work section. For the 

reference papers, we only consider and extract 

the abstract, introduction, related work and con-

clusion sections, because other sections like the 

method and evaluation sections always describe 

the extreme details of the specific work and they 

are not suitable for this task. Then we generate 

the related work section using both sentence sets 

which are extracted from the target paper and 

reference papers, respectively. 

Firstly, we use a PLSA model to group both 

sentence sets of the target paper and its reference 

papers into different topic-biased clusters. Sec-

ondly, the importance of each sentence in the 

target paper and the reference papers is learned 

by using two different Support Vector Regres-

sion (SVR) models. At last, a global optimization 

framework is proposed to generate the related 

work section by selecting sentences from both 

the target paper and the reference papers. Mean-

while, the framework selects sentences from dif-

ferent topic-biased clusters globally. 

Experimental results on a test set of 150 target 

papers show our method can generate related 

work sections with better quality than those of 

several baseline methods. With the ROUGE 

toolkit, the results indicate the related work sec-

tions generated by our system can get higher 

ROUGE scores. Moreover, our related work sec-

tions can get higher rating scores based on a user 

study. Therefore, our related work sections can 

be much more suitable for the authors to prepare 

their final related work sections. 

2 Related Work 

There are few studies to directly address auto-

matic related work generation. Hoang and Kan 

(2010) proposed a related work summarization 

system given the set of keywords arranged in a 

hierarchical fashion that describes the paperâ€™s 

topic. They used two different rule-based strate-

gies to extract sentences for general topics as 

well as detailed ones. 

A few studies focus on multi-document scien-

tific article summarization. Agarwal et al., (2011) 

introduced an unsupervised approach to the prob-

lem of multi-document summarization. The input 

is a list of papers cited together within the same 

source article. The key point of this approach is a 

topic based clustering of fragments extracted 

from each co-cited article. They rank all the clus-

ters using a query generated from the context 

surrounding the co-cited list of papers. Yeloglu 

et al., (2011) compared four different approaches 

for multi-document scientific articles summariza-

tion: MEAD, MEAD with corpus specific vo-

cabulary, LexRank and W3SS. 

Other studies investigate mainly on the single-

document scientific article summarization. Early 

works including (Luhn 1958; Baxendale 1958; 

Edumundson 1969) tried to use various features 

specific to scientific text (e.g., sentence position, 

or rhetorical clues features). They have proved 

that these features are effective for the scientific 

article summarization. Citation information has 

been already shown effective in summarize the 

scientific articles. Works including (Mei and 

Zhai 2008; Qazvinian and Radev 2008; Schwartz 

and Hearst 2006; Mohammad et al., 2009) em-

ployed citation information for the single scien-

tific article summarization. Earlier work (Nakov 

et al., 2004) indicated that citation sentences may 

contain important concepts that can give useful 

descriptions of a paper. 

Various methods have been proposed for news 

document summarization, including rule-based 

methods (Barzilay and Elhadad 1997; Marcu and 

Daniel 1997), graph-based methods (Mani and 

Bloedorn 2000; Erkan and Radev 2004; Michal-

cea and Tarau 2005), learning-based methods 

(Conroy et al., 2001; Shen et al., 2007; Ouyang 

et al., 2007; Galanis et al., 2008), optimization-

based methods (McDonald 2007; Gillick et al., 

2009; Xie et al., 2009; Berg-Kirkpatrick et al., 

2011; Lei Huang et al., 2011; Woodsend et al., 

2012; Galanis 2012), etc. 

The most relevant work is (Hoang and Kan, 

2010) as mentioned above. They also assumed 

the set of reference papers was given as part of 

the input. They also adopt the hierarchical topic 

tree that describes the topic structure in the target 

paper as an essential input for their system. 

However, it is non-trivial to build the hierar-

chical topic tree. Moreover, they do not consider 

the content of the target paper to construct the 

related work section, which is actually crucial in   

the related work section. To the best of our 

knowledge, no previous works have used super-

vised learning and optimization framework to 

deal with the multiple scientific article summari-

zation tasks. 

3 Problem Analysis and Corpus 

3.1 Problem Analysis 

1625



We firstly analyze the structure of related work 

sections briefly. By using examples for illustra-

tion, we can gain insight on how to generate re-

lated work sections. A specific related work ex-

ample is shown in Figure 1. 

This related work section introduces previous 

related works for a paper on Automatic Taxono-

my Induction. From Figure 1, we can have a 

glance at the structure of related work sections. 

Related work sections usually discuss several 

different topics, such as â€œpattern-basedâ€ and 
â€œcluster-basedâ€ approaches shown in the Figure 

1. Besides the knowledge of previous works, the 

author often compares his own work with the 

previous works. The differences and advantages 

are generally mentioned. The example in Figure 

1 also indicates this phenomenon. 

Therefore, we design our system to generate 

related work sections according to the related 

work section structure mentioned above. Our 

system takes the target paper for which a related 

work section needs to be drafted besides its ref-

erence papers as input. The goal of our system is 

to generate a related work section with the above 

structure. The generated related work section 

should have several topic-biased parts. The au-

thor's own work is also needed to be described 

and its difference with other works is needed to 

be emphasized on. 

3.2 Corpus and Preprocessing 

We build a corpus that contains academic papers 

and their corresponding reference papers. The 

academic papers are selected from the ACL An-

thology 1 . The ACL Anthology currently hosts 

                                                 
1 http://aclweb.org/anthology/ 

over 24,500 papers from major conferences such 

as ACL, EMNLP, COLING in the fields of com-

putational linguistics and natural language pro-

cessing. We remove the papers that contain relat-

ed work sections with very short length, and ran-

domly select 1050 target papers to construct our 

whole corpus. 

The papers are all in PDF format. We extract 

their texts by using PDFlib 2  and detect their 

physical structures of paragraphs, subsections 

and sections by using ParsCit3 . For the target 

papers, the related work sections are directly ex-

tracted as the gold summaries. The references are 

also extracted. For the references that can be 

found in the ACL Anthology, we download them 

from the ACL Anthology. The other reference 

papers are searched and downloaded by using 

Google Scholar. References to books and PhD 

theses are discarded, for their verbosity may 

change the problem drastically (Mihalcea and 

Ceylan, 2007). 

The input of our system includes the abstract 

and introduction sections of the target paper, and 

the abstract, introduction, related work and con-

clusion sections of the reference papers. As men-

tioned above, the method and evaluation sections 

in the reference papers are not used as input be-

cause these sections usually describe extreme 

details of the methods and evaluation results and 

they are not suitable for related work generation. 

Note that it is reasonable to make use of the ab-

stract and introduction sections of the target pa-

per to help generate the related work section, 

because an author usually has already written the 

abstract and introduction sections before he or 

she wants to write the related work section for 

the target paper.  Otherwise, we cannot get any 

information about the authorâ€™s own work. All 

other sections in the target paper are not used.  

4 Our Proposed System 

4.1 Overview 

In this paper, we propose a system called ARWG 

to automatically generate a related work section 

for a given target paper. The architecture of our 

system is shown in Figure 2. We take both the 

target paper and its reference papers as input and 

they are represented by several sections men-

tioned in Section 3.2. After preprocessing, we 

extract the feature vectors for sentences in the 

target paper and the reference papers, respective-

                                                 
2 http://www.pdflib.com/ 
3 http://aye.comp.nus.edu.sg/parsCit/ 

 

 

Figure 1: A sample related work section (Yang and 

Callan 2009) 

There has been a substantial amount of research on automatic 
taxonomy induction. As we mentioned earlier, two main 
approaches are pattern-based and clustering-based.
Pattern-based approaches are the main trend for automatic 
taxonomy induction. â€¦
Pattern-based approaches started from and still pay a great deal 
of attention to the most common is-a relations. â€¦

Clustering-based approaches usually represent word contexts as 
vectors and cluster words based on similarities of the vectors 
(Brown et al., 1992; Lin, 1998). â€¦

Many clustering-based approaches face the challenge of 
appropriately labeling non-leaf clusters. â€¦ In this paper, we take 
an incremental clustering approach,... The advantage of the 
incremental approach is that it eliminates the trouble of 
inventing cluster labels and concentrates on placing terms in the 
correct positions in a taxonomy hierarchy.
The work by Snow et al. (2006) is the most similar to ours â€¦ 
Moreover, our approach employs heterogeneous features from a 
wide range; while their approach only used syntactic dependency.

Two different 
topics

Comparison 
with the 

authorâ€™s work

1626



ly. The importance scores for sentences in the 

target paper and the reference papers are as-

signed by using two SVR based sentence scoring 

models. The two SVR models are trained for 

sentences in the target paper and the reference 

papers, respectively. Meanwhile, a topic model is 

applied to the whole set of sentences in both the 

target paper and reference papers. The sentences 

are grouped into several different topic-biased 

clusters. The sentences with importance scores 

and topic cluster information are taken as the 

input for the global optimization framework. The 

optimization framework extracts sentences to 

describe both the authorâ€™s own work and back-

ground knowledge. More details of each part will 

be discussed in the following sections. 

4.2 Topic Model Learning 

As mentioned in the previous section, the related 

work section usually addresses several different 

topics. The topics may be different research 

themes or different aspects of a broad research 

theme. The related work section should describe 

the specific details for each topic, respectively. 

Therefore, we aim to discover the hidden top-

ics of the input papers, and we use the Probabil-

istic latent semantic analysis (PLSA) (Hofmann, 

1999) to solve this problem.  

The PLSA approach models each word in a 

document as a sample from a mixture model. 

The mixture components are multinomial ran-

dom variables that can be viewed as representa-

tions of â€œtopicsâ€. Different words in a document 

may be generated from different topics. Each 

document is represented a list of mixing propor-

tions for these mixture components and can be 

reduced to a probability distribution on a fixed 

set of topics. 

Considering that the sentences in one paper 

may relate to different topics, we treat each sen-

tence as a â€œdocumentâ€ d. We treat the noun 

phases in the sentences as the â€œwordsâ€ w. In or-

der to extract the noun phrases, chunking imple-

mented by the OpenNLP toolkit 4 is applied to 

the sentences. Noun phrases that contain words 

such as â€œpaperâ€ and â€œdataâ€ are discarded.  

Then the sentences with their corresponding 

noun phrases are taken as input into the PLSA 

model. Here both the sentences in the target pa-

per and the sentences in the reference papers are 

treated the same in the model. Finally, we can 

get the sentence set with topic information and 

use it in the subsequent steps. Each sentence has 

a topic weight t in each topic. 

4.3 Sentence Important Assessment 

In our proposed system, sentence importance 

assessment aims to assign an importance score to 

each sentence in the target paper and reference 

papers. The score of each sentence will be used 

in the subsequent optimization framework. We 

propose to use the support vector regression 

model to achieve this goal. In the above topic 

model learning process, we do not distinguish the 

sentences in the target paper and reference pa-

pers. In contrast, we train two different support 

vector regression models separately for the sen-

tences in the target paper and the sentences in the 

reference papers. In the related work section, the 

sentences that describe the authorâ€™s own work 

usually address the differences from the related 

works, while the sentences that describe the re-

lated works often focus on the specific details. 

We think the two kinds of sentences should be 

treated differently. 

Scoring Method 

To construct training data based on the papers 

collected, we apply a similarity scoring method 

to assign the importance scores to the sentences 

in the papers. The main hypothesis is that the 

sentences in the gold related work sections 

should summarize the target paper and reference 

papers as well. Thus the sentences in the papers 

which are more similar to the sentences in the 

gold related work sections should be considered 

more important and suitable to be selected. Our 

scoring method should assign higher scores to 

them. 

                                                 
4 http://opennlp.apache.org/ 

 

 

Figure 2: System Architecture 

Target paper Reference papers

Preprocessing

Topic ModelSentence Score 
Assessment(target)

Sentence Score 
Assessment(reference)

Optimization 
Framework

Postprocessing
Related Work 

Section

1627



We define the importance score of a sentence 

in the papers as below: 

ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘ ) =  ğ‘šğ‘ğ‘¥
ğ‘ ğ‘–

âˆ—âˆˆğ‘†âˆ—
(ğ‘ ğ‘–ğ‘š(ğ‘ , ğ‘ ğ‘–

âˆ—))      (1) 

where s is a sentence in the papers,  ğ‘†âˆ— is the set 

of the sentences in the corresponding gold relat-

ed work section. The standard cosine measure is 

employed as the similarity function. 

Considering the difference between the sen-

tences that describe the authorâ€™s work and the 

sentences that describe the related works, we 

split the set of sentences in the gold related work 

section into two parts: one discusses the authorâ€™s 

own work and the other introduces the related 

works. We observe that sentences related to the 

authorâ€™s own work often feature specific words 

or phrases (such as â€œweâ€, â€œour workâ€, â€œin this 

paperâ€ etc.) in the related work section.  So we 

check the sentences about whether they contain 

clue words or phrases (i.e., â€œin this paperâ€, â€œour 

workâ€ and 18 other phrases). If the clue phrase 

check fails, the sentence belongs to the related 

work part. If not, it belongs the own work part. 

Thus for the sentences in the target paper,  ğ‘†âˆ— 

is the set of sentences in the own work part of the 

gold related work section, while for the sentences 

in the reference papers,  ğ‘†âˆ— is the set of sentences 

in the related work part of the gold related work 

section. Then we can use the scoring method to 

compute the target scores of the sentences in the 

training set. It is noteworthy that two SVR mod-

els can be trained on the two parts of the training 

data, respectively.  

Feature 

Each sentence is represented by a set of features. 

The common features used for the sentences of 

the target paper and reference papers are shown 

in Table 1. The additional features applied to the 

sentences of the target paper are introduced in 

Table 2. 

Here, s is a sentence that needs to extract fea-

tures. th is paper title, section headings and sub-

section headings set of the reference papers or 

target paper for the two SVR models, respective-

ly. Each feature with â€œ*â€ represent a feature set 

that contains similar features. 

All the features are scaled into [0, 1]. Thus we 

can learn SVR models based on the features and 

importance scores of the sentences, and then use 

the models to predict an importance score for 

each sentence in the test set. The SVR models 

are trained and applied for the target paper and 

reference papers, respectively. 

Table 1: Common features employed in the SVR 

models 

Feature Description 

ğ‘†ğ‘–ğ‘š(ğ‘ , ğ‘¡â„)âˆ— The similarity between s and each 
title in th; Stop words are removed 

and stemming is employed. 

WS(s,th) Number of words shared by s and 

th. 

ğ‘†ğ‘ƒ(ğ‘ )âˆ— The position of s in its section or 
subsection 

ğ‘ƒğ‘‡ğ¼(ğ‘ )âˆ— The parse tree information of s, 
including the number of noun 

phrase and verb phrases, the depth 

of the parse tree, etc. 

ğ¼ğ‘ ğ»ğ‘’ğ‘ğ‘‘(ğ‘ )âˆ— Indicates whether s is the first sen-
tence of the section or subsection 

ğ¼ğ‘ ğ¸ğ‘›ğ‘‘(ğ‘ )âˆ— Indicates whether s is the last sen-
tence of the section or subsection 

SWP(s) The percentage of the stop words  

Length(s) The length of sentence s 

Length_rw(s) The length of s after removing stop 

words 

SI(s) The section index of s that indi-

cates which section s is from. 

ğ¶ğ‘™ğ‘¢ğ‘’ğ‘ƒâ„ğ‘Ÿğ‘ğ‘ ğ‘’(ğ‘ )âˆ— Indicates whether a clue phrase 
appears in s. the clue phrases in-

clude â€œour workâ€, â€œproposeâ€ and 

other 20 words. Each clue phrase 

corresponds to one feature. 

 

Table 2: Additional features for sentences in the 

target paper 

Feature Description 

HasCitation(s) Indicates whether s contains a 

citation 

ğ‘ƒhğ‘Ÿğ‘ğ‘ ğ‘’ğ¹ğ‘œğ‘Ÿğ¶ğ‘šğ‘(ğ‘ )âˆ— Indicates whether s contains 
words or phrases used for com-

parison such as â€œin contrastâ€, 

â€œinsteadâ€ and other 26 words. 

Each word or phrase corre-

sponds to one feature. 

 

4.4 A Global Optimization Framework 

In the above steps, we can get the predicted im-

portance score and topic information for each 

sentence in the target paper and reference papers. 

Here, we introduce a global optimization frame-

work to generate the related work section. 

According to the structure of the related work 

section mentioned above, the related work sec-

tion usually discusses several topics. In each top-

ic, the related works and their details are intro-

duced. Besides, the author often compares his 

own work with these previous works. 

Therefore, we propose to formulate the genera-

tion as an optimization problem. Basically, we 

will be searching for a set of sentences to opti-

mize the objective function. 

1628



Table 3: Notations used in this section 

Symbol Description 

ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘– the sentence in the reference/target paper 
ğ‘™ğ‘Ÿğ‘–/ğ‘™ğ‘¡ğ‘– the length of sentence ğ‘ ğ‘Ÿğ‘–/ ğ‘ ğ‘¡ğ‘– 

ğ‘¤ğ‘Ÿğ‘–/ğ‘¤ğ‘¡ğ‘– the importance score of ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘– 

ğ‘¥ğ‘Ÿğ‘–ğ‘—/ğ‘¥ğ‘¡ğ‘–ğ‘— indicates whether ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘– is selected into 
the part of topic j in the generated related 

work section 

nr/nt the number of sentences in the refer-

ence/target papers 

m the topic count 

ğ‘¡ğ‘–ğ‘— the topic weight of  ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘–  in topic j from 
the PLSA model 

B the set of unique bigrams 

ğ‘¦ğ‘– indicates whether bigram ğ‘ğ‘–  is included 
in the result 

ğ‘ğ‘ğ‘– the count of the occurrences of bigram ğ‘ğ‘– 
in the both target paper and reference 

papers 

ğ¿ğ‘šğ‘ğ‘¥ the maximum word count of the related 
work section 

ğ¿ğ‘—  the maximum word count of the part of 
topic j which depends on the percentage 

of sentences belong to topic j 

ğµâˆ— the total set of bigrams in the whole pa-
per set 

ğµğ‘– the set of bigrams that sentence  ğ‘ ğ‘Ÿğ‘–/ğ‘ ğ‘¡ğ‘– 
contains 

ğ‘†ğ‘Ÿğ‘š/ğ‘†ğ‘¡ğ‘š the set of sentences that include bigram 
ğ‘ğ‘š in the reference/target papers 

 ğœ†1,  ğœ†2, ğœ†3 parameters for tuning 

 

To design the objective function, three aspects 

should be considered: 

1) First, the related work section we generate 
should introduce the previous works well. In 

our assumption, sentences with higher im-

portance scores are better to be selected. In 

addition, very short sentences should be pe-

nalized. So we introduce the first part of our 

objective function below: 

âˆ‘ (ğ‘™ğ‘Ÿğ‘–ğ‘¤ğ‘Ÿğ‘– âˆ‘ ğ‘¡ğ‘–ğ‘—ğ‘¥ğ‘Ÿğ‘–ğ‘—)
ğ‘š
ğ‘—=1

ğ‘›ğ‘Ÿ
ğ‘–=1                    (2) 

We add the sentence length as a multipli-

cation factor in order to penalize the very 

short sentences, or the objective function 

tends to select more and shorter sentences. 

At the same time, the objective function does 

not tend to select the very long sentences. 

The total length of the sentences selected is 

fixed. So if the objective function tends to 

select the longer sentences, the fewer sen-

tences can be selected. A tradeoff needs to be 

made between the number and the average 

length of the sentences selected. 

 The constraints introduced below ensure 

that the sentence can only be selected into 

one topic and the topic weight is used to 

measure the degree that the sentence is rele-

vant to the specific topic. 

2) Second, similar to the first part, we should 
consider the own work part of the related 

work section. Thus the second part of our ob-

jective function is shown as follows: 

âˆ‘ (ğ‘™ğ‘¡ğ‘–ğ‘¤ğ‘¡ğ‘– âˆ‘ ğ‘¡ğ‘–ğ‘—ğ‘¥ğ‘¡ğ‘–ğ‘—)
ğ‘š
ğ‘—=1

ğ‘›ğ‘¡
ğ‘–=1                    (3) 

3) At last, redundancy reduction should be con-
sidered in the objective function. The last 

part of the objective function is shown below: 

âˆ‘ ğ‘ğ‘ğ‘–ğ‘¦ğ‘–
|ğµ|
ğ‘–=1                               (4) 

The intuition is that the more unique bi-

grams the related work section contains, the 

less redundancy the related work section has. 

We add  ğ‘ğ‘ğ‘– as the weight of the bigram in or-

der to include more important bigrams. 

By combing all the parts defined above, we 

have the following full objective function: 

max
ğ‘¥ğ‘Ÿ,ğ‘¥ğ‘¡

ğœ†1 âˆ‘ (
ğ‘™ğ‘Ÿğ‘–

ğ›¼ğ¿ğ‘šğ‘ğ‘¥
ğ‘¤ğ‘Ÿğ‘– âˆ‘ ğ‘¡ğ‘–ğ‘—ğ‘¥ğ‘Ÿğ‘–ğ‘—)

ğ‘š
ğ‘—=1 +

ğ‘›ğ‘Ÿ
ğ‘–=1

ğœ†2 âˆ‘ (
ğ‘™ğ‘¡ğ‘–

(1âˆ’ğ›¼)ğ¿ğ‘šğ‘ğ‘¥
ğ‘¤ğ‘¡ğ‘– âˆ‘ ğ‘¡ğ‘–ğ‘—ğ‘¥ğ‘¡ğ‘–ğ‘—)

ğ‘š
ğ‘—=1

ğ‘›ğ‘¡
ğ‘–=1 +

ğœ†3 âˆ‘
ğ‘ğ‘ğ‘–ğ‘¦ğ‘–

|ğµâˆ—|

|ğµ|
ğ‘–=1                                                       (5) 

Subject to: 

âˆ‘ ğ‘™ğ‘Ÿğ‘–ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘›ğ‘Ÿ
ğ‘–=1 + âˆ‘ ğ‘™ğ‘¡ğ‘–ğ‘¥ğ‘¡ğ‘–ğ‘—

ğ‘›ğ‘¡
ğ‘–=1 <  ğ¿ğ‘—, ğ‘“ğ‘œğ‘Ÿ ğ‘— = 1, â€¦ ,

ğ‘š                        (6) 

âˆ‘ âˆ‘ ğ‘™ğ‘Ÿğ‘–ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘š
ğ‘—=1

ğ‘›ğ‘Ÿ
ğ‘–=1 <  ğ›¼ğ¿ğ‘šğ‘ğ‘¥         (7) 

âˆ‘ âˆ‘ ğ‘™ğ‘¡ğ‘–ğ‘¥ğ‘¡ğ‘–ğ‘—
ğ‘š
ğ‘—=1

ğ‘›ğ‘¡
ğ‘–=1 <  (1 âˆ’ ğ›¼)ğ¿ğ‘šğ‘ğ‘¥         (8) 

 âˆ‘ ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘š
ğ‘—=1 â‰¤ 1, ğ‘“ğ‘œğ‘Ÿ ğ‘– = 1, â€¦ , ğ‘›ğ‘Ÿ           (9) 

âˆ‘ ğ‘¥ğ‘¡ğ‘–ğ‘—
ğ‘š
ğ‘—=1 â‰¤ 1, ğ‘“ğ‘œğ‘Ÿ ğ‘– = 1, â€¦ , ğ‘›ğ‘¡          (10) 

âˆ‘ ğ‘¦ğ‘˜ğ‘ğ‘˜âˆˆğµğ‘–  â‰¥ |ğµğ‘–| âˆ‘ ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘š
ğ‘—=1 , ğ‘“ğ‘œğ‘Ÿ ğ‘– = 1, â€¦ , ğ‘›ğ‘Ÿ (11) 

âˆ‘ ğ‘¦ğ‘˜ğ‘ğ‘˜âˆˆğµğ‘–  â‰¥ |ğµğ‘–| âˆ‘ ğ‘¥ğ‘¡ğ‘–ğ‘—
ğ‘š
ğ‘—=1 , ğ‘“ğ‘œğ‘Ÿ ğ‘– = 1, â€¦ , ğ‘›ğ‘¡ (12) 

âˆ‘ âˆ‘ ğ‘¥ğ‘Ÿğ‘–ğ‘—
ğ‘š
ğ‘—=1 + âˆ‘ âˆ‘ ğ‘¥ğ‘¡ğ‘–ğ‘—

ğ‘š
ğ‘—=1ğ‘ ğ‘¡ğ‘–âˆˆğ‘†ğ‘¡ğ‘˜ğ‘ ğ‘Ÿğ‘–âˆˆğ‘†ğ‘Ÿğ‘˜ â‰¥ ğ‘¦ğ‘˜ ,

ğ‘˜ = 1, â€¦ |ğµ|                                                    (13) 

ğ‘¥ğ‘Ÿğ‘–ğ‘—, ğ‘¥ğ‘¡ğ‘–ğ‘—, ğ‘¦ğ‘– âˆˆ {0,1}                  (14) 

All the three parts in the objective function are 

normalized to [0, 1] by using the maximum 

length ğ¿ğ‘šğ‘ğ‘¥ and the total number of bigrams |ğµ
âˆ—|. 

 ğœ†1, ğœ†2 and ğœ†3 are parameters for tuning the three 
parts and we set  ğœ†1+ğœ†2+ğœ†3 = 1. 

We explain the constraints as follows: 

Constraint (6): It ensures that the total word 

count of the part of topic j does not exceed  ğ¿ğ‘—. 
Constraints (7), (8): The two constraints try to 

balance the lengths of the previous works part 

and the own work part, respectively. ğ›¼ is set to 
2/3. 

Constraints (9), (10): These two constraints 

guarantee that the sentence can only be included 

into one topic. 

1629



Constraints (11), (12): When these two con-

straints hold, all bigrams that ğ‘ ğ‘– has are selected 
if ğ‘ ğ‘– is selected.  

Constraint (13): This constraint makes sure 

that at least one sentence in ğ‘†ğ‘Ÿğ‘š or ğ‘†ğ‘¡ğ‘š is select-
ed if bigram ğ‘ğ‘š is selected. 

Therefore, we transform our optimization 

problem into a linear programing problem. We 

solve this linear programming problem by using 

the IBM CPLEX optimizer5. It generally takes 

tens of seconds to solve the problem and it is 

very efficient. 

Finally, ARWG post-processes sentences to 

improve readability, including replacing agentive 

forms with a citation to the specific article (e.g., 

â€œour workâ€ â†’ â€œ(Hoang and Kan, 2010)â€) for the 
sentences extracted from reference papers. The 

sentences belonging to different topics are placed 

separately.  

5 Evaluation 

5.1 Evaluation Setup 

To set up our experiments, we divide our dataset 

which contains 1050 target papers and their ref-

erence papers into two parts: 700 target papers 

for training, 150 papers for test and the other 200 

papers for validation. The PLSA topic model is 

applied to the whole dataset. We train two SVR 

regression models based on the own work part 

and the previous work part of the training data 

and apply the models to the test data. The global 

optimization framework is used to generate the 

related work sections. We set the maximum word 

count of the generated related work section to be 

equal to that of the gold related work section. 

The parameter values of  ğœ†1, ğœ†2 and ğœ†3 are set to 
0.3, 0.1 and 0.6, respectively. The parameter val-

ues are tuned on the validation data.  

We compare our system with five baseline sys-

tems: MEAD-WT, LexRank-WT, ARWG-WT, 

MEAD and LexRank. MEAD 6  (Radev et al., 

2004) is an open-source extractive multi-

document summarizer. LexRank 7  (Eran and 

Radev, 2004) is a multi-document summarization 

system which is based on a random walk on the 

similarity graph of sentences. We also implement 

the MEAD, LexRank baselines and our method 

                                                 
5 www-01.ibm.com/software/integration/optimization/cplex-
optimizer/ 
6 http://www.summarization.com/mead/ 
7 In our experiments, LexRank performs much better than 
the more complex variant - C-LexRank (Qazvinian and 
Radev, 2008), and thus we choose LexRank, rather than C-
LexRank, to represent graph-based summarization methods 
for comparison in this paper.  

with only the reference papers (i.e. the target pa-

perâ€™s content is not considered). Those methods 

are signed by â€œ-WTâ€.  

To evaluate the effectiveness of the SVR mod-

els we employ, we implement a baseline system 

RWGOF that uses the random walk scores as the 

important scores of the sentences and take the 

scores as inputs for the same global optimization 

framework as our system to generate the related 

work section. The random walk scores are com-

puted for the sentences in the reference papers 

and the target paper, respectively. 

We use the ROUGE toolkit to evaluate the 

content quality of the generated related work sec-

tions. ROUGE (Lin, 2004) is a widely used au-

tomatic summarization evaluation method based 

on n-gram comparison. Here, we use the F-

Measure scores of ROUGE-1, ROUGE-2 and 

ROUGE-SU4. The model texts are set as the 

gold related work sections extracted from the 

target papers, and word stemming is utilized. 

ROUGE-N is an n-gram based measure between 

a candidate text and a reference text. The recall 

oriented score, the precision oriented score and 

the F-measure score for ROUGE-N are comput-

ed as follows: 
     ğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ‘ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™  

= âˆ‘ âˆ‘ ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘šğ‘ğ‘¡ğ‘â„(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›)ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›ğ‘†âˆˆ{ğ‘…ğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ ğ‘‡ğ‘’ğ‘¥ğ‘¡}  / 

   âˆ‘ âˆ‘ ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›)ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›ğ‘†âˆˆ{ğ‘…ğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ ğ‘‡ğ‘’ğ‘¥ğ‘¡}        (15) 

     ğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ‘ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›  

= âˆ‘ âˆ‘ ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘šğ‘ğ‘¡ğ‘â„(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›)ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›ğ‘†âˆˆ{ğ‘…ğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ ğ‘‡ğ‘’ğ‘¥ğ‘¡}  / 

   âˆ‘ âˆ‘ ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›)ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›ğ‘†âˆˆ{ğ¶ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ ğ‘‡ğ‘’ğ‘¥ğ‘¡}       (16) 

     ğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ‘ğ¹âˆ’ğ‘šğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’ 

= 2 âˆ— ğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ‘ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ âˆ— ğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ‘ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› / 

   ğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ‘ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ + ğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ‘ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›       (17) 

where n stands for the length of the n-gram 

ğ‘”ğ‘Ÿğ‘ğ‘šğ‘› , and ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘šğ‘ğ‘¡ğ‘â„(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›) is the maxi-
mum number of n-grams co-occurring in a can-

didate text and a reference text. 

In addition, we conducted a user study to sub-

jectively evaluate the related work sections to get 

more evidences. We selected the related work 

sections generated by different methods for 15 

random target papers in the test set. We asked 

three human judges to follow an evaluation 

guideline we design and evaluate these related 

work sections. The human judges are graduate 

students in the computer science field and they 

did not know the identities of the evaluated relat-

ed work sections. They were asked to give a rat-

ing on a scale of 1 (very poor) to 5 (very good) 

for the correctness, readability and usefulness of 

the related work sections, respectively: 

1630



1) Correctness: Is the related work section ac-
tually related to the target paper? 

2) Readability: Is the related work section 
easy for the readers to read and grasp the 

key content? 

3) Usefulness: Is the related work section 
useful for the author to prepare their final 

related work section? 

Paired T-Tests are applied to both the ROUGE 

scores and rating scores for comparing ARWG 

and baselines and comparing the systems with 

WT and without WT. 

5.2 Results and Discussion 

Table 4: ROUGE F-measure comparison results 
Method ROUGE-1 ROUGE-2 ROUGE-

SU4 
Mead-

WT 

0.39720 0.08785 0.14694 

LexRank-
WT 

0.43267 0.09228 0.16312 

ARWG-

WT 
0.45077âˆ—{1,2} 0.09987âˆ—{1,2} 0.16731âˆ—{1}#{2} 

Mead 0.41012âˆ—{1} 0.09642âˆ—{1} 0.15441âˆ—{1} 

LexRank 0.44235âˆ—{2} 0.10090âˆ—{2} 0.17067âˆ—{2} 

ARWG ğŸ. ğŸ’ğŸ•ğŸ—ğŸ’ğŸâˆ—{ğŸâˆ’ğŸ“} ğŸ. ğŸğŸğŸğŸ•ğŸ”âˆ—{ğŸâˆ’ğŸ“} ğŸ. ğŸğŸ–ğŸ”ğŸğŸ–âˆ—{ğŸâˆ’ğŸ“} 

(* represents pairwise t-test value p < 0.01; # rep-

resents p < 0.05; the numbers in the brackets rep-

resent the indices of the methods compared, e.g. 

1 for MEAD-WT, 2 for LexRank-WT, etc.) 

 

Table 5: Average rating scores of judges 
Method Correctness Readability Usefulness 

Mead 2.971 2.664 2.716 

LexRank 2.958 2.847 2.784 

ARWG 3.433âˆ—# 3.420âˆ—# 3.382âˆ—# 

(*# represents pairwise t-test value p < 0.01, 

compared with Mead and LexRank, respectively.)  

 

Table 6: ROUGE F-measure comparison of dif-

ferent sentence importance scores 
Method ROUGE-1 ROUGE-2 ROUGE-SU4 

RWGOF 0.46932 0.11791 0.18426 

ARWG 0.47940 0.12176 0.18618 

 

The evaluation results over ROUGE metrics are 

presented in Table 4. It shows that our proposed 

system can get higher ROUGE scores, i.e., better 

content quality. In our system, we split the sen-

tence set into different topic-biased parts, and the 

importance scores of sentences in the target pa-

per and reference papers are learned differently. 

So the obtained importance scores of the sen-

tences are more reliable.  

The global optimization framework considers 

the extraction of both the previous work part and 

the own work part. We can see the importance of 

the own work part by comparing the results of 

the methods with or without considering the own 

work part. MEAD, LexRank and our method all 

get a significant improvement after considering 

the own work part by extracting sentences from 

the target paper. The results also prove our as-

sumption about the related work section structure. 

Figure 3 presents the fluctuation of ROUGE 

scores when tuning the parameters Î»1, Î»2 and Î»3. 
We can see our method generally performs better 

than the baselines. All the three parts in the ob-

jective function are useful to generate related 

work sections with good quality. 

The average scores rated by human judges for 

each method are showed in Table 5. We can see 

that the related work sections generated by our 

system are more related to the target papers. 

Moreover, because of the good structure of our 

generated related work sections, our generated 

related work sections are considered more reada-

ble and more useful for the author to prepare the 

final related work sections. 

T-test results show that the performance im-

provements of our method over baselines are 

statistically significant on both automatic and 

manual evaluations. Most of p-values for t-test 

are far smaller than 0.01. 

Overall, the results indicate that our method 

can generate much better related work sections 

0

0.6
0

0.2

0.4

0.6

0 0.3 0.6 0.9 ğœ†
1

R
O

U
G

E-
1

ğœ†2

ROUGE-1

0.4-
0.6

0.2-
0.4

0

0.6
0

0.05

0.1

0.15

0 0.3 0.6 0.9

ğœ†
1

R
O

U
G

E-
2

ğœ†2

ROUGE-2

0.1-
0.15

0.05-
0.1

0

0.6
0

0.05

0.1

0.15

0.2

0 0.3 0.6 0.9 ğœ†
1R
O

U
G

E-
SU

4

ğœ†2

ROUGE-SU4

0.15
-0.2
0.1-
0.15

Figure 3: Parameter influences (horizontal, vertical axis are ğœ†1, ğœ†2 , respectively, ğœ†3 = 1 âˆ’ ğœ†1 âˆ’ ğœ†2 ) 

1631



than the baselines on both automatic and human 

evaluations. 

Table 6 shows the comparison results between 

ARWG and RWGOF. We can see ARWG per-

forms better than RWGOF. It proves that the 

SVR models can better estimate the importance 

scores of the sentences. For the SVR models are 

trained from the large dataset, the sentence 

scores predicted by the SVR models can be more 

reliable to be used in the global optimization 

framework. 

6 Conclusion and Future Work 

This paper proposes a novel system called 

ARWG to generate related work sections for ac-

ademic papers. It first exploits a PLSA model to 

split the sentence set of the given papers into dif-

ferent topic-biased parts, and then applies regres-

sion models to learn the importance scores of the 

sentences. At last an optimization framework is 

proposed to generate the related work section. 

Evaluation results show that our system can gen-

erate much better related work sections than the 

baseline methods. 

In future work, we will make use of citation 

sentences to improve our system. Citation sen-

tences are the sentences that contains an explicit 

reference to another paper and they usually high-

light the most important aspects of the cited pa-

pers. So citation sentences are likely to contain 

important and rich information for generating 

related work sections. 

Acknowledgments 

The work was supported by National Natural 

Science Foundation of China (61170166, 

61331011), Beijing Nova Program (2008B03) 

and National Hi-Tech Research and Develop-

ment Program (863 Program) of China 

(2012AA011101). We also thank the anonymous 

reviewers for very helpful comments. The corre-

sponding author of this paper, according to the 

meaning given to this role by Peking University, 

is Xiaojun Wan. 

Reference 

Nitin Agarwal, Kiran Gvr, Ravi Shankar Reddy, and 

Carolyn Penstein RosÃ©. 2011. Towards multi-

document summarization of scientific articles: 

making interesting comparisons with SciSumm. 

In Proceedings of the Workshop on Automatic 

Summarization for Different Genres, Media, and 

Languages, pp. 8-15. Association for Computa-

tional Linguistics. 

Phyllis B. Baxendale. 1958. Machine-made index for 

technical literature: an experiment. IBM Journal of 

Research and Development 2, no. 4: 354-361. 

Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 

2011. Jointly learning to extract and compress. 

In Proceedings of the 49th Annual Meeting of the 

Association for Computational Linguistics: Human 

Language Technologies-Volume 1, pp. 481-490. 

Association for Computational Linguistics. 

Chih-Chung Chang, and Chih-Jen Lin. 2011. 

LIBSVM: a library for support vector ma-

chines. ACM Transactions on Intelligent Systems 

and Technology (TIST) 2, no. 3: 27. 

John M. Conroy, and Dianne P. O'leary. 2001. Text 

summarization via hidden markov models. 

In Proceedings of the 24th annual international 

ACM SIGIR conference on Research and develop-

ment in information retrieval, pp. 406-407. ACM. 

Harold P. Edmundson. 1969. New methods in auto-

matic extracting. Journal of the ACM (JACM) 16, 

no. 2: 264-285. 

GÃ¼nes Erkan, and Dragomir R. Radev. 2004. LexPag-

eRank: Prestige in Multi-Document Text Summa-

rization. In EMNLP, vol. 4, pp. 365-371. 

GÃ¼nes Erkan, and Dragomir R. Radev. 2004. 

LexRank: Graph-based lexical centrality as sali-

ence in text summarization. J. Artif. Intell. 

Res.(JAIR) 22, no. 1: 457-479. 

Dimitrios Galanis, Gerasimos Lampouras, and Ion 

Androutsopoulos. 2012. Extractive Multi-

Document Summarization with Integer Linear Pro-

gramming and Support Vector Regression. 

In COLING, pp. 911-926. 

Dimitrios Galanis, and Prodromos Malakasiotis. 2008. 

Aueb at tac 2008. InProceedings of the TAC 2008 

Workshop. 

Dan Gillick, and Benoit Favre. 2009. A scalable glob-

al model for summarization. InProceedings of the 

Workshop on Integer Linear Programming for 

Natural Langauge Processing, pp. 10-18. Associa-

tion for Computational Linguistics. 

Cong Duy Vu Hoang, and Min-Yen Kan. 2010. To-

wards automated related work summarization. 

In Proceedings of the 23rd International Confer-

ence on Computational Linguistics: Posters, pp. 

427-435. Association for Computational Linguis-

tics. 

Lei Huang, Yanxiang He, Furu Wei, and Wenjie Li. 

2010. Modeling document summarization as multi-

objective optimization. In Intelligent Information 

Technology and Security Informatics (IITSI), 2010 

Third International Symposium on, pp. 382-386. 

IEEE. 

1632



Thomas Hofmann. 1999. Probabilistic latent semantic 

indexing. In Proceedings of the 22nd annual inter-

national ACM SIGIR conference on Research and 

development in information retrieval, pp. 50-57. 

ACM. 

Chin-Yew Lin. 2004. Rouge: A package for automatic 

evaluation of summaries. InText Summarization 

Branches Out: Proceedings of the ACL-04 Work-

shop, pp. 74-81. 

Hans Peter Luhn. 1958. The automatic creation of 

literature abstracts. IBM Journal of research and 

development 2, no. 2: 159-165. 

Inderjeet Mani, and Eric Bloedorn. 1999. Summariz-

ing similarities and differences among related doc-

uments. Information Retrieval 1, no. 1-2: 35-67. 

Ryan McDonald. 2007. A study of global inference 

algorithms in multi-document summarization. 

Springer Berlin Heidelberg. 

Qiaozhu Mei, and ChengXiang Zhai. 2008. Generat-

ing Impact-Based Summaries for Scientific Litera-

ture. In ACL, vol. 8, pp. 816-824. 

Rada Mihalcea, and Paul Tarau. 2005. A language 

independent algorithm for single and multiple doc-

ument summarization. 

Rada Mihalcea, and Hakan Ceylan. 2007. Explora-

tions in Automatic Book Summarization. 

In EMNLP-CoNLL, pp. 380-389. 

Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed 

Hassan, Pradeep Muthukrishan, Vahed Qazvinian, 

Dragomir Radev, and David Zajic. 2009. Using ci-

tations to generate surveys of scientific paradigms. 

In Proceedings of Human Language Technologies: 

The 2009 Annual Conference of the North Ameri-

can Chapter of the Association for Computational 

Linguistics, pp. 584-592. Association for Computa-

tional Linguistics. 

Preslav Nakov, Ariel Schwartz, and M. Hearst. 2004. 

Citation sentences for semantic analysis of biosci-

ence text. In Proceedings of the SIGIR'04 work-

shop on Search and Discovery in Bioinformatics. 

Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, and 

William W. Cohen. 2008. Joint latent topic models 

for text and citations. In Proceedings of the 14th 

ACM SIGKDD international conference on 

Knowledge discovery and data mining, pp. 542-

550. ACM. 

Vahed Qazvinian, and Dragomir R. Radev. 2008. Sci-

entific paper summarization using citation sum-

mary networks. In Proceedings of the 22nd Inter-

national Conference on Computational Linguistics-

Volume 1, pp. 689-696. Association for Computa-

tional Linguistics. 

You Ouyang, Sujian Li, and Wenjie Li. 2007. Devel-

oping learning strategies for topic-based summari-

zation. In Proceedings of the sixteenth ACM con-

ference on Conference on information and 

knowledge management, pp. 79-86. ACM. 

Dragomir Radev, Timothy Allison, Sasha Blair-

Goldensohn, John Blitzer, Arda Celebi, Stanko 

Dimitrov, Elliott Drabek et al. 2004. MEAD-a plat-

form for multidocument multilingual text summa-

rization. Proceedings of the 4th International Con-

ference on Language Resources and Evaluation 

(LREC 2004). 

Ariel S. Schwartz, and Marti Hearst. 2006. Summariz-

ing key concepts using citation sentences. 

In Proceedings of the Workshop on Linking Natu-

ral Language Processing and Biology: Towards 

Deeper Biological Literature Analysis, pp. 134-135. 

Association for Computational Linguistics. 

Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and 

Zheng Chen. 2007. Document Summarization Us-

ing Conditional Random Fields. In IJCAI, vol. 7, 

pp. 2862-2867. 

Andreas  Stolcke, Klaus Ries, Noah Coccaro, Eliza-

beth Shriberg, Rebecca Bates, Daniel Jurafsky, 

Paul Taylor, Rachel Martin, Carol Van Ess-

Dykema, and Marie Meteer. 2000. Dialogue act 

modeling for automatic tagging and recognition of 

conversational speech. Computational linguis-

tics 26, no. 3: 339-373. 

Kristian Woodsend, and Mirella Lapata. 2012. Multi-

ple aspect summarization using integer linear pro-

gramming. In Proceedings of the 2012 Joint Con-

ference on Empirical Methods in Natural Lan-

guage Processing and Computational Natural 

Language Learning, pp. 233-243. Association for 

Computational Linguistics. 

Shasha Xie, Benoit Favre, Dilek Hakkani-TÃ¼r, and 

Yang Liu. 2009. Leveraging sentence weights in a 

concept-based optimization framework for extrac-

tive meeting summarization. In INTERSPEECH, 

pp. 1503-1506. 

Hui Yang, and Jamie Callan. 2009. A metric-based 

framework for automatic taxonomy induction. 

In Proceedings of the Joint Conference of the 47th 

Annual Meeting of the ACL and the 4th Interna-

tional Joint Conference on Natural Language Pro-

cessing of the AFNLP: Volume 1-Volume 1, pp. 

271-279. Association for Computational Linguis-

tics. 

Ozge Yeloglu, Evangelos Milios, and Nur Zincir-

Heywood. 2011. Multi-document summarization of 

scientific corpora. In Proceedings of the 2011 ACM 

Symposium on Applied Computing, pp. 252-258. 

ACM. 

1633


