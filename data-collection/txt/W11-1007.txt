










































Improving Reordering for Statistical Machine Translation with Smoothed Priors and Syntactic Features


Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61–69,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c©2011 Association for Computational Linguistics

Improving Reordering for Statistical Machine Translation with Smoothed
Priors and Syntactic Features

Bing Xiang, Niyu Ge, and Abraham Ittycheriah
IBM T. J. Watson Research Center

Yorktown Heights, NY 10598
{bxiang,niyuge,abei}@us.ibm.com

Abstract

In this paper we propose several novel ap-
proaches to improve phrase reordering for
statistical machine translation in the frame-
work of maximum-entropy-based modeling.
A smoothed prior probability is introduced to
take into account the distortion effect in the
priors. In addition to that we propose multi-
ple novel distortion features based on syntac-
tic parsing. A new metric is also introduced to
measure the effect of distortion in the transla-
tion hypotheses. We show that both smoothed
priors and syntax-based features help to sig-
nificantly improve the reordering and hence
the translation performance on a large-scale
Chinese-to-English machine translation task.

1 Introduction

Over the past decade, statistical machine translation
(SMT) has evolved into an attractive area in natural
language processing. SMT takes a source sequence,
S = [s1 s2 . . . sK ] from the source language, and
generates a target sequence, T ∗ = [t1 t2 . . . tL], by
finding the most likely translation given by:

T ∗ = arg max
T

p(T |S) (1)

In most of the existing approaches, following
(Brown et al., 1993), Eq. (1) is factored using the
source-channel model into

T ∗ = arg max
T

p(S|T )pλ(T ), (2)

where the two models, the translation model,
p(S|T ), and the language model (LM), p(T ), are es-

timated separately: the former using a parallel cor-
pus and a hidden alignment model and the latter us-
ing a typically much larger monolingual corpus. The
weighting factor λ is typically tuned on a develop-
ment test set by optimizing a translation accuracy
criterion such as BLEU (Papineni et al., 2002).

In recent years, among all the proposed ap-
proaches, the phrase-based method has become
the widely adopted one in SMT due to its capa-
bility of capturing local context information from
adjacent words. Word order in the translation
output relies on how the phrases are reordered
based on both language model scores and distor-
tion cost/penalty (Koehn et al., 2003), among all
the features utilized in a maximum-entropy (log-
linear) model (Och and Ney, 2002). The distor-
tion cost utilized during the decoding is usually a
penalty linearly proportional to the number of words
in the source sentence that are skipped in a transla-
tion path.

In this paper, we propose several novel ap-
proaches to improve reordering in the phrase-based
translation with a maximum-entropy model. In Sec-
tion 2, we review the previous work that focused on
the distortion and phrase reordering in SMT. In Sec-
tion 3, we briefly review the baseline of this work.
In Section 4, we introduce a smoothed prior prob-
ability by taking into account the distortions in the
priors. In Section 5, we present multiple novel dis-
tortion features based on syntactic parsing. A new
distortion evaluation metric is proposed in Section
6 and experimental results on a large-scale Chinese-
English machine translation task are reported in Sec-
tion 7. Section 8 concludes the paper.

61



2 Previous Work

Significant amount of research has been conducted
in the past on the word reordering problem in SMT.
In (Brown et al., 1993) IBM Models 3 through 5
model reordering based on the surface word infor-
mation. For example, Model 4 attempts to assign
target-language positions to source-language words
by modeling d(j|i,K,L) where j is the target-
language position, i is the source-language position,
K and L are respectively source and target sentence
lengths. These models are not effective in modeling
reordering because they do not have enough context
and lack structural information.

Phrase-based SMT systems such as (Koehn et al.,
2003) move from using words as translation units
to using phrases. One of the advantages of phrase-
based SMT systems is that the local reordering is in-
herent in the phrase translations. However, phrase-
based SMT systems capture reordering instances
and not reordering phenomena. It has trouble to pro-
duce the right translation order if the training data
does not contain the specific phrase pairs. For ex-
ample, phrases do not capture the phenomenon that
Arabic adjectives and nouns need to be reordered.

Instead of directly modeling the distance of word
movement, some phrase-level reordering models in-
dicate how to move phrases, also called orientations.
Orientations typically apply to the adjacent phrases.
Two adjacent phrases can be either placed mono-
tonically (sometimes called straight) or swapped
(non-monotonically or inverted). In (Och and Ney,
2004; Tillmann, 2004; Kumar and Byrne, 2005; Al-
Onaizan and Papineni, 2006; Xiong et al., 2006;
Zens and Ney, 2006; Ni et al., 2009), people pre-
sented models that use lexical features from the
phrases to predict their orientations. These models
are very powerful in predicting local phrase place-
ments. In (Galley and Manning, 2008) a hierar-
chical orientation model is introduced that captures
some non-local phrase reordering by a shift reduce
algorithm. Because of the heavy use of lexical fea-
tures, these models tend to suffer from data sparse-
ness problems.

Syntax information has been used for reordering,
such as in (Xia and McCord, 2004; Collins et al.,
2005; Wang et al., 2007; Li et al., 2007; Chang et
al., 2009). More recently, in (Ge, 2010) a proba-

bilistic reordering model is presented to model di-
rectly the source translation sequence and explicitly
assign probabilities to the reordering of the source
input with no restrictions on gap, length or adja-
cency. The reordering model is used to generate a re-
ordering lattice which encodes many reordering and
their costs (negative log probability). Another recent
work is (Green et al., 2010), which estimates future
linear distortion cost and presents a discriminative
distortion model that predicts word movement dur-
ing translation based on multiple features.

This work differentiates itself from all the previ-
ous work on the phrase reordering as the following.
Firstly, we propose a smoothed distortion prior prob-
ability in the maximum-entropy-based MT frame-
work. It not only takes into account the distortion
in the prior, but also alleviates the data sparseness
problem. Secondly, we propose multiple syntactic
features based on the source-side parse tree to cap-
ture the reordering phenomena between two differ-
ent languages. The correct reordering patterns will
be automatically favored during the decoding, due to
the higher weights obtained through the maximum
entropy training on the parallel data. Finally, we
also introduce a new metric to quantify the effect on
the distortions in different systems. The experiments
on a Chinese-English MT task show that these pro-
posed approaches additively improve both the dis-
tortion and translation performance significantly.

3 Maximum-Entropy Model for MT

In this section we give a brief review of a special
maximum-entropy (ME) model as introduced in (It-
tycheriah and Roukos, 2007). The model has the
following form,

p(t, j|s) =
p0(t, j|s)

Z
exp

∑

i

λiφi(t, j, s), (3)

where s is a source phrase, and t is a target phrase.
j is the jump distance from the previously translated
source word to the current source word. During
training j can vary widely due to automatic word
alignment in the parallel corpus. To limit the sparse-
ness created by long jumps, j is capped to a win-
dow of source words (-5 to 5 words) around the last
translated source word. Jumps outside the window
are treated as being to the edge of the window. In

62



Eq. (3), p0 is a prior distribution, Z is a normalizing
term, and φi(t, j, s) are the features of the model,
each being a binary question asked about the source
and target streams. The feature weights λi can be
estimated with the Improved Iterative Scaling (IIS)
algorithm.

Several categories of features have been pro-
posed:

• Lexical features that examine source word, tar-
get word and jump;

• Lexical context features that examine the pre-
vious and next source words, and also the pre-
vious two target words;

• Segmentation features based on morphological
analysis;

• Part-of-speech (POS) features that collect the
syntactic information from the source and tar-
get words;

• Coverage features that examine the coverage
status of the source words to the left and to the
right. They fire only if the left source is open
(untranslated) or the right source is closed.

 

 
                <=-5          -4             -3             -2            -1            1              2             3              4           >=5 
                                                                                         jump 

Figure 1: Counts of jumps for words with POS NN.

4 Distortion Priors

Generally the prior distribution in Eq. (3) can con-
tain any information we know about the future.

 

 
                <=-5          -4             -3             -2            -1            1              2             3              4           >=5 
                                                                                        jump 

Figure 2: Counts of jumps for words with POS NT.

In (Ittycheriah and Roukos, 2007), the normalized
phrase count is utilized as the prior, i.e.

p0(t, j|s) ≈
1

l
p0(t|s) =

C(s, t)

l ∗ C(s)
(4)

where l is the jump window size (a constant), C(s, t)
is the co-ocurrence count of phrase pair (s, t), and
C(s) is the source phrase count of s. It can be seen
that distortion j is not taken into account in Eq. (4).
The contribution of distortion solely comes from the
features. In this work, we estimate the prior proba-
bility with distortion included,

p0(t, j|s) = p0(t|s)p(j|s, t) (5)

where p(j|s, t) is the distortion probability for a
given phrase pair (s, t).

Due to the sparseness issue in the estimation of
p(j|s, t), we choose to smooth it with the global dis-
tortion probability through

p(j|s, t) = αpl(j|s, t) + (1− α)pg(j), (6)

where pl is the local distortion probability estimated
based on the counts of jumps for each phrase pair
in the training, pg is the global distortion probability
estimated on all the training data, and α is the inter-
polation weight. In this work, pg is estimated based
on either source POS (if it’s a single-word source
phrase) or source phrase size (if it’s more than one
word long), as shown below.

pg(j) =

{

Pg(j|POS), if |s| = 1

Pg(j||s|), if |s| > 1
(7)

63



In this way, the system can differentiate the distor-
tion distributions for single source words with differ-
ent POS tags, such as adjectives versus nouns. And
in the meantime, we also differentiate the distortion
distribution with different source phrase lengths. We
show several examples of the jump distributions in
Fig. 1 and 2 collected from 1M sentence pairs in
a Chinese-to-English parallel corpus with automatic
parsing and word alignment. Fig. 1 shows the count
histogram for single-word phrases with POS tag as
NN. The distortion with j = 1, i.e. monotone, domi-
nates the distribution with the highest count. The re-
ordering with j = −1 has the second highest count.
Such pattern is shared by most of the other POS tags.
However, Fig. 2 shows that the distribution of jumps
for NT is quite different from NN. The jump with
j = −1 is actually the most dominant, with higher
counts than monotone translation. This is due to the
different order in English when translating Chinese
temporal nouns.

5 Distortion Features

Although the maximum entropy translation model
has an explicit indicator of distortion, j, built into
the features, we discuss in this section some novel
features that try to capture the distortion phenomena
of translation. These features are questions about the
parse tree of the source language and in particular
about the local parse node neighborhood of the cur-
rent source word being translated. Figure 3 shows an
example sentence from the Chinese-English Parallel
Treebank (LDC2009E83) and the source language
parse is displayed on the left. The features below
can be viewed as either being within a parse node
or asking about the coverage status of neighborhood
nodes.

Since these features are asking about the current
coverage, they are specific to a path in the search lat-
tice during the decoding phase of translation. Train-
ing these features is done by evaluating on the path
defined by the automatic word alignment of the par-
allel corpus sentence.

5.1 Parse Tree Modifications

The ‘de’ construction in Chinese is by now famous.
In order to ask more coherent questions about the
parse neighborhood, we modify the parse structures

to “raise” the ‘de’ structure. The parse trees anno-
tated by the LDC have a structure as shown in Fig.
4. After raising the ‘de’ structure we obtain the tree
in Fig. 5.

NP-OBJ

CP

IP

...

DEC

de

QP

...

NP

NN

Figure 4: Original parse tree from LDC.

DNP

CP

IP

...

DEC

de

QP

...

NP

NN

Figure 5: The parse tree after transformation.

The transformation has been applied to the exam-
ple shown in Figure 3. The resulting flat structure
facilitates the parse sibling feature discussed below.

5.2 Parse Coverage Feature

The first set of new features we will introduce is the
source parse coverage feature. This feature is in-
terior to a source parse node and asks if the leaves
under this parse node are covered (translated) or not
so far. The feature has the following components:

φi(SourceWord, TargetWord, SourceParseParent,
jump, Coverage).

Unary parents in the source parse tree are ex-
cluded since the feature has no ambiguity in cover-
age. In Figure 3, the ‘PP’ node above position 5 has
two children, P, NP. When translating source posi-
tion 6, this feature indicates that the PP node has a
leaf that is already covered.

5.3 Parse Sibling Feature

The second set of new features is the source parse
sibling feature. This feature asks whether the neigh-

64



 

Figure 3: Chinese-English example.

boring parse node has been covered or not. The fea-
ture includes two types:

φi(SourceWord, TargetWord, SourceParseSibling,
jump, SiblingCoverage, SiblingOrientation)
and

φi(SourcePOS, TargetPOS, SourceParseSibling,
jump, SiblingCoverage, SiblingOrientation).

Some example features for the first type are
shown in Table 1, where αi = eλi . The coverage
status (Cov) of the parse sibling node indicates if the
node is covered completely (1), partially (2) or not
covered (0). In order to capture the relationship of
the neighborhood node, we indicate the orientation
which can be either of {left (-1), right (1)}. Given
the example shown in Figure 3, at source position
10, the system can now ask about the ‘CP’ structure
to the left and the ‘QP’ and ‘NP’ structures to the
right. An αi of greater than 1.0 (meaning λi > 0)
indicates that the feature increases the probability of
the related target block. From these examples, it’s
clear that the system prefers to produce an empty
translation for the Chinese word “de” when the ‘QP’
and ‘NP’ nodes to the right of it are already covered
(the first two features in Table 1) and when the ‘CP’
node to left is still uncovered (the third feature). The
last feature in the table shows αi for the case when
‘CP’ has already been covered.

These features are able to capture neighborhoods
that are much larger than the original baseline model
which only asked questions about the immediate
lexical neighborhood of the current source word.

Cnt αi Tgt Src Parse Cov Orien-
Node tation

18065 2.06 e0 de QP 1 1
366153 1.99 e0 de NP 1 1
143433 3.41 e0 de CP 0 -1
99297 1.05 e0 de CP 1 -1

Table 1: Parse Sibling Word Features (e0 represents
empty target).

6 A New Distortion Evaluation Metric

MT performance is usually measured by such met-
ric as BLEU which measures the MT output as a
whole including word choice and reordering. It is
useful to measure these components separately. Un-
igram BLEU (BLEUn1) measures the precision of
word choice. We need a metric for measuring re-
ordering accuracy. The naive way of counting accu-
racy at every source position does not account for the
case of the phrasal movement. If a phrase is moved
to the wrong place, every source word in the phrase
would be penalized whereas a more reasonable met-
ric would penalize the phrase movement only once
if the phrase boundary is correct.

We propose the following pair-wise distortion
metric. From an MT output, we first extract the
source visit sequence:

Hyp:{h1,h2, . . . hn}
where hi are the visit order of the source sentence.
From the reference, we extract the true visit se-
quence:

65



Ref:{r1,r2, . . . rn}
The Pair-wise Distortion metric PDscore can be

computed as follows:

PDscore(
−→
H ) =

n
∑

i=1

I(hi = rj ∧ hi−1 = rj−1)

n

(8)
It measures how often the translation output gets
the pair-wise source visit order correct. We notice
that an MT metric named LRscore was proposed in
(Birch and Osborne, 2010). It computes the distance
between two word order sequences, which is differ-
ent from the metric we proposed here.

7 Experiments

7.1 Data and Baseline

We conduct a set of experiments on a Chinese-to-
English MT task. The training data includes the UN
parallel corpus and LDC-released parallel corpora,
with about 11M sentence pairs, 320M words in to-
tal (counted at the English side). To evaluate the
smoothed distortion priors and different features, we
use an internal data set as the development set and
the NIST MT08 evaluation set as the test set, which
includes 76 documents (691 sentences) in newswire
and 33 documents (666 sentences) in weblog, both
with 4 sets of references for each sentence. Instead
of using all the training data, we sample the training
corpus based on the dev/test set to train the system
more efficiently. The most recent and good-quality
corpora are sampled first. For the given test set, we
obtain the first 20 instances of n-grams (length from
1 to 15) from the test that occur in the training uni-
verse and the resulting sentences then form the train-
ing sample. In the end, 1M sentence pairs are se-
lected for the sampled training for each genre of the
MT08 test set.

A 5-gram language model is trained from the En-
glish Gigaword corpus and the English portion of
the parallel corpus used in the translation model
training. The Chinese parse trees are produced
by a maximum entropy based parser (Ratnaparkhi,
1997). The baseline decoder is a phrase-based de-
coder that employs both normal phrases and also
non-contiguous phrases. The value of maximum
skip is set to 9 in all the experiments. The smoothing
parameter α for distortion prior is set to 0.9 empiri-

cally based on the results on the development set.

7.2 Distortion Evaluation

We evaluate the MT distortion using the metric in
Eq. (8) on two hand-aligned test sets. Test-278 in-
cludes 278 held-out sentences. Test-52 contains the
first 52 sentences from the MT08 Newswire set, with
the Chinese input sentences manually aligned to the
first set of reference translations. From the hand
alignment, we extract the true source visit sequence
and this is the reference.

The evaluation results are in Table 2. It is shown
that the smoothed distortion prior, parse coverage
feature and parse sibling feature each provides im-
provement on the PDscore on Test-278 and Test-52.
The final system scores are 2 to 3 points absolute
higher than the baseline scores. The state visit se-
quence in the final system is closer to the true visit
sequence than that of the baseline. This indicates
the advantage of using both parse-based syntactic
features and also the smoothed prior that takes into
account of the distortion effect. We also provide
an upper-bound in the last row by computing the
PDscore between the first and second set of refer-
ences for Test-52. The number shows the agreement
between two human translators in terms of PDscore
is around 71%.

System Test-278 Test-52

ME Baseline 44.58 48.96
+Prior 45.12 49.22
+COV 45.00 49.03
+SIB 45.43 49.20
+COV+SIB 46.16 49.45
+Prior+COV+SIB 47.68 51.04
Ref1 vs. Ref2 - 70.99

Table 2: Distortion accuracy PDscore (Prior:smoothed
distortion prior; COV:parse coverage feature; SIB:parse
sibling feature).

7.3 Translation Results

Translation results on the MT08 Newswire set and
MT08 Weblog set are listed in Table 3 and Table 4
respectively. The MT performance is measured with
the widely adopted BLEU and TER (Snover et al.,
2006) metrics. We also compare the results from
different configurations with a normal phrase-based

66



System Number of Features BLEU TER

PBT n/a 29.71 59.40
ME 9,008,382 32.12 56.78
+Prior 9,008,382 32.46 56.41
+COV 9,202,431 32.48 56.50
+SIB 10,088,487 32.73 56.26
+COV+SIB 10,282,536 32.94 55.97
+Prior+COV+SIB 10,282,536 33.15 55.62

Table 3: MT results on MT08 Newswire set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;
Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).

System Number of Features BLEU TER

PBT n/a 20.07 62.90
ME 9,192,617 22.42 60.36
+Prior 9,192,617 22.70 60.11
+COV 9,306,967 22.69 60.14
+SIB 9,847,445 22.91 59.92
+COV+SIB 9,961,795 23.04 59.78
+Prior+COV+SIB 9,961,795 23.25 59.56

Table 4: MT results on MT08 Weblog set (PBT:normal phrase-based MT; ME:Maximum-entropy baseline;
Prior:smoothed distortion prior; COV:parse coverage feature; SIB:parse sibling feature).

SMT system (Koehn et al., 2003) that is trained on
the same training data. The number of features used
in the systems are listed in the tables.

We start from the maximum-entropy baseline, a
system implemented similarly as in (Ittycheriah
and Roukos, 2007). It utilizes multiple features as
listed in Section 3, including lexical reordering fea-
tures, and produces an already significantly better
performance than the normal phrase-based MT sys-
tem (PBT). It is around 2.5 points better in both
BLEU and TER than the PBT baseline. By adding
smoothed priors, parse coverage features or parse
sibling features each separately, the MT perfor-
mance is improved by 0.3 to 0.6. The parse sibling
feature alone provides the largest individual contri-
bution. When adding both types of new features,
the improvement is around 0.6 to 0.8 on two gen-
res. Finally, applying all three results in the best
performance (the last row). On the Newswire set,
the final system is more than 3 points better than the
PBT baseline and 1 point better than the ME base-
line. On the Weblog set, it is more than 3 points
better than PBT and 0.8 better than the ME baseline.
All the MT results above are statistically significant

with p-value < 0.0001 by using the tool described in
(Zhang and Vogel, 2004).

7.4 Analysis

To better understand the distortion and translation
results, we take a closer look at the parse-based fea-
tures. In Table 5, we list the most frequent parse sib-
ling features that are related to the Chinese phrases
with “PP VV” structures. It is known that in Chi-
nese usually the preposition phrases (“PP”) are writ-
ten/spoken before the verbs (“VV”), with a different
order from English. Table 5 shows how such re-
ordering phenomenon is captured by the parse sib-
ling features. Recall that when αi is greater than 1,
the system prefers the reordering with that feature
fired. When αi is smaller than 1, the system will
penalize the corresponding translation order during
the decoding search. When the coverage is equal to
1, it means “PP” has been translated before translat-
ing current “VV”. As shown in the table, those fea-
tures with coverage equal to 1 have αi lower than 1,
which will result in penalties on incorrect translation
orders.

In Fig. 6, we show the comparison between the
67



Count αi j TgtPOS SrcPOS ParseSib Cov Orien-
Node tation

3052 1.10 5 VBD VV PP 0 -1
2662 1.10 -1 VBD VV PP 0 -1
2134 1.25 4 VBD VV PP 0 -1

50 0.73 5 VBD VV PP 1 -1
39 0.84 -5 VBD VV PP 1 -1
18 0.95 -2 VBD VV PP 1 -1

Table 5: Parse Sibling Word Features related to Chinese “PP VV”.
 
Src1 

��
 ��� �� �� 	
 �� � �� , 1850� � 2005 �� , �� �

 1800 � � �� ����(were) ����(at) ��������(annual) 3% ���� ��������(rate) ��������
(shrinking)�  

Ref a long-term follow-up research by glacier experts at the swiss academy of 
sciences found that from 1850 to 2005 the 1,800 plus glaciers in switzerland 
were shrinking at an annual rate of 3 % . 

Baseline the swiss academy of sciences glacier experts long-term follow-up study found 
that from 2005 to 1850 , with an average of more than 1800 glaciers in 
switzerland is the reduced rate of 3 % .  

New the swiss academy of sciences glacier experts long-term follow-up study found 
that from 1850 to 2005 , more than 1800 of swiss glaciers shrinking at an 
annual rate of 3 %.  

  
Src2 �   ! "# $%& '( ) , * + , -.-.-.-.(had been) /0/0/0/0(kidnapped) ����

(who) 12121212(german) 34343434(hostage) 56 78 9: , ;< => ?@ A BC
 DE � 

Ref but at the same time the taliban said that another german hostage who had 
been kidnapped was in extremely poor health , and had started to become 
comatose and to lose consciousness . 

Baseline but at the same time , another one was kidnapped by the taliban of the 
german hostage body very weak , began to fall into a coma and lost 
consciousness . 

New but at the same time , the taliban said that the body of another german 
hostage who was kidnapped very weak , began to fall into a coma and lost 
consciousness .  

  
 
 Figure 6: Chinese-English MT examples(Baseline:Maximum-entropy baseline; New:System with smoothed priors

and syntactic features).

ME baseline output and those from the improved
system with the parse-based features and smoothed
distortion priors. The differences are highlighted
in bold for easy understanding. The first example
shows that the new system fixes the order for “PP
VV”, while the second one shows the fix for the
translation of “CP de NP”. This is consistent with
the features we showed in Table 1 and 5. The new
features help to translate the Chinese text in the right
order.

8 Conclusion

In this paper we have presented several novel ap-
proaches that improved phrase reordering in the
framework of maximum entropy based translation.
A smoothed prior probability was proposed to take

into account the distortions in the priors. Several
novel distortion features were presented based on
the syntactic parsing. A new metric PDscore was
also introduced to measure the effect of distortion
in the translation hypotheses. We showed that both
smoothed prior and syntax-based features additively
improved the distortion and also the translation per-
formance significantly on a large-scale Chinese-
English machine translation task. How to further
take advantage of the syntactic information to im-
prove the reordering in SMT will continue to be an
interesting topic in the future.

Acknowledgments

We would like to acknowledge the support of
DARPA under Grant HR0011-08-C-0110 for fund-

68



ing part of this work. The views, opinions, and/or
findings contained in this article/presentation are
those of the author/presenter and should not be in-
terpreted as representing the official views or poli-
cies, either expressed or implied, of the Defense Ad-
vanced Research Projects Agency or the Department
of Defense.

References

Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
ACL, pages 529–536, Sydney, Australia.

Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In Pro-
ceedings of the Joint 5th Workshop on Statistical Ma-
chine Translation and MetricsMATR.

Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263–311.

Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative re-
ordering with chinese grammatical relations features.
In Proceedings of the Third Workshop on Syntax and
Structure in Statistical Translation.

Michael Collins, Philipp Koehn, and Ivona Kučerová.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531–540.

Michel Galley and Christoph D. Manning. 2008. A sim-
ple and effective hierarchical phrase reordering model.
In Proceedings of the EMNLP.

Niyu Ge. 2010. A direct syntax-driven reordering model
for phrase-based machine translation. In Proceedings
of HLT-NAACL, pages 849–857.

Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Proceedings of HLT-
NAACL.

Abraham Ittycheriah and Salim Roukos. 2007. Di-
rect translation model 2. In Proceedings HLT/NAACL,
pages 57–64, April.

Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL/HLT.

Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of HLT/EMNLP, pages 161–168.

Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proceedings of ACL.

Yizhao Ni, Craig J.Saunders, Sandor Szedmak, and Mah-
esan Niranjan. 2009. Handling phrase reorderings for
machine translation. In Proceedings of ACL.

Franz-Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translations. In 40th Annual Meeting of
the ACL, pages 295–302, Philadelphia, PA, July.

Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311–318.

Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models. In
Proceedings of EMNLP, pages 1–10.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Linea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proceedings of Association for Machine Translation in
the Americas.

Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL.

Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of EMNLP, pages 737–
745.

Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of COLING.

Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of ACL.

Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation.

Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalua-
tion metrics. In Proceedings of The 10th International
Conference on Theoretical and Methodological Issues
in Machine Translation.

69


