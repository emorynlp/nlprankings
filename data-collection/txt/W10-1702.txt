










































Fast Consensus Hypothesis Regeneration for Machine Translation


Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 11–16,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Fast Consensus Hypothesis Regeneration for Machine Translation 

 

Boxing Chen, George Foster and Roland Kuhn 
National Research Council Canada 

283 Alexandre-Taché Boulevard, Gatineau (Québec), Canada J8X 3X7 

{Boxing.Chen, George.Foster, Roland.Kuhn}@nrc.ca 

 

  

 

Abstract 

This paper presents a fast consensus hy-
pothesis regeneration approach for ma-

chine translation. It combines the advan-

tages of feature-based fast consensus de-
coding and hypothesis regeneration.  Our 

approach is more efficient than previous 

work on hypothesis regeneration, and it 
explores a wider search space than con-

sensus decoding, resulting in improved 

performance.  Experimental results show 

consistent improvements across language 
pairs, and an improvement of up to 0.72 

BLEU is obtained over a competitive 

single-pass baseline on the Chinese-to-
English NIST task. 

1 Introduction 

State-of-the-art statistical machine translation 

(SMT) systems are often described as a two-pass 
process. In the first pass, decoding algorithms are 

applied to generate either a translation N-best list 

or a translation forest.  Then in the second pass, 
various re-ranking algorithms are adopted to 

compute the final translation. The re-ranking al-

gorithms include rescoring (Och et al., 2004) and 

Minimum Bayes-Risk (MBR) decoding (Kumar 
and Byrne, 2004; Zhang and Gildea, 2008; 

Tromble et al., 2008). Rescoring uses more so-

phisticated additional feature functions to score 
the hypotheses. MBR decoding directly incorpo-

rates the evaluation metrics (i.e., loss function), 

into the decision criterion, so it is effective in 
tuning the MT performance for a specific loss 

function. In particular, sentence-level BLEU loss 

function gives gains on BLEU (Kumar and 

Byrne, 2004).  
The naïve MBR algorithm computes the loss 

function between every pair of k hypotheses, 

needing O(k
2
) comparisons. Therefore, only 

small number k is applicable. Very recently, De-

Nero et al. (2009) proposed a fast consensus de-

coding (FCD) algorithm in which the similarity 
scores are computed based on the feature expec-

tations over the translation N-best list or transla-

tion forest. It is equivalent to MBR decoding 

when using a linear similarity function, such as 
unigram precision.  

Re-ranking approaches improve performance 

on an N-best list whose contents are fixed. A   
complementary strategy is to augment the con-

tents of an N-best list in order to broaden the 

search space. Chen et al (2008) have proposed a 
three-pass SMT process, in which a hypothesis 

regeneration pass is added between the decoding 

and rescoring passes. New hypotheses are gener-

ated based on the original N-best hypotheses 
through n-gram expansion, confusion-network 

decoding or re-decoding. All three hypothesis 

regeneration methods obtained decent and com-
parable improvements in conjunction with the 

same rescoring model. However, since the final 

translation candidates in this approach are pro-

duced from different methods, local feature func-
tions (such as translation models and reordering 

models) of each hypothesis are not directly com-

parable and rescoring must exploit rich global 
feature functions to compensate for the loss of 

local feature functions. Thus this approach is de-

pendent on the use of computationally expensive 
features for rescoring, which makes it inefficient.  

In this paper, we propose a fast consensus hy-

pothesis regeneration method that combines the 

advantages of feature-based fast consensus de-
coding and hypothesis regeneration. That is, we 

integrate the feature-based similarity/loss func-

tion based on evaluation metrics such as BLEU 
score into the hypothesis regeneration procedure 

to score the partial hypotheses in the beam search 

and compute the final translations. Thus, our ap-
proach is more efficient than the original three-

pass hypothesis regeneration. Moreover, our ap-

proach explores more search space than consen-

11



sus decoding, giving it an advantage over the 

latter. 

In particular, we extend linear corpus BLEU 

(Tromble et al., 2008) to n-gram expectation-
based linear BLEU, then further extend the n-

gram expectation computed on full-length hypo-

theses to n-gram expectation computed on fixed-
length partial hypotheses. Finally, we extend the 

hypothesis regeneration with forward n-gram 

expansion to bidirectional n-gram expansion in-
cluding both the forward and backward n-gram 

expansion. Experimental results show consistent 

improvements over the baseline across language 

pairs, and up to 0.72 BLEU points are obtained 
from a competitive baseline on the Chinese-to-

English NIST task. 

2 Fast Consensus Hypothesis Regenera-
tion 

Since the three hypothesis regeneration methods 
with n-gram expansion, confusion network de-

coding and re-decoding produce very similar per-

formance (Chen et al., 2008), we consider only 
n-gram expansion method in this paper. N-gram 

expansion can (almost) fully exploit the search 

space of target strings which can be generated by 

an n-gram language model trained on the N-best 
hypotheses (Chen et al., 2007). 

2.1 Hypothesis regeneration with bidirec-
tional n-gram expansion 

N-gram expansion (Chen et al., 2007) works as 
follows: firstly, train an n-gram language model 

based on the translation N-best list or translation 

forest; secondly, expand each partial hypothesis 

by appending a word via overlapped (n-1)-grams 
until the partial hypothesis reaches the sentence 

ending symbol. In each expanding step, the par-

tial hypotheses are pruned through a beam-search 
algorithm with scoring functions. 

Duchateau et al. (2001) shows that the back-

ward language model contains information com-
plementary to the information in the forward 

language model. Hence, on top of the forward n-

gram expansion used in (Chen et al., 2008), we 

further introduce backward n-gram expansion to 
the hypothesis regeneration procedure. Backward 

n-gram expansion involves letting the partial hy-

potheses start from the last words that appeared 
in the translation N-best list and having the ex-

pansion go from right to left. 

Figure 1 gives an example of backward n-

gram expansion. The second row shows bi-grams 
which are extracted from the original hypotheses 

in the first row. The third row shows how a par-

tial hypothesis is expanded via backward n-gram 

expansion method. The fourth row lists some 

new hypotheses generated by backward n-gram 
expansion which do not exist in the original hy-

pothesis list. 

 

 

original 

 hypotheses 

about weeks' work . 

one week's work 

about one week's 

about a week work 

about one week work 

bi-grams about weeks', weeks' work, …, 

about one, …,  week work. 

backward 
n-gram 

 expansion 

partial hyp.     week's work 

n-gram one week's  

new partial hyp. one week's work 
 

 
new 

 hypotheses 

about one week's work 
about week's work 

one weeks' work . 

one week's work . 

one week's work . 

 
Figure 1: Example of original hypotheses; bi-grams 

collected from them; backward expanding a partial 

hypothesis via an overlapped n-1-gram; and new hy-

potheses generated through backward n-gram expan-

sion. 

2.2 Feature-based scoring functions 

To speed up the search, the partial hypotheses 

are pruned via beam-search in each expanding 

step. Therefore, the scoring functions applied 

with the beam-search algorithm are very impor-
tant. In (Chen et al., 2008), more than 10 addi-

tional global features are computed to rank the 

partial hypothesis list, and this is not an efficient 
way. In this paper, we propose to directly incor-

porate the evaluation metrics such as BLEU 

score to rank the candidates. The scoring func-

tions of this work are derived from the method of 
lattice Minimum Bayes-risk (MBR) decoding 

(Tromble et al., 2008) and fast consensus decod-

ing (DeNero et al., 2009), which were originally 
inspired from N-best MBR decoding (Kumar and 

Byrne, 2004). 

From a set of translation candidates E, MBR 
decoding chooses the translation that has the 

least expected loss with respect to other candi-

dates. Given a hypothesis set E, under the proba-

bility model )|( feP , MBR computes the trans-

lation e~  as follows: 
 

12



)|(),(minarg~ fePeeLe
EeEe

⋅′= ∑
∈′∈

        (1) 

 

where f is the source sentence, ),( eeL ′  is the loss 

function of two translations e and e′ . 
Suppose that we are interested in maximizing 

the BLEU score (Papineni et al., 2002) to optim-

ize the translation performance. The loss func-

tion is defined as ),(1),( eeBLEUeeL ′−=′ ,  

then the MBR objective can be re-written as 

 

)|(),(maxarg~ fePeeBLEUe
EeEe

⋅′= ∑
∈′∈

         (2) 

 
E represents the space of the translations. For 

N-best MBR decoding, this space is the N-best 

list produced by a baseline decoder (Kumar and 

Byrne, 2004). For lattice MBR decoding, this 
space is the set of candidates encoded in the lat-

tice (Tromble et al., 2008). Here, with hypothesis 

regeneration, this space includes: 1) the transla-
tions produced by the baseline decoder either in 

an N-best list or encoded in a translation lattice, 

and 2) the translations created by hypothesis re-
generation. 

However, BLEU score is not linear with the 

length of the hypothesis, which makes the scor-

ing process for each expanding step of hypothe-
sis regeneration very slow. To further speed up 

the beam search procedure, we use an extension 

of a linear function of a Taylor approximation to 
the logarithm of corpus BLEU which was devel-

oped by (Tromble et al., 2008).  The original 

BLEU score of two hypotheses e and e’ are 

computed as follows. 
 

)),(log(
4

1
exp(),(),(

4

1

∑
=

′×′=′
n

n
eePeeeeBLEU γ    (3) 

 

where ),( eePn ′  is the precision of n-grams in the 

hypothesis e given e’ and  ),( ee ′γ  is a brevity 
penalty. Let |e| denote the length of e. The corpus 
log-BLEU gain is defined as follows: 

 

)),(log(
4

1
)
||

||
1,0min()),(log(

4

1

∑
=

′+
′

−=′
n

n eeP
e

e
eeBLEU  (4) 

 

Therefore, the first-order Taylor approxima-
tion to the logarithm of corpus BLEU is shown 

in Equation (5). 

 

∑
=

′⋅+=′
4

1

0
),(

4

1
||),(

n

nn
eeceeeG θθ                    (5) 

where ),( eec
n

′ are the counts of the matched n-

grams and 
nθ  ( 40 ≤≤ n ) are constant weights 

estimated with held-out data.  

Suppose we have computed the expected n-

gram counts from the N-best list or translation 
forest. Then we may extend linear corpus BLEU 

in (5) to n-gram expectation-based linear corpus 

BLEU to score the partial hypotheses h. That is 
 

∑ ∑
= ∈

⋅⋅+=
4

1

0 ),()],'([
4

1
||)',(

n Tt

nnn

n

thtecEhehG δθθ
      (6) 

 

where ),( th
n

δ  are n-gram indicator functions that 

equal 1 if n-gram t  appears in h  and 0 other-

wise; )],'([ tecE n  ( 41 ≤≤ n ) are the real-valued 

n-gram expectations. Different from lattice MBR 

decoding, n-gram expectations in this work are 
computed over the original translation N-best list 

or translation forest; 
nT  ( 41 ≤≤ n ) are the sets of 

n-grams collected from translation N-best list or 
translation forest. Then we make a further exten-

sion: the expectations of the n-gram counts for 

each expanding step are computed over the par-

tial translations. The lengths of all partial hypo-
theses are the same in each n-gram expanding 

step. For instance, in the 5th n-gram expanding 

step, the lengths of all the partial hypotheses are 
5 words. Therefore, we use n-gram count expec-

tations computed over partial original transla-

tions that only contain the first 5 words. The rea-
son is that this solution contains more informa-

tion about word orderings, since some n-grams 

appear more than others at the beginning of the 

translations while they may appear with the same 
or even lower frequencies than others in the full 

translations.  

Once the expanding process of hypothesis re-
generation is finished, we use a more precise 

BLEU metric to score all the translation candi-

dates. We extend BLEU score in (3) to n-gram 

expectation-based BLEU. That is: 
 

















+







−=

=

∑
∑

∑

=

∈

∈
4

1 ),(

)]),'([),,(min(

log
4

1

||

|]'[|
1,0minexp

)',()(

n

Tt

n

Tt

nn

n

n

thc

tecEthc

h

eE

ehBLEUhScore

                                                        (7) 

 

where ),( thc
n

 is the count of  n-gram t in the 

hypothesis h. The step of choosing the final 

translation is the same as fast consensus decod-

ing (DeNero et al., 2009): first we compute n-

13



gram feature expectations, and then we choose 

the translation that is most similar to the others 

via expected similarity according to feature-

based BLEU score as shown in (7). The differ-
ence is the space of translations: the space of fast 

consensus decoding is the same as MBR decod-

ing, while the space of hypothesis regeneration is 
enlarged by the new translations produced via n-

gram expansion. 

2.3 Fast consensus hypothesis regeneration 

We first generate two new hypothesis lists via 
forward and backward n-gram expansion using 

the scoring function in Equation (6). Then we 

choose a final translation using the scoring func-

tion in Equation (7) from the union of the origi-
nal hypotheses and newly generated hypotheses. 

The original hypotheses are from the N-best list 

or extracted from the translation forest. The new 
hypotheses are generated by forward or back-

ward n-gram expansion or are the union of both 

two new hypothesis lists (this is called “bi-

directional n-gram expansion”). 

3 Experimental Results 

We carried out experiments based on translation 
N-best lists generated by a state-of-the-art 

phrase-based statistical machine translation sys-

tem, similar to (Koehn et al., 2007). In detail, the 

phrase table is derived from merged counts of 
symmetrized IBM2 and HMM alignments; the 

system has both lexicalized and distance-based 

distortion components (there is a 7-word distor-
tion limit) and employs cube pruning (Huang and 

Chiang, 2007). The baseline is a log-linear fea-

ture combination that includes language models, 
the distortion components, translation model, 

phrase and word penalties. Weights on feature 

functions are found by lattice MERT (Macherey 

et al., 2008). 

3.1 Data 

We evaluated with different language pairs: Chi-

nese-to-English, and German-to-English. Chi-

nese-to-English tasks are based on training data 
for the NIST 1  2009 evaluation Chinese-to-

English track. All the allowed bilingual corpora 

have been used for estimating the translation 

model. We trained two language models: the first 
one is a 5-gram LM which is estimated on the 

target side of the parallel data. The second is a 5-

                                                
1 http://www.nist.gov/speech/tests/mt 

gram LM trained on the so-called English Giga-

word corpus. 

 

   Chi Eng 

Parallel 

Train 

Large 

Data 

|S| 10.1M 

|W| 270.0M 279.1M 

   Dev |S| 1,506 1,506×4 
Test NIST06 |S| 1,664 1,664×4 

NIST08 |S| 1,357 1,357×4 
Gigaword |S| - 11.7M 

 
Table 1: Statistics of training, dev, and test sets for 
Chinese-to-English task. 

 

We carried out experiments for translating 

Chinese to English. We first created a develop-
ment set which used mainly data from the NIST 

2005 test set, and also some balanced-genre web-

text from the NIST training material. Evaluation 

was performed on the NIST 2006 and 2008 test 
sets. Table 1 gives figures for training, develop-

ment and test corpora; |S| is the number of the 

sentences, and |W| is the size of running words. 
Four references are provided for all dev and test 

sets. 

For German-to-English tasks, we used WMT 
20062 data sets. The parallel training data con-

tains about 1 million sentence pairs and includes 

21 million target words; both the dev set and test 

set contain 2000 sentences; one reference is pro-
vided for each source input sentence. Only the 

target-language half of the parallel training data 

are used to train the language model in this task. 

3.2 Results 

Our evaluation metric is IBM BLEU (Papineni et 

al., 2002), which performs case-insensitive 

matching of n-grams up to n = 4.  

Our first experiment was carried out over 
1000-best lists on Chinese-to-English task. For 

comparison, we also conducted experiments with 

rescoring (two-pass) and three-pass hypothesis 
regeneration with only forward n-gram expan-

sion as proposed in (Chen et al., 2008). In the 

“rescoring” and “three-pass” systems, we used 
the same rescoring model. There are 21 rescoring 

features in total, mainly translation lexicon 

scores from IBM and HMM models, posterior 

probabilities for words, n-grams, and sentence 
length, and language models, etc. For a complete 

description, please refer to (Ueffing et al., 2007). 

The results in BLEU-4 are reported in Table 2. 
 

                                                
2 http://www.statmt.org/wmt06/ 

14



testset NIST’06 NIST’08 

baseline 35.70 28.60 

rescoring 36.01 28.97 

three-pass 35.98 28.99 

FCD 36.00 29.10 

Fwd. 36.13 29.19 

Bwd. 36.11 29.20 

Bid. 36.20 29.28 
 
Table 2: Translation performances in BLEU-4(%) 
over 1000-best lists for Chinese-to-English task: “res-
coring” represents the results of rescoring; “three-
pass”, three-pass hypothesis regeneration with for-
ward n-gram expansion; “FCD”, fast consensus de-
coding; “Fwd”, the results of hypothesis regeneration 
with forward n-gram expansion; “Bwd”, backward n-
gram expansion; and “Bid”, bi-directional n-gram 
expansion. 
 

Firstly, rescoring improved performance over 

the baseline by 0.3-0.4 BLEU point. Three-pass 
hypothesis regeneration with only forward n-

gram expansion (“three-pass” in Table 2) ob-

tained almost the same improvements as rescor-
ing. Three-pass hypothesis regeneration exploits 

more hypotheses than rescoring, while rescoring 

involves more scoring feature functions than the 
former. They reached a balance in this experi-

ment. Then, fast consensus decoding (“FCD” in 

Table 2) obtains 0.3-0.5 BLEU point improve-

ments over the baseline. Both forward and back-
ward n-gram expansion (“Fwd.” and “Bwd.” in 

Table 2) improved about 0.1 BLEU point over 

the results of consensus decoding. Fast consen-
sus hypothesis regeneration (Fwd. and Bwd. in 

Table 2) got better improvements than three-pass 

hypothesis regeneration (“three-pass” in Table 2) 

by 0.1-0.2 BLEU point. Finally, combining hy-
pothesis lists from forward and backward n-gram 

expansion (“Bid.” in Table 2), further slight 

gains were obtained. 
 

testset Average time 

three-pass 3h 54m 

Fwd. 25m 

Bwd. 28m 

Bid. 40m 

 
Table 3: Average processing time of NIST’06 and 
NIST’08 test sets used in different systems. Times 
include n-best list regeneration and re-ranking. 
 

Moreover, fast consensus hypothesis regenera-

tion is much faster than the three-pass one, be-

cause the former only needs to compute one fea-

ture, while the latter needs to compute more than 

20 additional features. In this experiment, the 

former is about 10 times faster than the latter in 

terms of processing time, as shown in Table 3. 
 

In our second experiment, we set the size of 

N-best list N equal to 10,000 for both Chinese-to-

English and German-to-English tasks. The re-

sults are reported in Table 4. The same trend as 
in the first experiment can also be observed in 

this experiment. It is worth noticing that enlarg-

ing the size of the N-best list from 1000 to 
10,000 did not change the performance signifi-

cantly. Bi-directional n-gram expansion obtained 

improvements of 0.24 BLEU-score for WMT 
2006 de-en test set; 0.55 for NIST 2006 test set; 

and 0.72 for NIST 2008 test set over the base-

line. 

 

Lang. ch-en de-en 

testset NIST’06 NIST’08 Test2006 

baseline 35.70 28.60 26.92 

FCD 36.03 29.08 27.03 

Fwd. 36.16 29.25 27.11 

Bwd. 36.17 29.22 27.12 

Bid. 36.25 29.32 27.16 
 

Table 4: Translation performances in BLEU-4 (%) 
over 10K-best lists. 
 

We then tested the effect of the extension ac-
cording to which the expectations over n-gram 

counts are computed on partial hypotheses rather 

than whole candidate translations as described in 
Section 2.2. As shown in Table 5, we got tiny 

improvements on both test sets by computing the 

expectations over n-gram counts on partial hypo-
theses. 

 

testset NIST’06 NIST’08 

full 36.11 29.14 

partial 36.13 29.19 
 
Table 5: Translation performances in BLEU-4 (%) 
over 1000-best lists for Chinese-to-English task: 
“full” represents expectations over n-gram counts that 
are computed on whole hypotheses; “partial” 
represents expectations over n-gram counts that are 
computed on partial hypotheses. 

3.3 Discussion  

To speed up the search, the partial hypotheses in 
each expanding step are pruned. When pruning is 

applied, forward and backward n-gram expan-

sion would generate different new hypothesis 

lists. Let us look back at the example in Figure 1.  

15



Given 5 original hypotheses in Figure 1, if we set 

the beam size equal to 5 (the size of the original 

hypotheses), the forward and backward n-gram 

expansion generated different new hypothesis 
lists, as shown in Figure 2. 

 

forward backward 

one week's work . 

about week's work 

one week's work . 

about one week's work 

 
Figure 2: Different new hypothesis lists generated by 
forward and backward n-gram expansion. 

 
For bi-directional n-gram expansion, the cho-

sen translation for a source sentence comes from 

the decoder 94% of the time for WMT 2006 test 
set, 90% for NIST test sets; it comes from for-

ward n-gram expansion 2% of the time for WMT 

2006 test set, 4% for NIST test sets; it comes 
from backward n-gram expansion 4% of the time 

for WMT 2006 test set, 6% for NIST test sets. 

This proves bidirectional n-gram expansion is a 

good way of enlarging the search space. 

4 Conclusions and Future Work 

We have proposed a fast consensus hypothesis 
regeneration approach for machine translation. It 

combines the advantages of feature-based con-

sensus decoding and hypothesis regeneration. 

This approach is more efficient than previous 
work on hypothesis regeneration, and it explores 

a wider search space than consensus decoding, 

resulting in improved performance.  Experiments 
showed consistent improvements across lan-

guage pairs. 

Instead of N-best lists, translation lattices or 

forests have been shown to be effective for MBR 
decoding (Zhang and Gildea, 2008; Tromble et 

al., 2008), and DeNero et al. (2009) showed how 

to compute expectations of n-grams from a trans-
lation forest. Therefore, our future work may 

involve hypothesis regeneration using an n-gram 

language model trained on the translation forest. 

References  

B. Chen, M. Federico and M. Cettolo. 2007. Better N-
best Translations through Generative n-gram Lan-
guage Models. In: Proceedings of MT Summit XI. 
Copenhagen, Denmark. September. 

B. Chen, M. Zhang, A. Aw, and H. Li. 2008. Regene-
rating Hypotheses for Statistical Machine Transla-
tion. In: Proceedings of COLING. pp105-112. 
Manchester, UK, August. 

J. DeNero, D. Chiang and K. Knight. 2009. Fast Con-
sensus Decoding over Translation Forests. In: Pro-
ceedings of ACL. Singapore, August. 

J. Duchateau, K. Demuynck, and P. Wambacq. 2001. 
Confidence scoring based on backward language 
models. In: Proceedings of ICASSP 2001. Salt 
Lake City, Utah, USA, May. 

L. Huang and D. Chiang. 2007. Forest Rescoring: 
Faster Decoding with Integrated Language Models. 

In: Proceedings of ACL. pp. 144-151, Prague, 
Czech Republic, June.  

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Mo-
ran, R. Zens, C. Dyer, O. Bojar, A. Constantin and 
E. Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In: Proceedings of 
ACL. pp. 177-180, Prague, Czech Republic. 

S. Kumar and W. Byrne. 2004. Minimum Bayes-risk 
decoding for statistical machine translation. In: 
Proceedings of NAACL. Boston, MA, May. 

W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 
2008. Lattice-based Minimum Error Rate Training 

for Statistical Machine Translation. In: Proceed-

ings of EMNLP. pp. 725-734, Honolulu, USA, 
October. 

F. Och. 2003. Minimum error rate training in statistic-
al machine translation. In: Proceedings of ACL. 
Sapporo, Japan. July. 

F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. 
Eng, V. Jain, Z. Jin, and D. Radev. 2004. A Smor-
gasbord of Features for Statistical Machine Trans-
lation. In: Proceedings of NAACL. Boston. 

K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
BLEU: A method for automatic evaluation of ma-
chine translation. In: Proceedings of the ACL 2002. 

R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 
2008. Lattice minimum Bayes-risk decoding for 
statistical machine translation. In: Proceedings of 
EMNLP. Hawaii, US. October. 

N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.  
2007. NRC’s Portage system for WMT 2007. In: 
Proceedings of ACL Workshop on SMT. Prague, 
Czech Republic, June. 

H. Zhang and D. Gildea. 2008. Efficient multipass 
decoding for synchronous context free grammars. 
In: Proceedings of ACL. Columbus, US. June. 

16


