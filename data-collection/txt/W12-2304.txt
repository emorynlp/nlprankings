










































A Regularized Compression Method to Unsupervised Word Segmentation


Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 26–34,
Montréal, Canada, June 7, 2012. c©2012 Association for Computational Linguistics

A Regularized Compression Method To Unsupervised Word Segmentation

Ruey-Cheng Chen, Chiung-Min Tsai and Jieh Hsiang

National Taiwan University

1 Roosevelt Rd. Sec. 4

Taipei 106, Taiwan

rueycheng@turing.csie.ntu.edu.tw

cmtsai@mail.lis.ntu.edu.tw

jhsiang@ntu.edu.tw

Abstract

Languages are constantly evolving through

their users due to the need to communicate

more efficiently. Under this hypothesis, we

formulate unsupervised word segmentation as

a regularized compression process. We re-

duce this process to an optimization problem,

and propose a greedy inclusion solution. Pre-

liminary test results on the Bernstein-Ratner

corpus and Bakeoff-2005 show that the our

method is comparable to the state-of-the-art in

terms of effectiveness and efficiency.

1 Introduction

Unsupervised word segmentation has been a popular

research subject due to its close connection to lan-

guage acquisition. It has attracted researchers from

different communities, including linguistics, cogni-

tive science, and machine learning, to investigate

how human beings develop and harness their lan-

guages, and, more importantly, how knowledge is

acquired.

In this paper we propose a new formulation to the

unsupervised word segmentation problem. Our idea

is based on the observation that language evolves be-

cause of the need to reduce communication efforts.

For instance, new terminologies, abbreviations, and

slang that carry complex semantics which cannot be

efficiently expressed in the original languages are in-

vented so that concepts can be conveyed. Such an

evolution, we hypothesize, is limited to the extent

where the evolved vocabulary exhibits similar com-

plexity as the original one, in light of reducing the

extra cost to pick up the new language. This process

is realized as an optimization problem called regu-

larized compression, which gets this name from its

analogy to text compression.

The rest of the paper is organized as follows.

We briefly summarize related work on unsupervised

word segmentation in Section 2. In Section 3, we in-

troduce the proposed formulation. The iterative al-

gorithm and other technical details for solving the

optimization problem are covered in Section 4. In

Section 5, we describe the evaluation procedure and

discuss the experimental results. Finally, we present

concluding remarks in Section 6.

2 Related Work

The past few years have seen many nonparametric

Bayesian methods developed to model natural lan-

guages. Many such applications were applied to

word segmentation and have collectively reshaped

the entire research field. Two most notable exam-

ples are hierarchical Bayesian models and the min-

imum description length principle. Our method fits

in the latter category since we use this principle to

optimize model parameters.

Hierarchical Bayesian methods were first in-

troduced to complement conventional probabilistic

methods to facilitate context-aware word generation.

Goldwater et al. (2006) used hierarchical Dirichlet

processes (HDP) to induce contextual word mod-

els. Their approach was a significant improvement

over conventional probabilistic methods, and has in-

spired further explorations into more advanced hi-

erarchical modeling techniques. Such examples in-

clude the nested Pitman-Yor process (Mochihashi et

al., 2009), a sophisticated installment for hierarchi-

26



cal modeling at both word and character levels, and

adaptor grammars (Johnson and Goldwater, 2009), a

framework that aligns HDP to probabilistic context-

free grammars.

The minimum description length (MDL) princi-

ple, originally developed in the context of infor-

mation theory, was adopted in Bayesian statistics

as a principled model selection method (Rissanen,

1978). Its connection to lexical acquisition was

first uncovered in behavioral studies, and early ap-

plications focused mostly on applying MDL to in-

duce word segmentation that results in compact lex-

icons (Kit and Wilks, 1999; Yu, 2000; Argamon et

al., 2004). More recent approaches (Zhikov et al.,

2010; Hewlett and Cohen, 2011) used MDL in com-

bination with existing algorithms, such as branch-

ing entropy (Tanaka-Ishii, 2005; Jin and Ishii, 2006)

and bootstrap voting experts (Hewlett and Cohen,

2009), to determine the best segmentation parame-

ters. On various benchmarks, MDL-powered algo-

rithms have achieved state-of-the-art performance,

sometimes even surpassing that of the most sophis-

ticated hierarchical modeling methods.

3 Regularized Compression

3.1 Preliminaries

Consider that the unsegmented text consists of K ut-
terances and totally of N characters. We denote the
text as a sequence of characters c = 〈c1, . . . , cN 〉,
as if conceptually concatenating all the K utter-
ances into one string. The positions of all the ut-

terance boundaries in c are represented as a set

U = {u0 = 0, u1, . . . , uK}. In other words, the
k-th utterance (k = 1, . . . ,K) is stored as the sub-
sequence 〈cuk−1+1, . . . , cuk〉 in c.

A segmented text is denoted as a sequence of

words w = 〈w1, w2, . . . , wM 〉 for some M < N . It
represents the same piece of text as c does. The word

sequence w is said to respect the utterance bound-

aries U if any word in the sequence does not span
over two utterances. Unique elements in a charac-

ter or word sequence implicitly define an alphabet

set (or lexicon). Hereafter, we denote such alphabet

sets for c and w as Ac and Aw, respectively.

3.2 Effects of Compression

Word segmentation results from compressing a se-

quence of characters. By compression, we mean to

replace the occurrences for some k-characters sub-
sequence 〈c1, c2, . . . , ck〉 in the text with those for a
new string w = c1c2 . . . ck (word). This procedure
can be generalized to include more subsequences to

be replaced, each with a different length. The result-

ing sequence is a mixture of characters and words

introduced during compression. For clarity, we use

the term token sequence to refer to such a mixed se-

quence of characters or words.

Compression has a few effects to the token se-

quence: (i) it increases the total number of tokens,

(ii) it expands the alphabet set to include newly pro-

duced tokens, (iii) it affects the entropy rate esti-

mates. Note that, by seeing a token sequence as

a series of outcomes drawn from some underlying

stochastic process, we can estimate the entropy rate

empirically.

Items (i) and (ii) are natural consequences of com-

pression. The effort to describe the same piece

of information gets reduced at the expense of ex-

panding the vocabulary, and sometimes even chang-

ing the usage. A real-life example for this is that

language users invent new terminologies for effi-

ciently conveying complex information. Item (iii)

describes something more subtle. Observe that,

when some n occurrences of a k-character subse-
quence 〈c1, c2, . . . , ck〉 get compressed, each char-
acter ci loses n occurrences, and totally nk occur-
rences move away from the subsequence; as a result,

the newly created word w receives n occurrences. It
is clear that compression has this side effect of re-

distributing probability masses among the observa-

tions (i.e., characters), thereby causing deviation to

entropy rate estimates.

3.3 Formulation

The choice of subsequences to be compressed is es-

sential in the aforementioned process. We hypothe-

size that a good choice has the following two prop-

erties: (i) higher frequency, and (ii) low deviation in

entropy rate.

We motivate these two properties as follows.

First, high frequency subsequences are favorable

here since they are more likely to be character-level

27



collocations; compressing these subsequences re-

sults in better compression rate. Second, deviation

in entropy rate is reflected in vocabulary complex-

ity, and we believe that it directly translates to efforts

that language users pay to adapt to the new language.

In this case, there seems no reason to believe that ei-

ther increasing or decreasing vocabulary complexity

is beneficial, since in two trivial “bad choices” that

one can easily imagine, i.e., the text being fully seg-

mented or unsegmented, the entropy rates reach both

extremes.

Motivated by these observations, we expect that

the best word segmentation (i) achieves some prede-

fined compression rate, and (ii) minimizes deviation

in entropy rate. This idea is realized as an optimiza-

tion problem, called regularized compression. Con-

ceptually, this problem is defined as:

minimize DV(c,w)
subject to w respects U

| |w||c| − ρ| ≤ ǫ
(1)

where ρ denotes some expected compression ratio
and ǫ denotes the tolerance. Note that DV(c,w) =
|H̃(C) − H̃(W )| represents the deviation in en-
tropy rate with respect to sequences c and w. In

this definition, H̃(C) and H̃(W ) denote the em-
pirical entropy rates for random variables C ∈ Ac
and W ∈ Aw, estimated on the corresponding se-
quences c and w, respectively.

4 Iterative Algorithm

4.1 Ordered Ruleset

Acknowledging that exponentially many feasible

word sequences need to be checked, we propose an

alternative formulation in a restricted solution space.

The idea is, instead of optimizing for segmentations,

we search for segmentation generators, i.e., a set of

functions that generate segmentations from the in-

put. The generators we consider here is the ordered

rulesets.

An ordered ruleset R = 〈r1, r2, . . . , rk〉 is a se-
quence of translation rules, each of which takes the

following form:

w → c1c2 . . . cn,

where the right-hand side (c1c2 . . . cn) denotes the
n-token subsequence to be replaced, and the left-

hand side (w) denotes the new token to be intro-
duced. Applying a translation rule r to a token se-
quence has an effect of replacing all the occurrences

for subsequence c1c2 . . . cn with those for token w.
Applying an ordered ruleset R to a token se-

quence is equivalent to iteratively applying the trans-

lation rules r1, r2, . . . , rk in strict order. Specifi-
cally, consider that the initial token sequence is de-

noted as c(0) and let the final result be denoted as

c
(k). By iterative application, we mean to repeat the

following step for i = 1 . . . k:

Apply rule ri to c
(i−1) and save the result

as c(i).

4.2 Alternative Formulation

This notion of ordered rulesets allows one to explore

the search space efficiently using a greedy inclusion

algorithm. The idea is to maintain a globally best

ruleset B that covers the best translation rules we
have discovered so far, and then iteratively expand

B by discovering new best rule and adding it to rule-
set. The procedure repeats several times until the

compression rate reaches some predefined ratio ρ. In
each iteration, the best translation rule is determined

by solving a modified version of Equation (1), which

is written as follows:

(In iteration i)

minimize α |c
(i)|

|c(i−1)|
+ DV(c(i−1), c(i))

subject to r is a rule

r(c(i−1)) = c(i)

c
(i) respects U

(2)

Note that the alternative formulation is largely a

greedy version of Equation (1) except a few minor

changes. First, the compression rate constraint be-

comes the termination condition in the greedy in-

clusion algorithm. Second, we add an extra term

|c(i)|/|c(i−1)| to the objective to encourage early in-
clusion of frequent collocations. The trade-off pa-

rameter α is introduced in Equation (2) to scalarize
both terms in the objective.

A brief sketch of the algorithm is given in the fol-

lowing paragraphs.

1. Let B be an empty ordered ruleset, and let c(0)

be the original sequence of tokens.

28



2. Repeat the following steps for each i ∈ N ,
starting from i = 1, until the compression rate
reaches some predefined threshold.

(a) Find a rule r that maximizes Equation (2)

(b) Apply the rule r to form a new sequence
c
(i) from c(i−1).

(c) Add r to the end of B.

3. Output B and the final sequence.

4.3 Implementation

Additional care needs to be taken in implementing

Steps 2a and 2b. The simplest way to collect n-gram
counts for computing the objective in Equation (2) is

to run multiple scans over the entire sequence. Our

experience suggests that using an indexing structure

that keeps track of token positions can be more ef-

ficient. This is especially important when updating

the affected n-gram counts in each iteration. Since
replacing one occurrence for any subsequence af-

fects only its surrounding n-grams, the total num-
ber of such affected n-gram occurrences in one it-
eration is linear in the number of occurrences for

the replaced subsequence. Using an indexing struc-

ture in this case has the advantage to reduce seek

time. Note that, however, the overall running time

remains in the same complexity class regardless of

the deployment of an indexing structure. The time

complexity for this algorithm is O(TN), where T is
the number of iterations and N is the length of the
input sequence.

Although it is theoretically appealing to create an

n-gram search algorithm, in this preliminary study
we used a simple bigram-based implementation for

efficiency. We considered only bigrams in creat-

ing translation rules, expecting that the discovered

bigrams can grow into trigrams or higher-order n-
grams in the subsequent iterations. To allow un-

merged tokens (i.e., characters that was supposed

to be in one n-gram but eventually left out due to
bigram implementation) being merged into the dis-

covered bigram, we also required that that one of

the two participating tokens at the right-hand side

of any translation rule has to be an unmerged to-

ken. This has a side effect to exclude generation of

collocation-based words1. It can be an issue in cer-

1Fictional examples include “homework” or “cellphone”.

tain standards; on the test corpora we used, this kind

of problems is not obvious.

Another constraint that we added to the imple-

mentation is to limit the choice of bigrams to those

has more frequency counts. Generally, the number

of occurrence for any candidate bigram being con-

sidered in the search space has to be greater or equal

to some predefined threshold. In practice, we found

little difference in performance for specifying any

integer between 3 and 7 as the threshold; in this pa-

per, we stick to 3.

5 Evaluation

5.1 Setup

We conducted a series of experiments to investi-

gate the effectiveness of the proposed segmentation

method under different language settings and seg-

mentation standards. In the first and the second

experiments, we focus on drawing comparison be-

tween our method and state-of-the-art approaches.

The third experiment focuses on the influence of

data size to segmentation accuracy.

Segmentation performance is assessed using stan-

dard metrics, such as precision, recall, and F-

measure. Generally, these measures are reported

only at word level; in some cases where further anal-

ysis is called for, we report boundary-level and type-

level measures as well. We used the evaluation script

in the official HDP package to calculate these num-

bers.

The reference methods we considered in the com-

parative study include the following:

• Hierarchical Dirichlet process, denoted as HDP
(Goldwater et al., 2009);

• Nested Pitman-Yor process, denoted as NPY
(Mochihashi et al., 2009);

• Adaptor grammars, denoted as AG (Johnson
and Goldwater, 2009);

• Branching entropy + MDL, denoted as Ent-
MDL (Zhikov et al., 2010);

• Bootstrap voting experts + MDL, denoted as
BVE-MDL (Hewlett and Cohen, 2011);

• Description length gain, denoted as DLG (Zhao
and Kit, 2008).

29



The proposed method is denoted as RC; it is also

denoted as RC-MDL in a few cases where MDL is

used for parameter estimation.

5.2 Parameter Estimation

There are two free parameters α and ρ in our model.
The parameter α specifies the degree to which we
favors high-frequency collocations when solving

Equation (2). Experimentation suggests that α can
be sensitive when set too low2. Practically, we rec-

ommend optimizing α based on grid search on de-
velopment data, or the MDL principle. The formula

for calculating description length is not shown here;

see Zhikov et al. (2010), Hewlett and Cohen (2011),

and Rissanen (1978) for details.

The expected compression rate ρ determines
when to stop the segmentor. It is related to the

expected word length: When the compression rate

|c|/|w| reaches ρ and the segmentor is about to stop,
1/ρ is the average word length in the segmentation.
In this sense, it seems ρ is somehow connected to the
language of concern. We expect that optimal values

learned on one data set may thus generalize on the

other sets of the same language. Throughout the ex-

periments, we estimated this value based on devel-

opment data.

5.3 Evaluation on Bernstein-Ratner Corpus

We conducted the first experiment on the Bernstein-

Ratner corpus (Bernstein-Ratner, 1987), a standard

benchmark for English phonetic segmentation. We

used the version derived by Michael Brent, which

is made available in the CHILDES database (Brent

and Cartwright, 1996; MacWhinney and Snow,

1990). The corpus comprises 9,790 utterances,

which amount to 95,809 words in total. Its rel-

atively small size allows experimentation with the

most computational-intensive Bayesian models.

Parameter estimation for the proposed method has

been a challenge due to the lack of appropriate de-

velopment data. We first obtained a rough estimate

for the compression rate ρ via human inspection into
the first 10 lines of the corpus (these 10 lines were

later excluded in evaluation) and used that estimate

to set up the termination condition. Since the first

2Informally speaking, when α < H̃(c). The analysis is not
covered in this preliminary study.

P R F Time

HDP 0.752 0.696 0.723 –

NPY, bigram 0.748 0.767 0.757 17 min.

AG – – 0.890 –

Ent-MDL 0.763 0.745 0.754 2.6 sec.

BVE-MDL 0.793 0.734 0.762 2.6 sec.

RC-MDL 0.771 0.819 0.794 0.9 sec.

Table 2: Performance evaluation on the Bernstein-Ratner

corpus. The reported values for each method indicate

word precision, recall, F-measure and running time, re-

spectively. The boldface value for each column indicates

the top performer under the corresponding metric.

10 lines are too small to reveal any useful segmenta-

tion cues other than the word/token ration of interest,

we considered this setting (“almost unsupervised”)

a reasonable compromise. In this experiment, ρ is
set to 0.37; the trade-off parameter α is set to 8.3,
optimized using MDL principle in a two-pass grid

search (the first pass over {1, 2, . . . , 20} and the sec-
ond over {8.0, 8.1, . . . , 10.0}).

A detailed performance result for the proposed

method is described in Table 1. A reference run

for HDP is included for comparison. The pro-

posed method achieved satisfactory result at word

and boundary levels. Nevertheless, low type-level

numbers (in contrast to those for HDP) together with

high boundary recall suggested that we might have

experienced over-segmentation.

Table 2 covers the same result with less details

in order to compare with other reference methods.

All the reported measures for reference methods

are directly taken from the literature. The result

shows that AG achieved the best performance in F-

measure (other metrics are not reported), surpass-

ing all the other methods by a large margin (10 per-

cent). Among the other methods, our method paired

with MDL achieved comparable performance as the

others in precision; it does slightly better than the

others in recall (5 percent) and F-measure (2.5 per-

cent). Furthermore, our algorithm also seems to be

competitive in terms of computational efficiency. On

this benchmark it demanded only minimal memory

low as 4MB and finished the segmentation run in 0.9

second, even less than the reported running time for

both MDL-based algorithms.

30



P R F BP BR BF TP TR TF

HDP, Bernstein-Ratner 0.75 0.70 0.72 0.90 0.81 0.85 0.64 0.55 0.59

RC-MDL, Bernstein-Ratner 0.77 0.82 0.79 0.85 0.92 0.89 0.57 0.48 0.50

RC, CityU training 0.75 0.79 0.77 0.89 0.93 0.91 0.63 0.35 0.45

RC, MSR training 0.73 0.82 0.77 0.86 0.96 0.91 0.70 0.26 0.38

Table 1: Performance evaluation for the proposed method across different test corpora. The first row indicates a

reference HDP run (Goldwater et al., 2009); the other rows represent the proposed method tested on different test cor-

pora. Columns indicates performance metrics, which correspond to precision, recall, and F-measure at word (P/R/F),

boundary (BP/BR/BF), and type (TP/TR/TF) levels.

Corpus Training (W/T) Test (W/T)

AS 5.45M / 141K 122K / 19K

PKU 1.1M / 55K 104K / 13K

CityU 1.46M / 69K 41K / 9K

MSR 2.37M / 88K 107K / 13K

Table 3: A short summary about the subsets in the

Bakeoff-2005 dataset. The size of each subset is given in

number of words (W) and number of unique word types

(T).

5.4 Evaluation on Bakeoff-2005 Corpus

The second benchmark that we adopted is the

SIGHAN Bakeoff-2005 dataset (Emerson, 2005)

for Chinese word segmentation. The corpus has

four separates subsets prepared by different research

groups; it is among the largest word segmentation

benchmarks available. Table 3 briefly summarizes

the statistics regarding this dataset.

We decided to compare our algorithm with de-

scription length gain (DLG), for that it seems to de-

liver best segmentation accuracy among other un-

supervised approaches ever reported on this bench-

mark (Zhao and Kit, 2008). Since the reported

values for DLG were obtained on another closed

dataset Bakeoff-2006 (Levow, 2006), we followed a

similar experimental setup as suggested in the liter-

ature (Mochihashi et al., 2009): We compared both

methods only on the training sets for the common

subsets CityU and MSR. Note that this experimental

setup departed slightly from that of Mochihashi et al.

in that all the comparisons were strictly made on the

training sets. The approach is more straightforward

than the suggested sampling-based method.

Other baseline methods that we considered in-

clude HDP, Ent-MDL, and BVE-MDL, for their

representativeness in segmentation performance and

CityU MSR

RC, r = 0.65 0.770 0.774
DLG, ensemble 0.684 0.665

Ent-MDL, nmax = 3 0.798 0.795

Table 4: Performance evaluation on the common training

subsets in the Bakeoff-2005 and Bakeoff-2006 datasets.

The reported values are token F-measure. The boldface

value in each column indicates the top performer for the

corresponding set.

ease of implementation. The HDP implementation

we used is a modified version of the offical HDP

package3; we patched the package to make it work

with Unicode-encoded Chinese characters. For Ent-

MDL and BVE-MDL, we used the software pack-

age4 distributed by Hewlett and Cohen (2011). We

estimated the parameters using the AS training set

as the development data. We set α to 6 based on a
grid search. The expected compression rate ρ that
we learned from the development data is 0.65.

In Table 1, we give a detailed listing of vari-

ous performance measures for the proposed method.

Segmentation performance seems moderate at both

word and boundary levels. Nevertheless, high type

precision and low type recall on both CityU and

MSR training corpora signaled that our algorithm

failed to discover most word types. This issue, we

suspect, was caused by exclusion of low-frequency

candidate bigrams, as discussed in Section 4.3.

Table 4 summarizes the result for word segmen-

tation conducted on the CityU and MSR subsets of

Bakeoff-2005. Due to practical computational lim-

its, we were not able to run HDP and BVE-MDL

on any complete subset. The result shows that our

3http://homepages.inf.ed.ac.uk/sgwater/
4http://code.google.com/p/voting-experts

31



0.0 0.2 0.4 0.6 0.8 1.0

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

Data size in percentage

F
−

m
e

a
s
u

re

F

BF

TF

Figure 1: Performance evaluation for the proposed

method on the CityU training set.

CityU-1k MSR-1k

RC, r = 0.65 0.505 0.492
HDP, 10 sample average 0.591 0.623

RC, r = 0.65/punc. 0.599 0.591

Table 5: Performance evaluation on two random samples

from the common sets (CityU and MSR subsets) in the

Bakeoff-2005 and Bakeoff-2006 datasets.

algorithm outperforms DLG by 8 to 10 percents in

F-measure, while Ent-MDL still performs slightly

better, achieving the top performance among all the

experimental runs on both subsets.

To compare with HDP, we conducted another test

run on top of a random sample of 1,000 lines from

each subset. We chose 1,000 lines because HDP can

easily consume more than 4GB of main memory on

any larger sample. We adopted standard settings for

HDP: α0 = 3, 000, α1 = 300, and pb = 0.2. In
each trial run, we ran the Gibbs sampler for 20,000

iterations using simulated annealing (Goldwater et

al., 2009). We obtained 10 samples from the Gibbs

sampler and used the average performance in com-

parison. It took slightly more than 50 hours to col-

lect one trial run on one subset.

The evaluation result is summarized in Table 5.

We ran our algorithm to the desired compression

ratio r = 0.65 on this small sample. The result

0.0 0.2 0.4 0.6 0.8 1.0

0
.0

0
.2

0
.4

0
.6

0
.8

1
.0

Data size in percentage

F
−

m
e

a
s
u

re

F

BF

TF

Figure 2: Performance evaluation for the proposed

method on the MSR training set.

shows that the performance of regularized compres-

sion is inferior to that of HDP by 9 to 13 percents

in F-measure for both sets. To investigate why, we

looked into the segmentation output. We observed

that, in the regularized compression output, most of

the punctuation marks were incorrectly aligned to

their neighboring words, owing to the short of fre-

quency counts in this small sample. The HDP, how-

ever, does not seem to suffer from this issue.

We devised a simple post-processing step, in

which each punctuation mark was forced segmented

from the surrounding text. Another outside test was

conducted to see how well the algorithm works us-

ing heuristics derived from minimal domain knowl-

edge. The additional run is denoted as RC/punc.

The result is shown in Table 5. From the result,

we found that the combined approach works slightly

better than HDP in one corpus, but not in the other.

5.5 Effects of Data Size

We employed the third experiment to study the influ-

ence of corpora size to segmentation accuracy. Since

the proposed method relies on empirical estimates

for entropy rate to decide the word boundaries, we

were interested in learning about how it responds to

relatively low and high volume input.

This experiment was conducted on CityU and

32



MSR training sets. On each corpus, we took the first

k% of data (in terms of utterances) and tested the
proposed method against that subset; this test was

repeated several times with different values for k. In
this experiment, we chose the value for k from the
set {2, 4, 6, 8, 10, 20, 30, . . . , 90, 100}. The perfor-
mance is evaluated using word, boundary, and type

F-measures.

Figures 1 and 2 show the experiment results. Both

figures revealed similar patterns for segmentation

performance at different volume levels. Word F-

measures for both corpora begin at roughly 0.52,

climb up rapidly to 0.73 as the volume grows from

2% to 20%, and finally settle on some value around

0.77. Boundary F-measures for both corpora show a

similar trend—a less steep increase before 20% from

0.80 to 0.89 followed by a plateau at around 0.93.

Here, the result seems to suggest that estimating to-

ken entropy rate using less than 20% of data might

be insufficient for this type of text corpora. Further-

more, since performance is saturated at such an early

stage, it seems feasible to split the entire dataset into

a number of folds (e.g., 5, in this case) and solve

each fold individually in parallel. This technique

may greatly enhance the run-time efficiency of the

segmentor.

The patterns we observed for type F-measure tells

another story. On both corpora, type F-measures do

not seem to improve as data volume increases. On

CityU corpora, type F-measure gradually increased

from 0.42 to 0.48 and then slowly falling back to

0.45. On MSR corpora, type F-measure peaked at

0.45 when receiving 10% of data; after that it started

decreasing, going all the way down to 0.37, even

lower than the number 0.43 it received at the begin-

ning. Our guess is that, at some early point (20%),

the proposed method started to under-segment the

text. We suspect that there is some deep con-

nection between performance saturation and under-

segmentation, since from the result they both begin

at roughly the same level. Further investigation in

this respect is needed to give out definitive explana-

tions.

6 Concluding Remarks

Preliminary experimental results suggest that the

regularized compression method, even only with

partial evidence, seems as effective as the state-of-

the-art methods in different language settings. When

paired with MDL criteria, regularized compression

is comparable to hierarchical Bayesian methods and

MDL-based algorithms in terms of segmentation ac-

curacy and computational efficiency. Furthermore,

regularized compression is less memory-demanding

than the other approaches; thus, it scales more easily

to large corpora for carrying out certain tasks such

as segmenting historical texts written in ancient lan-

guages, or preprocessing a large dataset for subse-

quent manual annotation.

We have identified a number of limitations of reg-

ular compression. First, the choice of candidate n-
grams does not cover hapax legomena, i.e., words

that occur only once in the corpus. At present, pre-

cluding these low-frequency n-grams seems to be
a necessary compromise due to our limited under-

standing about the dynamics behind regular com-

pression. Second, regularized compression does not

work well with low volume data, since on smaller

dataset the distribution of frequency counts is less

precise. Third, the algorithm may stop identifying

new word types at some point. We suspect that this

is related to the choice of n-gram, since in our im-
plementation no two existing “words” can be aggre-

gated into one. These issues shall be addressed in

our future work.

Acknowledgments

We thank the anonymous reviewers for their valu-

able comments. The research efforts described in

this paper are supported under the National Tai-

wan University Digital Archives Project (Project

No. NSC-98-2631-H-002-005), which is sponsored

by National Science Council, Taiwan.

References

Shlomo Argamon, Navot Akiva, Amihood Amir, and

Oren Kapah. 2004. Efficient unsupervised recursive

word segmentation using minimum description length.

In Proceedings of the 20th international conference

on Computational Linguistics, COLING ’04, Strouds-

burg, PA, USA. Association for Computational Lin-

guistics.

Nan Bernstein-Ratner. 1987. The phonology of parent

child speech. Children’s language, 6:159–174.

33



Michael R. Brent and Timothy A. Cartwright. 1996. Dis-

tributional regularity and phonotactic constraints are

useful for segmentation. In Cognition, pages 93–125.

Thomas Emerson. 2005. The second international chi-

nese word segmentation bakeoff. In Proceedings of

the Fourth SIGHAN Workshop on Chinese Language

Processing, volume 133. Jeju Island, Korea.

Sharon Goldwater, Thomas L. Griffiths, and Mark John-

son. 2006. Contextual dependencies in unsupervised

word segmentation. In Proceedings of the 21st In-

ternational Conference on Computational Linguistics

and the 44th annual meeting of the Association for

Computational Linguistics, ACL-44, pages 673–680,

Stroudsburg, PA, USA. Association for Computational

Linguistics.

Sharon Goldwater, Thomas L. Griffiths, and Mark John-

son. 2009. A bayesian framework for word segmen-

tation: Exploring the effects of context. Cognition,

112(1):21–54, July.

Daniel Hewlett and Paul Cohen. 2009. Bootstrap voting

experts. In Proceedings of the 21st international jont

conference on Artifical intelligence, IJCAI’09, pages

1071–1076, San Francisco, CA, USA. Morgan Kauf-

mann Publishers Inc.

Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-

vised word segmentation with BVE and MDL. In

Proceedings of the 49th Annual Meeting of the Asso-

ciation for Computational Linguistics: Human Lan-

guage Technologies: short papers - Volume 2, HLT

’11, pages 540–545, Stroudsburg, PA, USA. Associ-

ation for Computational Linguistics.

Zhihui Jin and Kumiko T. Ishii. 2006. Unsupervised seg-

mentation of chinese text by use of branching entropy.

In Proceedings of the COLING/ACL on Main confer-

ence poster sessions, COLING-ACL ’06, pages 428–

435, Stroudsburg, PA, USA. Association for Compu-

tational Linguistics.

Mark Johnson and Sharon Goldwater. 2009. Improving

nonparameteric bayesian inference: experiments on

unsupervised word segmentation with adaptor gram-

mars. In Proceedings of Human Language Technolo-

gies: The 2009 Annual Conference of the North Ameri-

can Chapter of the Association for Computational Lin-

guistics, NAACL ’09, pages 317–325, Stroudsburg,

PA, USA. Association for Computational Linguistics.

Chunyu Kit and Yorick Wilks. 1999. Unsupervised

learning of word boundary with description length

gain. In CoNLL-99, pages 1–6, Bergen, Norway.

Gina-Anne Levow. 2006. The third international chinese

language processing bakeoff: Word segmentation and

named entity recognition. In Proceedings of the Fifth

SIGHAN Workshop on Chinese Language Processing,

volume 117. Sydney: July.

Brian MacWhinney and Catherine Snow. 1990. The

child language data exchange system: an update.

Journal of child language, 17(2):457–472, June.

Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.

2009. Bayesian unsupervised word segmentation with

nested Pitman-Yor language modeling. In Proceed-

ings of the Joint Conference of the 47th Annual Meet-

ing of the ACL and the 4th International Joint Confer-

ence on Natural Language Processing of the AFNLP:

Volume 1 - Volume 1, ACL ’09, pages 100–108,

Stroudsburg, PA, USA. Association for Computational

Linguistics.

Jorma Rissanen. 1978. Modeling by shortest data de-

scription. Automatica, 14(5):465–471, September.

Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of

context boundaries: An experiment using a web search

engine. In Robert Dale, Kam-Fai Wong, Jian Su,

and Oi Kwong, editors, Natural Language Process-

ing IJCNLP 2005, volume 3651 of Lecture Notes in

Computer Science, chapter 9, pages 93–105. Springer

Berlin / Heidelberg, Berlin, Heidelberg.

Hua Yu. 2000. Unsupervised word induction using MDL

criterion. In Proceedings of the International Sympo-

sium of Chinese Spoken Language Processing, Beijin,

China.

Hai Zhao and Chunyu Kit. 2008. An empirical compar-

ison of goodness measures for unsupervised chinese

word segmentation with a unified framework. In The

Third International Joint Conference on Natural Lan-

guage Processing (IJCNLP-2008).

Valentin Zhikov, Hiroya Takamura, and Manabu Oku-

mura. 2010. An efficient algorithm for unsupervised

word segmentation with branching entropy and MDL.

In Proceedings of the 2010 Conference on Empirical

Methods in Natural Language Processing, EMNLP

’10, pages 832–842, Stroudsburg, PA, USA. Associ-

ation for Computational Linguistics.

34


