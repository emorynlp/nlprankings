



















































Wordnet extension via word embeddings: Experiments on the Norwegian Wordnet


Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 298–302,
Gothenburg, Sweden, 23-24 May 2017. c©2017 Linköping University Electronic Press

Wordnet extension via word embeddings:
Experiments on the Norwegian Wordnet

Heidi Sand and Erik Velldal and Lilja Øvrelid
University of Oslo

Department of Informatics
{heidispe,erikve,liljao}@ifi.uio.no

Abstract

This paper describes the process of auto-
matically adding synsets and hypernymy
relations to an existing wordnet based
on word embeddings computed for POS-
tagged lemmas in a large news corpus,
achieving exact match attachment accu-
racy of over 80%. The reported experi-
ments are based on the Norwegian Word-
net, but the method is language indepen-
dent and also applicable to other wordnets.
Moreover, this study also represents the
first documented experiments of the Nor-
wegian Wordnet.

1 Introduction

This paper documents experiments with an un-
supervised method for extending a wordnet with
new words and automatically identifying the ap-
propriate hypernymy relations. Using word em-
beddings trained on a large corpus of news text
(~330 million tokens), candidate hypernyms for
a given target word are identified by computing
nearest neighbors lists towards the wordnet and re-
trieving the ancestors of the neighbors. The can-
didate hypernyms are then scored according to a
combination of distributional similarity and dis-
tance in the wordnet graph. While the particular
experimental results reported here are for the Nor-
wegian Wordnet (NWN), and using vectors esti-
mated on POS tagged lemmas of the Norwegian
news corpus, the methodology is generally appli-
cable and not specific to neither the language nor
the particular language resources used.

2 Background

Due to the coverage limitations of manually con-
structed semantic resources, several approaches
have attempted to enrich various taxonomies with
new relations and concepts. The general approach

is to attempt to insert missing concepts into a
taxonomy based on distributional evidence. In
their probabilistic formulation, Snow et al. (2006)
maximize the conditional probability of hyponym-
hypernym relations based on observed lexico-
syntactic patterns in a corpus. Jurgens and Pile-
hvar (2015) extend the existing WordNet taxon-
omy using an additional resource, Wiktionary, to
extract sense data based on information (morphol-
ogy/lexical overlap) in the term glosses.

The work which is most directly relevant to our
study is that of Yamada et al. (2009). They extend
an automatically generated Wikipedia-taxonomy
by inserting new terms based on various similar-
ity measures calculated from additional web doc-
uments. As the approach described in the current
paper will abstractly adapt the approach of Ya-
mada et al. (2009), we will devote some space to
elaborate how the algorithm works, glossing over
details that does not pertain to our setting. The in-
sertion mechanism works as follows: For a given
target word w we first find the k similar words
that are already present in the hierarchy, accord-
ing to some distributional similarity measure sim
and with the constraint that the similarity is greater
than some cutoff m. Secondly the hypernyms of
each of these k similar words are assigned scores,
based on a combination of the similarity measure
and a depth penalty d. The latter is a function of
the distance r in the hierarchy between the neigh-
bor and the hypernym, as the hypernym candidates
also include ancestor nodes beyond the immediate
parents. Finally, the hypernym h↑ with the highest
score will be selected for attaching w.

The function used for scoring hypernym candi-
dates is defined as follows (paraphrased here ac-
cording to the notation of the current paper):

score(h↑) = ∑
h↓∈desc(h↑)∩ksim(w)

dr(h↑,h↓)−1× sim(w,h↓) (1)

298



ksim(w) picks out the k nearest neighbors of the
target word w according to the distributional sim-
ilarity measure sim, with the constraint that the
similarity is greater than a cutoff m. The function
desc(h↑) picks out the descendants (hyponyms) of
the candidate hypernym h↑. The term dr(h↑,h↓)−1 is
the depth penalty where the parameter d can have
a value between 0 and 1, r(n↑,n↓) is the difference
in hierarchy depth between h↑ and h↓.

For sim, two distributional similarity measures
are tested by Yamada et al. (2009), based on re-
spectively 1) raw verb-noun dependencies and 2)
clustering of verb-noun dependencies. Yamada et
al. (2009) also apply a baseline approach, selecting
the hypernym of the most similar hyponym (base-
line approach 1), which essentially is the same as
specifying k=1 when computing the nearest neigh-
bors in the approach outlined above. Manually
evaluating a random sample of 200 of the high-
est scoring insertions, Yamada et al. (2009) report
an attachment accuracy of up to 91.0% among the
top 10,000, and 74.5% among the top 100,000,
when using the clustering based similarity – a re-
sult which substantially improves over the base-
line (yielding scores of ~55%).

Although Yamada et al. (2009) work with a se-
mantic taxonomy based on the Wikipedia struc-
ture, the scoring function in Equation 1 which
forms the pivot of the approach is general enough
to be adopted for other settings as well. Notably,
no assumptions are made about the particular sim-
ilarity function instantiating sim(wi,w j). In the
work reported in the current paper we experiment
with instead using a similarity function based on
word embeddings computed from a large unanno-
tated corpus, and apply this for extending NWN.

3 The Norwegian Wordnet

The Norwegian Wordnet (NWN) was created by
translation from the Danish Wordnet (DanNet)
(Pedersen et al., 2009). DanNet encodes both
relations found in Princeton Wordnet (Fellbaum,
1998) and EuroWordNet (Vossen, 1998), some
of which are also employed in NWN. There are
no prior publications documenting NWN, and
the current study provides the first reported ex-
periments on this resource. Before commenc-
ing our experiments it was therefore necessary to
pre-process NWN in order to (i) correct errors
in the format and/or the structure of NWN, and
(ii) remove named entities and multiword expres-

POS Lemmas Synsets Senses Monos. Polys.

Noun 38,440 43,112 48,865 31,957 6,483
Verb 2,816 4,967 5,580 1,612 1,204
Adj 2,877 3,179 3,571 2,413 464

Total 44,133 51,258 58,016 35,982 8,151

Table 1: Number of lemmas, synsets, senses and
monosemous/polysemous words for nouns, verbs
and adjectives in NWN.

sions. The resulting wordnet is summarized in Ta-
ble 1, showing the number of lemmas, synsets and
monosemous/polysemous terms broken down by
their part-of-speech (nouns, adjectives and verbs).
The modified NWN, which forms the basis for our
experiments, is made freely available.1

4 Word embeddings for tagged lemmas

This section describes how we generate the se-
mantic context vectors representing both unseen
target words and the words already present in
the existing wordnet synsets. These vectors form
the basis of our distributional similarity measure,
used both for computing nearest neighbors within
NWN for unclassified target words and for scor-
ing candidate hypernyms according to Equation 1
(where the similarity function will correspond to
the cosine of word vectors). Our semantic vectors
are given as word2vec-based word embeddings
(Mikolov et al., 2013a; Mikolov et al., 2013b)
estimated on the Norwegian Newspaper Corpus2.
This is a corpus of Norwegian newspaper texts
from the time period 1998–2014. We used approx-
imately 25% of this corpus (due to some technical
issues), which amounts to 331,752,921 tokens and
3,014,704 types.

Rather than estimating word embeddings from
raw text, we use POS-tagged lemmas in order to
have embeddings that more closely correspond to
the word representations found in NWN. In or-
der to extract lemmas and their parts of speech,
we pre-processed the data using the Oslo-Bergen
Tagger3 (Johannessen et al., 2012), a rule-based
POS-tagger for Norwegian which also performs
tokenization and lemmatization. The tagger was
accessed through the Language Analysis Portal
(LAP4) which provides a graphical web interface

1https://github.com/heisand/NWN
2http://www.nb.no/sprakbanken/repositorium
3http://www.tekstlab.uio.no/obt-ny
4https://lap.hpc.uio.no

299



to a wide range of language technology tools (Lap-
poni et al., 2013).

Word2vec implements two model types: con-
tinuous bag-of-words (CBOW) and skip-gram.
These differ in the prediction task they are trained
to solve: prediction of target words given context
words (CBOW) or the inverse, prediction of con-
text words given target words (skip-gram). We
used the word2vec implementation provided in the
free python library gensim (Řehůřek and Sojka,
2010), using the default parameters to train skip-
gram models. The defaults are a minimum of 5
occurrences in the corpus for the lemmas and an
embedding dimension of size 100. Five iterations
over the corpus was made.

5 The attachment process

In this section we detail the steps involved in clas-
sifying a new word w in the wordnet hierarchy.

5.1 Selecting nearest neighbors

The first step in the process is to compute the list
of k nearest neighbors of w according to the distri-
butional similarity sim(w,w′), a measure which in
our case corresponds to the cosine of word embed-
dings described in Section 4. Candidate neighbors
are words that are (a) already defined in NWN,
(b) have a hypernym in NWN, (c) have the same
part of speech as the target and (d) occur in the
news corpus with a sufficient frequency to have a
word embedding in the model described in Sec-
tion 4. In addition we discard any neighbors that
have a similarity less than some specified thresh-
old m. As described in Section 6 we tune both k
and m empirically.

5.2 Selecting candidate hypernyms

Hypernymy is a relation between synsets repre-
senting word senses, which means that the process
of selecting candidate hypernyms for scoring has
the following steps: First we (a) identify the list of
k nearest neighbors for a given target word w, and
then (b) for each neighbor word retrieve all synsets
that encode a sense for that word, before we finally
(c) retrieve all hypernym synsets, including all an-
cestor nodes, for those synsets.

Each candidate hypernym synset h↑ will in turn
be assigned a score according to Equation 1. The
synset with the highest score is finally chosen as
the hypernym synset for the target word to be at-
tached in NWN. Note that a given target word will

only be assigned a single hypernym.

5.3 Evaluation
There are several ways one could choose to evalu-
ate the quality of the words that are automatically
inserted into the hierarchy. For example, Yamada
et al. (2009) chose to manually evaluate a random
sample of 200 unseen words, while Jurgens and
Pilehvar (2015) treat the words already encoded
in the hierarchy as gold data and then try to re-
attach these. We here follow the latter approach.
However, while Jurgens and Pilehvar (2015) re-
strict their evaluation to monosemous words, we
also include polysemous words in order to make
the evaluation more realistic.

For evaluation and tuning we split the wordnet
into a development set and a test set, with 1388
target words in each. Potential targets only com-
prise words that have a hypernym encoded (which,
in fact, are not that many, as NWN is relatively
flat) and occur in the news corpus sufficiently of-
ten (≥ 5) to be represented by a word embedding.

We evaluate hypernym selection according to
both accuracy and attachment. While accuracy
reflects the percentage of target words added that
were correctly placed under the right hypernym,
the attachment score is the percentage of target
words that actually were inserted into NWN. A
candidate target word might end up not getting at-
tached if it has no neighbors fulfilling the require-
ments described in Section 5.1.

Computing accuracy based only on exactly cor-
rect insertions is rather strict. Intuitively, a hyper-
nymy relation can be right or wrong with varying
degrees. We therefore also include a soft accu-
racy measure that aims to take account of this by
counting how many hyponym or hypernym edges
that separates a lemma from its correct position.
Each edge will weight the count by a factor of 0.5,
partly based on the accuracy measure of Jurgens
and Pilehvar (2015), who, instead of weighting the
score, only measures accuracy as the number of
edges away that a lemma is placed from its orig-
inal position. We defined the formula for soft ac-
curacy as:

count(correct)+∑count(misplaced)0 1∗0.5edges
count(attached)

(2)

6 Experiments and results

The parameters that need to be empirically tuned
are: the depth penalty d, the number of k nearest

300



≥Freq. Dev. set #Words Att. Acc. Soft
5 1388 1337 96.33 55.80 63.25

100 854 840 98.36 61.67 68.08
500 461 448 97.18 62.50 68.89

1000 316 304 96.20 64.47 70.85

Table 2: Accuracy restricted to target words with
a frequency higher than some given threshold.
#Words shows the number of attached words.

neighbors to consider, and the minimum thresh-
old m for the similarity of neighbors towards the
target. After an initial round of experiments that
determined the approximate appropriate range of
values for these parameters, we performed an ex-
haustive grid search for the best parameter combi-
nation among the following values:

k ∈ [1,12] in increments of 1.
m ∈ [0.5,0.9] in increments of 0.05.
d ∈ [0.05,0.5] in increments of 0.05.

Optimizing for attachment accuracy, the best pa-
rameter configuration after tuning on the develop-
ment set was found to be k=6, m=0.5, and d=0.05,
yielding an accuracy of 55.80% and a degree of
attachment of 96.33%.

As one might expect that the embeddings are
more reliable for high-frequent words than for
low-frequent words, we also computed the dev-set
accuracy relative to frequency of occurrence in the
corpus used for estimating the embeddings. The
results are shown in Table 2. We indeed see that
the accuracy goes up when enforcing a higher fre-
quency cutoff, reaching 64.47% when setting the
cutoff to 1000, though per definition this means
sacrificing coverage.

We also evaluated the effect of only inserting
words that had hypernyms with a score higher than
a given cutoff, which again naturally leads to a
lower degree of attachment. Table 3 shows the ac-
curacies over the development set when enforcing
different cutoffs, showing an increased accuracy.
We see that the best performance is when the cut-
off on the hypernym score is set to 4.6, with a cor-
responding attachment accuracy of 83.26%.

Held-out results Applying the model configu-
ration (without cutoffs) to the held-out test words
of NWN yields an attachment of 95.97% and an
accuracy of 59.91% (soft acc. = 66.04%). We
see that there is a slight increase in the accuracy

≥Hyp. score #Words Att. Acc. Soft
0.2 1337 96.33 55.80 63.25
1.0 1185 85.38 59.41 66.85
1.8 958 69.02 65.66 73.20
2.6 720 51.87 72.92 80.16
3.4 505 36.38 78.22 85.00
4.6 239 17.22 83.26 89.33

Table 3: Accuracy restricted to hypernyms with
a score higher than some given threshold, com-
puted over the 1388 words in the development set.
#Words shows the number of attached words, e.g.
1337 is 96.33% of 1388.

for the insertions performed with the word embed-
dings when moving from the development data to
the held-out data. As a baseline approach we also
tried attaching each target word to the hypernym
of its 1-nearest-neighbor. (When there are several
candidate hypernyms available, we simply pick
the first candidate in the retrieved list.) Yielding
an accuracy of 47.61%, it is clear that we improve
substantially over the baseline when instead per-
forming insertion using the scoring function.

Applying the scoring function to the test set
using the cutoff with the highest accuracy from
Table 3, yields an accuracy of 84.96% (soft =
90.38%), though at the cost of a lower attachment
rate (16.28%).

7 Summary and further work

This paper has demonstrated the feasibility of us-
ing word embeddings for automatically extending
a wordnet with new words and assigning hyper-
nym relations to them. When scoring candidate
hypernyms we adopt the scoring function of Ya-
mada et al. (2009) and show that this yields high
accuracy even-though we apply it with a different
type of taxonomic hierarchy and different types
of distributional similarity measures. We com-
pute distributional similarity based on word em-
beddings estimated from the Norwegian news cor-
pus, using this as our basis for automatically at-
taching new words into hypernym relations in the
Norwegian Wordnet, with exact-match accuracies
of over 80%. For immediate follow-up work we
plan to let the parameter tuning be optimized to-
wards a combination of attachment and accuracy,
rather than just accuracy alone.

301



References

Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.

Janne Bondi Johannessen, Kristin Hagen, André
Lynum, and Anders Nøklestad. 2012. Obt+stat: A
combined rule-based and statistical tagger. In Gisle
Andersen, editor, Exploring Newspaper Language:
Using the web to create and investigate a large cor-
pus of modern Norwegian. John Benjamins, Amster-
dam, The Netherlands.

David Jurgens and Mohammad Taher Pilehvar. 2015.
Reserating the awesometastic: An automatic exten-
sion of the wordnet taxonomy for novel terms. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics Human Language Technologies
(NAACL HLT 2015).

Emanuele Lapponi, Erik Velldal, Nikolay Aleksandrov
Vazov, and Stephan Oepen. 2013. HPC-ready lan-
guage analysis for human beings. In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013).

Tomas Mikolov, Kai Chen, Gregory S. Corrado, and
Jeffrey Dean. 2013a. Efficient estimation of word
representations in vector space. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. Advances in Neural Information Pro-
cessing Systems.

Bolette Sandford Pedersen, Sanni Nimb, Jørg As-
mussen, Nicolai Hartvig Srensen, Lars Trap-Jensen,
and Henrik Lorentzen. 2009. DanNet: the chal-
lenge of compiling a WordNet for Danish by reusing
a monolingual dictionary. Language Resources and
Evaluation, 43:269–299.

Radim Řehůřek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics.

Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer, Dordrecht, The Netherlands.

Ichiro Yamada, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym dis-
covery based on distributional similarity and hierar-
chical structures. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.

302


