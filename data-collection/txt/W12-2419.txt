










































Evaluating Joint Modeling of Yeast Biology Literature and Protein-Protein Interaction Networks


Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 155–162,
Montréal, Canada, June 8, 2012. c©2012 Association for Computational Linguistics

Evaluating Joint Modeling of Yeast Biology Literature and Protein-Protein
Interaction Networks

Ramnath Balasubramanyan and Kathryn Rivard and William W. Cohen
School of Computer Science
Carnegie Mellon University

rbalasub,krivard,wcohen@cs.cmu.edu

Jelena Jakovljevic and John Woolford
Deparment of Biological Sciences

Carnegie Mellon University
jelena,jw17@andrew.cmu.edu

Abstract

Block-LDA is a topic modeling approach to
perform data fusion between entity-annotated
text documents and graphs with entity-entity
links. We evaluate Block-LDA in the yeast bi-
ology domain by jointly modeling PubMed R©

articles and yeast protein-protein interaction
networks. The topic coherence of the emer-
gent topics and the ability of the model to re-
trieve relevant scientific articles and proteins
related to the topic are compared to that of a
text-only approach that does not make use of
the protein-protein interaction matrix. Eval-
uation of the results by biologists show that
the joint modeling results in better topic co-
herence and improves retrieval performance in
the task of identifying top related papers and
proteins.

1 Introduction

The prodigious rate at which scientific literature
is produced makes it virtually impossible for re-
searchers to manually read every article to identify
interesting and relevant papers. It is therefore crit-
ical to have automatic methods to analyze the liter-
ature to identify topical structure in it. The latent
structure that is identified can be used for different
applications such as enabling browsing, retrieval of
papers related to a particular sub-topic etc. Such ap-
plications assist in common scenarios such as help-
ing a researcher identify a set of articles to read (per-
haps a set of well-regarded surveys) to familiarize
herself with a new sub-field; helping a researcher to
stay abreast with the latest advances in his field by
identifying relevant articles etc.

In this paper, we focus on the task of organiz-
ing a large collection of literature about yeast biol-
ogy to enable topic oriented browsing and retrieval
from the literature. The analysis is performed using
topic modeling(Blei et al., 2003) which has, in the
last decade, emerged as a versatile tool to uncover
latent structure in document corpora by identifying
broad topics that are discussed in it. This approach
complements traditional information retrieval tasks
where the objective is to fulfill very specific infor-
mation needs.

In addition to literature, there often exist other
sources of domain information related to it. In the
case of yeast biology, an example of such a resource
is a database of known protein-protein interactions
(PPI) which have been identified using wetlab exper-
iments. We perform data fusion by combining text
information from articles and the database of yeast
protein-protein interactions, by using a latent vari-
able model — Block-LDA (Balasubramanyan and
Cohen, 2011) that jointly models the literature and
PPI networks.

We evaluate the ability of the topic models to re-
turn meaningful topics by inspecting the top papers
and proteins that pertain to them. We compare the
performance of the joint model i.e. Block-LDA with
a model that only considers the text corpora by ask-
ing a yeast biologist to evaluate the coherence of
topics and the relevance of the retrieved articles and
proteins. This evaluation serves to test the utility of
Block-LDA on a real task as opposed to an internal
evaluation (such as by using perplexity metrics for
example). Our evaluaton shows that the joint model
outperforms the text-only approach both in topic co-

155



herence and in top paper and protein retrieval as
measured by precision@10 values.

The rest of the paper is organized as follows. Sec-
tion 2 describes the topic modeling approach used
in the paper. Section 3 describes the datasets used
followed by Section 4 which details the setup of the
experiments. The results of the evaluation are pre-
sented in Section 5 which is followed by the conclu-
sion.

2 Block-LDA

The Block-LDA model (plate diagram in Figure 1)
enables sharing of information between the compo-
nent on the left that models links between pairs of
entities represented as edges in a graph with latent
block structure, and the component on the right that
models text documents, through shared latent topics.
More specifically, the distribution over the entities of
the type that are linked is shared between the block
model and the text model.

The component on the right, which is an extension
of the LDA models documents as sets of “bags of en-
tities”, each bag corresponding to a particular type
of entity. Every entity type has a topic wise multi-
nomial distribution over the set of entities that can
occur as an instance of the entity type. This model
is termed as Link-LDA(Nallapati et al., 2008) in the
literature.

The component on the left in the figure is a gen-
erative model for graphs representing entity-entity
links with an underlying block structure, derived
from the sparse block model introduced by Parkki-
nen et al. (2009). Linked entities are generated from
topic specific entity distributions conditioned on the
topic pairs sampled for the edges. Topic pairs for
edges (links) are drawn from a multinomial defined
over the Cartesian product of the topic set with it-
self. Vertices in the graph representing entities there-
fore have mixed memberships in topics. In con-
trast to Mixed-membership Stochastic Blockmodel
(MMSB) introduced by Airoldi et al. (2008), only
observed links are sampled, making this model suit-
able for sparse graphs.

LetK be the number of latent topics (clusters) we
wish to recover. Assuming documents consist of T
different types of entities (i.e. each document con-
tains T bags of entities), and that links in the graph

are between entities of type tl, the generative process
is as follows.
1. Generate topics: For each type t ∈ 1, . . . , T , and
topic z ∈ 1, . . . ,K, sample βt,z ∼ Dirichlet(γ), the
topic specific entity distribution.
2. Generate documents. For every document d ∈
{1 . . . D}:

• Sample θd ∼ Dirichlet(αD) where θd is the
topic mixing distribution for the document.

• For each type t and its associated set of entity
mentions et,i, i ∈ {1, · · · , Nd,t}:

– Sample a topic zt,i ∼Multinomial(θd)
– Sample an entity et,i ∼

Multinomial(βt,zt,i)

3. Generate the link matrix of entities of type tl:

• Sample πL ∼ Dirichlet(αL) where πL de-
scribes a distribution over the Cartesian prod-
uct of the set of topics with itself, for links in
the dataset.

• For every link ei1 → ei2, i ∈ {1 · · ·NL}:

– Sample a topic pair 〈zi1, zi2〉 ∼
Multinomial(πL)

– Sample ei1 ∼Multinomial(βtl,zi1)
– Sample ei2 ∼Multinomial(βtl,zi2)

Note that unlike the MMSB model, this model
generates only realized links between entities.

Given the hyperparameters αD, αL and γ, the
joint distribution over the documents, links, their
topic distributions and topic assignments is given by

p(πL,θ,β, z, e, 〈z1, z2〉, 〈e1, e2〉|αD, αL, γ) ∝

(1)
K∏
z=1

T∏
t=1

Dir(βt,z|γt)×

D∏
d=1

Dir(θd|αD)
T∏
t=1

Nd,t∏
i=1

θ
z
(d)
t,i

d β
et,i

t,z
(d)
t,i

×

Dir(πL|αL)
NL∏
i=1

π
〈zi1,zi2〉
L β

ei1
tl,z1

βei2tl,z2

156



...

θd

αL
αD

πL

NL

γ

Dim: K x K
Dim: K

zi1 zi2

ei2ei1

Links

Docs

βt,z
T

K

D

z1,i

e1,i

zT,i

eT,i

Nd,TNd,1

αL - Dirichlet prior for the topic pair distribution for links
αD - Dirichlet prior for document specific topic distributions
γ - Dirichlet prior for topic multinomials
πL - multinomial distribution over topic pairs for links
θd - multinomial distribution over topics for document d
βt,z - multinomial over entities of type t for topic z
zt,i - topic chosen for the i-th entity of type t in a document
et,i - the i-th entity of type t occurring in a document
zi1 and zi2 - topics chosen for the two nodes participating in the i-th link
ei1 and ei2 - the two nodes participating in the i-th link

Figure 1: Block-LDA

A commonly required operation when using mod-
els like Block-LDA is to perform inference on the
model to query the topic distributions and the topic
assignments of documents and links. Due to the
intractability of exact inference in the Block-LDA
model, a collapsed Gibbs sampler is used to perform
approximate inference. It samples a latent topic for
an entity mention of type t in the text corpus con-
ditioned on the assignments to all other entity men-
tions using the following expression (after collaps-
ing θD):

p(zt,i = z|et,i, z¬i, e¬i, αD, γ) (2)

∝ (n¬idz + αD)
n¬iztet,i + γ∑
e′ n
¬i
zte′

+ |Et|γ

Similarly, we sample a topic pair for every link con-
ditional on topic pair assignments to all other links

after collapsing πL using the expression:

p(zi = 〈z1, z2〉|〈ei1, ei2〉, z¬i, 〈e1, e2〉¬i, αL, γ)(3)

∝
(
nL¬i〈z1,z2〉 + αL

)
×(

n¬iz1tlei1
+γ
)(
n¬iz2tlei2

+γ
)

(∑
e n

¬i
z1tle

+|Etl |γ
)(∑

e n
¬i
z2tle

+|Etl |γ
)

Et refers to the set of all entities of type t. The n’s
are counts of observations in the training set.

• nzte - the number of times an entity e of type t
is observed under topic z

• nzd - the number of entities (of any type) with
topic z in document d

• nL〈z1,z2〉 - count of links assigned to topic pair
〈z1, z2〉

The topic multinomial parameters and the topic
distributions of links and documents are easily re-
covered using their MAP estimates after inference

157



using the counts of observations.

β
(e)
t,z =

nzte + γ∑
e′ nzte′ + |Et|γ

, (4)

θ
(z)
d =

ndz + αD∑
z′ ndz′ +KαD

and (5)

π
〈z1,z2〉
L =

n〈z1,z2〉 + αL∑
z′1,z

′
2
n〈z′1,z′2〉 +K

2αL
(6)

A de-noised form of the entity-entity link matrix
can also be recovered from the estimated parame-
ters of the model. Let B be a matrix of dimensions
K × |Etl | where row k = βtl,k, k ∈ {1, · · · ,K}.
Let Z be a matrix of dimensions K ×K s.t Zp,q =∑NL

i=1 I(zi1 = p, zi2 = q). The de-noised matrix M
of the strength of association between the entities in
Etl is given by M = B

TZB.
In the context of this paper, de-noising the

protein-protein interaction networks studied is an
important application. The joint model permits in-
formation from the large text corpus of yeast publi-
cations to be used to de-noise the PPI network and
to identify potential interactions that are missing in
the observed network. While this task is important
and interesting, it is outside the scope of this paper
and is a direction for future work.

3 Data

We use a collection of publications about yeast bi-
ology that is derived from the repository of sci-
entific publications at PubMed R©. PubMed R© is a
free, open-access on-line archive of over 18 mil-
lion biological abstracts and bibliographies, includ-
ing citation lists, for papers published since 1948.
The subset we work with consists of approximately
40,000 publications about the yeast organism that
have been curated in the Saccharomyces Genome
Database (SGD) (Dwight et al., 2004) with anno-
tations of proteins that are discussed in the publi-
cation. We further restrict the dataset to only those
documents that are annotated with at least one pro-
tein from the protein-protein interactions databases
described below. This results in a protein annotated
document collection of 15,776 publications. The
publications in this set were written by a total of
47,215 authors. We tokenize the titles and abstracts
based on white space, lowercase all tokens and elim-
inate stopwords. Low frequency (< 5 occurrences)

terms are also eliminated. The vocabulary that is ob-
tained consists of 45,648 words.

The Munich Institute for Protein Sequencing
(MIPS) database (Mewes et al., 2004) includes a
hand-crafted collection of protein interactions cover-
ing 8000 protein complex associations in yeast. We
use a subset of this collection containing 844 pro-
teins, for which all interactions were hand-curated.

Finally, we use another dataset of protein-protein
interactions in yeast that were observed as a result of
wetlab experiments by collaborators of the authors
of the paper. This dataset consists of 635 interac-
tions that deal primarily with ribosomal proteins and
assembly factors in yeast.

4 Setup

We conduct three different evaluations of the emer-
gent topics. Firstly, we obtain topics from only
the text corpus using a model that comprises of the
right half of Figure 1 which is equivalent to using
the Link-LDA model. For the second evaluation,
we use the Block-LDA model that is trained on the
text corpus and the MIPS protein-protein interac-
tion database. Finally, for the third evaluation, we
replace the MIPS database with the interaction ob-
tained from the wetlab experiments. In all the cases,
we set K, the number of topics to be 15. In each
variant, we represent documents as 3 sets of entities
i.e. the words in the abstracts of the article, the set
of proteins associated with the article as indicated in
the SGD database and finally the authors who wrote
the article. Each topic therefore consists of 3 differ-
ent multinomial distributions over the sets of the 3
kinds of entities described.

Topics that emerge from the different variants can
possibly be assigned different indices even when
they discuss the same semantic concept. To com-
pare topics across variants, we need a method to
determine which topic indices from the different
variants correspond to the same semantic concept.
To obtain the mapping between topics from each
variant, we utilize the Hungarian algorithm (Kuhn,
1955) to solve the assignment problem where the
cost of aligning topics together is determined using
the Jensen-Shannon divergence measure.

Once the topics are obtained, we firstly obtain the
proteins associated with the topic by retrieving the

158



Figure 2: Screenshot of the Article Relevance Annotation Tool

Variant Num. Coherent Topics
Only Text 12 / 15
Text + MIPS 13 / 15
Text + Wetlab 15 / 15

Table 1: Topic Coherence Evaluation

top proteins from the multinomial distribution cor-
responding to proteins. Then, the top articles cor-
responding to each topic is obtained using a ranked
list of documents with the highest mass of their topic
proportion distributions (θ) residing in the topic be-
ing considered.

4.1 Manual Evaluation

To evaluate the topics, a yeast biologist who is an
expert in the field was asked to mark each topic with

a binary flag indicating if the top words of the dis-
tribution represented a coherent sub-topic in yeast
biology. This process was repeated for the 3 differ-
ent variants of the model. The variant used to obtain
results is concealed from the evaluator to remove the
possibility of bias. In the next step of the evaluation,
the top articles and proteins assigned to each topic
were presented in a ranked list and a similar judge-
ment was requested to indicate if the article/protein
was relevant to the topic in question. Similar to
the topic coherence judgements, the process was re-
peated for each variant of the model. Screenshots
of the tool used for obtaining the judgments can be
seen in Figure 2. It should be noted that since the
nature of the topics in the literature considered was
highly technical and specialized, it was impractical
to get judgements from multiple annotators.

159



Topic

P
re

ci
si

on
 @

 1
0

0.2

0.4

0.6

0.8

1.0 ● ● ● ●

●

● ● ● ● ● ● ●

Variant

● With MIPS interactions

Only Text

With Wetlab interactions

(a) Article Retrieval

Topic

P
re

ci
si

on
 @

 1
0

0.2

0.4

0.6

0.8

1.0 ●

●

●

● ● ● ● ● ●

Variant

● With MIPS interactions

Only Text

With Wetlab interactions

(b) Protein Retrieval

Figure 3: Retrieval Performance Evaluation (Horizontal lines indicate mean across all topics)

To evaluate the retrieval of the top articles and
proteins, we measure the quality of the results by
computing its precision@10 score.

5 Results

First we evaluate the coherence of the topics ob-
tained from the 3 variants described above. Table
1 shows that out of the 15 topics that were obtained,
12 topics were deemed coherent from the text-only
model and 13 and 15 topics were deemed coherent
from the Block-LDA models using the MIPS and
wetlab PPI datasets respectively.

Next, we study the precision@10 values for each
topic and variant for the article retrieval and protein
retrieval tasks, which is shown in Figure 3. The plots

also show horizontal lines representing the mean of
the precision@10 across all topics. It can be seen
from the plots that for both the article and protein
retrieval tasks, the joint models work better than the
text-only model on average. For the article retrieval
task, the model trained with the text + MIPS resulted
in the higher mean precision@10 whereas for the
protein retrieval task, the text + Wetlab PPI dataset
returned a higher mean precision@10 value. For
both the protein retrieval and paper retrieval tasks,
the improvements shown by the joint models using
either of the PPI datasets over the text-only model
(i.e. the Link LDA model) were statistically sig-
nificant at the 0.05 level using the paired Wilcoxon
sign test. The difference in performance between the

160



Topic: Protein Structure & Interactions
Top articles using Publications Only Top articles using Block-LDA with Wetlab PPI
* X-ray fiber diffraction of amyloid fibrils. * X-ray fiber diffraction of amyloid fibrils.
* Molecular surface area and hydrophobic effect. * Scalar couplings across hydrogen bonds.
* Counterdiffusion methods for macromolecular
crystallization.

* Dipolar couplings in macromolecular structure
determination.

* Navigating the ClpB channel to solution. * Structure of alpha-keratin.
* Two Rippled-Sheet Configurations of Polypep-
tide Chains, and a Note about the Pleated Sheets.

* Stable configurations of polypeptide chains.

* Molecular chaperones. Unfolding protein fold-
ing.

* The glucamylase and debrancher of S. diastati-
cus.

* The molten globule state as a clue for under-
standing the folding and cooperativity of globular-
protein structure.

* A study of 150 cases of pneumonia.

* Unfolding and hydrogen exchange of proteins:
the three-dimensional ising lattice as a model.

* Glycobiology.

* Packing of alpha-helices: geometrical con-
straints and contact areas.

* The conformation of thermolysin.

Topic: DNA Repair
Top articles using Publications Only Top articles using Block-LDA with Wetlab PPI
* Passing the baton in base excision repair. * Telomeres and telomerase.
* The bypass of DNA lesions by DNA and RNA
polymerases.

* Enzymatic photoreactivation: overview.

* The glucamylase and debrancher of S. diastati-
cus.

* High-efficiency transformation of plasmid DNA
into yeast.

* DNA replication fidelity. * The effect of ultraviolet light on recombination
in yeast.

* Base excision repair. * T-loops and the origin of telomeres.
* Nucleotide excision repair. * Directed mutation: between unicorns and goats.
* The replication of DNA in Escherichia Coli. * Functions of DNA polymerases.
* DNA topoisomerases: why so many? * Immortal strands? Give me a break.

Table 2: Sample of Improvements in Article Retrieval

two joint models that used the two different PPI net-
works were however insignificant which indicates
that there is no observable advantage in using one
PPI dataset over the other in conjunction with the
text corpus.

Table 2 shows examples of poor results of article
retrieval obtained using the publications-only model
and the improved set of results obtained using the
joint model.

5.1 Topics
Table 3 shows 3 sample topics that were retrieved
from each variant described earlier. The table shows
the top words and proteins associated with the top-

ics. The topic label on the left column was assigned
manually during the evaluation by the expert anno-
tator.

Conclusion

We evaluated topics obtained from the joint mod-
eling of yeast biology literature and protein-protein
interactions in yeast and compared them to top-
ics that were obtained from using only the litera-
ture. The topics were evaluated for coherence and
by measuring the mean precision@10 score of the
top articles and proteins that were retrieved for each
topic. Evaluation by a domain expert showed that

161



Topic Top Words & Proteins
Protein Structure & Inter-
actions

Words: protein structure binding residues domain structural beta complex
atp proteins alpha interactions folding structures form terminal peptide helix
model interaction bound domains molecular changes conformational

(Publications Only) Proteins: CYC1 SSA1 HSP82 SUP35 HSP104 HSC82 SSA2 YDJ1 URE2
KAR2 SSB1 SSA4 GCN4 SSA3 SSB2 PGK1 PDI1 SSC1 HSP60 STI1
SIS1 RNQ1 SEC61 SSE1 CCP1

DNA Repair Words:dna recombination repair replication strand single double cells mu-
tations stranded induced base uv mutants mutation homologous virus telom-
ere human type yeast activity telomerase mutant dna polymerase

(Using MIPS PPI) Proteins: RAD52 RAD51 RAD50 MRE11 RAD1 RAD54 SGS1 MSH2
RAD6 YKU70 REV3 POL30 RAD3 XRS2 RAD18 RAD2 POL3 RAD27
YKU80 RAD9 RFA1 TLC1 TEL1 EST2 HO

Vesicular Transport Words:membrane protein transport proteins atp golgi er atpase membranes
plasma membrane vesicles cells endoplasmic reticulum complex fusion
ca2 dependent translocation vacuolar intracellular yeast lipid channel hsp90
vesicle

(Using Wetlab PPI) Proteins: SSA1 HSP82 KAR2 PMA1 HSC82 SEC18 SSA2 YDJ1 SEC61
PEP4 HSP104 SEC23 VAM3 IRE1 SEC4 SSA4 SEC1 PMR1 PEP12
VMA3 VPH1 SSB1 VMA1 SAR1 HAC1

Table 3: Sample Topics

the joint modeling produced more coherent topics
and showed better precision@10 scores in the article
and protein retrieval tasks indicating that the model
enabled information sharing between the literature
and the PPI networks.

References
Edoardo M. Airoldi, David Blei, Stephen E. Fienberg,

and Eric P. Xing. 2008. Mixed membership stochastic
blockmodels. Journal of Machine Learning Research,
9:1981–2014, September.

Ramnath Balasubramanyan and William W. Cohen.
2011. Block-LDA: Jointly modeling entity-annotated
text and entity-entity links. In SDM, pages 450–461.
SIAM / Omnipress.

David. M Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of Ma-
chine Learning Research, 3:993–1022.

Selina S. Dwight, Rama Balakrishnan, Karen R.
Christie, Maria C. Costanzo, Kara Dolinski, Sta-
cia R. Engel, Becket Feierbach, Dianna G. Fisk,
Jodi Hirschman, Eurie L. Hong, Laurie Issel-Tarver,
Robert S. Nash, Anand Sethuraman, Barry Starr,
Chandra L. Theesfeld, Rey Andrada, Gail Binkley,
Qing Dong, Christopher Lane, Mark Schroeder, Shuai
Weng, David Botstein, and Michael Cherry J. 2004.

Saccharomyces genome database: Underlying prin-
ciples and organisation. Briefings in bioinformatics,
5(1):9.

Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2(1-2):83–97.

Hans-Werner Mewes, C. Amid, Roland Arnold, Dmitrij
Frishman, Ulrich Gldener, Gertrud Mannhaupt, Martin
Mnsterktter, Philipp Pagel, Normann Strack, Volker
Stmpflen, Jens Warfsmann, and Andreas Ruepp. 2004.
MIPS: Analysis and annotation of proteins from whole
genomes. Nucleic Acids Res, 32:41–44.

Ramesh M. Nallapati, Amr Ahmed, Eric P. Xing, and
William W. Cohen. 2008. Joint latent topic models
for text and citations. In Proceeding of the 14th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 542–550, Las Vegas,
Nevada, USA. ACM.

Juuso Parkkinen, Janne Sinkkonen, Adam Gyenge, and
Samuel Kaski. 2009. A block model suitable for
sparse graphs. In Proceedings of the 7th International
Workshop on Mining and Learning with Graphs (MLG
2009), Leuven. Poster.

162


