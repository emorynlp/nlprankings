








































Idiom Paraphrases: Seventh Heaven vs Cloud Nine

Maria Pershina Yifan He Ralph Grishman
Computer Science Department

New York University
New York, NY 10003, USA

{pershina,yhe,grishman}@cs.nyu.edu

Abstract

The goal of paraphrase identification is
to decide whether two given text frag-
ments have the same meaning. Of par-
ticular interest in this area is the identifi-
cation of paraphrases among short texts,
such as SMS and Twitter. In this paper,
we present idiomatic expressions as a new
domain for short-text paraphrase identifi-
cation. We propose a technique, utiliz-
ing idiom definitions and continuous space
word representations that performs com-
petitively on a dataset of 1.4K annotated
idiom paraphrase pairs, which we make
publicly available for the research commu-
nity.

1 Introduction

The task of paraphrase identification, i.e. finding
alternative linguistic expressions of the same or
similar meaning, attracted a great deal of attention
in the research community in recent years (Ban-
nard and Callison-Burch, 2005; Sekine, 2005;
Socher et al., 2011; Guo et al., 2013; Xu et al.,
2013; Wang et al., 2013; Zhang and Weld, 2013;
Xu et al., 2015).

This task was extensively studied in Twitter
data, where millions of user-generated tweets talk
about the same topics and thus present a nat-
ural challenge to resolve redundancy in tweets
for many applications, such as textual entailment
(Zhao et al., 2014), text summarization (Lloret et
al., 2008), first story detection (Petrovich, 2012),
search (Zanzotto et al., 2011), question answering
(Celikyilmaz, 2010), etc.

In this paper we explore a new domain for
the task of paraphrase identification - idiomatic
expressions, in which the goal is to determine
whether two idioms convey the same idea.

This task is related to previous short-text para-
phrase tasks, but it does not have access to many

of the information sources that can be exploited in
Twitter/short text paraphrasing: unlike tweets, id-
ioms do not have hashtags, which are very strong
topic indictors; unlike SMS, idioms do not have
timestamp or geographical metadata; and unlike
news headlines, there are no real world events that
can serve as anchors to cluster similar expressions.
In addition, an idea, or a moral of the idiom is of-
ten expressed in an indirect way, e.g. the idioms

(1) make a mountain out of a molehill
(2) tempest in a teapot

convey similar ideas1:

(1) If somebody makes a mountain out of a
molehill they exaggerate the importance or

seriousness of a problem.
(2) If people exaggerate the seriousness of a

situation or problem they are making a tempest in
a teapot.

There is a line of research focused on extract-
ing idioms from the text or identifying whether
a particular expression is idiomatic (or a non-
compositional multi-word expression) (Muzny
and Zettlemoyer, 2013; Shutova et al., 2010; Li
and Sporleder, 2009; Gedigian et al., 2006; Katz
and Giesbrecht, 2006). Without linguistic sources
such as Wiktionary, usingenglish.com, etc, it is
often hard to understand what the meaning of a
particular idiom is. It is even harder to deter-
mine whether two idioms convey the same idea
or find alternative idiomatic expressions. Using
idiom definitions, given by linguistic resources,
one can view this problem as identifying para-
phrases between definitions and thus deciding on
paraphrases between corresponding idioms. Effi-
cient techniques for identifying idiom paraphrases
would complement any paraphrase identification
system, and thus improve the downstream appli-
cations, such as question answering, summariza-

1Definitions of these idioms are taken from
http://www.usingenglish.com



tion, opinion mining, information extraction, and
machine translation.

To the best of our knowledge we are the first to
address the problem of determining whether two
idioms convey the same idea, and to propose a new
scheme that utilizes idiom definitions and continu-
ous space word representation (word embedding)
to solve it. By linking word- and sentence-level
semantics our technique outperforms state-of-the-
art paraphrasing approaches on a dataset of 1.4K
annotated idiom pairs that we make publicly avail-
able.

2 Related Work

There is no strict definition of a paraphrase (Bha-
gat and Hovy, 2013) and in linguistic literature
paraphrases are most often characterized by an
approximate equivalence of meanings across sen-
tences or phrases.

A growing body of research investigates ways
of paraphrase detection in both supervised (Qiu
et al., 2006; Wan et al., 2006; Das and Smith,
2009; Socher et al., 2011; Blacoe and Lapata,
2012; Madnani and Tetreault, 2012; Ji and Eisen-
stein, 2013) and unsupervised settings (Bannard
and Callison-Burch, 2005; Mihalcea et al., 2006;
Rus et al., 2008; Fernando and Stevenson, 2008;
Islam and Inkpen, 2007; Hassan and Mihalcea,
2011). These methods mainly work on large scale
news data. News data is very different from ours
in two aspects: most news text can be interpreted
literally and similar news events (passing a legis-
lation, death of a person, elections) happen repeat-
edly. Therefore, lexical anchors or event anchors
can work well on news text, but not necessarily on
our task.

Millions of tweets generated by Twitter users
every day provide plenty of paraphrase data for
NLP research. An increasing interest in this prob-
lem led to the Paraphrase and Semantic Similar-
ity In Twitter (PIT) task in SemEval-2015 com-
petition (Xu et al., 2015). Existing bias towards
Twitter paraphrases results in sophisticated sys-
tems that exploit character level similarity or meta-
data. But models relying on these insights are
not necessarily applicable to other domains where
misspellings are rare, or metadata is not available.

Idiomatic expressions constitute an essential
part of modern English. They often behave id-
iosyncratically and are therefore a significant chal-
lenge for natural language processing systems.

Recognizing when two idiomatic expressions con-
vey similar ideas is crucial to recognizing the sen-
timent of the author, identifying correct triggers
for events, and to translating the idiom properly.
However, although there are several existing mod-
els to identify paraphrases in short text, idioms
have very different characteristics from the data
that those models are built on. In this paper, we
experiment with two state-of-the-art paraphrasing
models that are outperformed on our dataset of id-
iomatic expressions by a simple technique, raising
a question on how well existing paraphrase models
generalize to new data.

3 The Challenge

Identifying idiom paraphrases is an interesting and
challenging problem. Lexical similarity is not a
reliable clue to find similar idioms. Some idioms
look very similar, differ in only one or two words,
and convey the same idea. For example, “like
two peas in a pod” vs “like peas in a pod” (“if
people or things are like peas in a pod they look
identical”), but other idioms that look similar can
have very different meaning, e.g. “well oiled” vs
“well oiled machine” (“if someone is well oiled
they have drunk a lot” vs “something that func-
tions very well is a well oiled machine”).

Finally, there are idioms that do not have any
words in common at all and may seem quite differ-
ent for a person not familiar with idiomatic expres-
sions, but still have similar meaning. For exam-
ple, “cross swords” vs “lock horns” (“when peo-
ple cross swords they argue or dispute” vs “when
people lock horns they argue or fight about some-
thing”). Thus, a natural way to identify idiom
paraphrases is to focus on idiom definitions that
explain meaning of an idiom in a clear and con-
cise way.

4 Lexical vs Semantic Similarities

Our dataset consists of pairs 〈idiom, definition〉.
We use two types of similarity measures to com-

pute how similar definitions of different idioms
are: the lexical similarity is based on a lexical
(word) overlap between two definitions, and the
semantic similarity captures the overall semantic
meaning of the whole sentence.
Lexical similarity. We compute cosine similarity
between vectors −→v d1 and

−→v d2 , representing id-
iom descriptions d1 and d2 and weight each word



in these vectors by its tf-idf score:

lexSim(d1, d2) = cosine(
−→v d1 ,

−→v d2), (1)

where −→v d is a |V |-dimensional vector with V
being the vocabulary of all definition words.

Semantic similarity. To capture the overall mean-
ing of the definitions d we combine word embed-
dings (Collobert et al., 2011; Turian et al., 2010)
for all words in d using two combination schemes:

• Averaged sum:
−−−−−−→
averagedd =

1

|d|
∑

word∈d

−−→
emb(word) (2)

• Weighted sum:
−−−−−−→
weightedd = (3)

1∑
word∈d

tfidfword

∑
word∈d

tfidfword ·
−−→
emb(word)

Then semantic similarity is measured as

semSim(d1,d2)= cosine(combd1 ,combd2) (4)

where
−−−→
combd is a 100-dimensional vector com-

bined from word embeddings
−−→
emb(word) (Turian

et al., 2010) for words in description d using ei-
ther averaged (2) or weighted (3) combination
schemes. 2

4.1 IdiomSim
There is a tradeoff between the two similarity mea-
sures lexSim and semSim (Section 4): while the
first one captures the actual lexical overlap, the
second one can better capture the closeness in se-
mantic meaning. To find an optimal balance be-
tween the two we consider their weighted sum

IdiomSim(d1, d2) = (5)

(1− α) · lexSim(d1, d2) + α · semSim(d1, d2)
and decide on an α by optimizing for a maximal
F-score on a development dataset.

5 Experiments

Data. We collected 2,432 idioms from
http://www.usingenglish.com, a site for English
learners, where every idiom has a unique de-
scription giving a clear explanation of the idiom’s
meaning. As opposed to tweets there are no hash-
tags, no topics or trends, no timestamps, or any
other default evidence, that two idioms may con-
vey similar ideas. Thus it becomes a challenging

Recall
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

P
re
ci
si
on

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

IdiomSim
LexicalSim
WTMF
CosSim

IdiomSim
LexicalSim
WTMF
CosSim

Model F1 P R

CosSim 53.7 37.1 97.2
ASOBEK 55.1 59.4 51.4
WTMF 61.4 51.4 76.3
LexicalSim 63.7 60.8 67.0
IdiomSimave 64.4 56.2 75.5
IdiomSim 65.9 59.2 74.4
IdiomSim+ 66.6 62.2 71.9

Figure 1: Comparison of IdiomSim with baselines
CosSim, LexicalSim, and state-of-the-art para-
phrasing models: ASOBEK, WTMF.

task itself to construct a dataset of pairs that is
guaranteed to have a certain fraction of true para-
phrases.

We used a simple cosine similarity between all
possible idiom definitions pairs to have a ranked
list and labeled the top 1.5K pairs. Three annota-
tors were asked to label each pair of idiom defi-
nitions as “similar” (score 2), “have something in
common” (score 1), “not similar” (score 0). 0.1K
pairs received a total score of 4 (either 2+2+0, or
2+1+1), and were further removed as debatable.
The rest of the labeled pairs were randomly split
into 1K for test data and 0.4K for development.
Only pairs that received a total score of 5 or higher
were considered as positive examples. There are
364 and 96 true paraphrases in our test and devel-
opment sets respectively. 3

Baselines. Our baselines are simple and tf-idf
weighted cosine similarity between idiom descrip-
tion sentences: CosSim and LexicalSim.

We compare our method with the deterministic
state-of-the-art ASOBEK model (Eyecioglu and

2We use 100-dimensional Turian word embeddings avail-
able at http://metaoptimize.com/projects/
wordreprs/

3https://github.com/masha-p/Idiom_
Paraphrases



Idioms Descriptions
seventh heaven if you are in seventh heaven you are extremely happy
cloud nine if you are on cloud nine you are extremely happy
face only a mother could love when someone has a face only a mother could love they are ugly
stop a clock a face that could stop a clock is very ugly indeed
take your medicine if you take your medicine you accept the consequences of something

you have done wrong
face the music if you have to face the music you have to accept

the negative consequences of something you have done wrong
well oiled if someone is well oiled they have drunk a lot
drunk as a lord someone who is very drunk is as drunk as a lord
cheap as chips if something is very inexpensive it is as cheap as chips
to be dog cheap if something is dog cheap it is very cheap indeed
great minds think alike if two people have the same thought at the same time
on the same wavelength if people are on the same wavelength they have the same ideas

and opinions about something
could eat a horse if you are very hungry you could eat a horse
hungry as a bear if you are hungry as a bear it means that you are really hungry
cross swords when people cross swords they argue or dispute
lock horns when people lock horns they argue or fight about something
talk the hind legs off a donkey a person who is excessively or extremely talkative

can talk the hind legs off a donkey
talk the legs off an iron pot somebody who is excessively talkative or is especially convincing

is said to talk the legs off an iron pot

Table 1: Examples of extracted idiom paraphrases.

Keller, 2015) that was ranked first among 19 teams
in the Paraphrase in Twitter (PIT) track on the Se-
mEval 2015 shared task (Xu et al., 2015). This
model extracts eight simple and elegant character
and word features from two sentences to train an
SVM with linear kernel. It achieves an F-score of
55.1% on our test set.4

We also compare our method with the state-
of-the-art Weighted Textual Matrix Factorization
model (WTMF) (Guo et al., 2013),5 which is
specifically developed for short sentences by mod-
eling the semantic space of words, that can be ei-
ther present or absent from the sentences (Guo and
Diab, 2012). This model achieves a maximal F-
score of 61.4% on the test set.

The state-of-the-art model for lexically diver-
gent paraphrases on Twitter (Xu et al., 2015) is
tailored for tweets and requires topic and anchor
words to be present in the sentence, which is not
applicable to idiom definitions.
Evaluation and Results. To evaluate models we

4We thank Asli Eyecioglu for running her ASOBEK
model on our test data.

5The source code for WTMF is available at http://
www.cs.columbia.edu/˜weiwei/code

plot precision-recall curves for CosSim, WTMF,
LexicalSim, and IdiomSim (for clarity we omit
curves for other models). We also compare maxi-
mal F-score for all models. We observe that simple
cosine similarity (CosSim) achieves a maximal F-
score of 53.7%, LexicalSim is a high baseline and
achieves an F-score of 63.75%. When we add av-
eraged word embeddings the maximal F-score is
64.4% (IdiomSimave). With tfidf weighted word
embeddings we achieve F-score of 65.9% (Idiom-
Sim). By filtering out uninformative words such as
“a”, “the”, etc (12 words total) we improve the F-
score to 66.6% (IdiomSim+), outperforming state-
of-the-art paraphrase models by more than 5% ab-
solute (Figure 1). Both IdiomSim and IdiomSim+
outperform WTMF significantly according to a
paired t-test with p less than 0.05.
Examples and Discussion. We use threshold,
corresponding to a maximal F-score obtained on
the development dataset, and explore paraphrases
from test dataset scored higher and lower than
this threshold. Examples of extracted idiom para-
phrases are in Table 1. Examples of false positives
and false negatives are in Table 2.

Simple word overlap is not a reliable clue to de-



Idioms Descriptions
False positives
healthy as a horse if you are as healthy as a horse you are very healthy
an apple a day keeps eating healthy food keeps you healthy
the doctor away
jersey justice jersey justice is a very severe justice
justice is blind justice is blind means that justice is impartial and objective
heart of steel when someone has a heart of steel they do not show emotion

or are not affected emotionally
heart of glass when someone has a heart of glass they are easily affected emotionally
False negatives
like a kid in a candy store if someone is like a kid in a candy store

they are very excited about something
bee in your bonnet if someone is very excited about something

they have a bee in their bonnet
easy as falling off a log something very easy or simple to do is as easy as falling off a log
no sweat no sweat means something is easy
hopping mad if you are hopping mad you are extremely angry
off on one if someone goes off on one they get extremely angry indeed

Table 2: Examples of false positive and false negative paraphrases.

cide on a paraphrase between two idiom descrip-
tions. Since words are main units in the computa-
tion (5) our metric is biased towards lexical simi-
larity. Thus we get a false positive paraphrase be-
tween “healthy as a horse” and “an apple a day”.
The first one is rather a statement about someone’s
health while the second one is an advice on how
to be healthy. Moreover, idioms “heart of steel”
vs “heart of glass” convey opposite ideas of be-
ing “not affected emotionally” vs being “easily af-
fected emotionally”. Having “heart” and “affected
emotionally” in both idiom descriptions leads to a
high cosine similarity between them and results in
a false positive decision. For the same reason lexi-
cally divergent idiom descriptions get a lower rank
while convey similar ideas, e.g. “hopping mad” vs
“off on one”.

Combining lexical and sentence similarity via
(5) performs better than lexical similarity alone
(Figure 1) but still does not capture all aspects of
a true paraphrase.

6 Conclusion and Future Work

In this paper we present a new domain for the
paraphrase identification task: to find paraphrases
among idiomatic expressions. We propose a sim-
ple scheme to compute the similarity of two id-
iom definitions that outperforms state-of-the-art
paraphrasing models on the dataset of idiom para-
phrases that we make publicly available.

Our future work will be focused on exploring
different strategies to compute semantic similarity
between sentences, developing a comprehensive
idiom similarity measure that will utilize both id-
ioms and their definitions, and on comparing text
with an idiom and a general text as a realistic sce-
nario for paraphrase identification. It is a new and
a challenging task and thus opens up many oppor-
tunities for further research in paraphrase identifi-
cation and all its downstream applications.

Acknowledgments

We thank Thien Huu Nguyen of New York Univer-
sity and Asli Eyecioglu of University of Sussex for
their help and advice.

References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor

Gonzalez-Agirre. (2012). Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. (2015). Semeval-2015 task 2: Semantic tex-
tual similarity, English, Spanish and Pilot on Inter-
pretability. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval).



Colin Bannard and Chris Callison-Burch. (2005).
Paraphrasing with Bilingual Parallel Corpora. In
Proceedings of the 43th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).

Marco Baroni, Georgiana Dinu, and German
Kruszewski. (2014). Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52th Annual Meeting of the Association for
Computational Linguistics (ACL).

Rahul Bhagat and Eduard Hovy. (2013). What is a
paraphrase? In Proceedings of the International
Conference on Computational Linguistics (COL-
ING).

Julia Birke and Anoop Sarkar. (2006). A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In Proceedings of the Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL).

William Blacoe and Mirella Lapata. (2012). A com-
parison of vector-based representations for semantic
composition. In Proceedings of EMNLP-CoLNN.

Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur.
(2010). LDA based similarity modeing for question
answering. In Proceedings of the NAACL HLT 2010
Workshop on Semantic Search.

Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa
(2011). Natural Language Processing (Almost)
from Scratch. In Journal of Machine Learning Re-
search (JMLR).

Dipanjan Das and Noah A. Smith. (2009). Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage (ACL-IJCNLP).

Asli Eyecioglu and Bill Keller. (2015). ASOBEK:
Twitter Paraphrase Identification with Simple Over-
lap Features and SVMs In Proceedings of 9th In-
ternational Workshop on Semantic Evaluation (Se-
mEval).

Samuel Fernando and Mark Stevenson (2008). A se-
mantic similarity approach to paraphrase detection.
Computational Linguistics UK (CLUK) 11th Annual
Research Colloquium.

Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. (2006). Catching metaphors. In Pro-
ceedings of the Third Workshop on Scalable Natural
Language Understanding (ScaNaLU).

Weiwei Guo and Mona Diab. (2013). Modeling Sen-
tences in the Latent Space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).

Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. (2013).
Linking Tweets to News: A Framework to Enrich
Short Text Data in Social Media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics (ACL).

Samer Hassan and Rada Mihalcea. (2011). Seman-
tic relatedness using salient semantic analysis. In
Proceedings of the twenty-fifth Association for the
Advancement of Artificial Intelligence Conference
(AAAI).

Aminul Islam and Diana Inkpen. (2007). Semantic
similarity of short texts. In Proceedings of Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP).

Yangfeng Ji and Jacob Eisenstein. (2013). Discrimi-
native improvements to distributional sentence sim-
ilarity. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).

Graham Katz and Eugenie Giesbrecht. (2006). Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties (MWE).

Linlin Li and Caroline Sporleder. (2009). Classifier
combination for contextual idiom detection without
labeled data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Elena Lloret, Oscar Ferrandez, Rafael Munoz, and
Manuel Palomar. (2008). A text summarization
approach under the influence of textual entailment.
In Proceedings of the 5th International Workshop
on Natural Language Processing and Cognitive Sci-
ence (NLPCS).

Nitin Madnani and Joel Tetreault. (2012). Re-
examining machine translation metrics for para-
phrase identification. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL-HLT).

Rada Mihalcea, Courtney Corley, and Strapparava.
(2006). Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the Association for the Advancement of Artificial In-
telligence Conference (AAAI).

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. (2013). Efficient estimation of word repre-
sentations in vector space. In Proceedings of Work-
shop at the International Conference on Learning
Representations (ICLR).

Grace Muzny and Luke Zettlemoyer. (2013). Auto-
matic Idiom Identification in Wiktionary. In Pro-
ceedings of the Conference on Empirical Methods
on Natural Language Processing (EMNLP).



Sasa Petrovic, Miles Osborne, and Victor Lavrenko
(2012). Using paraphrases for improving first story
detection in news and Twitter. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nologies (NAACL-HLT).

Long Qiu, Min-Yen Kan, and Tat-Seng Chua. (2006).
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP).

Vasile Rus, Philip M. McCarthy, Mihai C. Lintean,
Danielle S. McNamara, and Arthur C. Graesser
(2008). Paraphrase identification with lexico-
syntactic graph subsumption. In Proceedings of the
Twenty-First International FLAIRS Conference.

Satoshi Sekine, (2005). Automatic paraphrase dis-
covery based on context and keywords between NE
pairs. In Proceedings of the 3rd International Work-
shop on Paraphrasing.

Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
(2002). Automatic paraphrase acquisition from
news articles. In Proceedings of the 2nd Interna-
tional Conference on Human Language Technology
Research (HLT).

Ekaterina Shutova, Lin Sun, and Anna Korhonen.
(2010). Metaphor identification using verb and noun
clustering. In Proceedings of the International Con-
ference on Computational Linguistics (COLING).

Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. (2011).
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS).

Joseph Turian, Lev Ratinov, and Yoshua Bengio.
(2010). Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL).

Stephen Wan, Mark Dras, Robert Dale, and Cecile
Paris. (2006). Using dependency-based features to
take the parafarce out of paraphrase. In Proceedings
of the Australasian Language Technology Workshop.

Ling Wang, Chris Dyer, Alan W Black, and Isabel
Trancoso. (2013). Paraphrasing 4 microblog nor-
malization. In Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP).

Wei Xu, Chris Callison-Burch, and William B. Dolan.
(2015). SemEval-2015 Task 1: Paraphrase and Se-
mantic Similarity in Twitter (PIT). In Proceedings
of the 9th International Workshop on Semantic Eval-
uation (SemEval).

Wei Xu, Alan Ritter, Chris Callison-Burch, William B.
Dolan, and Yangfeng Ji. (2015). Extracting Lexi-
cally Divergent Paraphrases from Twitter. Transac-
tions of the Association for Computational Linguis-
tics (TACL).

Wei Xu, Alan Ritter, and Ralph Grishman. (2013).
Gathering and Generating Paraphrases from Twitter
with Application to Normalization. In Proceedings
of the Sixth Workshop on Building and Using Com-
parable Corpora (BUCC).

Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Kostas Tsioutsiouliklis (2011). Linguistic redun-
dancy in Twitter. In Proceedings of the Conference
on Empirical Methods on Natural Language Pro-
cessing (EMNLP).

Congle Zhang and Daniel S. Weld (2013). Harvest-
ing parallel news streams to generate paraphrases of
event relations. In Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP).

Jiang Zhao, Man Lan, Zheng-Yu Niu, and Dong-Hong
Ji. (2014). Recognizing cross-lingual textual entail-
ment with co-training using similarity and difference
views. In Proceedings of International Joint Confer-
ence on Neural Networks (IJCNN).

Jiang Zhao, Man Lan, and Jun Feng Tian. (2015).
ECNU: Using Traditional Similarity Measurements
and Word Embedding for Semantic Textual Similar-
ity Estimation. In Proceedings of the 9th Interna-
tional Workshop on Semantic Evaluation (SemEval).


