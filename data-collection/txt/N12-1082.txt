










































On The Feasibility of Open Domain Referring Expression Generation Using Large Scale Folksonomies


2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 641–645,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

On The Feasibility of Open Domain Referring Expression

Generation Using Large Scale Folksonomies

Fabián Pacheco Pablo Ariel Duboue∗

Facultad de Matemática, Astronomı́a y Fı́sica

Universidad Nacional de Córdoba

Córdoba, Argentina

Martı́n Ariel Domı́nguez

Abstract

Generating referring expressions has received

considerable attention in Natural Language

Generation. In recent years we start seeing

deployments of referring expression genera-

tors moving away from limited domains with

custom-made ontologies. In this work, we ex-

plore the feasibility of using large scale noisy

ontologies (folksonomies) for open domain

referring expression generation, an important

task for summarization by re-generation. Our

experiments on a fully annotated anaphora

resolution training set and a larger, volunteer-

submitted news corpus show that existing al-

gorithms are efficient enough to deal with

large scale ontologies but need to be extended

to deal with undefined values and some mea-

sure for information salience.

1 Introduction

Given an entity1 (the referent) and a set of com-

peting entities (the set of distractors), the task of

referring expression generation (REG) involves cre-

ating a mention to the referent so that, in the eyes

of the reader, it is clearly distinguishable from any

other entity in the set of distractors. In a traditional

generation pipeline, referring expression generation

happens at the sentence planning level. As a result,

its output is not a textual nugget but a description

employed later on by the surface realizer. In this pa-

per, we consider the output of the REG system to

∗To whom correspondence should be addressed. Email:

pablo.duboue@gmail.com.
1Or set of entities, but not in this work.

be Definite Descriptions (DD) consisting of a set of

positive triples and a set of negative triples, enumer-

ating referent-related properties.

Since the seminal work by Dale and Re-

iter (1995), REG has received a lot of attention in the

Natural Language Generation (NLG) community.

However, most of the early work on REG has been

on traditional NLG systems, using custom-tailored

ontologies. In recent years (Belz et al., 2010) there

has been a shift towards what we term “Open Do-

main Referring Expression Generation,” (OD REG),

that is, a REG task where the properties come from

a folksonomy, a large-scale volunteer-built ontology.

In particular, we are interested in changing

anaphoric references for entities appearing in sen-

tences drafted from different documents, as done

in multi-document summarization (Advaith et al.,

2011). For example, consider the following sum-

mary excerpt2 as produced by Newsblaster (McKe-

own et al., 2002):

Thousands of cheering, flag-waving Palestinians gave

Palestinian Authority President Mahmoud Abbas an en-

thusiastic welcome in Ramallah on Sunday, as he told

them triumphantly that a “Palestinian spring” had been

born following his speech to the United Nations last

week.3 The president pressed Israel, in unusually frank

terms, to reach a final peace agreement with the Pales-

tinians, citing the boundaries in place on the eve of the

June 1967 Arab-Israeli War as the starting point for ne-

2From http://newsblaster.cs.columbia.edu/archives/2011-10-

07-04-51-35/web/summaries/ 2011-10-07-04-51-35-011.html.
3After his stint at UN, Abbas is politically stronger than ever

(haaretz.com, 10/07/2011, 763 words).

641



gotiation about borders.4

Here the second sentence refers to U.S. presi-

dent Barack Obama and a referring expression of the

form “U.S. president” should have been used. Such

expressions depend on the set of distractors present

in the text, a requirement that highlights the dynamic

nature of the problem. Our experiments extracted

thousands of complex cases (such as distinguishing

one musician from a set of five) which we used to

test existing algorithms against a folksonomy, dbPe-

dia5 (Bizer et al., 2009). This folksonomy contains

1.7M triples (for its English version) and has been

curated from Wikipedia.6

We performed two experiments: first we em-

ployed sets of distractors derived from a set of docu-

ments annotated with anaphora resolution informa-

tion (Hasler et al., 2006). We found that roughly

half of the entities annotated in the documents were

present in the folksonomy, which speaks of the feasi-

bility of using a folksonomy for OD REG, given the

fact that Wikipedia has strict notability requirements

for adding information. In the second experiment,

we obtained sets of distractors from Wikinews,7 a

service where volunteers submit news articles inter-

spersed with Wikipedia links. We leveraged said

links to assemble 40k referring expression tasks.

For algorithms, we employed Dale and Re-

iter (1995), Gardent (2002) and Full Brevity (FB)

(Bohnet, 2007). Our results show that the first two

algorithms produce results in a majority of the re-

ferring expression tasks, with the Dale and Reiter

algorithm being the most efficient and resilient of

the three. The results, however, are of mixed quality

and more research is needed to overcome two prob-

lems we have identified in our experiments: dealing

with undefined information in the folksonomy and

the need to incorporate a rough user model in the

form of information salience.

In the next section we briefly summarize the three

algorithms we employed in our experiments. In Sec-

tion 3, we describe the data employed. Section 4

contains the results of our experiments and subse-

quent analysis. We conclude discussing future work.

4Obama prods Mideast allies to embrace reform, make

peace (Washington Post, 10/07/2011, 371 words).
5http://dbpedia.org
6http://wikipedia.org
7http://wikinews.org

2 Referring Expression Generation (REG)

REG literature is vast and spans decades of work.

We picked three algorithms with the following

desiderata: all the algorithms can deal with single

entity referents (a significant amount of recent work

went into multi-entity referents) and we wanted to

showcase a classic algorithm (Dale and Reiter’s), an

algorithm generating negations (Gardent’s) and an

algorithm with a more exhaustive search of the solu-

tions space (Full Brevity). We very briefly describe

each of the algorithms in turn, where R is the refer-

ent, C is the set of distractors and P is a list of prop-

erties, triples in the form (entity, property, value),

describing R:

Dale and Reiter (1995). They assume the prop-

erties in P are ordered according to an established

criteria. Then the algorithm iterates over P , adding

each triple one at a time and removing from C all

entities ruled out by the new triple. Triples that do

not eliminate any new entities from C are ignored.

The algorithm terminates when C is empty.

Gardent (2002). The algorithm uses Constraint

Satisfaction Programming to solve two basic con-

straints: find a set of positive properties P+ and neg-

ative properties P−, such that all properties in P+

are true for the referent and all in P− are false, and

it is the smaller P+ ∪ P− such that for every c ∈ C
there exist a property in P+ that does not hold for c

or a property in P− that holds for c.8

Full Brevity (Bohnet, 2007). Starting from a

state E of the form (L, C, P ) with L = ∅ (selected
properties), it keeps these states into a queue, where

it loops until C = ∅. In each loop it generates new
states (added to the end of the queue), as follows:

given a state E = (L, C, P ) for each p ∈ P , if p re-
moves elements rem from C, it adds (L∪ {p}, C −
rem, P − {p}), otherwise (L, C, P − {p}).

3 Data

dbPedia. dbPedia (Bizer et al., 2009) is

an ontology curated from Wikipedia infoboxes,

small tables containing structured information at

the top of most Wikipedia pages. The ver-

sion employed in this paper (“Ontology Infobox

Properties”) contains 1,7520,158 triples. Each

8We employed the Choco CSP solver Java library:

http://www.emn.fr/z-info/choco-solver/.

642



Former [[New Mexico]] {{w|Governor of New

Mexico|governor}} {{w|Gary Johnson}} ended

his campaign for the {{w|Republican Party

(United States)|Republican Party}} (GOP)

presidential nomination to seek the backing

of the {{w|Libertarian Party (United

States)|Libertarian Party}} (LP).

Figure 1: Wikinews example, from http://en.wikinews.org

/wiki/U.S. presidential candidate Gary Johnson leaves GOP to vie for

the LP nom

entity is represented by a URI starting with

http://dbpedia.org/resource/ followed by

the name of its associated Wikipedia title. See the

next section for some example triples.

Pilot. While creating unambiguous descriptions

is the NLG task known as referring expression gen-

eration, its NLU counterpart is anaphora resolu-

tion. We took a hand-annotated corpus for training

anaphora resolution algorithms (Hasler et al., 2006)

consisting of 74 documents containing 239 corefer-

ence chains. Each of the chains is an entity that can

be used for our experiments, if the entity is in db-

Pedia and there are other suitable distractors in the

same document. We hand annotated each of those

239 coreference chains by type (person, organiza-

tion and location) and associated them to dbPedia

URIs for the ones we found on Wikipedia. We found

roughly half of the chains in dbPedia (106 out of

239, 44%). This percentage speaks of the coverage

of dbPedia for OD REG. However, only 16 docu-

ments contain multiple entities of the same type and

present in dbPedia, our pilot study criteria. These 16

documents result in the 16 tasks for our pilot. For a

large scale evaluation we turned to Wikinews.

Wikinews. Wikinews is a news service operated

as a wiki. As the news articles are interspersed

with interwiki links, multiple entities can be disam-

biguated as Wikipedia pages (which in turn are db-

Pedia URIs). For example, in Figure 1, both the Lib-

ertarian Party and Republican Party can be consid-

ered potential distractors, as both are organizations.

The Wikimedia Foundation makes a database

dump available for all Wikinews interwiki links (the

links in braces in the above example). If a page con-

tains more than one organization or person, we ex-

tracted the whole set of people (or organizations) as

a referring expression task. To see whether a URI

is a person or an organization we check for a birth

date or creation date, respectively. In this manner,

we obtained 4,230 tasks for people and 12,998 for

organizations. This is dataset is freely available.9

4 Results

Pilot. The 16 tasks were split into 40 runs (a task

spans n runs each, where n is the number of entities

in the task, by rotating through the different alterna-

tive pairs of referent / set of distractors). From these

tasks, Dale and Reiter produced no output 12 times

and FB Brevity was unable to produce a result in 23

times. Gardent produced output for every run. We

consider this an example of the increased expressive

power of negative descriptions (it included a nega-

tion in 25% of the runs). For the other two algo-

rithms, the lack of an unique triple differentiating

one entity from the set of distractors seemed to be

the main issue but there were multiple cases were FB

ran out of memory for its queue of candidate nodes.

With respect to execution timings, Dale and Re-

iter ran into some corner cases and took time com-

parable to Gardent’s algorithm. FB was 16 times

slower (we found this counter-intuitive, as Gardent’s

algorithm is more demanding). Therefore, two of

these algorithms were able to produce results using

large scale ontological information. As FB ran into

problems both in terms of execution time and failure

rates, we omitted it from the large scale experiments.

We adjusted the parameters for the algorithms on

this set to obtain the best possible quality output

given the data and the problem. As such, we do not

report quality assessments on the pilot data.

Wikinews. The tasks obtained from wikinews

contained a large number of entities per task (an av-

erage of 12 people per task) and therefore span a

large number of runs: 17,814 runs for people (from

4,230 tasks) and 44,080 for organizations (from

12,998 tasks).

On these large runs, execution time differences

are in line with our a priori expectations: the greedy

approach of Dale and Reiter is very fast10 with Gar-

dent’s more comprehensive search taking about 40

times more time. Dale and Reiter failure rate was

9
http://www.cs.famaf.unc.edu.ar/˜pduboue/data/ also mirrored

at http://duboue.ca/data.
10Dale and Reiter takes less than 3’ for the 44,080 runs for

organizations in a 2.3 GHz machine.

643



Referent Dale and Reiter Output Gardent Output

EB { (EB occupation Software Freedom Law Center) } { (EB occupation Software Freedom Law Center) }

LL { (LL birthPlace United States), (LL, occupation Harvard Law School) } { (LL birthPlace Rapid City, South Dakota) }

LT { (LT occupation Software engineer) } { (LT nationality Finnish American) }

Figure 2: Example output for the task: {‘Eben Moglen’ (EB), ‘Lawrence Lessig’ (LL), ‘Linus Torvalds’ (LT) }.

comparable or better than in the pilot (for organiza-

tions that are more mixed, it was slightly lower but

for people it was as low 2.8%). Gardent missed 2%

of the people (and only 54 organizations), employ-

ing negatives 14% of the time for people and 12% of

the time for organizations.

Evaluating referring expressions is hard. Efforts

to automate this task in NLG (Gatt et al., 2007)

have taken an approach similar to machine transla-

tion BLEU scores (Papinini et al., 2001), for exam-

ple, by asking multiple judges to produce referring

expressions for a given scenario. These settings usu-

ally involve images of physical objects and relate to

small ontologies. While such an approach could be

adapted to the Open Domain case, a major problem

is the need for the judges to be acquainted with some

of the less popular entities in the training set. At

this point in our research, we decided to analyze the

quality of a sample of the output ourselves. This

process involved consulting information about each

entity to determine the soundness of the result.

We looked at a random sample of 20 runs and an-

notated it by two authors, measuring a Cohen’s κ of

60% for annotating DD results and 79% for deter-

mining whether the folksonomy had enough infor-

mation to build a satisfactory DD. We then extended

the evaluation to 60 runs and annotated them by one

author. We found that Dale and Reiter produced a

satisfactory DD in 41.6% of the cases and Gardent

in 43.4% of the cases and that the folksonomy con-

tained enough information 81.6% of the time. Fig-

ure 2 shows some example output.

From the evaluation we learned that the default

ordering strategy employed by Dale and Reiter is

not stable across different types of people (compare:

politicians vs. musicians) or organizations. We also

saw that Gardent’s algorithm in many cases selected

a single triple with very little practical value (an ob-

scure fact about the entity) or a negative piece of in-

formation which is actually true for the referent but

it is a missing piece of information.

The first two problems can be solved by either fur-

ther subdividing the taxonomies of entities or (more

interestingly) by incorporating some measure about

the salience of each piece of information, a possibil-

ity which we will discuss next. The last issue can be

addressed by having some form of meaningful de-

fault value.

The negations produced by Gardent’s algorithm

highlighted errors on the folksonomy. For example,

when referring to China with distractors Peru and

Taiwan, it will produce “the place where they do not

speak Chinese,” as China has the different Chinese

dialects spelled out on the folksonomy (and some

Peruvians do speak Chinese). Given these limita-

tions, we find the current results very encouraging

and we believe folksonomies can help focus on ro-

bust NLG for noisy (ontological) inputs.

5 Discussion

We have shown that by using a folksonomy it should

be possible to deploy traditional NLG referring ex-

pression generation algorithms in Open Domain

tasks. To fulfill this vision, three tasks remain:

Dealing with missing information. Some form of

smart default values are needed, we are considering

using a nearest-neighbor approach to find ontologi-

cal siblings which can provide such defaults.

Estimating salience of each piece of ontological

information. The importance for each triple has to

be obtained in a way consistent with the Open Do-

main nature of the task. For this problem, we believe

search engine salience can be of great help.

Transform the extracted triples into actual text.

This problem has received attention in the past. We

would like to explore traditional surface realizer

with a custom-made grammar.

Acknowledgments

We would like to thank the anonymous reviewers as

well as Annie Ying and Victoria Reggiardo.

644



References

Siddharthan Advaith, Nenkova Ani, and McKeown Kath-

leen. 2011. Information status distinctions and refer-

ring expressions: An empirical study of references to

people in news summaries. Computational Linguis-

tics, 37(4):811–842.

Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.

2010. Generating referring expressions in context:

The grec task evaluation challenges. In Emiel Krah-

mer and Marit Theune, editors, Empirical Methods

in Natural Language Generation, volume 5790 of

Lecture Notes in Computer Science, pages 294–327.

Springer.

C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker,

R. Cyganiak, and S. Hellmann. 2009. DBpedia-a

crystallization point for the web of data. Web Seman-

tics: Science, Services and Agents on the World Wide

Web, 7(3):154–165.

B. Bohnet. 2007. is-fbn, is-fbs, is-iac: The adaptation

of two classic algorithms for the generation of refer-

ring expressions in order to produce expressions like

humans do. MT Summit XI, UCNLG+ MT, pages 84–

86.

R. Dale and E. Reiter. 1995. Computational interpreta-

tions of the gricean maxims in the generation of refer-

ring expressions. Cognitive Science, 19(2):233–263.

C. Gardent. 2002. Generating minimal definite descrip-

tions. In Proceedings of the 40th Annual Meeting on

Association for Computational Linguistics, pages 96–

103. Association for Computational Linguistics.

A. Gatt, I. Van Der Sluis, and K. Van Deemter. 2007.

Evaluating algorithms for the generation of referring

expressions using a balanced corpus. In Proceedings

of the Eleventh European Workshop on Natural Lan-

guage Generation, pages 49–56. Association for Com-

putational Linguistics.

L. Hasler, C. Orasan, and K. Naumann. 2006. NPs

for events: Experiments in coreference annotation. In

Proceedings of the 5th edition of the International

Conference on Language Resources and Evaluation

(LREC2006), pages 1167–1172.

Kathleen R. McKeown, R. Barzilay, D. Evans, V. Hatzi-

vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,

B. Schiffman, and S. Sigelman. 2002. Tracking and

summarizing news on a daily basis with columbia’s

newsblaster. In Proc. of HLT.

Kishore Papinini, Salim Roukos, Todd Ward, and Wei-

Jing Zhu. 2001. Bleu: a method for automatic evalua-

tion of machine translation. Technical report, IBM.

645


