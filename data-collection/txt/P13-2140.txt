



















































Automatic Term Ambiguity Detection


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 804–809,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Automatic Term Ambiguity Detection

Tyler Baldwin Yunyao Li Bogdan Alexe Ioana R. Stanoi
IBM Research - Almaden

650 Harry Road, San Jose, CA 95120, USA
{tbaldwi,yunyaoli,balexe,irs}@us.ibm.com

Abstract

While the resolution of term ambiguity is
important for information extraction (IE)
systems, the cost of resolving each in-
stance of an entity can be prohibitively
expensive on large datasets. To combat
this, this work looks at ambiguity detec-
tion at the term, rather than the instance,
level. By making a judgment about the
general ambiguity of a term, a system is
able to handle ambiguous and unambigu-
ous cases differently, improving through-
put and quality. To address the term
ambiguity detection problem, we employ
a model that combines data from lan-
guage models, ontologies, and topic mod-
eling. Results over a dataset of entities
from four product domains show that the
proposed approach achieves significantly
above baseline F-measure of 0.96.

1 Introduction

Many words, phrases, and referring expressions
are semantically ambiguous. This phenomenon,
commonly referred to as polysemy, represents a
problem for NLP applications, many of which in-
herently assume a single sense. It can be particu-
larly problematic for information extraction (IE),
as IE systems often wish to extract information
about only one sense of polysemous terms. If
nothing is done to account for this polysemy, fre-
quent mentions of unrelated senses can drastically
harm performance.

Several NLP tasks, such as word sense disam-
biguation, word sense induction, and named en-
tity disambiguation, address this ambiguity prob-
lem to varying degrees. While the goals and initial
data assumptions vary between these tasks, all of
them attempt to map an instance of a term seen
in context to an individual sense. While making

a judgment for every instance may be appropri-
ate for small or medium sized data sets, the cost
of applying these ambiguity resolution procedures
becomes prohibitively expensive on large data sets
of tens to hundreds of million items. To combat
this, this work zooms out to examine the ambigu-
ity problem at a more general level.

To do so, we define an IE-centered ambiguity
detection problem, which ties the notion of am-
biguity to a given topical domain. For instance,
given that the terms Call of Juarez and A New
Beginning can both reference video games, we
would like to discover that only the latter case is
likely to appear frequently in non-video game con-
texts. The goal is to make a binary decision as
to whether, given a term and a domain, we can
expect every instance of that term to reference an
entity in that domain. By doing so, we segregate
ambiguous terms from their unambiguous coun-
terparts. Using this segregation allows ambiguous
and unambiguous instances to be treated differ-
ently while saving the processing time that might
normally be spent attempting to disambiguate in-
dividual instances of unambiguous terms.

Previous approaches to handling word ambigu-
ity employ a variety of disparate methods, vari-
ously relying on structured ontologies, gleaming
insight from general word usage patterns via lan-
guage models, or clustering the contexts in which
words appear. This work employs an ambiguity
detection pipeline that draws inspiration from all
of these methods to achieve high performance.

2 Term Ambiguity Detection (TAD)

A term can be ambiguous in many ways. It may
have non-referential senses in which it shares a
name with a common word or phrase, such as in
the films Brave and 2012. A term may have refer-
ential senses across topical domains, such as The
Girl with the Dragon Tattoo, which may reference
either the book or the film adaptation. Terms may

804



also be ambiguous within a topical domain. For
instance, the term Final Fantasy may refer to the
video game franchise or one of several individual
games within the franchise. In this work we con-
cern ourselves with the first two types of ambigu-
ity, as within topical domain ambiguity tends to
pose a less severe problem for IE systems.

IE systems are often asked to perform extrac-
tion over a dictionary of terms centered around
a single topic. For example, in brand manage-
ment, customers may give a list of product names
and ask for sentiment about each product. With
this use case in mind, we define the term ambigu-
ity detection (TAD) problem as follows: Given a
term and a corresponding topic domain, determine
whether the term uniquely references a member
of that topic domain. That is, given a term such
as Brave and a category such as film, the task is
make a binary decision as to whether all instances
of Brave reference a film by that name.

2.1 Framework

Our TAD framework is a hybrid approach consist-
ing of three modules (Figure 1). The first module
is primarily designed to detect non-referential am-
biguity. This module examines n-gram data from
a large text collection. Data from The Corpus of
Contemporary American English (Davies, 2008 )
was used to build our n-grams.

The rationale behind the n-gram module is
based on the understanding that terms appearing
in non-named entity contexts are likely to be non-
referential, and terms that can be non-referential
are ambiguous. Therefore, detecting terms that
have non-referential usages can also be used to
detect ambiguity. Since we wish for the ambigu-
ity detection determination to be fast, we develop
our method to make this judgment solely on the
n-gram probability, without the need to examine
each individual usage context. To do so, we as-
sume that an all lowercased version of the term is
a reasonable proxy for non-named entity usages in
formal text. After removing stopwords from the
term, we calculate the n-gram probability of the
lower-cased form of the remaining words. If the
probability is above a certain threshold, the term
is labeled as ambiguous. If the term is below the
threshold, it is tentatively labeled as unambiguous
and passed to the next module. To avoid making
judgments of ambiguity based on very infrequent
uses, the ambiguous-unambiguous determination

threshold is empirically determined by minimiz-
ing error over held out data.

The second module employs ontologies to de-
tect across domain ambiguity. Two ontologies
were examined. To further handle the common
phrase case, Wiktionary1 was used as a dictionary.
Terms that have multiple senses in Wiktionary
were labeled as ambiguous. The second ontology
used was Wikipedia disambiguation pages. All
terms that had a disambiguation page were marked
as ambiguous.

The final module attempts to detect both non-
referential and across domain ambiguity by clus-
tering the contexts in which words appear. To do
so, we utilized the popular Latent Dirichlet Allo-
cation (LDA (Blei et al., 2003)) topic modeling
method. LDA represents a document as a distri-
bution of topics, and each topic as a distribution
of words. As our domain of interest is Twitter,
we performed clustering over a large collection of
tweets. For a given term, all tweets that contained
the term were used as a document collection. Fol-
lowing standard procedure, stopwords and infre-
quent words were removed before topic modeling
was performed. Since the clustering mechanism
was designed to make predictions over the already
filtered data of the other modules, it adopts a con-
servative approach to predicting ambiguity. If the
category term (e.g., film) or a synonym from the
WordNet synset does not appear in the 10 most
heavily weighted words for any cluster, the term is
marked as ambiguous.

A term is labeled as ambiguous if any one of
the three modules predicts that it is ambiguous,
but only labeled as unambiguous if all three mod-
ules make this prediction. This design allows each
module to be relatively conservative in predicting
ambiguity, keeping precision of ambiguity predic-
tion high, under the assumption that other modules
will compensate for the corresponding drop in re-
call.

3 Experimental Evaluation

3.1 Data Set
Initial Term Sets We collected a data set of terms
from four topical domains: books, films, video
games, and cameras. Terms for the first three do-
mains are lists of books, films, and video games
respectively from the years 2000-2011 from db-
pedia (Auer et al., 2007), while the initial terms

1http://www.wiktionary.org/

805



Tweet Term Category Judgment
Woke up from a nap to find a beautiful mind on. #win A Beautiful Mind film yes
I Love Tyler Perry ; He Has A Beautiful Mind. A Beautiful Mind film no
I might put it in the top 1. RT @CourtesyFlushMo Splice. Top 5 worst movies ever Splice film yes
Splice is a great, free replacement to iMove for your iPhone, Splice film no

Table 1: Example tweet annotations.

Figure 1: Overview of the ambiguity detection
framework.

for cameras includes all the cameras from the six
most popular brands on flickr2.
Gold Standard A set of 100 terms per domain
were chosen at random from the initial term sets.
Rather than annotating each term directly, am-
biguity was determined by examining actual us-
age. Specifically, for each term, usage examples
were extracted from large amounts of Twitter data.
Tweets for the video game and film categories were
extracted from the TREC Twitter corpus.3 The
less common book and camera cases were ex-
tracted from a subset of all tweets from September
1st-9th, 2012.

For each term, two annotators were given the
term, the corresponding topic domain, and 10 ran-
domly selected tweets containing the term. They
were then asked to make a binary judgment as to
whether the usage of the term in the tweet referred
to an instance of the given category. The degree
of ambiguity is then determined by calculating the
percentage of tweets that did not reference a mem-
ber of the topic domain. Some example judgments
are given in Table 1. If all individual tweet judg-
ments for a term were marked as referring to a

2http://www.flickr.com/cameras/
3http://trec.nist.gov/data/tweets/

Configuration Precision Recall F-measure
Baseline 0.675 1.0 0.806

NG 0.979 0.848 0.909
ON 0.979 0.704 0.819
CL 0.946 0.848 0.895

NG + ON 0.980 0.919 0.948
NG + CL 0.942 0.963 0.952
ON + CL 0.945 0.956 0.950

NG + ON + CL 0.943 0.978 0.960

Table 2: Performance of various framework con-
figurations on the test data.

member of the topic domain, the term was marked
as fully unambiguous within the data examined.
All other cases were considered ambiguous.4

Inter-annotator agreement was high, with raw
agreement of 94% (κ = 0.81). Most disagree-
ments on individual tweet judgments had little ef-
fect on the final judgment of a term as ambiguous
or unambiguous, and those that did were resolved
internally.

3.2 Evaluation and Results

Effectiveness To understand the contribution of
the n-gram (NG), ontology (ON), and clustering
(CL) based modules, we ran each separately, as
well as every possible combination. Results are
shown in Table 2, where they are compared to a
majority class (ambiguous) baseline.

As shown, all configurations outperform the
baseline. Of the three individual modules, the n-
gram and clustering methods achieve F-measure
of around 0.9, while the ontology-based module
performs only modestly above baseline. Unsur-
prisingly, the ontology method is affected heav-
ily by its coverage, so its poor performance is pri-
marily attributable to low recall. As noted, many
IE tasks may involve sets of entities that are not
found in common ontologies, limiting the ability
of the ontology-based method alone. Additionally,
ontologies may be apt to list cases of strict ambi-
guity, rather than practical ambiguity. That is, an
ontology may list a term as ambiguous if there are

4The annotated data is available at http:
//researcher.watson.ibm.com/researcher/
view_person_subpage.php?id=4757.

806



several potential named entities it could refer to,
even if the vast majority of references were to only
a single entity.

Combining any two methods produced substan-
tial performance increases over any of the individ-
ual runs. The final system that employed all mod-
ules produced an F-measure of 0.960, a significant
(p < 0.01) absolute increase of 15.4% over the
baseline.
Usefulness To establish that term ambiguity de-
tection is actually helpful for IE, we conducted
a preliminary study by integrating our pipeline
into a commercially available rule-based IE sys-
tem (Chiticariu et al., 2010; Alexe et al., 2012).
The system takes a list of product names as input
and outputs tweets associated with each product.
It utilizes rules that employ more conservative ex-
traction for ambiguous entities.

Experiments were conducted over several mil-
lion tweets using the terms from the video game
and camera domains. When no ambiguity detec-
tion was performed, all terms were treated as un-
ambiguous. The system produced very poor pre-
cision of 0.16 when no ambiguity detection was
used, due to the extraction of irrelevant instances
of ambiguous objects. In contrast, the system pro-
duced precision of 0.96 when ambiguity detection
was employed. However, the inclusion of disam-
biguation did reduce the overall recall; the system
that employed disambiguation returned only about
57% of the true positives returned by the system
that did not employ disambiguation. Although
this reduction in recall is significant, the overall
impact of disambiguation is clearly positive, due
to the stark difference in precision. Nonetheless,
this limited study suggests that there is substantial
room for improvement in the extraction system, al-
though this is out of the scope of the current work.

4 Related Work

Polysemy is a known problem for many NLP-
related applications. Machine translation systems
can suffer, as ambiguity in the source language
may lead to incorrect translations, and unambigu-
ous sentences in one language may become am-
biguous in another (Carpuat and Wu, 2007; Chan
et al., 2007). Ambiguity in queries can also hin-
der the performance of information retrieval sys-
tems (Wang and Agichtein, 2010; Zhong and Ng,
2012).

The ambiguity detection problem is similar to

the well studied problems of named entity dis-
ambiguation (NED) and word sense disambigua-
tion (WSD). However, these tasks assume that
the number of senses a word has is given, essen-
tially assuming that the ambiguity detection prob-
lem has already been solved. This makes these
tasks inapplicable in many IE instances where the
amount of ambiguity is not known ahead of time.
Both named entity and word sense disambigua-
tion are extensively studied, and surveys on each
are available (Nadeau and Sekine, 2007; Navigli,
2009).

Another task that shares similarities with TAD
is word sense induction (WSI). Like NED and
WSD, WSI frames the ambiguity problem as one
of determining the sense of each individual in-
stance, rather than the term as a whole. Unlike
those approaches, the word sense induction task
attempts to both figure out the number of senses a
word has, and what they are. WSI is unsupervised,
relying solely on the information that surrounds
word mentions in the text.

Many different clustering-based WSI methods
have been examined. Pantel and Lin (2002) em-
ploy a clustering by committee method that itera-
tively adds words to clusters based on their sim-
ilarities. Topic model-based methods have been
attempted using variations of Latent Dirichlet Al-
location (Brody and Lapata, 2009) and Hierarchi-
cal Dirichlet Processes (Lau et al., 2012). Sev-
eral graph-based methods have also been exam-
ined (Klapaftis and Manandhar, 2010; Navigli and
Crisafulli, 2010). Although the words that sur-
round the target word are the primary source of
contextual information in most cases, additional
feature sources such as syntax (Van de Cruys,
2008) and semantic relations (Chen and Palmer,
2004) have also been explored.

5 Conclusion

This paper introduced the term ambiguity detec-
tion task, which detects whether a term is am-
biguous relative to a topical domain. Unlike other
ambiguity resolution tasks, the ambiguity detec-
tion problem makes general ambiguity judgments
about terms, rather than resolving individual in-
stances. By doing so, it eliminates the need for
ambiguity resolution on unambiguous objects, al-
lowing for increased throughput of IE systems on
large data sets.

Our solution for the term ambiguity detection

807



task is based on a combined model with three dis-
tinct modules based on n-grams, ontologies, and
clustering. Our initial study suggests that the com-
bination of different modules designed for differ-
ent types of ambiguity used in our solution is ef-
fective in determining whether a term is ambigu-
ous for a given domain. Additionally, an exami-
nation of a typical use case confirms that the pro-
posed solution is likely to be useful in improving
the performance of an IE system that does not em-
ploy any disambiguation.

Although the task as presented here was mo-
tivated with information extraction in mind, it is
possible that term ambiguity detection could be
useful for other tasks. For instance, TAD could
be used to aid word sense induction more gener-
ally, or could be applied as part of other tasks such
as coreference resolution. We leave this avenue of
examination to future work.

Acknowledgments

We would like to thank the anonymous review-
ers of ACL for helpful comments and suggestions.
We also thank Howard Ho and Rajasekar Krishna-
murthy for help with data annotation and Shivaku-
mar Vaithyanathan for his comments on a prelim-
inary version of this work.

References
Bogdan Alexe, Mauricio A. Hernández, Kirsten Hil-

drum, Rajasekar Krishnamurthy, Georgia Koutrika,
Meenakshi Nagarajan, Haggai Roitman, Michal
Shmueli-Scheuer, Ioana Roxana Stanoi, Chitra
Venkatramani, and Rohit Wagle. 2012. Surfacing
time-critical insights from social media. In SIG-
MOD Conference, pages 657–660.

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: a nucleus for a web of open data.
In Proceedings of the 6th international The seman-
tic web and 2nd Asian conference on Asian semantic
web conference, ISWC’07/ASWC’07, pages 722–
735, Berlin, Heidelberg. Springer-Verlag.

David Blei, Andrew Ng, and Micheal I. Jordan. 2003.
Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022, January.

Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ’09,
pages 103–111, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61–72.

Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 33–40, Prague, Czech Republic,
June. Association for Computational Linguistics.

Jinying Chen and Martha Palmer. 2004. Chinese verb
sense discrimination using an em clustering model
with rich linguistic features. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ’04, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Sriram Raghavan, Frederick Reiss, and Shivaku-
mar Vaithyanathan. 2010. SystemT: An algebraic
approach to declarative information extraction. In
ACL, pages 128–137.

Mark Davies. 2008-. The corpus of contempo-
rary american english: 450 million words, 1990-
present. Avialable online at: http://corpus.
byu.edu/coca/.

Ioannis P. Klapaftis and Suresh Manandhar. 2010.
Word sense induction & disambiguation using hi-
erarchical random graphs. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’10, pages 745–755,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
’12, pages 591–601, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

David Nadeau and Satoshi Sekine. 2007. A survey
of named entity recognition and classification. Lin-
guisticae Investigationes, 30(1):3–26.

Roberto Navigli and Giuseppe Crisafulli. 2010. Induc-
ing word senses to improve web search result clus-
tering. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 116–126, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1–10:69,
February.

Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth

808



ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’02, pages
613–619, New York, NY, USA. ACM.

Tim Van de Cruys. 2008. Using three way data for
word sense discrimination. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ’08, pages 929–
936, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Yu Wang and Eugene Agichtein. 2010. Query ambigu-
ity revisited: Clickthrough measures for distinguish-
ing informational and ambiguous queries. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 361–364,
Los Angeles, California, June. Association for Com-
putational Linguistics.

Zhi Zhong and Hwee Tou Ng. 2012. Word sense
disambiguation improves information retrieval. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 273–282, Jeju Island, Korea,
July. Association for Computational Linguistics.

809


