



















































Using Syntactic and Semantic Structural Kernels for Classifying Definition Questions in Jeopardy!


Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 712–724,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics

Using Syntactic and Semantic Structural Kernels for
Classifying Definition Questions in Jeopardy!

Alessandro Moschitti† Jennifer Chu-Carroll‡ Siddharth Patwardhan‡

James Fan‡ Giuseppe Riccardi†

†Department of Information Engineering and Computer Science
University of Trento, 38123 Povo (TN), Italy

{moschitti,riccardi}@disi.unitn.it
‡IBM T.J. Watson Research Center P.O. Box 704, Yorktown Heights, NY 10598, U.S.A.

{jencc,siddharth,fanj}@us.ibm.com

Abstract

The last decade has seen many interesting ap-
plications of Question Answering (QA) tech-
nology. The Jeopardy! quiz show is certainly
one of the most fascinating, from the view-
points of both its broad domain and the com-
plexity of its language. In this paper, we study
kernel methods applied to syntactic/semantic
structures for accurate classification of Jeop-
ardy! definition questions. Our extensive em-
pirical analysis shows that our classification
models largely improve on classifiers based on
word-language models. Such classifiers are
also used in the state-of-the-art QA pipeline
constituting Watson, the IBM Jeopardy! sys-
tem. Our experiments measuring their impact
on Watson show enhancements in QA accu-
racy and a consequent increase in the amount
of money earned in game-based evaluation.

1 Introduction

Question Answering (QA) is an important research
area of Information Retrieval applications, which re-
quires the use of core NLP capabilities, such as syn-
tactic and semantic processing for a more effective
user experience. While the development of most
existing QA systems are driven by organized eval-
uation efforts such as TREC (Voorhees and Dang,
2006), CLEF (Giampiccolo et al., 2007), and NT-
CIR (Sasaki et al., 2007), there exist efforts that
leverage data from popular quiz shows, such as Who
Wants to be a Millionaire (Clarke et al., 2001; Lam
et al., 2003) and Jeopardy! (Ferrucci et al., 2010), to
demonstrate the generality of the technology.

Jeopardy! is a popular quiz show in the US which
has been on the air for 27 years. In each game, three
contestants compete for the opportunity to answer
60 questions in 12 categories of 5 questions each.
Jeopardy! questions cover an incredibly broad do-
main, from science, literature, history, to popular
culture. We are drawn to Jeopardy! as a test bed
for open-domain QA technology due to its broad do-
main, complex language, as well as the emphasis on
accuracy, confidence, and speed during game play.

While the vast majority of Jeopardy! questions
are factoid questions, we find several other types
of questions in the Jeopardy! data, which can ben-
efit from specialized processing in the QA system.
The additional processing in these questions com-
plements that of the factoid questions to achieve im-
proved overall QA performance. Among the various
types of questions handled by the system are defini-
tion questions shown in the examples below:

(1) GON TOMORROW: It can be the basket
below a hot-air balloon or a flat-bottomed
boat used on a canal (answer: gondola);

(2) I LOVE YOU, “MIN”: Overbearing (an-
swer: domineering);

(3) INVEST: From the Latin for “year”, it’s
an investment or retirement fund that pays
out yearly (answer: an annuity)

where the upper case text indicates the Jeop-
ardy! category for each question1.

Several characteristics of this class of questions
warrant special processing: first, the clue (question)

1A Jeopardy! category indicates a theme is common among
its 5 questions.

712



often aligns well with dictionary entries, making
dictionary resources potentially effective. Second,
these clues often do not indicate an answer type,
which is an important feature for identifying cor-
rect answers in factoid questions (in the examples
above, only (3) provided an answer type, “fund”).
Third, definition questions are typically shorter in
length than the average factoid question. These dif-
ferences, namely the shorter clue length and the lack
of answer types, make the use of a specialized ma-
chine learning model potentially promising for im-
proving the overall system accuracy. The first step
for handling definitions is, of course, the automatic
separation of definitions from other question types,
which is not a simple task in the Jeopardy! domain.
For instance, consider the following example which
is a variation of (3) above:

(4) INVEST: From the Latin for “year”,
an annuity is an investment or retirement
fund that pays out this often (answer:
yearly)

Even though the clue is nearly identical to (3), the
clue does not provide a definition for the answer
yearly, although at first glance we may have been
misled. The source of complexity is given by the fact
that Jeopardy! clues are not phrased in interrogative
form as questions typically are. This complicates the
design of definition classifiers since we cannot di-
rectly use either typical structural patterns that char-
acterize definition/description questions, or previous
approaches, e.g. (Ahn et al., 2004; Kaisser and Web-
ber, 2007; Blunsom et al., 2006). Given the com-
plexity and the novelty of the task, we found it use-
ful to exploit the kernel methods technology. This
has shown state-of-the-art performance in Question
Classification (QC), e.g. (Zhang and Lee, 2003;
Suzuki et al., 2003; Moschitti et al., 2007) and it
is very well suited for engineering feature represen-
tations for novel tasks.

In this paper, we apply SVMs and kernel meth-
ods to syntactic/semantic structures for modeling
accurate classification of Jeopardy! definition ques-
tions. For this purpose, we use several levels of lin-
guistic information: word and POS tag sequences,
dependency, constituency and predicate argument
structures and we combined them using state-of-
the-art structural kernels, e.g. (Collins and Duffy,

2002; Shawe-Taylor and Cristianini, 2004; Mos-
chitti, 2006). The extensive empirical analysis of
several advanced models shows that our best model,
which combines different kernels, improves the F1
of our baseline model by 67% relative, from 40.37
to 67.48. Surprisingly, with respect to previous find-
ings on standard QC, e.g. (Zhang and Lee, 2003;
Moschitti, 2006), the Syntactic Tree Kernel (Collins
and Duffy, 2002) is not effective whereas the ex-
ploitation of partial tree patterns proves to be es-
sential. This is due to the different nature of Jeop-
ardy! questions, which are not expressed in the usual
interrogative form.

To demonstrate the benefit of our question clas-
sifier, we integrated it into our Watson by coupling
it with search and candidate generation against spe-
cialized dictionary resources. We show that in end-
to-end evaluations, Watson with kernel-based defi-
nition classification and specialized definition ques-
tion processing achieves statistically significant im-
provement compared to our baseline systems.

In the reminder of this paper, Section 2 describes
Watson by focusing on the problem of definition
question classification, Section 3 describes our mod-
els for such classifiers, Section 4 presents our exper-
iments on QC, whereas Section 5 shows the final im-
pact on Watson. Finally, Section 6 discusses related
work and Section 7 derives the conclusions.

2 Watson: The IBM Jeopardy! System

This section gives a quick overview of Watson and
the problem of classification of definition questions,
which is the focus of this paper.

2.1 Overview

Watson is a massively parallel probabilistic
evidence-based architecture for QA (Ferrucci et
al., 2010). It consists of several major stages for
underlying sub-tasks, including analysis of the
question, retrieval of relevant content, scoring and
ranking of candidate answers, as depicted in Figure
1. In the rest of this section, we provide an overview
of Watson, focusing on the task of answering
definitional questions.
Question Analysis: The first stage of the pipeline,
it applies several analytic components to identify
key characteristics of the question (such as answer

713



Figure 1: Overview of Watson

type, question classes, etc.) used by later stages of
the Watson pipeline. Various general purpose NLP
components, such as a parser and named entity de-
tector, are combined with task-specific modules for
this analysis.

The task-specific analytics include several QC
components, which determine if the question be-
longs to one or more broad “question classes”.
These question classes can influence later stages of
the Watson pipeline. For instance, a question de-
tected as an abbreviation question can invoke spe-
cialized candidate generators to produce possible ex-
pansions of the abbreviated term in the clue. Simi-
larly, the question classes can impact the methods
for answer scoring and the machine learning mod-
els used for ranking candidate answers. The focus
of this paper is on the definition class, which is de-
scribed in the next section.

Hypothesis Generation: Following question anal-
ysis, the Watson pipeline searches its document col-
lection for relevant documents and passages that are
likely to contain the correct answer to the question.
This stage of the pipeline generates search queries
based on question analysis results, and obtains a
ranked list of documents and passages most relevant
to the search queries. A variety of candidate gen-
eration techniques are then applied to the retrieved
results to produce a set of candidate answers.

Information obtained from question analysis can
be used to influence the search and candidate gener-
ation processes. The question classes detected dur-
ing question analysis can focus the search towards
specific subsets of the corpus. Similarly, during can-
didate generation, strategies used to generate the set

of candidate answers are selected based on the de-
tected question classes.
Hypothesis and Evidence Scoring: A wide variety
of answer scorers are then used to gather evidence
supporting each candidate answer as the correct an-
swer to the given question. The scorers include both
context dependent as well as context independent
scorers, relying on various structured and unstruc-
tured resources for their supporting evidence.
Candidate Ranking: Finally, machine learning
models are used to weigh the gathered evidence and
rank the candidate answers. The models generate a
ranked list of answers each with an associated con-
fidence. The system can also choose to refrain from
answering a question if it has low confidence in all
candidates. This stage of the pipeline employs sev-
eral machine learning models specially trained to
handle various types of questions. These models are
trained using selected feature sets based on question
classes and candidate answers are “routed” to the
appropriate model according to the question classes
detected during question analysis.

2.2 Answering Definition Questions
Among the many question classes that Watson iden-
tifies and leverages for special processing, of partic-
ular interest for this paper is the class we refer to
as definition questions. These are questions whose
clue texts contain one or more definitions of the cor-
rect answer. For instance, in example (3), the main
clause in the question corresponds to a dictionary
definition of the correct answer (annuity). Looking
up this definition in dictionary resources could en-
able us to answer this question correctly and with
high confidence. This suggests that special process-

714



ing of such definition questions could allow us to
hone in on the correct answer through processes dif-
ferent from those used for other types of questions.

This paper explores strategies for definition ques-
tion processing to improve overall question answer-
ing performance. A key challenge we have to ad-
dress is that of accurate recognition of such ques-
tions. Given an input question the Watson question
analysis stage uses a definition question recognizer
to detect this specific class of questions. We explore
several approaches for recognition, including a rule
based approach and a variety of statistical models.

Questions that are recognized as definition ques-
tions invoke search processes targeted towards
dictionary-like sources in our system. We use a va-
riety of such sources, such as standard English dic-
tionaries, Wiktionary, WordNet, etc. After gather-
ing supporting evidence for candidate answers ex-
tracted from these sources, our system routes the
candidates to definition-specific candidate ranking
models, which have been trained with selected fea-
ture sets.

The following sections present a description and
evaluation of our approach for identifying and an-
swering definition questions.

3 Kernel Models for Question
Classification

Previous work (Zhang and Lee, 2003; Suzuki et al.,
2003; Blunsom et al., 2006; Moschitti et al., 2007)
as shown that syntactic structures are essential for
QC. Given the novelty of both the domain and the
type of our classification items, we rely on kernel
methods to study and design effective representa-
tions. Indeed, these are excellent tools for auto-
matic feature engineering, especially for unknown
tasks and domains. Our approach consists of using
SVMs and kernels for structured data applied to sev-
eral types of structural lexical, syntactic and shallow
semantic information.

3.1 Tree and Sequence Kernels

Kernel functions are implicit scalar products be-
tween data examples (i.e. questions in our case)
in the very high dimensional space of substructures,
where each of the latter is a component of the im-
plicit vectors associated with the examples.

ROOT

SBARQ

WHADVP

WRB

When

S

VP

VBN

hit

PP

IN

by

NP

NNS

electrons

,

,

NP

DT

a

NN

phosphor

VP

VBZ

gives

PRP

RP

off

NP

NP

JJ

electromagnetic

NN

energy

PP

IN

in

NP

DT

this

NN

form

1

Figure 2: Constituency Tree

PASS

P

A1

phosphor

A0

electron

PR

hit

P

PR

energy

A2

electromag.

A1

phosphor

P

A1

energy

PR

give

AM-TMP

hit

A0

phosphor

ROOT

VBZ

OBJ

NN

NMOD

IN

PMOD

NN

formNMOD

DT

this

in

energyNMOD

JJ

electromag.

PRT

RP

off

givesSBJ

NN

phosphorNMOD

DT

a

P

,

TMP

VBN

LGS

IN

PMOD

NNS

electrons

by

hitTMP

WRB

when

1

Figure 3: Dependency Tree

negative mistake STK, ok PTK

NP

ADJP

JJ

conceited

CC

or

JJ

arrogant

NP

NN

meaning

NN

adjective

NN

5-letter

NN

fowl

positive mistake STK, ok PTK

NP

VP

PP

NP

NN

field

VBG

playing

DT

a

IN

on

VBN

used

NP

NN

grass

JJ

green

JJ

artificial

NP

VP

PP

NP

NN

canal

DT

a

IN

on

VBN

used

NP

NN

boat

JJ

flat-bottomed

DT

a

PASS

P

A1

phosphor

A0

electron

PR

hit

P

PR

energy

AM-MNR

electromag.

A1

phosphor

P

A1

energy

PR

give

AM-TMP

hit

A0

phosphor

1

Figure 4: A tree encoding a Predicate Argument Structure Set

Although several kernels for structured data have
been developed (see Section 6), the main distinc-
tions in terms of feature spaces is given by the fol-
lowing three different kernels:
• Sequence Kernels (SK); we implemented the
discontinuous string kernels described in (Shawe-
Taylor and Cristianini, 2004). This allows for rep-
resenting a string of symbols in terms of its possi-
ble substrings with gaps, i.e. an arbitrary number of
symbols can be skipped during the generation of a
substring. The symbols we used in the sequential de-
scriptions of questions are words and part-of-speech
tags (in two separate sequences). Consequently, all
possible multiwords with gaps are features of the im-
plicitly generated vector space.

715



• Syntactic Tree Kernel (STK) (Collins and Duffy,
2002) applied to constituency parse trees. This gen-
erates all possible tree fragments as features with
the conditions that sibling nodes from the original
trees cannot be separated. In other words, substruc-
tures are composed by atomic building blocks cor-
responding to nodes along with all their direct chil-
dren. These, in case of a syntactic parse tree, are
complete production rules of the associated parser
grammar2.
• Partial Tree Kernel (PTK) (Moschitti, 2006) ap-
plied to both constituency and dependency parse
trees. This generates all possible tree fragments, as
above, but sibling nodes can be separated (so they
can be part of different tree fragments). In other
words, a fragment is any possible tree path, from
whose nodes other tree paths can depart. Conse-
quently, an extremely rich feature space is gener-
ated. Of course, PTK subsumes STK but sometimes
the latter provides more effective solutions as the
number of irrelevant features is smaller as well.
When applied to sequences and tree structures, the
kernels discussed above produce many different
kinds of features. Therefore, the design of appro-
priate syntactic/semantic structures determines the
representational power of the kernels. Hereafter, we
show the models we used.

3.2 Syntactic Semantic Structures

We applied the above kernels to different structures.
These can be divided in sequences of words (WS)
and part of speech tags (PS) and different kinds of
trees. For example, given the non-definition Jeop-
ardy! question:

(5) GENERAL SCIENCE: When hit by elec-
trons, a phosphor gives off electromag-
netic energy in this form. (answer: light
or photons),

we use the following sequences:

WS: [when][hit][by][electrons][,][a][phosphor][gives]
[off][electromagnetic][energy][in][this][form]

PS: [wrb][vbn][in][nns][,][dt][nn][vbz][rp][jj][nn][in]
[dt][nn]

Additionally, we use constituency trees (CTs), see

2From here the name syntactic tree kernels

Figure 2 and dependency structures converted into
the dependency trees (DTs), e.g. shown in Figure
3. Note that, the POS-tags are central nodes, the
grammatical relation label is added as a father
node and all the relations with the other nodes are
described by means of the connecting edges. Words
are considered additional children of the POS-tag
nodes (in this case the connecting edge just serves
to add a lexical feature to the target POS-tag node).

Finally, we also use predicate argument structures
generated by verbal and nominal relations accord-
ing to PropBank (Palmer et al., 2005) and NomBank
(Meyers et al., 2004). Given the target sentence, the
set of its predicates are extracted and converted into
a forest, then a fake root node, PAS, is used to con-
nect these trees. For example, Figure 4 illustrates a
Predicate Argument Structures Set (PASS) encoding
two relations, give and hit, as well as the nominaliza-
tion energy along with all their arguments.

4 Experiments on Definition Question
Classification

In these experiments, we study the role of kernel
technology for the design of accurate classification
of definition questions. We build several classifiers
based on SVMs and kernel methods. Each classi-
fier uses advanced syntactic/semantic structural fea-
tures and their combination. We carry out an exten-
sive comparison in terms of F1 between the different
models on the Jeopardy! datasets.

4.1 Experimental Setup
Corpus: the data for our QC experiments consists
of a randomly selected set of 33 Jeopardy! games3.
These questions were manually annotated based on
whether or not they are considered definitional. This
resulted in 306 definition and 4964 non-definition
clues. Each test set is stored in a separate file con-
sisting of one line per question, which contains tab-
separated clue information and the Jeopardy! cate-
gory, e.g. INVEST in example (4).
Tools: for SVM learning, we used the SVMLight-
TK software4, which includes structural kernels in
SVMLight (Joachims, 1999)5. For generating con-

3Past Jeopardy! games can be downloaded from
http://www.j-archive.com.

4Available at http://dit.unitn.it/∼moschitt
5http://svmlight.joachims.org

716



stituency trees, we used the Charniak parser (Char-
niak, 2000). We also used the syntactic–semantic
parser by Johansson and Nugues (2008) to gener-
ate dependency trees (Mel’čuk, 1988) and predicate
argument trees according to the PropBank (Palmer
et al., 2005) and NomBank (Meyers et al., 2004)
frameworks.
Baseline Model: the first model that we used as a
baseline is a rule-based classifier (RBC). The RBC
leverages a set of rules that matches against lexical
and syntactic information in the clue to make a bi-
nary decision on whether or not the clue is consid-
ered definitional. The rule set was manually devel-
oped by a human expert, and consists of rules that
attempt to identify roughly 70 different constructs
in the clues. For instance, one of the rules matches
the parse tree structure for ”It’s X or Y”, which will
identify example (1) as a definition question.
Kernel Models: we apply the kernels described
in Section 3 to the structures extracted from Jeop-
ardy! clues. In particular, we design the following
models: BOW, i.e. linear kernel on bag-of-words
from the clues; WSK, PSK and CSK, i.e. SK applied
to the word and POS-tag sequences from the clues,
and the word sequence taken from the question cat-
egories, respectively; STK-CT, i.e. STK applied to
CTs of the clue; PTK-CT and PTK-DT, i.e. PTK
applied to CTs and DTs of the clues, respectively;
PASS, i.e. PTK applied to the Predicate Argument
Structure Set extracted from the clues; and RBC, i.e.
a linear kernel applied to the vector only constituted
by the 1/0 output of RBC.
Learning Setting: there is no particular parameteri-
zation. Since there is an imbalance between positive
and negative examples, we used a Precision/Recall
trade-off parameter in SVM-Light-TK equal to 5.6

Measures: the performance is measured with Pre-
cision, Recall and F1-measure. We estimated them
by means of Leave-One-Out7 (LOO) on the question
set.

4.2 Results and Discussion
Table 1 shows the performance obtained using dif-
ferent kernels (feature spaces) with SVMs. We note

6We have selected 5 as a reasonable value, which kept bal-
anced Precision and Recall on a validation set.

7LOO applied to a corpus of N instances consists in training
on N − 1 examples and testing on the single held-out example.
This process is repeated for all instances.

Kernel Space Prec. Rec. F1
RBC 28.27 70.59 40.38
BOW 47.67 46.73 47.20
WSK 47.11 50.65 48.82
STK-CT 50.51 32.35 39.44
PTK-CT 47.84 57.84 52.37
PTK-DT 44.81 57.84 50.50
PASS 33.50 21.90 26.49
PSK 39.88 45.10 42.33
CSK 39.07 77.12 51.86

Table 1: Kernel performance using leave-one-out cross-
validation.

that: first, RBC has good Recall but poor Precision.
This is interesting since, on one hand, these results
validate the complexity of the task: in order to cap-
ture the large variability of the positive examples,
the rules developed by a skilled human designer are
unable to be sufficiently precise to limit the recog-
nition to those examples. On the other hand, RBC,
being a rather different approach from SVMs, can be
successfully exploited in a joint model with them.

Second, BOW yields better F1 than RBC but it
does not generalize well since its F1 is still low.
When n-grams are also added to the model by
means of WSK, the F1 improves by about 1.5 ab-
solute points. As already shown in (Zhang and Lee,
2003; Moschitti et al., 2007), syntactic structures are
needed to improve generalization.

Third, surprisingly with respect to previous work,
STK applied to CT8 provides accuracy lower than
BOW, about 8 absolute points. The reason is due to
the different nature of the Jeopardy! questions: large
syntactic variability reduces the probability of find-
ing general and well formed patterns, i.e. structures
generated by entire production rules. This suggests
that PTK, which can capture patterns derived from
partial production rules, can be more effective. In-
deed, PTK-CT achieves the highest F1, outperform-
ing WSK also when used with a different syntactic
paradigm, i.e. PTK-DT.

Next, PSK and PASS provide a lower accuracy
but they may be useful in kernel combinations as
they can complement the information captured by
the other models. Interestingly, CSK alone is rather
effective for classifying definition questions. We be-

8Applying it to DT does not make much sense as already
pointed out in (Moschitti, 2006).

717



 

Figure 5: Similarity according to PTK and STK

lieve this is because definition questions are some-
times clustered into categories such as 4-LETTER
WORDS or BEGINS WITH ”B”.

Moreover, we carried out qualitative error analy-
sis on the PTK and STK outcome, which supported
our initial hypothesis. Let us consider the bottom
tree in Figure 5 in the training set. The top tree is
a test example correctly classified by PTK but in-
correctly classified by STK. The dashed line in the
top tree contains the largest subtree matched by PTK
(against the bottom tree), whereas the dashed line in
the bottom tree indicates the largest subtree matched
by STK (against the top tree). As the figure shows,
PTK can exploit a larger number of partial patterns.

Finally, the above points suggest that different
kernels produce complementary information. It is
thus promising to experiment with their combina-
tions. The joint models can be simply built by
summing kernel functions together. The results are
shown in Table 2. We note that: (i) CSK comple-
ments the WSK information, achieving a substan-
tially better result, i.e. 62.95; (ii) PTK-CT+CSK
performs even better than WSK+CSK (as PTK out-
performs WSK); and (iii) adding RBC improves
further on the above combinations, i.e. 68.11 and
67.32, respectively. This evidently demonstrates
that RBC captures complementary information. Fi-
nally, more complex kernels, especially the overall
kernel summation, do not seem to improve the per-

Kernel Space Prec. Rec. F1
WSK+CSK 70.00 57.19 62.95
PTK-CT+CSK 69.43 60.13 64.45
PTK-CT+WSK+CSK 68.59 62.09 65.18
CSK+RBC 47.80 74.51 58.23
PTK-CT+CSK+RBC 59.33 74.84 65.79
BOW+CSK+RBC 60.65 73.53 66.47
PTK-CT+WSK+CSK+RBC 67.66 66.99 67.32
PTK-CT+PASS+CSK+RBC 62.46 71.24 66.56
WSK+CSK+RBC 69.26 66.99 68.11
ALL 61.42 67.65 64.38

Table 2: Performance of Kernel Combinations using
leave-one-out cross-validation.

formance. This is also confirmed by the PASS re-
sults derived in (Moschitti et al., 2007) on TREC
QC.

5 Experiments on the Jeopardy System

Since the kernel-based classifiers perform substan-
tially better than RBC, we incorporate the PTK-
CT+WSK+CSK model9 into Watson for definition
classification and evaluated the QA performance
against two baseline systems. For the end-to-end ex-
periments, we used Watson’s English Slot Grammar
parser (McCord, 1980) to generate the constituency
trees. The component level evaluation shows that
we achieved comparable performance as previously
discussed with ESG.

5.1 Experimental Setup

We integrated the classifier into the question analy-
sis module, and incorporated additional components
to search against dictionary resources and extract
candidate answers from these search results when a
question is classified as definitional. In the final ma-
chine learning models, a separate model is trained
for definition questions to enable scoring tailored to
the specific characteristics of those questions.

Based on our manually annotated gold standard,
less than 10% of Jeopardy! questions are classified
as definition questions. Due to their relatively low
frequency we conduct two types of evaluations. The
first is definition-only evaluation, in which we apply
our definition question classifier to identify a large

9Since we aim to compare a purely statistical approach to
the rule-based approach, we did not experiment with the model
that uses RBC as a feature in our end-to-end experiments.

718



set of definition questions and evaluate the end-to-
end system’s performance on this large set of ques-
tions. These results enable us to draw statistically
significant conclusions about our approach to ad-
dressing definition questions.

The second type of evaluation is game-based
evaluation, which assesses the impact of our defi-
nition question processing on Watson performance
while preserving the natural distribution of these
question types in Jeopardy! data. Game-based eval-
uations situate the system’s performance on defini-
tion questions relative to other types of questions,
and enable us to gauge the component’s contribu-
tions in a game-based setting.

For both evaluation settings, three configurations
of Watson are used as follows:

• the NoDef system, in which Watson is config-
ured without definition classification and pro-
cessing, thereby treating all definition ques-
tions as regular factoid questions;

• the StatDef system, which leverages the sta-
tistical classifier and subsequent definition spe-
cific search and candidate generation compo-
nents as described above; and

• the RuleDef system, in which Watson adopts
RBC and employs the same additional defini-
tion search and candidate generation compo-
nents as the StatDef system.

For the definition-only evaluation, we selected all
questions recognized as definitional by the statistical
classifier from roughly 1000 unseen games (60000
questions), resulting in a test set of 1606 questions.
Due to the size of the initial set, it is impractical to
manually create a gold standard for measuring Pre-
cision and Recall of the classifier. Instead, we com-
pare the StatDef system against the NoDef on these
1606 questions using two metrics: accuracy, defined
as the percentage of questions correctly answered,
and p@70, the system’s Precision when answering
only the top 70% most confident questions. P@70 is
an important metric in Jeopardy! game play as well
as in real world applications where the system may
refrain from answering a question when it is not con-
fident about any of its answers. Since RBC identifies
significantly more definition questions, we started

NoDef StatDef NoDef RuleDef
# Questions 1606 1606 1875 1875
Accuracy 63.76% 65.57% 56.64% 57.51%
P@70 82.22% 84.53% 72.73% 74.87%

Table 3: Definition-Only Evaluation Results

with an initial set of roughly 300 games, from which
the RBC identified 1875 questions as definitional.
We compared the RuleDef system’s performance on
these questions against the NoDef baseline using the
accuracy and p@70 metrics.

For the game-based evaluation, we randomly se-
lected 66 unseen Jeopardy! games, consisting of
3546 questions after excluding audio/visual ques-
tions.10 We contrast the StatDef system perfor-
mance against that of NoDef and RuleDef along
several dimensions: accuracy and p@70, described
above, as well as earnings, the average amount of
money earned for each game.

5.2 Definition-Only Evaluation

For the definition-only evaluation, we compared the
StatDef system against the NoDef system on a set of
1606 questions that the StatDef system classified as
definitional. The results are shown in the first two
columns in Table 3. To contrast the gain obtained
by the StatDef system against that achieved by the
RuleDef system, we ran the RuleDef system over
the 1875 questions identified as definitional by the
rule-based classifier. We contrast the RuleDef sys-
tem performance with that of the NoDef system, as
shown in the last two columns in Table 3.

Our results show that based on both evaluation
metrics, StatDef improved upon the NoDef baseline
more than RuleDef improved on the same baseline
system. Furthermore, for the accuracy metric where
all samples are paired and independent, the differ-
ence in performance between the StatDef and NoDef
systems is statistically significant at p<0.05, while
that between the RuleDef and NoDef systems is not.

5.3 Game-Based Evaluation

The game-based evaluation was carried out on 66
unseen games (roughly 3500 questions). Of these

10Audio/visual questions are those accompanied by either an
image or an audio clip. The text portions of these questions are
often insufficient for identifying the correct answers.

719



# Def Q’s Accuracy P@70 Earnings
NoDef 0 69.71% 86.79% $24,818
RuleDef 480 69.23% 86.31% $24,397
StatDef 131 69.85% 87.19% $25,109

Table 4: Game-Based Evaluation Results

questions, the StatDef system classified 131 of them
as definitional while the RuleDef system identified
480 definition questions. Both systems were com-
pared against the NoDef system using the accuracy,
p@70, and earnings metric computed over all ques-
tions, as shown in Table 4.

Our results show that even though in the
definition-only evaluation both the RuleDef and
StatDef systems outperformed the NoDef baseline,
in our game-based evaluation, the RuleDef system
performed worse than the NoDef baseline. The low-
ered performance is due to the fact that the Preci-
sion of the RBC is much lower than that of the sta-
tistical classifier, and the special definition process-
ing applied to questions that are erroneously clas-
sified as definitional was harmful. Our evaluation
of this false positive set showed that its accuracy
dropped by 6% compared to the NoDef system. On
the other hand, the StatDef system outperformed the
two other systems, and its accuracy improvement
upon the RuleDef system is statistically significant
at p<0.05.

6 Related Work
Our paper studies the use of advanced representa-
tion for QC in the Jeopardy! domain. As previously
mentioned Jeopardy! questions are stated as affir-
mative sentences, which are different from the typ-
ical QA questions. For the design of our models,
we have carefully taken into account previous work.
This shows that semantics and syntax are essential
to retrieve precise answers, e.g (Hickl et al., 2006;
Voorhees, 2004; Small et al., 2004).

We focus on definition questions, which typically
require more complex processing than factoid ques-
tions (Blair-Goldensohn et al., 2004; Chen et al.,
2006; Shen and Lapata, 2007; Bilotti et al., 2007;
Moschitti et al., 2007; Surdeanu et al., 2008; Echi-
habi and Marcu, 2003). For example, language mod-
els were applied to definitional QA in (Cui et al.,
2005) to learn soft pattern models based on bigrams.
Other related work, such as (Sasaki, 2005; Suzuki

et al., 2002), was also very tied to bag-of-words
features. Predicate argument structures have been
mainly used for reranking (Shen and Lapata, 2007;
Bilotti et al., 2007; Moschitti et al., 2007; Surdeanu
et al., 2008).

Our work and methods are similar to (Zhang and
Lee, 2003; Moschitti et al., 2007), which achieved
the state-of-the-art in QC by applying SVMs along
with STK-CT. The results were derived by experi-
menting with a TREC dataset11(Li and Roth, 2002),
reaching an accuracy of 91.8%. However, such data
refers to typical instances from QA, whose syntactic
patterns can be easily generalized by STK. In con-
trast, we have shown that STK-CT is not effective
for our domain, as it presents very innovative ele-
ments: questions in affirmative and highly variable
format. Thus, we employed new methods such as
PTK, dependency structures, multiple sequence ker-
nels including category information and many com-
binations.

Regarding the use of Kernel Methods, there is
a considerably large body of work in Natural Lan-
guage Processing, e.g. regarding syntactic parsing
(Collins and Duffy, 2002; Kudo et al., 2005; Shen
et al., 2003; Kudo and Matsumoto, 2003; Titov and
Henderson, 2006; Toutanova et al., 2004), named
entity recognition and chunking (Cumby and Roth,
2003; Daumé III and Marcu, 2004), relation extrac-
tion (Zelenko et al., 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005; Zhang et al.,
2005; Bunescu, 2007; Nguyen et al., 2009a), text
categorization (Cancedda et al., 2003), word sense
disambiguation (Gliozzo et al., 2005) and seman-
tic role labeling (SRL), e.g. (Kazama and Torisawa,
2005; Che et al., 2006a; Moschitti et al., 2008).

However, ours is the first study on the use of sev-
eral combinations of kernels applied to several struc-
tures on very complex data from the Jeopardy! do-
main.

7 Final Remarks and Conclusion
In this paper we have experimented with advanced
structural kernels applied to several kinds of syntac-
tic/semantic linguistic structures for the classifica-
tion of questions in a new application domain, i.e.
Jeopardy!. Our findings are summarized hereafter:

11Available at http://cogcomp.cs.illinois.
edu/Data/QA/QC/

720



First, it should be noted that basic kernels, such
as STK, PTK and SK, when applied to new repre-
sentations, i.e. syntactic/semantic structures, con-
stitute new kernels. Thus structural representations
play a major role and, from this perspective, our pa-
per makes a significant contribution.

Second, the experimental results show that the
higher variability of Jeopardy! questions prevents us
from achieving generalization with typical syntactic
patterns even if they are derived by powerful meth-
ods such as STK. In contrast, partial patterns, such
as those provided by PTK applied to constituency
(or dependency) trees, prove to be effective.

In particular, STK has been considered as the best
kernel for exploiting syntactic information in con-
stituency trees, e.g. it is state-of-the-art in: QC
(Zhang and Lee, 2003; Moschitti et al., 2007; Mos-
chitti, 2008); SRL, (Moschitti et al., 2008; Mos-
chitti et al., 2005; Che et al., 2006b); pronominal
coreference resolution (Yang et al., 2006; Versley
et al., 2008) and Relation Extraction (Zhang et al.,
2006; Nguyen et al., 2009b). We showed that, in
the complex domain of Jeopardy!, STK surprisingly
provides low accuracy whereas PTK is rather ef-
fective and greatly outperforms STK. We have also
provided an explanation of such behavior by means
of error analysis: in contrast with traditional ques-
tion classification, which focuses on basic syntactic
patterns (e.g. ”what”, ”where”, ”who” and ”how”).
Figure 5 shows that PTK captures partial patterns
that are important for more complex questions like
those in Jeopardy!

Third, we derived other interesting findings for
NLP related to this novel domain, e.g.: (i) the im-
pact of dependency trees is similar to the one of
constituency trees. (ii) A simple computational rep-
resentation of shallow semantics, i.e. PASS (Mos-
chitti, 2008), does not work in Jeopardy!. (iii) Se-
quence kernels on category cues, i.e., higher level of
lexical semantics, improve question classification.
(iv) RBC jointly used with statistical approaches is
helpful to tackle the Jeopardy! complexity.

Next, our kernel models improve up to 20 abso-
lute percent points over n-grams based approaches,
reaching a significant accuracy of about 70%. Wat-
son, exploiting such a classifier, improved previ-
ous versions using RBC and no definition classifica-
tion both in definition-only evaluations and in game-

based evaluations.
Finally, we point out that:

• Jeopardy! has a variety of different special ques-
tion types that are handled differently. We focus on
kernel methods for definition question for two rea-
sons. First, their recognition relies heavily on parse
structures and is therefore more amenable to the ap-
proach proposed in the paper than the recognition
of other question types. Second, definition is by far
the most frequent special question type in Jeopardy!;
therefore, we can obtain sufficient data for training
and testing.

• We were unable to address the whole QC prob-
lem using a statistical model due to the lack of suffi-
cient training data for most special question classes.
Furthermore, we focused only on the definition clas-
sification and its impact on system performance due
to space reasons.

• Our RBC has a rather imbalanced trade-off be-
tween Precision and Recall. This may not be the
best operating point, but the optimal point is diffi-
cult to obtain empirically for an RBC, which is a
strong motivation of the work in this paper. We ex-
perimented with tuning the trade-off between Preci-
sion and Recall with the RBC, but since RBC uses
hand-crafted rules and does not have a parameter for
that, ultimately the statistical approach proved more
effective.

In future work, we plan to extend the current re-
search by investigating models capable of exploit-
ing predicate argument structures for question clas-
sification and answer reranking. The use of syntac-
tic/semantic kernels is a promising research direc-
tion (Basili et al., 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). In this
perspective kernel learning is a very interesting re-
search line, considering the complexity of represen-
tation and classification problems in which our ker-
nels operate.

Acknowledgements

This work has been supported by the IBM’s Open
Collaboration Research (OCR) awards program. We
are deeply in debt with Richard Johansson, who pro-
duced the earlier syntactic/semantic representations
of the Jeopardy! questions from the text format.

721



References
Kisuh Ahn, Johan Bos, Stephen Clark, James R. Cur-

ran, Tiphaine Dalmas, Jochen L. Leidner, Matthew B.
Smillie, and Bonnie Webber. 2004. Question an-
swering with qed and wee at trec-2004. In E. M.
Voorhees and L. P. Buckland, editors, The Thirteenth
Text REtrieval Conference, TREC 2004, pages 595–
599, Gaitersburg, MD.

Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of CoNLL-
2005, pages 1–8, Ann Arbor, Michigan. Association
for Computational Linguistics.

M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of ACM SIGIR.

S. Blair-Goldensohn, K. R. McKeown, and A. H.
Schlaikjer. 2004. Answering definitional questions:
A hybrid approach. In M. Maybury, editor, Proceed-
ings of AAAI 2004. AAAI Press.

Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.

Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ’07.

Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear models.
In SIGIR ’06: Proceedings of the 29th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 615–616, New
York, NY, USA. ACM.

Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724–731,
Vancouver, British Columbia, Canada, October.

Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.

Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059–1082.

E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL.

Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006a. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 73–
80, Sydney, Australia, July. Association for Compu-
tational Linguistics.

Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006b. A hybrid convolution tree kernel for semantic
role labeling. In Proceedings of the COLING/ACL on

Main conference poster sessions, COLING-ACL ’06,
pages 73–80, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Y. Chen, M. Zhou, and S. Wang. 2006. Reranking an-
swers from definitional QA using language models. In
Proceedings of ACL.

Charles Clarke, Gordon Cormack, and Thomas Lynam.
2001. Exploiting redundancy in question answering.
In Proceedings of the 24th SIGIR Conference, pages
358–365.

Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL’02.

H. Cui, M. Kan, and T. Chua. 2005. Generic soft pattern
models for definitional QA. In Proceedings of SIGIR,
Salvador, Brazil. ACM.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423–429, Barcelona, Spain, July.

Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.

Hal Daumé III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP’04.

A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proceedings of ACL.

David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. 2010. Building watson:
An overview of the deepqa project. AI Magazine,
31(3).

Danilo Giampiccolo, Pamela Froner, Anselmo Peñas,
Christelle Ayache, Dan Cristea, Valentin Jijkoun,
Petya Osenova, Paulo Rocha, Bogdan Sacaleanu, and
Richard Suteliffe. 2007. Overview of the CLEF 2007
multilingual question anwering track. In Proceedings
of the Cross Language Evaluation Forum.

Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL’05, pages 403–410.

A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and
B. Rink. 2006. Question answering with lcc chaucer
at trec 2006. In Proceedings of TREC.

Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods – Sup-
port Vector Learning, 13.

Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with PropBank and NomBank. In CoNLL 2008:
Proceedings of the Twelfth Conference on Natural
Language Learning, pages 183–187, Manchester,
United Kingdom.

722



Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings of
the Workshop on Deep Linguistic Processing, DeepLP
’07, pages 41–48, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Jun’ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP’05.

Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL’03.

Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL’05.

Shyong Lam, David Pennock, Dan Cosley, and Steve
Lawrence. 2003. 1 billion pages = 1 milllion dollars?
mining the web to pay ”who wants to be a millionaire?
In Proceedings of the 19th Conference on Uncertainty
in AI.

X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL.

Michael C. McCord. 1980. Slot grammars. Computa-
tional Linguistics.

Igor A. Mel’čuk. 1988. Dependency Syntax: Theory and
Practice. State University Press of New York, Albany.

Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24–31, Boston, United
States.

Alessandro Moschitti, Bonaventura Coppola, Ana-Maria
Giuglea, and Roberto Basili. 2005. Hierarchical se-
mantic role labeling. In CoNLL 2005 shared task.

Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL’07.

Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193–224.

Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML’06, pages 318–329.

Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of CIKM ’08, NY, USA.

Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009a. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378–1387, Singapore, August.

Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009b. Convolution kernels on

constituent, dependency and sequential structures for
relation extraction. In EMNLP ’09: Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1378–1387, Morristown,
NJ, USA. Association for Computational Linguistics.

Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71–105.

Yutaka Sasaki, Chuan-Jie Lin, Kuang-hua Chen, and
Hsin-Hsi Chen. 2007. Overview of the NTCIR-6
cross-lingual question answering (CLQA) task. In
Proceedings of the 6th NTCIR Workshop on Evalua-
tion of Information Access Technologies.

Y. Sasaki. 2005. Question answering as question-biased
term extraction: A new approach toward multilingual
qa. In Proceedings of ACL, pages 215–222.

John Shawe-Taylor and Nello Cristianini. 2004. LaTeX
User’s Guide and Document Reference Manual. Ker-
nel Methods for Pattern Analysis, Cambridge Univer-
sity Press.

D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL.

L. Shen, A. Sarkar, and A. Joshi. 2003. Using LTAG
Based Features in Parse Reranking. In Proceedings of
EMNLP, Sapporo, Japan.

S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin,
N. Shimizu, P. Kantor, D. Kelly, and N. Wacholder.
2004. Hitiqa: Towards analytical question answering.
In Proceedings of COLING.

M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online QA collec-
tions. In Proceedings of ACL-HLT, Columbus, Ohio.

J. Suzuki, Y. Sasaki, and E. Maeda. 2002. Svm an-
swer selection for open-domain question answering.
In Proceedings of Coling, pages 974–980.

Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku
Maeda. 2003. Question classification using hdag ker-
nel. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answering,
pages 61–68, Sapporo, Japan, July. Association for
Computational Linguistics.

Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.

Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.

Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In The 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing’08), Manchester, England.

723



Ellen M. Voorhees and Hoa Trang Dang. 2006.
Overview of the TREC 2005 question answering track.
In Proceedings of the TREC 2005 Conference.

E. M. Voorhees. 2004. Overview of the trec 2004 ques-
tion answering track. In Proceedings of TREC 2004.

Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proc. COLING-ACL 06.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181–201.

Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26–32. ACM Press.

Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP’2005, Lecture Notes in Computer Science (LNCS
3651), pages 378–389, Jeju Island, South Korea.

Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.

724


