



















































Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1437‚Äì1447
Melbourne, Australia, July 15 - 20, 2018. c¬©2018 Association for Computational Linguistics

1437

Sequicity: Simplifying Task-oriented Dialogue Systems with Single
Sequence-to-Sequence Architectures

Wenqiang Lei‚Ä°‚àó, Xisen Jin¬ß‚àó, Zhaochun Ren‚Ä†, Xiangnan He‚Ä°, Min-Yen Kan‚Ä°, Dawei Yin‚Ä†
‚Ä°National University of Singapore, Singapore
¬ßFudan University, Shanghai, China

‚Ä†Data Science Lab, JD.com, Beijing, China
{wenqianglei,xisenjin}@gmail.com renzhaochun@jd.com

kanmy@comp.nus.edu.sg xiangnanhe@gmail.com yindawei@acm.org

Abstract

Existing solutions to task-oriented dia-
logue systems follow pipeline designs
which introduce architectural complex-
ity and fragility. We propose a novel,
holistic, extendable framework based on
a single sequence-to-sequence (seq2seq)
model which can be optimized with su-
pervised or reinforcement learning. A
key contribution is that we design text
spans named belief spans to track dia-
logue believes, allowing task-oriented dia-
logue systems to be modeled in a seq2seq
way. Based on this, we propose a sim-
plistic Two Stage CopyNet instantiation
which demonstrates good scalability: sig-
nificantly reducing model complexity in
terms of number of parameters and train-
ing time by an order of magnitude. It
significantly outperforms state-of-the-art
pipeline-based methods on two datasets
and retains a satisfactory entity match rate
on out-of-vocabulary (OOV) cases where
pipeline-designed competitors totally fail.

1 Introduction

The challenge of achieving both task comple-
tion and human-like response generation for task-
oriented dialogue systems is gaining research in-
terest. Wen et al. (2017b, 2016a, 2017a) pioneered
a set of models to address this challenge. Their
proposed architectures follow traditional pipeline
designs, where the belief tracking component is
the key component (Chen et al., 2017).

In the current paradigm, such a belief tracker
builds a complex multi-class classifier for each

* Work performed during an internship at Data Science
Lab, JD.com.

slot (See ¬ß3.2) which can suffer from high com-
plexity, especially when the number of slots and
their values grow. Since all the possible slot values
have to be pre-defined as classification labels, such
trackers also cannot handle the requests that have
out-of-vocabulary (OOV) slot values. Moreover,
the belief tracker requires delexicalization, i.e., re-
placing slot values with their slot names in utter-
ances (MrksÃåicÃÅ et al., 2017). It does not scale well,
due to the lexical diversity. The belief tracker also
needs to be pre-trained, making the models unre-
alistic for end-to-end training (Eric and Manning,
2017a). While Eric and Manning (2017a,b) inves-
tigated building task-oriented dialogue systems by
using a seq2seq model, unfortunately, their meth-
ods are rather preliminary and do not perform well
in either task completion or response generation,
due to their omission of a belief tracker.

Questioning the basic pipeline architecture, in
this paper, we re-examine the tenets of belief
tracking in light of advances in deep learning. We
introduce the concept of a belief span (bspan), a
text span that tracks the belief states at each turn.
This leads to a new framework, named Sequicity,
with a single seq2seq model. Sequicity decom-
poses the task-oriented dialogue problem into the
generation of bspans and machine responses, con-
verting this problem into a sequence optimization
problem. In practice, Sequicity decodes in two
stages: in the first stage, it decodes a bspan to fa-
cilitate knowledge base (KB) search; in the sec-
ond, it decodes a machine response on the con-
dition of knowledge base search result and the
bspan.

Our method represents a shift in perspective
compared to existing work. Sequicity employs a
single seq2seq model, resulting in a vastly simpli-
fied architecture. Unlike previous approaches with
an overly parameterized delexicalization-based
belief tracker, Sequicity achieves much less train-



1438

ing time, better performance on larger a dataset
and an exceptional ability to handle OOV cases.
Furthermore, Sequicity is a theoretically and aes-
thetically appealing framework, as it achieves true
end-to-end trainability using only one seq2seq
model. As such, Sequicity leverages the rapid
development of seq2seq models (Gehring et al.,
2017; Vaswani et al., 2017; Yu et al., 2017) in de-
veloping solutions to task-oriented dialogue sce-
narios. In our implementation, we improve on
CopyNet (Gu et al., 2016) to instantiate Sequic-
ity framework in this paper, as key words present
in bspans and machine responses recur from previ-
ous utterances. Extensive experiments conducted
on two benchmark datasets verify the effectiveness
of our proposed method.

Our contributions are fourfold: (1) We pro-
pose the Sequicity framework, which handles both
task completion and response generation in a sin-
gle seq2seq model; (2) We present an implemen-
tation of the Sequicity framework, called Two
Stage CopyNet (TSCP), which has fewer number
of parameters and trains faster than state-of-the-art
baselines (Wen et al., 2017b, 2016a, 2017a); (3)
We demonstrate that TSCP significantly outper-
forms state-of-the-art baselines on two large-scale
datasets, inclusive of scenarios involving OOV; (4)
We release source code of TSCP to assist the com-
munity to explore Sequicity1.

2 Related Work

Historically, task-oriented dialog systems have
been built as pipelines of separately trained mod-
ules. A typical pipeline design contains four com-
ponents: 1) a user intent classifier, 2) a belief
tracker, 3) a dialogue policy maker and a 4) re-
sponse generator. User intent detectors classify
user utterances to into one of the pre-defined in-
tents. SVM, CNN and RNN models (Silva et al.,
2011; Hashemi et al., 2016; Shi et al., 2016) per-
form well for intent classification. Belief track-
ers, which keep track of user goals and constraints
every turn (Henderson et al., 2014a,b; Kim et al.,
2017) are the most important component for task
accomplishment. They model the probability dis-
tribution of values over each slot (Lee, 2013).
Dialogue policy makers then generate the next
available system action. Recent experiments sug-
gest that reinforcement learning is a promising
paradigm to accomplish this task (Young et al.,

1http://github.com/WING-NUS/sequicity

2013a; CuayaÃÅhuitl et al., 2015; Liu and Lane,
2017), when state and action spaces are carefully
designed (Young et al., 2010). Finally, in the
response generation stage, pipeline designs usu-
ally pre-define fixed templates where placehold-
ers are filled with slot values at runtime (Dhin-
gra et al., 2017; Williams et al., 2017; Henderson
et al., 2014b,a). However, this causes rather static
responses that could lower user satisfaction. Gen-
erating a fluent, human-like response is considered
a separate topic, typified by the topic of conversa-
tion systems (Li et al., 2015).

3 Preliminaries

3.1 Encoder-Decoder Seq2seq Models

Current seq2seq models adopt encoder‚Äìdecoder
structures. Given a source sequence of tokens
X = x1x2...xn, an encoder network represent X
as hidden states: H(x) = h(x)1 h

(x)
2 ...h

(x)
n . Based

on H(x), a decoder network generates a target se-
quence of tokens Y = y1y2...ym whose likelihood
should be maximized given the training corpus.

As of late, the recurrent neural network with
attention (Att-RNN) is now considered a base-
line encoder‚Äìdecoder architecture. Such networks
employ two (sometimes identical) RNNs, one for
encoding (i.e., generating H(x)) and another for
decoding. Particularly, for decoding yj , the de-
coder RNN takes the embedding yj‚àí1 to gener-
ate a hidden vector h(y)j . Afterwards, the de-
coder attends to X: calculating attention scores
between all h(x)i ‚àà H(x) and h

(y)
j (Eq. (1)), and

then sums all h(x)i , weighted by their correspond-
ing attention scores (Eqs. (2)). The summed result
hÃÉ

(x)
j concatenates h

(y)
j as a single vector which is

mapped into an output space for a softmax oper-
ation (Eq. (3)) to decode the current token:

uij = v
T tanh(W1h

(x)
i + W2h

(y)
j ) (1)

hÃÉ
(x)
j =

n‚àë
i=1

euij‚àë
i e
uij

h
(x)
i (2)

yj = softmax(O

[
hÃÉ

(x)
j

h
(y)
j

]
) (3)

where v ‚àà R1√ól; W1, W2 ‚àà Rl√ód and O ‚àà
R|V |√ód. d is embedding size and V is vocabulary
set and |V | is its size.

http://github.com/WING-NUS/sequicity


1439

3.2 Belief Trackers

In the multi-turn scenarios, a belief tracker is the
key component for task completion as it records
key information from past turns (Wen et al.,
2017b; Henderson et al., 2013, 2014a,b). Early
belief trackers are designed as Bayesian networks
where each node is a dialogue belief state (Paek
and Horvitz, 2000; Young et al., 2013b). Recent
work successfully represents belief trackers as dis-
criminative classifiers (Henderson et al., 2013;
Williams, 2012; Wen et al., 2017b).

Wen et al. (2017b) apply discrimination ap-
proaches (Henderson et al., 2013) to build one
classifier for each slot in their belief tracker. Fol-
lowing the terminology of (Wen et al., 2017b),
a slot can be either informable or requestable,
which have been annotated in CamRes676 and
KVRET. Individually, an informable slot, speci-
fied by user utterances in previous turns, is set to
a constraint for knowledge base search; whereas
a requestable slot records the user‚Äôs need in the
current dialogue. As an example of belief track-
ers in CamRes676, food type is an informable
slot, and a set of food types is also predefined (e.g.,
Italian) as corresponding slot values. In (Wen
et al., 2017b), the informable slot food type is
recognized by a classifier, which takes user utter-
ances as input to predict if and which type of food
should be activated, while the requestable slot of
address is a binary variable. address will be
set to true if the slot is requested by the user.

4 Method

We now describe the Sequicity framework, by first
explaining the core concept of bspans. We then
instantiate the Sequicity framework with our intro-
duction of an improved CopyNet (Gu et al., 2016).

4.1 Belief Spans for Belief Tracking

The core of belief tracking is keeping track of in-
formable and requestable slot values when a di-
alogue progresses. In the era of pipeline-based
methods, supervised classification is a straightfor-
ward solution. However, we observe that this tra-
ditional architecture can be updated by applying
seq2seq models directly to the problem. In con-
trast to (Wen et al., 2017b) which treats slot val-
ues as classification labels, we record them in a
text span, to be decoded by the model. This lever-
ages the state-of-the-art neural seq2seq models to
learn and dynamically generate them. Specifically,

our bspan has an information field (marked with
<Inf></Inf>) to store values of informable
slots since only values are important for knowl-
edge base search. Bspans can also feature a re-
quested field (marked with <Req></Req>), stor-
ing requestable slot names if the corresponding
value is True.

At turn t, given the user utterance Ut, we show
an example of both bspan Bt and machine re-
sponseRt generation in Figure 1, where annotated
slot values at each turn are decoded into bspans.
B1 contains an information slot Italian be-
cause the user stated ‚ÄúItalian food‚Äù in U1. During
the second turn, the user adds an additional con-
straint cheap resulting in two slot values in B2‚Äôs
information field. In the third turn, the user further
asks for the restaurant‚Äôs phone and address, which
are stored in requested slots of B3.

Our bspan solution is concise: it simplifies
multiple sophisticated classifiers with a single se-
quence model. Furthermore, it can be viewed as
an explicit data structure that expedite knowledge
base search as its format is fixed: following (Wen
et al., 2017b), we use the informable slots values
directly for matching fields of entries in databases.

4.2 The Sequicity Framework

We make a key observation that at turn t, a sys-
tem only needs to refer to Bt‚àí1, Rt‚àí1 and Ut to
generate a new belief span Bt and machine re-
sponse Rt, without appealing to knowing all past
utterances. Such Markov assumption allows Se-
quicity to concatenate Bt‚àí1, Rt‚àí1 and Ut (de-
noted as Bt‚àí1Rt‚àí1Ut) as a source sequence for
seq2seq modeling, to generate Bt and Rt as tar-
get output sequences at each turn. More for-
mally, we represent the dialogue utterances as
{(B0R0U1;B1R1); (B1R1U2;B2R2); ...; (Bt‚àí1
Rt‚àí1Ut;BtRt)} where B0 and R0 are initialized
as empty sequences. In this way, Sequicity ful-
fills both task accomplishment and response gen-
eration in an unified seq2seq model. Note that we
process Bt and Rt separately, as the belief state
Bt depends only on Bt‚àí1Rt‚àí1Ut, while the re-
sponse Rt is additionally conditioned on Bt and
the knowledge base search results (denoted as kt);
that is, Bt informs theRt‚Äôs contents. For example,
Rt must include all the request slots fromBt when
communicating the entities fulfilling the requests
found in the knowledge base. Here, kt helps gen-
erate Rt pragmatically.



1440

ùêµ

<Inf> Italian ; cheap </Inf> 
<Req></Req>

ùëà

Tell   me   the   address   and   the   
phone   number   please . </s>

<Inf> Italian ; cheap </Inf> <Req>address ; 
phone</Req>

ùêµ R

Turn Dialogue

User1 Can I have some Italian food please?

Mach
ine1

<Inf> Italian </Inf><Req> </Req>
What price range are you looking for?

User2 I want cheap ones.

Mach
ine2

<Inf> Italian ; cheap </Inf> 
<Req></Req>
NAME_SLOT is a cheap restaurant 
serving western food

User3 Tell   me   the   address   and   the   
phone   number   please   . 

Mach
ine3

<Inf> Italian ; cheap </Inf> 
<Req>address ; phone</Req>
The  address  is  ADDRESS_SLOT 
and the phone number is 
PHONE_SLOT

Knowledge 
Base

The  address  is  ADDRESS_SLOT and the phone number is 
PHONE_SLOT

NAME_SLOT is a cheap restaurant 
serving western food

ùëÖ

Figure 1: Sequicity overview. The left shows a sample dialogue; the right illustrates the Sequicity. Bt
is employed only by the model, and not visible to users. During training, we substitute slot values
with placeholders bearing the slot names for machine response. During testing, this is inverted: the
placeholders are replaced by actual slot values, according to the item selected from the knowledge base.

Generally, kt has three possibilities: 1) multiple
matches, 2) exact match and 3) no match, while
the machine responses differ accordingly. As an
example, let‚Äôs say a user requests an Italian restau-
rant. In the scenario of multiple matches, the sys-
tem should prompt for additional constraints for
disambiguation (such as restaurant price range).
In the second exact match scenario where a sin-
gle target (i.e., restaurant) has been found, the sys-
tem should inform the user their requested infor-
mation (e.g., restaurant address). If no entity is
obtained, the system should inform the user and
perhaps generate a cooperative response to retry a
different constraint.

We thus formalize Sequicity as a seq2seq model
which encodes Bt‚àí1Rt‚àí1Ut jointly, but decodes
Bt and Rt separately, in two serial stages. In the
first stage, the seq2seq model decodesBt uncondi-
tionally (Eq. 4a). Once Bt obtained, the decoding
pauses to perform the requisite knowledge base
search based on Bt, resulting in kt. Afterwards,
the seq2seq model continues to the second decod-
ing stage, where Rt is generated on the additional
conditions of Bt and kt (Eq. 4b).

Bt = seq2seq(Bt‚àí1Rt‚àí1Ut|0, 0) (4a)
Rt = seq2seq(Bt‚àí1Rt‚àí1Ut|Bt,kt) (4b)

Sequicity is a general framework suitably imple-
mented by any of the various seq2seq models.
The additional modeling effort beyond a general

seq2seq model is to add the conditioning on Bt
and kt to decode the machine response Rt. For-
tunately, natural language generation with spe-
cific conditions has been extensively studied (Wen
et al., 2016b; Karpathy and Fei-Fei, 2015; Mei
et al., 2016) which can be employed within this
framework.

4.3 Sequicity Instantiation: A Two Stage
CopyNet

Although there are many possible instantiations,
in this work we purposefully choose a simplistic
architecture, leaving more sophisticated modeling
for future work. We term our instantiated model
a Two Stage CopyNet (TSCP). We denote the first
m‚Ä≤ tokens of target sequence Y areBt and the rests
are Rt, i.e. Bt = y1...ym‚Ä≤ and Rt = ym‚Ä≤+1...ym.

Two-Stage CopyNet. We choose to improve
upon CopyNet (Gu et al., 2016) as our seq2seq
model. This is a natural choice as we observe
that target sequence generation often requires the
copying of tokens from the input sequence. Let‚Äôs
discuss this in more detail. From a probabilis-
tic point of view, the traditional encoder‚Äìdecoder
structure learns a language model. To decode yj ,
we can employ a softmax (e.g., Eq. 3) to calcu-
late the probability distribution over V i.e., P gj (v)
where v ‚àà V , and then choose the token with
the highest generation probability. However, in
our case, tokens in the target sequence Y might



1441

be exactly copied from the input X (e.g., ‚ÄúItal-
ian‚Äù). These copied words need to be explicitly
modeled. CopyNet (Gu et al., 2016) is a natural fit
here, as it enlarges the decoding output space from
V to V ‚à™ X . For yj , it considers an additional
copy probability P cj (v), indicating the likelihood
of yj copied from v ‚àà X . Following (Gu et al.,
2016), the simple summation of both probabilities
Pj(v) = P

g
j (v) + P

c
j (v), v ‚àà V ‚à™X is treated as

the final probability in the original paper.
In Sequicity, simply applying original CopyNet

architecture is insufficient, since Bt and Rt have
different distributions. We here employ two sep-
arate RNN (GRU in our implementation) in de-
coder: one for Bt and the other for Rt. In the first
decoding stage, we have a copy-attention mecha-
nism on X to decode Bt; then calculate the gener-
ation probability through attending to X as intro-
duced in Sec 3.1, as well as the copy probability
for each word v ‚àà X following (Gu et al., 2016)
by Eq. 5:

P cj (v) =
1

Z

|X|‚àë
i:xi=v

eœà(xi), j 6 m‚Ä≤ (5)

where Z is a normalization term and œà(xi) is the
score of ‚Äúcopying‚Äù word xi and is calculated by:

œà(xi) = œÉ(h
(x)
i

T
Wc)h

(y)
j , j 6 m

‚Ä≤ (6)

where Wc ‚àà Rd√ód.
In the second decoding stage (i.e., decoding

Rt), we apply the last hidden state ofBt as the ini-
tial hidden state of the Rt GRU. However, as we
need to explicitly model the dependency onBt, we
have copy-attention mechanism on Bt instead of
onX: treating all tokens ofBt as the candidate for
copying and attention. Specifically, we use hidden
state generated by Bt GRU, i.e., h

(y)
1 , ...,h

(y)
m‚Ä≤ , to

calculate copying using Eqs. 7 and 8 and attention
score as introduced in Sec 3.1. It helps to reduce
search space because all key information of X for
task completion has been included in Bt.

P cj (v) =
1

Z

‚àë
i:yi=v

eœà(yi), i 6 m‚Ä≤ < j 6 m (7)

œà(yi) = œÉ(h
(y)
i

T
Wc)h

(y)
j , i 6 m

‚Ä≤ < j 6 m (8)

In contrast to recent work (Eric and Manning,
2017a) that also employs a copy-attention mech-
anism to generate a knowledge-base search API
and machine responses, our proposed method ad-
vances in two aspects: on one hand, bspans
reduce the search space from U1R1...UtRt to
Bt‚àí1Rt‚àí1Ut by compressing key points for the
task completion given past dialogues; on the other
hand, because bspans revisit context by only han-
dling the Bt with a fixed length, the time com-
plexity of TSCP is only O(T ), comparing O(T 2)
in (Eric and Manning, 2017a).

Involving kt when decoding Rt. As kt has
three possible values: obtaining only one, multi-
ple or no entities. We let kt be a vector of three
dimensions, one of which signals a value. We ap-
pend kt to the embeddings yj , as shown in Eq. (9)
that is fed into an GRU for generating h(y)j+1. This
approach is also referred to as Language Model
Type condition (Wen et al., 2016b)

y‚Ä≤j =

[
yj
kt

]
, j ‚àà [m‚Ä≤ + 1,m] (9)

4.4 Training

The standard cross entropy is adopted as our ob-
jective function to train a language model:

m‚àë
j=1

yjlogPj(yj) (10)

In response generation, every token is treated
equally. However, in our case, tokens for task
completion are more important. For example,
when a user asks for the address of a restau-
rant, it matters more to decode the placeholder
<address> than decode words for language flu-
ency. We can employ reinforcement learning to
fine tune the trained response decoder with an em-
phasis to decode those important tokens.

Inspired by (Wen et al., 2017a), in the context
of reinforcement learning, the decoding network
can be viewed as a policy network, denoted as
œÄŒò(yj) for decoding yj (m‚Ä≤ + 1 6 j 6 m). Ac-
cordingly, the choice of word yj is an action and
its hidden vector generated by decoding GRU is
the corresponding state. In reinforcement tuning
stage, the trained response decoder is the initial
policy network. By defining a proper reward func-
tion r(j) for decoding yj , we can update the trained



1442

Dataset Cam676
Size Train:408 / Test: 136 / Dev: 136
Domains restaurant reservation
Slot types price, food style etc.
Distinct slot values 99
Dataset KVRET
Size Train:2425 / Test: 302 / Dev: 302
Domains calendar weather info. POI
Slot types date, etc. location, etc. poi, etc.
Distinct slot values 79 65 140

Table 1: Dataset demographics. Following the
respective literature, Cam676 is split 3:1:1 and
KVRET is split 8:1:1, into training, developing
and testing sets, respectively.

response model with policy gradient:

1

m‚àím‚Ä≤
m‚àë

j=m‚Ä≤+1

r(j)
‚àÇlogœÄŒò(yj)

‚àÇŒò
(11)

where r(j) = r(j) + Œªr(j+1) + Œª2r(j+2) +
...+Œªm‚àíj+1r(m). To encourage our generated re-
sponse to answer the user requested information
but avoid long-winded response, we set the reward
at each step r(j) as follows: once the placeholder
of requested slot has been decoded, the reward for
current step is 1; otherwise, current step‚Äôs reward
is -0.1. Œª is a decay parameter. Sec 5.2 for Œª set-
tings.

5 Experiments

We assess the effectiveness of Sequicity in three
aspects: the task completion, the language qual-
ity, and the efficiency. The evaluation metrics are
listed as follows:

¬∑ BLEU to evaluate the language quality (Papineni
et al., 2002) of generated responses (hence top-1
candidate in (Wen et al., 2017b)).
¬∑ Entity match rate evaluates task completion.
According to (Wen et al., 2017b), it determines
if a system can generate all correct constraints to
search the indicated entities of the user. This met-
ric is either 0 or 1 for each dialogue.
¬∑ Success F1 evaluates task completion and is
modified from the success rate in (Wen et al.,
2017b, 2016a, 2017a). The original success rate
measures if the system answered all the requested
information (e.g. address, phone number). How-
ever, this metric only evaluates recall. A system
can easily achieve a perfect task success by always
responding all possible request slots. Instead, we
here use success F1 to balance both recall and pre-

cision. It is defined as the F1 score of requested
slots answered in the current dialogue.
¬∑ Training time. The training time is important for
iteration cycle of a model in industry settings.

5.1 Datasets

We adopt the CamRest676 (Wen et al., 2017a)
and KVRET (Eric and Manning, 2017b) datasets.
Both datasets are created by a Wizard-of-Oz (Kel-
ley, 1984) method on Amazon Mechanical Turk
platform, where a pair of workers are recruited
to carry out a fluent conversation to complete an
assigned task (e.g. restaurant reservation). Dur-
ing conversation, both informable and requestable
slots are recorded by workers.

CamRest676‚Äôs dialogs are in the single domain
of restaurant searching, while KVRET is broader,
containing three domains: calendar scheduling,
weather information retrieval and point of inter-
est (POI) Navigation. Detailed slot information in
each domain are shown in Table 1. We follow the
data splits of the original papers as shown in 1.

5.2 Parameter Settings

For all models, the hidden size and the embedding
size d is set to 50. |V | is 800 for CamRes676
and 1400 for KVRET. We train our model with
an Adam optimizer (Kingma and Ba, 2015), with
a learning rate of 0.003 for supervised training and
0.0001 for reinforcement learning. Early stopping
is performed on developing set. In reinforcement
learning, the decay parameter Œª is set to 0.8. We
also use beam search strategy for decoding, with a
beam size of 10.

5.3 Baselines and Comparisons

We first compare our model with the state-of-the-
art baselines as follow:

‚Ä¢ NDM (Wen et al., 2017b). As described in
Sec 1, it adopts pipeline designs with a be-
lief tracker component depending on delexi-
calization.

‚Ä¢ NDM+Att+SS. Based on the NDM model, an
additional attention mechanism is performed
on the belief trackers and a snapshot learning
mechanism (Wen et al., 2016a) is adopted.

‚Ä¢ LIDM (Wen et al., 2017a). Also based on
NDM, this model adopts neural variational
inference with reinforcement learning.



1443

CamRes676 KVRET
Mat. BLEU Succ. F1 Timefull TimeN.B. Mat. BLEU Succ. F1 Timefull TimeN.B.

(1) NDM 0.904 0.212 0.832 91.9 min 8.6 min 0.724 0.186 0.741 285.5 min 29.3 min
(2) NDM + Att + SS 0.904 0.240 0.836 93.7 min 10.4 min 0.724 0.188 0.745 289.7 min 33.5 min
(3) LIDM 0.912 0.246 0.840 97.7 min 14.4 min 0.721 0.173 0.762 312.8 min 56.6 min
(4) KVRN N/A 0.134 N/A 21.4 min ‚Äì 0.459 0.184 0.540 46.9 min ‚Äì
(5) TSCP 0.927 0.253 0.854 7.3 min ‚Äì 0.845 0.219 0.811 25.5 min ‚Äì
(6) Att-RNN 0.851 0.248 0.774 7.2 min ‚Äì 0.805 0.208 0.801 23.0 min ‚Äì
(7) TSCP\kt 0.927 0.232 0.835 7.2 min ‚Äì 0.845 0.168 0.759 25.3 min ‚Äì
(8) TSCP\RL 0.927 0.234 0.834 4.1 min ‚Äì 0.845 0.191 0.774 17.5 min ‚Äì
(9) TSCP\Bt 0.888 0.197 0.809 22.9 min ‚Äì 0.628 0.182 0.755 42.7 min ‚Äì

Table 2: Model performance on CamRes676 and KVRET. This table is split into two parts: competitors
on the upper side and our ablation study on the bottom side. Mat. and Succ. F1 are for match rate and
success F1 respectively. Timefull column reports training time till converge. For NDM, NDM+Att+SS
and LIDM, we also calculate the training time for the rest parts except for the belief tracker (TimeN.B.).

‚Ä¢ KVRN (Eric and Manning, 2017b) uses one
seq2seq model to generate response as well
as interacting with knowledge base. How-
ever, it does not incorporate a belief tracking
mechanism.

For NDM, NDM+Att+SS, LIDM, we run the
source code released by the original authors2. For
KVRN, we replicate it since there is no source
code available. We also performed an ablation
study to examine the effectiveness of each com-
ponent.

‚Ä¢ TSCP\kt. We removed the conditioning on
kt when decoding Rt.

‚Ä¢ TSCP\RL. We removed reinforcement learn-
ing which fine tunes the models for response
generation.

‚Ä¢ Att-RNN. The standard seq2seq baseline
as described in the preliminary section
(See ¬ß3.1).

‚Ä¢ TSCP\Bt. We removed bspans for dialogue
state tracking. Instead, we adopt the method
in (Eric and Manning, 2017a): concatenat-
ing all past utterance in a dialogue into a
CopyNet to generate user information slots
for knowledge base search as well as machine
response.

5.4 Experimental Results
As shown in Table 2, TSCP outperforms all base-
lines (Row 5 vs. Rows 1‚Äì4) in task completion
(entity match rate, success F1) and language qual-
ity (BLEU). The more significant performance of
TSCP in KVRET dataset indicates the scalability

2https://github.com/shawnwun/NNDIAL

of TSCP. It is because KVRET dataset has sig-
nificant lexical variety, making it hard to perform
delexicalization for Wen et al.‚Äôs model (Rows 1‚Äì
3)3. However, CamRes676 is relatively small with
simple patterns where all systems work well. As
predicted, KVRN (Row 4) performs worse than
TSCP (Row 5) due to lack of belief tracking.

Compared with Wen et al.‚Äôs models (Rows 1‚Äì3),
TSCP takes a magnitude less time to train. Al-
though TSCP is implemented in PyTorch while
Wen et al.‚Äôs models in Theano, such speed com-
parison is still valid, as the rest of the NDM model
‚Äî apart from its belief tracker ‚Äî has a compara-
ble training speed to TSCP (7.3 mins vs. 8.6 mins
on CamRes676 and 25.5 mins vs. 29.3 mins on
KVRET), where model complexities are similar.
The bottleneck in the time expense is due to belief
tracker training. In addition, Wen et al.‚Äôs models
perform better at the cost of more training time
(Rows 1, 2 and 3), suggesting the intrinsic com-
plexity of pipeline designs.

Importantly, ablation studies validate the ne-
cessity of bspans. With bspans, even a stan-
dard seq2seq model (Att-RNN, Row 6) beats so-
phisticated models such as attention copyNets
(TSCP\Bt, Row 9) in KVRET. Furthermore,
TSCP (Row 5) outperforms TSCP\Bt (Row 9) in
all aspects: task completion, language quality and
training speed. This validate our theoretical analy-
sis in Sec 4.3. Other components of TSCP are also
important. If we only use vanilla Attention-based
RNN instead of copyNet, all metrics for model
effectiveness decrease, validating our hypothesize
that the copied words need to be specifically mod-
eled. Secondly, BLEU score is sensitive to knowl-

3We use the delexicalization lexicon provided by the orig-
inal author of KVRET(Eric and Manning, 2017b)



1444

edge base search result kt (Row 7 vs. Row 5).
By examining error cases, we find that the system
is likely to generate common sentences like ‚Äúyou
are welcome‚Äù regardless of context, due to corpus
frequency. Finally, reinforcement learning effec-
tively helps both BLEU and success F1 although it
takes acceptable additional time for training.

5.5 OOV Tests

Previous work predefines all slot values in a be-
lief tracker. However, a user may request new at-
tributes that has not been predefined as a classifi-
cation label, which results in an entity mismatch.
TSCP employs copy mechanisms, gaining an in-
trinsic potential to handle OOV cases. To conduct
the OOV test, we synthesize OOV test instances
by adding a suffix unk to existing slot fillers. For
example, we change ‚ÄúI would like Chinese food‚Äù
into ‚ÄúI would like Chinese unk food.‚Äù We then
randomly make a proportion of testing data OOV
and measure its entity match rate. For simplicity,
we only show the three most representative mod-
els pre-trained in the in-vocabulary data: TSCP,
TSCP\Bt and NDM.

M
a
t.

0

0.25

0.5

0.75

1

OOV rate
0% 25% 50% 75% 100%

TSCP NDM TSCP\Bt

(a) CamRes676

M
a
t.

0

0.25

0.5

0.75

1

OOV rate
0% 25% 50% 75% 100%

TSCP NDM TSCP\Bt

(b) KVRET

Figure 2: OOV tests. 0% OOV rate means no
OOV instance while 100% OOV rate means all in-
stances are changed to be OOV.

Compared with NDM, TSCP still performs well
when all slot fillers are unknown. This is because
TSCP actually learns sentence patterns. For exam-
ple, CamRes676 dataset contains a frequent pat-
tern ‚ÄúI would like [food type] food‚Äù where the
[food type] should be copied in Bt regardless
what exact word it is. In addition, the performance
of TSCP\Bt decreases more sharply than TSCP as
more instances set to be OOV. This might be be-
cause handling OOV cases is much harder when
search space is large.

5.6 Empirical Model Complexity

Traditional belief trackers like (Wen et al., 2017b)
are built as a multi-class classifier, which mod-
els each individual slot and its corresponding val-
ues, introducing considerable model complexities.
This is especially severe in large datasets with a
number of slots and values. In contrast, Sequic-
ity reduces such a complex classifier to a language
model. To compare the model complexities of two
approaches, we empirically measure model size.
We split KVRET dataset by their domains, result-
ing in three sub-datasets. We then accumulatively
add the sub-datasets into training set to examine
how the model size grows. We here selectively
present TSCP, NDM and its separately trained be-
lief tracker, since Wen et al.‚Äôs set of models share
similar model sizes.

M
od

el
 S

ize
(m

illi
on

)

0

2

4

5

7

Distinct Slot Values
79 144 284

TSCP NDM Belief Tracker

Figure 3: Model size sensitivity with respect to
KVRET. Distinct slot values of 79, 144, 284 cor-
respond to the number of slots in KVRET‚Äôs calen-
dar, calendar + weather info., and all 3 domains.

As shown in Figure 3, TSCP has a magni-
tude less number of parameters than NDM and
its model size is much less sensitive to distinct
slot values increasing. It is because TSCP is a
seq2seq language model which has a approximate
linear complexity to vocabulary size. However,
NDM employs a belief tracker which dominates
its model size. The belief tracker is sensitive to the
increase of distinct slot values because it employs
complex structures to model each slot and corre-
sponding values. Here, we only perform empirical
evaluation, leaving theoretically complexity anal-
ysis for future works.

5.7 Discussions

In this section we discuss if Sequicity can tackle
inconsistent user requests , which happens when
users change their minds during a dialogue. Incon-
sistent user requests happen frequently and are dif-



1445

ficult to tackle in belief tracking (Williams, 2012;
Williams et al., 2013). Unlike most of previous
pipeline-based work that explicitly defines model
actions for each situation, Sequicity is proposed to
directly handle various situations from the training
data with less manual intervention. Here, given
examples about restaurant reservation, we provide
three different scenarios to discuss:

‚Ä¢ A user totally changes his mind. For ex-
ample, the user request a Japanese restaurant
first and says ‚ÄúI dont want Japanese food any-
more, I‚Äôd like French now.‚Äù Then, all the slot
activated before should be invalid now. The
slot annotated for this turn is only French. Se-
quicity can learn this pattern, as long as it is
annotated in the training set.

‚Ä¢ User requests cannot be found in the
KB (e.g., Japanese food). Then the sys-
tem should respond like ‚ÄúSorry, there is no
Japanese food...‚Äù. Consequently, the user can
choose a different option: ‚ÄúOK, then French
food.‚Äù The activated slot Japanese will be
replaced as French, which our system can
learn. Therefore, an important pattern is the
machine-response (e.g., ‚Äúthere is no [XXX
constraint]‚Äù) in the immediate previous utter-
ance.

‚Ä¢ Other cases. Sequicity is expected to gen-
erate both slot values in a belief span if it
doesn‚Äôt know which slot to replace. To main-
tain the belief span, we run a simple post-
processing script at each turn, which detects
whether two slot values have the same slot
name (e.g., food type) in a pre-defined
slot name-value table. Then, such script only
keeps the slot value in the current turn of user
utterance. Given this script, Sequicity can ac-
curately discover the slot requested by a user
in each utterance. However, this script only
works when slot values are pre-defined. For
inconsistent OOV requests, we need to build
another classifier to recognize slot names for
slot values.

To sum up, Sequicity, as a framework, is able
to handle various inconsistent user input despite
its simple design. However, detailed implementa-
tions should be customized depends on different
applications.

6 Conclusion

We propose Sequicity, an extendable framework,
which tracks dialogue believes through the decod-
ing of novel text spans: belief spans. Such belief
spans enable a task-oriented dialogue system to be
holistically optimized in a single seq2seq model.
One simplistic instantiation of Sequicity, called
Two Stage CopyNet (TSCP), demonstrates better
effectiveness and scalability of Sequicity. Exper-
iments show that TSCP outperforms the state-of-
the-art baselines in both task accomplishment and
language quality. Moreover, our TSCP implemen-
tation also betters traditional pipeline architectures
by a magnitude in training time and adds the ca-
pability of handling OOV. Such properties are im-
portant for real-world customer service dialog sys-
tems where users‚Äô inputs vary frequently and mod-
els need to be updated frequently. For our future
work, we will consider advanced instantiations for
Sequicity, and extend Sequicity to handle unsuper-
vised cases where information and requested slots
values are not annotated.

Acknowledgments

We would like to thank the anonymous reviewers
for their detailed comments and suggestions for
this paper. This work is also supported by the Na-
tional Research Foundation, Prime Ministers Of-
fice, Singapore under its IRC@SG Funding Initia-
tive.

References
Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang

Tang. 2017. A survey on dialogue systems: Re-
cent advances and new frontiers. arXiv preprint
arXiv:1711.01731 .

Heriberto CuayaÃÅhuitl, Simon Keizer, and Oliver
Lemon. 2015. Strategic dialogue management
via deep reinforcement learning. arXiv preprint
arXiv:1511.08099 .

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.
Towards end-to-end reinforcement learning of dia-
logue agents for information access. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). volume 1, pages 484‚Äì495.

Mihail Eric and Christopher D Manning. 2017a. A
copy-augmented sequence-to-sequence architecture
gives good performance on task-oriented dialogue.



1446

Mihail Eric and Christopher D Manning. 2017b. Key-
value retrieval networks for task-oriented dialogue.
SIGDIAL .

Jonas Gehring, Michael Auli, David Grangier, and
Yann N Dauphin. 2017. A convolutional encoder
model for neural machine translation. ACL .

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. ACL .

Homa B Hashemi, Amir Asiaee, and Reiner Kraft.
2016. Query intent detection using convolutional
neural networks. In International Conference on
Web Search and Data Mining, Workshop on Query
Understanding.

Matthew Henderson, Blaise Thomson, and Jason D
Williams. 2014a. The second dialog state tracking
challenge. In SIGDIAL Conference. pages 263‚Äì272.

Matthew Henderson, Blaise Thomson, and Jason D
Williams. 2014b. The third dialog state tracking
challenge. In Spoken Language Technology Work-
shop (SLT), 2014 IEEE. IEEE, pages 324‚Äì329.

Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for the
dialog state tracking challenge. In Proceedings of
the SIGDIAL 2013 Conference. pages 467‚Äì471.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference
on computer vision and pattern recognition. pages
3128‚Äì3137.

John F Kelley. 1984. An iterative design methodol-
ogy for user-friendly natural language office infor-
mation applications. ACM Transactions on Infor-
mation Systems (TOIS) 2(1):26‚Äì41.

Seokhwan Kim, Luis Fernando DHaro, Rafael E
Banchs, Jason D Williams, and Matthew Henderson.
2017. The fourth dialog state tracking challenge. In
Dialogues with Social Robots, Springer, pages 435‚Äì
449.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization.

Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In SIGDIAL Conference.
pages 442‚Äì451.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Bing Liu and Ian Lane. 2017. Iterative policy learning
in end-to-end trainable task-oriented neural dialog
models. arXiv preprint arXiv:1709.06136 .

Hongyuan Mei, Mohit Bansal, and Matthew R Walter.
2016. What to talk about and how? selective gener-
ation using lstms with coarse-to-fine alignment. In
NAACL.

Nikola MrksÃåicÃÅ, Diarmuid O SeÃÅaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. ACL .

Tim Paek and Eric Horvitz. 2000. Conversation as ac-
tion under uncertainty. In Proceedings of the Six-
teenth conference on Uncertainty in artificial intel-
ligence. Morgan Kaufmann Publishers Inc., pages
455‚Äì464.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 311‚Äì318.

Yangyang Shi, Kaisheng Yao, Le Tian, and Daxin
Jiang. 2016. Deep lstm based feature mapping for
query classification. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. pages 1501‚Äì1511.

Joao Silva, Luƒ±ÃÅsa Coheur, Ana Cristina Mendes, and
Andreas Wichert. 2011. From symbolic to sub-
symbolic information in question classification. Ar-
tificial Intelligence Review 35(2):137‚Äì154.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. arXiv preprint arXiv:1706.03762 .

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes,
David Vandyke, and Steve Young. 2016a. Condi-
tional generation and snapshot learning in neural di-
alogue systems. EMNLP .

Tsung-Hsien Wen, Milica Gasic, Nikola MrksÃåicÃÅ,
Lina M. Rojas Barahona, Pei-Hao Su, Ste-
fan Ultes, David Vandyke, and Steve Young.
2016b. Conditional generation and snap-
shot learning in neural dialogue systems. In
EMNLP. ACL, Austin, Texas, pages 2153‚Äì2162.
https://aclweb.org/anthology/D16-1233.

Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, and
Steve Young. 2017a. Latent intention dialogue mod-
els. ICML .

Tsung-Hsien Wen, David Vandyke, Nikola Mrksic,
Milica Gasic, Lina M Rojas-Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017b. A network-
based end-to-end trainable task-oriented dialogue
system. EACL .

https://aclweb.org/anthology/D16-1233
https://aclweb.org/anthology/D16-1233
https://aclweb.org/anthology/D16-1233


1447

Jason Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the SIGDIAL 2013
Conference. pages 404‚Äì413.

Jason D Williams. 2012. A belief tracking challenge
task for spoken dialog systems. In NAACL-HLT
Workshop on future directions and needs in the spo-
ken dialog community: tools and data. Association
for Computational Linguistics, pages 23‚Äì24.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: practical and efficient
end-to-end dialog control with supervised and rein-
forcement learning. Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) .

Steve Young, Milica GasÃåicÃÅ, Simon Keizer, FrancÃßois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for pomdp-based spoken dia-
logue management. Computer Speech & Language
24(2):150‚Äì174.

Steve Young, Milica GasÃåicÃÅ, Blaise Thomson, and Ja-
son D Williams. 2013a. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE 101(5):1160‚Äì1179.

Steve Young, Milica GasÃåicÃÅ, Blaise Thomson, and Ja-
son D Williams. 2013b. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE 101(5):1160‚Äì1179.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In AAAI. pages 2852‚Äì2858.


