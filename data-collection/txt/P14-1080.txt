



















































Enhancing Grammatical Cohesion: Generating Transitional Expressions for SMT


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 850–860,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Enhancing Grammatical Cohesion: 

Generating Transitional Expressions for SMT 

 

Mei Tu             Yu Zhou           Chengqing Zong 

National Laboratory of Pattern Recognition,  

Institute of Automation,  

Chinese Academy of Sciences 

{mtu,yzhou,cqzong}@nlpr.ia.ac.cn 

 

  

 

 

Abstract 

Transitional expressions provide glue that 

holds ideas together in a text and enhance the 

logical organization, which together help im-

prove readability of a text. However, in most 

current statistical machine translation (SMT) 

systems, the outputs of compound-complex 

sentences still lack proper transitional expres-

sions. As a result, the translations are often 

hard to read and understand. To address this 

issue, we propose two novel models to en-

courage generating such transitional expres-

sions by introducing the source compound-

complex sentence structure (CSS). Our models 

include a CSS-based translation model, which 

generates new CSS-based translation rules, 

and a generative transfer model, which en-

courages producing transitional expressions 

during decoding. The two models are integrat-

ed into a hierarchical phrase-based translation 

system to evaluate their effectiveness. The ex-

perimental results show that significant im-

provements are achieved on various test data 

meanwhile the translations are more cohesive 

and smooth.  

1 Introduction 

During the last decade, great progress has been 

made on statistical machine translation (SMT) 

models. However, these translations still suffer 

from poor readability, especially translations of 

compound-complex sentences. One of the main 

reasons may be that most existing models con-

centrate more on producing well-translated local 

sentence fragments, but largely ignore global 

cohesion between the fragments. Generally, co-

hesion, including lexical and grammatical cohe-

sion, contributes much to the understandability 

and smoothness of a text.  

Recently, researchers have begun addressing 

the lexical cohesion of SMT (Gong et al., 2011; 

Xiao et al., 2011; Wong and Kit, 2012; Xiong, 

2013). These efforts focus mainly on the co-

occurrence of lexical items in a similar environ-

ment. Grammatical cohesion1 (Halliday and Has-

san, 1976) in SMT has been little mentioned in 

previous work. Translations without grammatical 

cohesion is hard to read, mostly due to loss of 

cohesive and transitional expressions between 

two sentence fragments. Thus, generating transi-

tional expressions is necessary for achieving 

grammatical cohesion. However, it is not easy to 

produce such transitional expressions in SMT. 

As an example, consider the Chinese-to-English 

translation in Figure 1.  

Source Chinese sentence:  

  [尽管         减轻     污染     的   呼声     不断      ，]1   [  公众          
Although   reduce  pollution  of   calls    continue  ,           public      

   日渐       愤怒   ，]2   [污染       还是    变得       更        糟糕                               
growing    angry  ,         pollution  still    become   more   worse 

   了   ，]3  [越发  显出         环保                         的    紧迫性 。]4
already   ,   more   show  environment  protection  of    urgent .

Target English golden translation:

Despite frequent calls for cutting pollution, and 

growing public anger, the problem has only got worse, 

which increasingly shows the urgency of environmental 

protection.

Figure 1: An example of Chinese-to-English transla-

tion. The English translation sentence has three transi-

tional phrases: Despite, and, which. 

 

There are 4 sub-sentences separated by com-

mas in the Chinese sentence. We have tried to 

translate the Chinese sentence using many well-

                                                 
1
 Grammatical cohesion can make relations among sentenc-

es more explicit. There are various grammatically cohesive 

devices (reference, substitution ellipsis and conjunction) 

that tie fragments together in a cohesive way.    

850



known online translators, but find that it is very 

difficult to generate the target transitional ex-

pressions, especially when there is no explicit 

connective word in the source sentence, such as 

generating “and ” and “which” in Figure 1. 

Fortunately, the functional relationships be-

tween two neighboring source sub-sentences 

provide us with a good perspective and the inspi-

ration to generate those transitional phrases. Fig-

ure 1 shows that the first and the second Chinese 

sub-sentences form a parallel relation. Thus, 

even though there is no distinct connective word 

at the beginning of the second source sub-

sentence, a good translator is still able to insert or 

generate an “and” as a connection word to make 

the target translation more cohesive.  
Based on the above analysis, this paper focus-

es on the target grammatical cohesion in SMT to 

make the translation more understandable, espe-

cially for languages with great difference in lin-

guistic structure like Chinese and English. To the 

best of our knowledge, our work is the first at-

tempt to generate target transitional expressions 

for SMT grammatical cohesion by introducing 

the functional relationships of source sentences. 

In this work, we propose two models. One is a 

new translation model that is utilized to generate 

new translation rules combined with the infor-

mation of source functional relationships. The 

other is a generative transfer model that encour-

ages producing transitional phrases during de-

coding. Our experimental results on Chinese-to-

English translation demonstrate that the transla-

tion readability is greatly improved by introduc-

ing the cohesive information. 

The remainder of the paper is organized as 

follows. In Section 2, we describe the functional 

relationships of Chinese compound-complex sen-

tences. In Section 3, we present our models and 

show how to integrate the models into an SMT 

system. Our experimental results are reported in 

Section 4. A survey of related work is conducted 

in Section 5, and we conclude our work and out-

line the future work in Section 6.  

2 Chinese Compound-Complex Sen-
tence Structure 

To acquire the functional relationships of a Chi-

nese compound-complex sentence, Zhou (2004) 

proposed a well-annotated scheme to build the 

Compound-complex Sentence Structure (CSS). 

The structure explicitly shows the minimal se-

mantic spans, called elementary units (eus), and 

also depicts the hierarchical relations among eus. 

There are 11 common types of functional rela-

tionships 2  annotated in the Tsinghua Chinese 

Treebank (Zhou, 2004).  

Under the annotation scheme of the Tsinghua 

Chinese Treebank, the Chinese sentence of ex-

ample in Figure 1 is represented as the tree 

shown in Figure 2. In this example, each sub-

sentence is an eu. eu1 and eu2 are combined with 

a parallel relationship, followed by eu3 with an 

adversative relationship. eu1, eu2, and eu3 form a 

large semantic span3, connected with eu4 by a 

consequence relationship. All of the eus are or-

ganized into various functional relationships and 

finally form a hierarchical tree. 

parallel-[(1,1), (2,2)]

adversative-[(1,2),(3,3)]

consequence-[(1,3),(4,4)]

污染  还是 
变得  更   
糟糕  了   ，

越发    显出   
环保  的  
紧迫性 。

eu1 eu2
eu3

eu4
尽管  减轻      
污染  的  
呼声 不断 ，

公众 日渐 
愤怒     ，

 

Figure 2: The compound-complex sentence 

structure of the Chinese sentence in Figure 1. 

Formally, given a compound-complex sen-

tence structure (CSS), each node in the CSS can 

be represented as a tuple

1 1[( , ),...( , ),..., ( , )] l l L LR s e s e s e . R represents the 

relationship, which has L children. For each 

child of R , a pair ( , )lls e records its start and end 

eus. For example, adversative-[(1,2), (3,3)] in 

Figure 2 means that two children are controlled 

by the relationship adversative, and the left child 

consists of eu1 and eu2, while the right child con-

tains only eu3.  

CSS has much in common with Rhetorical 

Structure (Mann and Thompson, 1988) in Eng-

lish, which also describe the semantic relation 

between discourse units. But the Rhetorical 

Structure involves much richer relations on the 

document-level, and little corpus is open for 

Chinese.    

In the following, we will describe in detail 

how to utilize such CSS information for model-

ling in SMT.  

                                                 
2 They are parallel, consequence, progressive, alternative, 
causal, purpose, hypothesis, condition, adversative, expla-

nation, and flowing relationships. 
3 A semantic span can include one or more eus. 

851



3 Modelling 

Our purpose is to enhance the grammatical cohe-

sion by exploiting the source CSS information. 

Therefore, theoretically, the conditional probabil-

ity of a target translation es conditioned on the 

source CSS-based tree ft is given by ( | )s tP e f , 

and the final translation se  is obtained with the 

following formula: 

argmax{P( | )} (1)
S

s s t
e

e e f   

    Following Och and Ney (2002), our model is 

framed as a log-linear model: 

 
exp ( , )

( | ) (2)
exp ( , )








 
s

k k k s t
s t

k k k s t

h
P

he

e f
e f

e' f
 

 

where ( , )s th e f is a feature with weight . Then, 

the best translation is: 

 argmaxexp ( , ) (3)
s

s k k k s th 
e

e e f  

Our models make use of CSS with two strate-

gies:  

1) CSS-based translation model: following 

formula (1), we obtain the cohesion information 

by modifying the translation rules with their 

probabilities ( | )s tP e f  based on word align-

ments between the source CSS-tree and the tar-

get string; 

 2) CSS-based transfer model: following 

formula (3), we introduce a transfer score to en-

courage the decoder to generate transitional 

words and phrases; the score is utilized as an ad-

ditional feature ( , )k s th e f  in the log-linear model.  

3.1 CSS-based Translation Model 

For the existing translation models, the entire 

training process is conducted at the lexical or 

syntactic level without grammatically cohesive 

information. As a result, it is difficult to utilize 

such cohesive information during decoding. In-

stead, we reserve the cohesive information in the 

training process by converting the original source 

sentence into tagged-flattened CSS and then per-

form word alignment and extract the translation 

rules from the bilingual flattened source CSS and 

the target string.  

As introduced in Section 2, a CSS consists of 

nodes, and a node can be represented as a tuple

1 1[( , ),...( , ),...,( , )]L Ll lR s e s e s e . In this represen-

tation, the relationship R is the most important 

factor because different relationships directly 

reflect different cohesive expressions. In addition, 

the children’s positions always play a strong role 

in choosing cohesive expressions because transi-

tional expressions vary for children with differ-

ent positions. For example, when translating the 

last child of a parallel relation, we always use 

word “and” as the transitional expression seen in 

Figure 3, but we will not use it for the first child 

of a parallel relation. Therefore, in the training 

process we just keep the information of relation-

ships and children’s positions when converting 

Despite    frequent    calls   for   cutting   pollution  ,   and   growing   public   anger   ,

<Parallel  @B>  尽管  减轻   污染  的   呼声  不断  ，          <Parallel  @E>  公众      日渐   愤怒  ，

parallel

尽管     减轻    污染    的     呼声     不断  ， 公众    日渐    愤怒     ，

Despite    frequent    calls   for   cutting   pollution  ,   and   growing   public   anger   ,

(a)

(b)

Original hierarchical rules:

                               [X] 日渐 |||  and growing [X]

Modified hierarchical rules:

                       <parallel  @E >  [X] 日渐  |||  and growing [X]

(c)

 

Figure 3: An example of modifying translation rules. @B means the current structure information 

comes from the first child, and @E means from the last child.  

852



the source CSS to a tagged-flattened string. 

 Considering that the absolute position (index 

of the eu, such as 1, 2, 3) is somehow sparse in 

the corpus, we employ the relative position in-

stead. B (Beginning) represents the first child of 

a relationship, E (End) means the last child of a 

relationship, and M (Middle) represents all the 

middle children.  

Under this agreement, the original Chinese 

CSS-based tree will be converted to a new 

tagged-flattened string. Note the converting ex-

ample from Figure 3(a) to Figure 3(b): node par-

allel-[(1,1), (2,2)] (see Figure 2) is converted to 

a flat string. Its first child is represented as <par-

allel, @B> with the semantic span, while the last 

child is <parallel, @E> with the corresponding 

semantic span. 

We then perform word alignment on the modi-

fied bilingual sentences, and extract the new 

translation rules based on the new alignment, as 

shown in Figure 3(b) to Figure 3(c). Now the 

newly extracted rule “<parallel, @E > [X] 日渐 
||| and growing [X] ” is tagged with cohesive in-

formation. Thus, if the similar relationship paral-

lel occurs in the test source sentence, this type of 

rule is more likely to be chosen to generate the 

cohesive word “and” during decoding because it 

is more discriminating than the original rules ([X] 

日渐 ||| and growing [X]). The conditional prob-
abilities of the new translation rules are calculat-

ed following (Chiang, 2005). 

3.2 CSS-based Transfer model  

In general, according to formula (3), the transla-

tion quality based on the log-linear model is re-

lated tightly with the features chosen. Most trans-

lation systems adopt the features from a transla-

tion model, a language model, and sometimes a 

reordering model. To give a bonus to generating 

cohesive expressions during decoding, we have 

designed a special additional feature. The addi-

tional feature is represented as a probability cal-

culated by a transfer model. 

Given the source CSS information, we want 

our transfer model to predict the most possible 

cohesive expressions. For example, given two 

semantic spans with a parallel relationship and 

many translation candidates, our transfer model 

is expected to assign higher scores to those with 

transitional expressions such as “and” or “as well 

as”. 

Let 0 1, ,... nw w ww  represent the transitional 

expressions observed in the target string. Our 

transfer model can be represented as a condition-

al probability: 

( | ) (4)P CSSw  

    By deriving each node of the CSS, we can 

obtain a factored formula: 

,( | ) ( | , ) (5)i j ij i jP CSS P R RPw w  

where ijw is the transitional expression produced 

by the 
thj child of the thi node of the CSS. iR is 

the relationship type of the thi node. For the 
thj

child in the thi  node, jRP is its relative position 

(B, M or E) introduced in Section 3.1.  

    The process of training this transfer model and 

smoothing is similar to the process of training a 

language model. We obtain the factored transfer 

probability as follows, 

1

1

0 0

( | , )

( | , ) ( | , , ) (6)

ij i j

i

n
k

j k

k

i j

P R RP

P w R RP P w w R RP



 

w

 

where  

 0 0 ,... (7)
n

ij nw w w w  

Following (Bilmes and Kirchhoff, 2003), the 

conditional probabilities 10( | , , )i
k

jkP w w R RP
 in 

formula (6) are estimated in the same way as a 

factored language model, which has the ad-

vantage of easily incorporating various linguistic 

information. 

Considering that ijw  commonly appears at the 

beginning of the target translation of a source 

semantic span such as “which …”, namely, the 

left-frontier phrases, we focus only on the left-

frontier phrases when training this model. Note 

that if there exists a target word before a left 

frontier, and this word is aligned to NULL, we 

will expand the left frontier to this word. The 

expansion process will be repeated until there is 

no such word. For example, if we take the CSS 

and the alignment in Figure 3(a) for training, the 

left frontier of the second child will be expanded 

from “growing” to “and”. In addition, taking the 

tri-gram left-frontier phrase for example, we can 

obtain a training sample such as ijw = and grow-

ing public, R=parallel, RP = E. 

By learning such probabilities for different 

transitional expressions conditioned on different 

relationships, we are able to capture the inner 

connection between the source CSS and the pro-

jected target cohesive phrases. Thus, during de-

coding, if we add the probability generated by 

the transfer model of ( | )P CSSw as a feature in 

853



formula (3), it will certainly contribute to select-

ing more cohesive candidates.  

3.3 Elementary-Unit Cohesion Constraint 

As mentioned in Section 3.2, in the transfer 

model, the transitional phrases are expected to 

occur at the left frontier of a projected span on 

target side. In fact, this depends on the assump-

tion that the projected translations of any two 

disjoint source semantic spans are also disjoint to 

keep their own semantic integrity. We call this 

assumption the integrity assumption. This as-

sumption is intuitive and supported by statistics. 

After analyzing 1,007 golden aligned Chinese-

English sentence-pairs, we find that approxi-

mately 90% of the pairs comply with the as-

sumption. However, in real automatically aligned 

noisy data, the ratio of complying pairs reduces 

to 71%4. Two projected translations that violate 

the integrity assumption may mutually overlap, 

which causes our confusion on where to extract 

the transitional phrases. In this case, extracted 

transitional phrases are likely to be wrong. 

    To increase the chance of extracting correct 

transitional phrases, the alignment results must 

be modified to reduce the impact of incorrect 

alignment. We propose a dynamic cleaning 

method to ensure that the most expressive transi-

tional phrases fall in the accessible extraction 

range before training the transfer model. 

3.3.1 EUC and non-EUC 

As we have defined in Section 2, the minimal 

semantic span is called elementary unit (eu). If 

the source eu and its projected target span com-

ply with the integrity assumption, we say that 

such an eu and its projected span have Elemen-

tary-Unit-Cohesion (EUC). We define EUC 

formally as follows. 

Given two elementary units Aeu  and Beu , 

and their projected target spans Aps and Bps

bound by the word alignment, the alignment 

complies with EUC only if there is no overlap 

between Aps  and Bps . Otherwise, the alignment 

is called non-EUC. The common EUC and non-

EUC cases are illustrated in Figure 4. 

EUC is the basic case for the integrity as-

sumption. For the best cases, the elementary 

units comply with EUC, and thus the semantic 

                                                 
4 The aligning tool is GIZA++ with 5 iterations of Model 1, 
5 iterations of HMM, and 10 iterations of Model 4. The 

GIZA++ code can be downloaded from 

https://code.google.com/p/giza-pp/ 

spans combined by elementary units are certainly 

subject to the integrity assumption.  

 
euA euB

psA psB  

(a) mono EUC case 

euA euB

psApsB  

(b) swap EUC case 

euA euB

psA psB  

(c) non-EUC case 

Figure.4 The schematic diagram of EUC cases 

and non-EUC case.  

3.3.2 A Dynamic Cleaning Method 

An intuitive method to clean the alignment re-

sults is to drop off the noisy word-to-word links 

that cause non-EUC. Considering that the drop-

ping process is a post-editing method for the 

original alignment obtained by a state-of-the-art 

aligner such as GIZA++, we do not expect over-

deleting. Therefore, we tend to take a relatively 

conservative strategy to minimize the deleting 

operation. 

Given a sentence-pair (f, e), suppose that 

0{ ,..., ,..., }i If f ff  is divided into M elemen-

tary units 0{ ,..., ,..., }m MU u u u , and e has N 

words, that is, 0{ ,..., ,..., }n Ne e ee . If A  is the 

word alignment of (f, e), then the goal is to con-

struct the maximum subset *A A under the 

condition that *A  is the word alignment with the 

constraint of EU. The search process can be de-

scribed as the pseudo code in Figure 5. 

In Figure 5, we scan each target word and each 

source eu to assign each word to a unique eu un-

der the EUC constraint with the lowest cost. 

Function cost( , )n m  in line 6 computes the 

counts of deleted links that force the thn target 

word to align only to words in the range of the 
thm eu. For example, if the thn target word is 

aligned to the thi , ( 1)thi  , and ( 2)thi  word in 

source side, while the thi word belongs to 
1̀m

u  

and the ( 1)thi   and ( 2)thi   words belong to 

2m
u , then 

1
cost( , ) 2mn u  , and 2cost( , ) 1mn u  . 

In line 6, Score[n][m] saves a list of scores, each 

score computed by adding the current cost(n, m) 

with the history score of each list of Score[n-1]. 

854



Before the next iteration, the bad branches are 

pruned, as seen in line 5. We adopt the following 

two ways to prune:  

(1) EUC constraint: if the current link violates 
EUC alignment, delete it. 

(2) Keep the hypothesis with a fixed maximum 
size to avoid too large a searching space. 

 

 
 

Figure 5. The pseudo code of dynamic cleaning 

method.  

4 Experiments 

4.1 Experimental Setup 

To obtain the CSSs of Chinese sentences, we use 

the Chinese parser proposed in (Tu et al., 2013a). 

Their parser first segments the compound-

complex sentence into a series of elementary 

units, and then builds structure of the hierarchical 

relationships among these elementary units. 

Their parser was reported to achieve an F-score 

for elementary unit segmentation of approxi-

mately 0.89. The progressive, causal, and condi-

tion terms of functional relationships can be rec-

ognized with precisions of 0.86, 0.8, and 0.75, 

respectively, while others, such as purpose, par-

allel, and flowing, achieve only 0.5, 0.59 and 

0.62, respectively.  

The translation experiments have been con-

ducted in the Chinese-to-English direction. The 

bilingual training data for translation model and 

CSS-based transfer model is FBIS corpus with 

approximately 7.1 million Chinese words and 9.2 

million English words. We obtain the word 

alignment with the grow-diag-final-and strategy 

with GIZA++. Before training the CSS-based 

transfer model, the alignment for transfer model 

is modified by our dynamic cleaning method. 

During the cleaning process, the maximum size 

of hypothesis is limited to 5. A 5-gram language 

model is trained with SRILM5 on the combina-

tion of the Xinhua portion of the English Giga-

word corpus combined with the English part of 

FBIS. For tuning and testing, we use NIST03 

evaluation data as the development set. 

NIST04/05/06, CWMT08-Development 6  and 

CWMT08-Evaluation data are used for testing 

under the measure metric of BLEU-4 (Papineni 

et al. 2002) with the shortest length penalty.  

Table 1 shows how the CSS is distributed in 

all testing sets. According to the statistics in Ta-

ble 1, we see that CSS is really widely distribut-

ed in the NIST and CWMT corpora, which im-

plies that the translation quality may benefit sub-

stantially from the CSS information, if it is well 

considered in SMT.  

 

4.2 Extracted Transitional Expressions 

Eleven types of Chinese functional relationships 

and their English left-frontier phrases (tri-gram) 

learned by our transfer model are given in Table 

2.  

The results in Table 2 show that some left-

frontier phrases reflect the source functional rela-

tionship well, especially for those with better 

precision of relationship recognition, such as 

progressive, causal and condition. Conversely, 

lower precision of relationship recognition may 

weaken the learning ability of the transfer model. 

For example, noisy left-frontier phrases are easi-

ly generated under relationships such as parallel 

and purpose. 

                                                 
5 http://www.speech.sri.com/projects/srilm/ 
6 The China Workshop on Machine Translation 

//Pseudo code for dynamic cleaning                             

1: Score [N+1][M]={[0]}N M          /* initialize  

                                      cumulative cost score chart*/ 

2: Path [M]=[[]]                  /*initialize tracking path*/ 

3: for n = 1 N :{           /*  scan target words*/ 
4:   for 0 1m M   :{        /*scan source U set */ 

5:     PrunePath();   

                 /* prune invalid  path and high-cost path*/ 

6:     Score[n][m]=GetScore(Score[n-1], cost(n, m)) 

       /*compute current cumulative cost score by previ-

ous score and current cost*/ 

7:      SaveCurrentPath(Path[m]);  

/*add current index to Path*/ 

8:  }//end m    
9:}//end n  

10: OptimalPath = 
[ ]

argmax{ [ ][ ]}
Path m

Score N m ; 

 

 Total CSS Ratio(%) 

NIST04 1,788 1,307 73.1 

NIST05 1,082 849 78.5 

NIST06 1,000 745 74.5 

CWMT08-Dev. 1,006 818 81.3 

CWMT08-Eval. 1,006 818 81.3 

Table 1. The numbers of sentences and the 

CSS ratios of all sentences. CWMT08-Dev. is 

short for CWMT08 Development data and 

CWMT08-Eval. is CWMT08 Evaluation da-

ta. 

855



 

4.3 Results on SMT with Different Strategies 

For this work, we use an in-house decoder to 

build the SMT baseline; it combines the hierar-

chical phrase-based translation model (Chiang, 

2005; Chiang, 2007) with the BTG (Wu, 1996) 

reordering model (Xiong et al., 2006; Zens and 

Ney, 2006; He et al., 2010).  

To test the effectiveness of the proposed mod-

els, we have compared the translation quality of 

different integration strategies. First, we adopted 

only the tagged-flattened rules in the hierarchical 

translation system. Next, we added the log prob-

ability generated by the transfer model as a fea-

ture into the baseline features. The baseline fea-

tures include bi-directional phrase translation 

probabilities, bi-directional lexical translation 

probabilities, the BTG re-ordering features, and 

the language model feature. The tri-gram left-

frontier phrase was adopted in the experiment. 

Then the probability generated by the transfer 

model with EUC constraint is added. Finally, we 

incorporated the tagged-flattened rules and the 

additional transfer model feature together.  

Table 3 shows the results of these different in-

tegrated strategies. In Table 3, almost all BLEU 

scores are improved, no matter what strategy is 

used. In particular, the best performance marked 

in bold is as high as 1.24, 0.94, and 0.82 BLEU 

points, respectively, over the baseline system on 

NIST04, CWMT08 Development, and CWMT08 

Evaluation data. The strategy of “TFS+ Flat-

tened Rule” is the most stable. Meanwhile the 

“Flattened Rule” achieves better performance 

than “TFS”. The merits of “Flattened Rule” are 

two-fold: 1) In training process, the new word 

alignment upon modified sentence pairs can 

align transitional expressions to flattened CSS 

tags; 2) In decoding process, the CSS-based rules 

are more discriminating than the original rules, 

which is more flexible than “TFS”.  From the 

table, we cannot conclude that the EUC con-

straint will certainly promote translation quality, 

but the transfer model performs better with the 

constraint on most testing sets. 

4.4 Analysis of Different Effects of Different 
N-grams 

As mentioned in Section 4.3, we have noted the 

effectiveness of tri-gram transfer model, which 

means 2n  in formula (7). In fact, the lengths of 

common transitional expressions vary from one 

word to several words. To evaluate the effects of 

different n-grams for our proposed transfer mod-

el, we compared the uni-/bi-/tri-gram transfer 

models in SMT, and illustrate the results in Fig-

Relation Left-frontier phrases (tri-gram) 

parallel as well as;   at the same; … 

progressive but will also; in addition to;… 

causal 
therefore , the;   for this reason;   as a 

result; because it is;   so it is;… 

condition as long as;   only when the… 

hypothesis if we do; if it is;  if the us; … 

alternative regardless of whether;… 

purpose 
it is necessary;  

further promote the ;… 

explanation that is ,;  the first is; first is the;… 

adversative however , the ;  but it is; … 

flowing this is a; which is an; … 

consequence so that the; to ensure that… 

Table 2. Chinese functional relations and their 

corresponding English left-frontier phrases 

learned by our transfer model. The noun phrases 

starting with a definite / indefinite word are fil-

tered because they are unlikely to be the transi-

tional phrases. 

 

 

 
NIST04 NIST05 NIST06 

CWMT08’s 

Dev. 

CWMT08’s 

Eval. 

Baseline   33.42   31.99   33.88       26.14       23.88 

+Flattened Rule   34.54**   32.32   34.58**       26.79**       24.70** 

+TFS (without EUC)   33.93**   32.04   34.40*       26.44       24.58** 

+TFS   33.84**   32.63*   34.15       27.08**       24.65** 

+TFS+ Flattened Rule   34.66**   32.54 34.52**       26.87**       24.49** 
       + Flattened Rule: only use the tagged-flattened translation rules 

       + TFS:  only use the transfer model score as an additional feature (based on 3-gramtransitional phrase) 

       + TFS + Flattened Rule: both are used 

       *: value with * means that it is significantly better than the baseline with p<0.05 

       **: value with ** means that it is significantly better than the baseline with p<0.01 

Table 3. BLEU scores of the testing sets with different integrating strategies 

856



ure 6. In this experiment, the CSS-based transla-

tion rules and the CSS-based transfer model are 

both incorporated. Considering time and compu-

ting resources, in the rest of our paper, our analy-

sis is conducted on NIST05 and NIST06.  

We choose 0,1, 2n   in this experiment for 

that the common English transitional expressions 

are primarily conjunctions, most of which are 

less than 4 words. Results in Figure 6 show that 

the uni-gram and tri-gram transitional expres-

sions seem more fitting for our transfer model. 

One possible reason is that uni-gram or tri-gram 

conjunctions are more utilized in an English text. 

In a conjunction expression list proposed by 

(Williams, 1983) which summarizes the differ-
ent kinds of conjunctions based on the work of 

Halliday and Hassan (1976), we obtain the statis-

tical results on uni-/bi-/tri-gram expressions, 

which are about 52.1%/16.9%/23.9% respective-

ly. 

 

4.5 Experiments on Big Training Data 

To further evaluate the effectiveness of the pro-

posed models, we also conducted an experiment 

on a larger set of bilingual training data from the 

LDC corpus7 for translation model and transfer 

model. The training corpus contains 2.1M sen-

tence pairs with approximately 27.7M Chinese 

words and 31.9M English words. All the other 

settings were the same as the SMT experiments 

of sub-section 4.3. The final BLEU scores on 

NIST05 and NIST06 are given in Table 4.  

The results in Table 4 further verify the effec-

tiveness of our proposed models. The best per-

formance with bold marking scored as high as 

0.83 and 0.64 BLEU points, respectively over the 

                                                 
7 LDC category number: LDC2000T50, DC2002E18, 
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, 

LDC2005T10 and LDC2005T34. 

baseline system on NIST05 and NIST06 evalua-

tion data.  

 

4.6 Translation Examples  

Two SMT examples of Chinese-to-English are 

given in Table 5. We observe that compared to 

the baseline, our approach has obvious ad-

vantages on translating the implicit relations, due   

to generating translational expressions on target 

side. Moreover, with the transitional expressions, 

cohesion of the entire translation improves. No-

tably, the transitional expressions in this work 

like “including, there are, the core of which” are 

not linguistic conjunctions. We would like to call 

them “generalized” conjunctions, because they 

tie semantic fragments together, analogously to 

linguistic conjunctions. 

5 Related Work  

Improving cohesion for complex sentences or 

discourse translation has attracted much attention 

in recent years. Such research efforts can be 

roughly divided into two groups: 1) research on 

lexical cohesion, which mainly contributes to the 

selection of generated target words; 2) efforts to 

improve the grammatical cohesion, such as dis-

ambiguation of references and connectives.  

In lexical cohesion work, (Gong et al., 2011; 

Xiao et al., 2011; Wong and Kit, 2012) built dis-

course-based models to ensure lexical cohesion 

or consistency. In (Xiong et al., 2013a), three 

different features were designed to capture the 

lexical cohesion for document-level machine 

translation. (Xiong et al., 2013b) incorporated 

lexical-chain-based models (Morris and Hirst, 

1991) into machine translation. They generated 

the target lexical chains based on the source 

 

Figure 6.  Different translation qualities along 

with different n-grams for transfer model.  

30

31

32

33

34

35

NIST05 NIST06

BLEU 

Testing Set 

Uni-gram

Bi-gram

Tri-gram

 NIST05 NIST06 

Baseline    35.20     35.52 

+Flattened Rule    36.03** 36.10* 

+TFS    35.56* 36.04* 

+TFS +Flattened Rule    36.02**    36.16** 

+ Flattened Rule: only use the tagged-flattened transla-

tion rules 

 + TFS:  only use the transfer model score as an addi-

tional feature (3-gram transitional phrase) 

+ TFS + Flattened Rule: both are used 

*: value with * means that it is significantly better than 

the baseline with p<0.05 

**: value with ** means that it is significantly better 

than the baseline with p<0.01 

Table 4. BLEU scores on the large-scale training 

data.  

857



chains via maximum entropy classifiers, and 

used the target chains to work on the word selec-

tion. 

 Limited work has been conducted on gram-

matical cohesion. (Marcu et al., 2000) designed a 

discourse structure transfer module, but it fo-

cused on converting the semantic structure rather 

than actual translation. (Tu et al., 2013b) provid-

ed a Rhetorical-Structure-Theory-based tree-to-

string translation method for complex sentences 

with explicit relations inspired by (Marcu et al., 

2000), but their models worked only for explicit 

functional relations, and they were concerned 

mainly with the translation integrity of semantic 

span rather than cohesion. (Meyer and Popescu-

Belis, 2012) used sense-labeled discourse con-

nectives for machine translation from English to 

French. They added the labels assigned to con-

nectives as an additional input to an SMT system, 

but their experimental results show that the im-

provements under the evaluation metric of BLEU 

were not significant. (Nagard and Koehn, 2010) 

addresses the problems of reference or anaphora 

resolution inspired by work of Mitkov et al. 

(1995). 

To the best of our knowledge, our work is the 

first attempt to exploit the source functional rela-

tionship to generate the target transitional ex-

pressions for grammatical cohesion, and we have 

successfully incorporated the proposed models 

into an SMT system with significant improve-

ment of BLEU metrics. 

6 Conclusion 

In this paper, we focus on capturing cohesion 

information to enhance the grammatical cohesion 

of machine translation. By taking the source CSS 

into consideration, we build bridges to connect 

the source functional relationships in CSS to tar-

get transitional expressions; such a process is 

very similar to human translating. 

    Our contributions can be summarized as: 1) 

the new translation rules are more discriminative 

and sensitive to cohesive information by convert-

ing the source string into a CSS-based tagged-

flattened string; 2) the new additional features 

embedded in the log-linear model can encourage 

the decoder to produce transitional expressions. 

The experimental results show that significant 

improvements have been achieved on various 

test data, meanwhile the translations are more 

cohesive and smooth, which together demon-

strate the effectiveness of our proposed models.  

In the future, we will extend our methods to 

other translation models, such as the syntax-

based model, to study how to further improve the 

performance of SMT systems. Besides, more 

language pairs with various linguistic structures 

will be taken into consideration.  

Acknowledgement 

We would like to thank Jiajun Zhang for provid-

ing the BTG-based hierarchical decoder. The 

research work has been partially funded by the 

Natural Science Foundation of China under 

Grant No. 61333018, the Hi-Tech Research and 

Development Program (“863” Program) of China 

under Grant No. 2012AA011101, and also 

the Key Project of Knowledge Innovation Pro-

gram of Chinese Academy of Sciences under 

Grant No. KGZD-EW-501 as well.  

     

Source 过去三年中，已有三对染色体完成排序， 包括第二十对、第二十一对和第二十二 对 。 

Reference 
In the past three years, the sequencing of three chromosomes has been completed, including 

chromosomes 20 , 21 , and 22 . 

Baseline 
In the past three years , now has three terms of the completion of the chromosomes , 20 , 21 

and 22 . 

Improved 
In the past three years , there are three chromosomes to accomplish , including 20 , 21 and 

22 . 

Source 上述主张构成了一个中国原则的基本涵义，核心是维护中国的主权和领土完整。 

Reference 
The above-mentioned propositions constitute the basic connotation of this one-china principle 

with safeguarding china ' s sovereignty and territorial integrity as its core . 

Baseline 
The above-mentioned propositions constitute the basic meaning of the one-china principle is 

the core of safeguard china ' s sovereignty and territorial integrity . 

Improved 
The above-mentioned propositions constitute the basic meaning of the one-china principle , 

the core of which is to safeguard china ' s sovereignty and territorial integrity . 

Table 5. Examples of baseline and the improved system outputs. 

 

858



References  

Jeff A. Bilmes and Katrin Kirchhoff. Factored lan-

guage models and generalized parallel backoff. In 

Proceedings of the 2003 Conference of the North 

American Chapter of the Association for Compu-

tational Linguistics on Human Language Technol-

ogy: companion volume of the Proceedings of 

HLT-NAACL 2003--short papers-Volume 2: 4-6. 

David Chiang. 2005. A hierarchical phrase-based 

model for statistical machine translation. In Pro-

ceedings of the 43rd Annual Meeting of the Asso-

ciation for Computational Linguistics, pages 263–

270. 

David Chiang. 2007. Hierarchical phrase-based 

translation. Computational Linguistics, pages 

33(2):201–228. 

Zhengxian Gong, Min Zhang, and Guodong Zhou. 

Cache-based document-level statistical machine 

translation, 2011, Edinburgh, Scotland, UK. In 

Proceedings of the 2011 Conference on Empirical 

Methods in Natural Language Processing, pages 

909–919. 

Liane Guillou. 2013. Analysing lexical consistency in 

translation. In Proceedings of the Workshop on 

Discourse in Machine Translation, pages 10–18, 

Sofia 

Michael A.K. Halliday, Hasan R. Cohesion in English. 

1976. London: Longman. 

Zhongjun He, Yao Meng, and Hao Yu. 2010b. Maxi-

mum Entropy Based Phrase Reordering for Hier-

archical Phrase-based Translation. In Proc. of the 

Conf. on Empirical Methods for Natural Language 

Processing (EMNLP), pages 555–563. 

Annie Louis and Ani Nenkova. 2012. A coherence 

model based on syntactic patterns. In Proceedings 

of the 2012 Joint Conference on Empirical Meth-

ods in Natural Language Processing and Computa-

tional Natural Language Learning, pages 1157–

1168, Jeju Island, Korea, July.  

William C Mann and Sandra A Thompson. 1988. 

Rhetorical structure theory: Toward a functional 

theory of text organization. Text, 8(3):243–281. 

Ruslan Mitkov, Sung-Kwon Choi, and Randall Sharp. 

1995. Anaphora resolution in Machine Transla-

tion. In Proceedings of the Sixth International 

Conference on Theoretical and Methodological Is-

sues in Machine Translation. 

Thomas Meyer and Andrei Popescu-Belis. Using 

sense-labeled discourse connectives for statistical 

machine translation, 2012, In Proceedings of the 

Joint Workshop on Exploiting Synergies between 

Information Retrieval and Machine Translation 

(ESIRMT) and Hybrid Approaches to Machine 

Translation (HyTra), pages:129-138. 

Jane Morris and Graeme Hirst. 1991. Lexical cohe-

sion computed by thesaural relations as an indica-

tor of the structure of text. Comput. Linguist., 

17(1):21–48, March. 

Ronan L Nagard and Philipp Koehn. 2010, Aiding 

pronoun translation with co-reference resolution, 

In proceedings of the Joint Fifth Workshop on Sta-

tistical Machine Translation and MetricsMATR, 

pages 252-261. 

Franz J Och and Hermann Ney. 2002. Discriminative 

training and maximum entropy models for statisti-

cal machine translation. In Proc. of ACL, pages 

295–302. 

Kishore Papineni, Salim Roukos, Todd Ward, et al. 

2002, BLEU: a method for automatic evaluation 

of machine translation. In proceedings of the 40th 

annual meeting on association for computational 

linguistics. pages: 311-318. 

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 

Miltsakaki, Livio Robaldo, Aravind Joshi, and 

Bonnie Webber. 2008. The Penn Discourse Tree-

bank 2.0. In Proceedings of the 6th International 

Conference on Language Resources and Evalua-

tion (LREC 2008). 

Williams Ray. Teaching the Recognition of Cohesive 

Ties in Reading a Foreign, 1983. Reading in a 

foreign language, 1(1), pages: 35-52. 

Radu Soricut and Daniel Marcu. 2003. Sentence level 

discourse parsing using syntactic and lexical in-

formation. In Proceedings of the 2003 Conference 

of the North American Chapter of the Association 

for Computational Linguistics on Human Lan-

guage Technology-Volume 1, pages 149–156. 

Mei Tu, Yu Zhou, and Chengqing Zong. 2013a, A 

Novel Translation Framework Based on Rhetori-

cal Structure Theory. In Proceedings of the 51st 

Annual Meeting of the Association for Computa-

tional Linguistics, short paper, Sofia, Bulgaria, 

pages 370–374. 

Mei Tu, Yu Zhou, Chengqing Zong. 2013b, Automat-

ically Parsing Chinese Discourse Based on Maxi-

mum Entropy. In The 2nd Conference on Natural 

Language Processing & Chinese Computing. 

Ashish Vaswani, Liang Huang and David Chiang, 

Huang L, Chiang D. 2012, Smaller alignment 

models for better translations: unsupervised word 

alignment with the l 0-norm. In Proceedings of the 

50th Annual Meeting of the Association for Com-

putational Linguistics: Long Papers-Volume 

1,pages 311-319. 

Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 

Document-level consistency verification in ma-

chine translation. September 2011, Xiamen, China. 

In Proceedings of the 2011 MT summit XIII, pag-

es 131–138. 

859



Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-

mum entropy based phrase reordering model for 

statistical machine translation. In Proceedings of 

the 44th Annual Meeting of the Association for 

Computational Linguistics, pages 521–528. 

Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv, 

and Qun Liu. 2013 (a). Modeling lexical cohesion 

for document-level machine translation. In Pro-

ceedings of the Twenty-Third International Joint 

Conference on Artificial Intelligence (IJCAI-13), 

Beijing, China, August. 

Deyi Xiong, Ding Yang, Min Zhang and Chew Lim 

Tan, 2013 (b). Lexical Chain Based Cohesion 

Models for Document-Level Statistical Machine 

Translation. In Proceedings of the 2013 Confer-

ence on Empirical Methods in Natural Language 

Processing, pages: 1563-1573. 

Richard Zens and Hermann Ney. 2006. Discrimina-

tive reordering models for statistical machine 

translation. In Proceedings of theWorkshop on 

Statistical Machine Translation, pages 55–63. 

Qiang Zhou, 2004, Annotation Scheme for Chinese 

Treebank, Journal of Chinese Information Pro-

cessing, 18(4): 1-8. 

 

 

 

 

 

 

860


