



















































THU_NGN at SemEval-2018 Task 10: Capturing Discriminative Attributes with MLP-CNN model


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 958â€“962
New Orleans, Louisiana, June 5â€“6, 2018. Â©2018 Association for Computational Linguistics

THU NGN at SemEval-2018 Task 10: Capturing Discriminative
Attributes with MLP-CNN model

Chuhan Wu1, Fangzhao Wu2, Zhigang Yuan1, Sixing Wu1 and Yongfeng Huang1
1Tsinghua National Laboratory for Information Science and Technology,

Department of Electronic Engineering, Tsinghua University Beijing 100084, China
2Microsoft Research Asia

{wuch15,yuanzg14,wu-sx15,yfhuang}@mails.tsinghua.edu.cn
wufangzhao@gmail.com

Abstract
Existing semantic models are capable of iden-
tifying the semantic similarity of words. How-
ever, itâ€™s hard for these models to discrimi-
nate between a word and another similar word.
Thus, the aim of SemEval-2018 Task 10 is
to predict whether a word is a discrimina-
tive attribute between two concepts. In this
task, we apply a multilayer perceptron (MLP)-
convolutional neural network (CNN) model to
identify whether an attribute is discriminative.
The CNNs are used to extract low-level fea-
tures from the inputs. The MLP takes both the
flatten CNN maps and inputs to predict the la-
bels. The evaluation F-score of our system on
the test set is 0.629 (ranked 15th), which in-
dicates that our system still needs to be im-
proved. However, the behaviours of our sys-
tem in our experiments provide useful infor-
mation, which can help to improve the collec-
tive understanding of this novel task.

1 Introduction

Evaluating the similarity of words is an important
task in semantic modeling. There have been dif-
ferent approaches based on corpus statistics (Jiang
and Conrath, 1997; Mihalcea et al., 2006) and on-
tology (Seco et al., 2004; SaÌnchez et al., 2012).
After an effective word representation proposed by
mikolov et al (2013), word similarity can be evalu-
ated based on word embedding weights (Levy and
Goldberg, 2014). Usually higher cosine similar-
ity of word embedding vectors indicates higher se-
mantic similarity.

However, existing semantic methods are not
capable of discriminating similar words between
each other without additional information. For ex-
ample, it is easy for these models to tell â€œdogâ€ and
â€œpuppyâ€ is similar, but they canâ€™t tell the differ-
ences between each other. It limits the use of these
models to mine such fine-grained semantic infor-
mation from texts. Thus, the SemEval-2018 Task

10 is proposed to determine whether an attribute
can help to discriminate between two words(Krebs
et al., 2018). One can express semantic differences
between concepts by referring to attributes associ-
ated with those concepts. The differences between
concepts can usually be identified by the presence
or absence of specific attributes. For example, the
attributes â€œredâ€ and â€œyellowâ€ are discriminative
for concepts â€œappleâ€ and â€œbananaâ€, while â€œsweetâ€
or â€œfruitâ€ are not discriminative.

Capturing such discriminative attributes can be
regarded as a binary classification task: given two
words and an attribute, predict whether the at-
tribute is a difference between the two words. Ex-
isting methods to capture discriminative attributes
are mainly based on dictionary (Parikh and Grau-
man, 2011). In recent years, CNN have been suc-
cessfully applied to text classification task (Kim,
2014). In order to address this task, we develop
a system based on MLP-CNN model. Firstly, the
input words will be converted into dense vectors
using the combination of different word embed-
dings. Then the CNN layers are used to extract
features from these vectors. Finally, a MLP classi-
fier is used to predict binary labels based on both
embedding and CNN features. The experimental
results show that our model outperforms several
baseline neural model, and the additional features
can improve the model performance. Our system
still has room for development according to the ex-
perimental analysis. The behaviours of our system
in our experiments can help to further fix and ex-
tend our model.

2 MLP-CNN model with Word Feature

2.1 Network Architecture

The architecture of our MLP-CNN model is
shown in Figure 1. The input of our system is
a pair of words with an attribute. First, an em-

958



bedding layer is used to provide different kinds of
pre-trained embedding weights (v1âˆ’dim) and the
word features (vf âˆ’ dim) . We use three differ-
ent pre-trained embedding weights and concate-
nate them together with the additional features of
each word. Thus, the output of embedding layer is
(v1 + vf )âˆ’ dim.

Second, a 2-layer convolutional neural network
take these vectors as input, and output the flatten
feature maps. We use zeros to pad in both sides
to keep the same output length. Since the length
of inputs is 3, the 3 time steps of the convolutional
feature maps can respectively extract the inherent
relatedness of the first word with attribute, the sec-
ond word with attribute and all three words. The
feature map dimensions of the two CNN layers are
v2 and v3 respectively. In order to reduce the diffi-
culties of gradient propagation in neural networks,
we use a over-layer connection between input and
output of CNN. We concatenate the flatten feature
maps with all word embedding and features to-
gether. Finally a MLP with ReLU and sigmoid ac-
tivation function is used to predict the normalized
binary label. With the help of the over-layer con-
nection, the MLP classifier can learn from high-
level word information and raw semantic infor-
mation at the same time. Since the final labels
are obtained from the triples of words through the
embedding, CNN and dense layers, all parameters
can be tuned in model training.

2.2 Word Embedding

Since there are several out-of-vocabulary words
in the dataset when using single pre-trained word
embedding, we use three different embedding
models to cover them. The three embedding mod-
els include pre-trained word2vec embedding1 pro-
vided by Mikolov et al. (Mikolov et al., 2013),
the Glove embedding2 provided by Pennington
et al. (Pennington et al., 2014) and the fastText
embedding3 released by bojanowski et al. (Bo-
janowski et al., 2016). These embedding weights
are all 300-dim. They are concatenated together as
the representation of input words.

2.3 Word Feature

In our model, we use one-hot encoded POS
tags and two binary features obtained by Word-

1https://code.google.com/archive/p/word2vec/
2http://nlp.stanford.edu/data/glove.840B.300d.zip
3https://s3-us-west-1.amazonaws.com/fasttext-

vectors/wiki.en.vec

banana long     cucumber 

Embedding

word 

embedding

(ğ‘£1 âˆ’ ğ‘‘ğ‘–ğ‘š)

Dense 

layers
ReLU

sigmoid

Output

(feature1) (feature2)     (feature3)

Convolutional layer

Convolutional layer

Flatten

3(ğ‘£1 + ğ‘£ğ‘“)-
ğ‘‘ğ‘–ğ‘šğ‘£2 âˆ’ ğ‘‘ğ‘–ğ‘š

ğ‘£3 âˆ’ ğ‘‘ğ‘–ğ‘š

3ğ‘£3-
ğ‘‘ğ‘–ğ‘š

Features

(ğ‘£ğ‘“ âˆ’ ğ‘‘ğ‘–ğ‘š)

Figure 1: The architecture of our MLP-CNN model.

Net (Miller and Fellbaum, 1998). In the dataset,
the words to be discriminated are nouns, but at-
tributes are nouns, adjectives, verbs and so on.
Thus, POS tags of words can help to identify the
types of attributes and the relationship between
them. )We use the Stanford parser tool4 to get the
POS tags of words.

The WordNet feature we use is based on
synsets. Among every three input words, if one
word is in the synset of another word, the corre-
sponding feature digit will be set to 1, or it will be
set to 0. In this way, a 2-dim synset feature of each
word can be obtained. We use the nltk tool(Bird
et al., 2009) to generate the WordNet features. The
features above are concatenated with word embed-
ding as the input of MLP-CNN model.

2.4 Model Training and Ensemble

Since the train set is unbalanced, we randomly se-
lect same numbers of positive (the attribute is dis-
criminative) and negative (the attribute is not dis-
criminative) samples from the train set every time.
Thus, the training data we used in our experiments

4https://nlp.stanford.edu/software/lex-parser.shtml

959



consists of the sampled data from the train set and
80% data sampled from the dev set. The remaining
20% part in dev set is used for validation.

Model ensemble strategy has been proven use-
ful to neural networks (Wu et al., 2017). There-
fore, we build different training samples using the
method described in the above paragraph and train
our model for 10 times. The final predictions on
the test set are the average of the predictions of the
10 models. In this way, the performance of neural
model can be further improved.

3 Experiment

3.1 Experiment Settings

The dataset we use is constructed based on the ap-
proach proposed by Lazaridou et al. (2016) and
the initial source of data is provided by McRae et
al. (2005). The entire dataset contains 17,547 sam-
ples for training, 2,722 for validation and 2,340
for testing. The training set is automatically gen-
erated, while the validation and test set are manu-
ally refined. The models will be evaluated by F1-
measure, as is standard in a binary classification
task.

In our network, the kernel sizes of CNN are set
to 3. The dimensions of feature maps v2 and v3
are set to 256, and the dimensions of dense lay-
ers are 300. The dropout rate of both embedding
weights and CNN is set to 0.2. The training batch
size is set to 50. We use Adam as the optimizer for
network training, which takes 10 epochs per time.
We train our model for 10 times and average their
predictions on the test set.

3.2 Performance Evaluation

The experimental results on the test and validation
set are shown in Table 1. For comparison, we also
present several baseline models here. Our official
submission is the MLP-CNN model with ensem-
ble techniques. Our F-score is 62.9 (ranked 15th)
in the evaluation phase. From the evaluation re-
sults, we can see that our model outperforms these
baseline models. It shows that our network archi-
tecture can learn more semantic information from
the words and attributes. However, our system
needs to be improved compared with the top sys-
tem (75.0 of F-score). In addition, the testing re-
sults are much lower than validation results. Some
detailed information will be analysis in the next
section.

Model F-score
test validation

MLP 53.5 63
CNN 57.8 66.2

MLP-CNN 61.7 71.5
MLP-CNN w/o over-layer 61.0 70.3

MLP-CNN+ensemble (ours) 62.9 73.4

Table 1: Performance evaluation of our system and sev-
eral baselines.

4 Discussion

4.1 Influence of Trainable Word Embedding
The influence of different word embedding
weights and fine-tuning them or not is shown in
Table 2. Note that we donâ€™t apply the model en-
semble technique here. From the results, we can
see that the combination of different word embed-
ding can significantly improve the model perfor-
mance. It may be because that using different
word embedding can provide richer semantic in-
formation. In addition, using the combinations of
different word embedding can cover more words
and the out-of-vocabulary words in the single em-
bedding file can be reduced. Thus, the predictions
of such words can be more accurate.

However, we find fine-tuning the pre-trained
word embedding is not a good choice. The fine-
tuned model performance is significantly worse
than models using untrainable embedding. Since
the training, validation and test sets have no fea-
ture overlap between them, fine-tuning the embed-
ding weights will lead to serious over-fitting and
poor model generalization ability. We fine-tuned
the embedding of our models used in the official
submission, so the results are lower than the mod-
els with untrainable embedding.

4.2 Influence of Word Features
The influence of the two types of features is shown
in Table 3. The results show that additional word
features can improve the performance of our neu-
ral model. Attributes with different POS tags pro-
vide different semantic information. For example,
given a pair of words â€œboyâ€ and â€œwomanâ€, the at-
tributes â€œyoungâ€ and â€œrunâ€ describe very differ-
ent aspects. Therefore, POS tag features can help
the model to extract different features from the in-
put words. Another feature based on WordNet can
also improve our model. It may be because if the
attribute is in the synsets of a concept, itâ€™s usu-

960



pre-trained
Embedding

F-score
test validation

w/o 44.9 63.7
word2vec+tune 50.6 65.8

Glove+tune 53.5 66.7
fastText+tune 51.5 64.4

all+tune 61.7 71.5
word2vec-tune 54.9 66

Glove-tune 57.7 69.6
fastText-tune 58.3 68.9

all-tune 65.7 75.5

Table 2: Comparisons of using different pre-trained
embedding.

ally an attribute of this concept. Thus, the synset
information can help the model to identify the re-
lationships between words and attibutes.

Features F-score
w/o 59.2

+POS 61
+WordNet 60.1

+POS+WordNet 61.7

Table 3: Influence of word features on the test set.

4.3 Case Study

Several examples of model predictions on the test
set are shown in Table 4. From the true predic-
tions, we can see that our model can capture sim-
ple attributes of concepts such as colors. How-
ever, more complex relationships between words
and attributes are difficult for our system to mine.
For example, the word â€œmouseâ€ can be an animal
or electronic device. Itâ€™s hard to identify such se-
mantic differences without incorporating external
knowledge, since the information provided by the
training data is limited.

True Positive False Positive
corn,tomato,yellow alcohol,liquor,strong

ant,snail,black bar,shop,sell
True Negative False Negative

father,brother,family mouse,dog,plastic
father,mother,parent engine,vehicle,component

Table 4: Representative examples of the predictions on
the test set.

5 Conclusion

Discriminating similar words between each other
without additional information is difficult for ex-
isting semantic models. Therefore, the SemEval-
2018 task 10 is proposed to fill this gap. In this
paper, we apply a MLP-CNN model with word
feature to this task. In our model, the input and
output of our CNN are highway connected. They
are taken by a MLP classifier for binary classifi-
cation. Based on this model, the local relation-
ships between each pair of words can be mined.
Our evaluation F-score is 62.9 (ranked 15th). The
detailed analysis on our system shows our system
can be further improved.

Acknowledgments

The authors thank the reviewers for their in-
sightful comments and constructive suggestions
on improving this work. This work was sup-
ported in part by the National Key Research
and Development Program of China under Grant
2016YFB0800402 and in part by the National Nat-
ural Science Foundation of China under Grant
U1705261, Grant U1536207, Grant U1536201
and U1636113.

References
Steven Bird, Ewan Klein, and Edward Loper. 2009.

Natural language processing with Python: analyz-
ing text with the natural language toolkit. â€ Oâ€™Reilly
Media, Inc.â€.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

Jay J Jiang and David W Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxon-
omy. arXiv preprint cmp-lg/9709008.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Alicia Krebs, Alessandro Lenci, and Denis Paperno.
2018. Semeval-2018 task 10: Capturing discrim-
inative attributes. In Proceedings of the 12th inter-
national workshop on semantic evaluation (SemEval
2018).

Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2016. The red one!: On learning to refer
to things based on their discriminative properties.
arXiv preprint arXiv:1603.02618.

961



Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems,
pages 2177â€“2185.

Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving
things. Behavior research methods, 37(4):547â€“559.

Rada Mihalcea, Courtney Corley, Carlo Strapparava,
et al. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775â€“780.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

George Miller and Christiane Fellbaum. 1998. Word-
net: An electronic lexical database.

Devi Parikh and Kristen Grauman. 2011. Interactively
building a discriminative vocabulary of nameable at-
tributes. In Computer Vision and Pattern Recog-
nition (CVPR), 2011 IEEE Conference on, pages
1681â€“1688. IEEE.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532â€“1543.

David SaÌnchez, Montserrat Batet, David Isern, and
Aida Valls. 2012. Ontology-based semantic similar-
ity: A new feature-based approach. Expert Systems
with Applications, 39(9):7718â€“7728.

Nuno Seco, Tony Veale, and Jer Hayes. 2004. An in-
trinsic information content metric for semantic sim-
ilarity in wordnet. In ECAI, volume 16, page 1089.

Chuhan Wu, Fangzhao Wu, Yongfeng Huang, Sixing
Wu, and Zhigang Yuan. 2017. Thu ngn at ijcnlp-
2017 task 2: Dimensional sentiment analysis for chi-
nese phrases with deep lstm. Proceedings of the
IJCNLP 2017, Shared Tasks, pages 47â€“52.

962


