










































Revisiting the Case for Explicit Syntactic Information in Language Models


NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 50–58,
Montréal, Canada, June 8, 2012. c©2012 Association for Computational Linguistics

Revisiting the Case for Explicit Syntactic Information in Language Models

Ariya Rastrow, Sanjeev Khudanpur, Mark Dredze
Human Language Technology Center of Excellence,

Center for Language and Speech Processing, Johns Hopkins University
Baltimore, MD USA

{ariya,khudanpur,mdredze}@jhu.edu

Abstract

Statistical language models used in deployed
systems for speech recognition, machine
translation and other human language tech-
nologies are almost exclusively n-gram mod-
els. They are regarded as linguistically naı̈ve,
but estimating them from any amount of text,
large or small, is straightforward. Further-
more, they have doggedly matched or out-
performed numerous competing proposals for
syntactically well-motivated models. This un-
usual resilience of n-grams, as well as their
weaknesses, are examined here. It is demon-
strated that n-grams are good word-predictors,
even linguistically speaking, in a large major-
ity of word-positions, and it is suggested that
to improve over n-grams, one must explore
syntax-aware (or other) language models that
focus on positions where n-grams are weak.

1 Introduction

Language models (LM) are crucial components in
tasks that require the generation of coherent natu-
ral language text, such as automatic speech recog-
nition (ASR) and machine translation (MT). Most
language models rely on simple n-gram statistics
and a wide range of smoothing and backoff tech-
niques (Chen and Goodman, 1998). State-of-the-art
ASR systems use (n − 1)-gram equivalence classi-
fication for the language model (which result in an
n-gram language model).

While simple and efficient, it is widely believed
that limiting the context to only the (n − 1) most
recent words ignores the structure of language, and
several statistical frameworks have been proposed

to incorporate the “syntactic structure of language
back into language modeling.” Yet despite consider-
able effort on including longer-dependency features,
such as syntax (Chelba and Jelinek, 2000; Khudan-
pur and Wu, 2000; Collins et al., 2005; Emami
and Jelinek, 2005; Kuo et al., 2009; Filimonov and
Harper, 2009), n-gram language models remain the
dominant technique in automatic speech recognition
and machine translation (MT) systems.

While intuition suggests syntax is important, the
continued dominance of n-gram models could in-
dicate otherwise. While no one would dispute that
syntax informs word choice, perhaps sufficient in-
formation aggregated across a large corpus is avail-
able in the local context for n-gram models to per-
form well even without syntax. To clearly demon-
strate the utility of syntactic information and the de-
ficiency of n-gram models, we empirically show that
n-gram LMs lose significant predictive power in po-
sitions where the syntactic relation spans beyond the
n-gram context. This clearly shows a performance
gap in n-gram LMs that could be bridged by syntax.

As a candidate syntactic LM we consider the
Structured Language Model (SLM) (Chelba and Je-
linek, 2000), one of the first successful attempts to
build a statistical language model based on syntac-
tic information. The SLM assigns a joint probabil-
ity P (W,T ) to every word sequence W and every
possible binary parse tree T , where T ’s terminals
are words W with part-of-speech (POS) tags, and
its internal nodes comprise non-terminal labels and
lexical “heads” of phrases. Other approaches in-
clude using the exposed headwords in a maximum-
entropy based LM (Khudanpur and Wu, 2000), us-

50



ing exposed headwords from full-sentence parse tree
in a neural network based LM (Kuo et al., 2009),
and the use of syntactic features in discriminative
training (Rastrow et al., 2011). We show that the
long-dependencies modeled by SLM, significantly
improves the predictive power of the LM, specially
in positions where the syntactic relation is beyond
the reach of regular n-gram models.

2 Weaknesses of n-gram LMs

Consider the following sentence, which demon-
strates why the (n− 1)-gram equivalence classifica-
tion of history in n-gram language models may be
insufficient:
<s> i asked the vice president for
his endorsement </s>
In an n-gram LM, the word for would be modeled
based on a 3-gram or 4-gram history, such as <vice
president> or <the vice president>.
Given the syntactic relation between the preposition
for and the verb asked (which together make a
compound verb), the strongest evidence in the his-
tory (and hence the best classification of the history)
for word for should be <asked president>,
which is beyond the 4-gram LM. Clearly, the
syntactic relation between a word position and the
corresponding words in the history spans beyond
the limited (n − 1)-gram equivalence classification
of the history.

This is but one of many examples used for moti-
vating syntactic features (Chelba and Jelinek, 2000;
Kuo et al., 2009) in language modeling. How-
ever, it is legitimate to ask if this deficiency could
be overcome through sufficient data, that is, accu-
rate statistics could somehow be gathered for the n-
grams even without including syntactic information.
We empirically show that (n− 1)-gram equivalence
classification of history is not adequate to predict
these cases. Specifically, n-gram LMs lose predic-
tive power in the positions where the headword rela-
tion, exposed by the syntactic structure, goes beyond
(n− 1) previous words (in the history.)

We postulate the following three hypotheses:

Hypothesis 1 There is a substantial difference in
the predictive power of n-gram LMs at positions
within a sentence where syntactic dependencies
reach further back than the n-gram context versus

positions where syntactic dependencies are local.

Hypothesis 2 This difference does not diminish by
increasing training data by an order of magnitude.

Hypothesis 3 LMs that specifically target positions
with syntactically distant dependencies will comple-
ment or improve over n-gram LMs for these posi-
tions.

In the following section (Section 3), we present a set
of experiments to support the hypotheses 1 and 2.
Section 4 introduces a SLM which uses dependency
structures followed by experiments in Section 5.

3 Experimental Evidence

In this section, we explain our experimental evi-
dence for supporting the hypotheses stated above.
First, Section 3.1 presents our experimental design
where we use a statistical constituent parser to iden-
tify two types of word positions in a test data,
namely positions where the headword syntactic re-
lation spans beyond recent words in the history and
positions where the headword syntactic relation is
within the n-gram window. The performance of
an n-gram LM is measured on both types of posi-
tions to show substantial difference in the predictive
power of the LM in those positions. Section 3.3 de-
scribes the results and analysis of our experiments
which supports our hypotheses.

Throughout the rest of the paper, we refer to
a position where the headword syntactic relation
reaches further back than the n-gram context as a
syntactically-distant position and other type of posi-
tions is referred to as a syntactically-local position.

3.1 Design

Our experimental design is based on the idea of
comparing the performance of n-gram LMs for
syntactically-distant vs. syntactically-local . To this
end, we first parse each sentence in the test set us-
ing a constituent parser, as illustrated by the exam-
ple in Figure 1. For each word wi in each sentence,
we then check if the “syntactic heads” of the preced-
ing constituents in the parse ofw1, w2, · · · , wi−1 are
within an (n− 1) window of wi. In this manner, we
split the test data into two disjoint sets, M and N ,

51



!"!#$%&'!!!!!()&!*"+&!,-&$"'&.(!!!!!!/0-!!!!!!!)"$!&.'0-$&1&.(!

232!

42!

5!

67!

62!

89! 442! 442!

42!

:4! 232;! 44!

442!

22!

#$%&'!

#$%&'!

/0-!

&.'0-$&1&.(!,-&$"'&.(!
"!

Figure 1: Example of a syntactically distant position in
a sentence: the exposed headwords preceding for are
h.w−2 =asked and h.w−1 = president, while the
two preceding words are wi−2 = vice and wi−1 =
president.

as follows,

M = {j|positions s.t h.w−1, h.w−2 = wj−1, wj−2}
N = {j|positions s.t h.w−1, h.w−2 6= wj−1, wj−2}

Here, h−1 and h−2 correspond, respectively, to the
two previous exposed headwords at position i, based
on the syntactic structure. Therefore, M corre-
sponds to the positions in the test data for which two
previous exposed heads match exactly the two previ-
ous words. Whereas, N corresponds to the position
where at least on of the exposed heads is further back
in the history than the two previous words, possibly
both.

To extract the exposed headwords at each posi-
tion, we use a constituent parser to obtain the syn-
tactic structure of a sentence followed by headword
percolation procedure to get the headwords of cor-
responding syntactic phrases in the parse tree. The
following method, described in (Kuo et al., 2009),
is then used to extract exposed headwords from the
history of position i from the full-sentence parse
trees:

1. Start at the leaf corresponding to the word posi-
tion (wi) and the leaf corresponding to the pre-
vious context word (wi−1).

2. From each leaf, go up the tree until the two
paths meet at the lowest common ancestor
(LCA).

3. Cut the link between the LCA and the child that
is along the path from the context word wi−1.

The head word of the the LCA child, the one
that is cut, is chosen as previous exposed head-
word h.w−1.

These steps may be illustrated using the parse tree
shown in Figure 1. Let us show the procedure for
our example from Section 2. Figure 1 shows the cor-
responding parse tree of our example. Considering
word position wi=for and wi−1=president and
applying the above procedure, the LCA is the node
VPasked. Now, by cutting the link from VPasked to
NPpresident the word president is obtained as
the first exposed headword (h.w−1).

After the first previous exposed headword has
been extracted, the second exposed headword also
can be obtained using the same procedure, with
the constraint that the node corresponding the sec-
ond headword is different from the first (Kuo et al.,
2009). More precisely,

1. set k = 2

2. Apply the above headword extraction method
between wi and wi−k.

3. if the extracted headword has previously been
chosen, set k = k + 1 and go to step (2).

4. Otherwise, return the headword as h.w−2.

Continuing with the example of Figure 1, after
president is chosen as h.w−1, asked is cho-
sen as h.w−2 of position for by applying the pro-
cedure above. Therefore, in this example the po-
sition corresponding to word for belongs to the
set N as the two extracted exposed headwords
(asked,president) are different from the two
previous context words (vice,president).

After identifying sets N andM in our test data,
we measure perplexity of n-gram LMs on N , M
and N ∪M separately. That is,

PPLN∪M = exp

[
−
∑

i∈N∪M log p(wi|W
i−1
i−n+1)

|N ∪M|

]

PPLN = exp

[
−
∑
i∈N

log p(wi|W i−1i−n+1)
|N |

]

PPLM = exp

[
−
∑
i∈M

log p(wi|W i−1i−n+1)
|M|

]
,

52



where p(wi|wi−1wi−2 · · ·wi−n+1) is the condi-
tional probability calculated by an n-gram LM at
position i and |.| is the size (in number of words)
of the corresponding portion of the test.

In addition, to show the performance of n-gram
LMs as a function of training data size, we train
different n-gram LMs on 10%,20%,· · · ,100% of a
large corpus of text and report the PPL numbers us-
ing each trained LM with different training data size.
For all sizes less than 100%, we select 10 random
subset of the training corpus of the required size, and
report the average perplexity of 10 n-gram models.
This will enable us to observe the improvement of
the n-gram LMs on as we increase the training data
size. The idea is to test the hypothesis that not only
is there significant gap between predictive power of
the n-gram LMs on setsN andM, but also that this
difference does not diminish by adding more train-
ing data. In other words, we want to show that the
problem is not due to lack of robust estimation of
the model parameters but due to the fact that the in-
cluded features in the model (n-grams) are not in-
formative enough for the positions N .

3.2 Setup

The n-gram LMs are built on 400M words from
various Broadcast News (BN) data sources includ-
ing (Chen et al., 2006): 1996 CSR Hub4 Language
Model data, EARS BN03 closed captions, GALE
Phase 2 Distillation GNG Evaluation Supplemen-
tal Multilingual data, Hub4 acoustic model training
scripts (corresponding to the 300 Hrs), TDT4 closed
captions, TDT4 newswire, GALE Broadcast Con-
versations, and GALE Broadcast News. All the LMs
are trained using modified Kneser-Ney smoothing.
To build the LMs, we sample from each source and
build a source specific LM on the sampled data. The
final LMs are then built by interpolating those LMs.
Also, we do not apply any pruning to the trained
LMs, a step that is often necessary for speech recog-
nition but not so for perplexity measurement. The
test set consists of the NIST rt04 evaluation data set,
dev04f evaluation set, and rt03 evaluation set. The
test data includes about 70K words.

We use the parser of (Huang and Harper, 2009),
which achieves state-of-the-art performance on
broadcast news data, to identify the word poisons
that belong to N and M, as was described in Sec-

tion 3.1. The parser is trained on the Broadcast News
treebank from Ontonotes (Weischedel et al., 2008)
and the WSJ Penn Treebank (Marcus et al., 1993)
along with self-training on 1996 Hub4 CSR (Garo-
folo et al., 1996) utterances.

3.3 Analysis

We found that |N ||N∪M| ≈ 0.25 in our test data. In
other words, two previous exposed headwords go
beyond 2-gram history for about 25% of the test
data.

!"#

$%#

$"#

&%#

&"#

'%%#

'%# (%# )%# *%# "%# +%# !%# $%# &%# '%%#
,-

./
01
-#
22

3#
45

6#
78/9:9:;#</=/#>9?-#456#

@AB# @# B#

(a)

!"#

$%#

$"#

&%#

&"#

'%#

'"#

(%%#

(%# )%# *%# +%# "%# !%# $%# &%# '%# (%%#

,-
./
01
-#
22

3#
45

6#

78/9:9:;#</=/#>9?-#456#

@AB# @# B#

(b)

Figure 2: Reduction in perplexity with increasing training
data size on the entire test setN +M, on its syntactically
local subset M, and the syntactically distant subset N .
The figure shows relative perplexity instead of absolute
perplexity — 100% being the perplexity for the smallest
training set size — so that (a) 3-gram and (b) 4-gram LMs
may be directly compared.

We train 3-gram and 4-gram LMs on
10%,20%,· · · ,100% of the BN training data,
where each 10% increase corresponds to about
40M words of training text data. Figure 2 shows
reduction in perplexity with increasing training data
size on the entire test setN+M, on its syntactically
local subsetM, and the syntactically distant subset
N . The figure basically shows relative perplexity
instead of absolute perplexity — 100% being the

53



Position Training Data Size
in 40M words 400M words

Test Set 3-gram 4-gram 3-gram 4-gram
M 166 153 126 107
N 228 217 191 171

N +M 183 170 143 123
PPLN
PPLM

138% 142% 151% 161%

Table 1: Perplexity of 3-gram and 4-gram LMs on syntac-
tically local (M) and syntactically distant (N ) positions
in the test set for different training data sizes, showing the
sustained higher perplexity in distant v/s local positions.

perplexity for the smallest training set size — so the
rate of improvement for 3-grams and 4-gram LMs
can be compared. As can be seen from Figure 2,
there is a substantial gap between the improvement
rate of perplexity in syntactically distant positions
compared to that in syntactically local positions
(with 400M woods of training data, this gap is about
10% for both 3-gram and 4-gram LMs). In other
words, increasing the training data size has much
more effect on improving the predictive power of
the model for the positions included inM. Also, by
comparing Figure 2(a) to 2(b) one can observe that
the gap is not overcome by increasing the context
length (using 4-gram features).

Also, to better illustrate the performance of the n-
gram LMs for different portions of our test data, we
report the absolute values of PPL results in Table 1.
It can be seen that there exits a significant difference
between perplexity of sets N and M and that the
difference gets larger as we increase the training data
size.

4 Dependency Language Models

To overcome the lack of predictive power of n-gram
LMs in syntactically-distant positions, we use the
SLM framework to build a long-span LM. Our hope
is to show not only that long range syntactic depen-
dencies improve over n-gram features, but also that
the improvement is largely due to better predictive
power in the syntactically distant positions N .

Syntactic information may be encoded in terms
of headwords and headtags of phrases, which may
be extracted from a syntactic analysis of a sen-
tence (Chelba and Jelinek, 2000; Kuo et al., 2009),

such as a dependency structure. A dependency in
a sentence holds between a dependent (or modifier)
word and a head (or governor) word: the dependent
depends on the head. These relations are encoded in
a dependency tree (Figure 3), a directed graph where
each edge (arc) encodes a head-dependent relation.

The specific parser used to obtain the syntactic
structure is not important to our investigation. What
is crucial, however, is that the parser proceeds left-
to-right, and only hypothesized structures based on
w1, . . . , wi−1 are used by the SLM to predict wi.

Similarly, the specific features used by the parser
are also not important: more noteworthy is that the
SLM uses (h.w−3, h.w−2, h.w−1) and their POS
tags to predict wi. The question is whether this
yields lower perplexity than predicting wi from
(wi−3, wi−2, wi−1).

For the sake of completeness, we next describe
the parser and SLM in some detail, but either may
be skipped without loss of continuity.

The Parser: We use the shift-reduce incremen-
tal dependency parser of (Sagae and Tsujii, 2007),
which constructs a tree from a transition sequence
governed by a maximum-entropy classifier. Shift-
reduce parsing places input words into a queue Q
and partially built structures are organized by a stack
S. Shift and reduce actions consume the queue and
build the output parse on the stack. The classi-
fier g assigns probabilities to each action, and the
probability of a state pg(π) can be computed as the
product of the probabilities of a sequence of ac-
tions that resulted in the state. The parser therefore
provides (multiple) syntactic analyses of the history
w1, . . . , wi−1 at each word position wi.

The Dependency Language Model: Parser states
at position wi, called history-states, are denoted
Π−i = {π0−i, π1−i · · · , π

Ki
−i }, where Ki is the total

number of such states. Given Π−i, the probability
assignment for wi is given by

p(wi|W−i) =
|Π−i|∑
j=1

p
(
wi|f(πj−i)

)
pg(π

j
−i|W−i) (1)

where, W−i is the word history w1, . . . , wi−1 for
wi, π

j
−i is the j

th history-state of position i,
pg(π

j
−i|W−i) is the probability assigned to π

j
−i by

54



step action stack queue
i asked the vice president ...-0

asked the vice president ...shift1 i
the vice president for ...shift2 i asked
the vice president for ...left-reduce3 asked

i

for his endorsement ...shift6 asked the vice president

i

for his endorsement ...left-reduce7 asked the president

i vice

<s>   i   asked   the vice president   for    his  endorsement

Thursday, March 29, 12

for his endorsement ...left-reduce8 asked president

i vicethe

for his endorsement ...right-reduce9 asked

i

vicethe

president

Thursday, March 29, 12

step action stack queue
i asked the vice president ...-0

asked the vice president ...shift1 i
the vice president for ...shift2 i asked
the vice president for ...left-reduce3 asked

i

for his endorsement ...shift6 asked the vice president

i

for his endorsement ...left-reduce7 asked the president

i vice

<s>   i   asked   the vice president   for    his  endorsement

Thursday, March 29, 12

Tuesday, April 3, 12

Figure 3: Actions of a shift-reduce parser to produce
the dependency structure (up to the word president)
shown above.

the parser, and f(πj−i) denotes an equivalence clas-
sification of the parser history-state, capturing fea-
tures from πj−i that are useful for predicting wi.

We restrict f(π) to be based on only the heads of
the partial trees {s0 s1 · · · } in the stack. For exam-
ple, in Figure 3, one possible parser state for pre-
dicting the word for is the entire stack shown after
step 8, but we restrict f(·) to depend only on the
headwords asked/VB and president/NNP.

Given a choice of f(·), the parameters of the
model p(wi|f(πj−i)) are estimated to maximize the
log-likelihood of the training data T using the
Baum-Welch algorithm (Baum, 1972), and the re-
sulting estimate is denoted pML(wi|f(πj−i)).

The estimate pML(w|f(·)) must be smoothed to
handle unseen events, which we do using the method
of Jelinek and Mercer (1980). We use a fine-to-
coarse hierarchy of features of the history-state as
illustrated in Figure 4. With

fM (π−i) → fM−1(π−i) → . . . → f1(π−i)

denoting the set of M increasingly coarser equiv-
alence classifications of the history-state π−i,
we linearly interpolate the higher order esti-
mates pML

(
w|fm(π−i)

)
with lower order estimates

pML
(
w|fm−1(π−i)

)
as

pJM(wi|fm(π−i))
= λfmpML(wi|fm(π−i))

+(1− λfm)pJM(wi|fm−1(π−i)),

for 1 ≤ m ≤ M , where the 0-th order model
pJM(wi|f0(π−i)) is a uniform distribution.

HW+HT :

(h.w0h.t0, h.w�1h.t�1, h.w�2h.t�2)

(h.w0h.t0)

()

(h.w0, h.t0, h.w�1, h.t�1, h.t�2)

(h.w0, h.t0, h.t�1)

(h.t0)

Saturday, April 14, 12

Figure 4: The hierarchal scheme of fine-to-coarse con-
texts used for Jelinek-Mercer smoothing in the SLM.

The coefficients λfm(π−i) are estimated on a held-
out set using the bucketing algorithm suggested by
Bahl (1983), which ties λfm(π−i)’s based on the
count of fm(π−i)’s in the training data. We use the
expected count of the features from the last iteration
of EM training, since the π−i are latent states.

We perform the bucketing algorithm for each level
f1, f2, · · · , fM of equivalence classification sepa-
rately, and estimate the bucketed λc(fm) using the
Baum-Welch algorithm (Baum, 1972) to maximize
the likelihood of held out data, where the word prob-
ability assignment in Eq. 1 is replaced with:

p(wi|W−i) =
|Πi|∑
j=1

pJM

(
wi|fM (πj−i)

)
pg(π

j
−i|W−i).

The hierarchy shown in Figure 4 is used1 for obtain-
ing a smooth estimate pJM(·|·) at each level.

5 SLM Experiments

We train a dependency SLM for two different tasks,
namely Broadcast News (BN) and Wall Street Jour-
nal (WSJ). Unlike Section 3.2, where we swept
through multiple training sets of multiple sizes,

1The original SLM hierarchical interpolation scheme is ag-
gressive in that it drops both the tag and headword from the
history. However, in many cases the headword’s tag alone is
sufficient, suggesting a more gradual interpolation. Keeping the
headtag adds more specific information and at the same time
is less sparse. A similar idea is found, e.g., in the back-off hi-
erarchical class n-gram language model (Zitouni, 2007) where
instead of backing off from the n-gram right to the (n − 1)-
gram a more gradual backoff — by considering a hierarchy of
fine-to-coarse classes for the last word in the history— is used.

55



training the SLM is computationally intensive. Yet,
useful insights may be gained from the 40M word
case. So we choose the source of text most suitable
for each task, and proceed as follows.

5.1 Setup

The following summarizes the setup for each
task:

• BN setup : EARS BN03 corpus, which has
about 42M words serves as our training text.
We also use rt04 (45K words) as our evaluation
data. Finally, to interpolate our structured lan-
guage models with the baseline 4-gram model,
we use rt03+dev04f (about 40K words) data sets
to serve as our development set. The vocabulary
we use in BN experiments has about 84K words.

• WSJ setup : The training text consists of about
37M words. We use eval92+eval93 (10K
words) as our evaluation set and dev93 (9K
words) serves as our development set for inter-
polating SLMs with the baseline 4-gram model.

In both cases, we sample about 20K sentences from
the training text (we exclude them from training
data) to serve as our heldout data for applying the
bucketing algorithm and estimating λ’s. To apply
the dependency parser, all the data sets are first
converted to Treebank-style tokenization and POS-
tagged using the tagger of (Tsuruoka et al., 2011)2.
Both the POS-tagger and the shift-reduce depen-
dency parser are trained on the Broadcast News tree-
bank from Ontonotes (Weischedel et al., 2008) and
the WSJ Penn Treebank (after converting them to
dependency trees) which consists of about 1.2M to-
kens. Finally, we train a modified kneser-ney 4-gram
LM on the tokenized training text to serve as our
baseline LM, for both experiments.

5.2 Results and Analysis

Table 2 shows the perplexity results for BN and WSJ
experiments, respectively. It is evident that the 4-
gram baseline for BN is stronger than the 40M case
of Table 1. Yet, the interpolated SLM significantly
improves over the 4-gram LM, as it does for WSJ.

2To make sure we have a proper LM, the POS-tagger and
dependency parser only use features from history to tag a word
position and produce the dependency structure. All lookahead
features used in (Tsuruoka et al., 2011) and (Sagae and Tsujii,

Language Model Dev Eval
BN

Kneser-Ney 4-gram 165 158
SLM 168 159

KN+SLM Interpolation 147 142
WSJ

Kneser-Ney 4-gram 144 121
SLM 149 125

KN+SLM Interpolation 132 110

Table 2: Test set perplexities for different LMs on the BN
and WSJ tasks.

Also, to show that, in fact, the syntactic depen-
dencies modeled through the SLM parameterization
is enhancing predictive power of the LM in the prob-
lematic regions, i.e. syntactically-distant positions,
we calculate the following (log) probability ratio for
each position in the test data,

log
pKN+SLM(wi|W−i)
pKN(wi|W−i)

, (2)

where pKN+SLM is the word probability assign-
ment of the interpolated SLM at each position, and
pKN(wi) is the probability assigned by the baseline
4-gram model. The quantity above measures the im-
provement (or degradation) gained as a result of us-
ing the SLM parameterization3.

Figures 5(a) and 5(b) illustrate the histogram of
the above probability ratio for all the word positions
in evaluation data of BN and WSJ tasks, respectively.
In these figures the histograms for syntactically-
distant and syntactically-local are shown separately
to measure the effect of the SLM for either of the
position types. It can be observed in the figures
that for both tasks the percentage of positions with
log

pKN+SLM(wi|W−i)
pKN(wi|W−i) around zero is much higher for

syntactically-local (blue bars) than the syntactically-
distant (red bars). To confirm this, we calculate
the average log pKN+SLM(wi|W−i)pKN(wi|W−i) —this is the aver-
age log-likelihood improvement, which is directly

2007) are excluded.
3If log pKN+SLM(wi|W−i)

pKN(wi|W−i)
is greater than zero, then the SLM

has a better predictive power for word position wi. This is a
meaningful comparison due to the fact that the probability as-
signment using both SLM and n-gram is a proper probability
(which sums to one over all words at each position).

56



−1 −0.5 0 0.5 1 1.5 2 2.5 3 3.5 4
0

2

4

6

8

10

12

14

16

Probability Ratio (Log)

P
e

rc
e

n
ta

g
e

 P
o

s
it
io

n
s
 (

%
)

 

 

Syntactically−local positions    (mean=0.1372)
Syntactically−distant postions  (mean=0.2351)

(a) BN

−1 −0.5 0 0.5 1 1.5 2 2.5 3 3.5 4
0

2

4

6

8

10

12

14

16

18

20

22

Probability Ratio (Log)

P
e

rc
e

n
ta

g
e

 P
o

s
it
io

n
s
 (

%
)

 

 

Syntactically−local positions    (mean=0.0984)
Syntactically−distant postions  (mean=0.2124)

(b) WSJ

Figure 5: Probability ratio histogram of SLM to 4-gram
model for (a) BN task (b) WSJ task.

related to perplexity improvement— for each posi-
tion type in the figures.

Table 3, reports the perplexity performance of
each LM (baseline 4-gram, SLM and interpolated
SLM) on different positions of the evaluation data
for BN and WSJ tasks. As it can be observed from
this table, the use of long-span dependencies in the
SLM partially fills the gap between the performance
of the baseline 4-gram LM on syntactically-distant
positionsN versus syntactically-local positionsM.
In addition, it can be seen that the SLM by itself
fills the gap substantially, however, due to its under-
lying parameterization which is based on Jelinek-
Mercer smoothing it has a worse performance on
regular syntactically-local positions (which account
for the majority of the positions) compared to the
Kneser-Ney smoothed LM4. Therefore, to improve
the overall performance, the interpolated SLM takes
advantage of both the better modeling performance
of Kneser-Ney for syntactically-local positions and

4This is merely due to the superior modeling power and
better smoothing of the Kneser-Ney LM (Chen and Goodman,
1998).

Test Set 4-gram SLM 4-gram + SLM
Position BN
M 146 152 132
N 201 182 171

N +M 158 159 142
PPLN
PPLM

138% 120% 129%

WSJ
M 114 120 105
N 152 141 131

N +M 121 125 110
PPLN
PPLM

133% 117% 125%

Table 3: Perplexity on the BN and WSJ evaluation sets for
the 4-gram LM, SLM and their interpolation. The SLM
has lower perplexity than the 4-gram in syntactically dis-
tant positions N , and has a smaller discrepancy PPLNPPLM
between preplexity on the distant and local predictions,
complementing the 4-gram model.

the better features included in the SLM for improv-
ing predictive power on syntactically-distant posi-
tions.

6 Conclusion

The results of Table 1 and Figure 2 suggest that
predicting the next word is about 50% more diffi-
cult when its syntactic dependence on the history
reaches beyond n-gram range. They also suggest
that this difficulty does not diminish with increas-
ing training data size. If anything, the relative diffi-
culty of word positions with nonlocal dependencies
relative to those with local dependencies appears to
increase with increasing training data and n-gram
order. Finally, it appears that language models that
exploit long-distance syntactic dependencies explic-
itly at positions where the n-gram is least effective
are beneficial as complementary models.

Tables 2 and 3 demonstrates that a particular,
recently-proposed SLM with such properties im-
proves a 4-gram LM trained on a large corpus.

Acknowledgments

Thanks to Kenji Sagae for sharing his shift-reduce
dependency parser and the anonymous reviewers for
helpful comments.

57



References
LR Bahl. 1983. A maximum likelihood approach to

continuous speech recognition. IEEE Transactions
on Pattern Analysis and Machine Inteligence (PAMI),
5(2):179–190.

L. E. Baum. 1972. An equality and associated maxi-
mization technique in statistical estimation for proba-
bilistic functions of Markov processes. Inequalities,
3:1–8.

C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283–332.

SF Chen and J Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report, Computer Science Group, Harvard Univer-
sity.

S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596–1608.

M Collins, B Roark, and M Saraclar. 2005. Discrimina-
tive syntactic language modeling for speech recogni-
tion. In ACL.

Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine learning,
60:195–227.

Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.

John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.

Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In EMNLP.

Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381–397.

S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, pages 355–372.

H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and
L. Young-Suk. 2009. Syntactic features for Arabic
speech recognition. In Proc. ASRU.

M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):330.

Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.
2011. Efficient discrimnative training of long-span

language models. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU).

K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. EMNLP-CoNLL, volume 7, pages
1044–1050.

Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi
Kazama. 2011. Learning with Lookahead :
Can History-Based Models Rival Globally Optimized
Models ? In Proc. CoNLL, number June, pages 238–
246.

Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.

Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model unseen
events in speech recognition. Computer Speech &
Language, 21(1):88–104.

58


