



















































Joint A* CCG Parsing and Semantic Role Labelling


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1444–1454,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Joint A∗ CCG Parsing and Semantic Role Labeling

Mike Lewis Luheng He Luke Zettlemoyer
Computer Science & Engineering

University of Washington
Seattle, WA 98195

{mlewis,luheng,lsz}@cs.washington.edu

Abstract

Joint models of syntactic and semantic
parsing have the potential to improve
performance on both tasks—but to date,
the best results have been achieved with
pipelines. We introduce a joint model us-
ing CCG, which is motivated by the close
link between CCG syntax and semantics.
Semantic roles are recovered by labelling
the deep dependency structures produced
by the grammar. Furthermore, because
CCG is lexicalized, we show it is possible
to factor the parsing model over words and
introduce a new A∗ parsing algorithm—
which we demonstrate is faster and more
accurate than adaptive supertagging. Our
joint model is the first to substantially im-
prove both syntactic and semantic accu-
racy over a comparable pipeline, and also
achieves state-of-the-art results for a non-
ensemble semantic role labelling model.

1 Introduction

Joint models of syntactic and semantic parsing
are attractive; they can potentially avoid the error
propagation that is inherent in pipelines by using
semantic models to inform syntactic attachments.
However, in practice, the performance of joint sys-
tems for semantic role labelling (SRL) has been
substantially beneath that of pipelines (Sutton and
McCallum, 2005; Lluís et al., 2009; Johansson,
2009; Titov et al., 2009; Naradowsky et al., 2012;
Lluís et al., 2013; Henderson et al., 2013). In
this paper, we present the first approach to break
this trend, by building on the close relationship of
syntax and semantics in CCG grammars to enable
both (1) a simple but highly effective joint model
and (2) an efficient A∗ parsing algorithm.

Semantic dependencies can span an unbounded
number of syntactic dependencies, causing signif-
icant inference and sparsity challenges for joint

He refused to confirm or deny the reports

nsubj
xcomp

mark cc
conj

det

dobj

ARG0 ARG1

ARG0
ARG1

ARG0

ARG1

Figure 1: Mismatch between syntactic and se-
mantic dependencies.

He refused to confirm or deny reports
NP (S\NP)/(S\NP) S/S (S\NP)/NP conj (S\NP)/NP NP

ARG0 ARG1

∅

ARG0 ARG1

ARG0

ARG1

Figure 2: Dependencies produced by a CCG
parse. SRL dependencies can be recovered by la-
belling the edges with a semantic role or ∅. Figure
3 shows a CCG derivation for these dependencies.

models. For example, in the Figure 1, the se-
mantic dependency between He and deny spans
three syntactic edges. This fact makes it difficult
to jointly parse syntactic and semantic dependen-
cies with dynamic programs, and means that de-
pendency path features can be sparse. Syntactic
dependencies also often have ambiguous seman-
tic interpretations—for example in He opened the
door and The door opened, the syntactic subject
corresponds to different semantic roles.

We address these challenges with a new joint
model of CCG syntactic parsing and semantic
role labelling. The CCG formalism is particu-
larly well suited; it models both short- and long-
range syntactic dependencies which correspond
directly to the semantic roles we aim to recover.
The joint model simply involves labelling a subset
of these dependencies with the appropriate roles,
as seen in Figure 2. This labelling decision can
be easily integrated into existing parsing algo-
rithms. CCG also helps resolve cases where in-
terpretation depends on the valency of the pred-

1444



he refused to confirm or deny reports

NPhe (Srefuse\NPi)/(Sj\NPi)j Sz/Sz (Sconfirm\NPu)/NPv conj (Sdeny\NPx )/NPy NPreports
{} {refuse ?−→i, refuse ?−→j} {to ?−→z} {confirm ?−→u, confirm ?−→v} {} {deny ?−→x, deny ?−→y} {}

>
((Sp\NPx )/NPy)\((Sp\NPx )/NPy)
{deny ?−→x, deny ?−→y, p ?−→x, p ?−→y}

<
(Sconfirm\NPx )/NPy

{confirm ?−→x, confirm ?−→y, deny ?−→x, deny ?−→y}
>

Sconfirm\NPx
{confirm ?−→x, confirm A1−−→reports, deny ?−→x, deny A1−−→reports}

>
Sconfirm\NPx

{confirm ?−→x, confirm A1−−→reports, deny ?−→x, deny A1−−→reports, to ∅−→confirm}
>

Srefuse\NPx
{refuse ?−→x, refuse A1−−→confirm, confirm ?−→x, confirm A1−−→reports, deny ?−→x, deny A1−−→reports, to ∅−→confirm}

<
Srefuse

{refuse A0−−→he, refuse A1−−→confirm, confirm A0−−→he, confirm A1−−→reports, deny A0−−→he, deny A1−−→reports, to ∅−→confirm}

Figure 3: Jointly building a CCG parse and semantic dependencies representation. Subscripts beneath
categories denote heads, which are unified when spans combine. One dependency is created for each
argument of each lexical category. In our approach, dependency labels are initially underspecified (rep-
resented f ?−→a) until an attachment is determined by the derivation and a label is chosen by the model.

icate, such as ergative verbs, by learning lexical
entries that pair syntactic arguments with seman-
tic roles, such as open : S\NPARG1 and open :
(S\NPARG0 )/NPARG1 . Figure 3 shows a de-
tailed trace of how the example from Figure 2 is
parsed with our model.

We also present a new A∗ algorithm for the joint
model. Because CCG is strongly lexicalized, we
are able to introduce a new type of extended lexi-
cal entries that allows us to factor the model over
words and develop effective new upper bounds on
the Viterbi outside parse score. A∗ parsing algo-
rithms have previously been developed for models
with tree-structured syntactic dependencies (Klein
and Manning, 2003; Auli and Lopez, 2011b), and
models with no bi-lexical dependencies, includ-
ing supertag-factored CCGs (Lewis and Steed-
man, 2014a). We generalize these techniques to
SRL-style graph-structured dependencies.

Experiments demonstrate that our model not
only outperforms pipeline semantic role labelling
models, but improves the quality of the syntactic
parser. PropBank SRL performance is 1.6 points
higher than comparable existing work, and se-
mantic features improve syntactic accuracy by 1.6
points. Our A∗ algorithm is 5 times faster than
CKY parsing, with no loss in accuracy. The com-
bination of CCG-based joint modelling and A∗ de-
coding gives an efficient, accurate, and linguisti-
cally principled parser.1

1The parser is available from: https://github.com/
mikelewis0/EasySRL

2 Background

2.1 CCG Dependencies

CCG parses define an implicit dependency graph,
by associating each argument of each category
with a dependency. In contrast to Stanford de-
pendency trees, CCG dependency graphs can be
non-projective, and words can have multiple par-
ents. For example, in Figure 2, He is an argument
of refuse, confirm and deny.

To create dependencies, categories are marked
with headedness information—which we denote
with subscripts. For example, in the category
(Sdeny\NPx )/NPy , the final head for the sentence
S will be deny, and that two dependencies will be
introduced from deny to unspecified arguments x
and y. During parsing, variables are unified with
heads, creating fully specified dependencies.

Re-using variables allows dependencies to
propagate. For example, the determiner category
NPi/Ni marks that the head of the resulting NP is
equal to the head of its N argument (e.g. the head
of the report is report), as in the following parse:

deny the report

(Sdeny\NPx )/NPy NPz/Nz Nreport
{deny−→x, deny−→y} {the−→z} {}

>
NPreport

{the−→report}
>

Sdeny\NPx
{deny−→x, deny−→report, the−→report}

The same mechanism allows long-range argu-
ments to be propagated. Figure 3 shows several
long-range arguments, such as how co-indexation

1445



propagates the subject of deny to he. For more de-
tails, see Hockenmaier (2003).

2.2 A∗ CCG parsing

A∗ parsing searches for an optimal parse, with-
out building a complete chart (in contrast to CKY
parsing). This is particularly attractive for CCG,
because the formalism’s lexical and derivational
ambiguity causes the parse charts to be very dense,
in practice. Lewis and Steedman (2014a) showed
that A∗ CCG parsing can be highly efficient, but
used a restricted model without bi-lexical features.

In A∗ parsing, entries y are added to the chart
in order of their cost f(y) = g(y) + h(y), where
g(y) is the inside score for the entry, and h(y) is
an upper bound on the Viterbi outside score. Be-
cause partial parses are built in order of increasing
f(y), the first complete parse added to the chart is
guaranteed to be optimal. The key to making A∗

parsing efficient is computing tight upper bounds
on the outside score. In Lewis and Steedman’s
supertag-factored model, the bound can be com-
puted as the sum of the highest-scoring supertags
in the outside parse.

The agenda is initialized with items represent-
ing every category for every word. Then, after
each item is added to the chart, the agenda is up-
dated with all binary and unary rules that can be
applied to the new item. For more details, see
Lewis and Steedman (2014a).

3 Model

Our joint model of CCG and SRL parsing sim-
ply involves labelling CCG syntactic dependen-
cies (which are implicit in CCG parses), with SRL
roles (or null). This formulation allows us to easily
adapt the log-linear CCG parsing model of Clark
and Curran (2007) to the joint setting by working
with extended dependencies that including syntac-
tic and semantic information.

More formally, we can define a notion of con-
sistency to specify the labelling of syntactic de-
pendencies. A set of semantic dependencies π
is consistent with a CCG derivation d if each se-
mantic dependency corresponds to a single syntac-
tic dependency—for example, see the dependen-
cies in Figure 3. The CCG derivation in Figure 3
would also be consistent with the SRL dependency
refuse

ARG2−−−−→he, but not refuse ARG1−−−−→reports
(because the derivation does not produce the re-
quired syntactic dependency).

Now, we can define a log-linear model over
pairs of consistent CCG derivations d and SRL de-
pendencies π:

p(d, π|x) = e
θ·φ(x,d,π)∑

(d′,π′)∈GEN(x) eθ·φ(x,d
′,π′)

whereGEN(x) is the space of consistent pairs for
sentence x.

3.1 Dependencies
Because we enforce consistency, we can work
with joint dependencies that are a combination of
CCG and SRL dependencies. We denote a depen-
dency as a 6-tuple of functor f , lexical category c,
argument number n, preposition p, argument a and
semantic role r: 〈f, c, n, p, a, r〉. The first three of
these (f ,c,n) are lexically specified, but a and r
are lexically underspecified until the attachment is
determined by the derivation, and a label is cho-
sen by the model. For example, in Figure 3, the
lexical entry for deny has two underspecified de-
pendencies: 〈deny, (S\NP)/NP , 1,∅, ?, ?〉 and
〈deny, (S\NP)/NP, 2,∅, ?, ?〉.2 At the end of
the derivation, the dependencies are specified
as: 〈deny, (S\NP)/NP , 1,∅, he,ARG0〉 and
〈deny, (S\NP)/NP, 2,∅, reports, ARG1〉.

The preposition p is lexically underspecified for
PP arguments, but otherwise ∅. Prepositions are
marked lexically on PP/∗ categories, and then
propagated, for example:

fly to Lisbon

(Sfly\NPx )/PPpy PP toz /NPz NPLisbon
{fly ?−→x, fly ?−→y} {to ?−→z} {}

<

PP toLisbon
{to ∅−→Lisbon}

<
Sfly\NPx

{fly ?−→x, fly ARG1−−−−→Lisbon, to ∅−→Lisbon}

In the above example, the dependency from
fly to Lisbon corresponds to the tuple:
〈fly, (S\NP)/PP, 2, to, Lisbon,ARG1〉.
Propagating both the noun and preposition from
prepositional phrases allows the model to use both
for features, and improved results.

3.2 Features
We use the following feature classes, which de-
compose over categories, dependencies, and local
rule instantiations.
2Bold-face is used to highlight the argument of a category
corresponding to a dependency. For example (S\NP)/NP
denotes the first (subject) dependency of a transitive verb.

1446



3.2.1 Supertagging Features
Supertagging features φCAT score categories for
words. A single feature is used, which is the
(unnormalized) score from Lewis and Steedman
(2014b)’s supertagging model. The supertagger
outputs a distribution over categories for each
word independently. The model is trained on su-
pertags extracted from an adaptation of CCGre-
bank (Honnibal et al., 2010). The adaptation
makes minor changes to better match PropBank.

3.2.2 Preposition Features
Preposition features φPP score whether the nth
argument of the word at index f with category c
should take a PP argument headed by preposition
p. We use features xf+p, lf+c and lf+c+n+p,
where xf is the f th word, and lf is its lemma.

3.2.3 Labelling Features
Labelling features φROLE determine whether the
nth argument of a word with lemma l, and
category c should take a role r. We use
n+r, c+n+r, c+n+p+r, l+c+n+p+r, n+r, r,
l+p+r, c+n+r, l+r, h+r, where h is an indica-
tor of whether l is hyphenated, and p is the prepo-
sition of PP arguments.

3.2.4 Dependency Features
Dependency features φDEP score dependencies,
based on the functor index f , argument index a
and role r. We use lf+r+la, lf+r+ca, d+r,
cf+o+r, ca+o+r, where o is an offset from
−3 . . .+3, d is the distance between f and a, ci is
a cluster or POS tag for word i, and li is the lemma
of word i. Clusters were created from pre-trained
word embeddings (Levy and Goldberg (2014)) us-
ing k-means, with k = 20, 50, 200, 1000, 2500.
These features only apply when the role r 6= ∅.
3.2.5 Derivation Features
Derivation features φDERIV score properties of
the syntactic derivation. To simplify computation
of the upper bounds for the A∗ parsing algorithm
given in Section 5, the weights of these features
are constrained to be ≤ 0. For simplicity, we only
use features for unary rules—for example, a fea-
ture records when the unary rule N → NP con-
verts a determiner-less N to a NP .

4 Lexical Factorization

In this section, we show how to factor the model
into extended lexical entries. This factorization al-

lows us to efficiently compute upper bounds on the
scores of partial parses, which is crucial to the A∗

algorithm described in Section 5.
The key observation is that our supertagging,

labelling and dependency features can each be as-
sociated with exactly one word (using the functor
for the dependency features). Therefore, we can
equivalently view the complete parse as a series of
extended lexical entries: y = y0...yN . Extended
lexical entries are composed of a lexical category,
and a role and attachment for each argument of
the category. The set of extended lexical entries
specifies the yield of the parse. For example, the
parse from Figure 2 can be represented as the
following extended lexical entries:

he ` NP
refused ` (S\NPARG0=he)/(S\NP)ARG1=confirm
to ` S/S∅=confirm
confirm ` (S\NPARG0=he)/NPARG1=reports
or ` conj
deny ` (S\NPARG0=he)/NPARG1=reports
reports ` NP

The score for a sentence x and joint SRL-CCG
parse y, θ ·φ(x, y), can be decomposed into scores
of its extended lexical entries yi, plus a score for
the derivation features:

θ · φ(x, y) =
∑
yi∈y

θ · φ(x, yi) + θ · φDERIV (x, y)

The space of possible extended lexical entries
for word xi is defined by GENLEX (xi), which is
expressed with a CFG, as shown in Figure 4a.

The features defined in Section 3.2 decom-
pose over the rules of GENLEX —so it can be
weighted with the globally trained feature weights.

Expressing GENLEX (xi) as a CFG allows us
to efficiently compute upper bounds on the score
of yi when it is only partially specified, using the
Viterbi algorithm—which we will make use of in
Section 5. Making the attachment choice indepen-
dent of the syntactic category greatly improves the
efficiency of these calculations.

5 A∗ Parsing

To efficiently decode our model, we introduce a
new A∗ parsing algorithm. As discussed in Sec-
tion 2.2, the key to efficient A∗ parsing is comput-
ing tight upper bounds on the Viterbi outside score
of partial parses, with the function h.

The intuition behind our algorithm is that we
can use the lexical factorization of Section 4 to
compute upper bounds for words individually,

1447



Lexical Category Choice
xi →NP | S\NP | (S\NP)/PP | . . .
One dependency is created for every argument of the category
S\NP → S\NP
(S\NP)/PP → (S\NP)/PP, (S\NP)/PP
. . .
Preposition choice for PP arguments
(S\NP)/PP → (S\NP)/PPin | (S\NP)/PPfor | . . .
. . .
Semantic role label choice for the argument
S\NP → S\NPARG0 | S\NPARG1 | . . .
(S\NP)/PP → (S\NPARG0)/PP | (S\NPARG1)/PP . . .
(S\NP)/PPX → (S\NP)/PPXARG0 | (S\NP)/PPXARG1 . . .
. . .
Attachment choice
ARG0 → x0 | . . . | xi−1 | xi+1 | . . . | xN
ARG1 → x0 | . . . | xi−1 | xi+1 | . . . | xN
. . .

(a) The grammar GENLEX (xi)

confirm

S\NP (S\NP)/NP NP

S\NP (S\NP)/NP (S\NP)/NP

ARG0 ARG1 ∅

He reports refused

(b) Visualization of a fragment of GENLEX (confirm)

Figure 4: (a) The grammar GENLEX (xi), which defines the space of extended lexical entries,
and (b) a visualization of a fragment of GENLEX (confirm). Extended lexical entries, including
confirm `(S\NPARG0=he)/NPARG1=reports and confirm `NP , are specified by choosing one cate-
gory (top level in both a and b), enumerating all arguments (second level), selecting the preposition for
PP arguments (when present), selecting a semantic role label for each, and finally choosing the argument
head word. The features are local to the grammar rules, enabling efficient dynamic programs for upper
bound computations on partially specified entries, such as confirm `(S\NPr=a)/NPARG1=reports .

then create an upper bound for the parse as a sum
of upper bounds for words. The bound is not exact,
because the grammar may not allow the combina-
tion of the best lexical entry for each word.

Section 5.1 gives a declarative definition of h
for any partial parse, and 5.2 explains how to effi-
ciently compute h during parsing.

5.1 Upper Bounds for Partial Parses

This section defines the upper bound on the Viterbi
outside score h(yi,j) for any partial parse yi,j of
span i . . . j. For example, in the parse in Figure
3, y3,5 is the partial parse of confirm or deny with
category (S\NP)/NP .

As explained in Section 4, a parse can be de-
composed into a series of extended lexical entries.
Similarly, a partial parse can be viewed as a se-
ries of partially specified extended lexical entries
y0 . . . yN . For example, in Figure 3, the partial
parse of the span confirm or deny reports, the ex-
tended lexical entries for the words outside the
span (He, refused and to) are completely unspeci-
fied. The extended lexical entries for words inside
the span have specified categories, but can contain
underspecified dependencies:

confirm ` (S\NPr=a)/NPARG1=reports
or ` conj
deny ` (S\NPr ′=a ′)/NPARG1=reports
reports ` NP

Therefore, we can compute an upper bound for

the outside score of a partial parse as a sum of
the upper bounds of the unspecified components of
each extended lexical entry. Note that because the
derivation features are constrained to be ≤ 0, they
do not affect the calculation of the upper bound.

We can then find an upper bound for completely
unspecified spans using by summing the upper
bounds for the words. We can pre-compute an up-
per bound for the span hi,j for every span i, j as:

hi,j =
j∑
k=i

max
yk∈GENLEX (xk)

θ · φ(x, yk)

The max can be efficiently computed using the
Viterbi algorithm on GENLEX (xk) (as described
in Section 4).

The upper bound on the outside score of a par-
tial parse is then the sum of the upper bounds of
the words outside the parse, and the sum of the
scores of the best possible specifications for each
underspecified dependency:

h(yi,j) =
∑

〈f,c,n,?,?,?〉∈deps(yi,j)
max
a′,p′,r′

θ · φ(〈f, c, n, p′, a′, r′〉)

+ h0,i−1 + hj+1,N

where deps(y) returns the underspecified depen-
dencies from partial parse.

For example, in Figure 3, the upper bound for
the outside score of the partial parse of confirm
or deny reports is the sum of the upper bounds

1448



for the other words independently (h0,3) added to
the score of the best attachments and roles for the
subjects of confirm and deny independently.

5.2 Additive Updates
During parsing, the upper bound h can be ef-
ficiently computed recursively with additive up-
dates. Initially h is the sum of the upper bounds
for each word independently h0,N . Then, when
specifying categories or labelling dependencies,
the score is updated.

• When specifying a category for a word xi, the
Viterbi score of GENLEX (xi) is subtracted
from h, and the sum of Viterbi scores for each
of the category’s (underspecified) dependen-
cies is added to h.

• When specifying a semantic role for a de-
pendency with functor f , causing the depen-
dency to be fully specified, the Viterbi score
for the node representing that dependency in
GENLEX (xf ) is subtracted from h.

• When a binary rule combines two partial
parses yi,j and yj+1,k, the bound on the out-
side score is updated by summing the bounds
of the outside scores of the child parses, and
subtracting the overall upper bound for the
sentence (to avoid double-counting):
h(yi,k) = h(yi,j) + h(yj+1,k)− h0,N
This update can be derived from the defini-
tion of h.

These are the only cases where h is updated.

6 Training

During training, we optimize parameters θ for
the marginal likelihood of the semantic depen-
dencies πi given a sentence xi, treating syntactic
derivations d as a latent variable and using L2
regularization.

L(θ) = log
∏
i

pθ(πi|xi)−
∑
j θ

2
j

2σ2

=
∑
i

log

∑
d∈∆(xi,πi) e

θ·φ(xi,d,πi)∑
(d′,π′)∈GEN(xi) e

θ·φ(xi,d′,π′) −
∑
j θ

2
j

2σ2

where GEN(xi) is the set of consistent CCG and
SRL parses for a sentence xi (see Section 3), and
∆(xi, πi) is the set of CCG derivations that are
maximally consistent with gold SRL parses πi.
More formally, if labelled(y) returns the set of

labelled dependencies from parse y, then:

∆(x, π) = arg max
y∈GEN(x)

|π ∩ labelled(y)|

The arguments of PropBank dependencies can
span multiple words, so CCG dependencies are
marked as equivalent if their argument is any-
where within the PropBank span.

The approach is closely related to the hy-
brid dependency model (Clark and Curran, 2007).
However the CCGbank dependencies used by
Clark and Curran’s model constrain all lexical
and attachment decisions (only allowing ‘spuri-
ous’ derivational ambiguity) whereas our use of
semantic dependencies models most of the syntac-
tic parse as latent.

6.1 Hyperparameters

Calculating the gradient of the loss function re-
quires computing the marginal probability of the
correct parses. Computing marginal probabilities
exactly involves summing over all possible CCG
parses, which is intractable. Instead, following
Clark and Curran, we limit the size of training
charts using a variable-width supertagger beam β,
initially 10−3. If the chart size exceeds 100000
nodes, we double β and re-parse. For computing
∆, we use a more restrictive beam of β = 0.01
to improve the quality of the positive training ex-
amples. We optimize using L-BFGS (Liu and No-
cedal, 1989), with σ2 = 0.06.

6.2 Pruning

To improve efficiency, we compute a number of
thresholds, by aligning gold CCGbank dependen-
cies with PropBank. If an argument of a cate-
gory occurs with a particular semantic role less
than 3 times in the aligned data, it is pruned from
the training and decoding charts. We also fil-
ter infrequent features before training. We count
the number of times each feature occurs in the
aligned data, and filter features occurring less than
3 times. During decoding, we prune lexical cate-
gories whose probability under the supertagging
model is less than a factor of 10−2 of that of
the best category for that word. If the chart size
exceeds 20000 nodes, we back off to a pipeline
model (roughly 3% of sentences). Finally, we
build and use a tag dictionary in the same way as
Lewis and Steedman (2014a).

1449



7 Experiments

7.1 Experimental Setup
We used PropBank Section 00 for development,
Sections 02-21 for training, and Section 23 for
testing. The Pipeline baseline first parses with a
supertag-factored A∗ model, and chooses a seman-
tic role for each CCG dependency with a log-linear
classifier. The classifier uses the role and attach-
ment features used by the parser.

7.2 Semantic Role Labelling
We evaluate our parser as a dependency-based
SRL model on PropBank, comparing with
CoNLL-2008 systems (Surdeanu et al., 2008).

Comparison systems Following Täckström et
al. (2015), we compare with the best ‘single
parser’ SRL models, which use only a single syn-
tactic parse.3

Much recent work has evaluated using gold
predicate identification (Hajič et al., 2009;
FitzGerald et al., 2015). This setting is particu-
larly unrealistic for our joint model, where gold
predicate identification would be a highly useful
feature for the supertagger; we only compare with
models that use automatic predicate identification.
To the best of our knowledge, the best SRL re-
sults with automatic predicate identification were
achieved at CoNLL-2008.

A number of other models have been evaluated
on CoNLL-2008 data. While we cannot com-
pare directly on our metric, the best reported joint
model (Johansson, 2009) scored 1.4 points lower
than the Che et al. (2008) system we compare to
on the CoNLL metric on PropBank. Other joint
models, such as those of Titov et al. (2009) and
Lluís et al. (2013), achieve similar performance.

Evaluation Metric Comparing fairly with ex-
isting work is complicated by the mismatch be-
tween heads found by our model and those used in
other evaluations. Headedness decisions are often
arbitrary—for example, whether a prepositional
phrase is headed by the noun or preposition—and
different choices were made in the design of CCG-
bank and the CoNLL-2008 headedness rules.

To solve this problem, we introduce a new
within-constituent metric, which awards depen-
3The best models use reranking with powerful global features
(Toutanova et al., 2008; Johansson and Nugues, 2008) or en-
semble methods (Surdeanu et al., 2007; Punyakanok et al.,
2008). These techniques have the potential to improve any
SRL system, including ours, at some expense in speed.

PropBank Brown
Model P R F1 P R F1
Vickrey 87.3 77.3 82.0 74.0 64.5 68.9
Che 85.3 78.6 81.8 71.1 65.7 68.0
Zhao 82.4 79.8 81.1 66.6 64.9 65.7
Riedel 83.6 74.7 78.9 69.3 62.7 65.8
Pipeline 79.2 73.9 76.4 69.3 64.0 66.1
Joint 84.8 82.2 83.5 71.2 69.2 70.2

Table 1: Comparison with the best single-parser
SRL models on PropBank from CoNLL-2008.
The comparison models are Vickrey and Koller
(2008), Che et al. (2008), Zhao and Kit (2008) and
Riedel and Meza-Ruiz (2008).

dencies as correct if they attach anywhere within
the original PropBank-annotated argument spans.
For example, if the PropBank annotates that the
ARG0 of owned is by Google, a dependency to
either by or Google is judged correct. We com-
pute new scores for the CoNLL-2008 submissions
on our metric, filtering reference and continuation
arguments (which are artifacts of the CoNLL con-
version of PropBank, but not required by our met-
ric), and nominal predicates based on POS tag.
The ranking of the top 5 CoNLL-2008 open-track
models is identical under our metric and the orig-
inal one (up to statistical significance), suggesting
that our metric is equally discriminative. However,
perhaps interestingly, the ranking of Vickrey and
Koller (2008) does improve—likely due to the use
of a syntactic formalism with different headedness
rules. For simplicity, we do not include predicate
senses in the evaluation.

Results are given in Table 1 and show that our
joint model greatly outperforms the pipeline ver-
sion, demonstrating the value of joint reasoning.
It also scores 1.5 points higher than the best com-
parable models in-domain, and 1.3 points higher
out-of-domain. To the best of our knowledge, this
is the first joint syntax/SRL model to outperform
strong pipeline models.

7.3 Efficiency Experiments

We explore whether our A∗ decoding is more ef-
ficient than alternatives, including both algorith-
mic and feature computation. While the A∗ search
builds very small charts, the features must be pre-
computed for the heuristic. In CKY parsing, fea-
tures can be computed lazily.

1450



Model Sentences
per second

PropBank
F1

Pipeline A∗ 38.3 76.4
Joint CKY 6.0 83.5
Joint AST 20.3 83.0
Joint A∗ 31.3 83.5

Table 2: Parser speed on PropBank Section 23

We compare with a CKY parsing over the same
space. If no parse is found, or the chart size ex-
ceeds 400000 nodes, we back off to the pipeline
(tuned so that the backoff is used roughly as often
as for the A∗ parser). We also compare with adap-
tive supertagging (AST, Clark and Curran (2007)),
which is the same except for first attempting to
parse with a restrictive supertagger beam β = 0.1.

Table 2 shows the results. The A∗ pipeline is
fast, but inaccurate. CKY is 5 times slower than
A∗ in the same space, whereas adaptive supertag-
ging trades accuracy for speed. The best previ-
ously reported speed improvement using A∗ pars-
ing is a factor of 1.2 times faster (Auli and Lopez,
2011b). Our new A∗ algorithm dominates existing
alternatives in both speed and accuracy.

7.4 Syntactic Parsing

We also evaluate our model for CCG parsing ac-
curacy, using CCGrebank (Honnibal et al., 2010),
and comparing with a C&C parser model adapted
to this dataset. Results are shown in Table 3. Of
course, given our latent syntax, and the fact we
have no model of CCGbank dependencies, we do
not expect state-of-the-art accuracy. However, the
1.6 point improvement over the pipeline shows
that SRL dependencies are useful for disambiguat-
ing syntactic attachment decisions. Many errors
were caused by disagreements between CCGbank
and PropBank—PropBank is likely to be more ac-
curate as it was hand-annotated, rather than auto-
matically converted from the Penn Treebank. In
effect, our latent model of syntax is successfully
learning a grammar that better produces the cor-
rect semantics.

Table 4 shows the syntactic dependencies which
are improved most by modelling semantics. Un-
surprisingly, verb arguments and adjuncts rela-
tions show large improvements. However, we also
see more accurate attachment of relative clauses.
While we do not model relative clauses explicitly,
correctly attaching them is essential for propagat-

Model P R F1
C&C 81.8 81.2 81.5
Pipeline 76.6 77.7 77.2
Joint 78.3 79.4 78.8

Table 3: Labelled F1 for CCGbank dependencies
on CCGrebank Section 23

Dependency Type ∆ F1
((Sdcl\NP)/PP)/NP +12.6
((Sdcl\NP)/PP)/NP +11.0
((Sb\NP)/PP)/NP Verb +10.7
(Sdcl\NP)/PP arguments +8.1
(Sng\NP)/NP +7.6
(Spt\NP)/NP +6.8
((S\NP)\(S\NP))/NP Verb +16.9
((S\NP)\(S\NP))/Sdcl adjuncts +11.9
(N \N)/(Sdcl\NP) Relative +12.5
(NP\NP)/(Sdcl\NP) clauses +8.5

Table 4: CCG syntactic dependencies with the
largest change in F1 between the Pipeline and
Joint models (of those occurring at least 100 times
in the development set).

ing certain verb arguments (e.g. the subject of
broke in the glass on the table that broke).

7.5 Error Analysis

Table 5 gives an analysis of recall errors from the
first 100 sentences of PropBank Section 00. One
third of errors were syntactic attachment errors.
A further 14% were triggered by cases where the
parser found the correct attachment, but gave an
adjunct a core argument CCG category (or vice
versa). Many of these decisions are not obvious—
for example in rose sharply, PropBank considers
sharply to be an argument and not an adverb. 21%
of errors were caused by choosing the wrong SRL

Error Percentage
Attachment error 33%
Correct attachment, wrong label 21%
Correct attachment, unlabelled 20%
Argument/adjunct distinction 14%
Problematic constructions 9%
Dubious annotation 4%

Table 5: Error analysis of recall errors from 100
development set sentences.

1451



label. Another 20% were caused by predicates as-
signing arguments the null semantic role ∅. The
major cause of these errors is predicates that act
syntactically like adjectives (e.g. publishing in
Dutch publishing group) where syntactic cues are
weak. Finally, 9% involved long-range arguments
that our current grammar is unable to project. One
common case is constructions like X has a plan
to buy Y, where the grammar does not propagate
the subject of buy to X. Further improvements to
CCGbank may help to resolve these cases.

8 Related Work

Joint syntactic and SRL models There have
been many proposals for jointly parsing syntac-
tic and semantic dependencies. Lluís et al. (2013)
introduce a joint arc-factored model for parsing
syntactic and semantic dependencies, using dual-
decomposition to maximize agreement between
the models. SRL performance is slightly worse
than a pipeline version. Naradowsky et al. (2012)
introduce a SRL model with latent syntax repre-
sentations, by modelling a latent dependency tree
during training, which is marginalized out at test
time. However, performance at English SRL is
roughly 7 points beneath state of the art. Other
notable models include those of Johansson (2009)
and Titov et al. (2009).

CCG parsing Our log-linear model is closely
related to that of Clark and Curran (2007), but
we model SRL dependencies instead of CCG de-
pendencies. The best CCG parsing results were
achieved by Auli and Lopez (2011a), who, like us,
score CCG parses based jointly on supertagging
and dependency model scores. Decoding their
model requires dual-decomposition, to maximize
agreement between the separate models. We avoid
the need for this technique by using a unigram su-
pertagging model, rather than a sequence model.

CCG semantics Work on semantic parsing has
mapped sentences onto semantic representations
with latent CCGs (Zettlemoyer and Collins, 2009;
Kwiatkowski et al., 2010; Kwiatkowski et al.,
2013) for restricted domains. Recent work has
scaled these techniques to wide-coverage datasets
(Artzi et al., 2015). Krishnamurthy and Mitchell
(2014) also explore joint CCG syntactic and se-
mantic parsing. They use a smaller semantic lex-
icon, containing 130 predicates, rather than the
3257 PropBank verbs. In contrast to our re-

sults, jointly modelling the semantics lowers their
model’s syntactic accuracy.

Other CCG-based SRL models haved used
CCG dependencies as features for predicting se-
mantic roles (Gildea and Hockenmaier, 2003;
Boxwell et al., 2009), but performance is limited
by relying on 1-best parses—a problem we re-
solved with a joint model.

A∗ parsing A∗ parsing has previously been ex-
plored for less general models than ours. Klein
and Manning (2003) and Auli and Lopez (2011b)
use A∗ parsing for models with tree-structured de-
pendencies. The best reported speed improvement
is parsing 1.2 times faster, whereas we improve
by a factor of 5. Our model also allows the more
complex graph-structured dependencies required
for semantic role labelling. Lewis and Steedman
(2014a) demonstrate an efficient A∗ algorithm for
CCG, but cannot model dependencies.

9 Conclusions and Future Work

We have shown that using CCG can allow joint
models of syntax and semantics to outperform
pipelines, and achieve state-of-the-art results on
PropBank SRL. Our new A∗ parsing algorithm
is 5 times faster than CKY parsing, without
loss of accuracy. Using latent syntax allows us
to train the model purely from semantic depen-
dencies, enabling future work to train against
other annotations such as FrameNet (Baker et al.,
1998), Ontonotes (Hovy et al., 2006) or QA-
SRL (He et al., 2015). The semantic labels pro-
vided by PropBank can also be integrated into
wide-coverage CCG semantic parsers (Bos, 2008;
Lewis and Steedman, 2013) to improve perfor-
mance on downstream applications.

Acknowledgements

This research was supported in part by the NSF
(IIS-1252835), DARPA under the DEFT program
through the AFRL (FA8750-13-2-0019), an Allen
Distinguished Investigator Award, and a gift from
Google. We thank Nicholas Fitzgerald, Dan Gar-
rette, Kenton Lee, Swabha Swayamdipta, Mark
Yatskar and the anonymous reviewers for their
helpful comments on earlier versions of this paper,
and Matthew Honnibal for useful discussions.

1452



References
Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.

Broad-coverage CCG Semantic Parsing with AMR.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing.

Michael Auli and Adam Lopez. 2011a. A Comparison
of Loopy Belief Propagation and Dual Decomposi-
tion for Integrated CCG Supertagging and Parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1.

Michael Auli and Adam Lopez. 2011b. Efficient CCG
parsing: A* versus Adaptive Supertagging. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1.

Johan Bos. 2008. Wide-coverage Semantic Analy-
sis with Boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics.

Stephen A. Boxwell, Dennis Mehay, and Chris Brew.
2009. Brutus: A Semantic Role Labeling System In-
corporating CCG, CFG, and Dependency Features.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1.

Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang
Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A cas-
caded Syntactic and Semantic Dependency Parsing
System. In Proceedings of the Twelfth Conference
on Computational Natural Language Learning.

Stephen Clark and James R Curran. 2007. Wide-
coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4).

Nicholas FitzGerald, Oscar Täckström, Kuzman
Ganchev, and Dipanjan Das. 2015. Semantic Role
Labeling with Neural Network Factors. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing.

Daniel Gildea and Julia Hockenmaier. 2003. Identify-
ing Semantic Roles Using Combinatory Categorial
Grammar. In Proceedings of the 2003 Conference
on Empirical Methods in Natural Language Pro-
cessing.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martí, Lluís

Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of
the Thirteenth Conference on Computational Natu-
ral Language Learning: Shared Task.

Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing.

James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multi-lingual Joint Pars-
ing of Syntactic and Semantic Dependencies with a
Latent Variable Model. Computational Linguistics.

Julia Hockenmaier. 2003. Data and models for sta-
tistical parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh. College
of Science and Engineering. School of Informatics.

M. Honnibal, J.R. Curran, and J. Bos. 2010. Rebank-
ing CCGbank for Improved NP Interpretation. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.

Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% Solution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers.

Richard Johansson and Pierre Nugues. 2008.
Dependency-based Syntactic-Semantic Analysis
with Propbank and Nombank. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning.

Richard Johansson. 2009. Statistical Bistratal Depen-
dency Parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2.

Dan Klein and Christopher D Manning. 2003. A*
Parsing: Fast Exact Viterbi Parse Selection. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1.

Jayant Krishnamurthy and Tom M. Mitchell. 2014.
Joint Syntactic and Semantic Parsing with Combi-
natory Categorial Grammar. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing Probabilis-
tic CCG Grammars from Logical Form with Higher-
order Unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223–1233.

1453



Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling Semantic Parsers with
On-the-Fly Ontology Matching. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing.

Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers).

Mike Lewis and Mark Steedman. 2013. Combined
Distributional and Logical Semantics. Transactions
of the Association for Computational Linguistics, 1.

Mike Lewis and Mark Steedman. 2014a. A* CCG
Parsing with a Supertag-factored Model. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing.

Mike Lewis and Mark Steedman. 2014b. Improved
CCG parsing with Semi-supervised Supertagging.
Transactions of the Association for Computational
Linguistics.

D. C. Liu and J. Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Math. Program., 45(3).

Xavier Lluís, Stefan Bott, and Lluís Màrquez. 2009. A
Second-order Joint Eisner Model for Syntactic and
Semantic Dependency Parsing. In Proceedings of
the Thirteenth Conference on Computational Natu-
ral Language Learning: Shared Task.

Xavier Lluís, Xavier Carreras, and Lluís Màrquez.
2013. Joint Arc-factored Parsing of Syntactic and
Semantic Dependencies. Transactions of the As-
sociation of Computational Linguistics – Volume 1,
pages 219–230.

Jason Naradowsky, Sebastian Riedel, and David A.
Smith. 2012. Improving NLP Through Marginal-
ization of Hidden Syntactic Structure. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2).

Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collec-
tive Semantic Role Labelling with Markov Logic. In
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning.

Mihai Surdeanu, Lluís Màrquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination Strategies for
Semantic Role Labeling. Journal of Artificial Intel-
ligence Research, 29(1).

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluís Màrquez, and Joakim Nivre. 2008. The

CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings
of the Twelfth Conference on Computational Natural
Language Learning.

Charles Sutton and Andrew McCallum. 2005. Joint
Parsing and Semantic Role Labeling. In Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning.

Oscar Täckström, Kuzman Ganchev, and Dipanjan
Das. 2015. Efficient Inference and Structured
Learning for Semantic Role Labeling. Transactions
of the Association for Computational Linguistics,
3:29–41.

Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online Projectivisation for
Synchronous Parsing of Semantic and Syntactic De-
pendencies. In In Proceedings of the International
Joint Conference on Artificial Intelligence.

Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Semantic
Role Labeling. Computational Linguistics, 34(2).

David Vickrey and Daphne Koller. 2008. Applying
Sentence Simplification to the CoNLL-2008 Shared
Task. In Proceedings of the Twelfth Conference on
Computational Natural Language Learning.

Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing Context-dependent Mappings from Sentences to
Logical Form. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2.

Hai Zhao and Chunyu Kit. 2008. Parsing Syntactic
and Semantic Dependencies with Two Single-stage
Maximum Entropy Models. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning.

1454


