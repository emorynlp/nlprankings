










































HLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 171–177, Atlanta, Georgia, June 14-15, 2013. c©2013 Association for Computational Linguistics

HLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10

Alex Rudnick, Can Liu and Michael Gasser
Indiana University, School of Informatics and Computing
{alexr,liucan,gasser}@indiana.edu

Abstract

We present our entries for the SemEval-
2013 cross-language word-sense disambigua-
tion task (Lefever and Hoste, 2013). We
submitted three systems based on classifiers
trained on local context features, with some
elaborations. Our three systems, in increasing
order of complexity, were: maximum entropy
classifiers trained to predict the desired target-
language phrase using only monolingual fea-
tures (we called this system L1); similar clas-
sifiers, but with the desired target-language
phrase for the other four languages as features
(L2); and lastly, networks of five classifiers,
over which we do loopy belief propagation to
solve the classification tasks jointly (MRF).

1 Introduction

In the cross-language word-sense disambiguation
(CL-WSD) task, given an instance of an ambigu-
ous word used in a context, we want to predict the
appropriate translation into some target language.
This setting for WSD has an immediate application
in machine translation, since many words have mul-
tiple possible translations. Framing the resolution of
lexical ambiguities as an explicit classification task
has a long history, and was considered in early SMT
work at IBM (Brown et al., 1991). More recently,
Carpuat and Wu have shown how to use CL-WSD
techniques to improve modern phrase-based SMT
systems (Carpuat and Wu, 2007), even though the
language model and phrase-tables of these systems
mitigate the problem of lexical ambiguities some-
what.

In the SemEval-2013 CL-WSD shared task
(Lefever and Hoste, 2013), entrants are asked to

build a system that can provide translations for
twenty ambiguous English nouns, given appropri-
ate contexts – here the particular usage of the am-
biguous noun is called the target word. The five tar-
get languages of the shared task are Spanish, Dutch,
German, Italian and French. In the evaluation, for
each of the twenty ambiguous nouns, systems are to
provide translations for the target word in each of
fifty sentences or short passages. The translations
of each English word may be single words or short
phrases in the target language, but in either case,
they should be lemmatized.

Following the work of Lefever and Hoste (2011),
we wanted to make use of multiple bitext corpora
for the CL-WSD task. ParaSense, the system of
Lefever and Hoste, takes into account evidence from
all of the available parallel corpora. Let S be the set
of five target languages and t be the particular target
language of interest at the moment; ParaSense cre-
ates bag-of-words features from the translations of
the target sentence into the languages S−{t}. Given
corpora that are parallel over many languages, this
is straightforward at training time. However, at test-
ing time it requires a complete MT system for each
of the four other languages, which is computation-
ally prohibitive. Thus in our work, we learn from
several parallel corpora but require neither a locally
running MT system nor access to an online transla-
tion API.

We presented three systems in this shared task,
all of which were variations on the theme of a max-
imum entropy classifier for each ambiguous noun,
trained on local context features similar to those
used in previous work and familiar from the WSD
literature. The first system, L1 (“layer one”), uses
maximum entropy classifiers trained on local con-

171



text features. The second system, L2 (“layer two”),
is the same as the L1 system, with the addition
of the correct translations into the other target lan-
guages as features, which at testing time are pre-
dicted with L1 classifiers. The third system, MRF
(“Markov random field”) uses a network of inter-
acting classifiers to solve the classification problem
for all five target languages jointly. Our three sys-
tems are all trained from the same data, which we
extracted from the Europarl Intersection corpus pro-
vided by the shared task organizers.

At the time of the evaluation, our simplest sys-
tem had the top results in the shared task for the
out-of-five evaluation for three languages (Spanish,
German, and Italian). However, after the evaluation
deadline, we fixed a simple bug in our MRF code,
and the MRF system then achieved even better re-
sults for the oof evaluation. For the best evaluation,
our two more sophisticated systems posted better re-
sults than the L1 version. All of our systems beat the
“most-frequent sense” baseline in every case.

In the following sections, we will describe our
three systems1, our training data extraction process,
the results on the shared task, and conclusions and
future work.

2 L1

The “layer one” classifier, L1, is a maximum en-
tropy classifier that uses only monolingual features
from English. Although this shared task is described
as unsupervised, the L1 classifiers are trained with
supervised learning on instances that we extract pro-
grammatically from the Europarl Intersection cor-
pus; we describe the preprocessing and training data
extraction in Section 5.

Having extracted the relevant training sentences
from the aligned bitext for each of the five lan-
guage pairs, we created training instances with local
context features commonly used in WSD systems.
These are described in Figure 1. Each instance is
assigned the lemma of the translation that was ex-
tracted from the training sentence as its label.

We trained one L1 classifier for each target lan-
guage and each word of interest, resulting in 20∗5 =

1Source is available at
http://github.iu.edu/alexr/semeval2013

• target word features
– literal word form
– POS tag
– lemma

• window unigram features (within 3 words)
– word form
– POS tag
– word with POS tag
– word lemma

• window bigram features (within 5 words)
– bigrams
– bigrams with POS tags

Figure 1: Features used in our classifiers

100 classifiers. Classifiers were trained with the
MEGA Model optimization package 2 and its corre-
sponding NLTK interface (Bird et al., 2009). Upon
training, we cache these classifiers with Python
pickles, both to speed up L1 experiments and also
because they are used as components of the other
models.

We combined the word tokens with their tags
in some features so that the classifier would not
treat them independently, since maximum entropy
classifiers learn a single weight for each feature.
Particularly, the “POS tag” feature is distinct from
the “word with tag” feature; for the tagged word
“house/NN”, the “POS tag” feature would be NN ,
and the “word with tag” feature is house NN .

3 L2

The “layer two” classifier, L2, is an extension to
the L1 approach, with the addition of multilingual
features. Particularly, L2 makes use of the trans-
lations of the target word into the four target lan-
guages other than the one we are currently trying to
predict. At training time, since we have the transla-
tions of each of the English sentences into the other
target languages, the appropriate features are ex-
tracted from the corresponding sentences in those
languages. This is the same as the process by which
labels are given to training instances, described in
Section 5. At testing time, since translations of the

2http://www.umiacs.umd.edu/˜hal/megam/

172



es
de

nl

fr it

Figure 2: The network structure used in the MRF
system: a complete graph with five nodes where
each node represents a variable for the translation
into a target language

test sentences are not given, we estimate the transla-
tions for the target word in the four other languages
using the cached L1 classifiers.

Lefever and Hoste (2011) used the Google Trans-
late API to translate the source English sentences
into the four other languages, and extracted bag-of-
words features from these complete sentences. The
L2 classifiers make use of a similar intuition, but
they do not rely on a complete MT system or an
available online MT API; we only include the trans-
lations of the specific target word as features.

4 MRF

Our MRF model builds a Markov network (often
called a “Markov random field”) of L1 classifiers
in an effort to find the best translation into all five
target languages jointly. This network has nodes
that correspond to the distributions produced by the
L1 classifiers, given an input sentence, and edges
with pairwise potentials that are derived from the
joint probabilities of target-language labels occur-
ring together in the training data. Thus the task of
finding the optimal translations into five languages
jointly is framed as a MAP (Maximum A Posteriori)
inference problem, where we try to maximize the
joint probability P (wfr, wes, wit, wde, wnl), given
the evidence of the features extracted from the
source-language sentence. The inference process is
performed using loopy belief propagation (Murphy
et al., 1999), which is an approximate but tractable

inference algorithm that, while it gives no guaran-
tees, often produces good solutions in practice.

The intuition behind using a Markov network for
this task is that, since we must make five decisions
for each source-language sentence, we should make
use of the correlations between the target-language
words. Correlations might occur in practice due to
cognates – the languages in the shared task are fairly
closely related – or they may simply reflect ambigu-
ities in the source language that are resolved in two
target languages.

So by building a Markov network in which all of
the classifiers can communicate (see Figure 2), we
allow nodes to influence the translation decisions of
their neighbors, but only proportionally to the cor-
relation between the translations that we observe in
the two languages.

We frame the MAP inference task as a minimiza-
tion problem; we want to find an assignment that
minimizes the sum of all of our penalty functions,
which we will describe next. First, we have a unary
function from each of the five L1 classifiers, which
correspond to nodes in the network. These func-
tions each assign a penalty to each possible label for
the target word in the corresponding language; that
penalty is simply the negative log of the probability
of the label, as estimated by the classifier.

Formally, a unary potential φi, for some fixed set
of features f and a particular language i, is a func-
tion from a label l to some positive penalty value.

φi(l) = −logP (Li = l|F = f)

Secondly, for each unordered pair of classifiers
(i, j) (i.e., each edge in the graph) there is a pairwise
potential function φ(i,j) that assigns a penalty to any
assignment of that pair of variables.

φ(i,j)(li, lj) = −logP (Li = li, Lj = lj)

Here by P (Li = li, Lj = lj), we mean the prob-
ability that, for a fixed ambiguous input word, lan-
guage i takes the label li and language j takes the
label lj . These joint probabilities are estimated from
the training data; we count the number of times
each pair of labels li and lj co-occurs in the train-

173



ing sentences and divide, with smoothing to avoid
zero probabilities and thus infinite penalties.

When it comes time to choose translations, we
want to find a complete assignment to the five vari-
ables that minimizes the sum of all of the penal-
ties assigned by the φ functions. As mentioned ear-
lier, we do this via loopy belief propagation, using
the formulation for pairwise Markov networks that
passes messages directly between the nodes rather
than first constructing a cluster graph (Koller and
Friedman, 2009, §11.3.5.1).

As we are trying to compute the minimum-
penalty assignment to the five variables, we use the
min-sum version of loopy belief propagation. The
messages are mappings from the possible values
that the recipient node could take to penalty values.

At each time step, every node passes to each of
its neighbors a message of the following form:

δti→j(Lj) = min
li∈Li

[
φi(li) + φ(i,j)(li, lj)

+
∑

k∈S−{i,j}

δt−1k→i(li)
]

By this expression, we mean that the message
from node i to node j at time t is a function from
possible labels for node j to scalar penalty values.
Each penalty value is determined by minimizing
over the possible labels for node i, such that we find
the label li that minimizes sum of the unary cost for
that label, the binary cost for li and lj taken jointly,
and all of the penalties in the messages that node i
received at the previous time step, except for the one
from node j.

Intuitively, these messages inform a given neigh-
bor about the estimate, from the perspective of the
sending node and what it has heard from its other
neighbors, of the minimum penalty that would be
incurred if the recipient node were to take a given
label. As a concrete example, when the nl node
sends a message to the fr node at time step 10, this
message is a table mapping from all possible French
translations of the current target word to their as-
sociated penalty values. The message depends on
three things: the function φnl (itself dependent on
the probability distribution output by the L1 classi-
fier), the binary potential function φ(nl,fr), and the

messages from es, it and de from time step 9. Note
that the binary potential functions are symmetric be-
cause they are derived from joint probabilities.

Loopy belief propagation is an approximate infer-
ence algorithm, and it is neither guaranteed to find
a globally optimal solution, nor even to converge
at all, but it does often find good solutions in prac-
tice. We run it for twenty iterations, which empir-
ically works well. After the message-passing iter-
ations, each node chooses the value that minimizes
the sum of the penalties from messages and from its
own unary potential function. To avoid accumulat-
ing very large penalties, we normalize the outgoing
messages at each time step and give a larger weight
to the unary potential functions. These normaliza-
tion and weighting parameters were set by hand, but
seem to work well in practice.

5 Training Data Extraction

For simplicity and comparability with previous
work, we worked with the Europarl Intersection
corpus provided by the task organizers. Europarl
(Koehn, 2005) is a parallel corpus of proceedings of
the European Parliament, currently available in 21
European languages, although not every sentence is
translated into every language. The Europarl Inter-
section is the intersection of the sentences from Eu-
roparl that are available in English and all five of the
target languages for the task.

In order to produce the training data for the classi-
fiers, we first tokenized the text for all six languages
with the default NLTK tokenizer and tagged the En-
glish text with the Stanford Tagger (Toutanova et
al., 2003). We aligned the untagged English with
each of the target languages using the Berkeley
Aligner (DeNero and Klein, 2007) to get one-to-
many alignments from English to target-language
words, since the target-language labels may be
multi-word phrases. We used nearly the default set-
tings for Berkeley Aligner, except that we ran 20
iterations each of IBM Model 1 and HMM align-
ment.

We used TreeTagger (Schmid, 1995) to lemma-
tize the text. At first this caused some confusion in
our pipeline, as TreeTagger by default re-tokenizes
input text and tries to recognize multi-word expres-

174



sions. Both of these, while sensible behaviors, were
unexpected, and resulted in a surprising number of
tokens in the TreeTagger output. Once we turned off
these behaviors, TreeTagger provided useful lem-
mas for all of the languages.

Given the tokenized and aligned sentences, with
their part-of-speech tags and lemmas, we used
a number of heuristics to extract the appropriate
target-language labels for each English-language in-
put sentence. For each target word, we extracted a
sense inventory Vi from the gold standard answers
from the 2010 iteration of this task (Lefever and
Hoste, 2009). Then, for each English sentence that
contains one of the target words used as a noun,
we examine the alignments to determine whether
that word is aligned with a sense present in Vi , or
whether the words aligned to that noun are a sub-
sequence of such a sense. The same check is per-
formed both on the lemmatized and unlemmatized
versions of the target-language sentence. If we do
find a match, then that sense from the gold stan-
dard Vi is taken to be the label for this sentence.
While a gold standard sense inventory will clearly
not be present for general translation systems, there
will be some vocabulary of possible translations for
each word, taken from a bilingual dictionary or the
phrase table in a phrase-based SMT system.

If a label from Vi is not found with the align-
ments, but some other word or phrase is aligned
with the ambiguous noun, then we trust the output
of the aligner, and the lemmatized version of this
target-language phrase is assigned as the label for
this sentence. In this case we used some heuristic
functions to remove stray punctuation and attached
articles (such as d’ from French or nell’ from Ital-
ian) that were often left appended to the tokens by
the default NLTK English tokenizer.

We dropped all of the training instances with
labels that only occurred once, considering them
likely alignment errors or other noise.

6 Results

There were two settings for the evaluation, best and
oof. In either case, systems may present multiple
possible answers for a given translation, although
in the best setting, the first answer is given more

weight in the evaluation, and the scoring encour-
ages only returning the top answer. In the oof set-
ting, systems are asked to return the top-five most
likely translations. In both settings, the answers are
compared against translations provided by several
human annotators for each test sentence, who pro-
vided a number of possible target-language transla-
tions in lemmatized form, and more points are given
for matching the more popular translations given by
the annotators. In the “mode” variant of scoring,
only the one most common answer for a given test
sentence is considered valid. For a complete ex-
planation of the evaluation and its scoring, please
see the shared task description (Lefever and Hoste,
2013).

The scores for our systems3 are reported in Figure
3. In all of the settings, our systems posted some of
the top results among entrants in the shared task,
achieving the best scores for some evaluations and
some languages. For every setting and language,
our systems beat the most-frequent sense baseline,
and our best results usually came from either the L2
or MRF system, which suggests that there is some
benefit in using multilingual information from the
parallel corpora, even without translating the whole
source sentence.

For the best evaluation, considering only the
mode gold-standard answers, our L2 system
achieved the highest scores in the competition for
Spanish and German. For the oof evaluation, our
MRF system – with its post-competition bug fix –
posted the best results for Spanish, German and Ital-
ian in both complete and mode variants. Also, cu-
riously, our L1 system posted the best results in the
competition for Dutch in the oof variant.

For the best evaluation, our results were lower
than those posted by ParaSense, and in the stan-
dard best setting, they were also lower than those
from the c1lN system (van Gompel and van den
Bosch, 2013) and adapt1 (Carpuat, 2013). This,
combined with the relatively small difference be-
tween our simplest system and the more sophisti-
cated ones, suggests that there are many improve-
ments that could be made to our system; perhaps

3The oof scores for the MRF system reflect a small bug fix
after the competition.

175



system es nl de it fr
MFS 23.23 20.66 17.43 20.21 25.74
best 32.16 23.61 20.82 25.66 30.11
PS 31.72 25.29 24.54 28.15 31.21
L1 29.01 21.53 19.5 24.52 27.01
L2 28.49 22.36 19.92 23.94 28.23

MRF 29.36 21.61 19.76 24.62 27.46

(a) best evaluation results: precision

system es nl de it fr
MFS 53.07 43.59 38.86 42.63 51.36
best 62.21 47.83 44.02 53.98 59.80
L1 61.69 46.55 43.66 53.57 57.76
L2 59.51 46.36 42.32 53.05 58.20

MRF 62.21 46.63 44.02 53.98 57.83

(b) oof evaluation results: precision

system es nl de it fr
MFS 27.48 24.15 15.30 19.88 20.19
best 37.11 27.96 24.74 31.61 26.62
PS 40.26 30.29 25.48 30.11 26.33
L1 36.32 25.39 24.16 26.52 21.24
L2 37.11 25.34 24.74 26.65 21.07

MRF 36.57 25.72 24.01 26.26 21.24

(c) best evaluation results: mode precision

system es nl de it fr
MFS 57.35 41.97 44.35 41.69 47.42
best 65.10 47.34 53.75 57.50 57.57
L1 64.65 47.34 53.50 56.61 51.96
L2 62.52 44.06 49.03 54.06 53.57

MRF 65.10 47.29 53.75 57.50 52.14

(d) oof evaluation results: mode precision

Figure 3: Task results for our systems. Scores in bold are the best result for that language and evaluation
out of our systems, and those in bold italics are the best posted in the competition. For comparison, we
also give scores for the most-frequent-sense baseline (“MFS”), ParaSense (“PS”), the system developed by
Lefever and Hoste, and the best posted score for competing systems this year (“best”).

we could integrate ideas from the other entries in
the shared task this year.

7 Conclusions and future work

Our systems had a strong showing in the compe-
tition, always beating the MFS baseline, achiev-
ing the top score for three of the five languages in
the oof evaluation, and for two languages in the
best evaluation when considering the mode gold-
standard answers. The systems that took into ac-
count evidence from multiple sources had better
performance than the one using monolingual fea-
tures: our top result in every language came from
either the L2 or the MRF classifier for both eval-
uations. This suggests that it is possible to make
use of the evidence in several parallel corpora in a
CL-WSD task without translating every word in a
source sentence into many target languages.

We expect that the L2 classifier could be im-
proved by adding features derived from more classi-
fiers and making use of information from many dis-
parate sources. We would like to try adding classi-

fiers trained on the other Europarl languages, as well
as completely different corpora. The L2 classifier
approach only requires that the first-layer classifiers
make some prediction based on text in the source
language. They need not be trained from the same
source text, depend on the same features, or even
output words as labels. In future work we will ex-
plore all of these variations. One could, for exam-
ple, train a monolingual WSD system on a sense-
tagged corpus and use this as an additional informa-
tion source for an L2 classifier.

There remain a number of avenues that we would
like to explore for the MRF system; thus far, we
have used the joint probability of two labels to set
the binary potentials. We would like to investigate
other functions, especially ones that do not incur
large penalties for rare labels, as the joint probabil-
ity of two labels that often co-occur but are both rare
will be low. Also, in the current system, the relative
weights of the binary potentials and the unary po-
tentials were set by hand, with a very small amount
of empirical tuning. We could, in the future, tune the

176



weights with a more principled optimization strat-
egy, using a development set.

As with the L2 classifiers, it would be helpful in
the future for the MRF system to not require many
mutually parallel corpora for training – however, the
current approach for estimating the edge potentials
requires the use of bitext for each edge in the net-
work. Perhaps these correlations could be estimated
in a semi-supervised way, with high-confidence au-
tomatic labels being used to estimate the joint dis-
tribution over target-language phrases. We would
also like to investigate approaches to jointly disam-
biguate many words in the same sentence, since lex-
ical ambiguity is not just a problem for a few nouns.

Aside from improvements to the design of our
CL-WSD system itself, we want to use it in a practi-
cal system for translating into under-resourced lan-
guages. We are now working on integrating this
project with our rule-based MT system, L3 (Gasser,
2012). We had experimented with a similar, though
less sophisticated, CL-WSD system for Quechua
(Rudnick, 2011), but in the future, L3 with the inte-
grated CL-WSD system should be capable of trans-
lating Spanish to Guarani, either as a standalone
system, or as part of a computer-assisted translation
tool.

References
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-

ural Language Processing with Python. O’Reilly Me-
dia.

Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-Sense Dis-
ambiguation Using Statistical Methods. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics, pages 264–270.

Marine Carpuat and Dekai Wu. 2007. How Phrase
Sense Disambiguation Outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation.

Marine Carpuat. 2013. NRC: A Machine Translation
Approach to Cross-Lingual Word Sense Disambigua-
tion (SemEval-2013 Task 10). In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval 2013), Atlanta, USA.

John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In Pro-

ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17–24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.

Michael Gasser. 2012. Toward a Rule-Based Sys-
tem for English-Amharic Translation. In LREC-2012:
SALTMIL-AfLaT Workshop on Language technology
for normalisation of less-resourced languages.

Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
The Tenth Machine Translation Summit, Phuket, Thai-
land.

D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.

Els Lefever and Véronique Hoste. 2009. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 82–87, Boulder, Colorado, June.
Association for Computational Linguistics.

Els Lefever and Véronique Hoste. 2013. SemEval-2013
Task 10: Cross-Lingual Word Sense Disambiguation.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), Atlanta, USA.

Els Lefever, Véronique Hoste, and Martine De Cock.
2011. ParaSense or How to Use Parallel Corpora for
Word Sense Disambiguation. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 317–322, Portland, Oregon, USA, June. Asso-
ciation for Computational Linguistics.

Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy Belief Propagation for Approximate In-
ference: An Empirical Study. In UAI ’99: Proceed-
ings of the Fifteenth Conference on Uncertainty in Ar-
tificial Intelligence, Stockholm, Sweden.

Alex Rudnick. 2011. Towards Cross-Language Word
Sense Disambiguation for Quechua. In Proceedings
of the Second Student Research Workshop associated
with RANLP 2011, pages 133–138, Hissar, Bulgaria,
September. RANLP 2011 Organising Committee.

Helmut Schmid. 1995. Improvements In Part-of-Speech
Tagging With an Application To German. In Proceed-
ings of the ACL SIGDAT-Workshop, pages 47–50.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In PROCEEDINGS OF HLT-NAACL, pages 252–259.

Maarten van Gompel and Antal van den Bosch. 2013.
WSD2: Parameter optimisation for Memory-based
Cross-Lingual Word-Sense Disambiguation. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), Atlanta, USA.

177


