Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 313–321,

Beijing, August 2010

313

Mixture Model-based Minimum Bayes Risk Decoding using Multiple 

Machine Translation Systems 

Nan Duan1 

Mu Li, Dongdong Zhang, Ming Zhou 

School of Computer Science and Technology 

Microsoft Research Asia 

Tianjin University 

v-naduan@microsoft.com 

Abstract 

We  present  Mixture  Model-based  Mini-
mum  Bayes  Risk  (MMMBR)  decoding, 
an  approach  that  makes  use  of  multiple 
SMT  systems  to  improve  translation  ac-
curacy.  Unlike  existing  MBR  decoding 
methods  defined  on  the  basis  of  single 
SMT  systems,  an  MMMBR  decoder  re-
ranks translation outputs in the combined 
search  space  of  multiple  systems  using 
the MBR decision rule and a mixture dis-
tribution  of  component  SMT  models for 
translation hypotheses. MMMBR decod-
ing  is  a  general  method  that  is  indepen-
dent of specific SMT models and can be 
applied to various commonly used search 
spaces. Experimental results on the NIST 
Chinese-to-English  MT  evaluation  tasks 
show that our approach brings significant 
improvements  to  single  system-based 
MBR  decoding  and  outperforms  a  state-
of-the-art system combination method. 1 

1 

Introduction 

Minimum  Bayes  Risk  (MBR)  decoding  is  be-
coming more and more popular in recent Statis-
tical  Machine  Translation  (SMT)  research.  This 
approach  requires  a  second-pass  decoding  pro-
cedure  to  re-rank  translation  hypotheses  by  risk 
scores computed based on model’s distribution. 

Kumar  and  Byrne  (2004)  first  introduced 
MBR decoding to SMT field and developed it on 
the  N-best  list  translations.  Their  work  has 
shown that  MBR decoding  performs  better  than 
Maximum  a  Posteriori  (MAP)  decoding  for  dif-
ferent evaluation criteria. After that, many dedi-

                                                 
1 This work has been done while the author was visiting 
Microsoft Research Asia. 

muli@microsoft.com  

dozhang@microsoft.com 

mingzhou@microsoft.com 

cated efforts have been made to improve the per-
formances  of  SMT  systems  by  utilizing  MBR-
inspired  methods.  Tromble  et  al.  (2008)  pro-
posed  a  linear  approximation  to  BLEU  score 
(log-BLEU) as a new loss function in MBR de-
coding  and  extended  it  from  N-best  lists  to  lat-
tices,  and  Kumar  et  al.  (2009)  presented  more 
efficient  algorithms  for  MBR  decoding  on  both 
lattices  and  hypergraphs  to  alleviate  the  high 
computational  cost  problem  in  Tromble  et  al.’s 
work. DeNero et al. (2009) proposed a fast con-
sensus  decoding  algorithm  for  MBR  for  both 
linear and non-linear similarity measures. 

All  work  mentioned  above  share  a  common 
setting:  an  MBR  decoder  is  built  based  on  one 
and only one MAP decoder. On the other hand, 
recent  research  has  shown  that  substantial  im-
provements can be achieved by utilizing consen-
sus  statistics  over  multiple  SMT  systems  (Rosti 
et  al.,  2007;  Li  et  al.,  2009a;  Li  et  al.,  2009b; 
Liu  et  al.,  2009).  It  could  be  desirable  to  adapt 
MBR decoding to multiple SMT systems as well. 

In  this  paper,  we  present  Mixture  Model-

based Minimum Bayes Risk (MMMBR) decoding, 
an  approach  that  makes  use  of  multiple  SMT 
systems  to  improve  translation  performance.  In 
this  work,  we  can  take  advantage  of  a  larger 
search  space  for  hypothesis  selection,  and  em-
ploy  an  improved  probability  distribution  over 
translation  hypotheses  based  on  mixture  model-
ing,  which  linearly  combines  distributions  of 
multiple  component  systems  for  Bayes  risk 
computation. The key contribution of this paper 
is the usage of mixture modeling in MBR, which 
allows  multiple  SMT  models  to  be  involved  in 
and makes the computation of n-gram consensus 
statistics to be more accurate. Evaluation results 
have  shown  that  our  approach  not  only  brings 
significant improvements to single system-based 
MBR  decoding  but  also  outperforms  a  state-of-
the-art word-level system combination method. 

314

The rest of the paper is organized as follows: 
In  Section  2,  we  first  review  traditional  MBR 
decoding method and summarize various search 
spaces that can be utilized by an MBR decoder. 
Then,  we describe  how  a mixture  model can be 
used  to  combine  distributions  of  multiple  SMT 
systems  for  Bayes  risk  computation.  Lastly,  we 
present  detailed  MMMBR  decoding  model  on 
multiple  systems  and  make  comparison  with 
single  system-based  MBR  decoding  methods. 
Section  3  describes  how  to  optimize  different 
types of parameters. Experimental results will be 
shown  in  Section  4.  Section  5  discusses  some 
related work and Section 6 concludes the paper. 

2  Mixture Model-based MBR Decoding 

2.1  Minimum Bayes Risk Decoding 

Given a source sentence  , MBR decoding aims 
to  find  the  translation  with  the  least  expected 
loss  under  a  probability  distribution. The  objec-
tive of an MBR decoder can be written as: 

           
     

                    
  
    

(1) 

where    denotes a search space for hypothesis 
selection;      denotes  an  evidence  space  for 
Bayes  risk  computation;      denotes  a  function 
that measures the loss between    and  ;      is 
the underlying distribution based on   . 

Some of existing work on MBR decoding fo-
cused  on  exploring  larger  spaces  for  both     
and     ,  e.g.  from  N-best  lists  to  lattices  or 
hypergraphs (Tromble et al., 2008; Kumar et al., 
2009).  Various  loss  functions  have  also  been 
investigated  by  using  different  evaluation  crite-
ria  for  similarity  computation,  e.g.  Word  Error 
Rate,  Position-independent  Word  Error  Rate, 
BLEU and log-BLEU (Kumar and Byrne, 2004; 
Tromble  et  al.,  2008).  But  less  attention  has 
been  paid  to  distribution     .  Currently,  many 
SMT  systems  based  on  different  paradigms  can 
yield similar performances but are good at mod-
eling  different  inputs  in  the  translation  task 
(Koehn  et  al.,  2004a;  Och  et  al.,  2004;  Chiang, 
2007; Mi et al., 2008; Huang, 2008). We expect 
to  integrate  the  advantages  of  different  SMT 
models  into  MBR  decoding  for  further  im-
provements.  In  particular, we  make  in-depth in-
vestigation into MBR decoding concentrating on 

the  translation  distribution      by  leveraging  a 
mixture model based on multiple SMT systems. 

2.2  Summary of Translation Search Spaces 

There  are  three  major  forms  of  search  spaces 
that can be obtained from an MAP decoder as a 
byproduct,  depending  on  the  design  of  the  de-
coder: N-best lists, lattices and hypergraphs. 

An  N-best  list  contains  the   most  probable 
translation hypotheses produced by a decoder. It 
only  presents  a  very  small  portion  of  the  entire 
search space of an SMT model. 

A  hypergraph  is  a  weighted  acyclic  graph 
which  compactly  encodes  an  exponential  num-
ber  of  translation  hypotheses.  It  allows  us  to 
represent  both  phrase-based  and  syntax-based 
systems  in  a  unified  framework.  Formally,  a 
hypergraph   is  a  pair         ,  where   is  a 
set  of  hypernodes  and   is  a  set  of  hyperedges. 
Each  hypernode         corresponds  to  transla-
tion  hypotheses  with  identical  decoding  states, 
which  usually  include  the  span          of  the 
words  being  translated,  the  grammar  symbol   
for  that  span  and  the  left  and  right  boundary 
words  of  hypotheses  for  computing  language 
model  (LM)  scores.  Each  hyperedge       cor-
responds  to  a  translation  rule  and  connects  a 
head node      and a set of tail nodes     . The 
number of tail nodes        is called the arity of 
the hyperedge   and the arity of a hypergraph is 
the maximum arity of its hyperedges. If the arity 
of  a  hyperedge   is  zero,      is  then  called  a 
source node. Each hypergraph has a unique root 
node  and  each  path  in  a  hypergraph  induces  a 
translation  hypothesis.  A  lattice  (Ueffing  et  al., 
2002) can be viewed as a special hypergraph, in 
which the maximum arity is one. 

2.3  Mixture Model for SMT 

We first describe how to construct a general dis-
tribution for translation hypotheses over multiple 
SMT systems using mixture modeling for usage 
in MBR decoding. 

Mixture modeling is a technique that has been 
applied  to  many  statistical  tasks  successfully. 
For  the  SMT  task  in  particular,  given    SMT 
systems with their corresponding model distribu-
tions, a mixture model is defined as a probability 
distribution  over  the  combined  search  space  of 
all  component  systems  and  computed  as  a 
weighted sum of component model distributions: 

315

 

                           

  

(2) 

   

In  Equation  2,                are  system  weights 
which  hold  following  constraints:             
 
       ,             is  the   th  distri-
and  
bution  estimated  on  the  search  space    based 
on the log-linear formulation: 

  

             

               

 

     

                

  

where          is  the  score  function  of  the   th 
system for translation  ,             is a scaling 
factor  that  determines  the  flatness  of  the  distri-
bution    sharp (      ) or smooth (      ). 

Due to the inherent differences in SMT mod-
els,  translation  hypotheses  have  different  distri-
butions  in  different  systems.  A  mixture  model 
can  effectively  combine  multiple  distributions 
with tunable system weights. The distribution of 
a  single  model  used  in  traditional  MBR  can  be 
seen as a special mixture model, where   is one. 

2.4  Mixture Model for SMT 

Let                  denote   machine translation 
systems,     denotes  the  search  space  produced 
by  system     in  MAP  decoding  procedure.  An 
MMMBR  decoder  aims  to  seek  a  translation 
from the combined search space         
 that 
maximizes  the  expected  gain  score  based  on  a 
mixture  model          .  We  write  the  objec-
tive function of MMMBR decoding as: 

 

           

    

                   
  
   

(3) 

For  the  gain  function     ,  we  follow  Trom-
ble  et  al.  (2008)  to  use  log-BLEU,  which  is 
scored  by  the  hypothesis  length  and  a  linear 
function of n-gram matches as: 

                                  

     

 

In  this  definition,     is  a  reference  translation, 
     is  the  length  of  hypothesis    ,    is  an  n-
gram  presented  in   ,        is  the  number  of 
times  that   occurs  in   ,  and       is  an  indi-
cator function which equals to 1 when   occurs 
in     and  0  otherwise.                  are  model 
parameters,  where   is  the  maximum  order  of 
the n-grams involved. 

For  the  mixture  model     ,  we  replace  it  by 
Equation  2  and  rewrite  the  total  gain  score  for 
hypothesis    in Equation 3: 

                   
 
   
                            
 

   

      

 

      

 

 

                     
 
   

 
                       
   

    

 

 

 

 

 

 

(4) 

  

In  Equation  4,  the  total  gain  score  on  the  com-
bined  search  space     can  be  further  decom-
posed  into  each  local  search  space     with  a 
specified distribution            . This is a nice 
property  and  it  allows  us  to  compute  the  total 
gain  score  as  a  weighted  sum  of  local  gain 
scores on different search spaces. We expand the 
local gain score for    computed on search space 
   with             using log-BLEU as: 

                       
 
    

                               

 

            

    

 

                               

                           

 

    

We  make  two  approximations  for  the  situations 
when      :  the  first  is  
               
 
    
and  the  second  is   
                  
          In  fact,  due  to  the  differences  in  ge-
nerative  capabilities  of  SMT  models,  training 
data  selection  and  various  pruning  techniques 
used,  search  spaces  of  different  systems  are  al-
ways  not  identical  in  practice.  For  the  conveni-
ence of formal analysis, we treat all             
as  ideal  distributions  with  assumptions  that  all 
systems work in similar settings, and translation 
candidates are shared by all systems. 

The  method  for  computing  n-gram  posterior 
probability          in  Equation  5  depends  on 
different types of search space   : 
  When     is  an  N-best  list,  it  can  be  com-
puted immediately by enumerating all trans-
lation candidates in the N-best list: 

                             

  

    

316

  When    is  a  hypergraph  (or  a  lattice)  that 
encodes  exponential  number  of  hypotheses, 
it is often impractical to compute this proba-
bility  directly.    In  this  paper,  we  use the al-
gorithm  presented  in  Kumar  et  al.  (2009) 
which is described in Algorithm 12: 

                                  

           
 

    

   

                                              
 

   

    

                                   
  

   

             counts  the  edge   with  n-gram 
  that has the highest edge posterior proba-
bility  relative  to  predecessors  in  the  entire 
graph   , and          is the edge posterior 
probability  that  can  be  efficiently  computed 
with  standard  inside  and  outside  probabili-
ties      and      as: 

          

 

    

                  

  

      

where      is  the  weight  of  hyperedge   in 
   ,        is  the  normalization  factor  that 
equals  to  the  inside  probability  of  the  root 
node in   .  

 

Algorithm 1: Compute n-gram posterior proba-
bilities on hypergraph    (Kumar et al., 2009) 
1: 
2: 

sort hypernodes topologically 
compute  inside/outside  probabilities      and      
for each hypernode        
compute  edge  posterior  probability            for 
each hyperedge         
for each hyperedge        do  
      merge  n-grams  on        and  keep  the  highest 

probability when n-grams are duplicated 

      apply  the  rule  of  edge   to  n-grams  on      and 

propagate       gram prefixes/suffixes to      

         for each n-gram   introduced by   do  
     if                         then 
 
          

 

                                    
                        

          
else 
                               
       
 
 
end if 
 
end for   
end for 
return n-gram posterior probability set             

3: 

4: 
5: 

6: 

7: 
8: 
9: 

10: 
11: 
12: 
13: 
14: 
15: 

                                                 
2 We omit the similar algorithm for lattices because of their 
homogenous  structures  comparing  to  hypergraphs  as  we 
discussed in Section 2.2. 

Thus, the total gain score for hypothesis    on 

        

 

 can be further expanded as: 

 
                       
 
   

    
 

    
 
                                
   

 

 

 

      

 

      

                                

  

 

 

         

                          

        

  

 

 

 

                              

                            

 

 

where             
          is  a  mixture  n-
gram  posterior  probability.  The  most  important 
fact derived from Equation 6 is that, the mixture 
of different distributions can be simplified to the 
weighted  sum  of  n-gram  posterior  probabilities 
on different search spaces.  

We now derive the decision rule of MMMBR 

decoding based on Equation 6 below: 

           

    

                         

  

(7) 

 

We also notice that MAP decoding and MBR 
decoding  are  two  different  ways  of  estimating 
the  probability          and  each  of  them  has 
advantages  and  disadvantages.  It  is  desirable  to 
interpolate  them  together  when  choosing  the  fi-
nal translation outputs.  So we  include  each  sys-
tem’s  MAP  decoding  cost  as  an  additional  fea-
ture further and modify Equation 7 to: 

           
 ′  

                         

 

 

 
 
 

 

                       
  

 
(8) 
where                  is  the  model  cost  as-
signed by the MAP decoder    for hypothesis   . 
Because the costs of MAP decoding on different 
SMT  models  are  not  directly  comparable,  we 
utilize  the  MERT  algorithm  to  assign  an  appro-
priate weight    for each component system.  

Compared  to  single  system-based  MBR  de-

coding, which obeys the decision rule below:  

           
     

                            

   

 

317

MMMBR decoding has a similar objective func-
tion  (Equation  8).  The  key  difference  is  that,  in 
MMMBR  decoding,  n-gram  posterior  probabili-
ty      is computed as     
         based on 
an  ensemble  of  search  spaces;  meanwhile,  in 
single system-based MBR decoding, this quanti-
ty is computed locally on single search space   . 
The  procedure  of  MMMBR  decoding  on  mul-
tiple SMT systems is described in Algorithm 2. 
 

 

Algorithm  2:  MMMBR  decoding  on  multiple 
SMT systems 
1: 
2: 

for each component system    do 
     run MAP decoding and generate the correspond-

3: 

 

ing search space    
compute  the  n-gram  posterior  probability  set 
            for    based on Algorithm 1 

 

         for each  : 

                    

end for 
compute  the  mixture  n-gram  posterior    probability 
           
for each unique n-gram   appeared in     
      for each search space    do 
 
        end for 
end for  
for each hyperedge   in     
    assign      to the edge   for all   contained in   
end for 
return the best path according to Equation 8 

 do 

 do 

 

 

4: 
5 

6: 
7: 
8 
9: 
10: 
11: 
12: 
13: 
14: 

 

empirically  based  on  different  performances,  or 
equally without any bias. Note that although we 
tune  MBR  parameters  on  N-best  lists,  n-gram 
posterior  probabilities  used  for  Bayes  risk 
computation  could 
still  be  estimated  on 
hypergraphs for non N-best-based search spaces. 

3.2  Parameter  Optimization  for  Mixture 

Model 

After  MBR  parameters  optimized,  we  begin  to 
tune system weights for the mixture model in the 
second-pass training. We rewrite Equation 8 as: 

           
 ′  

    
 

         

                                                       
 

                                                         

          

 

 

For  each     ,  the  aggregated  score  surrounded 
with braces can be seen as its feature value. Eq-
uation 9 now turns to be a linear function for all 
weights and can be optimized by the MERT. 

4  Experiments 

4.1  Data and Metric 

3  A Two-Pass Parameter Optimization 

In Equation 8, there are two types of parameters: 
parameters introduced by the gain function      
and the model cost        , and system weights 
introduced  by  the  mixture  model     .  Because 
Equation  8 is not a  linear function  when all  pa-
rameters  are  taken  into  account,  MERT  algo-
rithm  (Och,  2003)  cannot  be  directly  applied  to 
optimize them at the same time. Our solution is 
to employ a two-pass training strategy, in which 
we  optimize  parameters  for  MBR  first  and  then 
system weights for the mixture model. 

3.1  Parameter Optimization for MBR 

The  inputs  of  an  MMMBR  decoder  can  be  a 
combination  of  translation  search  spaces  with 
arbitrary structures. For the sake of a general and 
convenience solution for optimization, we utilize 
the  simplest  N-best  lists  with  proper  sizes  as 
approximations  to  arbitrary  search  spaces  to 
optimize  MBR  parameters  using  MERT  in  the 
first-pass  training.  System  weights  can  be  set 

We  conduct  experiments  on  the  NIST  Chinese-
to-English machine translation tasks. We use the 
newswire  portion  of  the  NIST  2006  test  set 
(MT06-nw) as the development set for parameter 
optimization,  and  report  results  on  the  NIST 
2008  test  set  (MT08).  Translation  performances 
are measured in terms of case-insensitive BLEU 
scores. Statistical significance is computed using 
the  bootstrap  re-sampling  method  proposed  by 
Koehn (2004b). Table 1 gives data statistics. 
 

Data Set 

#Sentence 

MT06-nw (dev) 

MT08 (test) 

616 
1,357 

#Word 
17,316 
31,600 

Table 1. Statistics on dev and test data sets 

All  bilingual  corpora  available  for  the  NIST 
2008  constrained  track  of  Chinese-to-English 
machine translation task are used as training data, 
which  contain  5.1M  sentence  pairs,  128M  Chi-
nese  words  and  147M  English  words  after  pre-
processing.  Word  alignments  are  performed  by 
GIZA++ with an intersect-diag-grow refinement.  

318

A  5-gram  language  model  is  trained  on  the 
English side of all bilingual data plus the Xinhua 
portion of LDC English Gigaword Version 3.0. 

4.2  System Description 

We  use  two  baseline  systems.  The  first  one 
(SYS1)  is  a  hierarchical  phrase-based  system 
(Chiang,  2007)  based  on  Synchronous  Context 
Free  Grammar  (SCFG),  and  the  second  one 
(SYS2)  is  a  phrasal  system  (Xiong  et  al.,  2006) 
based  on  Bracketing  Transduction  Grammar 
(Wu,  1997)  with  a  lexicalized  reordering  com-
ponent  based  on  maximum  entropy  model. 
Phrasal  rules  shared  by  both  systems  are  ex-
tracted  on  all  bilingual  data,  while  hierarchical 
rules  for  SYS1  only  are  extracted  on  a  selected 
data set, including LDC2003E07, LDC2003E14, 
LDC2005T06,  LDC2005T10,  LDC2005E83, 
LDC2006E26, LDC2006E34, LDC2006E85 and 
LDC2006E92, which contain about 498,000 sen-
tence pairs. Translation hypergraphs are generat-
ed by each baseline system during the MAP de-
coding  phase,  and  1000-best  lists  used  for 
MERT  algorithm  are  extracted  from  hyper-
graphs  by  the  k-best  parsing  algorithm  (Huang 
and  Chiang,  2005).  We  tune  scaling  factor  to 
optimize  the  performance  of  HyperGraph-based 
MBR  decoding  (HGMBR)  on  MT06-nw  for 
each system (0.5 for SYS1 and 0.01 for SYS2). 

4.3  MMMBR Results on Multiple Systems 

We first present the overall results of MMMBR 
decoding on two baseline systems. 

To  compare  with  single  system-based  MBR 
methods,  we  re-implement  N-best  MBR,  which 
performs MBR decoding on 1000-best lists with 
the  fast  consensus  decoding  algorithm  (DeNero 
et  al.,  2009),  and  HGMBR,  which  performs 
MBR  decoding  on  a  hypergraph  (Kumar  et  al., 
2009).  Both  methods  use  log-BLEU  as  the  loss 
function.  We  also  compare  our  method  with 
IHMM Word-Comb, a state-of-the-art word-level 
system  combination  approach  based  on  incre-
mental  HMM  alignment  proposed  by  Li  et  al. 
(2009b).  We  report  results  of  MMMBR  decod-
ing  on  both  N-best  lists  (N-best  MMMBR)  and 
hypergraphs  (Hypergraph  MMMBR)  of  two 
baseline systems. As MBR decoding can be used 
for  any  SMT  system,  we  also  evaluate  MBR-
IHMM  Word-Comb,  which  uses  N-best  lists 
generated by HGMBR on each baseline systems. 

The  default  beam  size is  set to  50  for MAP  de-
coding  and  hypergraph  generation.  The  setting 
of  N-best  candidates  used  for  (MBR-)  IHMM 
Word-Comb is the same as the one used in Li et 
al. (2009b). The maximum order of n-grams in-
volved in MBR model is set to 4. Table 2 shows 
the evaluation results. 
 

 
 

MAP 

N-best MBR 

HGMBR 
IHMM 

Word-Comb 
MBR-IHMM 
Word-Comb 

N-best 

MMMBR 
Hypergraph 
MMMBR 

MT06-nw 

MT08 

SYS1  SYS2  SYS1  SYS2 
28.0 
38.1 
28.1 
38.3 
38.3 
28.3 

37.1 
37.4 
37.5 

28.5 
29.0 
29.1 

39.1 

39.3 

39.0* 

39.4*+ 

29.3 

29.7 

29.4* 

29.9*+ 

Table  2.  MMMBR  decoding  on  multiple  sys-
tems  (*:  significantly  better  than  HGMBR  with 
         ;  +:  significantly  better  than  IHMM 
Word-Comb with         ) 

From  Table  2  we  can  see  that,  compared  to 
MAP decoding, N-best MBR and HGMBR only 
improve  the  performance  in  a  relative  small 
range  (+0.1~+0.6  BLEU),  while  MMMBR  de-
coding on multiple systems can yield significant 
improvements  on  both  dev  set  (+0.9  BLEU  on 
N-best  MMMBR  and  +1.3  BLEU  on  Hyper-
graph MMMBR) and test set (+0.9 BLEU on N-
best  MMMBR  and  +1.4  BLEU  on  Hypergraph 
MMMBR);  compared  to  IHMM  Word-Comb, 
N-best MMMBR can achieve comparable results 
on  both  dev  and  test  sets,  while  Hypergraphs 
MMMBR  can  achieve  even  better  results  (+0.3 
BLEU  on  dev  and  +0.6  BLEU  on  test);  com-
pared to MBR-IHMM Word-Comb, Hypergraph 
MMMBR  can  also  obtain  comparable  results 
with tiny improvements (+0.1 BLEU on dev and 
+0.2  BLEU  on  test).  However,  MBR-IHMM 
Word-Comb  has  ability  to  generate  new  hypo-
theses, while Hypergraph MMMBR only choos-
es translations from original search spaces. 

We  next  evaluate  performances  of  MMMBR 
decoding  on  hypergraphs  generated  by  different 
beam size settings, and compare them to (MBR-) 

319

IHMM  Word-Comb  with  the  same  candidate 
size and HGMBR with the same beam size. We 
list the results of MAP decoding for comparison. 
The comparative  results  on  MT08  are shown  in 
Figure  1,  where  X-axis  is  the  size  used  for  all 
methods  each  time,  Y-axis  is  the  BLEU  score, 
MAP-  and  HGMBR-  stand  for  MAP  decoding 
and HGMBR decoding for the  th system. 

30.5

30.0

29.5

29.0

28.5

28.0

27.5

10

20

50

100

150

MAP-1

MAP-2

HGMBR-1

HGMBR-2

IHMM

MBR-IHMM

MMMBR

 

Figure  1.  MMMBR  vs.  (MBR-)  IHMM  Word-
Comb and HGMBR with different sizes 

From Figure 1 we can see that, MMMBR de-
coding  performs  consistently  better  than  both 
(MBR-)  IHMM  Word-Comb  and  HGMBR  on 
all  sizes.  The  gains  achieved  are  around  +0.5 
BLEU  compared  to  IHMM  Word-Comb,  +0.2 
BLEU  compared  to  MBR-IHMM  Word-Comb, 
and  +0.8  BLEU  compared  to  HGMBR.  Com-
pared to MAP decoding, the best result (30.1) is 
obtained  when  the  size  is  100,  and  the  largest 
improvement (+1.4 BLEU) is obtained when the 
size is 50. However, we did not observe signifi-
cant improvement when the size is larger than 50.  

We then setup an experiment to verify that the 
mixture model based on multiple distributions is 
more  effective  than  any  individual  distributions 
for  Bayes  risk  computation  in  MBR  decoding. 
We use Mix-HGMBR to denote MBR decoding 
performed  on  single  hypergraph  of  each  system 
in  the  meantime  using  a  mixture  model  upon 
distributions of two systems for Bayes risk com-
putation.  We  compare  it  with  HGMBR  and 
Hypergraph MMMBR and list results in Table 3. 
 

 
 

HGMBR 

Mix-HGMBR 

MT08 

SYS1  SYS2 
28.3 
29.1 
29.4 
28.9 

Hypergraph MMMBR 

29.9 

Table 3. Performance of MBR decoding on dif-
ferent settings of search spaces and distributions 

It  can  be  seen  that  based  on  the  same  search 
space,  the  performance  of  Mix-HGMBR  is  sig-
nificantly better than that of HGMBR (+0.3/+0.6 
BLEU  on  dev/test).  Yet  the  performance  is  still 
not  as  good  as  Hypergraph,  which  indicates  the 
fact that the mixture model and the combination 
of search spaces are both helpful to MBR decod-
ing, and the best choice is to use them together. 

We also empirically investigate the impacts of 
different  system  weight  settings  upon  the  per-
formances  of  Hypergraph  MMMBR  on  dev  set 
in  Figure  2,  where  X-axis  is  the  weight    for 
SYS1, Y-axis is the BLEU score. The weight    
for  SYS2  equals  to        as  only  two  systems 
involved. The best evaluation result on dev set is 
achieved  when  the  weight  pair  is  set  to  0.7/0.3 
for  SYS1/SYS2,  which  is  also  very  close to the 
one trained automatically by the training strategy 
presented  in  Section  3.2.  Although  this  training 
strategy  can  be  processed  repeatedly,  the  per-
formance is stable after the 1st round finished. 

39.5

39.3

39.1

38.9

38.7

38.5

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

MMMBR

 

Figure 2. Impacts of different system weights in 
the mixture model 

4.4  MMMBR  Results  on  Identical  Systems 

with Different Translation Models 

Inspired  by  Macherey  and  Och  (2007),  we  ar-
range a similar experiment to test MMMBR de-
coding for each baseline system on an ensemble 
of sub-systems built by the following two steps. 

Firstly,  we  iteratively  apply  the  following 
procedure  3  times:  at  the  th  time,  we  randomly 
sample  80%  sentence  pairs  from  the  total  bilin-
gual  data  to  train  a  translation  model  and  use  it 
to build a new system based on the same decod-
er,  which  is  denoted  as  sub-system-  .  Table  4 
shows  the  evaluation  results  of  all  sub-systems 
on  MT08,  where  MAP  decoding  (the  former 
ones)  and  corresponding  HGMBR  (the  latter 
ones) are grouped together by a slash. We set all 
beam sizes to 20 for a time-saving purpose. 

320

MT08 

5  Related Work 

 
 

Baseline 

sub-system-1 
sub-system-2 
sub-system-3 

SYS1 

28.4/29.0 
28.1/28.5 
28.3/28.4 
27.7/28.0 

SYS2 

27.6/27.8 
26.8/27.3 
27.0/27.1 
27.3/27.6 

Table 4. Performance of sub-systems 

Secondly, starting from each baseline system, 
we  gradually  add  one  more  sub-system  each 
time  and  perform  Hypergraph  MMMBR  on 
hypergraphs  generated  by  current  involved  sys-
tems. Table 5 shows the evaluation results. 

 

 
 

MAP 

HGMBR 

MT08 

SYS1  SYS2 
27.6 
28.4 
29.0 
27.8 

Hypergraph MMMBR 

+ sub-system-1 
+ sub-system-2 
+ sub-system-3 

29.1 
29.1 
29.3 

27.9 
28.1 
28.3 

Table  5.  Performance  of  Hypergraph  MMMBR 
on multiple sub-systems 

We  can  see  from  Table  5  that,  compared  to 
the  results  of  MAP  decoding,  MMMBR  decod-
ing  can  achieve  significant  improvements  when 
more than one sub-system are involved; however, 
compared to the results of HGMBR on baseline 
systems,  there  are  few  changes  of  performance 
when the number of sub-systems increases. One 
potential reason is that the translation hypotheses 
between  multiple  sub-systems  under  the  same 
SMT  model  hold  high  degree  of  correlation, 
which is discussed in Macherey and Och (2007). 
We  also  evaluate  MBR-IHMM  Word-Comb 
on  N-best  lists  generated  by  each  baseline  sys-
tem  with  its  corresponding  three  sub-systems. 
Evaluation  results  are  shown  in  Table  6,  where 
Hypergraph  MMMBR  still  outperforms  MBR-
IHMM Word-Comb on both baseline systems. 
 

 
 

MBR-IHMM Word-Comb 

Hypergraph MMMBR 

MT08 

SYS1  SYS2 
28.0 
29.1 
29.3 
28.3 

Table 6. Hypergraph MMMBR vs. MBR-IHMM 
Word-Comb with multiple sub-systems 

Employing consensus between multiple systems 
to improve machine translation quality has made 
rapid progress in recent years. System combina-
tion methods based on confusion networks (Ros-
ti  et  al.,  2007;  Li  et  al.,  2009b)  have  shown 
state-of-the-art performances in MT benchmarks. 
Different  from  them,  MMMBR  decoding  me-
thod does not generate new translations. It main-
tains  the  essential  of  MBR  methods  to  seek 
translations  from  existing  search  spaces.  Hypo-
thesis  selection  method  (Hildebrand  and  Vogel, 
2008) resembles more our method in making use 
of n-gram statistics. Yet their work does not be-
long  to  the  MBR  framework  and  treats  all  sys-
tems  equally.  Li  et  al.  (2009a)  presents  a  co-
decoding  method,  in  which  n-gram  agreement 
and  disagreement  statistics  between  translations 
of  multiple  decoders  are  employed  to  re-rank 
both full and partial hypotheses during decoding. 
Liu  et  al.  (2009)  proposes  a  joint-decoding  me-
thod  to  combine  multiple SMT  models  into  one 
decoder  and  integrate  translation  hypergraphs 
generated  by  different  models.  Both  of  the  last 
two methods work in a white-box way and need 
to  implement  a  more  complicated  decoder  to 
integrate multiple SMT models to work together; 
meanwhile our method can be conveniently used 
as  a  second-pass  decoding  procedure,  without 
considering any system implementation details. 

6  Conclusions and Future Work 

In  this  paper,  we  have  presented  a  novel 
MMMBR  decoding  approach  that  makes  use  of 
a  mixture  distribution  of  multiple  SMT  systems 
to  improve  translation  accuracy.  Compared  to 
single  system-based  MBR  decoding  methods, 
our  method  can  achieve  significant  improve-
ments  on  both  dev  and  test  sets.  What  is  more, 
MMMBR decoding approach also outperforms a 
state-of-the-art system combination method.  We 
have empirically verified that the success of our 
method  comes  from  both  the  mixture  modeling 
of  translation  hypotheses  and  the  combined 
search space for translation selection. 

In the future, we will include more SMT sys-
tems  with  more  complicated  models  into  our 
MMMBR  decoder  and  employ  more  general 
MERT  algorithms  on  hypergraphs  and  lattices 
(Kumar et al., 2009) for parameter optimization. 

321

References 

Chiang  David.  2007.  Hierarchical  Phrase  Based 
Translation.  Computational  Linguistics,  33(2): 
201-228. 

DeNero John, David Chiang, and Kevin Knight. 2009. 
Fast  Consensus  Decoding  over  Translation 
Forests.  In  Proc.  of  47th  Meeting  of  the  Associa-
tion for Computational Linguistics, pages 567-575. 

Hildebrand  Almut  Silja  and  Stephan  Vogel.  2008. 
Combination of Machine Translation Systems 
via  Hypothesis  Selection  from  Combined  N-
best lists. In Proc. of the Association for Machine 
Translation in the Americas, pages 254-261. 

Huang Liang and David Chiang. 2005. Better k-best 
Parsing.  In  Proc.  of 7th  International  Conference 
on Parsing Technologies, pages 53-64. 

Huang Liang. 2008. Forest Reranking: Discrimin-
ative  Parsing  with  Non-Local  Features.  In 
Proc. of  46th Meeting of the Association for Com-
putational Linguistics, pages 586-594. 

Koehn  Philipp.  2004a.  Phrase-based  Model  for 

SMT. Computational Linguistics, 28(1): 114-133. 

Koehn  Philipp.  2004b.  Statistical  Significance 
Tests  for  Machine  Translation  Evaluation.  In 
Proc. of Empirical Methods on Natural Language 
Processing, pages 388-395. 

Kumar Shankar and William Byrne. 2004. Minimum 
Bayes-Risk  Decoding  for  Statistical  Machine 
Translation.  In  Proc.  of  the  North  American 
Chapter of the Association for Computational Lin-
guistics, pages 169-176.  

Kumar  Shankar,  Wolfgang  Macherey,  Chris  Dyer, 
and  Franz  Och.  2009.  Efficient  Minimum  Error 
Rate  Training  and  Minimum  Bayes-Risk  De-
coding for Translation Hypergraphs and Lat-
tices. In Proc.  of  47th  Meeting of  the  Association 
for Computational Linguistics, pages 163-171. 

Li Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming  Zhou.  2009a.  Collaborative  Decoding: 
Partial  Hypothesis  Re-Ranking  Using  Trans-
lation  Consensus  between  Decoders.  In  Proc. 
of  47th  Meeting  of  the  Association  for  Computa-
tional Linguistics, pages 585-592. 

Liu Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009. 
Joint  Decoding  with  Multiple  Translation 
Models.  In  Proc.  of  47th  Meeting  of  the  Associa-
tion for Computational Linguistics, pages 576-584. 

Li Chi-Ho, Xiaodong He, Yupeng Liu, and Ning Xi. 
2009b.  Incremental  HMM  Alignment  for  MT 
system Combination. In Proc. of 47th Meeting of 
the  Association  for  Computational  Linguistics, 
pages 949-957. 

Mi Haitao, Liang Huang, and Qun Liu. 2008. Forest-
Based  Translation.  In  Proc.  of  46th  Meeting  of 
the  Association  for  Computational  Linguistics, 
pages 192-199. 

Macherey  Wolfgang  and  Franz  Och.  2007.  An  Em-
pirical Study on Computing Consensus Trans-
lations  from  multiple  Machine  Translation 
Systems. In Proc. of Empirical Methods on Natu-
ral Language Processing, pages 986-995. 

Och  Franz.  2003.  Minimum  Error  Rate  Training 
in  Statistical  Machine  Translation.  In  Proc.  of 
41th Meeting of the Association for Computational 
Linguistics, pages 160-167. 

Och Franz and Hermann Ney. 2004. The Alignment 
template  approach  to  Statistical  Machine 
Translation.  Computational  Linguistics,  30(4): 
417-449. 

Rosti  Antti-Veikko,  Spyros  Matsoukas,  and  Richard 
Schwartz.  2007.  Improved  Word-Level  System 
Combination for Machine Translation. In Proc. 
of  45th  Meeting  of  the  Association  for  Computa-
tional Linguistics, pages 312-319. 

Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk Decoding for Statistical Machine Trans-
lation. In Proc. of Empirical Methods on Natural 
Language Processing, pages 620-629. 

Ueffing Nicola, Franz Och, and Hermann Ney. 2002. 
Generation of Word Graphs in Statistical Ma-
chine  Translation.  In  Proc.  of  Empirical  Me-
thods  on  Natural  Language  Processing,  pages 
156-163. 

Wu  Dekai.  1997.  Stochastic  Inversion  Transduc-
tion  Grammars  and  Bilingual  Parsing  of  Pa-
rallel  Corpora.  Computational  Linguistics, 
23(3): 377-404. 

Xiong Deyi, Qun Liu, and Shouxun Lin. 2006. Max-
imum  Entropy  based  Phrase  Reordering 
Model  for  Statistical  Machine  Translation.  In 
Proc. of  44th Meeting of the Association for Com-
putational Linguistics, pages 521-528. 

