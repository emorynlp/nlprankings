647

Coling 2010: Poster Volume, pages 647–655,

Beijing, August 2010

Text Mining for Automatic Image Tagging

Chee Wee Leong and Rada Mihalcea and Samer Hassan

Department of Computer Science and Engineering

University of North Texas

cheeweeleong@my.unt.edu, rada@cs.unt.edu, samer@unt.edu

Abstract

This paper introduces several extractive
approaches for automatic image tagging,
relying exclusively on information mined
from texts. Through evaluations on two
datasets, we show that our methods ex-
ceed competitive baselines by a large mar-
gin, and compare favorably with the state-
of-the-art that uses both textual and image
features.
Introduction

1
With continuously increasing amounts of images
available on the Web and elsewhere, it is impor-
tant to ﬁnd methods to annotate and organize im-
age databases in meaningful ways. Tagging im-
ages with words describing their content can con-
tribute to faster and more effective image search
and classiﬁcation. In fact, a large number of ap-
plications, including the image search feature of
current search engines (e.g., Yahoo!, Google) or
the various sites providing picture storage services
(e.g., Flickr, Picasa) rely exclusively on the tags
associated with an image in order to search for rel-
evant images for a given query.

However, the task of developing accurate and
robust automatic image annotation models entails
daunting challenges. First, the availability of large
and correctly annotated image databases is cru-
cial for the training and testing of new annotation
models. Although a number of image databases
have emerged to serve as evaluation benchmarks
for different applications, including image anno-
tation (Duygulu et al., 2002), content-based im-
age retrieval
(Li and Wang, 2008) and cross
language information retrieval (Grubinger et al.,
2006), such databases are almost exclusively cre-
ated by manual labeling of keywords, requiring
signiﬁcant human effort and time. The content of
these image databases is often restricted only to a

few domains, such as medical and natural photo
scenes (Grubinger et al., 2006), and speciﬁc ob-
jects like cars, airplanes, or buildings (Fergus et
al., 2003). For obvious practical reasons, it is im-
portant to develop models trained and evaluated
on more realistic and diverse image collections.

The second challenge concerns the extraction
of useful image and text features for the construc-
tion of reliable annotation models. Most tradi-
tional approaches relied on the extraction of image
colors and textures (Li and Wang, 2008), or the
identiﬁcation of similar image regions clustered as
blobs (Duygulu et al., 2002) to derive correlations
between image features and annotation keywords.
In comparison, there are only a few efforts that
leverage on the multitude of resources available
for natural language processing to derive robust
linguistic-based image annotation models. One
of the earliest efforts involved the use of captions
for face recognition in photographs through the
construction of a speciﬁc lexicon that integrates
linguistic and photographic information (Srihari
and Burhans, 1994). More recently, several ap-
proaches have proposed the use of WordNet as
a knowledge-base to improve content-based im-
age annotation models, either by removing noisy
keywords through semantic clustering (Jin et al.,
2005) or by inducing a hierarchical classiﬁcation
of candidate labels (Srikanth et al., 2005).

In this paper, we explore the use of several natu-
ral language resources to construct image annota-
tion models that are capable of automatically tag-
ging images from unrestricted domains with good
accuracy. Unlike traditional image annotation
methodologies that generate tags using image-
based features, we propose to extract them in a
manner analogous to keyword extraction. Given a
target image and its surrounding text, we extract
those words and phrases that are most likely to
represent meaningful tags. More importantly, we

648

are interested to investigate the potential of such
linguistic-based models on image annotation ac-
curacy and reliability. Our work is motivated by
the need for annotation models that can be efﬁ-
ciently applied on a very large scale (e.g. har-
vesting images from the web), which are required
in applications that cannot afford the complexity
and time associated with current image process-
ing techniques.

The paper makes the following contributions.
We ﬁrst propose a new evaluation framework for
image tagging, which is based on an analogy
drawn between the tasks of image labeling and
lexical substitution. Next, we present three extrac-
tive approaches for the task of image annotation.
The methods proposed are based only on the text
surrounding an image, without the use of image
features. Finally, by combining several orthogo-
nal methods through machine learning, we show
that it is possible to achieve a performance that is
competitive to a state-of-the-art image annotation
system that relies on visual and textual features,
thus demonstrating the effectiveness of text-based
extractive annotation models.

2 Related Work

Several online systems have sprung into exis-
tence to achieve annotation of real world images
through human collaborative efforts (Flickr) and
stimulating competition (von Ahn and Dabbish,
2004). Although a large number of image tags can
be generated in short time, these approaches de-
pend on the availability of human annotators and
are far from being automatic. Similarly, research
in the other direction via text-to-image synthesis
(Li and Fei-Fei, 2008; Collins et al., 2008; Mi-
halcea and Leong, 2009) has also helped to har-
vest images, mostly for concrete words, by reﬁn-
ing image search engines.

Most approaches to automatic image annota-
tion have focused on the generation of image la-
bels using annotation models trained with image
features and human annotated keywords (Barnard
and Forsyth, 2001; Jeon et al., 2003; Makadia et
al., 2008; Wang et al., 2009). Instead of predict-
ing speciﬁc words, these methods generally target
the generation of semantic classes (e.g. vegeta-
tion, animal, building, places etc), which they can
achieve with a reasonable amount of success. Re-
cent work has also considered the generation of
labels for real-world images (Li and Wang, 2008;
Feng and Lapata, 2008). To our knowledge, we
are unaware of any other work that performs ex-

tractive annotation for images from unrestricted
domains through the exclusive use of textual fea-
tures.

3 Dataset

As the methods we propose are extractive, stan-
dard image databases with no surrounding text
such as Corel (Duygulu et al., 2002) are not suit-
able, nor are they representative for the challenges
associated with raw data from unrestricted do-
mains. We thus create our own dataset using im-
ages randomly extracted from the Web.

To avoid sparse searches, we use a list of the
most frequent words in the British National Cor-
pus as seed words, and query the web using the
Google Image API. A webpage is randomly se-
lected from the query results if it contains a single
image in the speciﬁed size range (width and height
of 275 to 1000 pixels1) and its text contains more
than 10 words. Next, we use a Document Object
Model (DOM) HTML parser2 to extract the con-
tent of the webpage. Note that we do not perform
manual ﬁltering of our images except where they
contain undesirable qualities (e.g. porn, corrupted
or blank images).

In total, we collected 300 image-text pairs from
the web. The average image size is 496 pixels
width and 461 pixels height. The average text
length is 278 tokens and the average document ti-
tle length is 6 tokens. In total, there are 83,522
words and the total vocabulary is 8,409 words.

For each image, we also create a gold stan-
dard of manually assigned tags, by using the la-
bels assigned by ﬁve human annotators. The im-
age annotation is conducted via Amazon Mechan-
ical Turk, which was shown in the past to produce
reliable annotations (Snow et al., 2008). For in-
creased annotation reliability, we only accept an-
notators with an approval rating of 98%.

Given an image, an annotator extracts from
the associated text a minimum of ﬁve words or
collocations.Annotators can choose words freely
from the text, while collocation candidates are re-
stricted to a ﬁxed set obtained from the n-grams (n
≤ 7) in the text that also appear as article names or
surface forms in Wikipedia. Moreover, when in-
terpreting the image, the annotators are instructed
to focus on both the denotational and conotational
attributes present in the image3.

1Empirically determined to ﬁlter advertisements, banners

and undersized images.

2http://search.cpan.org/dist/HTML-ContentExtractor/
3Annotation instructions, dataset and gold standard can

649

Normal Image

Mode Image

Gold standard

czech (5), festival (5), oklahoma (4), yukon (4),
october (4), web page (2), the ﬁrst (2), event (2),
success (1), every (1), year (1)

train (5), station (4), steam (4), trans siberian (4),
steam train (4), travel (3), park (3), siberian (3),
old (3), photo (1), trans (2), yekaterinburg (2),
the web (2), photo host (1)

Table 1: Two sample images. The number besides each label indicates the number of human annotators
agreeing on that label. Note that the mode image has a tag (i.e.“train”) in the gold standard set most
frequently selected by the annotators

4 A New Evaluation Framework : Image

Tagging as Lexical Substitution

While evaluations of previous work in image an-
notation were often based on labels provided with
the images, such as tags or image captions, in our
dataset such annotations are either missing or un-
reliable. We rely instead on human-produced ex-
tractive annotations (as described in the previous
section), and formulate a new evaluation frame-
work based on the intuition that an image can be
substituted with one or more tags that convey the
same meaning as the image itself. Ideally, there is
a single tag that “best” describes the image over-
all (i.e. the gold standard tag agreed by the major-
ity of human annotators), but there are also mul-
tiple tags that describe the ﬁne-grained concepts
present in the image. Our evaluation framework
is inspired by the lexical substitution task (Mc-
Carthy and Navigli, 2007), where a system at-
tempts to generate a word (or a set of words) to
replace a target word, such that the meaning of
the sentence is preserved.

Given this analogy, the evaluation metrics used
for lexical substitution can be adapted to the eval-
uation of image tagging. Speciﬁcally, we measure
the precision and the recall of a tagging method
using four subtasks: best normal: provides preci-
sion and recall for the top-ranked tag returned by a
method; best mode: provides precision and recall
only if the top-ranked tag by a method matches the
tag in the gold standard that was most frequently
selected by the annotators; out of ten (oot) nor-

be downloaded at
http://lit.csci.unt.edu/index.php/Downloads

Formally,

mal: provides precision and recall for the top ten
tags by the system; and out of ten (oot) mode:
similar to best mode, but it considers the top ten
tags returned by the system instead of one. Table
1 show examples of a normal and a mode image.
let us assume that H is the set
of annotators, namely {h1, h2, h3, ...}, and I,
{i1, i2, i3, ...} is the set of images for which each
human annotator provide at least ﬁve tags. For
each ij, we calculate mj, which is the most fre-
quent tag for that image, if available. We also col-
j, which is the set of tags for the image
lect all rk
ij from the annotator hk.

Let the set of those images where there is a tag
agreed upon by the most annotators (i.e. the im-
ages with a mode) be denoted by IM, such that
IM ⊆ I. Also, let A ⊆ I be the set of images for
which the system provides more than one tag. Let
the corresponding set for the images with modes
be denoted by AM, such that AM ⊆ IM. Let aj ∈ A
be the set of system’s extracted tags for the image
ij.

Thus, for each image ij, we have the set of tags
extracted by the system, and the set of tags from
the human annotators. As the next step, the multi-
set union of the human tags is calculated, and the
frequencies of the unique tags is noted. Therefore,
j, and
the individual unique tag in Rj, say res, will have
a frequency associated with it, namely freqres.

for image ij, we calculate Rj, which isP rk

Given this setting, the precision (P ) and recall

(R) metrics we use are deﬁned below.

650

Best measures:

|aj|
|Rj|

P = Paj :ij∈A Pres∈aj
R = Paj :ij∈I Pres∈aj

|A|

|I|

|aj|
|Rj|

f reqres

f reqres

modeP = Pbestguessj∈AM (1if best guess = mj)
modeR = Pbestguessj∈IM (1if best guess = mj)

|AM|

|IM|

Out of ten (oot) measures:

|Rj|

P = Paj :ij∈APres∈aj
R = Paj :ij∈IPres∈aj

|A|

|I|

|Rj|

f reqres

f reqres

modeP = Paj :ij∈AM (1if any guess ∈ aj = mj)
modeR = Paj :ij∈IM (1if any guess ∈ aj = mj)

|AM|

|IM|

As a simpliﬁed example (with less tags), con-
sider ij showing a picture of a Chihuahua being
labeled by ﬁve annotators with the following tags :

Annotator

1
2
3
4
5

Tags
dog,pet
chihuahua
animal,dog
dog,chihuahua
dog

j = {dog,pet}, r2

In this case, r1
j = {chihuahua},
j = {animal,dog} and so on. The tag “dog” ap-
r3
pears the most frequent among the ﬁve annotators,
hence mj = {dog}. Rj={dog, dog, dog, dog, chi-
huahua, chihuahua, animal, pet}. The res with
associated frequencies would be dog 4, chihuahua
2, animal 1, pet 1. If the system’s proposed tag for
ij is {dog, animal}, then the numerator of P and
R for best subtask would be 4+1
8 = 0.313. Simi-
larly, the numerator of P and R for oot subtask is
8 = 0.625.

4+1

2

5 Extractive Image Annotation
The main idea underlying our work is that we can
perform effective image annotation using infor-
mation drawn from the associated text. Follow-
ing (Feng and Lapata, 2008), we propose that an
image can be annotated with keywords capturing
the denotative (entities or objects depicted) and
connotative (semantics or ideologies interpreted)
attributes in the image. For instance, a picture
showing a group of athletes and a ball may also be
tagged with words like “soccer,” or “sports activ-
ity.” Speciﬁcally, we use a combination of knowl-
edge sources to model the denotative quality of a
word as its picturability, and the connotative at-
tribute as its saliency. The idea of visualness and
salience as textual features for discovering named
entities in an image was ﬁrst pursued by (De-
schacht and Moens, 2007), using data from the
news domain.
In contrast, we are able to per-
form annotation of images from unrestricted do-
mains using content words (nouns, verbs and ad-
jectives). In the following, we ﬁrst describe three
unsupervised extractive approaches for image an-
notation, followed by a supervised method using a
re-ranking hypothesis that combines all the meth-
ods.

5.1 Flickr Picturability
Featuring a repository of four billion images,
Flickr (http://www.ﬂickr.com) is one of the most
comprehensive image resources on the web. As a
photo management and sharing application, it pro-
vides users with the ability to tag, organize, and
share their photos online. Interestingly, an inspec-
tion of Flickr tags for randomly selected images
reveal that users tend to describe the denotational
attributes of images, using concrete and picturable
words such as cat, bug, car etc. This observation
lends evidence to Flickr’s suitability as a resource
to model the picturability of words.

Given the text (T ) of an image, we can use
the getRelatedT ags API to retrieve the most fre-
quent Flickr tags associated with a given word,
and use them as corpus evidence to ﬁlter or pro-
mote words in the text.
In the ﬁltering phase
we ignore any words that return an empty list of
Flickr’s related tags, based on the assumption that
these words are not used in the Flickr tags repos-
itory. We also discard words with a length that is
less than three characters (α=3). In the promotion
phase, we reward any retrieved tags that appear as
surface forms in the text. This reward is propor-
tional to the term frequency of these tags in the

651

Algorithm 1 Flickr Picturability Algorithm
Start : L[]=φ , TF[]=tf of each word in T
for each word in T do

if length(word) ≥ α then

RelatedTags=getRelatedTags(word);
if size(RelatedT ags) > 0 then

L[word]+=β*TF[word]
for each tag in RelatedTags do

if exists T F [tag] then

L[tag]+=TF[tag]

end if
end for

end if

end if
end for

text. Additionally, we also include in the ﬁnal la-
bel set any word that returns a non-empty related
tags set with a discounted weight (β=0.5) of its
term frequency, to the end of enriching our labels
set while assuring more credit are given to the pic-
turable words.

To extract multiword labels, we locate all n-
grams formed exclusively from our extracted set
of possible labels. The subsequent score for each
of these n-grams is:

L[wi..wi+k] = (

L[wj])/k

j=i+kXj=i

By reverse sorting the associative array in L, we
can retrieve the top K words to label the image.
For illustration, let us consider the following text
snippet.

On the Origin of Species, published by
Charles Darwin in 1859, is considered
to be the foundation of evolutionary bi-
ology.

charles, darwin,

After removing stopwords, we consider the re-
For each
maining words as candidate labels.
origin, species,
of these candidates wi (i.e.
published,
f oundation,
evolutionary, and biology), we query Flickr and
obtain their related tag set Ri. origin, published,
and f oundation return an empty set of related
tags and hence are removed from our set of can-
didate labels, leaving species, charles, darwin,
evolutionary, and biology as possible annotation
keywords with the initial score of 0.5. In the pro-
motion phase, we score each wi based on the num-
ber of votes it receives from the remaining wj

(Figure 1). Each vote represents an occurrence
of the candidate tag wi in the related tag set Rj
of the candidate tag wj. For example, darwin
appeared in the Flickr related tags for charles,
evolutionary, and biology, hence it has a weight
of 3.5. The ﬁnal list of candidate labels are shown
in Table 2.

... Species, published by Charles Darwin … foundaƟon of evoluƟonary biology

Figure 1: Flickr Picturability Labels

Label
darwin
charles darwin
charles
biology
evolutionary biology
evolutionary
species

S(wi)
3.5
2.5
1.5
1.5
1.0
0.5
0.5

Table 2: Candidate labels obtained for a sample
text using the Flickr model

5.2 Wikipedia Salience
We hypothesize that an image often describes the
most important concepts in the associated text.
Thus, the keywords selected from a text could be
used as candidate labels for the image. We use
a graph-based keyword extraction method similar
to (Mihalcea and Tarau, 2004), enhanced with a
semantic similarity measure. Starting with a text,
we extract all the candidate labels and add them as
vertices in the graph. A measure of word similar-
ity is then used to draw weighted edges between
the nodes. Using the PageRank algorithm, the
words are assigned with a score indicating their
salience within the given text.

To determine the similarity between words, we
use a directed measure of similarity. Most word
similarity metrics provide a single-valued score
between a pair of words w1 and w2 to indicate
their semantic similarity. Intuitively, this is not al-
ways the case, as w1 may be represented by con-
cepts that are entirely embedded in other concepts,
represented by w2. In psycholinguistics terms, ut-
tering w1 may bring to mind w2, while the appear-
ance of w2 without any contextual clues may not
associate with w1. For example, Obama brings
to mind the concept of president, but president

652

may trigger other concepts such as W ashington,
Lincoln, F ord etc., depending on the existing
contextual clues. Thus, the degree of similarity
of w1 with respect to w2 should be separated from
that of w2 with respect to w1. Speciﬁcally, we use
the following measure of similarity, based on the
Explicit Semantic Analysis (ESA) vectors derived
from Wikipedia
(Gabrilovich and Markovitch,
2007):

DSim(wi, wj) =

Cij
Ci ∗ Sim(wi, wj)

where Cij is the count of articles in Wikipedia
containing words wi and wj, Ci is the count of ar-
ticles containing words wi, and Sim(wi, wj) is the
cosine similarity of the ESA vectors representing
the input words.The directional weight (Cij/Ci)
amounts to the degree of association of wi with re-
spect to wj. Using the directional inferential sim-
ilarity scores as directed edges and distinct words
as vertices, we obtain a graph for each text. The
directed edges denotes the idea of “recommenda-
tion” where we say w1 recommends w2 if and
only if there is a directed edge from w1 to w2, with
the weight of the recommendation being the direc-
tional similarity score. Starting with this graph,
we use the graph iteration algorithm from (Mi-
halcea and Tarau, 2004) to calculate a score for
each vertex in the graph. The output is a sorted
list of words in decreasing order of their ranks,
which are used as candidate labels to annotate the
image. This is achieved by using Cj instead of Ci
for the denominator in the directional weight. As
an example, consider the text snippet :

Microsoft Corporation is a multina-
tional computer technology corporation
that develops, manufactures, licenses,
and supports a wide range of software
products for computing devices

after stopword removal,
the list of nouns ex-
tracted is Microsoft, computer, corporation, de-
vices, products, technology, software. Note that
the top-ranked word must infer some or all of the
words in the text. In this case, the word Microsoft
infers the terms computer, technology and soft-
ware.

To calculate the semantic relatedness between
two collocations, we use a simpliﬁed version of
the text-to-text relatedness technique proposed by
and (Mihalcea et al., 2006) that incorporate the
directional inferential similarity as an underlying
semantic metric.

5.3 Topical Modeling

Intuitively, every text is written with a topic in
mind, and the associated image serves as an illus-
tration of the text meaning. In this paper, we in-
vestigate the effect of topical modeling on image
annotation accuracy directly. We use the Pachinko
Allocation Model (PAM)
(Li and McCallum,
2006) to model the topics in a text, where key-
words forming the dominant topic are assumed as
our set of annotation keywords. Compared with
previous topic modeling approaches, such as La-
tent Dirichlet allocation (LDA) or its improved
variant Correlated Topic Model (CTM) (Blei and
Lafferty, 2007), PAM captures correlations be-
tween all the topic pairs using a directed acyclic
graph (DAG). It also supports ﬁner-grained topic
modeling, and has state-of-the-art performance on
the tasks of document classiﬁcation and topical
keyword coherence. Given a text, we use the PAM
model to infer a list of super-topics and sub-topics
together with words weighted according to the
likelihood that they belong to each of these topics.
For each text, we retrieve the top words belong-
ing to the dominant super-topic and sub-topic. We
use 50 super-topics and 100 sub-topics as operat-
ing parameters for PAM, since these values were
found to provide good results in previous work on
topic modeling. Default values are used for other
parameters in the model.

5.4 Supervised Learning

The three tagging methods target different aspects
of what constitutes a good label for an image. We
use them as features in a machine learning frame-
work, and introduce a ﬁnal rank attribute S(tj),
which is a linear combination of the reciprocals of
the rank of each tag as given by each method,

S(tj) = Xm∈methods

λm

1
rm
tj

where rm
is the rank for tag tj given by method
tj
m. The weight of each method λm is estimated
from the training set using information gain val-
ues. Since our predicted variable (mode precision
or recall) is continuous, we use the Support Vec-
tor Algorithm (nu-SVR) implementation of SVM
(Chang and Lin, 2001) to perform regression anal-
ysis on the weights for each method via a radial
basis function kernel. A ten-fold cross-validation
is applied on the entire dataset of 300 images.

653

Models
Flickr picturability
Wikipedia Salience
Topic modeling
Combined (SVM)
Doc Title
tf * idf
Random
Upper bound (human)

Best

Normal
P
6.32
6.40
5.99
6.87
6.40
5.94
3.76
12.23

R
6.32
6.40
5.99
6.87
6.40
5.94
3.76
12.07

Mode

P
78.57
7.14
42.86
67.49
75.00
14.29
3.57
81.48

R
78.57
7.14
42.86
67.49
75.00
14.29
3.57
81.48

out-of-ten (oot)

Normal
P
35.61
35.19
37.13
37.85
18.97
38.40
30.20
82.44

R
35.61
35.19
37.13
37.85
18.97
38.40
30.20
81.55

Mode

R
92.86
92.86
85.71
100.00
82.14
78.57
50.00
100.00

P
92.86
92.86
85.71
100.00
82.14
78.57
50.00
100.00

Table 3: Results obtained on the Web dataset

6 Experiments and Evaluations
We evaluate the performance of each of the three
tagging methods separately, followed by an eval-
uation of the combined method. Each system pro-
duces a ranked list of K words or collocations
as tags assigned to a given image. A system can
discretionary generate less (but not more) than K
tags, depending on its conﬁdence level.

For comparison, we implement three baselines:
tf*idf, Doc Title and Random. For tf*idf, we use
the British National Corpus to calculate the idf
scores, while the frequency of a term is calcu-
lated from the entire text associated with an im-
age. The Doc Title baseline is similar, except that
the term frequency is calculated based on the title
of the document. The Random baseline randomly
selects words from a co-occurrence window of
size K before and after an image as its annota-
tion. Following other tagging methods, we apply a
pre-processing stage, where we part-of-speech tag
the text (to retain only nouns), followed by stem-
ming. We also determine an upper bound, which
is calculated as follows. For each image, the la-
bels assigned by each of the ﬁve annotators are
in turn evaluated against a gold standard consist-
ing of the annotations of the other four annotators.
The best performing annotator is then recorded.
This process is repeated for each of the 300 im-
ages, and the average precision and recall are cal-
culated. This represents an upper bound, as it is
the best performance that a human can achieve on
this dataset. Table 3 shows our experimental re-
sults.

Among the individual methods, the method im-
plementing Flickr picturability has the highest in-
dividual score for best and oot modes, yielding
a precision and recall of 78.57% and 92.86% re-
spectively. The Wikipedia Saliency method also
scores the highest (jointly with Flickr) in the oot
mode, but for the best mode achieves a score only
marginally better than the random baseline. A
plausible explanation is that it tends to favor “all-

inferring” over-speciﬁc labels, while the most fre-
quently selected tags in mode pictures are typi-
cally more “picturable” than being speciﬁc (e.g.
“train” for the mode picture in Table 1). The topic
modeling method has mixed results:
its scores
for oot normal and mode are somewhat compet-
itive with tf*idf, but it scores consistently lower
than the DocTitle in the best subtask, possibly
due to the absence of a more sophisticated re-
ranking algorithm tailored for the image annota-
tion task other than the intrinsic ranking mecha-
nism in PAM. It is worth noting that the combined
supervised system provides the overall best results
(6.87%) on the best normal, and achieves a perfect
precision and recall (100%) for oot mode, which
means perfect agreement with the human tagging.

7 Comparison with Related Work

We also compare our work against (Feng and Lap-
ata, 2008) as it allows for a direct comparison with
models using both image and textual features un-
der a standard evaluation framework. We obtained
the BBC dataset used in their experiments, which
consists of 3121 training and 240 testing images.
In this dataset, images are implicitly tagged with
captions by the author of the corresponding BBC
article. The evaluations are run against these cap-
tions.

In their experiments, Feng and Lapata created
four annotation models. The ﬁrst two (tf*idf and
Document Title) are the same as used in our base-
line experiments. The third model (Lavrenko03)
is an application of the continuous relevance
model in (Jeon et al., 2003), trained with the BBC
image features and captions. Finally, the forth
(ExtModel) is an extension of the relevance model
using additional information in auxiliary texts.
Brieﬂy, the model assumes a multiple Bernoulli
distribution for words in a caption, and generates
tags for a test image using a weighted combina-
tion of the accompanying document, caption and
image features learned during training.

654

Models
tf*idf
DocTitle
Lavrenko03
ExtModel
Flickr picturability
Wikipedia Salience
Topic Modeling
Combined (SVM)

P

4.37
9.22
9.05
14.72
12.13
11.63
11.42
13.38

Top 10

R
7.09
7.03
16.01
27.95
22.82
21.89
21.49
25.17

F1
5.41
7.20
11.81
19.82
15.84
15.18
14.91
17.47

P

3.57
9.22
7.73
11.62
9.52
9.28
9.28
11.08

Top 15

R
8.12
7.03
17.87
32.99
26.82
26.20
26.20
31.29

F1
4.86
7.20
10.71
17.18
14.05
13.70
13.70
16.37

P

2.65
9.22
6.55
9.72
8.23
7.81
7.86
9.50

Top 20

R
8.89
7.03
19.38
36.77
29.80
29.41
29.57
35.76

F1
4.00
7.20
9.79
15.39
12.90
12.35
12.42
15.01

Table 4: Results obtained on the BBC dataset used in (Feng and Lapata, 2008)

The experimental setup is similar to the earlier
section, but a few modiﬁcations are made for a fair
and direct comparison. First, we extend our mod-
els coverage to include content words (i.e. nouns,
verbs, adjectives) determined using the Tree Tag-
ger (Schmid, 1994). Second, no collocations are
used. Third, we adopt the evaluation framework
used by Feng and Lapata to extract the top 10, 15
and 20 tags. Note that in our methods, the extrac-
tion of tags for a test image is only done on the
document surrounding the image, after excluding
the caption. As the number of negative examples
(words not present in the caption) greatly outnum-
ber the positive instances, we employ an under-
sampling method (Kubat and Matwin, 1997) to
balance the dataset for training.

The results are shown in Table 4.

Interest-
ingly, all our unsupervised extraction-based mod-
els perform consistently above the supervised
Lavrenko03 model,
indicating that textual fea-
tures are more informative than captions and im-
age features taken together. Comparing with mod-
els using signiﬁcantly less document informa-
tion (tf*idf and Doc title), our models gain even
greater advantage. Note that the title of any BBC
article does not exceed 10 words, hence compar-
ison is only meaningful given the top 10 tags re-
trieved.

Feng and Lapata used LDA to perform rerank-
ing of ﬁnal candidates in their ExtModel. How-
ever, when used as a model alone, the PAM topic
model achieved promising scores in all the cate-
gories, performing best for top 10 keywords (F1
of 14.91%). Flickr picturability stands out as
the best performing unsupervised method, scor-
ing the highest precision (12.13%, top 10), recall
(29.80%, top 20) and F1 (15.84%, top 10).

Overall,

this comparative evaluation yields
some important insights. First, our combined
model using SVM is statistically better (p<0.1 for
top 10, 15, 20) than the Laverenko03 model, but
not statistically different from the ExtModel. This
demonstrates the effectiveness of textual-based

models over traditional models trained with im-
age features and captions. While it is intuitively
clear that image features help in improving tag-
ging performance, we show that mining only the
text surrounding an image, where it exists, can
yield a performance that is comparable to a state-
of-the-art system that uses both textual and vi-
sual features. Moreover, an increase in complex-
ity of a model by using more features may hinder
its applicability to large datasets, but not neces-
sarily improving annotation performance (Maka-
dia et al., 2008). On this, text-based annotation
models can provide a desirable compromise. For
instance, our unsupervised models implementing
Flickr picturability and Wikipedia Salience are
able to extract annotations from a BBC article (av-
erage 133.85 tokens) in approximately 1 second
and 20 seconds respectively.

8 Conclusions and Future Work

In this paper, we introduced several text-based ex-
tractive approaches for automatic image annota-
tion and showed that they compare favorably with
the state-of-the-art in image annotation using both
text and image features. We believe our work
has practical applications in mining and annotat-
ing images over the Web, where texts are nat-
urally associated with images, and scalability is
important. Our next direction seeks to derive ro-
bust annotation models using additional ontolog-
ical knowledge-bases. We would also like to ad-
vance the the state-of-the-art by augmenting cur-
rent textual models with image features.

Acknowledgments

This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340. Any opinions, ﬁndings,
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reﬂect the views of the National Sci-
ence Foundation.

655

References
Kobus Barnard and David Forsyth. 2001. Learning the
semantics of words and pictures. In Proceedings of
International Conference on Computer Vision.

David Blei and John Lafferty. 2007. A correlated topic
In Annals of Applied Statistics,

model of science.
volume 1, pages 17–35.

Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:

a library for support vector machines.

Brendan Collins, Jia Deng, Kai Li, and Li Fei-Fei.
2008. Towards scalable dataset construction: An
active learning approach. In Proceedings of Euro-
pean Conference on Computer Vision.

Koen Deschacht and Marie-Francine Moens. 2007.
Text analysis for automatic image annotation.
In
Proceedings of the Association for Computational
Linguistics.

Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David Forsyth. 2002. Object recognition as ma-
chine translation:learning a lexicon for a ﬁxed im-
age vocabulary. In Proceedings of the 7th European
Conference on Computer Vision.

Yansong Feng and Mirella Lapata. 2008. Automatic
image annotation using auxiliary text information.
In Proceedings of the Association for Computa-
tional Linguistics.

Rob Fergus, Pietro Perona, and Andrew Zisserman.
2003. Object class recognition by unsupervised
scale-invariant learning. In Proceedings of the In-
ternational Conference on Computer Vision and
Pattern Recognition.

Evgeniy Gabrilovich and Shaul Markovitch.

2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis.
In International
Joint Conferences on Artiﬁcial Intelligence.

Michael Grubinger, Clough Paul, Mller Henning, and
Deselaers Thomas. 2006. The iapr benchmark: A
new evaluation resource for visual information sys-
tems. In International Conference on Language Re-
sources and Evaluation.

Jiwoon Jeon, Victor Lavrenko, and R Manmatha.
2003. Automatic image annotation and retrieval us-
ing cross-media relevance models. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval.

Yohan Jin, Latifur Khan, Lei Wang, and Mamoun
Awad. 2005. Image annotations by combining mul-
tiple evidence & wordnet. In Proceedings of Annual
ACM Multimedia.

Miroslav Kubat and Stan Matwin. 1997. Addressing
the curse of imbalanced training sets: one-sided se-
lection. In Proceedings of International Conference
on Machine Learning.

Li-Jia Li and Li Fei-Fei.

au-
tomatic online picture collection via incremental
model learning.
In International Journal of Com-
puter Vision.

2008. Optimol:

Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In Proceedings of the International Con-
ference on Machine learning.

Jia Li and James Wang. 2008. Real-time computer-
ized annotation of pictures. In Proceedings of Inter-
national Conference on Computer Vision.

Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-
mar. 2008. A new baseline for image annotation. In
Proceedings of European Conference on Computer
Vision.

Diana McCarthy and Roberto Navigli. 2007. The se-
meval English lexical substitution task. In Proceed-
ings of the ACL Semeval workshop.

Rada Mihalcea and Chee Wee Leong.

2009. To-
wards communicating simple sentences using pic-
torial representations. In Machine Translation, vol-
ume 22, pages 153–173.

Rada Mihalcea and Paul Tarau.

Bringing order into texts.
pirical Methods in Natural Language Processing.

2004. Textrank:
In Proceedings of Em-

Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity.
In Proceed-
ings of Association for the Advancement of Artiﬁcial
Intelligence, pages 775–780.

Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of Empirical Methods
in Natural Language Processing.

Srihari and Burhans. 1994. Visual semantics: Extract-
ing visual information from text accompanying pic-
tures. In Proceedings of the American Association
for Artiﬁcial Intelligence.

Munirathnam Srikanth, Joshua Varner, Mitchell Bow-
den, and Dan Moldovan. 2005. Exploiting ontolo-
gies for automatic image annotation.
In Proceed-
ings of the ACM Special Interest Group on Research
and Development in Information Retrieval.

Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
ACM Special Interest Group on Computer Human
Interaction.

Chong Wang, David Blei, and Li Fei-Fei. 2009. Si-
multaneous image classiﬁcation and annotation. In
Proceedings of IEEE Conference on Computer Vi-
sion and Pattern Recognition.

