



















































Pooled Contextualized Embeddings for Named Entity Recognition


Proceedings of NAACL-HLT 2019, pages 724–728
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

724

Pooled Contextualized Embeddings for Named Entity Recognition

Alan Akbik
Zalando Research
Mühlenstraße 25

10243 Berlin

Tanja Bergmann
Zalando Research
Mühlenstraße 25

10243 Berlin
{firstname.lastname}@zalando.de

Roland Vollgraf
Zalando Research
Mühlenstraße 25

10243 Berlin

Abstract

Contextual string embeddings are a recent type
of contextualized word embedding that were
shown to yield state-of-the-art results when
utilized in a range of sequence labeling tasks.
They are based on character-level language
models which treat text as distributions over
characters and are capable of generating em-
beddings for any string of characters within
any textual context. However, such purely
character-based approaches struggle to pro-
duce meaningful embeddings if a rare string
is used in a underspecified context. To ad-
dress this drawback, we propose a method in
which we dynamically aggregate contextual-
ized embeddings of each unique string that
we encounter. We then use a pooling oper-
ation to distill a global word representation
from all contextualized instances. We eval-
uate these pooled contextualized embeddings
on common named entity recognition (NER)
tasks such as CoNLL-03 and WNUT and show
that our approach significantly improves the
state-of-the-art for NER. We make all code and
pre-trained models available to the research
community for use and reproduction.

1 Introduction

Word embeddings are a crucial component in
many NLP approaches (Mikolov et al., 2013; Pen-
nington et al., 2014) since they capture latent se-
mantics of words and thus allow models to bet-
ter train and generalize. Recent work has moved
away from the original “one word, one embed-
ding” paradigm to investigate contextualized em-
bedding models (Peters et al., 2017, 2018; Akbik
et al., 2018). Such approaches produce different
embeddings for the same word depending on its
context and are thus capable of capturing latent
contextualized semantics of ambiguous words.

Recently, Akbik et al. (2018) proposed a
character-level contextualized embeddings ap-

Fung
B-PER

Permadi
E-PER

( Taiwan
S-LOC

) v Indra
S-ORG

Figure 1: Example sentence that provides underspecified
context. This leads to an underspecified contextual word em-
bedding for the string “Indra” that ultimately causes a mis-
classification of “Indra” as an organization (ORG) instead of
person (PER) in a downstream NER task.

proach they refer to as contextual string embed-
dings. They leverage pre-trained character-level
language models from which they extract hidden
states at the beginning and end character positions
of each word to produce embeddings for any string
of characters in a sentential context. They showed
these embeddings to yield state-of-the-art results
when utilized in sequence labeling tasks such as
named entity recognition (NER) or part-of-speech
(PoS) tagging.
Underspecified contexts. However, such contex-
tualized character-level models suffer from an in-
herent weakness when encountering rare words in
an underspecified context. Consider the example
text segment shown in Figure 1: “Fung Permadi
(Taiwan) v Indra”, from the English CONLL-03
test data split (Tjong Kim Sang and De Meulder,
2003). If we consider the word “Indra” to be rare
(meaning no prior occurrence in the corpus used
to generate word embeddings), the underspecified
context allows this word to be interpreted as either
a person or an organization. This leads to an un-
derspecified embedding that ultimately causes an
incorrect classification of “Indra” as an organiza-
tion in a downstream NER task.
Pooled Contextual Embeddings. In this paper,
we present a simple but effective approach to ad-
dress this issue. We intuit that entities are nor-
mally only used in underspecified contexts if they
are expected to be known to the reader. That is,
they are either more clearly introduced in an ear-
lier sentence, or part of general in-domain knowl-



725

2

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

NAACL-HLT 2019 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

figure2-crop.pdf

Figure 2: PLACEHOLDER Illustration of character-level RNN language model.

edge a reader is expected to have. Indeed, the
string “Indra” in the CONLL-03 data also occurs
in the earlier sentence “Indra Wijaya (Indonesia)
beat Ong Ewe Hock”. Based on this, we propose
an approach in which we dynamically aggregate
contextualized embeddings of each unique string
that we encounter as we process a dataset. We then
use a pooling operation to distill a global word rep-
resentation from all contextualized instances that
we use in combination with the current contextu-
alized representation as new word embedding.

We evaluate our proposed embedding approach
on the task of named entity recognition on the
CONLL-03 (English, German and Dutch) and
WNUT datasets. In all cases, we find that our
approach outperforms previous approaches and
yields new state-of-the-art scores. We contribute
our approach and all pre-trained models to the
open source FLAIR1 framework, to ensure repro-
ducibility of these results.

2 Method

Our proposed approach dynamically builds up a
“memory” of contextualized embeddings and ap-
plies a pooling operation to distill a global con-
textualized embedding for each word. It requires
an embed() function that produces a contextual-
ized embedding for a given word in a sentence
context (see Akbik et al. (2018)). It also requires
a memory that records for each unique word all
previous contextual embeddings, and a pool() op-
eration to pool embedding vectors.

This is illustrated in Algorithm 1: to embed a
word (in a sentential context), we first call the
embed() function (line 2) and add the resulting

1https://github.com/zalandoresearch/flair

embedding to the memory for this word (line 3).
We then call the pooling operation over all contex-
tualized embeddings for this word in the memory
(line 4) to compute the pooled contextualized em-
bedding. Finally, we concatenate the original con-
textual embedding together with the pooled repre-
sentation, to ensure that both local and global in-
terpretations are represented (line 5). This means
that the resulting pooled contextualized embed-
ding has twice the dimensionality of the original
embedding.

Algorithm 1 Compute pooled embedding
Input: sentence, memory

1: for word in sentence do
2: embcontext ←

embed(word) within sentence
3: add embcontext to memory[word]
4: embpooled ← pool(memory[word])
5: word.embedding ←

concat(embpooled, embcontext)
6: end for

Crucially, our approach expands the memory
each time we embed a word. Therefore, the same
word in the same context may have different em-
beddings over time as the memory is built up.
Pooling operations. Per default, we use mean
pooling to average a word’s contextualized em-
bedding vectors. We also experiment with min
and max pooling to compute a vector consisting
of all element-wise minimum or maximum values.
Training downstream models. When training
downstream task models (such as for NER), we
typically make many passes over the training data.
As Algorithm 2 shows, we reset the memory at the
beginning of each pass over the training data (line

embcontext Indra2

I n d r a  W i j a y a  b e a t  O n g  E w e

embcontext Indra3

A n d  I n d r a  s a i d  t h a t . . .

memory

Indraembproposed

F u n g  P e r m a d i  v  I n d r a

Character Language Model

embcontext Indra1

pooling
concatenation

current sentence

Figure 2: Example of how we generate our proposed embedding (embproposed) for the word “Indra” in the example text
segment “Fung Permadi v Indra”. We extract a contextual string embedding (embcontext) for this word and retrieve from
the memory all embeddings that were produced for this string on previous sentences. We pool and concatenate all local
contextualized embeddings to produce the final embedding.

edge a reader is expected to have. Indeed, the
string “Indra” in the CONLL-03 data also occurs
in the earlier sentence “Indra Wijaya (Indonesia)
beat Ong Ewe Hock”.

Based on this, we propose an approach in which
we dynamically aggregate contextualized embed-
dings of each unique string that we encounter as
we process a dataset. We then use a pooling opera-
tion to distill a global word representation from all
contextualized instances that we use in combina-
tion with the current contextualized representation
as new word embedding. Our approach thus pro-
duces evolving word representations that change
over time as more instances of the same word are
observed in the data.

We evaluate our proposed embedding approach
on the task of named entity recognition on the
CONLL-03 (English, German and Dutch) and
WNUT datasets. In all cases, we find that our
approach outperforms previous approaches and
yields new state-of-the-art scores. We contribute
our approach and all pre-trained models to the
open source FLAIR1 framework (Akbik et al.,
2019), to ensure reproducibility of these results.

2 Method

Our proposed approach (see Figure 2) dynami-
cally builds up a “memory” of contextualized em-
beddings and applies a pooling operation to distill
a global contextualized embedding for each word.
It requires an embed() function that produces a
contextualized embedding for a given word in a

1https://github.com/zalandoresearch/flair

sentence context (see Akbik et al. (2018)). It also
requires a memory that records for each unique
word all previous contextual embeddings, and a
pool() operation to pool embedding vectors.

This is illustrated in Algorithm 1: to embed a
word (in a sentential context), we first call the
embed() function (line 2) and add the resulting
embedding to the memory for this word (line 3).
We then call the pooling operation over all contex-
tualized embeddings for this word in the memory
(line 4) to compute the pooled contextualized em-
bedding. Finally, we concatenate the original con-
textual embedding together with the pooled repre-
sentation, to ensure that both local and global in-
terpretations are represented (line 5). This means
that the resulting pooled contextualized embed-
ding has twice the dimensionality of the original
embedding.

Algorithm 1 Compute pooled embedding
Input: sentence, memory

1: for word in sentence do
2: embcontext ←

embed(word) within sentence
3: add embcontext to memory[word]
4: embpooled← pool(memory[word])
5: word.embedding←

concat(embpooled, embcontext)
6: end for

Pooling operations. We experiment with differ-
ent pooling operations: mean pooling to average
a word’s contextualized embedding vectors, and
min and max pooling to compute a vector of all



726

Approach CONLL-03 EN CONLL-03 DE CONLL-03 NL WNUT-17

Pooled Contextualized Embeddingsmin 93.18 ± 0.09 88.27 ± 0.30 90.12 ± 0.14 49.07 ± 0.31
Pooled Contextualized Embeddingsmax 93.13 ± 0.09 88.05 ± 0.25 90.26 ± 0.10 49.05 ± 0.26
Pooled Contextualized Embeddingsmean 93.10 ± 0.11 87.69 ± 0.27 90.44 ± 0.20 49.59 ± 0.41
Contextual String Emb. (Akbik et al., 2018) 92.86 ± 0.08 87.41 ± 0.13 90.16 ± 0.26 49.49 ± 0.75

best published
BERT (Devlin et al., 2018)† 92.8
CVT+Multitask (Clark et al., 2018)† 92.6
ELMo (Peters et al., 2018)† 92.22
Stacked Multitask (Aguilar et al., 2018)† 45.55
Character-LSTM (Lample et al., 2016)† 90.94 78.76 81.74

Table 1: Comparative evaluation of proposed approach with different pooling operations (min, max, mean) against current
state-of-the-art approaches on four named entity recognition tasks († indicates reported numbers). The numbers indicate that
our approach outperforms all other approaches on the CoNLL datasets, and matches baseline results on WNUT.

element-wise minimum or maximum values.
Training downstream models. When training
downstream task models (such as for NER), we
typically make many passes over the training data.
As Algorithm 2 shows, we reset the memory at the
beginning of each pass over the training data (line
2), so that it is build up from scratch at each epoch.

Algorithm 2 Training
1: for epoch in epochs do
2: memory← map of word to list
3: train and evaluate as usual
4: end for

This approach ensures that the downstream task
model learns to leverage pooled embeddings that
are built up (e.g. evolve) over time. It also ensures
that pooled embeddings during training are only
computed over training data. After training, (i.e.
during NER prediction), we do not reset embed-
dings and instead allow our approach to keep ex-
panding the memory and evolve the embeddings.

3 Experiments

We verify our proposed approach in four named
entity recognition (NER) tasks: We use the En-
glish, German and Dutch evaluation setups of the
CONLL-03 shared task (Tjong Kim Sang and
De Meulder, 2003) to evaluate our approach on
classic newswire data, and the WNUT-17 task on
emerging entity detection (Derczynski et al., 2017)
to evaluate our approach in a noisy user-generated
data setting with few repeated entity mentions.

3.1 Experimental Setup
We use the open source FLAIR framework in
all our experiments. It implements the stan-
dard BiLSTM-CRF sequence labeling architec-
ture (Huang et al., 2015) and includes pre-trained

contextual string embeddings for many languages.
To FLAIR, we add an implementation of our pro-
posed pooled contextualized embeddings.
Hyperparameters. For our experiments, we fol-
low the training and evaluation procedure outlined
in Akbik et al. (2018) and follow most hyperpa-
rameter suggestions as given by the in-depth study
presented in Reimers and Gurevych (2017). That
is, we use an LSTM with 256 hidden states and
one layer (Hochreiter and Schmidhuber, 1997), a
locked dropout value of 0.5, a word dropout of
0.05, and train using SGD with an annealing rate
of 0.5 and a patience of 3. We perform model se-
lection over the learning rate ∈ {0.01, 0.05, 0.1}
and mini-batch size ∈ {8, 16, 32}, choosing the
model with the best F-measure on the validation
set. Following Peters et al. (2017), we then re-
peat the experiment 5 times with different random
seeds, and train using both train and development
set, reporting both average performance and stan-
dard deviation over these runs on the test set as
final performance.
Standard word embeddings. The default setup
of Akbik et al. (2018) recommends contextual
string embeddings to be used in combination with
standard word embeddings. We use GLOVE em-
beddings (Pennington et al., 2014) for the English
tasks and FASTTEXT embeddings (Bojanowski
et al., 2017) for all newswire tasks.
Baselines. Our baseline are contextual string em-
beddings without pooling, i.e. the original setup
proposed in Akbik et al. (2018)2. By compar-
ing against this baseline, we isolate the impact of
our proposed pooled contextualized embeddings.

2Our reproduced numbers are slightly lower than we re-
ported in Akbik et al. (2018) where we used the official
CONLL-03 evaluation script over BILOES tagged entities.
This introduced errors since this script was not designed for
S-tagged entities.



727

Approach CONLL-03 EN CONLL-03 DE CONLL-03 NL WNUT-17

Pooled Contextualized Embeddings (only) 92.42 ± 0.07 86.21 ± 0.07 88.25 ± 0.11 44.29 ± 0.59
Contextual String Embeddings (only) 91.81 ± 0.12 85.25 ± 0.21 86.71 ± 0.12 43.43 ± 0.93

Table 2: Ablation experiment using contextual string embeddings without word embeddings. We find a more significant
impact on evaluation numbers across all datasets, illustrating the need for capturing global next to contextualized semantics.

In addition, we list the best reported numbers for
the four tasks. This includes the recent BERT ap-
proach using bidirectional transformers by Devlin
et al. (2018), the semi-supervised multitask learn-
ing approach by Clark et al. (2018), the ELMo
word-level language modeling approach by Peters
et al. (2018), and the best published numbers for
WNUT-17 (Aguilar et al., 2018) and German and
Dutch CONLL-03 (Lample et al., 2016).

3.2 Results

Our experimental results are summarized in Ta-
ble 1 for each of the four tasks.
New state-of-the-art scores. We find that our ap-
proach outperforms all previously published re-
sults, raising the state-of-the-art for CONLL-03
on English to 93.18 F1-score (↑0.32 pp vs. previ-
ous best), German to 88.27 (↑0.86 pp) and Dutch
to 90.44 (↑0.28 pp). The consistent improvements
against the contextual string embeddings baseline
indicate that our approach is generally a viable op-
tion for embedding entities in sequence labeling.
Less pronounced impact on WNUT-17. How-
ever, we also find no significant improvements on
the WNUT-17 task on emerging entities. Depend-
ing on the pooling operation, we find compara-
ble results to the baseline. This result is expected
since most entities appear only few times in this
dataset, giving our approach little evidence to ag-
gregate and pool. Nevertheless, since recent work
has not yet experimented with contextual embed-
dings on WNUT, as side result we report a new
state-of-the-art of 49.59 F1 vs. the previous best
reported number of 45.55 (Aguilar et al., 2018).
Pooling operations. Comparing the pooling op-
erations discussed in Section 2, we generally find
similar results. As Table 1 shows, min pooling
performs best for English and German CoNLL,
while mean pooling is best for Dutch and WNUT.

3.3 Ablation: Character Embeddings Only

To better isolate the impact of our proposed ap-
proach, we run experiments in which we do not
use any classic word embeddings, but rather rely
solely on contextual string embeddings. As Ta-
ble 2 shows, we observe more pronounced im-

provements of pooling vis-a-vis the baseline ap-
proach in this setup. This indicates that pooled
contextualized embeddings capture global seman-
tics words similar in nature to classical word em-
beddings.

4 Discussion and Conclusion

We presented a simple but effective approach that
addresses the problem of embedding rare strings in
underspecified contexts. Our experimental evalu-
ation shows that this approach improves the state-
of-the-art across named entity recognition tasks,
enabling us to report new state-of-the-art scores
for CONLL-03 NER and WNUT emerging entity
detection. These results indicate that our embed-
ding approach is well suited for NER.
Evolving embeddings. Our dynamic aggrega-
tion approach means that embeddings for the same
words will change over time, even when used in
exactly the same contexts. Assuming that entity
names are more often used in well-specified con-
texts, their pooled embeddings will improve as
more data is processed. The embedding model
thus continues to “learn” from data even after the
training of the downstream NER model is com-
plete and it is used in prediction mode. We con-
sider this idea of constantly evolving representa-
tions a very promising research direction.
Future work. Our pooling operation makes
the conceptual simplification that all previous in-
stances of a word are equally important. However,
we may find more recent mentions of a word - such
as words within the same document or news cycle
- to be more important for creating embeddings
than mentions that belong to other documents or
news cycles. Future work will therefore examine
methods to learn weighted poolings of previous
mentions. We will also investigate applicability of
our proposed embeddings to tasks beside NER.
Public release. We contribute our code to the
FLAIR framework3. This allows full reproduction
of all experiments presented in this paper, and al-

3The proposed embedding is added to FLAIR in release
0.4.1. as the PooledFlairEmbeddings class (see Akbik
et al. (2019) for more details).



728

lows the research community to use our embed-
dings for training downstream task models.

Acknowledgements

We would like to thank the anonymous reviewers for their

helpful comments. This project has received funding from the

European Union’s Horizon 2020 research and innovation pro-

gramme under grant agreement no 732328 (“FashionBrain”).

References
Gustavo Aguilar, Adrian Pastor Lopez Monroy, Fabio

González, and Thamar Solorio. 2018. Modeling
noisiness to recognize named entities using multi-
task neural networks on social media. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), volume 1, pages 1401–1412.

Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif
Rasul, Stefan Schweter, and Roland Vollgraf. 2019.
Flair: An easy-to-use framework for state-of-the-
art nlp. In NAACL, 2019 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: System Demonstrations.

Alan Akbik, Duncan Blythe, and Roland Vollgraf.
2018. Contextual string embeddings for sequence
labeling. In COLING 2018, 27th International Con-
ference on Computational Linguistics, pages 1638–
1649.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Kevin Clark, Minh-Thang Luong, Christopher D. Man-
ning, and Quoc V. Le. 2018. Semi-supervised
sequence modeling with cross-view training. In
EMNLP.

Leon Derczynski, Eric Nichols, Marieke van Erp, and
Nut Limsopatham. 2017. Results of the wnut2017
shared task on novel and emerging entity recogni-
tion. In Proceedings of the 3rd Workshop on Noisy
User-generated Text, pages 140–147.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270. Association for Computational Lin-
guistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language mod-
els. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1756–1765, Vancouver,
Canada. Association for Computational Linguistics.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proc. of NAACL.

Nils Reimers and Iryna Gurevych. 2017. Reporting
Score Distributions Makes a Difference: Perfor-
mance Study of LSTM-networks for Sequence Tag-
ging. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 338–348, Copenhagen, Denmark.

Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142–147. Association for Computational Lin-
guistics.

https://doi.org/10.18653/v1/N16-1030

