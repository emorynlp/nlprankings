418

Coling 2010: Poster Volume, pages 418–426,

Beijing, August 2010

Word Sense Disambiguation-based Sentence Similarity 

Chukfong Ho1, Masrah Azrifah 

Rabiah Abdul Kadir, Shyamala 

Azmi Murad2 

Department of Information System 

University Putra Malaysia 

hochukfong@yahoo.com1, 

masrah@fsktm.upm.edu.my2 

C. Doraisamy 

Department of Multimedia 
University Putra Malaysia 

{rabiah, shya-

mala}@fsktm.upm.edu.my 

 

Abstract 

the  comparison  of 

Previous  works  tend  to  compute  the 
similarity  between  two  sentences  based 
their  nearest 
on 
meanings.  However, 
the 
nearest 
meanings  do  not  always  represent  their 
actual  meanings.  This  paper  presents  a 
method  which  computes  the  similarity 
between  two  sentences  based  on  a  com-
parison  of  their  actual  meanings.  This  is 
achieved  by  transforming  an  existing 
most-outstanding  corpus-based  measure 
into  a  knowledge-based  measure,  which 
is  then  integrated  with  word  sense  dis-
ambiguation.  The  experimental  results 
on a standard data set show that the pro-
posed  method  outperforms  the  baseline 
and the improvement achieved is statisti-
cally significant at 0.025 levels. 

1 

Introduction 

Although  measuring  sentence  similarity  is  a 
complicated  task,  it  plays  an  important  role  in 
natural language processing applications. In text 
categorization  (Yang  and  Wen,  2007),  docu-
ments  are  retrieved  based  on  similar  or  related 
features.  In  text  summarization  (Zhou  et  al., 
2006)  and  machine  translation  (Kauchak  and 
Barzilay,  2006),  summaries  comparison  based 
on  sentence  similarity  has  been  applied  for 
automatic  evaluation.  In  text  coherence  (Lapata 
and  Barzilay,  2005),  different  sentences  are 
linked together based on the sequence of similar 
or related words.  

Two main issues are investigated in this paper: 

1) the performance between corpus-based  meas-
ure  and  knowledge-based  measure,  and  2)  the 

influence  of  word  sense  disambiguation  (WSD) 
on  measuring  sentence  similarity.  WSD  is  the 
task  of  determining  the  sense  of  a  polysemous 
word  within  a  specific  context  (Wang  et  al., 
2006). Corpus-based  methods typically compute 
sentence  similarity  based  on  the  frequency  of  a 
word’s occurrence or the co-occurrence between 
collocated words. Although these methods bene-
fit  from  the  statistical  information  derived  from 
the corpus, this statistical information is closer to 
syntactic  representation  than  to  semantic  repre-
sentation. 
In  comparison,  knowledge-based 
methods  compute  the  similarity  between  two 
sentences  based  on  the  semantic  information 
collected  from  knowledge  bases.  However,  this 
semantic  information  is  applied  in  a  way  that, 
for  any  two  sentences,  the  comparison  of  their 
nearest  meanings  is  taken  into  consideration  in-
stead of the comparison of their actual meanings. 
More  importantly,  the  nearest  meaning  does  not 
always  represent  the  actual  meaning.  In  this  pa-
per,  a  solution  is  proposed  that  seeks  to  address 
these  two  issues.  Firstly,  the  most  outstanding 
existing  corpus-based  sentence  similarity  meas-
ure  is  transformed  into  a  knowledge-based 
measure.  Then,  its  underlying  concept,  which  is 
the  comparison  of  the  nearest  meanings,  is  re-
placed  by  another  underlying  concept,  the  com-
parison of the actual meanings.         

The  rest  of  this  paper  is  organized  into  five 
sections.  Section  2  presents  an  overview  of  the 
related  works.  Section  3  details  the  problem  of 
the  existing  method and the  improvement  of the 
proposed  method.  Section  4  describes  the  ex-
perimental design. In Section 5, the experimental 
results  are  discussed.  Finally,  the  implications 
and contributions are addressed in Section 6.   

419

2  Related Work 

In general, related works can be categorized into 
corpus-based,  knowledge-based  and  hybrid-
based  methods.  Islam  and  Inkpen  (2008)  pro-
posed  a  corpus-based  sentence  similarity  meas-
ure as a function  of string similarity, word simi-
larity and common word order similarity (CWO). 
They  claimed  that  a  corpus-based  measure  has 
the advantage of large coverage  when compared 
to  a  knowledge-based  measure.  However,  the 
judgment  of  similarity  is  situational  and  time 
dependent (Feng et al., 2008). This suggests that 
the statistical information collected from the past 
corpus  may  not  be  relevant  to  sentences  present 
in the current corpus. Apart from that, the role of 
string  similarity  is  to  identify  any  misspelled 
word. A malfunction may occur whenever string 
similarity  deals  with  any  error-free  sentences 
because the purpose for its existence is no longer 
valid.   

For knowledge-based methods, Li et al. (2009) 
adopted  an  existing  word  similarity  measure  to 
deal  with  the  similarities  of  verbs  and  nouns 
while  the  similarities  of  adjectives  and  adverbs 
were measured only based on simple word over-
laps.  However,  Achananuparp  et  al.  (2008)  pre-
viously  showed  that  the  word  overlap-based 
method performed badly in measuring text simi-
larity.  Liu  et  al.  (2007)  integrated  the  Dynamic 
Time  Warping  (DTW)  technique  into  the  simi-
larity  measure  to  identify  the  distance  between 
words.  The  main  drawback  of  DTW  is  that  the 
computational  cost  and  time  will  increase  pro-
portionately with the sentence’s length. Wee and 
Hassan (2008) proposed a method that takes into 
account  the  directionality  of  similarity  in  which 
the  similarity  of  any  two  words  is  treated  as 
asymmetric.  The  asymmetric  issue  between  a 
pair  of  words  was  resolved  by  considering  both 
the  similarity  of  the  first  word  to  the  second 
word, and vice versa.  

Corley  and  Mihalcea  (2005)  proposed  a  hy-
brid  method  by  combining  six  existing  knowl-
edge-based  methods.  Mihalcea  et  al.  (2006)  fur-
ther combined those six  knowledge-based  meth-
ods with two corpus-based methods and claimed 
that they usually achieved better performance  in 
terms of precision and recall respectively. How-
ever,  those  methods  were  only  combined  by  us-
ing simple average calculation.  

Perhaps the  most closely related work is a re-
cently  proposed  query  extension 
technique. 
Perez-Agüera  and  Zaragoza  (2008)  made  use  of 
WSD  information  to  map  the  original  query 
words  and  the  expansion  words  to  WordNet 
senses.  However,  without  the  presence  of  or 
considering the surrounding words, the  meaning 
of  the  expansion  words  alone  tend  to  be  repre-
sented by their most general meanings instead of 
the  disambiguated  meanings,  which  results  in 
the  possibility  of  WSD  information  not  being 
useful  for  word  expansions.  In  contrast  to  their 
work,  which  is  more  suitable  to  be  applied  on 
word-to-word  similarity  task,  the  method  pro-
posed  in  this  paper  is  more  suitable  for  applica-
tion on sentence-to-sentence similarity tasks. 

Overall,  the  above-mentioned  related  works 
compute  similarity  based  either  on  statistical 
information  or  on  a  comparison  of  the  nearest 
meanings in terms of words. None of them com-
pute sentence similarity based on the comparison 
of  actual  meanings.  Our  proposed  method, 
which  is  a  solution  to  this  issue,  will  be  ex-
plained in detail in the next section. 

3  Sentence Similarity 

 

Figure 1. The proposed method 

 
Our  proposed  method  shown  in  Figure  1,  is  the 
outcome  of  some  modifications  on  an  existing 
method,  which  is  also  the  most  outstanding 
method,  the  Semantic  Text  Similarity  (STS) 
model  (Islam  and  Inkpen,  2008).  First  of  all, 
CWO  is  removed  from  STS  as  the  previous 
works  (Islam  and  Inkpen,  2007;  Islam  and  Ink-
pen,  2008)  have  shown  that  the  presence  of 
CWO  has  no  influence  on  the  outcome.  Then, 

420

the  corpus-based  word  similarity  function  of 
STS is replaced by an existing knowledge-based 
word  similarity  measure  called  YP  (Yang  and 
Powers,  2005).    Finally,  the  underlying  concept 
of  YP  is  modified  by  the  integration  of  WSD 
and  is  based  on  the  assumption  that  any  disam-
biguated  sense  of  a  word  represents  its  actual 
meaning.  Thus,  the  proposed  method  is  also 
called WSD-STS. 

3.1  String similarity measure 

The  string  similarity  between  two  words  is 
measured by using the following equations:  

     

v
1

=

NLCS

(

b
ww
j

a
i

,

)

=

l

(

LCS

(

(
wl
i

)

b
ww
j

,

2

))

a
i
×

(
wl

)

j

        (1) 

v
2

=

NMCLCS
1

(

v
3

=

NMCLCS
n

(

b
ww
j

a
i

,

l

(

)

=

MCLCS
1
×

(
wl
i

)

(

b
ww
j

a
i

,

2

))

  (2) 

(
wl

)

j

b
ww
j

a
i

,

l

(

)

=

MCLCS

(

b
ww
j

a
i

,

n
×

(
wl
i

)

(
wl

)

j

2

))

 (3) 

Simstring

(

YX

,

)

=

33.0
v
1

+

33.0
v

+

33.0
v
3

2

       (4) 

where  l(x)  represents  the  length  of  x;  a  and  b 
represent  the  lengths  of  sentences  X  and  Y  re-
spectively  after  removing  stop  words;  wi  repre-
sents  the  i-th  word  in  sequence  a;  wj  represents 
the  j-th  word  in  sequence  b;  and  Simstring(X,Y) 
represents  the  overall  string  similarity.  The  un-
derlying  concept  of  string  similarity  is  based  on 
character  matching.  NLCS  represents  the  nor-
malized  version  of  the  traditional  longest  com-
mon  subsequence  (LCS)  technique  in  which  the 
lengths  of  the  two  words  are  taken  into  consid-
eration. MCLCS1 represents the modified version 
of the traditional LCS in which the string match-
ing  must  start  from  the  first  character  while 
MCLCSn  represents  the  modified  version  of  the 
traditional  LCS  in  which  the  string  matching 
may  start  from  any  character.  NMCLCS1  and 
NMCLCSn  represent  the  normalized  versions  of 
MCLCS1  and  MCLCSn  respectively.  More  de-
tailed  information  regarding  string  similarity 
measure  can  be  found  in  the  original  paper  (Is-
lam and Inkpen, 2008). 

3.2  Adopted word similarity measure 

Yang and Powers (2005) proposed  YP based  on 
the assumptions that every single path in the  hi-
erarchical  structure  of  WordNet  1)  is  identical; 
and  2)  represents  the  shortest  distance  between 
any  two  connected  words.  The  similarity  be-

tween  two  words  in  sequence  a  and  sequence  b 
can be represented by the following equation: 

Sim

word

(

b
ww
j

a
i

,

)

=



βα
it



−
1
l
∏
=
1
i
,0
      

t

,
          

  
l

<

          

        
l

≥

γ





γ

  (5) 

where  0  ≤ 

Sim

word

(

b
ww
j

a
i

,

)

≤  1;  d  is  the  depth  of 

b

a

or 

iw  and 

synonym 

LCS; l is the length  of path between disambigu-
ated 
jw ;  t  represents  the  type  of  path 
(hypernyms/hyponym, 
holo-
nym/meronym)  which  connects  them;  αt  repre-
sents  their  path  type  factor;  βt  represents  their 
path distance factor; and γ represents an arbitrary 
threshold  on  the  distance  introduced  for  effi-
ciency, representing human cognitive limitations. 
The values of αt, βt and γ have already been em-
pirically  tuned  as  0.9,  0.85  and  12  respectively. 
More  detailed  information  regarding  YP  can  be 
found  in  the  original  paper  (Yang  and  Powers, 
2005).  
In  order to adapt a different underlying concept, 
which  is  the  comparison  of  actual  meanings,  l 
has to be redefined as the path distance between 
disambiguated  words, 
jw .  Since  YP 
only  differs  from  the  modified  version  of  YP 
(MYP)  in  terms  of  the  definition  of  l, MYP  can 
also be represented by equation (5). 

iw  and 

a

b

3.3  The proposed measure 

The gap 

Generally, all the related works in Section 2 can 
be  abstracted  as  a  function  of  word  similarity. 
This reflects the importance of a word similarity 
measure in  measuring sentence similarity. How-
ever,  measuring  sentence  similarity  is  always  a 
more  complicated  task  than  measuring  word 
similarity. The reason is that while a word simi-
larity  measure  only  involves  a  single  pair  of 
words, a sentence similarity measure has to deal 
with  multiple pairs of words. In addition, due to 
the  presence  of  the  surrounding  words  in  a  sen-
tence, the possible  meaning  of a word  is always 
being restricted (Kolte and Bhirud, 2008). Thus, 
without some modifications, the traditional word 
similarity measures, which are based on the con-
cept  of  a  comparison  of  the  nearest  meanings, 
are  inapplicable  in  the  context  of  sentence  simi-
larity measures.  

The importance of WSD in reducing the gap 

421

Before  performing  the  comparison  of  actual 
meanings,  WSD  has to  be  integrated  so  that  the 
most  suitable  sense  can  be  assigned  to  any 
polysemous  word.  The  importance  of  WSD  can 
be investigated by using a simple example. Con-
sider  a  pair  of  sentences,  collected  from  Word-
Net 2.1, which use two words, “dog” and “cat”: 

X: The dog barked all night. 

Y: What a cat she is! 

Based  on  the  definition  in  WordNet  2.1,  the 
word  “dog”  in  X  is  annotated  as  the  first  sense 
which  means  “a  member  of  the  genus  Canis 
(probably  descended  from  the  common  wolf) 
that has been domesticated by man since prehis-
toric  times”.  Meanwhile,  the  word  “cat”  in  Y  is 
annotated  as  the  third  sense  with  the  definition 
of  “a  spiteful  woman’s  gossip”.  The  path  dis-
tance  between  “cat”  and  “dog”  based  on  their 
actual senses is equal to 7. However, their short-
est path distance (SPD), which is based  on their 
nearest  senses,  is  equal  to  4.  SPD  is  the  least 
number  of  edges  connecting  two  words  in  the 
hierarchical  structure  of  WordNet.  In  other 
words, “cat” and “dog” in X and Y respectively, 
are  not  as  similar  as  the  one  measured  by  using 
SPD.  The  presence  of  the  additional  path  dis-
tances  is  significant  as  it  is  almost  double  the 
actual path distance between “cat” and “dog”. 

WSD-STS 

The  adopted  sentence  similarity  measure,  STS, 
can be represented by the following equations: 

  

Sim

semantic

(

YX

,

)

=

δ
(

+

  

SIM

(

YX

,

)

=

Sim

smeantic

(

)

c
1∑
= τ
i
i
2
ab
+
)

YX

,

×

(

ba

+

)

        (6) 

Sim

string

(

YX

,

)

(7) 

2

where  for  equation  (6):  δ  represents  the  number 
of  overlapped  words  between  the  words  in  se-
quence  a  and  sequence  b;  c represents  the  num-
ber  of  semantically  matched  words  between  the 
words in sequence a and sequence b, in which c 
=  a  if  a  <  b  or  c  =  b  if  b  <  a,  τi  represents  the 
highest matching similarity score of i-th word in 
the  shorter  sequence  with  respect  to  one  of  the 
words  in  the  longer  sequence;  and  Στ represents 
the sum of the  highest  matching similarity score 
between  the  words  in  sequence  a  and  sequence 
b.  

For STS,  the  similarity  between  two  words  is 
measured  by  using  a  corpus-based  measure.  For 
WSD-STS,  this  corpus-based  measure  is  re-

placed  by  MYP.  Finally,  the  overall  sentence 
similarity is represented by equation (7). 

4  Experimental Design 

4.1  Data set 

Li  et  al.,  (2006)  constructed  a  data  set  which 
consists of 65 pairs of human-rated sentences by 
applying the similar experimental design for cre-
ating the standard data set for the word similarity 
task (Rubenstein and Goodenough, 1965). These 
65  sentence  pairs  were  the  definitions  collected 
from  the  Collin  Cobuild  Dictionary.  Out  of 
these,  30  sentence  pairs  with  rated  similarity 
scores  that  ranged  from  0.01  to  0.96  were  se-
lected  as  test  data  set.  The  corresponding  30 
word pairs for these 30 sentence pairs are shown 
in the second column of Table 1. A further set of 
66 sentence pairs is still under development and 
it  will  be  combined  with  the  existing  data  set  in 
the future (O’Shea et al., 2008b). 

4.2  Procedure 
Firstly,  Stanford  parser 1 is  used  to  parse  each 
sentence  and  to  tag  each  word  with  a  part  of 
speech  (POS).  Secondly,  Structural  Semantic 
Interconnections2 (SSI), which is an online WSD 
system,  is  used  to  disambiguate  and  to  assign  a 
sense for each word in the 30 sentences based on 
the  assigned  POS.  SSI  is  applied  based  on  the 
assumption  that  it  is  able  to  perform  WSD  cor-
rectly. The main reason for choosing SSI to per-
form  WSD  is  its  promising  results  reported  in  a 
study by Navigli and Verladi (2006). Thirdly, all 
the  stop  words  which  exist  in  these  30  pairs  of 
sentences  are  removed.  It  is  important  to  note 
that the 100 most frequent  words collected from 
British  National  Corpus  (BNC)  were  applied  as 
the  stop  words  list  on  the  baseline,  STS.  How-
ever,  due  to  the  limited  accessibility  to  BNC,  a 
different  stop  words  list 3 ,  which  is  available 
online, is applied in this paper. 

                                                
1 http://nlp.stanford.edu/software/lex-parser.shtml 
2 http://lcl.uniroma1.it/ssi 
3 http://www.translatum.gr/forum/index.php?topic=    
  2476.0 

422

Table 1. Data Set Results 

 

 

Finally,  the  remaining  content  words  are 
lemmatized  by  using  Natural  Language  Toolkit4 
(NLTK).  Nevertheless,  those  words  which  can 
be  found  in  WordNet  and  which  have  different 
definitions  from  their  lemmatized  form  will  be 
excluded from lemmatization. For instance, 

Cooking[NN] can be a great art. 

The  word  in  the  bracket  represents  the  tagged 
POS for its corresponding  word. Since based  on 
the definitions provided by WordNet, “cooking”, 
which is tagged as a noun, has a different mean-
ing  from  its  lemmatized  form  “cook”,  which  is 
also  tagged  as  a  noun.  Therefore,  “cooking”  is 
excluded from lemmatization. 

4.3  Experimental conditions 

Sentence  similarity  is  measured  under  the  fol-
lowing three conditions: 

                                                
4 http://www.nltk.org/ 

•  OLP-STS:  A  modified  version  of  the 
baseline, STS (Islam and Inkpen, 2008), 
in which it only relies on the presence of 
overlapped  words.  This  means  that  the 
c
component  ∑ = i
,  which  represents  the 
i τ1
word  similarity,  is  removed  from  equa-
tion (6).  

•  SPD-STS: The corpus-based word simi-
larity  measure  of  the  baseline,  STS, 
c
which  is  represented  by  ∑ = i
 in  equa-
i τ1
tion  (6),  is  replaced  by  a  knowledge-
based word similarity measure, YP.   

•  WSD-STS: A modified version of SPD-
the  knowledge-based 

STS 
measure, YP, is replaced by MYP. 

in  which 

 
As  mentioned  in  Section  4.2,  different  stop 
words  lists  were  applied  between  the  baseline 
and  the  proposed  methods  under  different  ex-

423

perimental  conditions  in  this  paper.  Since  this 
issue  may  be  questioned  due  to  the  unfair  com-
parison, the performance  of WSD-STS is evalu-
ated  on  top  of  a  number  of  different  stop  words 
lists which are available online in order to inves-
tigate  any  influence  which  may  be  caused  by 
stop words list. 

5  Results and Discussion 

Table  1  presents  the  similarity  scores  obtained 
from the mean of human ratings, the benchmarks, 
and different experimental conditions of the pro-
posed  methods.  Figure  2  presents  the  corre-
sponding  Pearson  correlation  coefficients  of 
various measures as listed in Table 1.  
 

Figure 2. Pearson Correlation Coefficient 

 

 

Figure  2  shows  that  STS  appears  to  be  the 
most  outstanding  measure  among  the  existing 
works  with  a  correlation  coefficient  of  0.853. 
However, Figure 2 also shows that both the pro-
posed  methods  in  this  paper,  WSD-STS  and 
SPD-STS, outperform STS. This result indicates 
that  knowledge-based  method  tends  to  perform 
better than a corpus-based method. The reason is 
that  a  knowledge  base  is  much  closer  to  human 
representation  of  knowledge  (WordNet  is  the 
knowledge  base  applied  in  this  paper)  than  a 
corpus.  A  corpus  only  reflects  the  usage  of  lan-
guages  and  words  while  WordNet  is  a  model  of 
human  knowledge  constructed  by  many  expert 
lexicographers (Li et al., 2006). In other words, a 
corpus  is  more  likely  to  provide  unprocessed 
raw  data  while  a  knowledge  base  tends  to  pro-
vide ready-to-use information.  

The  results  of  the  performance  of  the  two 
proposed  methods  are  as  expected.  SPD-STS 

achieved  a  bigger  but  statistically  insignificant 
improvement  while  WSD-STS  achieved  a 
smaller  but  statistically  significant  improvement 
at  0.01  levels.  The  significance  of  a  correlation 
is  calculated  by  using  an  online  calculator,  Vas-
sarStats5. The reason for the variance in the out-
comes  between  SPD-STS  and  WSD-STS  is  ob-
vious; it is the difference in terms of their under-
lying concepts. In other words, sentence similar-
ity computation, which is based on a comparison 
of  the  nearest  meanings,  results  in  insignificant 
improvement while sentence similarity computa-
tion,  which  is  based  on  a  comparison  of  actual 
meanings,  achieves  statistically  significant  im-
provement.  These  explanations  indicate  that 
WSD  is  essential  in  confirming  the  validity  of 
the task of measuring sentence similarity.  

Figure 2 also reveals that a relatively low cor-
relation  is  achieved  by  OLP-STS. This  is  not  at 
all  surprising  since  Achananuparp  et  al.  (2008) 
has  already  demonstrated  that  the  overlapped 
word-based  method  tends  to  perform  badly  in 
measuring  sentence  similarity.  However,  it  is 
interesting to find that the difference in perform-
ance  between  STS  and  OLP-STS  is  very  small. 
This  indirectly  suggests  that  the  presence  of  the 
string  similarity  measure  and  the  corpus-based 
word  similarity  measure  has  only  a  slight  im-
provement on the performance of OLP-STS. 
 

 
Figure 3. The performance of the WSD-SPD 

versus different stop words lists 

 

Next,  in  order  to  address  the  issue  of  unfair 
comparison  due  to  the  usage  of  different  stop 
words  lists,  the  performance  of  WSD-SPD  has 
been  evaluated  on  top  of  a  number  of  different 

                                                
5 http://faculty.vassar.edu/lowry/rdiff.html? 

424

stop  words  lists.  A  total  of  five  stop  words  lists 
with  different  lengths  (896,  2237,  319,  5718 and 
6599)  of  stop  words  were  applied.  The  perform-
ances  of  WSD-SPD  with  respect  to  these  stop 
words  lists  are  portrayed  in  Figure  3.  They  are 
found  to  be  in  a  comparable  condition.  This  re-
sult  connotes  that  the  influence  caused  by  the 
usage  of  different  stop  words  lists  is  small  and 
can  be  ignored.  Hence,  the  unfair  comparison 
between  our  proposed  method  and  the  baseline 
should  not  be  treated  as  an  issue  for  the  bench-
marking purpose of this paper. 

On  the  other  hand,  although  an assumption  is 
made  that  SSI  performs  WSD  correctly,  we  no-
ticed  that  not  all  the  words  were  disambiguated 
confidently.  The  confident  scores  which  were 
assigned  to  the  disambiguated  words  by  SSI 
range  between  30%  and  100%.  These  confident 
scores  reflect  the  confidence  of  SSI  in  perform-
ing WSD. Thus, it is possible that some of those 
words  which  were  assigned  with  low  confident 
scores  were  disambiguated  incorrectly.  Conse-
quently,  the  final  sentence  similarity  score  is 
likely  to  be  affected  negatively.  In  order  to  re-
duce the negative effect which may be caused by 
incorrect WSD, any words pair which is not con-
fidently disambiguated is assigned the similarity 
score  based  on  the  concept  of  comparing  the 
nearest  meanings  instead  of  comparing  the  ac-
tual  meanings.  In  other  words,  WSD-STS  and 
SPD-STS  are  combined  and  results  in  WSD-
SPD.  The  performance  of  WSD-SPD  across  a 
range  of  confident  scores  is  essential  in  reveal-
ing  the  impact  of  WSD  and  SPD  on  the  task  of 
measuring sentence similarity.     

Figure 4 outlines the performance achieved by 
WSD-SPD  across  different  confident  scores  as-
signed by SSI. The confident score of at least 0.7 
is  identified  as  the  threshold  in  which  SSI  opti-
mizes  its  performance.  The  performance  of 
WSD-SPD  is  found  to  be  statistically  insignifi-
cant for those confident scores above the thresh-
old. The explanation for this phenomenon can be 

                                                
6 http://msdn.microsoft.com/en-us/library/   
  bb164590.aspx 
7 http://snowball.tartarus.org/algorithms/english/   
  stop.txt 
8 http://truereader.com/manuals/onix/ 
    stopwords2.html 
9 http://www.link-assistant.com/seo-stop- 
    words.html 

found  in  Figure  5.  Figure  5  illustrates  the  per-
centage  of  the  composition  between  WSD  and 
SPD  in  WSD-SPD.  It  is  obvious  that  once  the 
portion of WSD exceeds the portion of SPD, the 
performance  of  WSD-SPD  is  found  to  be  statis-
tically  insignificant.  This  finding  suggests  that 
SPD,  which  reflects  the  application  of  the  con-
cept of  nearest  meaning comparison, is  likely to 
decrease  the  validity  of  sentence  similarity 
measurement  while  WSD,  which  reflects  the 
application  of  the  concept  of  actual  meaning 
comparison, is essential in confirming the  valid-
ity of sentence similarity measurement.   
 

Figure 4. The performance of WSD-SPD versus 

 

confident scores 

 

 
Figure 5. The percentage of WSD/SPD versus 

confident score 

 

The  trend  of  the  performance  of  string  simi-
larity measure and word similarity measure with 
respect  to  different  weight  assignments  is  de-
lineated  in  Figure  6.  The  lowest  correlation  of 
0.856 is obtained when only the string similarity 
function  is  considered  while  the  word  similarity 

425

function  is  excluded.  A  better  performance  is 
achieved  by  taking  the  two  measures  into  con-
sideration  where  more  weight  is  given  to  the 
measure of  word similarity. This trend intimates 
that the string similarity measure offers a smaller 
contribution  in  measuring  sentence  similarity 
than  word  similarity  measure.  In  contrast  to  a 
word  similarity  measure,  a  string  similarity 
measure  is  purposely  proposed  to  address  the 
issue of misspelled words. Since the data set ap-
plied  in  this  experiment  does  not  contain  any 
misspelled words, it is obvious that a string simi-
larity  measure  performs  badly.  In  addition,  the 
underlying  concept  of  string  similarity  is  ques-
tionable.  Does  it  make  sense  to  determine  the 
similarity  of  two  words  based  on  the  matching 
between  their  characters  or  the  matching  of  the 
sequence  of  characters?  Consider  four  pairs  of 
words:  “play”  versus  “pray”,  “plant”  versus 
“plane”, “plane” versus “plan” and “stationary” 
versus “stationery”. These word pairs are highly 
similar  in  terms  of  characters  but  they  are  se-
mantically dissimilar or unrelated.  

 

Figure 6. The performance of the different 

 

measures versus the weight between string simi-

larity and word similarity 

 

Figure  6  also  depicts  that  the  combination  of 
word  similarity  measure  (70%)  and  string  simi-
larity  measure  (30%)  performs  better  than  the 
measure  which  is  solely  based  on  word  similar-
ity  function.  It  is  obvious  that  the  difference  is 
caused  by  the  presence  of  string  similarity 
measure.  The  combination  assigns  similarity 
scores to all word pairs while the word similarity 
measure  only  assigns  similarity  scores  to  those 
word  pairs  which  fulfill  two  requirements:  1) 

any  two  words  which  share  an  identical  POS, 
and  2)  any  two  words  which  must  either  be  a 
pair  of  nouns  or  a  pair  of  verbs.  In  fact,  adjec-
tives  and  adverbs  do  contribute  to  representing 
the  meaning  of  a  sentence  although  their  contri-
bution is relatively smaller than the contribution 
of  nouns  and  verbs  (Liu  et  al.,  2007;  Li  et  al., 
2009).  Therefore,  by  ignoring  the  presence  of 
adjectives  and  adverbs,  the  performance  will 
definitely be affected negatively. 

6  Conclusion 

This  paper  has  presented  a  knowledge-based 
method  which  measures  the  similarity  between 
two  sentences  based  on  their  actual  meaning 
comparison.  The  result  shows  that  the  proposed 
method,  which  is  a  knowledge-based  measure, 
performs  better  than  the  baseline,  which  is  a 
corpus-based  measure.  The  improvement  ob-
tained  is  statistically  significant  at  0.025  levels. 
This  result  also  shows  that  the  validity  of  the 
output  of  measuring  the  similarity  of  two  sen-
tences  can  be  improved  by  comparing  their  ac-
tual meanings  instead of their nearest  meanings. 
These are achieved by transforming the baseline 
into  a  knowledge-based  method  and  then  by  in-
tegrating  WSD  into  the  adopted  knowledge-
based measure. 

Although  the  proposed  method  significantly 
improves  the  quality  of  measuring  sentence 
similarity,  it  has  a  limitation.  The  proposed 
method  only  measures  the  similarity  between 
two  words  with  an  identical  part  of  speech 
(POS) and these two words must either be a pair 
of  nouns or a pair of verbs. By ignoring the  im-
portance of adjectives and adverbs, and the rela-
tionship  between  any  two  words  with  different 
POS, a slight decline is observed in the obtained 
result.  In  future  research,  these  two  issues  will 
be  addressed  by  taking  into  account  the  related-
ness  between  two  words  instead  of  only  consid-
ering their similarity. 
 

References 

Achananuparp,  Palakorn,  Xiao-Hua  Hu,  and  Xiao-
Jiong  Shen.  2008.  The  Evaluation  of  Sentence 
Similarity  Measures.  In  Proceedings  of  the  10th 
International  Conference  on  Data  Warehousing 

426

and  Knowledge  Discovery  (DaWak),  pages  305-
316, Turin, Italy. 

Corley, Courtney, and Rada Mihalcea. 2005. Measur-
ing  the  Semantic  Similarity  of  Texts.  In  Proceed-
ings of the ACL Workshop on Empirical Modeling 
of  Semantic  Equivalence  and  Entailment,  pages 
48-55, Ann Arbor. 

Feng,  Jin,  Yi-Ming  Zhou,  and  Trevor  Martin.  2008. 
Sentence  Similarity  based  on  Relevance.  In  Pro-
ceedings of IPMU, pages 832-839. 

Islam,  Aminul,  and  Diana  Inkpen.  2007.  Semantic 
Similarity  of  Short  Texts.  In  Proceedings  of 
RANLP, pages 291-297. 

Islam,  Aminul,  and  Diana  Inkpen.  2008.  Semantic 
Text  Similarity  Using  Corpus-Based  Word  Simi-
larity and String Similarity. ACM Transactions on 
Knowledge Discovery from Data, 2(2):10. 

Kauchak,  David,  and  Regina  Barzilay.  2006.  Para-
phrasing for Automatic Evaluation. In Proceedings 
of HLT-NAACL, pages 455-462, New York. 

Kolte,  Sopan  Govind,  and  Sunil  G.  Bhirud.  2008. 
Word  Sense  Disambiguation  using  WordNet  Do-
mains.  In  The  First  International  Conference  on 
Emerging Trends in Engineering and Technology, 
pages 1187-1191. 

Lapata,  Mirella,  and  Regina  Barzilay.  2005.  Auto-
matic  Evaluation  of  Text  Coherence:  Models  and 
Representations. In Proceedings of the 19th Inter-
national  Joint  Conference  on  Artificial  Intelli-
gence. 

Li, Lin, Xia Hu, Bi-Yun Hu, Jun Wang, and Yi-Ming 
Zhou.  2009.  Measuring  Sentence  Similarity  from 
Different Aspects. In Proceedings of the Eighth In-
ternational  Conference  on  Machine  Learning  and 
Cybernetics, pages 2244-2249. 

Li, Yu-Hua, David McLean, Zuhair A. Bandar, James 
D.O'Shea,  and  Keeley  Crockett.  2006.  Sentence 
Similarity  Based  on  Semantic  Nets  and  Corpus 
Statistics.  IEEE  Transactions  on  Knowledge  and 
Data Engineering, 18(8):1138-50. 

Liu,  Xiao-Ying,  Yi-Ming  Zhou, and  Ruo-Shi  Zheng. 
2007. Sentence Similarity based on Dynamic Time 
Warping.  In  The  International  Conference  on  Se-
mantic Computing, pages 250-256. 

Mihalcea,  Rada,  Courtney  Corley,  and  Carlo  Strap-
parava. 2006. Corpus-based and Knowledge-based 
Measures of Text Semantic Similarity. In Proceed-
ings  of the American Association for  Artificial  In-
telligence. 

Navigli, Roberto, and Paola Velardi. 2005. Structural 
Semantic  Interconnections:  A  Knowledge-Based 
Approach  to  Word  Sense  Disambiguation.  IEEE 
Transactions on Pattern Analysis and Machine In-
telligence 27(7):1075-86. 

O'Shea,  James,  Zuhair  Bandar,  Keeley  Crockett, and 
David  McLean.  2008a.  A  Comparative  Study  of 
Two  Short Text  Semantic  Similarity  Measures.  In 
KES-AMSTA, LNAI: Springer Berlin / Heidelberg. 

O'Shea,  James,  Zuhair  Bandar,  Keeley  Crockett, and 
David  McLean. 2008b.  Pilot  Short  Text  Semantic 
Similarity  Benchmark  Data  Set:  Full  Listing  and 
Description. 

Perez-Aguera,  Jose  R.,  and  Hugo  Zaragoza.  2008. 
UCM-Y!R  at  Clef  2008  Robust  and  WSD  Tasks. 
In Working Notes for CLEF Workshop. 

Rubenstein, Herbert, and John B. Goodenough. 1965. 
Contextual Correlates  of  Synonymy.  Communica-
tions of the ACM, pages 627-633. 

Wang,  Yao-Feng,  Yue-Jie  Zhang,  Zhi-Ting  Xu,  and 
Tao Zhang. 2006. Research on Dual Pattern of Un-
supervised  and  Supervised  Word  Sense  Disam-
biguation.  In  Proceedings  of  the  Fifth  Interna-
tional  Conference  on  Machine  Learning  and  Cy-
bernetics, pages 2665-2669. 

Wee, Leong Chee, and Samer Hassan. 2008. Exploit-
ing  Wikipedia  for  Directional  Inferential  Text 
Similarity.  In  Proceedings  of  Fifth  International 
Conference  on  Information  Technology:  New 
Generations, pages 686-691. 

Yang,  Cha,  and  Jun  Wen. 2007. Text  Categorization 
Based  on  Similarity  Approach.  In  Proceedings  of 
International  Conference  on  Intelligence  Systems 
and Knowledge Engineering (ISKE). 

Yang,  Dong-Qiang,  and  David  M.W.  Powers.  2005. 
Measuring  Semantic  Similarity  in  the  Taxonomy 
of  WordNet.  In  Proceedings  of  the  28th  Austral-
asian  Computer  Science  Conference,  pages  315-
332, Australia. 

Zhou,  Liang,  Chin-Yew  Lin,  Dragos  Stefan 
Munteanu, and Eduard Hovy. 2006. ParaEval: Us-
ing Paraphrases to Evaluate Summaries Automati-
cally.  In  Proceedings  of  Human  Language  Tech-
nology Conference of the North American Chapter 
of the ACL, pages 447-454, New York. 

 

