










































Exploring Topic Coherence over Many Models and Many Topics


Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 952–961, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics

Exploring Topic Coherence over many models and many topics

Keith Stevens1,2 Philip Kegelmeyer3 David Andrzejewski2 David Buttler2
1University of California Los Angeles; Los Angeles , California, USA

2Lawrence Livermore National Lab; Livermore, California, USA
3Sandia National Lab; Livermore, California, USA

{stevens35,andrzejewski1,buttler1}@llnl.gov
wpk@sandia.gov

Abstract

We apply two new automated semantic eval-
uations to three distinct latent topic models.
Both metrics have been shown to align with
human evaluations and provide a balance be-
tween internal measures of information gain
and comparisons to human ratings of coher-
ent topics. We improve upon the measures
by introducing new aggregate measures that
allows for comparing complete topic models.
We further compare the automated measures
to other metrics for topic models, compar-
ison to manually crafted semantic tests and
document classification. Our experiments re-
veal that LDA and LSA each have different
strengths; LDA best learns descriptive topics
while LSA is best at creating a compact se-
mantic representation of documents and words
in a corpus.

1 Introduction

Topic models learn bags of related words from large
corpora without any supervision. Based on the
words used within a document, they mine topic level
relations by assuming that a single document cov-
ers a small set of concise topics. Once learned,
these topics often correlate well with human con-
cepts. For example, one model might produce topics
that cover ideas such as government affairs, sports,
and movies. With these unsupervised methods, we
can extract useful semantic information in a variety
of tasks that depend on identifying unique topics or
concepts, such as distributional semantics (Jurgens
and Stevens, 2010), word sense induction (Van de
Cruys and Apidianaki, 2011; Brody and Lapata,
2009), and information retrieval (Andrzejewski and
Buttler, 2011).

When using a topic model, we are primarily con-
cerned with the degree to which the learned top-
ics match human judgments and help us differen-
tiate between ideas. But until recently, the evalua-
tion of these models has been ad hoc and applica-
tion specific. Evaluations have ranged from fully
automated intrinsic evaluations to manually crafted
extrinsic evaluations. Previous extrinsic evaluations
have used the learned topics to compactly represent
a small fixed vocabulary and compared this distribu-
tional space to human judgments of similarity (Jur-
gens and Stevens, 2010). But these evaluations are
hand constructed and often costly to perform for
domain-specific topics. Conversely, intrinsic mea-
sures have evaluated the amount of information en-
coded by the topics, where perplexity is one com-
mon example(Wallach et al., 2009), however, Chang
et al. (2009) found that these intrinsic measures do
not always correlate with semantically interpretable
topics. Furthermore, few evaluations have used the
same metrics to compare distinct approaches such
as Latent Dirichlet Allocation (LDA) (Blei et al.,
2003), Latent Semantic Analysis (LSA) (Landauer
and Dutnais, 1997), and Non-negative Matrix Fac-
torization (NMF) (Lee and Seung, 2000). This has
made it difficult to know which method is most use-
ful for a given application, or in terms of extracting
useful topics.

We now provide a comprehensive and automated
evaluation of these three distinct models (LDA,
LSA, NMF), for automatically learning semantic
topics. While these models have seen significant im-
provements, they still represent the core differences
between each approach to modeling topics. For our
evaluation, we use two recent automated coherence
measures (Mimno et al., 2011; Newman et al., 2010)

952



originally designed for LDA that bridge the gap be-
tween comparisons to human judgments and intrin-
sic measures such as perplexity. We consider several
key questions:

1. How many topics should be learned?
2. How many learned topics are useful?
3. How do these topics relate to often used semantic tests?

4. How well do these topics identify similar documents?

We begin by summarizing the three topic mod-
els and highlighting their key differences. We then
describe the two metrics. Afterwards, we focus on
a series of experiments that address our four key
questions and finally conclude with some overall re-
marks.

2 Topic Models

We evaluate three latent factor models that have seen
widespread usage:

1. Latent Dirichlet Allocation
2. Latent Semantic Analysis with Singular Value De-

composition
3. Latent Semantic Analysis with Non-negative Ma-

trix Factorization

Each of these models were designed with differ-
ent goals and are supported by different statistical
theories. We consider both LSA models as topic
models as they have been used in a variety of sim-
ilar contexts such as distributional similarity (Jur-
gens and Stevens, 2010) and word sense induction
(Van de Cruys and Apidianaki, 2011; Brody and
Lapata, 2009). We evaluate these distinct models
on two shared tasks (1) grouping together similar
words while separating unrelated words and (2) dis-
tinguishing between documents focusing on differ-
ent concepts.

We distill the different models into a shared repre-
sentation consisting of two sets of learned relations:
how words interact with topics and how topics inter-
act with documents. For a corpus with D documents
and V words, we denote these relations in terms of
T topics as
(1) a V × T matrix, W , that indicates the strength

each word has in each topic, and

(2) a T × D matrix, H , that indicates the strength
each topic has in each document.

T serves as a common parameter to each model.

2.1 Latent Dirichlet Allocation

Latent Dirichlet Allocation (Blei et al., 2003) learns
the relationships between words, topics, and docu-
ments by assuming documents are generated by a
particular probabilistic model. It first assumes that
there are a fixed set of topics, T used throughout the
corpus, and each topic z is associated with a multi-
nomial distribution over the vocabulary Φz , which is
drawn from a Dirichlet prior Dir(β). A given docu-
ment Di is then generated by the following process

1. Choose Θi ∼ Dir(α), a topic distribution for Di

2. For each word wj ∈ Di:

(a) Select a topic zj ∼ Θi
(b) Select the word wj ∼ Φzj

In this model, the Θ distributions represent the
probability of each topic appearing in each docu-
ment and the Φ distributions represent the proba-
bility of words being used for each topic. These
two sets of distributions correspond to our H and W
matrices, respectively. The process above defines a
generative model; given the observed corpus, we use
collapsed Gibbs sampling implementation found in
Mallet1 to infer the values of the latent variables Φ
and Θ (Griffiths and Steyvers, 2004). The model re-
lies only on two additional hyper parameters, α and
β, that guide the distributions.

2.2 Latent Semantic Analysis

Latent Semantic Analysis (Landauer and Dutnais,
1997; Landauer et al., 1998) learns topics by first
forming a traditional term by document matrix used
in information retrieval and then smoothing the
counts to enhance the weight of informative words.
Based on the original LSA model, we use the Log-
Entropy transform. LSA then decomposes this
smoothed, term by document matrix in order to gen-
eralize observed relations between words and docu-
ments. For both LSA models, we used implementa-
tions found in the S-Space package.2

Traditionally, LSA has used the Singular Value
Decomposition, but we also consider Non-negative
Matrix Factorization as we’ve seen NMF applied
in similar situations (Pauca et al., 2004) and others

1http://mallet.cs.umass.edu/
2https://github.com/fozziethebeat/S-Space

953



Model Label Top Words UMass UCI
High Quality Topics

LDA
interview told asked wanted interview people made thought time called knew -2.52 1.29
wine wine wines bottle grapes made winery cabernet grape pinot red -1.97 1.30

NMF
grilling grilled sweet spicy fried pork dish shrimp menu dishes sauce -1.01 1.98
cloning embryonic cloned embryo human research stem embryos cell cloning cells -1.84 1.46

SVD
cooking sauce food restaurant water oil salt chicken pepper wine cup -1.87 -1.21
stocks fund funds investors weapons stocks mutual stock movie film show -2.30 -1.88

Low Quality Topics

LDA
rates 10-yr rate 3-month percent 6-month bds bd 30-yr funds robot -1.94 -12.32
charity fund contributions .com family apartment charities rent 22d children assistance -2.43 -8.88

NMF
plants stem fruitful stems trunk fruiting currants branches fence currant espalier -3.12 -12.59
farming buzzards groundhog prune hoof pruned pruning vines wheelbarrow tree clematis -1.90 -12.56

SVD
city building city area buildings p.m. floors house listed eat-in a.m. -2.70 -8.03
time p.m. system study a.m. office political found school night yesterday -1.67 -7.02

Table 1: Top 10 words from several high and low quality topics when ordered by the UCI Coherence
Measure. Topic labels were chosen in an ad hoc manner only to briefly summarize the topic’s focus.

have found a connection between NMF and Proba-
bilistic Latent Semantic Analysis (Ding et al., 2008),
an extension to LSA. We later refer to these two LSA
models simply as SVD and NMF to emphasize the
difference in factorization method.

Singular Value Decomposition decomposes M
into three smaller matrices

M = UΣV T

and minimizes Frobenius norm of M ’s reconstruc-
tion error with the constraint that the rows of U and
V are orthonormal eigenvectors. Interestingly, the
decomposition is agnostic to the number of desired
dimensions. Instead, the rows and columns in U and
V T are ordered based on their descriptive power, i.e.
how well they remove noise, which is encoded by
the diagonal singular value matrix Σ. As such, re-
duction is done by retaining the first T rows and
columns from U and V T . For our generalization,
we use W = UΣ and H = ΣV T . We note that
values in U and V T can be both negative and pos-
itive, preventing a straightforward interpretation as
unnormalized probabilities

Non-negative Matrix Factorization also factor-
izes M by minimizing the reconstruction error, but
with only one constraint: the decomposed matrices
consist of only non-negative values. In this respect,
we can consider it to be learning an unnormalized
probability distributions over topics. We use the

original Euclidean least squares definition of NMF3.
Formally, NMF is defined as

M = WH

where H and W map directly onto our generaliza-
tion. As in the original NMF work, we learn these
unnormalized probabilities by initializing each set of
probabilities at random and update them according
to the following iterative update rules

W = W MH
T

WHHT
H = H W

T M
W T WH

3 Coherence Measures

Topic Coherence measures score a single topic by
measuring the degree of semantic similarity between
high scoring words in the topic. These measure-
ments help distinguish between topics that are se-
mantically interpretable topics and topics that are ar-
tifacts of statistical inference, see Table 1 for exam-
ples ordered by the UCI measure. For our evalua-
tions, we consider two new coherence measures de-
signed for LDA, both of which have been shown to
match well with human judgements of topic quality:
(1) The UCI measure (Newman et al., 2010) and (2)
The UMass measure (Mimno et al., 2011).

Both measures compute the coherence of a topic
as the sum of pairwise distributional similarity

3We note that the alternative KL-Divergence form of NMF
has been directly linked to PLSA (Ding et al., 2008)

954



scores over the set of topic words, V . We generalize
this as

coherence(V ) =
∑

(vi,vj)∈V
score(vi, vj , �)

where V is a set of word describing the topic and �
indicates a smoothing factor which guarantees that
score returns real numbers. (We will be exploring
the effect of the choice of �; the original authors used
� = 1.)

The UCI metric defines a word pair’s score to
be the pointwise mutual information (PMI) between
two words, i.e.

score(vi, vj , �) = log
p(vi, vj) + �

p(vi)p(vj)

The word probabilities are computed by counting
word co-occurrence frequencies in a sliding window
over an external corpus, such as Wikipedia. To some
degree, this metric can be thought of as an external
comparison to known semantic evaluations.

The UMass metric defines the score to be based
on document co-occurrence:

score(vi, vj , �) = log
D(vi, vj) + �

D(vj)

where D(x, y) counts the number of documents con-
taining words x and y and D(x) counts the num-
ber of documents containing x. Significantly, the
UMass metric computes these counts over the orig-
inal corpus used to train the topic models, rather
than an external corpus. This metric is more intrin-
sic in nature. It attempts to confirm that the models
learned data known to be in the corpus.

4 Evaluation

We evaluate the quality of our three topic models
(LDA, SVD, and NMF) with three experiments. We
focus first on evaluating aggregate coherence meth-
ods for a complete topic model and consider the
differences between each model as we learn an in-
creasing number of topics. Secondly, we compare
coherence scores to previous semantic evaluations.

Lastly, we use the learned topics in a classifica-
tion task and evaluate whether or not coherent top-
ics are equally informative when discriminating be-
tween documents.

For all our experiments, we trained our models on
92,600 New York Times articles from 2003 (Sand-
haus, 2008). For all articles, we removed stop words
and any words occurring less than 200 times in the
corpus, which left 35,836 unique tokens. All doc-
uments were tokenized with OpenNLP’s MaxEnt4

tokenizer. For the UCI measure, we compute the
PMI between words using a 20 word sliding win-
dow passed over the WaCkypedia corpus (Baroni et
al., 2009). In all experiments, we compute the co-
herence with the top 10 words from each topic that
had the highest weight, in terms of LDA and NMF
this corresponds with a high probability of the term
describing the topic but for SVD there is no clear
semantic interpretation.

4.1 Aggregate methods for topic coherence

Before we can compare topic models, we require an
aggregate measure that represents the quality of a
complete model, rather than individual topics. We
consider two aggregates methods: (1) the average
coherence of all topics and (2) the entropy of the co-
herence for all topics. The average coherence pro-
vides a quick summarization of a model’s quality
whereas the entropy provides an alternate summa-
rization that differentiates between two interesting
situations. Since entropy measures the complexity
of a probability distribution, it can easily differenti-
ate between uniform distributions and multimodal,
distributions. This distinction is relevant when users
prefer to have roughly uniform topic quality instead
of a wide gap between high- and low-quality topics,
or vice versa. We compute the entropy by dropping
the log and � factor from each scoring function.

Figure 1 shows the average coherence scores for
each model as we vary the number of topics. These
average scores indicate some simple relationships
between the models: LDA and NMF have approx-
imately the same performance and both models are
consistently better than SVD. All of the models
quickly reach a stable average score at around 100
topics. This initially suggests that learning more

4http://incubator.apache.org/opennlp/

955



Number of topics

Av
er

ag
e 

To
pic

 C
oh

er
en

ce

−5

−4

−3

−2

−1

0

100 200 300 400 500

(a) UMass

Number of topics

Av
er

ag
e 

To
pic

 C
oh

er
en

ce

−10

−8

−6

−4

−2

0

2

100 200 300 400 500

Method

LDA

NMF

SVD

(b) UCI

Figure 1: Average Topic Coherence for each model

Number of topics

Co
he

re
nc

e 
En

tro
py

0

1

2

3

4

5

6

7

100 200 300 400 500

(a) UMass

Number of topics

Co
he

re
nc

e 
En

tro
py

0

1

2

3

4

5

6

7

100 200 300 400 500

Method

LDA

NMF

SVD

(b) UCI

Figure 2: Entropy of the Topic Coherence for each model

topics neither increases or decreases the quality of
the model, but Figure 2 indicates otherwise. While
the entropy for the UMass score stays stable for all
models, NMF produces erratic entropy results under
the UCI score as we learn more topics. As entropy is
higher for even distributions and lower for all other
distributions, these results suggest that the NMF is
learning topics with drastically different levels of
quality, i.e. some with high quality and some with
very low quality, but the average coherence over all
topics do not account for this.

Low quality topics may be composed of highly
unrelated words that can’t be fit into another topic,
and in this case, our smoothing factor, �, may be ar-

tificially increasing the score for unrelated words.
Following the practice of the original use of these
metrics, in Figures 1 and 2 we set � = 1. In Fig-
ure 3, we consider � = 10−12, which should sig-
nificantly reduce the score for completely unrelated
words. Here, we see a significant change in the per-
formance of NMF, the average coherence decreases
dramatically as we learn more topics. Similarly, per-
formance of SVD drops dramatically and well below
the other models. In figure 4 we lastly compute the
average coherence using only the top 10% most co-
herence topics with � = 10−12. Here, NMF again
performs on par with LDA. With the top 10% topics
still having a high average coherence but the full set

956



Number of topics

Av
er

ag
e 

To
pic

 C
oh

er
en

ce

−5

−4

−3

−2

−1

0

100 200 300 400 500

(a) UMass

Number of topics

Av
er

ag
e 

To
pic

 C
oh

er
en

ce

−10

−8

−6

−4

−2

0

2

100 200 300 400 500

Method

LDA

NMF

SVD

(b) UCI

Figure 3: Average Topic Coherence with � = 10−12

Number of topics

Av
er

ag
e 

Co
he

re
nc

e 
of

 to
p 

10
%

−5

−4

−3

−2

−1

0

100 200 300 400 500

(a) UMass

Number of topics

Av
er

ag
e 

Co
he

re
nc

e 
of

 to
p 

10
%

−10

−8

−6

−4

−2

0

2

100 200 300 400 500

Method

LDA

NMF

SVD

(b) UCI

Figure 4: Average Topic Coherence of the top 10% topics with � = 10−12

of topics having a low coherence, NMF appears to
be learning more low quality topics once it’s learned
the first 100 topics, whereas LDA learns fewer low
quality topics in general.

4.2 Word Similarity Tasks

The initial evaluations for each coherence mea-
sure asked human judges to directly evaluate top-
ics (Newman et al., 2010; Mimno et al., 2011). We
expand upon this comparison to human judgments
by considering word similarity tasks that have of-
ten been used to evaluate distributional semantic
spaces (Jurgens and Stevens, 2010). Here, we use
the learned topics as generalized semantics describ-

ing our knowledge about words. If a model’s topics
generalize the knowledge accurately, we would ex-
pect similar words, such as “cat” and “dog”, to be
represented with a similar set of topics. Rather than
evaluating individual topics, this similarity task con-
siders the knowledge within the entire set of topics,
the topics act as more compact representation for the
known words in a corpus.

We use the Rubenstein and Goodenough (1965)
and Finkelstein et al. (2002) word similarity tasks.
In each task, human judges were asked to evaluate
the similarity or relatedness between different sets of
word pairs. Fifty-One Evaluators for the Rubenstein
and Goodenough (1965) dataset were given 65 pairs

957



T

sc
or

e

0.0

0.1

0.2

0.3

0.4

0.5

0.6

100 200 300 400 500

model

LDA

NMF

SVD

(a) Rubenstein & Goodenough

T

sc
or

e

0.0

0.1

0.2

0.3

0.4

0.5

100 200 300 400 500

model

LDA

NMF

SVD

(b) Wordsim 353/Finklestein et. al.

Figure 5: Word Similarity Evaluations for each model

Topics

Co
rre

lat
ion

0.0

0.2

0.4

0.6

100 200 300 400 500

(a) UMass

Topics

Co
rre

lat
ion

0.0

0.2

0.4

0.6

100 200 300 400 500

model

LDA

NMF

SVD

(b) UCI

Figure 7: Correlation between topic coherence and topic ranking in classification

of words and asked to rate their similarity on a scale
from 0 to 4, where a higher score indicates a more
similar word pair. Finkelstein et al. (2002) broadens
the word similarity evaluation and asked 13 to 16
different subjects to rate 353 word pairs on a scale
from 0 to 10 based on their relatedness, where relat-
edness includes similarity and other semantic rela-
tions. We can evaluate each topic model by comput-
ing the cosine similarity between each pair of words
in the evaluate set and then compare the model’s
ratings to the human ratings by ranked correlation.
A high correlation signifies that the topics closely
model human judgments.

Figure 5 displays the results. SVD and LDA

both surpass NMF on the Rubenstein & Goode-
nough test while SVD is clearly the best model on
the Finklestein et. al test. While our first experi-
ment showed that SVD was the worst model in terms
of topic coherence scores, this experiment indicates
that SVD provides an accurate, stable, and reliable
approximation to human judgements of similarity
and relatedness between word pairs in comparison
to other topic models.

4.3 Coherence versus Classification

For our final experiment, we examine the relation-
ship between topic coherence and classification ac-
curacy for each topic model. We suspect that highly

958



score

Co
rre

lat
ion

0.01

0.02

0.03

0.04

0.05

0.06

0.07

−25 −20 −15 −10 −5

(a) UMass

score

Co
rre

lat
ion

0.01

0.02

0.03

0.04

0.05

0.06

0.07

−30 −20 −10 0

model

LDA

NMF

SVD

(b) UCI

Figure 8: Comparison between topic coherence and topic rank with 500 topics

Topics

Ac
cu

ra
cy

20

30

40

50

60

70

80

100 200 300 400 500

Model

LDA

NMF

SVD

Figure 6: Classification accuracy for each model

coherent topics, and coherent topic models, will per-
form better for classification. We address this ques-
tion by performing a document classification task
using the topic representations of documents as in-
put features and examine the relationship between
topic coherence and the usefulness of the corre-
sponding feature for classification.

We trained each topic model with all 92,600 New
York Times articles as before. We use the sec-
tion labels provided for each article as class labels,
where each label indicates the on-line section(s) un-
der which the article was published and should thus
be related to the topics contained in each article. To
reduce the noise in our data set we narrow down the
articles to those that have only one label and whose

label is applied to at least 2000 documents. This re-
sults in 57,696 articles with label distributions listed
in Table 2. We then represent each document using
columns in the topic by document matrix H learned
for each topic model.

Label Count Label Count
New York and Region 11219 U.S. 3675
Paid Death Notices 11152 Arts 3437
Opinion 8038 World 3330
Business 7494 Style 2137
Sports 7214

Table 2: Section label counts for New York Times
articles used for classification

For each topic model trained on N topics, we
performed stratified 10-fold cross-validation on the
57,696 labeled articles. In each fold, we build an
automatically-sized bagged ensemble of unpruned
CART-style decision trees(Banfield et al., 2007) on
90% of the dataset5, use that ensemble to assign la-
bels to the other 10%, and measure the accuracy of
that assignment. Figure 6 shows the average classifi-
cation accuracy over all ten folds for each model. In-
terestingly, SVD has slightly, but statistically signif-
icantly, higher accuracy results than both NMF and
LDA. Furthermore, performance quickly increases

5The precise choice of the classifier scheme matters little, as
long as it is accurate, speedy, and robust to label noise; all of
which is true of the choice here.

959



and plateaus with well under 50 topics.
Our bagged decision trees can also determine the

importance of each feature during classification. We
evaluate the strength of each topic during classifi-
cation by tracking the number of times each node
in our decision trees observe each topic, please see
(Caruana et al., 2006) for more details. Figure 8 plot
the relationship between this feature ranking and the
topic coherence for each topic when training LDA,
SVD, and NMF on 500 topics. Most topics for each
model provide little classification information, but
SVD shows a much higher rank for several topics
with a relatively higher coherence score. Interest-
ingly, for all models, the most coherent topics are not
the most informative. Figure 7 plots a more compact
view of this same relationship: the Spearman rank
correlation between classification feature rank and
topic coherence. NMF shows the highest correlation
between rank and coherence, but none of the mod-
els show a high correlation when using more than
100 topics. SVD has the lowest correlation, which
is probably due to the model’s overall low coherence
yet high classification accuracy.

5 Discussion and Conclusion

Through our experiments, we made several excit-
ing and interesting discoveries. First, we discov-
ered that the coherence metrics depend heavily on
the smoothing factor �. The original value, 1.0 cre-
ated a positive bias towards NMF from both met-
rics even when NMF generated incoherent topics.
The high smoothing factor also gave a significant in-
crease to SVD scores. We suspect that this was not
an issue in previous studies with the coherence mea-
sures as LDA prefers to form topics from words that
co-occur frequently, whereas NMF and SVD have
no such preferences and often create low quality top-
ics from completely unrelated words. Therefore, we
suggest a smaller � value in general.

We also found that the UCI measure often agreed
with the UMass measure, but the UCI-entropy ag-
gregate method induced more separation between
LSA, SVD, and NMF in terms of topic coherence.
This measure also revealed the importance of the
smoothing factor for topic coherence measures.

With respects to human judgements, we found
that coherence scores do not always indicate a bet-

ter representation of distributional information. The
SVD model consistently out performed both LDA
and NMF models, which each had higher coherence
scores, when attempting to predict human judge-
ments of similarity.

Lastly, we found all models capable of producing
topics that improved document classification. At the
same time, SVD provided the most information dur-
ing classification and outperformed the other mod-
els, which again had more coherent topics. Our com-
parison between topic coherence scores and feature
importance in classification revealed that relatively
high quality topics, but not the most coherent topics,
drive most of the classification decisions, and most
topics do not affect the accuracy.

Overall, we see that each topic model paradigm
has it’s own strengths and weaknesses. Latent Se-
mantic Analysis with Singular Value Decomposition
fails to form individual topics that aggregate similar
words, but it does remarkably well when consider-
ing all the learned topics as similar words develop
a similar topic representation. These topics simi-
larly perform well during classification. Conversely,
both Non Negative Matrix factorization and Latent
Dirichlet Allocation learn concise and coherent top-
ics and achieved similar performance on our evalua-
tions. However, NMF learns more incoherent topics
than LDA and SVD. For applications in which a hu-
man end-user will interact with learned topics, the
flexibility of LDA and the coherence advantages of
LDA warrant strong consideration. All of code for
this work will be made available through an open
source project.6

6 Acknowledgments

This work was performed under the auspices of
the U.S. Department of Energy by Lawrence Liv-
ermore National Laboratory under Contract DE-
AC52-07NA27344 (LLNL-CONF-522871) and by
Sandia National Laboratory under Contract DE-
AC04-94AL85000.

References

David Andrzejewski and David Buttler. 2011. Latent
topic feedback for information retrieval. In Proceed-

6https://github.com/fozziethebeat/TopicModelComparison

960



ings of the 17th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
’11, pages 600–608, New York, NY, USA. ACM.

Robert E. Banfield, Lawrence O. Hall, Kevin W. Bowyer,
and W. Philip Kegelmeyer. 2007. A comparison of de-
cision tree ensemble creation techniques. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
29(1):173–180, January.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.

Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ’09, pages 103–
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Rich Caruana, Mohamed Elhawary, Art Munson, Mirek
Riedewald, Daria Sorokina, Daniel Fink, Wesley M.
Hochachka, and Steve Kelling. 2006. Mining cit-
izen science data to predict orevalence of wild bird
species. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ’06, pages 909–915, New York,
NY, USA. ACM.

Jonathan Chang, Sean Gerrish, Chong Wang, and
David M Blei. 2009. Reading tea leaves : How hu-
mans interpret topic models. New York, 31:1–9.

Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Comput. Stat.
Data Anal., 52:3913–3927, April.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Trans. Inf. Syst., 20:116–131, January.

T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235, April.

David Jurgens and Keith Stevens. 2010. The s-space
package: an open source package for word space mod-
els. In Proceedings of the ACL 2010 System Demon-
strations, ACLDemos ’10, pages 30–35, Stroudsburg,
PA, USA. Association for Computational Linguistics.

Thomas K Landauer and Susan T. Dutnais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211–240.

Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, (25):259–284.

Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In In
NIPS, pages 556–562. MIT Press.

David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum. 2011. Optimizing
semantic coherence in topic models. In Proceedings of
the 2011 Conference on Emperical Methods in Natu-
ral Language Processing, pages 262–272, Edinburgh,
Scotland, UK. Association of Computational Linguis-
tics.

David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010. Evaluating topic
models for digital libraries. In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
’10, pages 215–224, New York, NY, USA. ACM.

V Paul Pauca, Farial Shahnaz, Michael W Berry, and
Robert J Plemmons, 2004. Text mining using nonnega-
tive matrix factorizations, volume 54, pages 452–456.
SIAM.

Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627–633, October.

Evan Sandhaus. 2008. The New York Times Annotated
Corpus.

Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 1476–1485, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In Proceedings of the 26th International Con-
ference on Machine Learning (ICML). Omnipress.

961


