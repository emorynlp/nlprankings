



















































Target word prediction and paraphasia classification in spoken discourse


Proceedings of the BioNLP 2017 workshop, pages 1–8,
Vancouver, Canada, August 4, 2017. c©2017 Association for Computational Linguistics

Target word prediction and paraphasia classification in spoken discourse

Joel Adams1, Steven Bedrick1, Gerasimos Fergadiotis2, Kyle Gorman3 and Jan van Santen1

1Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
2Speech & Hearing Sciences Department, Portland State University, Portland, OR

3Google, Inc., New York, NY

Abstract

We present a system for automatically
detecting and classifying phonologically
anomalous productions in the speech of
individuals with aphasia. Working from
transcribed discourse samples, our system
identifies neologisms, and uses a combina-
tion of string alignment and language mod-
els to produce a lattice of plausible words
that the speaker may have intended to pro-
duce. We then score this lattice accord-
ing to various features, and attempt to de-
termine whether the anomalous production
represented a phonemic error or a genuine
neologism. This approach has the potential
to be expanded to consider other types of
paraphasic errors, and could be applied to
a wide variety of screening and therapeutic
applications.

1 Introduction

Aphasia is an acquired neurogenic language dis-
order in which an individual’s ability to produce
or comprehend language is compromised. It can
be caused by a number of different underlying
pathologies, but can generally be traced back to
physical damage to the individual’s brain: tissue
damage following ischemic or hemorrhagic stroke,
lesions caused by a traumatic brain injury or infec-
tion, etc. It can also be associated with various neu-
rodegenerative diseases, as in the case of Primary
Progressive Aphasia. According to the National
Institute of Neurological Disorders and Stroke, ap-
proximately 1,000,000 people in the United States
suffer from aphasia, and aphasia is a common con-
sequence of strokes (prevalence estimates for apha-
sia among stroke patients vary, but are typically in
the neighborhood of 30% (Engelter et al., 2006)).

Anomia is a the inability to access and re-
trieve words during language production, and is a
common manifestation of aphasia (Goodglass and
Wingfield, 1997). An anomic individual will ex-
perience difficulty producing words and naming
items, which can cause substantial difficulties in
day-to-day communication.
The process of screening for, diagnosing, and

assessing anomia is typically manual in nature,
and requires substantial time, labor, and exper-
tise. Compared to other neuropsychological as-
sessment instruments, aphasia-related assessments
are particularly difficult to computerize, as they
typically depend on subtle and complex linguis-
tic judgments about the phonological and semantic
similarity of words, and also require the examiner
to interpret phonologically disordered speech. Fur-
thermore, themost commonly used assessments fo-
cus for practical reasons on relatively constrained
tasks such as picture naming, which may lack eco-
logical validity (Mayer and Murray, 2003).
In this work, we describe an approach to au-

tomatically detecting and analyzing certain cate-
gories of word production errors characteristic of
anomia in connected speech. Our approach is a
first step towards an automated anomia assessment
tool that could be used cost effectively in both
clinical and research settings,1 and could also be
applied to other disorders of speech production.
The method we propose uses statistical language
models to identify possible errors, and employs a
phonologically-informed edit distancemodel to de-
termine phonological similarity between the sub-
ject’s utterance and a set of plausible “intended
words.” We then apply machine learning tech-
niques to determine which of several categories
a given erroneous production may fall into. We

1As in the computer-administered (but manually-scored)
assessments developed by Fergadiotis and colleagues (Ferga-
diotis et al., 2015; Hula et al., 2015).

1



show results on intrinsic evaluations comparable
to state-of-the-art sentence completion, as well as
an extrinsic measure of classification well above a
“most-frequent” baseline strategy.

1.1 Anomia and Paraphasias

Anomia can take several different forms, but in this
work we are concerned with paraphasias, which
are unintended errors in word production.2
There are several categories of paraphasic error.

Semantic errors arise when an individual uninten-
tionally produces a semantically-related word to
their original, intended word (their “target word”).
A classic semantic error would be saying “cat”
when one intended to say “dog.”

Phonemic (sometimes called “formal”) errors
occur when the speaker produces an unrelated
word that is phonemically related to their target:
“mat” for “cat”, for example. It is also possible for
an erroneous production to be mixed, that is both
semantically and phonemically related to the tar-
get word: “rat” for “cat.” Individuals with anomia
also produce unrelated errors, which are words
that are neither semantically or phonemically re-
lated to their intended target word: for example,
producing “skis” instead of “zipper.”
Each of these categories shares the commonal-

ity that the word produced by the individual is a
“real” word. There is another family of anomic er-
rors, neologisms, in which the individual produces
non-word productions. A neologistic production
may be phonemically related to the target, but con-
taining phonological errors: “[dɑɪnoʊsɔɹ]” for “di-
nosaur.” These are often referred to as phonologi-
cal paraphasias. Alternatively, the individual may
produce abstruse neologisms, in which the pro-
duced phonemes bear no discernable similarity to
any “real” lexical item (“[æpməl]” for “comb”3).
The present work focuses exclusively on neol-

ogisms, both of the phonological variety as well
as the abstruse variety. However, our fundamental
approach can be extended to include other forms,

2Note that individuals without any sort of language dis-
order do occasionally produce errors in their speech; this
fact has led to a truly shocking amount of study by linguists.
Frisch &Wright (2002) provide a reasonable overview of the
background and phonology of the phenomenon.

3This example was taken from a corpus of responses to a
confrontation naming test (Mirman et al., 2010), in which the
subject is shown a picture and required to name its contents.
As such, in the case of this specific error, we have a priori
knowledge of what the target word “should” have been. Ob-
viously, in a more naturalistic task or setting, we would not
have this advantage.

as described in section 6.
Typical methods of diagnosing, staging, and oth-

erwise characterizing anomia involve determining
the number and kinds of paraphasias produced by
an individual while undergoing some structured
language elicitation process, for example a con-
frontation naming test (see (Kendall et al., 2013)
and (Brookshire et al., 2014) for examples of such
a study). As alluded to previously, producing these
counts and classifications is a complex and labori-
ous process. Furthermore, it is also often an in-
herently subjective process: are “carrot” and “ba-
nana” semantically related? What about “hose”
and “rope”?
Reliability estimates of expert human perfor-

mance at paraphasia classification in confronta-
tion naming scenarios reflect the difficulty in this
task. One recent study reported a kappa-equivalent
score of 0.76 — a score that that is certainly ac-
ceptable, but that leaves much room for disagree-
ment on the status of specific erroneous produc-
tions (Minkina et al., 2015). Other reported scores
fall in a similar range (Kristensson et al., 2015), in-
cluding when the productions are from neurotyp-
ical individuals (Nicholas et al., 1989). Automat-
ing this aspect of the task would not only improve
efficiency, but would also decrease scoring vari-
ability. Having a reliable, automated method to
analyze paraphasic errors would also expand the
scope of what is currently possible in terms of as-
sessment methodologies.
Notably, the approach we outline in this paper is

explicitly designed to work on samples of natural,
connected speech. It builds upon previous work by
Fergadiotis et al. (2016) on automated analysis of
errors produced in confrontation naming tests, and
extends it into the realm of naturalistic discourse.
It is our hope that, by enabling automated calcu-
lation of error frequencies and types on narrative
speech, we might make using such material far eas-
ier in practice than it is today.

2 Data

For this work, we use the data set provided by the
AphasiaBank project (MacWhinney et al., 2011),
which has assembled a large database of tran-
scribed interactions between examiners and people
with aphasia, nearly all of whom have suffered a
stroke. Notably, AphasiaBank also includes tran-
scribed sessions with neurotypical controls. Each
interaction follows a common protocol and script,

2



and is transcribed in great detail using a standard-
ized set of annotation guidelines. The transcripts
include word-level error codes, according to a de-
tailed taxonomy of errors and associated annota-
tions. In the case of semantic, formal, and phone-
mic errors, the word-level annotations include a
“best guess” on the part of the transcriber as to what
the speaker’s intended production may have been.
Each transcribed session consists of a prescribed

sequence of language elicitation activities, includ-
ing a set of personal narratives (e.g.,“Do you re-
member when you had your stroke? Please tell me
about it.”), standardized picture description tasks,
a story retelling task (involving the story of Cin-
derella), and a procedural discourse task.
We noted that the distribution of errors within

sentences seems to obey the power law , with the
majority of error-containing sentences containg-
ing a single error, followed somewhat distantly by
sentences containing two errors, with a relatively
steep dropoff thereafter. For the present study, we
restricted our analysis to sentences that contained
a single error. Our reasoning for this restriction
was that we do not presently have a theoretically-
informed model of what, if any, relationship there
may be between multiple errors within a sentence.
However, it seems quite likely that the errors oc-
curring in a sentence containing (for instance) five
paraphasic errors might be somehow related to one
another. We anticipate exploring this phenomenon
in the future (see section 6).
We chose to restrict our data to the story retelling

task due to the constrained and focused vocabulary
of the Cinderella story. This resulted in ≈ 1000
sentences from 385 individuals. We then identi-
fied sentences containing instances of our errors of
interest: phonological paraphasia (AphasiaBank
codes “p:n”, “p:m”) or abstruse neologism (“n:uk”
and “n:k”).

3 Methods

We first tokenized the AphasiaBank data using a
modified version of the Penn Treebank tokenizer
which left contractions as a single word and simply
removed the connecting apostrophe, as these occa-
sionally appear as target words and thus we needed
to treat them as a single token. We left stopwords
intact, and case-folded all sentences to upper-case.
Cardinal numbers were collapsed into a category
token, as were ordinal numbers and dates (each
category was given its own token). The Aphasia-

Bank transcripts include detailed IPA-encoded rep-
resentations of neologistic productions, but any
“real-world” usage scenario for our algorithm is
unlikely to benefit from such high-quality tran-
scription. We therefore translated the non-lexical
productions into a simulated “best-guess” ortho-
graphic representation of the subject’s non-lexical
productions.
We next turned to the question of identifying ne-

ologisms in our sentences. Simply using a stan-
dard dictionary to determine lexicality could re-
sult in numerous “false positives,” driven largely
by proper names of people, brands, etc. To
avoid this, we used the SUBTLEX-US corpus
(Brysbaert and New, 2009) to identify neologisms.
SUBTLEX-US was build using subtitles from
English-language television shows and movies,
and Brysbaert and New have demonstrated that it
correlates with a number of psycholinguistic be-
havior measures (most notably, naming latencies)
better than better-known frequency norms such as
those derived from the Brown corpus or CELEX-
2.
Upon identifying a possible non-word produc-

tion, recall that our next goal is to determine
whether it represents a phonemic error (substi-
tuting “[dɑɪnoʊsɔɹ]” for “dinosaur”) or an ab-
struse neologism (a completely novel sequence of
phonemes that does not correspond to an actual
word). To help accomplish this, we train a lan-
guage model to identify plausible words that could
fit in the slot occupied by the erroneous produc-
tion, and produce a lattice of these candidate target
words (i.e., words that the subject may have been
intending to produce, given what we know about
the context in which they were speaking).
Our languagemodels for this studywere built us-

ing the New York Times section of the Gigaword
newswire corpus (Parker et al., 2011). After suc-
cess in preliminary experiments, we filtered this
corpus by first training a Latent Dirichlet Alloca-
tion (LDA) topic model on the corpus using Gen-
sim (Řehůřek and Sojka, 2010) over 20 topics. We
then projected the text of each of the Cinderella nar-
rative samples into the resulting topic space, and
calculated the centroids for the narrative task. This
yielded a subset of the larger corpus of New York
Times articles that was “most similar” to the Cin-
derella retellings, and we used these to build our
language models.
We investigated two different language model-

3



ing approaches: a traditional FST-encoded ngram
language model, and a neural-net based log-
bilinear (LBL) language model. For the FST rep-
resentation, we used the the OpenGrm-NGram
language modeling toolkit (Roark et al., 2012)
and used an n-gram order of 4, with Kneser-Ney
smoothing (Kneser and Ney, 1995). For the LBL
approach, we used a Python implementation4 of
the language model described by Mnih and Teh
(Mnih and Teh, 2012). We used word embeddings
of dimension 100, and a 5-gram context window.
In both cases we trained two language models: one
trained on the “task-specific” subset of Gigaword,
and another trained on the AphasiaBank control
data. We combined these with a simple mixing co-
efficient, λ as shown in Equation 1 where PGW(w)
is the language model probability of word w com-
puted against the Gigaword corpus and the PAB(w)
is the language model probability trained on the
AphasiaBank controls.

P(w) = λ · PAB(w) + (1− λ) · PGW(w) (1)
We evaluate non-lexical productions as fol-

lows. First, we use the Phonetisaurus grapheme-
to-phoneme toolkit (Novak et al., 2012) to trans-
late our orthographic representation into an esti-
mated phoneme sequence. We then calculate a
phonologically-aware edit distance between each
non-word production and every word in our lexi-
con up to somemaximum edit distance (in our case
4.0). Phonemes from a related class (e.g. vowels)
are considered lower cost replacements than those
from another class (e.g. unvoiced fricatives). This
gives us a set of candidates which are phonologi-
cally similar to the production.
We next used our language models to produce

lattices representing a set of possible sentences that
the subject could plausibly have been intending to
produce. At the point in the produced sentence
where our error detection system indicated that a
non-word production occurred, we represent the
anomaly by the union of all possible words in our
edit-distance constrained lexicon (see figure 3 for
an example sentence lattice). Finally, we use the
languagemodels to score the resulting sentence lat-
tice so as to be able to rank the candidate words,
and use the estimated sentence-level probability
for each candidate word (i.e., the hypothesized in-
tended utterance featuring that word). Put simply,

4https://github.com/ddahlmeier/neural_lm

� �� ������ ����� ��� �

�����

���

�����
����
�����
�����
���

�����

Figure 1: An example candidate word lattice for
the production “I can’t move my [vɑɪ] hand.”

for each candidate intended word, we produce a
version of the subject’s utterance with that hypoth-
esized word in place of the anomalous utterance,
and score this hypothesized utterance with the lan-
guage model.
At this point in the process, we have the follow-

ing information about each erroneous production:
a best-guess orthographic transcription of what the
individual actually produced, and a ranked list of
plausible words that they could potentially have
been attempting to produce, together with proba-
bility estimates for each hypothesized production.
To determine the category of our error

productions— again, between productions repre-
senting phonological errors such as “[dɑɪnoʊsɔɹ]”
for “dinosaur”, and productions representing ab-
struse neologisms— we trained a binary classifier
using features representing the characteristics
of the candidate word space surrounding the
erroneous production. Our intuition is that phone-
mic errors were much more likely than abstruse
neologisms to have highly-ranked candidate target
words that were also phonologically similar to the
subject’s actual production.
To capture this, we performed the following pro-

cedure for each error-containing utterance. We
first divide our list of candidate intended words
into buckets by edit distance (0.5, 1.0, 1.5, etc.5).
Each bucket can now be thought of as a ranked
list of probabilities, each representing a possible
hypothesized intended utterance featuring a word
within that bucket’s edit distance of the actual
(anomalous) utterance.
We next represent each bucket with a feature

vector consisting of the count of words in that

5Recall that our phonological edit distance metric allows
for partial costs for related phonological substitutions.

4



bucket, as well as descriptive statistics regard-
ing the distribution of language model probabil-
ities in that bucket (min, max, etc.). We then
concatenate each bucket’s features together into a
master feature vector for the utterance. Our ex-
pectation is that these features will be relatively
evenly distributed across buckets in the case of ut-
terances containing abstruse neologisms, whereas
utterances featuring phonological paraphasias will
vary according to phonological edit distance.
Oncewe have computed feature vectors for each

utterance, we used the Scikit-learn Python ma-
chine learning library (Pedregosa et al., 2011) to
train a Support Vector Machine classifier to dis-
tinguish between utterances phonological and ab-
struse neologisms. We evaluate its performance
using leave-one-out cross-validation.

4 Results

We perform two evaluations of our model: an in-
trinsic evaluation of how often our system includes
the target word in the top-n ranked candidates, and
an extrinsic evaluation where we attempt to clas-
sify a paraphasia between phonological errors and
abstruse neologisms.
Our motivation for evaluating our system’s per-

formance on target word prediction is tied to our
classification assumptions. In an ideal case for
a phonological error, the target word should fall
within one of the buckets representing a low edit
distance. If our language model successfully rates
the target as likely, we would see an high maxi-
mum probability within that bucket, which is a fea-
ture in our classifier.
The performance of our language models on

the top-n ranked evaluation can be seen in Table
1. The log-bilinear model outperformed the FST
in all cases. This finding is similar to state of
the art results for automatic sentence completion
systems–particularly for phonemic errors–as we’ll
discuss in greater detail in Section 5. Both systems
did a better job of predicting the target word for
phonemic errors than they did for abstruse neolo-
gisms. It’s not immediately clear what the reason
for this is. However, anecdotally, sentences includ-
ing abstruse neologisms are also often agrammati-
cal.
For the evaluation of our classification, we cre-

ated a simple majority class baseline classifier that
always chooses the largest class of errors (phone-
mic errors in this case). This baseline classifier has

Error n FST LBL
Phonemic 1 .43 .52
Phonemic 5 .54 .66
Phonemic 10 .59 .69
Phonemic 20 .67 .77
Phonemic 50 .72 .81
Abstruse Neo. 1 .29 .35
Abstruse Neo. 5 .41 .49
Abstruse Neo. 10 .44 .51
Abstruse Neo. 20 .51 .59
Abstruse Neo. 50 .54 .60

Table 1: Accuracy of language model predicting
the correct target word within the first n results.

Features FST LBL
count, mean .612 .661
count, mean, max .621 .680
count, mean, max, min .610 .659
count, mean, max, dist. .610 .659

Table 2: Classification accuracy by model. Base-
line accuracy of choosing the most common error
type is .510.

a classification accuracy of .51. The results of clas-
sification can be seen in Table 2. Both of our sys-
tems handily outperformed baseline: the FST by
a relative 20% improvement, and the LBL nearly
33%. Aswe expected from the top-n results, classi-
fication based on the LBL outperformed that based
on the FST.
The “dist” feature listed in table 2 is the edit

distance of a given bucket normalized by the num-
ber of phonemes in the actual error production. It
was not found to be productive as a feature, nor
was the minimum language model probability of
words in a given bucket (“min” in the table). The
best results for both systems were a combination of
count of candidate terms per bucket (“count”) con-
catenated with the maximum and mean language
model probabilities within a bucket (“max” and
“min”, respectively).

We varied the mixing-coefficient (λ) from Equa-
tion 1 in both the FST and LBL approaches. As
long as the resulting model includes a non-trivial
weighting of the Cinderella corpus (typically any-
thing better than λ = 3), the actual value of the
mixing coefficient was irrelevant to either of our
evaluations. In this, it appears to work as designed,
with the Gigaword corpus providing background
probabilities, and the AphasiaBank Cinderella con-

5



trol retellings increasing the weight of topically im-
portant words that are otherwise rare (like “Cin-
derella” and “carriage”).

5 Related Work & Discussion

As far back as Shannon’s word-guessing game
(Shannon, 1951), researchers have sought to lever-
age the statistical regularities in natural language to
predict missing or subsequent words. In practice,
however, this proves to be a surprisingly challeng-
ing problem. Language occurs at levels beyond
simply choosing lexical items, and local statisti-
cal characteristics of language often fail to capture
syntactic and semantic patterns. Zweig & Burges
(2012) provide an enlightening discussion on the
limitations of relying on n-gram guessing for syn-
tactically complex tasks such as “identify the miss-
ing word in the sentence,” and also describe a very
challenging language model evaluation task built
around this problem. They tested a variety of lan-
guage modeling approaches using their task, and
report that well-trained generative n-gram models
achieve correct predictions ≈ 30% of the time.
State-of-the-art performance on the their word pre-
diction task using recurrent neural network lan-
gage models,6 report highest scores are in the mid-
50% range (Mirowski and Vlachos, 2015; Mnih
and Kavukcuoglu, 2013).
In our case, the nature of our data renders this

task even more challenging. Our sentences are of-
ten short and agrammatical (often missing or mis-
using determiners, for example), and are produced
by individuals with impaired language ability.
As such, our results are actually quite similar to

those reported in recent literature. Our average ac-
curacy of our FST n-gram (over both classes of
errors) selecting the appropriate word is ≈ 35%
while our LBL model’s performance of ≈ 43%
is comparable to the 5-gram LBL performance
of 49.3 reported by Mnih and Teh on the MSR
Sentence Completion Challenge dataset (Mnih and
Teh, 2012).

6 Conclusion & Future Work

While the system’s performance is quite good on
both intrinsic and extrinsic evaluation, there re-
mains much interesting work left to do on the prob-
lem.

6See De Mulder et al. (2015) for a recent review on this
subject.

We currently only evaluate sentences with a sin-
gle error, and more generally have not investigated
whether sentences with multiple errors are differ-
ent from mono-error sentences in terms of error
distribution or structure. However, our approach
should be able to generalize to sentences with ad-
ditional errors, and we will be investigating this in
future work.

Additionally, the AphasiaBank transcripts in-
clude phrasal dependency and part-of-speech tags
which we are currently not using. In future work
we will investigate including these as features in
language modelling, as there is some evidence that
this improves the conceptually related task of con-
textual spellcheck(Fossati and Di Eugenio, 2008).

There is quite a bit of work that can be done
on the language models as well. A more nuanced
approach to topic adaptation is worth investigat-
ing, and we plan to experiment with using non-
newswire corpora. Despite our attempts to focus
the corpus via LDA, there is a major difference be-
tween the written language of the NewYork Times,
and the spoken dialogue between the aphasic sub-
jects and their clinicians.

The most exciting area for further research is the
inclusion of semantic information in our classifica-
tion. While our topic-specific language model pro-
vides our model with some implicit semantic infor-
mation, amore principled approach to semantic rel-
evance could potentially improve the classification
of phonemic errors versus abstruse neologisms by
determining whether a given candidate word is se-
mantically relevant in context. More intriguingly,
it would give us a way to start investigating se-
mantic errors, and those errors that include “real”
words (for example, the previously discussed error
of replacing “dog” with “cat”).

One major limitation of our current system is
its reliance on human-produced transcriptions of
speech samples. In practice, transcription is rarely
feasible in clinical settings, and even in research
settings is often challenging, which may seem
to limit the applicability of our approach. No-
tably, however, our system does not require de-
tailed phonetic transcription, and merely requires
“best-guess” orthographic transcription of neolo-
gisms. As such, one could in principle use au-
tomatic speech recognition (ASR) to analyze a
recording of a patient or research subject, and pro-
duce a transcript on which our methods could be

6



run.7 Fraser et al. (2015) have had some success
at applying ASR to samples of aphasic speech and
performing downstream analysis on the resulting
transcripts, and we anticipate experimenting with
similar techniques in the future.

Acknowledgments

We thank the BioNLP reviewers for their helpful
comments and advice. This material is based upon
work supported in part by the National Institute
on Deafness and Other Communication Disorders
of the National Institutes of Health under awards
R01DC012033 and R03DC014556. The content
is solely the responsibility of the authors and does
not necessarily represent the official views of the
granting agencies or any other individual.

References
C. E. Brookshire, T. Conway, R. H. Pompon, M. Oelke,

and D. L. Kendall. 2014. Effects of intensive
phonomotor treatment on reading in eight individu-
als with aphasia and phonological alexia. American
Journal of Speech-Language Pathology 23(2):S300–
S311.

M. Brysbaert and B. New. 2009. Moving beyond
Kučera and Francis: A critical evaluation of current
word frequency norms and the introduction of a new
and improvedword frequencymeasure for American
English. Behavior Research Methods 41(4):977–
990.

Wim De Mulder, Steven Bethard, and Marie-Francine
Moens. 2015. A survey on the application of recur-
rent neural networks to statistical languagemodeling.
Computer Speech & Language 30(1):61–98.

S. T. Engelter, M. Gostynski, S. Papa, M. Frei, C. Born,
V. Ajdacic-Gross, F. Gutzwiller, and P. A. Lyrer.
2006. Epidemiology of aphasia attributable to first
ischemic stroke: Incidence, severity, fluency, etiol-
ogy, and thrombolysis. Stroke 37(6):1379–1384.

G. Fergadiotis, S. Kellough, and W. D. Hula. 2015.
Item Response Theory modeling of the Philadelphia
Naming Test. Journal of Speech, Language, and
Hearing Research 58(3):865–877.

Gerasimos Fergadiotis, Kyle Gorman, and Steven
Bedrick. 2016. Algorithmic Classification of Five
Characteristic Types of Paraphasias. American Jour-
nal of Speech-Language Pathology 25(4S):S776–12.

Davide Fossati and Barbara Di Eugenio. 2008. I saw
tree trees in the park: How to correct real-word
spelling mistakes. In LREC.
7Depending on the specifics of the ASR system, it may

in fact be possible to retain phonological information, which,
while not necessary, certainly could be helpful to our system.

K. C. Fraser, N. Ben-David, G. Hirst, N. Graham, and
E. Rochon. 2015. Sentence segmentation of aphasic
speech. In ACL. pages 862–871.

S. A. Frisch and R. Wright. 2002. The phonetics of
phonological speech errors: An acoustic analysis of
slips of the tongue. Journal of Phonetics 30(2):139–
162.

H. Goodglass and A. Wingfield. 1997. Anomia: Neu-
roanatomical and cognitive correlates. Academic
Press, New York.

W. D. Hula, S. Kellough, and G. Fergadiotis. 2015. De-
velopment and simulation testing of a computerized
adaptive version of the Philadelphia Naming Test.
Journal of Speech, Language, andHearing Research
58(3):878–890.

D. L. Kendall, R. H. Pompon, C. E. Brookshire, I.Mink-
ina, and L. Bislick. 2013. An analysis of apha-
sic naming errors as an indicator of improved lin-
guistic processing following phonomotor treatment.
American Journal of Speech-Language Pathology
22(2):S240–S249.

R Kneser and H Ney. 1995. Improved backing-off for
M-gram language modeling. In 1995 International
Conference on Acoustics, Speech, and Signal Pro-
cessing. IEEE, pages 181–184.

J. Kristensson, I. Behrns, and C. Saldert. 2015. Effects
on communication from intensive treatment with se-
mantic feature analysis in aphasia. Aphasiology
29(4):466–487.

B. MacWhinney, D. Fromm, M. Forbes, and A. Hol-
land. 2011. AphasiaBank: Methods for studying dis-
course. Aphasiology 25(11):1286–1307.

J. Mayer and L. Murray. 2003. Functional measures
of naming in aphasia: Word retrieval in confronta-
tion naming versus connected speech. Aphasiology
17(5):481–497.

I. Minkina, M. Oelke, L. P. Bislick, C. E. Brookshire,
R. Hunting Pompon, J. P. Silkes, and D. L. Kendall.
2015. An investigation of aphasic naming error evo-
lution following phonomotor treatment. Aphasiol-
ogy epub ahead of print.

D. Mirman, T. J. Strauss, A. Brecher, G. M. Walker,
P. Sobel, G. S. Dell, and M. F. Schwartz. 2010.
A large, searchable, web-based database of apha-
sic performance on picture naming and other tests
of cognitive function. Cognitive Neuropsychology
27(6):495–504.

Piotr Mirowski and Andreas Vlachos. 2015. Depen-
dency Recurrent Neural Language Models for Sen-
tence Completion. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers). Association for Computational Lin-
guistics, Beijing, China, pages 511–517.

7



Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In C J C Burges, L Bottou, M Welling,
Z Ghahramani, and K Q Weinberger, editors, Ad-
vances in Neural Information Processing Systems 26,
Curran Associates, Inc., pages 2265–2273.

Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic lan-
guage models. arXiv preprint arXiv:1206.6426 .

L. E. Nicholas, R. H.and Maclennan D. L. Brook-
shire, J. G. Schumacher, and S. A. Porrazzo. 1989.
Revised administration and scoring procedures for
the Boston Naming Test and norms for non-brain-
damaged adults. Aphasiology 3(6):569–580.

J. R. Novak, N. Minematsu, and K. Hirose. 2012.
WFST-based grapheme-to-phoneme conversion:
Open source tools for alignment, model-building
and decoding. In International Workshop on Finite
State Methods and Natural Language Processing.
pages 45–49.

R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011. English Gigaword 5th Edition. Linguistic
Data Consortium: LDC2011T07.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research
12:2825–2830.

B. Roark, R. Sproat, C. Allauzen, M. Riley, J. Sorensen,
and T. Tai. 2012. The OpenGrm open-source finite-
state grammar software libraries. In ACL. pages 61–
66.

C. Shannon. 1951. Prediction and entropy of printed
English. Bell System Technical Journal 50:50–64.

Geoffrey Zweig and Chris J C Burges. 2012. A Chal-
lenge Set for Advancing Language Modeling. In
Proceedings of the NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT . As-
sociation for Computational Linguistics, Montréal,
Canada, pages 29–36.

R. Řehůřek and P. Sojka. 2010. Software framework for
topic modelling with large corpora. In LREC. pages
45–50.

8


