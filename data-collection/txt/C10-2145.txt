



















































Citation Author Topic Model in Expert Search


Coling 2010: Poster Volume, pages 1265–1273,
Beijing, August 2010

Citation Author Topic Model in Expert Search

Yuancheng Tu, Nikhil Johri, Dan Roth, Julia Hockenmaier
University of Illinois at Urbana-Champaign

{ytu,njohri2,danr,juliahmr}@illinois.edu

Abstract

This paper proposes a novel topic model,
Citation-Author-Topic (CAT) model that
addresses a semantic search task we define
as expert search – given a research area as
a query, it returns names of experts in this
area. For example, Michael Collins would
be one of the top names retrieved given the
query Syntactic Parsing.

Our contribution in this paper is two-fold.
First, we model the cited author informa-
tion together with words and paper au-
thors. Such extra contextual information
directly models linkage among authors
and enhances the author-topic association,
thus produces more coherent author-topic
distribution. Second, we provide a prelim-
inary solution to the task of expert search
when the learning repository contains ex-
clusively research related documents au-
thored by the experts. When compared
with a previous proposed model (Johri
et al., 2010), the proposed model pro-
duces high quality author topic linkage
and achieves over 33% error reduction
evaluated by the standard MAP measure-
ment.

1 Introduction

This paper addresses the problem of searching for
people with similar interests and expertise, given
their field of expertise as the query. Many existing
people search engines need people’s names to do a

“keyword” style search, using a person’s name as
a query. However, in many situations, such infor-
mation is insufficient or impossible to know be-
forehand. Imagine a scenario where the statistics
department of a university invited a world-wide
known expert in Bayesian statistics and machine
learning to give a keynote speech; how can the
organizer notify all the people on campus who
are interested without spamming those who are
not? Our paper proposes a solution to the afore-
mentioned scenario by providing a search engine
which goes beyond “keyword” search and can re-
trieve such information semantically. The orga-
nizer would only need to input the research do-
main of the keynote speaker, i.e. Bayesian statis-
tics, machine learning, and all professors and stu-
dents who are interested in this topic will be re-
trieved and an email agent will send out the infor-
mation automatically.

Specifically, we propose a Citation-Author-
Topic (CAT) model which extracts academic re-
search topics and discovers different research
communities by clustering experts with similar in-
terests and expertise. CAT assumes three steps of
a hierarchical generative process when producing
a document: first, an author is generated, then that
author generates topics which ultimately generate
the words and cited authors. This model links
authors to observed words and cited authors via
latent topics and captures the intuition that when
writing a paper, authors always first have topics
in their mind, based on which, they choose words
and cite related works.

Corpus linguists or forensic linguists usually

1265



identify authorship of disputed texts based on
stylistic features, such as vocabulary size, sen-
tence length, word usage that characterize a spe-
cific author and the general semantic content is
usually ignored (Diederich et al., 2003). On the
other hand, graph-based and network based mod-
els ignore the content information of documents
and only focus on network connectivity (Zhang
et al., 2007; Jurczyk and Agichtein, 2007). In
contrast, the model we propose in this paper fully
utilizes the content words of the documents and
combines them with the stylistic flavor contex-
tual information to link authors and documents to-
gether to not only identify the authorship, but also
to be used in many other applications such as pa-
per reviewer recommendation, research commu-
nity identification as well as academic social net-
work search.

The novelty of the work presented in this pa-
per lies in the proposal of jointly modeling the
cited author information and using a discrimi-
native multinomial distribution to model the co-
author information instead of an artificial uni-
form distribution. In addition, we apply and eval-
uate our model in a semantic search scenario.
While current search engines cannot support in-
teractive and exploratory search effectively, our
model supports search that can answer a range of
exploratory queries. This is done by semantically
linking the interests of authors to the topics of the
collection, and ultimately to the distribution of the
words in the documents.

In the rest of this paper, we first present some
related work on author topic modeling and expert
search in Sec. 2. Then our model is described in
Sec. 3. Sec. 4 introduces our expert search system
and Sec. 5 presents our experiments and the evalu-
ation. We conclude this paper in Sec. 6 with some
discussion and several further developments.

2 Related Work

Author topic modeling, originally proposed
in (Steyvers et al., 2004; Rosen-Zvi et al., 2004),
is an extension of Latent Dirichlet Allocation
(LDA) (Blei et al., 2003), a probabilistic genera-
tive model that can be used to estimate the proper-
ties of multinomial observations via unsupervised
learning. LDA represents each document as a

mixture of probabilistic topics and each topic as
a multinomial distribution over words. The Au-
thor topic model adds an author layer over LDA
and assumes that the topic proportion of a given
document is generated by the chosen author.

Author topic analysis has attracted much atten-
tion recently due to its broad applications in ma-
chine learning, text mining and information re-
trieval. For example, it has been used to pre-
dict authors for new documents (Steyvers et al.,
2004), to recommend paper reviewers (Rosen-Zvi
et al., 2004), to model message data (Mccallum et
al., 2004), to conduct temporal author topic anal-
ysis (Mei and Zhai, 2006), to disambiguate proper
names (Song et al., 2007), to search academic so-
cial networks (Tang et al., 2008) and to generate
meeting status analyses for group decision mak-
ing (Broniatowski, 2009).

In addition, there are many related works on
expert search at the TREC enterprise track from
2005 to 2007, which focus on enterprise scale
search and discovering relationships between enti-
ties. In that setting, the task is to find the experts,
given a web domain, a list of candidate experts
and a set of topics 1. The task defined in our paper
is different in the sense that our topics are hid-
den and our document repositories are more ho-
mogeneous since our documents are all research
papers authored by the experts. Within this set-
ting, we can explore in depth the influence of the
hidden topics and contents to the ranking of our
experts. Similar to (Johri et al., 2010), in this pa-
per we apply CAT in a semantic retrieval scenario,
where searching people is associated with a set of
hidden semantically meaningful topics instead of
their personal names.

In recent literature, there are three main lines of
work that extend author topic analyses. One line
of work is to relax the model’s “bag-of-words”
assumption by automatically discovering multi-
word phrases and adding them into the original
model (Johri et al., 2010). Similar work has also
been proposed for other topic models such as
Ngram topic models (Wallach, 2006; Wang and
McCallum, 2005; Wang et al., 2007; Griffiths et
al., 2007).

1http://trec.nist.gov/pubs.html

1266



Another line of work models authors informa-
tion as a general contextual information (Mei and
Zhai, 2006) or associates documents with network
structure analysis (Mei et al., 2008; Serdyukov et
al., 2008; Sun et al., 2009). This line of work
aims to propose a general framework to deal with
collections of texts with an associated networks
structure. However, it is based on a different topic
model than ours; for example, Mei’s works (Mei
and Zhai, 2006; Mei et al., 2008) extend proba-
bilistic latent semantic analysis (PLSA), and do
not have cited author information explicitly.

Our proposal follows the last line of work
which extends author topic modeling with spe-
cific contextual information and directly captures
the association between authors and topics to-
gether with this contextual information (Tang et
al., 2008; Mccallum et al., 2004). For exam-
ple, in (Tang et al., 2008), publication venue is
added as one extra piece of contextual informa-
tion and in (Mccallum et al., 2004), email recip-
ients, which are treated as extra contextual infor-
mation, are paired with email authors to model an
email message corpus. In our proposed method,
the extra contextual information consists of the
cited authors in each documents. Such contextual
information directly captures linkage among au-
thors and cited authors, enhances author-topic as-
sociations, and therefore produces more coherent
author-topic distributions.

3 The Citation-Author-Topic (CAT)
Model

CAT extends previously proposed author topic
models by explicitly modelling the cited author
information during the generative process. Com-
pared with these models (Rosen-Zvi et al., 2004;
Johri et al., 2010), whose plate notation is shown
in Fig. 1, CAT (shown in Fig. 2) adds cited au-
thor information and generates authors according
to the observed author distribution.

Four plates in Fig. 1 represent topic (T ), au-
thor (A), document (D) and words in each doc-
ument (Nd) respectively. CAT (Fig. 2) has one
more plate, cited-author topic plate, in which each
topic is represented as a multinomial distribution
over all cited authors (λc).

Within CAT, each author is associated with a

�D A N d
Figure 1: Plate notation of the previously pro-
posed author topic models (Rosen-Zvi et al.,
2004; Johri et al., 2010).

�
D

A
N d�

Figure 2: Plate notation of our current model:
CAT generates words W and cited authors C in-
dependently given the topic.

multinomial distribution over all topics, ~θa, and
each topic is a multinomial distribution over all
words, ~φt, as well as a multinomial distribution
over all cited authors ~λc. Three symmetric Dirich-
let conjugate priors, η, β and γ, are defined for
each of these three multinomial distributions in
CAT as shown in Fig. 2.

The generative process of CAT is formally de-
fined in Algorithm 1. The model first samples
the word-topic, cited author-topic and the author-
topic distributions according to the three Dirich-
let hyperparameters. Then for each word in each
document, first the author k is drawn from the
observed multinomial distribution and that author
chooses the topic zi, based on which word wi and
cited author ci are generated independently.

CAT differs from previously proposed MAT
(Multiword-enhanced Author Topic) model (Johri
et al., 2010) in two aspects. First of all, CAT uses

1267



Algorithm 1: CAT: A, T ,D,N are four
plates as shown in Fig. 2. The generative pro-
cess of CAT modeling.
Data: A, T ,D,N
for each topic t ∈ T do

draw a distribution over words:
~φt ∼ DirN (β) ;
draw a distribution over cited authors:
~λc ∼ DirC(γ) ;

for each author a ∈ A do
draw a distribution over topics:
~θa ∼ DirT (η) ;

for each document d ∈ D and k authors ∈ d
do

for each word w ∈ d do
choose an author
k ∼ Multinomial(Ad) ;
assign a topic i given the author:
zk,i|k ∼ Multinomial(θa) ;
draw a word from the chosen topic:
wd,k,i|zk,i ∼ Multinomial(φzk,i) ;
draw a cited author from the topic:
cd,k,i|zk,i ∼ Multinomial(λzk,i)

cited author information to enhance the model
and assumes independence between generating
the words and cited authors given the topic. Sec-
ondly, instead of an artificial uniform distribution
over all authors and co-authors, CAT uses the ob-
served discriminative multinomial distribution to
generate authors.

3.1 Parameter Estimation

CAT includes three sets of parameters. The T
topic distribution over words, φt which is similar
to that in LDA. The author-topic distribution θa as
well as the cited author-topic distribution λc. Al-
though CAT is a relatively simple model, finding
its posterior distribution over these hidden vari-
ables is still intractable due to their high dimen-
sionality. Many efficient approximate inference
algorithms have been used to solve this problem
including Gibbs sampling (Griffiths and Steyvers,
2004; Steyvers and Griffiths, 2007; Griffiths et al.,
2007) and mean-field variational methods (Blei et
al., 2003). Gibbs sampling is a special case of

Markov-Chain Monte Carlo (MCMC) sampling
and often yields relatively simple algorithms for
approximate inference in high dimensional mod-
els.

In our CAT modeling, we use a collapsed Gibbs
sampler for our parameter estimation. In this
Gibbs sampler, we integrated out the hidden vari-
ables θ, φ and λ using the Dirichlet delta func-
tion (Heinrich, 2009). The Dirichlet delta func-
tion with an M dimensional symmetric Dirichlet
prior δ is defined as:

∆M (δ) =
Γ
(
δM

)

Γ (Mδ)

Based on the independence assumptions de-
fined in Fig. 2, the joint distribution of topics,
words and cited authors given all hyperparame-
ters which originally represented by integrals can
be transformed into the delta function format and
formally derived in Equation 1.

P (~z, ~w,~c|β, η, λ) (1)
= P (~z|β, η, λ)P (~w,~c|~z, β, η, λ)
= P (~z)P (~w|~z)P (~c|~z)

=
A∏

a=1

∆(nA+η)
∆(η)

T∏
z=1

∆(nZw+β)
∆(β)

T∏
z=1

∆(nZc+λ)
∆(λ)

The updating equation from which the Gibbs
sampler draws the hidden variable for the current
state j, i.e., the conditional probability of drawing
the kth author Kkj , the i

th topic Zij , and the c
th

cited author Ccj tuple, given all the hyperparame-
ters and all the observed documents and authors,
cited authors except the current assignment (the
exception is denoted by the symbol ∀¬j), is de-
fined in Equation 2.

P (Zij ,K
k
j , C

c
j |Wwj ,∀¬j, Ad, β, η, γ) (2)

∝ ∆(nZ+β)
∆(nZ,¬j+β)

∆(nK+η)

∆(nK,¬j+η)
∆(nC+γ)

∆(nC,¬j+γ)

=
nwi,¬j+βw

V
P

w=1
nwi,¬j+V βw

nik,¬j+ηi
T
P

i=1
nik,¬j+Tηi

nci,¬j+λc
C
P

c=1
nci,¬j+Cλc

The parameter sets φ and θ, λ can be interpreted
as sufficient statistics on the state variables of
the Markov Chain due to the Dirichlet conjugate
priors we used for the multinomial distributions.

1268



These three sets of parameters are estimated based
on Equations 3 , 4 and 5 respectively, in which nwi
is defined as the number of times the word w is
generated by topic i; nik is defined as the number
of times that topic i is generated by author k and
nic is defined as the number of times that the cited
author c is generated by topic i. The vocabulary
size is V , the number of topics is T and the cited-
author size is C.

φw,i =
nwi + βw

V∑
w=1

nwi + V βw

(3)

θk,i =
nik + ηi

T∑
i=1

nik + Tηi

(4)

λc,i =
nci + λc

C∑
c=1

nci + Cλc

(5)

The Gibbs sampler used in our experiments is
adapted from the Matlab Topic Modeling Tool-
box 2.

4 Expert Search

In this section, we describe a preliminary re-
trieval system that supports expert search, which
is intended to identify groups of research experts
with similar research interests and expertise by in-
putting only general domain key words. For ex-
ample, we can retrieve Michael Collins via search
for natural language parsing.

Our setting is different from the standard TREC
expert search in that we do not have a pre-defined
list of experts and topics, and our documents are
all research papers authored by experts. Within
this setting, we do not need to identify the status of
our experts, i.e., a real expert or a communicator,
as in TREC expert search. All of our authors and
cited authors are experts and the task amounts to
ranking the experts according to different topics
given samples of their research papers.

The ranking function of this retrieval model is
derived through the CAT parameters. The search

2http://psiexp.ss.uci.edu/research/programs data/

aims to link research topics with authors to by-
pass the proper names of these authors. Our re-
trieval function ranks the joint probability of the
query words (W ) and the target author (a), i.e.,
P (W,a). This probability is marginalized over all
topics, and the probability that an author is cited
given the topic is used as an extra weight in our
ranking function. The intuition is that an author
who is cited frequently should be more prominent
and ranked higher. Formally, we define the rank-
ing function of our retrieval system in Equation 6.
ca denotes when the author is one of the cited au-
thors in our corpus. CAT assumes that words and
authors, and cited authors are conditionally inde-
pendent given the topic, i.e., wi ⊥ a ⊥ ca.

P (W,a) =
∑

wi

αi
∑

t

P (wi, a|t, ca)P (t, ca)

=
∑

wi

αi
∑

t

P (wi|t)P (a|t)P (ca|t)P (t)

(6)

W is the input query, which may contain one or
more words. If a multiword is detected within the
query, it is added into the query. The final score
is the sum of all words in this query weighted by
their inverse document frequency αi.

In our experiments, we chose ten queries which
cover several popular research areas in computa-
tional linguistics and natural language processing
and run the retrieval system based on three mod-
els: the original author topic model (Rosen-Zvi
et al., 2004), the MAT model (Johri et al., 2010)
and the CAT model. In the original author topic
model, query words are treated token by token.
Both MAT and CAT expand the query terms with
multiwords if they are detected inside the original
query. For each query, top 10 authors are returned
from the system. We manually label the relevance
of these 10 authors based on the papers collected
in our corpus.

Two standard evaluation metrics are used to
measure the retrieving results. First we evaluate
the precision at a given cut-off rank, namely pre-
cision at rank k with k ranging from 1 to 10. We
then calculate the average precision (AP) for each
query and the mean average precision (MAP) for

1269



the queries. Unlike precision at k, MAP is sensi-
tive to the ranking and captures recall information
since it assumes the precision of the non-retrieved
documents to be zero. It is formally defined as
the average of precisions computed at the point of
each of the relevant documents in the ranked list
as shown in Equation 7.

AP =

∑n
r=1(Precision(r)× rel(r))
| relevant documents | (7)

To evaluate the recall of our system, we col-
lected a pool of authors for six of our queries re-
turned from an academic search engine, Arnet-
Miner (Tang et al., 2008)3 as our reference author
pool and evaluate our recall based on the number
of authors we retrieved from that pool.

5 Experiments and Analysis

In this section, we describe the empirical evalua-
tion of our model qualitatively and quantitatively
by applying our model to the expert search we de-
fined in Sec. 4. We compare the retrieving results
with two other models: Multiword- enhanced Au-
thor Topic (MAT) model (Johri et al., 2010) and
the original author topic model (Rosen-Zvi et al.,
2004).

5.1 Data set and Pre-processing

We crawled the ACL anthology website and col-
lected papers from ACL, EMNLP and CONLL
over a period of seven years. The ACL anthol-
ogy website explicitly lists each paper together
with its title and author information. Therefore,
the author information of each paper can be ob-
tained accurately without extracting it from the
original paper. However, many author names are
not represented consistently. For example, the
same author may have his/her middle name listed
in some papers, but not in others. We therefore
normalized all author names by eliminating mid-
dle names from all authors.

Cited authors of each paper are extracted from
the reference section and automatically identified
by a named entity recognizer tuned for citation ex-
traction (Ratinov and Roth, 2009). Similar to reg-
ular authors, all cited authors are also normalized

3http://www.arnetminer.org

Conf. Year Paper Author uni. Vocab.
ACL 03-09 1,326 2,084 34,012 205,260
EMNLP 93-09 912 1,453 40,785 219,496
CONLL 97-09 495 833 27,312 123,176
Total 93-09 2,733 2,911 62,958 366,565

Table 1: Statistics about our data set. Uni. denotes
unigram words and Vocab. denotes all unigrams
and multiword phrases discovered in the data set.

with their first name initial and their full last name.
We extracted about 20,000 cited authors from our
corpus. However, for the sake of efficiency, we
only keep those cited authors whose occurrence
frequency in our corpus is above a certain thresh-
old. We experimented with thresholds of 5, 10 and
20 and retained the total number of 2,996, 1,771
and 956 cited authors respectively.

We applied the same strategy to extract mul-
tiwords from our corpus and added them into
our vocabulary to implement the model described
in (Johri et al., 2010). Some basic statistics about
our data set are summarized in Table 1 4.

5.2 Qualitative Coherence Analysis

As shown by other previous works (Wallach,
2006; Griffiths et al., 2007; Johri et al., 2010),
our model also demonstrates that embedding mul-
tiword tokens into the model can achieve more co-
hesive and better interpretable topics. We list the
top 10 words from two topics of CAT and compare
them with those from the unigram model in Ta-
ble 2. Unigram topics contain more general words
which can occur in every topic and are usually less
discriminative among topics.

Our experiments also show that CAT achieves
better retrieval quality by modeling cited authors
jointly with authors and words. The rank of an
author is boosted if that author is cited more fre-
quently. We present in Table 3 the ranking of one
of our ten query terms to demostrate the high qual-
ity of our proposed model. When compared to the
model without cited author information, CAT not
only retrieves more comprehensive expert list, its
ranking is also more reasonable than the model
without cited author information.

Another observation in our experiments is that

4Download the data and the software package at:
http://L2R.cs.uiuc.edu/∼cogcomp/software.php.

1270



Query term: parsing
Proposed CAT Model Model without cited authors

Rank Author Prob. Author Prob.
1 J. Nivre 0.125229 J. Nivre 0.033200
2 C. Manning 0.111252 R. Barzilay 0.023863
3 M. Johnson 0.101342 M. Johnson 0.023781
4 J. Eisner 0.063528 D. Klein 0.018937
5 M. Collins 0.047347 R. McDonald 0.017353
6 G. Satta 0.042081 L. Marquez 0.016003
7 R. McDonald 0.041372 A. Moschitti 0.015781
8 D. Klein 0.041149 N. Smith 0.014792
9 K. Toutanova 0.024946 C. Manning 0.014040
10 E. Charniak 0.020843 K. Sagae 0.013384

Table 3: Ranking for the query term: parsing. CAT achieves more comprehensive and reasonable rank
list than the model without cited author information.

CAT Uni. AT Model
TOPIC 49 Topic 27
pronoun resolution anaphor
antecedent antecedents
coreference resolution anaphoricity
network anphoric
resolution is
anaphor anaphora
pronouns soon
anaphor antecedent determination
semantic knowledge pronominal
proper names salience
TOPIC 14 Topic 95
translation quality hypernym
translation systems seeds
source sentence taxonomy
word alignments facts
paraphrases hyponym
decoder walk
parallel corpora hypernyms
translation system page
parallel corpus logs
translation models extractions

Table 2: CAT with embedded multiword com-
ponents achieves more interpretable topics com-
pared with the unigram Author Topic (AT) model.

some experts who published many papers, but on
heterogeneous topics, may not be ranked at the
very top by models without cited author infor-
mation. However, with cited author information,
those authors are ranked higher. Intuitively this
makes sense since many of these authors are also
the most cited ones.

5.3 Quantitative retrieval results

One annotator labeled the relevance of the re-
trieval results from our expert search system. The
annotator was also given all the paper titles of each

Precision@K
K CAT Model Model w/o Cited Authors
1 0.80 0.80
2 0.80 0.70
3 0.73 0.60
4 0.70 0.50
5 0.68 0.48
6 0.70 0.47
7 0.69 0.40
8 0.68 0.45
9 0.73 0.44

10 0.70 0.44

Table 4: Precision at K evaluation of our proposed
model and the model without cited author infor-
mation.

corresponding retrieved author to help make this
binary judgment. We experiment with ten queries
and retrieve the top ten authors for each query.

We first used the precision at k for evaluation.
We calculate the precision at k for both our pro-
posed CAT model and the MAT model, which
does not have the cited author information. The
results are listed in Table 4. It can be observed
that at every rank position, our CAT model works
better. In order to focus more on relevant retrieval
results, we also calculated the mean average pre-
cision (MAP) for both models. For the given ten
queries, the MAP score for the CAT model is 0.78,
while the MAT model without cited author infor-
mation has a MAP score of 0.67. The CAT model
with cited author information achieves about 33%
error reduction in this experiment.

1271



Query ID Query Term

1 parsing
2 machine translation
3 dependency parsing
4 transliteration
5 semantic role labeling
6 coreference resolution
7 language model
8 Unsupervised Learning
9 Supervised Learning
10 Hidden Markov Model

Table 5: Queries and their corresponding ids we
used in our experiments.

Recall for each query
Query ID CAT Model Model w/o Cite

1 0.53 0.20
2 0.13 0.20
3 0.27 0.13
4 0.13 0.2
5 0.27 0.20
6 0.13 0.26

Average 0.24 0.20

Table 6: Recall comparison between our proposed
model and the model without cited author infor-
mation.

Since we do not have a gold standard experts
pool for our queries, to evaluate recall, we col-
lected a pool of authors returned from an aca-
demic search engine, ArnetMiner (Tang et al.,
2008) as our reference author pool and evaluated
our recall based on the number of authors we re-
trieved from that pool. Specifically, we get the
top 15 returned persons from that website for each
query and treat them as the whole set of relevant
experts for that query and our preliminary recall
results are shown in Table 6.

In most cases, the CAT recall is better than that
of the compared model, and the average recall is
better as well. All the queries we used in our ex-
periments are listed in Table 5. And the average
recall value is based on six of the queries which
have at least one overlap author with those in our
reference recall pool.

6 Conclusion and Further Development

This paper proposed a novel author topic model,
CAT, which extends the existing author topic
model with additional cited author information.

We applied it to the domain of expert retrieval
and demonstrated the effectiveness of our model
in improving coherence in topic clustering and au-
thor topic association. The proposed model also
provides an effective solution to the problem of
community mining as shown by the promising re-
trieval results derived in our expert search system.

One immediate improvement would result from
extending our corpus. For example, we can ap-
ply our model to the ACL ARC corpus (Bird et
al., 2008) to check the model’s robustness and en-
hance the ranking by learning from more data. We
can also apply our model to data sets with rich
linkage structure, such as the TREC benchmark
data set or ACL Anthology Network (Radev et al.,
2009) and try to enhance our model with the ap-
propriate network analysis.

Acknowledgments

The authors would like to thank Lev Ratinov for
his help with the use of the NER package and the
three anonymous reviewers for their helpful com-
ments and suggestions. The research in this pa-
per was supported by the Multimodal Information
Access & Synthesis Center at UIUC, part of CCI-
CADA, a DHS Science and Technology Center of
Excellence.

References

Bird, S., R. Dale, B. Dorr, B. Gibson, M. Joseph,
M. Kan, D. Lee, B Powley, D. Radev, and Y. Tan.
2008. The acl anthology reference corpus: A refer-
ence dataset for bibliographic research in computa-
tional linguistics. In Proceedings of LREC’08.

Blei, D., A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.

Broniatowski, D. 2009. Generating status hierar-
chies from meeting transcripts using the author-
topic model. In In Proceedings of the Workshop:
Applications for Topic Models: Text and Beyond.

Diederich, J., J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with support
vector machines. Applied Intelligence, 19:109–123.

1272



Griffiths, T. and M. Steyvers. 2004. Finding scientific
topic. In Proceedings of the National Academy of
Science.

Griffiths, T., M. Steyvers, and J. Tenenbaum. 2007.
Topics in semantic representation. Psychological
Review.

Heinrich, G. 2009. Parameter estimation for text anal-
ysis. Technical report, Fraunhofer IGD.

Johri, N., D. Roth, and Y. Tu. 2010. Experts’ retrieval
with multiword-enhanced author topic model. In
Proceedings of NAACL-10 Semantic Search Work-
shop.

Jurczyk, P. and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using
link analysis. In Proceedings of CIKM’07.

Mccallum, A., A. Corrada-emmanuel, and X. Wang.
2004. The author-recipient-topic model for topic
and role discovery in social networks: Experiments
with enron and academic email. Technical report,
University of Massachusetts Amherst.

Mei, Q. and C. Zhai. 2006. A mixture model for con-
textual text mining. In Proceedings of KDD-2006,
pages 649–655.

Mei, Q., D. Cai, D. Zhang, and C. Zhai. 2008. Topic
modeling with network regularization. In Proceed-
ing of WWW-08:, pages 101–110.

Radev, D., M. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis
of the field of Computational Linguistics. Journal
of the American Society for Information Science and
Technology.

Ratinov, L. and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL).

Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smyth.
2004. the author-topic model for authors and docu-
ments. In Proceedings of UAI.

Serdyukov, P., H. Rode, and D. Hiemstra. 2008. Mod-
eling multi-step relevance propagation for expert
finding. In Proceedings of CIKM’08.

Song, Y., J. Huang, and I. Councill. 2007. Efficient
topic-based unsupervised name disambiguation. In
Proceedings of JCDL-2007, pages 342–351.

Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.

Steyvers, M., P. Smyth, and T. Griffiths. 2004. Proba-
bilistic author-topic models for information discov-
ery. In Proceedings of KDD.

Sun, Y., J. Han, J. Gao, and Y. Yu. 2009. itopicmodel:
Information network-integrated topic modeling. In
Proceedings of ICDM-2009.

Tang, J., J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
2008. Arnetminer: Extraction and mining of aca-
demic social networks. In Proceedings of KDD-
2008, pages 990–998.

Wallach, H. 2006. Topic modeling; beyond bag of
words. In International Conference on Machine
Learning.

Wang, X. and A. McCallum. 2005. A note on topi-
cal n-grams. Technical report, University of Mas-
sachusetts.

Wang, X., A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discoery with an appli-
cation to information retrieval. In Proceedings of
ICDM.

Zhang, J., M. Ackerman, and L. Adamic. 2007. Ex-
pertise networks in online communities: Structure
and algorithms. In Proceedings of WWW 2007.

1273


