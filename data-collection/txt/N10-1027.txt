










































Language Identification: The Long and the Short of the Matter


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 229–237,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Language Identification: The Long and the Short of the Matter

Timothy Baldwin and Marco Lui

Dept of Computer Science and Software Engineering

University of Melbourne, VIC 3010 Australia

tb@ldwin.net, saffsd@gmail.com

Abstract

Language identification is the task of identify-

ing the language a given document is written

in. This paper describes a detailed examina-

tion of what models perform best under dif-

ferent conditions, based on experiments across

three separate datasets and a range of tokeni-

sation strategies. We demonstrate that the task

becomes increasingly difficult as we increase

the number of languages, reduce the amount

of training data and reduce the length of docu-

ments. We also show that it is possible to per-

form language identification without having to

perform explicit character encoding detection.

1 Introduction

With the growth of the worldwide web, ever-

increasing numbers of documents have become

available, in more and more languages. This growth

has been a double-edged sword, however, in that

content in a given language has become more preva-

lent but increasingly hard to find, due to the web’s

sheer size and diversity of content. While the ma-

jority of (X)HTML documents declare their charac-

ter encoding, only a tiny minority specify what lan-

guage they are written in, despite support for lan-

guage declaration existing in the various (X)HTML

standards.1 Additionally, a single encoding can gen-

erally be used to render a large number of languages

such that the document encoding at best filters out

a subset of languages which are incompatible with

the given encoding, rather than disambiguates the

source language. Given this, the need for automatic

means to determine the source language of web doc-

1http://dev.opera.com/articles/view/

mama-head-structure/

uments is crucial for web aggregators of various

types.

There is widespread misconception of language

identification being a “solved task”, generally as

a result of isolated experiments over homogeneous

datasets with small numbers of languages (Hughes

et al., 2006; Xia et al., 2009). Part of the motivation

for this paper is to draw attention to the fact that, as

a field, we are still a long way off perfect language

identification of web documents, as evaluated under

realistic conditions.

In this paper we describe experiments on lan-

guage identification of web documents, focusing on

the broad question of what combination of tokenisa-

tion strategy and classification model achieves the

best overall performance. We additionally evalu-

ate the impact of the volume of training data and

the test document length on the accuracy of lan-

guage identification, and investigate the interaction

between character encoding detection and language

identification.

One assumption we make in this research, follow-

ing standard assumptions made in the field, is that all

documents are monolingual. This is clearly an un-

realistic assumption when dealing with general web

documents (Hughes et al., 2006), and we plan to re-

turn to investigate language identification over mul-

tilingual documents in future work.

Our contributions in this paper are: the demon-

stration that language identification is: (a) trivial

over datasets with smaller numbers of languages

and approximately even amounts of training data per

language, but (b) considerably harder over datasets

with larger numbers of languages with more skew

in the amount of training data per language; byte-

based tokenisation without character encoding de-

tection is superior to codepoint-based tokenisation

229



with character encoding detection; and simple co-

sine similarity-based nearest neighbour classifica-

tion is equal to or better than models including sup-

port vector machines and naive Bayes over the lan-

guage identification task. We also develop datasets

to facilitate standardised evaluation of language

identification.

2 Background Research

Language identification was arguably established as

a task by Gold (1967), who construed it as a closed

class problem: given data in each of a predefined set

of possible languages, human subjects were asked

to classify the language of a given test document. It

wasn’t until the 1990s, however, that the task was

popularised as a text categorisation task.

The text categorisation approach to language

identification applies a standard supervised classi-

fication framework to the task. Perhaps the best-

known such model is that of Cavnar and Tren-

kle (1994), as popularised in the textcat tool.2

The method uses a per-language character frequency

model, and classifies documents via their relative

“out of place” distance from each language (see

Section 5.1). Variants on this basic method in-

clude Bayesian models for character sequence pre-

diction (Dunning, 1994), dot products of word fre-

quency vectors (Darnashek, 1995) and information-

theoretic measures of document similarity (Aslam

and Frost, 2003; Martins and Silva, 2005). More

recently, support vector machines (SVMs) and ker-

nel methods have been applied to the task of lan-

guage identification task with success (Teytaud and

Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al.,

2005), and Markov logic has been used for joint in-

ferencing in contexts where there are multiple evi-

dence sources (Xia et al., 2009).

Language identification has also been carried out

via linguistically motivated models. Johnson (1993)

used a list of stop words from different languages to

identify the language of a given document, choos-

ing the language with the highest stop word over-

lap with the document. Grefenstette (1995) used

word and part of speech (POS) correlation to de-

termine if two text samples were from the same

or different languages. Giguet (1995) developed a

2http://www.let.rug.nl/vannoord/TextCat/

cross-language tokenisation model and used it to

identify the language of a given document based

on its tokenisation similarity with training data.

Dueire Lins and Gonçalves (2004) considered the

use of syntactically-derived closed grammatical-

class models, matching syntactic structure rather

than words or character sequences.

The observant reader will have noticed that some

of the above approaches make use of notions such

as “word”, typically based on the naive assumption

that the language uses white space to delimit words.

These approaches are appropriate in contexts where

there is a guarantee of a document being in one of

a select set of languages where words are space-

delimited, or where manual segmentation has been

performed (e.g. interlinear glossed text). However,

we are interested in language identification of web

documents, which can be in any language, includ-

ing languages that do not overtly mark word bound-

aries, such as Japanese, Chinese and Thai; while

relatively few languages fall into this categories,

they are among the most populous web languages

and therefore an important consideration. There-

fore, approaches that assume a language is space-

delimited are clearly not suitable for our purposes.

Equally, approaches which make assumptions about

the availability of particular resources for each lan-

guage to be identified (e.g. POS taggers, or the ex-

istence of precompiled stop word lists) cannot be

used.

Language identification has been applied in a

number of contexts, the most immediate applica-

tion being in multilingual text retrieval, where re-

trieval results are generally superior if the language

of the query is known, and the search is restricted

to only those documents predicted to be in that lan-

guage (McNamee and Mayfield, 2004). It can also

be used to “word spot” foreign language terms in

multilingual documents, e.g. to improve parsing per-

formance (Alex et al., 2007), or for linguistic corpus

creation purposes (Baldwin et al., 2006; Xia et al.,

2009; Xia and Lewis, 2009).

3 Datasets

In the experiments reported in this paper, we em-

ploy three novel datasets, with differing properties

relevant to language identification research:

230



Corpus Documents Languages Encodings Document Length (bytes)

EUROGOV 1500 10 1 17460.5±39353.4
TCL 3174 60 12 2623.2±3751.9

WIKIPEDIA 4963 67 1 1480.8±4063.9

Table 1: Summary of the three language identification datasets

Figure 1: Distribution of languages in the three datasets

(vector of languages vs. the proportion of documents in

that language)

EUROGOV: longer documents, all in a single en-

coding, spread evenly across a relatively small num-

ber (10) of Western European languages; this dataset

is comparable to the datasets conventionally used in

language identification research. As the name would

suggest, the documents were sourced from the Euro-

GOV document collection, as used in the 2005 Web-

CLEF task.

TCL: a larger number of languages (60) across a

wider range of language families, with shorter docu-

ments and a range of character encodings (12). The

collection was manually sourced by the Thai Com-

putational Linguistics Laboratory (TCL) in 2005

from online news sources.

WIKIPEDIA: a slightly larger number of lan-

guages again (67), a single encoding, and shorter

documents; the distribution of languages is intended

to approximate that of the actual web. This col-

lection was automatically constructed by taking the

dumps of all versions of Wikipedia with 1000 or

more documents in non-constructed languages, and

randomly selecting documents from them in a bias-

preserving manner (i.e. preserving the document

distribution in the full collection); this is intended to

represent the document language bias observed on

the web. All three corpora are available on request.

We outline the characteristics of the three datasets

in Table 1. We further detail the language distri-

bution in Figure 1, using a constant vector of lan-

guages for all three datasets, based on the order of

languages in the WIKIPEDIA dataset (in descending

order of documents per language). Of note are the

contrasting language distributions between the three

datasets, in terms of both the languages represented

and the relative skew of documents per language. In

the following sections, we provide details of the cor-

pus compilation and document sampling method for

each dataset.

4 Document Representation

As we are interested in performing language iden-

tification over arbitrary web documents, we re-

quire a language-neutral document representation

which does not make artificial assumptions about the

source language of the document. Separately, there

is the question of whether it is necessary to deter-

mine the character encoding of the document in or-

der to extract out character sequences, or whether

the raw byte stream is sufficient. To explore this

question, we experiment with two document repre-

sentations: (1) byte n-grams, and (2) codepoint n-

grams. In both cases, a document is represented as a

feature vector of token counts.

Byte n-grams can be extracted directly without

explicit encoding detection. Codepoint n-grams, on

the other hand, require that we know the character

encoding of the document in order to perform to-

kenisation. Additionally, they should be based on a

common encoding to prevent: (a) over-fragmenting

the feature space (e.g. ending up with discrete fea-

ture spaces for euc-jp, s-jis and utf-8 in

the case of Japanese); and (b) spurious matches be-

tween encodings (e.g. Japanese hiragana and Ko-

rean hangul mapping onto the same codepoint in

euc-jp and euc-kr, respectively). We use uni-

231



code as the common encoding for all documents.

In practice, character encoding detection is an is-

sue only for TCL, as the other two datasets are in

a single encoding. Where a character encoding was

provided for a document in TCL and it was possi-

ble to transcode the document to unicode based on

that encoding, we used the encoding information. In

cases where a unique encoding was not provided,

we used an encoding detection library based on the

Mozilla browser.3 Having disambiguated the encod-

ing for each document, we transcoded it into uni-

code.

5 Models

In our experiments we use a number of different

language identification models, as outlined below.

We first describe the nearest-neighbour and nearest-

prototype models, and a selection of distance and

similarity metrics combined with each. We then

present three standalone text categorisation models.

5.1 Nearest-Neighbour and Nearest-Prototype

Models

The 1-nearest-neighbour (1NN) model is a common

classification technique, whereby a test document

D is classified based on the language of the clos-

est training document Di (with language l(Di)), as
determined by a given distance or similarity metric.

In nearest-neighbour models, each training doc-

ument is represented as a single instance, mean-

ing that the computational cost of classifying a test

document is proportional to the number of training

documents. A related model which aims to reduce

this cost is nearest-prototype (AM), where each lan-

guage is represented as a single instance, by merging

all of the training instances for that language into a

single centroid via the arithmetic mean.

For both nearest-neighbour and nearest-prototype

methods, we experimented with three similarity and

distance measures in this research:

Cosine similarity (COS): the cosine of the angle

between two feature vectors, as measured by the dot

product of the two vectors, normalised to unit length.

Skew divergence (SKEW): a variant of Kullback-

Leibler divergence, whereby the second distribution

3http://chardet.feedparser.org/

(y) is smoothed by linear interpolation with the first

(x) using a smoothing factor α (Lee, 2001):

sα(x, y) = D(x || αy + (1− α)x)

where:

D(x || y) =
∑

i

xi(log2 xi − log2 yi)

In all our experiments, we set α to 0.99.

Out-of-place (OOP): a ranklist-based distance

metric, where the distance between two documents

is calculated as (Cavnar and Trenkle, 1994):

oop(Dx, Dy) =
∑

t∈Dx∨Dy

abs(RDx(t)−RDy(t))

RD(t) is the rank of term t in document D, based
on the descending order of frequency in document

D; terms not occurring in document D are conven-

tionally given the rank 1 + maxi RD(ti).

5.2 Naive Bayes (NB)

Naive Bayes is a popular text classification model,

due to it being lightweight, robust and easy to up-

date. The language of test document D is predicted

by:

l̂(D) = arg max
li∈L

P (li)

|V |∏

j=1

P (tj |li)
ND,tj

ND,tj !

where L is the set of languages in the training data,

ND,tj is the frequency of the jth term in D, V is the

set of all terms, and:

P (t|li) =
1 +

∑|D |
k=1 Nk,tP (li|Dk)

|V |+
∑|V |

j=1

∑|D |
k=1 Nk,tjP (li|Dk)

In this research, we use the rainbow imple-

mentation of multinominal naive Bayes (McCallum,

1996).

5.3 Support Vector Machines (SVM)

Support vector machines (SVMs) are one of the

most popular methods for text classification, largely

because they can automatically weight large num-

bers of features, capturing feature interactions in the

process (Joachims, 1998; Manning et al., 2008). The

basic principle underlying SVMs is to maximize the

232



margin between training instances and the calculated

decision boundary based on structural risk minimi-

sation (Vapnik, 1995).

In this work, we have made use of bsvm,4 an

implementation of SVMs with multiclass classifica-

tion support (Hsu et al., 2008). We only report re-

sults for multi-class bound-constrained support vec-

tor machines with linear kernels, as they were found

to perform best over our data.

6 Experimental Methodology

We carry out experiments over the cross-product of

the following options, as described above:

model (×7): nearest-neighbour (COS1NN,
SKEW1NN, OOP1NN), nearest-prototype

(COSAM, SKEWAM),
5 NB, SVM

tokenisation (×2): byte, codepoint

n-gram (×3): 1-gram, 2-gram, 3-gram

for a total of 42 distinct classifiers. Each classi-

fier is run across the 3 datasets (EUROGOV, TCL

and WIKIPEDIA) based on 10-fold stratified cross-

validation.

We evaluate the models using micro-averaged

precision (Pµ), recall (Rµ) and F-score (Fµ), as well
as macro-averaged precision (PM ), recall (RM ) and
F-score (FM ). The micro-averaged scores indicate
the average performance per document; as we al-

ways make a unique prediction per document, the

micro-averaged precision, recall and F-score are al-

ways identical (as is the classification accuracy).

The macro-averaged scores, on the other hand, indi-

cate the average performance per language. In each

case, we average the precision, recall and F-score

across the 10 folds of cross validation.6

As a baseline, we use a majority class, or ZeroR,

classifier (ZEROR), which assigns the language with

highest prior in the training data to each of the test

documents.

4http://www.csie.ntu.edu.tw/˜cjlin/bsvm/
5We do not include the results for nearest-prototype classi-

fiers with the OOP distance metric as the results were consid-

erably lower than the other methods.
6Note that this means that the averaged FM is not necessar-

ily the harmonic mean of the averaged PM andRM .

Model Token PM RM FM Pµ/Rµ/Fµ
ZEROR — .020 .084 .032 .100

COS1NN byte .975 .978 .976 .975

COS1NN codepoint .968 .973 .970 .971

COSAM byte .922 .938 .926 .937

COSAM codepoint .908 .930 .913 .931

SKEW1NN byte .979 .979 .979 .977

SKEW1NN codepoint .978 .978 .978 .976

SKEWAM byte .974 .972 .972 .969

SKEWAM codepoint .974 .972 .973 .970

OOP1NN byte .953 .952 .953 .949

OOP1NN codepoint .961 .960 .960 .957

NB byte .975 .973 .974 .971

NB codepoint .975 .973 .974 .971

SVM byte .989 .985 .987 .987

SVM codepoint .988 .985 .986 .987

Table 2: Results for byte vs. codepoint (bigram) tokeni-

sation over EUROGOV

7 Results

In our experiments, we first compare the different

models for fixed n-gram order, then come back to

vary the n-gram order. Subsequently, we examine

the relative performance of the different models on

test documents of differing lengths, and finally look

into the impact of the amount of training data for

a given language on the performance for that lan-

guage.

7.1 Results for the Different Models and

Tokenisation Strategies

First, we present the results for each of the classifiers

in Tables 2–4, based on byte or codepoint tokenisa-

tion and bigrams. In each case, we present the best

result in each column in bold.

The relative performance over EUROGOV and

TCL is roughly comparable for all methods barring

SKEW1NN, with near-perfect scores over all 6 eval-

uation metrics. SKEW1NN is near-perfect over EU-

ROGOV and TCL, but drops to baseline levels over

WIKIPEDIA; we return to discuss this effect in Sec-

tion 7.2.

In the case of EUROGOV, the near-perfect re-

sults are in line with our expectations for the dataset,

based on its characteristics and results reported for

comparable datasets. The results for WIKIPEDIA,

however, fall off considerably, with the best model

achieving an FM of .671 and Fµ of .869, due to

233



Model Token PM RM FM Pµ/Rµ/Fµ
ZEROR — .003 .017 .005 .173

COS1NN byte .981 .975 .975 .982

COS1NN codepoint .931 .930 .925 .961

COSAM byte .967 .975 .965 .965

COSAM codepoint .979 .977 .974 .964

SKEW1NN byte .984 .974 .976 .987

SKEW1NN codepoint .910 .210 .320 .337

SKEWAM byte .962 .959 .950 .972

SKEWAM codepoint .968 .961 .957 .967

OOP1NN byte .964 .945 .951 .974

OOP1NN codepoint .901 .892 .893 .933

NB byte .905 .905 .896 .969

NB codepoint .722 .711 .696 .845

SVM byte .981 .973 .977 .984

SVM codepoint .979 .970 .974 .980

Table 3: Results for byte vs. codepoint (bigram) tokeni-

sation over TCL

the larger number of languages, smaller documents,

and skew in the amounts of training data per lan-

guage. All models are roughly balanced in the rel-

ative scores they attain for PM , RM and FM (i.e.
there are no models that have notably higherPM rel-
ative to RM , for example).

The nearest-neighbour models outperform the

corresponding nearest-prototype models to varying

degrees, with the one exception of SKEW1NN over

WIKIPEDIA. The nearest-prototype classifiers were

certainly faster than the nearest-neighbour classi-

fiers, by roughly an order of 10, but this is more

than outweighed by the drop in classification per-

formance. With the exception of SKEW1NN over

WIKIPEDIA, all methods were well above the base-

lines for all three datasets.

The two methods which perform consistently well

at this point are COS1NN and SVM, with COS1NN
holding up particularly well under micro-averaged

F-score while NB drops away over WIKIPEDIA, the

most skewed dataset; this is due to the biasing effect

of the prior in NB.

Looking to the impact of byte- vs. codepoint-

tokenisation on classifier performance over the three

datasets, we find that overall, bytes outperform

codepoints. This is most notable for TCL and

WIKIPEDIA, and the SKEW1NN and NB models.

Given this result, we present only results for byte-

based tokenisation in the remainder of this paper.

Model Token PM RM FM Pµ/Rµ/Fµ
ZEROR — .004 .013 .007 .328

COS1NN byte .740 .646 .671 .869

COS1NN codepoint .685 .604 .625 .835

COSAM byte .587 .634 .573 .776

COSAM codepoint .486 .556 .483 .725

SKEW1NN byte .005 .013 .008 .304

SKEW1NN codepoint .006 .013 .007 .241

SKEWAM byte .605 .617 .588 .844

SKEWAM codepoint .552 .575 .532 .807

OOP1NN byte .619 .518 .548 .831

OOP1NN codepoint .598 .486 .520 .807

NB byte .496 .454 .442 .851

NB codepoint .426 .349 .360 .798

SVM byte .667 .545 .577 .845

SVM codepoint .634 .494 .536 .818

Table 4: Results for byte vs. codepoint (bigram) tokeni-

sation over WIKIPEDIA

The results for byte tokenisation of TCL are par-

ticularly noteworthy. The transcoding into unicode

and use of codepoints, if anything, hurts perfor-

mance, suggesting that implicit character encoding

detection based on byte tokenisation is the best ap-

proach: it is both more accurate and simplifies the

system, in removing the need to perform encoding

detection prior to language identification.

7.2 Results for Differing n-gram Sizes

We present results with byte unigrams, bigrams and

trigrams in Table 5 for WIKIPEDIA.7 We omit re-

sults for the other two datasets, as the overall trend is

the same as for WIKIPEDIA, with lessened relative

differences between n-gram orders due to the rela-

tive simplicity of the respective classification tasks.

SKEW1NN is markedly different to the other meth-

ods in achieving the best performance with uni-

grams, moving from the worst-performing method

by far to one of the best-performing methods. This

is the result of the interaction between data sparse-

ness and heavy-handed smoothing with the α con-

stant. Rather than using a constant α value for all

n-gram orders, it may be better to parameterise it

using an exponential scale such as α = 1−βn (with

7The results for OOP1NN over byte trigrams are missing

due to the computational cost associated with the method, and

our experiment hence not having run to completion at the time

of writing. Extrapolating from the results for the other two

datasets, we predict similar results to bigrams.

234



Model n-gram PM RM FM Pµ/Rµ/Fµ
ZEROR — .004 .013 .007 .328

COS1NN 1 .644 .579 .599 .816

COS1NN 2 .740 .646 .671 .869

COS1NN 3 .744 .656 .680 .862

COSAM 1 .526 .543 .487 .654

COSAM 2 .587 .634 .573 .776

COSAM 3 .553 .632 .545 .761

SKEW1NN 1 .691 .598 .625 .848

SKEW1NN 2 .005 .013 .008 .304

SKEW1NN 3 .005 .013 .004 .100

SKEWAM 1 .552 .569 .532 .740

SKEWAM 2 .605 .617 .588 .844

SKEWAM 3 .551 .631 .554 .825

OOP1NN 1 .519 .446 .468 .747

OOP1NN 2 .619 .518 .548 .831

NB 1 .576 .578 .555 .778

NB 2 .496 .454 .442 .851

NB 3 .493 .435 .432 .863

SVM 1 .585 .505 .523 .812

SVM 2 .667 .545 .577 .845

SVM 3 .717 .547 .594 .840

Table 5: Results for different n-gram orders over

WIKIPEDIA

β = 0.01, e.g.), based on the n-gram order. We
leave this for future research.

For most methods, bigrams and trigrams are bet-

ter than unigrams, with the one notable exception

of SKEW1NN. In general, there is little separating

bigrams and trigrams, although the best result for is

achieved slightly more often for bigrams than for tri-

grams.

For direct comparability with Cavnar and Tren-

kle (1994), we additionally carried out a preliminary

experiment with hybrid byte n-grams (all of 1- to 5-

grams), combined with simple frequency-based fea-

ture selection of the top-1000 features for each n-

gram order. The significance of this setting is that it

is the strategy adopted by textcat, based on the

original paper of Cavnar and Trenkle (1994) (with

the one exception that we use 1000 features rather

than 300, as all methods other than OOP1NN bene-

fitted from more features). The results are shown in

Table 6.

Compared to the results in Table 5, SKEW1NN and

SKEWAM both increase markedly to achieve the best

overall results. OOP1NN, on the other hand, rises

slightly, while the remaining three methods actually

Model PM RM FM Pµ/Rµ/Fµ
ZEROR .004 .013 .007 .328

COS1NN .735 .664 .682 .865

COSAM .592 .626 .580 .766

SKEW1NN .789 .708 .729 .902

SKEWAM .681 .718 .680 .870

OOP1NN .697 .595 .626 .864

SVM .669 .500 .544 .832

Table 6: Results for mixed n-grams (1–5) and feature se-

lection over WIKIPEDIA (a lá Cavnar and Trenkle (1994))

drop back slightly. Clearly, there is considerably

more experimentation to be done here with mixed

n-gram models and different feature selection meth-

ods, but the results indicate that some methods cer-

tainly benefit from n-gram hybridisation and feature

selection, and also that we have been able to sur-

pass the results of Cavnar and Trenkle (1994) with

SKEW1NN in an otherwise identical framework.

7.3 Breakdown Across Test Document Length

To better understand the impact of test document

size on classification accuracy, we divided the test

documents into 5 equal-size bins according to their

length, measured by the number of tokens. We then

computed Fµ individually for each bin across the 10
folds of cross validation. We present the breakdown

of results for WIKIPEDIA in Figure 2.

WIKIPEDIA shows a pseudo-logarithmic growth

in Fµ (= Pµ = Rµ) as the test document size in-
creases. This fits with our intuition, as the model

has progressively more evidence to base the classi-

fication on. It also suggests that performance over

shorter documents appears to be the dominating fac-

tor in the overall ranking of the different methods.

In particular, COS1NN and SVM appear to be able to

classify shorter documents most reliably, leading to

the overall result of them being the best-performing

methods.

While we do not show the graph for reasons of

space, the equivalent graph for EUROGOV displays

a curious effect: Fµ drops off as the test documents
get longer. Error analysis of the data indicates that

this is due to longer documents being more likely

to be “contaminated” with either data from a sec-

ond language or extra-linguistic data, such as large

tables of numbers or chemical names. This sug-

gests that all the models are brittle when the assump-

235



Figure 2: Breakdown of Fµ over WIKIPEDIA for test
documents of increasing length

Figure 3: Per-language FM for COS1NN, relative to the
training data size (in MB) for that language

tion of strict monolingualism is broken, or when

the document is dominated by extra-linguistic data.

Clearly, this underlines our assumption of monolin-

gual documents, and suggests multilingual language

identification is a fertile research area even in terms

of optimising performance over our “monolingual”

datasets.

7.4 Performance Relative to Training Data Size

As a final data point in our analysis, we calculated

the FM for each language relative to the amount of
training data available for that language, and present

the results in the form of a combined scatter plot for

the three datasets in Figure 3. The differing distri-

butions of the three datasets are self-evident, with

most languages in EUROGOV (the squares) both

having reasonably large amounts of training data and

achieving high FM values, but the majority of lan-
guages in WIKIPEDIA (the crosses) having very lit-

tle data (including a number of languages with no

training data, as there is a singleton document in that

language in the dataset). As an overall trend, we can

observe that the greater the volume of training data,

the higher the FM across all three datasets, but there
is considerable variation between the languages in

terms of their FM for a given training data size (the
column of crosses for WIKIPEDIA to the left of the

graph is particularly striking).

8 Conclusions

We have carried out a thorough (re)examination of

the task of language identification, that is predict-

ing the language that a given document is written

in, focusing on monolingual documents at present.

We experimented with a total of 7 models, and

tested each over two tokenisation strategies (bigrams

vs. codepoints) and three token n-gram orders (un-

igrams, bigrams and trigrams). At the same time

as reproducing results from earlier research on how

easy the task can be over small numbers of lan-

guages with longer documents, we demonstrated

that the task becomes much harder for larger num-

bers of languages, shorter documents and greater

class skew. We also found that explicit character

encoding detection is not necessary in language de-

tection, and that the most consistent model overall

is either a simple 1-NN model with cosine similar-

ity, or an SVM with a linear kernel, using a byte

bigram or trigram document representation. We also

confirmed that longer documents tend to be easier to

classify, but also that multilingual documents cause

problems for the standard model of language identi-

fication.

Acknowledgements

This research was supported by a Google Research
Award.

References

Beatrice Alex, Amit Dubey, and Frank Keller. 2007.

Using foreign inclusion detection to improve parsing

performance. In Proceedings of the Joint Conference

236



on Empirical Methods in Natural Language Process-

ing and Computational Natural Language Learning

2007 (EMNLP-CoNLL 2007), pages 151–160, Prague,

Czech Republic.

Javed A. Aslam and Meredith Frost. 2003. An

information-theoretic measure for document similar-

ity. In Proceedings of 26th International ACM-SIGIR

Conference on Research and Development in Informa-

tion Retrieval (SIGIR 2003), pages 449–450, Toronto,

Canada.

Timothy Baldwin, Steven Bird, and Baden Hughes.

2006. Collecting low-density language materials on

the web. In Proceedings of the 12th Australasian Web

Conference (AusWeb06). http://www.ausweb.

scu.edu.au/ausweb06/edited/hughes/.

William B. Cavnar and John M. Trenkle. 1994. N-

gram-based text categorization. In Proceedings of the

Third Symposium on Document Analysis and Informa-

tion Retrieval, Las Vegas, USA.

Marc Darnashek. 1995. Gauging similarity with n-

grams: Language-independent categorization of text.

Science, 267:843–848.

Rafael Dueire Lins and Paulo Gonçalves. 2004. Au-

tomatic language identification of written texts. In

Proceedings of the 2004 ACM Symposium on Applied

Computing (SAC 2004), pages 1128–1133, Nicosia,

Cyprus.

Ted Dunning. 1994. Statistical identification of lan-

guage. Technical Report MCCS 940-273, Computing

Research Laboratory, New Mexico State University.

Emmanuel Giguet. 1995. Categorization according to

language: A step toward combining linguistic knowl-

edge and statistic learning. In Proceedings of the

4th International Workshop on Parsing Technologies

(IWPT-1995), Prague, Czech Republic.

E. Mark Gold. 1967. Language identification in the

limit. Information and Control, 5:447–474.

Gregory Grefenstette. 1995. Comparing two language

identification schemes. In Proceedings of Analisi Sta-

tistica dei Dati Testuali (JADT), pages 263–268.

Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.

2008. A practical guide to support vector classifica-

tion. Technical report, Department of Computer Sci-

ence National Taiwan University.

Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy

Nicholson, and Andrew MacKinlay. 2006. Recon-

sidering language identification for written language

resources. In Proceedings of the 5th International

Conference on Language Resources and Evaluation

(LREC 2006), pages 485–488, Genoa, Italy.

Thorsten Joachims. 1998. Text categorization with sup-

port vector machines: learning with many relevant fea-

tures. In Proceedings of the 10th European Confer-

ence on Machine Learning, pages 137–142, Chemnitz,

Germany.

Stephen Johnson. 1993. Solving the problem of lan-

guage recognition. Technical report, School of Com-

puter Studies, University of Leeds.

Canasai Kruengkrai, Prapass Srichaivattana, Virach

Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-

guage identification based on string kernels. In Pro-

ceedings of the 5th International Symposium on Com-

munications and Information Technologies (ISCIT-

2005), pages 896–899, Beijing, China.

Lillian Lee. 2001. On the effectiveness of the skew diver-

gence for statistical language analysis. In Proceedings

of Artificial Intelligence and Statistics 2001 (AISTATS

2001), pages 65–72, Key West, USA.

Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello

Cristianini, and Chris Watkins. 2002. Text classifica-

tion using string kernels. Journal of Machine Learning

Research, 2:419–444.

Christopher D. Manning, Prabhakar Raghavan, and Hin-

rich Schütze. 2008. Introduction to Information Re-

trieval. Cambridge University Press, Cambridge, UK.

Bruno Martins and Mário J. Silva. 2005. Language iden-

tification in web pages. In Proceedings of the 2005

ACM symposium on Applied computing, pages 764–

768, Santa Fe, USA.

Andrew Kachites McCallum. 1996. Bow: A toolkit for

statistical language modeling, text retrieval, classifica-

tion and clustering. http://www.cs.cmu.edu/

˜mccallum/bow.

Paul McNamee and James Mayfield. 2004. Character N -

gram Tokenization for European Language Text Re-

trieval. Information Retrieval, 7(1–2):73–97.

Olivier Teytaud and Radwan Jalam. 2001. Kernel-

based text categorization. In Proceedings of the

International Joint Conference on Neural Networks

(IJCNN’2001), Washington DC, USA.

Vladimir N. Vapnik. 1995. The Nature of Statistical

Learning Theory. Springer-Verlag, Berlin, Germany.

Fei Xia and William Lewis. 2009. Applying NLP tech-

nologies to the collection and enrichment of language

data on the web to aid linguistic research. In Pro-

ceedings of the EACL 2009 Workshop on Language

Technology and Resources for Cultural Heritage, So-

cial Sciences, Humanities, and Education (LaTeCH –

SHELT&R 2009), pages 51–59, Athens, Greece.

Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-

guage ID in the context of harvesting language data off

the web. In Proceedings of the 12th Conference of the

EACL (EACL 2009), pages 870–878, Athens, Greece.

237


