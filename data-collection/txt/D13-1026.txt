










































Max-Margin Synchronous Grammar Induction for Machine Translation


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 255–264,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Max-Margin Synchronous Grammar Induction for Machine Translation

Xinyan Xiao and Deyi Xiong∗

School of Computer Science and Technology
Soochow University

Suzhou 215006, China
xyxiao.cn@gmail.com, dyxiong@suda.edu.cn

Abstract

Traditional synchronous grammar induction
estimates parameters by maximizing likeli-
hood, which only has a loose relation to trans-
lation quality. Alternatively, we propose a
max-margin estimation approach to discrim-
inatively inducing synchronous grammars for
machine translation, which directly optimizes
translation quality measured by BLEU. In
the max-margin estimation of parameters, we
only need to calculate Viterbi translations.
This further facilitates the incorporation of
various non-local features that are defined on
the target side. We test the effectiveness of our
max-margin estimation framework on a com-
petitive hierarchical phrase-based system. Ex-
periments show that our max-margin method
significantly outperforms the traditional two-
step pipeline for synchronous rule extraction
by 1.3 BLEU points and is also better than pre-
vious max-likelihood estimation method.

1 Introduction

Synchronous grammar induction, which refers to
the process of learning translation rules from bilin-
gual corpus, still remains an open problem in sta-
tistical machine translation (SMT). Although state-
of-the-art SMT systems model the translation pro-
cess based on synchronous grammars (including
bilingual phrases), most of them still learn trans-
lation rules via a pipeline with word-based heuris-
tics (Koehn et al., 2003). This pipeline first builds
word alignments using heuristic combination strate-
gies, then heuristically extracts rules that are consis-
tent with word alignments. Such heuristic pipeline

∗Corresponding author

is not elegant theoretically. It brings an undesirable
gap that separates modeling and learning in an SMT
system.

Therefore, researchers have proposed alternative
approaches to learning synchronous grammars di-
rectly from sentence pairs without word alignments,
via generative models (Marcu and Wong, 2002;
Cherry and Lin, 2007; Zhang et al., 2008; DeNero
et al., 2008; Blunsom et al., 2009; Cohn and Blun-
som, 2009; Neubig et al., 2011; Levenberg et al.,
2012) or discriminative models (Xiao et al., 2012).
Theoretically, these approaches describe how sen-
tence pairs are generated by applying sequences of
synchronous rules in an elegant way. However, they
learn synchronous grammars by maximizing likeli-
hood,1 which only has a loose relation to transla-
tion quality (He and Deng, 2012). Moreover, gen-
erative models are normally hard to be extended to
incorporate useful features, and the discriminative
synchronous grammar induction model proposed by
Xiao et al. (2012) only incorporates local features
defined on parse trees of the source language. Non-
local features, which encode information from parse
trees of the target language, have never been ex-
ploited before due to the computational complexity
of normalization in max-likelihood estimation.

Consequently, we would like to learn syn-
chronous grammars in a discriminative way that can
directly maximize the end-to-end translation quality
measured by BLEU (Papineni et al., 2002), and is
also able to incorporate non-local features from tar-
get parse trees.

We thus propose a max-margin estimation method

1More precisely, the discriminative model by Xiao et al.
(2012) maximizes conditional likelihood.

255



to discriminatively induce synchronous grammar di-
rectly from sentence pairs without word alignments.
We try to maximize the margin between a reference
translation and a candidate translation with transla-
tion errors that are measured by BLEU. The more
serious the translation errors, the larger the margin.
In this way, our max-margin method is able to learn
synchronous grammars according to their translation
performance. We further incorporate various non-
local features defined on target parse trees. We ef-
ficiently calculate the non-local feature values of a
translation over its exponential derivation space us-
ing the inside-outside algorithm. Because our max-
margin estimation optimizes feature weights only by
the feature values of Viterbi and reference transla-
tions, we are able to efficiently perform optimization
even with non-local features.

We apply the proposed max-margin estimation
method to learn synchronous grammars for a hi-
erarchical phrase-based translation system (Chiang,
2007) which typically produces state-of-the-art per-
formance. With non-local features defined on tar-
get parse trees, our max-margin method significantly
outperforms the baseline that uses synchronous
rules learned from the traditional pipeline by 1.3
BLEU points on large-scale Chinese-English bilin-
gual training data.

The remainder of this paper is organized as fol-
lows. Section 2 presents the discriminative syn-
chronous grammar induction model with the non-
local features. In Section 3, we elaborate our max-
margin estimation method which is able to directly
optimize BLEU, and discuss how we induce gram-
mar rules. Local and non-local features are de-
scribed in Section 4. Finally, in Section 5, we verify
the effectiveness of our method through experiments
by comparing it against both the traditional pipeline
and max-likelihood estimation method.

2 Discriminative Model with Non-local
Features

Let S denotes the set of all strings in a source lan-
guage. Given a source sentence s ∈ S , T (s) denotes
all candidate translations in the target language that
can be generated by a synchronous grammar G. A
translation t ∈ T (s) is generated by a sequence of
translation steps (r1, ..., rn), where we apply a syn-

10 2 3 4 5

 [1,3]

 [1,5]

 [0,5]

[1,6]

[0,6]

[4,6]

10 2 3 4 5 6

bushi yu shalong juxing huitan

r1: ⟨ yu shalong⇒ with Sharon ⟩
r2: ⟨ X juxing huitan⇒ held a talk X ⟩
r3: ⟨ bushi X ⇒ Bush X ⟩

Figure 1: A derivation of a sentence pair represented by
a synchronous tree. The above and below part are the
parses in the source language side and the target language
side respectively. Left subscript of a node X denotes the
source span, while right subscript denotes the target span.
A dashed line denotes an alignment from a source span
to a target span. The annotation for a dashed line cor-
responds to the rewriting rule used in the corresponding
step of the derivation.

chronous rule r ∈ G in one step. We refer to such
a sequence of translation steps as a derivation (See
Figure 1) and denote it as d ∈ D(s), where D(s)
represents the derivation space of a source sentence.
Given an input source sentence s, we output a pair
⟨t,d⟩ in SMT. Thus, we study the triple ⟨s, t,d⟩ in
SMT.

In our discriminative model, we calculate the
value of a triple ⟨s, t,d⟩ according to the following
scoring function:

f(s, t,d) = θT Φ(s, t,d) (1)

where θ ∈ Θ is a feature weight vector, and Φ is the
feature function.

There are exponential outputs in SMT. Therefore
it is necessary to factorize the feature function in or-
der to perform efficient calculation over the SMT
output space using dynamic programming. We de-
compose the feature function of a triple ⟨s, t,d⟩ into

256



 [1,5]

[1,6]

Figure 2: Example features for the derivation in Figure 1.
Shaded nodes denote information encoded in the feature.

a sum of values of each synchronous rule in the
derivation d.

Φ(s, t,d) =
∑
r∈d

ϕ(r, s)︸ ︷︷ ︸
local

+
∑
r∈d

ϕ(r, s, t)︸ ︷︷ ︸
non-local

(2)

Our feature functions include both local and non-
local features. A feature is a local feature if and
only if it can be factored among the translation steps
in a derivation. In other words, the value of a lo-
cal feature for ⟨s, t,d⟩ can be calculated as a sum of
local scores in each translation step, and the calcula-
tion of each local score only requires to look at the
rule used in corresponding step and the input sen-
tence. Otherwise, the feature is a non-local feature.
Our discriminative model allows to incorporate non-
local features that are defined on target translations.

For example, a rule feature in Figure 2(a), which
indicates the application of a specific rule in a
derivation, is a local feature. A source span bound-
ary feature in Figure 2(b) that is defined on the
source parse tree is also a local feature. However,
a target span boundary feature in Figure 2(c), which
assesses the target parse structure, is a non-local fea-
ture. According to Figure 1, the span is parsed in
step r2, but it also depends on the translation bound-
ary word “held” generated in previous step r1. We
will describe the details of both local and non-local
features that we use in Section 4.

Non-local features enable us to model the target
parse structure in a derivation. However, it is com-
putationally expensive to calculate the expected val-
ues of non-local features over D(s), as non-local
features require to record states of target boundary

s, S, S s is a sentence in a source language;
S means source training sentences;
S denotes all the possible sentences;

t, T, T symbols for the target language that
similar to s, S, S;

d, D derivation and derivation space;
D(s) space of derivations for

a source sentence;
D(s, t) space of derivations for

a source sentence with its translation;
H(s) hypergraph that represents D(s);
H(s, t) hypergraph that represents D(s, t);

Table 1: Notations in this paper. We give an abstract of
related notations for clarity.

words and result in an extremely large number of
states during dynamic programming. Fortunately,
when integrating out derivations over the derivation
space D(s, t) of a source sentence and its transla-
tion, we can efficiently calculate the non-local fea-
tures. Because all derivations in D(s, t) share the
same translation, there is no need to maintain states
for target boundary words. We will discuss this com-
putational problem in details in Section 3.3. In the
proposed max-margin estimation described in next
section, we only need to integrate out derivation
for a Viterbi translation and a reference translation
when updating feature weights. Therefore, the de-
fined non-local features allow us to not only explore
useful knowledge on the target parse trees, but also
compute them efficiently over D(s, t) during max-
margin estimation.

3 Max-Margin Estimation

In this section, we describe how we use a parallel
training corpus {S,T} = {(s(i), t(i))}Ni=1 to esti-
mate feature weights θ, which contain parameters of
the induced synchronous grammars and the defined
non-local features.

We choose the parameters that maximize the
translation quality measured by BLEU using the
max-margin estimation (Taskar et al., 2004). Mar-
gin refers to the difference of the model score be-
tween a reference translation t(i) and a candidate
translation t. We hope that the worse the transla-
tion quality of t, the larger the margin between t
and t(i). In this way, we penalize larger translation

257



errors more severely than smaller ones. This intu-
ition is expressed by the following equation.

min
1

2
∥θ∥2 (3)

s.t. f(s(i), t(i))− f(s(i), t) ≥ cost(t(i), t)
∀t ∈ T (s(i))

Here, f(s, t) is the feature function of a translation,
and cost function cost(t(i), t) measures the trans-
lation errors of a candidate translation t comparing
with a reference translation t(i). We define the cost
function via the widely-used translation evaluation
metric BLEU. We use the smoothed sentence level
BLEU-4 (Lin and Och, 2004) here:

cost(t(i), t) = 1− BLEU-4(t(i), t) (4)

In Section 3.1, we will discuss how we use the
scoring function f(s, t,d) to calculate f(s, t). Then
in Section 3.2, we recast the equation (3) as an un-
constrained empirical loss minimization problem,
and describe the learning algorithm for optimizing
θ and inducing G. Finally, we give the details of
inference for the learning algorithm in Section 3.3.

3.1 Integrate Out Derivation by Averaging
Although we only model the triple ⟨s, t,d⟩ in the
equation (1), it’s necessary to calculate the scoring
function f(s, t) of a translation by integrating out
the variable of derivation as derivation is not ob-
served in the training data.

We use an averaging computation over all possi-
ble derivations of a translation D(s, t). We call this
an average derivation based estimation:

f(s, t) =
1

|D(s, t)|
∑

d∈D(s,t)

f(s, t,d) (5)

The “average derivation” can be considered as the
geometric central point in the space D(s, t).

Another possible way to deal with the latent
derivation is max-derivation, which uses the max-
operator over D(s, t). The max derivation method
sets f(s, t) as maxd∈D(s,t) f(s, t,d). It is often
adopted in traditional SMT systems. Nevertheless,
we instead use average-derivation for two reasons.2

2Imagine that H(s, t) in the Algorithm 1 is replaced by a
maximum derivation inH(s, t).

First, as a translation has an exponential number of
derivations, finding the max derivation of a refer-
ence translation for learning is nontrivial (Chiang et
al., 2009). Second, the max derivation estimation
will result in a low rule coverage, as rules in a max
derivation only covers a small fraction of rules in
the D(s, t). Because rule coverage is important in
synchronous grammar induction, we would like to
explore the entire derivation space using the average
operator.

3.2 Learning Algorithm

We reformulate the equation (3) as an unconstrained
empirical loss minimization problem as follows:

min
λ

2
∥θ∥2 + 1

N

N∑
n=1

L(s(i), t(i), θ) (6)

Where λ denotes the regularization strength for
L2-norm. The loss function of a sentence pair
L(s(i), t(i), θ) is a convex hinge loss function de-
noted by:

max{0,−f(s(i), t(i)) (7)

+ max
t∈T (s(i))

(
f(s(i), t) + cost(t(i), t)

)
}

According to the second max-operator in the
hinge loss function, the optimization towards BLEU
is expressed by cost-augmented inference. Cost-
augmented inference finds a translation that has a
maximum model score augmented with cost.

t̂ = max
t∈T (s(i))

(
f(s(i), t) + cost(t(i), t)

)
(8)

We applied the Pegasos algorithm for the op-
timization of equation (6) (Shalev-Shwartz et al.,
2007). This is an online algorithm, which alternates
between stochastic gradient descent steps and pro-
jection steps. When the loss function is non-zero, it
updates weights according to the sub-gradient of the
hinge loss function. Using the average scoring func-
tion in the equation (5), the sub-gradient of hinge
loss function for a sentence pair is the difference of
average feature values between a Viterbi translation

258



Algorithm 1 UPDATE(s, t, θ,G) ◃ One step in online algorithm. s, t are short for s(i), t(i) here
1: H(s, t)← BIPARSE(s, t, θ) ◃ Build hypergraph of reference translation
2: G←G +H(s, t) ◃ Discover rules fromH(s, t)
3: t̂, d̂← arg max⟨t′,d′⟩∈D(s) f(s, t′,d′) + cost(t, t′) ◃ Find Viterbi translation
4: H(s, t̂)← BIPARSE(s, t̂, θ) ◃ Build hypergraph of Viterbi translation
5: if f(s, t) < f(s, t̂) + cost(t, t̂) then
6: θ ← (1− ηλ)θ + η × ∂L∂θ (H(s, t),H(s, t̂)) ◃ Update θ by gradient

∂L
∂θ and learning rate η

7: θ ← min {1, 1/
√

λ
∥θ∥ } × θ ◃ Projection by scaling

8: return G, θ

and a reference translation.

∂L

∂θ
=

1

|D(s(i), t(i))|
∑

d∈D(s(i),t(i))

Φ(s(i), t(i),d)

− 1
|D(s(i), t̂)|

∑
d∈D(s(i),t̂)

Φ(s(i), t̂,d) (9)

Algorithm 1 shows the procedure of one step in
the online optimization algorithm. The procedure
discovers rules and updates weights in an online
fashion. In the procedure, we first biparse the sen-
tence pair to construct a synchronous hypergraph of
a reference translation (line 1). In the biparsing al-
gorithm, synchronous rules for constructing hyper-
edges are not required to be in G, but can be any
rules that follow the form defined in Chiang (2007).
Thus, the biparsing algorithm can discover new rules
that are not in G. Then we collect the translation
rules discovered in the hypergraph of the reference
translation (line 2), which are rules indicated by hy-
peredges in the hypergraph. We then calculate the
Viterbi translation according to the scoring function
and cost function (see Section 3.3) (line 3), and build
the synchronous hypergraph for the Viterbi transla-
tion (line 4). Finally, we update weights according to
the Pegasos algorithm (line 5). The sub-gradient is
calculated based on the hypergraph of Viterbi trans-
lation and reference translation.

In practice, in order to process the data in a paral-
lel manner, we use a larger step size of 1000 for the
learning algorithm. In each step of our online opti-
mization algorithm, we first biparse 1000 reference
sentence pairs in parallel. Then, we collect grammar
rules from the generated reference hypergraphs. Af-
ter that, we compute the gradients of 1000 sentence
pairs in parallel, by calculating feature weights over
reference hypergraphs and Viterbi hypergraphs. Fi-

nally, we update the feature weights using the sum
of these gradients.

3.3 Inference

There are two parts that need to be calculated in
the learning algorithm: finding a cost-augmented
Viterbi translation according to the scoring func-
tion and cost function (Equation 8), and constructing
synchronous hypergraphs for the Viterbi and refer-
ence translation so as to discover rules and calculate
average feature values in Equation (9). Following
the traditional decoding procedure, we resort to the
cube-pruning based algorithm for approximation.

To find the Viterbi translation, we run the tra-
ditional translation decoding algorithm (Chiang,
2007) to get the best derivation. Then we use
the translation yielded by the best derivation as the
Viterbi translation. In order to obtain the BLEU
score in the cost function, we need to calculate the
ngram precision. It is calculated in a way similar to
the calculation of the ngram language model. The
computation of BLEU-4 requires to record 3 bound-
ary words in both the left and right side during dy-
namic programming. Therefore, even when we use
a language model whose order is less than 4, we still
expands the states to record 3 boundary words so as
to calculate the cost measured by BLEU.

We build synchronous hypergraphs using the
cube-pruning based biparsing algorithm (Xiao et al.,
2012). Algorithm 2 shows the procedure. Using
a chart, the biparsing algorithm constructs k-best
alignments for every source word (lines 1-5) and k-
best hyperedges for every source span (lines 6-13)
from the bottom up. Thus, a synchronous hyper-
graph is generated during the construction of the
chart. More specifically, for a source span, it first
creates cubes L for all source parses γ that are in-

259



Algorithm 2 BIPARSE(s, t, θ) ◃ (Xiao et al., 2012)
� Create k-best alignments for each source word

1: for i← 1, .., |s| do
2: for j ← 1, .., |t| do
3: Lj ← {ε, tj} ◃ si aligns to tj or not
4: L← ⟨L1, ..., L|t|⟩
5: chart[s, i]← KBEST(L,⊗,θ)

� Create k-best hyperedges for each source span
6: H← ∅
7: for h← 1, .., |s| do ◃ h is the size of span
8: for all i, j s.t. j − i = h do
9: L← ∅

10: for γ inferable from chart do
11: L← L + ⟨chart[γ1], ..., chart[γ|γ|]⟩
12: chart[X, i, j]← KBEST(L,⊗,θ)
13: H←H + chart[X, i, j] ◃ save hyperedges
14: returnH

ferable from the chart (lines 9-11). Here γi is a par-
tial source parse that covers either a single source
word or a span of source words. Then it uses the
cube pruning algorithm to keep the top k derivations
among all partial derivations that share the same
source span [i, j] (line 12). Notably, this biparsing
algorithm does not require specific translation rules
as input. Instead, it is able to discover new syn-
chronous grammar rules when constructing a syn-
chronous hypergraph: extracting each hyperedge in
the hypergraph as a synchronous rule.

Based on the biparsing algorithm, we are able to
construct the reference hypergraph H(s(i), t(i)) and
Viterbi hypergraph H(s(i), t̂). By the reference hy-
pergraph, we collect new synchronous translation
rules and record them in the grammar G. We also
calculate the average feature values of hypergraphs
using the inside-outside algorithm (Li et al., 2009),
so as to compute the gradients.

4 Features

One advantage of the discriminative method is that
it enables us to incorporate arbitrary features. As
shown in Section 2, our model incorporates both lo-
cal and non-local features.

4.1 Local Features
Rule features We associate each rule with an indi-
cator feature. Each indicator feature counts the num-
ber of times that a rule appears in a derivation. In

this way, we are able to learn a weight for every rule
according to the entire structure of sentence.

Word association features Lexicalized features
are widely used in traditional SMT systems. Here
we adopt two lexical weights called noisy-or fea-
tures (Zens and Ney, 2004). The noisy-or feature
is estimated by word translation probabilities output
by GIZA++. We set the initial weight of these two
lexical scores with equivalent positive values. The
lexical weights enable our system to score and rank
the hyperedges at the beginning. Although word
alignment features are used, we do not constrain the
derivation space of a sentence pair by prefixed word
alignment, and do not require any heuristic align-
ment combination strategy.

Length feature We integrate the length of target
translation that is used in traditional SMT system as
our feature.

Source span boundary features We use this kind
of feature to assess the source parse tree in a deriva-
tion. Previous work (Xiong et al., 2010) has shown
the importance of phrase boundary features for
translation. Actually, this kind of feature is a good
cue for deciding the boundary where a rule is to be
learnt. Following Taskar et al. (2004), for a bispan
[i, j, k, l] in a derivation, we define the feature tem-
plates that indicates the boundaries of a span by its
beginning and end words: {B : si+1; E : sj ; BE :
si+1, sj}.

Source span orientation features Orientation
features are only used for those spans that are swap-
ping. In Figure 1, the translation of source span [1, 3]
is swapping with that of span [4, 5] by r2, thus ori-
entation feature for span [1, 3] is activated. We also
define three feature templates for a swapping span
similar to the boundary features: {B : si+1; E :
sj ;BE : si+1, sj}. In practice, we add a prefix to
the orientation features so as to distinguish these fea-
tures from the boundary features.

4.2 Non-local Features
Target span boundary features We also want to
assess the target tree structure in a derivation. We
define these features in a way similar to source span
boundary features. For a bispan [i, j, k, l] in a deriva-
tion, we define the feature templates that indicates

260



System Grammar Size MT03 MT04 MT05 Avg.

Moses 302.5M 34.26 36.56 32.69 34.50
Baseline 77.8M 33.83 35.81 33.23 34.29

Max-margin
59.4M

34.62 37.14 34.00 35.25
+Sparse feature 35.48 37.31 34.07 35.62

Table 2: Experiment results. Baseline is an in-house implementation of hierarchical phrase based system. Moses
denotes the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007). +Sparsefeature means
that those sparse features used in the grammar induction are also used during decoding. The improvement of max-
margin over Baseline is statistically significant (p < 0.01).

target span boundary as: {B : tk+1; E : tl; BE :
tk+1, tl}.

Target span orientation features Similar target
orientation features are used for a swapping span
[i, j, k, l] with feature templates {B : tk+1; E :
tl; BE : tk+1, tl}.

Relative position features Following Blunsom
and Cohn (Blunsom and Cohn, 2006), we integrate
features indicating the closeness to the alignment
matrix diagonal. For an aligned word pair with
source position i and target position j, the value of
this feature is | i|s| −

j
|t| |. As this feature depends

on the length of the target sentence, it is a non-local
feature.

Language model We also incorporate an ngram
language model which is an important component
in SMT. For efficiency, we use a 3-gram language
model trained on the target side of our training data
during the induction of synchronous grammars.

5 Experiment

In this section, we present our experiments on the
NIST Chinese-to-English translation tasks. We first
compare our max-margin based method with the tra-
ditional pipeline on a large bitext which contains
1.1 million sentences. We then present a detailed
comparison on a smaller dataset, in order to analyze
the effectiveness of max-margin estimation compar-
ing with the max likelihood estimation (Xiao et al.,
2012), and also the effectiveness of the non-local
features that are defined on the target side.

5.1 Setup
The baseline system is the hierarchical phrase based
system (Chiang, 2007). We used a bilingual corpus

that contains 1.1M sentences (44.6 million words)
of up to length 40 from the LDC data.3 Our 5-gram
language model was trained by SRILM toolkit (Stol-
cke, 2002). The monolingual training data includes
the Xinhua section of the English Gigaword corpus
and the English side of the entire LDC data (432 mil-
lion words).

We used the NIST 2002 (MT02) as our develop-
ment set, and the NIST 2003-2005 (MT03-05) as the
test set. Case-insensitive NIST BLEU-4 (Papineni
et al., 2002) is used to measure translation perfor-
mance, and also the cost function in the max-margin
estimation. Statistical significance in BLEU differ-
ences was tested by paired bootstrap re-sampling
(Koehn, 2004). We used minimum error rate train-
ing (MERT) (Och, 2003) to optimize feature weights
for the traditional log-linear model.

We used the same decoder as the baseline system
in all estimation methods. Without special explana-
tion, we used the same features as those in the tra-
ditional pipeline: forward and backward translation
probabilities, forward and backward lexical weights,
count of extracted rules, count of glue rules, length
of translation, and language model. For the lexical
weights we used the noisy-or in all configurations
including the baseline system. For the discrimina-
tive grammar induction, rule translation probabili-
ties were calculated using the expectations of rules
in the synchronous hypergraphs of sentence pairs.

As our max-margin synchronous grammar induc-
tion is trained on the entire bitext, it is necessary to
load all the rules into the memory during training.
To control the size of rule table, we used Viterbi-

3Including LDC2002E18, LDC2003E07, LDC2003E14,
LDC2004T07, LDC2005T06 and Hansards portion of
LDC2004T08.

261



System Feature Function MT03 MT04 MT05 Avg.

Baseline — 31.76 33.08 31.06 31.96

Max-likelihood local 32.84 34.54 31.61 33.00

Max-margin
local 32.97 34.92 31.99 33.29

local,non-local 33.27 34.83 32.32 33.47

Table 3: Comparison of Max-margin and Max-likelihood estimation on a smaller corpus. For max-margin method, we
present two results according to the usages of non-local features. The max-margin with non-local features significantly
outperforms the Baseline (p < 0.01) and also the max-likelihood estimation (p < 0.05).

pruning (Huang, 2008) when collecting rules as
shown in line 2 of optimization procedure in Section
3.2. Furthermore, we aggressively discarded those
large rules (The number of source symbols or the
number of target symbols are more than two) that
occur only in one sentence. Whenever the learning
algorithm processes 50K sentences, we performed
this discarding operation for large rules.

5.2 Result on Large Dataset
Table 2 shows the translation results. Our method
induces 59.4 million synchronous rules, which are
76.3% of the grammar size of baseline. Note that
Moses allows the boundary words of a phrase to be
unaligned, while our baseline constraints the initial
phrase to be tightly consistent with word alignment.
Therefore, Moses extract a much larger rule table
than that of our baseline.

With fewer translation rules, our method obtains
an average improvement of +0.96 BLEU points on
the three test sets over the Baseline. As the differ-
ence between the baseline and our max-margin syn-
chronous grammar induction model only lies in the
grammar, this result clearly denotes that our learnt
grammar does outperform the grammar extracted by
the traditional two-step pipeline.

We also incorporate the sparse features during de-
coding in a way similar to Xiao et al. (2012) and
Dyer et al. (2011). In order to optimize these sparse
features with the dense features by MERT, we group
features of the same type into one coarse “summary
feature”, and get three such features including: rule,
phrase-boundary and phrase orientation features. In
this way, we rescale the weights of the three “sum-
mary features” with the 8 dense features by MERT.
We achieve a further improvement of +0.37 BLEU
points. Therefore, our training algorithm is able to

learn the useful information encoded by the sparse
features for translation.

5.3 Comparison of Estimation Objective and
Non-Local Feature

We want to investigate whether the max-margin esti-
mation is able to outperform the max-likelihood es-
timation method (Xiao et al., 2012). Therefore we
carried out experiments to compare them directly.
As the max-margin method is able to use non-local
features, we compare two settings of features for the
max-margin method. One uses only local features,
the other uses both local and non-local features. Be-
cause the training procedure need to run on the entire
corpus, which is time consuming, we therefore use
a smaller corpus containing 50K sentences from the
entire bitext for comparison.

Table 3 shows the results. When using only local
features, the max-margin method consistently out-
performs the max-likelihood method in all three test
sets. This clearly shows the advantage of learning
grammars by optimizing BLEU over likelihood.

When incorporating the non-local features into
the max-margin method, we achieve further im-
provement against the max-margin method with-
out non-local features. With non-local features,
our max-margin estimation method outperforms the
baseline by 1.5 BLEU points, and is better than
the max-likelihood estimation by 0.5 BLEU points.
Based on these results, we believe that non-local fea-
tures, which encode information from target parse
structures, are helpful for grammar induction. This
further confirms the advance of the max-margin es-
timation, as it provides us a convenient way to use
non-local features.

262



6 Related Work

As the synchronous grammar is the key compo-
nent in SMT systems, researchers have proposed
various methods to improve the quality of gram-
mars. In addition to the generative and discrimina-
tive models introduced in Section 1, researchers also
have made efforts on word alignment and grammar
weight rescoring.

The first line is to modify word alignment by ex-
ploring information of syntactic structures (May and
Knight, 2007; DeNero and Klein, 2010; Pauls et
al., 2010; Burkett et al., 2010; Riesa et al., 2011).
Such syntactic information is combined with word
alignment via a discriminative framework. These
methods prefer word alignments that are consistent
with syntactic structure alignments. However, la-
beled word alignment data are required in order to
learn the discriminative model.

Yet another line is to rescore the weights of trans-
lation rules. This line of work tries to improve the
relative frequency estimation used in the traditional
pipeline. They rescore the weights or probabilities
of extracted rules. The rescoring is done by using
the similar latent log-linear model as ours (Blun-
som et al., 2008; Kääriäinen, 2009; He and Deng,
2012), or incorporating various features using la-
beled word aligned bilingual data (Huang and Xi-
ang, 2010). However, in rescoring, translation rules
are still extracted by the heuristic two-step pipeline.
Therefore these previous work still suffers from the
inelegance problem of the traditional pipeline.

Our work also relates to the discriminative train-
ing (Och, 2003; Watanabe et al., 2007; Chiang et al.,
2009; Xiao et al., 2011; Gimpel and Smith, 2012)
that has been widely used in SMT systems. Notably,
these discriminative training methods are not used to
learn grammar. Instead, they assume that grammar
are extracted by the traditional two-step pipeline.

7 Conclusion

In this paper we have presented a max-margin esti-
mation for discriminative synchronous grammar in-
duction. By associating the margin with the transla-
tion quality, we directly learn translation rules that
optimize the translation performance measured by
BLEU. Max-margin estimation also provides us a
convenient way to incorporate non-local features.

Experiment results validate the effectiveness of opti-
mizing parameters by BLEU, and the importance of
incorporating non-local features defined on the tar-
get language. These results confirm the advantage of
our max-margin estimation framework as it can both
optimize BLEU and incorporate non-local features.

Feature engineering is very important for discrim-
inative models. Researchers have proposed various
types of features for machine translation, which are
often estimated from word alignments. We would
like to investigate whether further improvement can
be achieved by incorporating such features, espe-
cially the context model (Shen et al., 2009) in the
future. Because our proposed model is quite general,
we are also interested in applying this method to
induce linguistically motivated synchronous gram-
mars for syntax-based SMT.

Acknowledgments

The first author was partially supported by 863
State Key Project (No. 2011AA01A207) and
National Key Technology R&D Program (No.
2012BAH39B03). We are grateful to the anony-
mous reviewers for their insightful comments. We
also thank Yi Lin for her invaluable feedback.

References

Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Prob. ACL 2006, July.

Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL 2008.

Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. ACL 2009.

David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Proc. NAACL 2010.

Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proc. SSST 2007, NAACL-HLT Workshop on Syntax
and Structure in Statistical Translation, April.

David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. NAACL 2009.

David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.

263



Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. EMNLP 2009.

John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proc. ACL 2010.

John DeNero, Alexandre Bouchard-Côté, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP 2008.

Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The cmu-ark german-english
translation system. In Proc. WMT 2011.

Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. NAACL 2012.

Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation models.
In Proc. ACL 2012.

Fei Huang and Bing Xiang. 2010. Feature-rich discrimi-
native phrase rescoring for smt. In Proc. Coling 2010.

Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL 2008.

Matti Kääriäinen. 2009. Sinuhe – statistical machine
translation using a globally trained conditional expo-
nential family translation model. In Proc. EMNLP
2009.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL 2007 (demonstration session).

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.

Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A bayesian model for learning scfgs with discontigu-
ous rules. In Proc. EMNLP 2012. Association for
Computational Linguistics, July.

Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL 2009.

Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Pro. Coling 2004.

Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP 2002.

Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Proc.
EMNLP 2007.

Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL 2011.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.

Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In Proc. NAACL
2010.

Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In Proc.
EMNLP 2011.

Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proc. ICML 2007.

Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.

Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit.

Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. EMNLP 2004.

Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proc. EMNLP-CoNLL
2007.

Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Fast generation of translation forest for large-
scale smt discriminative training. In Proc. EMNLP
2011.

Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and
Shouxun Lin. 2012. Unsupervised discriminative in-
duction of synchronous grammar for machine transla-
tion. In Proc. Coling 2012.

Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In Proc. NAACL2010.

Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Prob.
NAACL 2004.

Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proc. ACL 2008.

264


