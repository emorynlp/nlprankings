










































Cross Language Text Classification by Model Translation and Semi-Supervised Learning


Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1057–1067,
MIT, Massachusetts, USA, 9-11 October 2010. c©2010 Association for Computational Linguistics

Cross Language Text Classification by Model Translation and
Semi-Supervised Learning

Lei Shi
Yahoo! Global R&D

Beijing, China
lshi@yahoo-inc.com

Rada Mihalcea
University of North Texas

Denton, TX, U.S.A.
rada@cs.unt.edu

Mingjun Tian
Yahoo! Global R&D

Beijing, China
mingjun@yahoo-inc.com

Abstract

In this paper, we introduce a method that au-
tomatically builds text classifiers in a new lan-
guage by training on already labeled data in
another language. Our method transfers the
classification knowledge across languages by
translating the model features and by using
an Expectation Maximization (EM) algorithm
that naturally takes into account the ambigu-
ity associated with the translation of a word.
We further exploit the readily available un-
labeled data in the target language via semi-
supervised learning, and adapt the translated
model to better fit the data distribution of the
target language.

1 Introduction

Given the accelerated growth of the number of mul-
tilingual documents on the Web and elsewhere, the
need for effective multilingual and cross-lingual text
processing techniques is becoming increasingly im-
portant. There is a growing number of methods that
use data available in one language to build text pro-
cessing tools for another language, for diverse tasks
such as word sense disambiguation (Ng et al., 2003),
syntactic parsing (Hwa et al., 2005), information re-
trieval (Monz and Dorr, 2005), subjectivity analysis
(Mihalcea et al., 2007), and others.

In this paper, we address the task of cross-lingual
text classification (CLTC), which builds text classi-
fiers for multiple languages by using training data in
one language, thereby avoiding the costly and time-
consuming process of labeling training data for each
individual language. The main idea underlying our
approach to CLTC is that although content can be
expressed in different forms in different languages,

there is a significant amount of knowledge that is
shared for similar topics that can be effectively used
to port topic classifiers across languages.

Previous methods for CLTC relied mainly on ma-
chine translation, by translating the training data into
the language of the test data or vice versa, so that
both training and test data belong to the same lan-
guage. Monolingual text classification algorithms
can then be applied on these translated data. Al-
though intuitive, these methods suffer from two ma-
jor drawbacks.

First, most off-the-shelf machine translation sys-
tems typically generate only their best translation for
a given text. Since machine translation is known
to be a notoriously hard problem, applying mono-
lingual text classification algorithms directly on the
erroneous translation of training or test data may
severely deteriorate the classification accuracy.

Second, similar to domain adaptation in statisti-
cal machine learning, due to the discrepancy of data
distribution between the training domain and test do-
main, data distribution across languages may vary
because of the difference of culture, people’s inter-
ests, linguistic expression in different language re-
gions. So even if the translation of training or test
data is perfectly correct, the cross language classi-
fier may not perform as well as the monolingual one
trained and tested on the data from the same lan-
guage.

In this paper, we propose a new approach to
CLTC, which trains a classification model in the
source language and ports the model to the target
language, with the translation knowledge learned us-
ing the EM algorithm. Unlike previous methods
based on machine translation (Fortuna and Shawe-
Taylor, 2005), our method takes into account dif-

1057



ferent possible translations for model features. The
translated model serves as an initial classifier for a
semi-supervised process, by which the model is fur-
ther adjusted to fit the distribution of the target lan-
guage. Our method does not require any labeled
data in the target language, nor a machine transla-
tion system. Instead, the only requirement is a rea-
sonable amount of unlabeled data in the target lan-
guage, which is often easy to obtain.

In the following sections, we first review related
work. In section 3, we introduce our method that
translates the classification model with the trans-
lation knowledge learned using the EM algorithm.
Section 4 describes model adaptation by training the
translated model with unlabeled documents in the
target language. Experiments and evaluations are
presented in section 5 and finally we conclude the
paper in section 6.

2 Related Work

Text classification has rightfully received a lot of at-
tention from both the academic and industry com-
munities, being one of the areas in natural language
processing that has a very large number of practi-
cal applications. Text classification techniques have
been applied to many diverse problems, ranging
from topic classification (Joachims, 1997), to genre
detection (Argamon et al., 1998), opinion identifica-
tion (Pang and Lee, 2004), spam detection (Sahami
et al., 1998), gender and age classification (Schler et
al., 2006).

Text classification is typically formulated as a
learning task, where a classifier learns how to distin-
guish between categories in a given set, using fea-
tures automatically extracted from a collection of
documents. In addition to the learning methodol-
ogy itself, the accuracy of the text classifier also de-
pends to a large extent upon the amount of training
data available at hand. For instance, distinguish-
ing between two categories for which thousands of
manually annotated examples are already available
is expected to perform better than trying to separate
categories that have only a handful of labeled docu-
ments.

Some of the most successful approaches to date
for text classification involve the use of machine
learning methods, which assume that enough an-

notated data is available such that a classification
model can be automatically learned. These include
algorithms such as Naive Bayes (Joachims, 1997;
McCallum and Nigam, 1998), Rocchio classifiers
(Joachims, 1997; Moschitti, 2003), Maximum En-
tropy (Nigam et al., 1999) or Support Vector Ma-
chines (Vapnik, 1995; Joachims, 1998). If only
a small amount of annotated data is available, the
alternative is to use semi-supervised bootstrapping
methods such as co-training or self-training, which
can also integrate raw unlabeled data into the learn-
ing model (Blum and Mitchell, 1998; Nigam and
Ghani, 2000).

Despite the attention that monolingual text clas-
sification has received from the research commu-
nity, there is only very little work that was done
on cross-lingual text classification. The work that
is most closely related to ours is (Gliozzo and Strap-
parava, 2006), where a multilingual domain kernel is
learned from comparable corpora, and subsequently
used for the cross-lingual classification of texts. In
experiments run on Italian and English, Gliozzo and
Strapparava showed that the multilingual domain
kernel exceeds by a large margin a bag-of-words ap-
proach. Moreover, they demonstrated that the use
of a bilingual dictionary can drastically improve the
performance of the models learned from corpora.

(Fortuna and Shawe-Taylor, 2005; Olsson et al.,
2005) studied the use of machine translation tools
for the purpose of cross language text classification
and mining. These approaches typically translate
the training data or test data into the same language,
followed by the application of a monolingual classi-
fier. The performance of such classifiers very much
depends on the quality of the machine translation
tools. Unfortunately, the development of statistical
machine translation systems (Brown et al., 1993) is
hindered by the lack of availability of parallel cor-
pora and the quality of their output is often erro-
neous. Several methods were proposed (Shi et al.,
2006; Nie et al., 1999) to automatically acquire a
large quantity of parallel sentences from the web,
but such web data is however predominantly con-
fined to a limited number of domains and language
pairs.

(Dai et al., 2007) experimented with the use of
transfer learning for text classification. Although in
this method the transfer learning is performed across

1058



different domains in the same language, the under-
lying principle is similar to CLTC in the sense that
different domains or languages may share a signif-
icant amount of knowledge in similar classification
tasks. (Blum and Mitchell, 1998) employed semi-
supervised learning for training text classifiers. This
method bootstraps text classifiers with only unla-
beled data or a small amount of labeled training data,
which is close to our setting that tries to leverage la-
beled data and unlabeled data in different languages
to build text classifiers.

Finally, also closely related is the work carried out
in the field of sentiment and subjectivity analysis
for cross-lingual classification of opinions. For in-
stance, (Mihalcea et al., 2007) use an English corpus
annotated for subjectivity along with parallel text to
build a subjectivity classifier for Romanian. Sim-
ilarly, (Banea et al., 2008) propose a method based
on machine translation to generate parallel texts, fol-
lowed by a cross-lingual projection of subjectivity
labels, which are used to train subjectivity annota-
tion tools for Romanian and Spanish. A related, yet
more sophisticated technique is proposed in (Wan,
2009), where a co-training approach is used to lever-
age resources from both a source and a target lan-
guage. The technique is tested on the automatic sen-
timent classification of product reviews in Chinese,
and showed to successfully make use of both cross-
language and within-language knowledge.

3 Cross Language Model Translation

To make the classifier applicable to documents in
a foreign language, we introduce a method where
model features that are learned from the training
data are translated from the source language into
the target language. Using this translation process,
a feature associated with a word in the source lan-
guage is transferred to a word in the target language
so that the feature is triggered when the word occurs
in the target language test document.

In a typical translation process, the features would
be translated by making use of a bilingual dictio-
nary. However, this translation method has a major
drawback, due to the ambiguity usually associated
with the entries in a bilingual dictionary: a word in
one language can have multiple translations in an-
other language, with possibly disparate meanings.

If an incorrect translation is selected, it can distort
the classification accuracy, by introducing erroneous
features into the learning model. Therefore, our goal
is to minimize the distortion during the model trans-
lation process, in order to maximize the classifica-
tion accuracy in the target language.

In this paper, we introduce a method that em-
ploys the EM algorithm to automatically learn fea-
ture translation probabilities from labeled text in the
source language and unlabeled text in the target lan-
guage. Using the feature translation probabilities,
we can derive a classification model for the target
language from a mixture model with feature transla-
tions.

3.1 Learning Feature Translation Probabilities
with EM Algorithm

Given a document d from the document collection D
in the target language, the probability of generating
the document P (d) is the mixture of generating d
with different classes c ∈ C:

P (d) =
∑

c

P (d|c)P (c)

In our cross-lingual setting, we view the generation
of d given a class c as a two step process. In the
first step, a pseudo-document d′ is generated in the
source language, followed by a second step, where
d′ is translated into the observed document d in the
target language. In this generative model, d′ is a la-
tent variable that cannot be directly observed. Since
d could have multiple translations d′ in the source
language, the probability of generating d can then
be reformulated as a mixture of probabilities as in
the following equation.

P (d) =
∑

c

P (c)
∑
d′

P (d|d′, c)P (d′|c)

According to the bag-of-words assumption,
the document translation probability P (d|d′, c) is
the product of the word translation probabilities
P (wi|w′i, c) , where w′i in d′ is the source language
word that wi is translated from. P (d′|c) is the prod-
uct of P (w′i|c). The formula is rewritten as:

P (d) =
∑

c

P (c)
∑
d′

l∏
i=1

P (wi|w′i, c)P (w′i|c)

1059



where wi is the ith word of the document d with l
words. The prior probability P (c) and the proba-
bility of the source language word w′ given class c
are estimated using the labeled training data in the
source language, so we use them as known parame-
ters. P (wi|w′i, c) is the probability of translating the
word w′i in the source language to the word wi in
the target language given class c, and these are the
parameters we want to learn from the corpus in the
target language.

Using the Maximum Likelihood Estimation
(MLE) framework, we learn the model parameters θ
– the translation probability P (wi|w′i, c) – by max-
imizing the log likelihood of a collection of docu-
ments in the target language:

θ̂ = argmaxθ

m∑
j=1

log(P (dj , θ))

= argmaxθ

m∑
j=1

log(
∑

c

P (c)
∑
d′

lj∏
i=1

P (wi|w′i, c)P (w′i|c))

where m is the number of documents in the corpus
in the target language and lj is the number of words
in the document dj .

In order to estimate the optimal values of the pa-
rameters, we use the EM algorithm (Dempster et al.,
1977). At each iteration of EM we determine those
values by maximizing the expectation using the pa-
rameters from the previous iteration and this itera-
tive process stops when the change in the parameters
is smaller than a given threshold. We can repeat the
following two steps for the purpose above.

• E-step

P (w′c|w) ← P (cw
′w)

P (w)

=
P (w|w′c)P (w′c)∑

c

∑
w′ P (w|w′c)P (w′c)

(1)

• M-step

P (w|w′c)← f(w)P (w
′c|w)∑

w∈K f(w)P (w
′c|w)

(2)

Algorithm 1 EM algorithm for learning translation
probabilities
Dl ← labeled data in the source language
Du ← unlabeled data in the target language
L← bilingual lexicon

1: Initialize P0(w|w′c) = 1nw′ , where (w,w
′) ∈ L,

otherwise P0(w|w′c) = 0;
2: Compute P (w′c) with Dl according to equa-

tion 3
3: repeat
4: Calculate Pt(w′c|w) with Du based on

Pt−1(w|w′c) according to equation 1
5: Calculate Pt(w|w′c) based on Pt−1(w′c|w)

according to equation 2
6: until change of P (w|w′c) is smaller than the

threshold
7: return P (w|w′c)

Here f(w) is the occurrence frequency of the word
w in the corpus. K is the set of translation candi-
dates in the target language for the source language
word w′ according to the bilingual lexicon. P(w’c) is
the probability of occurrence of the source language
word w′ under the class c. It can be estimated from
the labeled source language training data available
as follows and it is regarded as a known parameter
of the model.

P (w′c) =
f(w′c)∑

w′∈V f(w
′c)

(3)

where V is the vocabulary of the source language.
Algorithm 1 illustrates the EM learning process,
where nw′ denotes the number of translation candi-
dates for w′ according to the bilingual lexicon.

Our method requires no labeled training data
in the target language. Many statistical machine
translation systems such as IBM models (Brown
et al., 1993) learn word translation probabilities
from millions of parallel sentences which are mu-
tual translations. However, large scale parallel cor-
pora rarely exist for most language pairs. (Koehn
and Knight, 2000) proposed to use the EM algo-
rithm to learn word translation probabilities from
non-parallel monolingual corpora. However, this
method estimates only class independent transla-
tion probabilities P (wi|w′i), while our approach is
able to learn class specific translation probabilities

1060



P (wi|w′i, c) by leveraging available labeled training
data in the source language. For example, the prob-
ability of translating “bush” as “树丛” (small trees)
is higher than translating as “布什” (U.S. president)
when the category of the text is “botany.”

3.2 Model Translation

In order to classify documents in the target language,
a straightforward approach to transferring the classi-
fication model learned from the labeled source lan-
guage training data is to translate each feature from
the bag-of-words model according to the bilingual
lexicon. However, because of the translation ambi-
guity of each word, a model in the source language
could be potentially translated into many different
models in the target language. Thus, we think of
the probability of the class of a target language doc-
ument as the mixture of the probabilities by each
translated model from the source language model,
weighed by their translation probabilities.

P (c|d,mt) ≈
∑

m′t
P (m′t|ms, c)P (c|d,m′t)

where mt is the target language classification model
and m′t is a candidate model translated from the
model ms trained on the labeled training data in
the source language. This is a very generic rep-
resentation for model translation and the model m
could be any type of text classification. Specifically
in this paper, we take the Maximum Entropy (ME)
model(Berger et al., 1996) as an example for the
model translation across languages, since the ME
model is one of the most widely used text classifica-
tion models. The maximum entropy classifier takes
the form

P (c|d) = 1
Z(d)

∏
w∈V

eλwf(w,c)

where: V is the vocabulary of the language; f(w, c)
is the feature function associated with the word w
and class c and its value is set to 1 when w occurs in
d and the class is c or otherwise 0. λw is the feature
weight for f(wi, c) indicating the importance of the
feature in the model. During model translation, the
feature weight for f(wi, c) is transferred to f(w′i, c)
in the target language model, where w′i is the trans-
lation of wi. Z(d) is the normalization factor which

is invariant to c and hence we can omit it for classi-
fication since our objective is to find the best c. Ac-
cording to the formulation of the Maximum Entropy
model, the document can be classified as follows.

ĉ = argmaxc∈C
∑
m′t

P (m′t|ms, c)
v∏

i=1

e
λ

wis
f(wit,c)

The model translation probability P (m′t|ms, c) can
be modeled as the product of the translation proba-
bilities of each of its individual bag-of-words fea-
tures P (m′t|ms, c) ≈

∏l
i=1 P (w

i
t|wis, c) and the

classification model can be further written as

ĉ = argmaxc∈C
∑
m′t

v∏
i=1

P (wit|wis, c)e
λ

wis
f(wit,c)

where feature translation probabilities P (wit|wis, c)
are estimated with the EM algorithm described in
the previous section. Note that if the average number
of translations for a word w is n and v is the num-
ber of words in the vocabulary there are nv possible
models m′t translated from ms. However, we can
do the following mathematical transformation on the
equation which leads to a polynomial time complex-
ity algorithm. The idea is that instead of enumerat-
ing the exponential number of different translations
of the entire model, we will instead handle one fea-
ture at a time.

∑
m′t

v∏
i=1

P (wit|wis, c)e
λ

wis
f(wit,c) =

n1∑
j=1

P (w1jt |w1s , c)eλ1f(w
1j
t ,c)

∑
m2,vt

v∏
i=2

P (wit|wis, c)eλif(w
i
t,c)

Here w1 is the first word in the vocabulary of the
source language and w1j is a translation of w1 in the
target language with n denoting the number its trans-
lations according to the bilingual lexicon.

∑
m2,vt

are all the target language models translated from
the model consisting of the rest of the words w2 ...
wv in the source language. This process is recur-
sive until the last word wvs of the vocabulary and this
transforms the equation into a polynomial form as

1061



follows. ∑
m′t

v∏
i=1

P (wit|wis, c)e
λ

wis
f(wit,c)

=

v∏
i=1

ni∑
j=1

P (wijt |wis, c)e
λ

wis
f(wijt ,c)

Based on the above transformation, the class ĉ for
the target language document d is then calculated
with the following equation.

ĉ = argmaxc∈C

v∏
i=1

ni∑
j=1

P (wijt |wis, c)e
λ

wis
f(wijt ,c)

The time complexity of computing the above equa-
tion is n× v.

4 Model Adaptation with Semi-
Supervised Learning

In addition to translation ambiguity, another chal-
lenge in building a classifier using training data in
a foreign language is the discrepancy of data distri-
bution in different languages. Direct application of a
classifier translated from a foreign model may not fit
well the distribution of the current language. For ex-
ample, a text about “sports” in (American) English
may talk about “American football,” “baseball,” and
“basketball,” whereas Chinese tend to discuss about
“soccer” or “table tennis.”

To alleviate this problem, we employ semi-
supervised learning in order to adapt the model to
the target language. Specifically, we first start by us-
ing the translated classifier from English as an initial
classifier to label a set of Chinese documents. The
initial classifier is able to correctly classify a num-
ber of unlabeled Chinese documents with the knowl-
edge transferred from English training data. For
instance, words like “game(比赛),” “score(比分),”
“athlete(运动员),” learned from English can still ef-
fectively classify Chinese documents. We then pick
a set of labeled Chinese documents with high con-
fidence to train a new Chinese classifier. The new
classifier can then learn new knowledge from these
Chinese documents. E.g. it can discover that words
like “soccer(足球)” or “badminton(羽毛球)” occur
frequently in the Chinese “sports” documents, while
words that are frequently occurring in English doc-
uments such as “superbowl(超级碗)” and “NHL(全

Algorithm 2 Semi-supervised learning for cross-
lingual text classification
Ls ← labeled data in the source language
Ut ← unlabeled data in the target lan-
guage

1: Cs = train(Ls)
2: Ct = translate(Cs)
3: repeat
4: Label(U,Ct)
5: L← select(confidence(U,Ct))
6: Ct ← train(L)
7: until stopping criterion is met
8: return Ct

美冰球联盟)” do not occur as often. Re-training the
classifier with the Chinese documents can adjust the
feature weights for these words so that the model fits
better the data distribution of Chinese documents,
and thus it improves the classification accuracy. The
new classifier then re-labels the Chinese documents
and the process is repeated for several iterations. Al-
gorithm 2 illustrates this semi-supervised learning
process.

The confidence score associated with the docu-
ments is calculated based on the probabilities of the
class. For a binary classifier the confidence of clas-
sifying the document d is calculated as:

confidence(d) =

∣∣∣∣log(P (c|d)P (c|d))
∣∣∣∣

An unlabeled document is selected as training
data for a new classifier when its confidence score
is above a threshold.

5 Experiments and Evaluation

To evaluate the effectiveness of our method, we
carry out several experiments. First, we compare the
performance of our method on five different cate-
gories, from five different domains, in order to see
its generality and applicability on different domains.
We also run experiments with two different language
pairs - English-Chinese and English-French - to see
if the distance between language families influences
the effectiveness of our method.

To determine the performance of the method with
respect to other approaches, we compare the classi-
fication accuracy with that of a machine translation

1062



approach that translates the training (test) data from
the source language to the target language, as well
as with a classifier trained on monolingual training
data in the target language.

Finally, we evaluate the performance of each of
the two steps of our proposed method. First, we
evaluate the model translated with the parameters
learned with EM, and then the model after the semi-
supervised learning for data distribution adaptation
with different parameters, including the number of
iterations and different amounts of unlabeled data.

5.1 Data Set

Since a standard evaluation benchmark for cross-
lingual text classification is not available, we built
our own data set from Yahoo! RSS news feeds. The
news feed contains news articles from October 1st
2009 to December 31st 2009. We collected a total
of 615731 news articles, categorized by their edi-
tors into topics such as “sports” or “business”. We
selected five categories for our experiments, namely
“sports”, “health”, “business”, “entertainment”, “ed-
ucation”. The Yahoo! RSS news feed includes
news in many languages, including English, Chi-
nese, French, Spanish, and others.

We experimented on two language pairs, English-
Chinese and English-French, selected for their diver-
sity: English and Chinese are disparate languages
with very little common vocabulary and syntax,
whereas English and French are regarded as more
similar. We expect to evaluate the impact of the
distance of languages on the effectiveness of our
method. In both cases, English is regarded as the
source language, where training data are available,
and Chinese and French are the target languages
for which we want to build text classifiers. Note
that regardless of the language, the documents are
assigned with one of the five category labels men-
tioned above. Table 1 shows the distribution of doc-
uments across categories and across languages.

Category English Chinese French
sports 23764 14674 18398
health 15627 11769 12745
business 34619 23692 28740
entertainment 26876 21470 23756
education 16488 14353 15753

Table 1: number of documents in each class

Before building the classification model, several
preprocessing steps are applied an all the docu-
ments. First, the HTML tags are removed, and ad-
vertisements and navigational information are also
eliminated. For the Chinese corpus, all the Chinese
characters with BIG5 encoding are converted into
GB2312 and the Chinese texts are segmented into
words. For the translation, we use the LDC bilin-
gual dictionary1 for Chinese English and “stardict”
2 for Spanish English.

5.2 Model Translation

To transfer a model learned in one language to an-
other, we can translate all the bag-of-word features
according to a bilingual lexicon. Due to the trans-
lation ambiguity of each feature word, we com-
pare three different ways of model translation. One
method is to equally assign probabilities to all the
translations for a given source language word, and
to translate a word we randomly pick a translation
from all of its translation candidates. We denote this
as “EQUAL” and it is our baseline method. Another
way is to calculate the translation probability based
on the frequencies of the translation words in the tar-
get language itself. For instance, the English word
“bush” can be translated into “布什” , “树丛” or “套
管” . We can obtain the following unigram counts
of these translation words in our Yahoo! RSS news
corpus.

count translation sense
582 布什 Goerge W. Bush
43 树丛 small trees
2 套管 canula

We can estimate that P (布什|bush) = 582/(582 +
43+2) = 92.8% and so forth. This method often al-
lows us to estimate reasonable translation probabili-
ties and we use “UNIGRAM” to denote this method.
And finally the third model translation approach is
to use the translation probability learned with the
EM algorithm proposed in this paper. The initial
parameters of the EM algorithm are set to the prob-
abilities calculated with the “UNIGRAM” method
and we use 4000 unlabeled documents in Chinese

1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002L27

2http://stardict.sourceforge.net/Dictionaries.php

1063



to learn translation probabilities with EM. We first
train an English classification model for the topic of
“sport” and then translate the model into Chinese us-
ing translation probabilities estimated by the above
three different methods. The three translated models
are applied to Chinese test data and we measure the
precision, recall and F-score as shown in Table 2.

Method P R F
EQUAL 71.1 70.6 70.8
UNIGRAM 79.5 77.8 78.6
EM 83.1 84.7 83.9

Table 2: Comparison of different methods for model
translation

From this table we can see that the baseline method
has lowest classification accuracy due to the fact that
it is unable to handle translation ambiguity since
picking any one of the translation word is equally
likely. “UNIGRAM” shows significant improve-
ment over “EQUAL” as the occurrence count of the
translation words in the target language can help
disambiguate the translations. However occurrence
count in a monolingual corpus may not always be
the true translation probability. For instance, the
English word “work” can be translated into “工
作(labor)” and “工厂(factory)” in Chinese. How-
ever, in our Chinese monolingual news corpus, the
count for “工厂(factory)” is more than that of “工
作(labor)” even though “工作(labor)” should be a
more likely translation for “work”. The “EM” algo-
rithm has the best performance as it is able to learn
translation probabilities by looking at documents in
both source language and target language instead of
just a single language corpus.

5.3 Cross Language Text Classification

To evaluate the effectiveness of our method on cross
language text classification, we implement several
methods for comparison. In each experiment, we
run a separate classification for each class, using a
one-versus-all binary classification.

ML (Monolingual). We build a monolingual
text classifier by training and testing the text classi-
fication system on documents in the same language.
This method plays the role of an upper-bound, since
the best classification results are expected when

monolingual training data is available.

MT (Machine Translation). We use the Sys-
tran 5.0 machine translation system to translate
the documents from one language into the other
in two directions. The first direction translates the
training data from the source language into the
target language, and then trains a model in the target
language. This direction is denoted as MTS. The
second direction trains a classifier in the source
language and translates the test data into the source
language. This direction is denoted as MTT. In
our experiments, Systran generates the single best
translation of the text as most off-the-shelf machine
translation tools do.

EM (Model Translation with EM). This is the
first step of our proposed method. We used 4,000
unlabeled documents to learn translation proba-
bilities with the EM algorithm and the translation
probabilities are leveraged to translate the model.
The rest of the unlabeled documents are used for
other experimental purpose.

SEMI (Adapted Model after Semi-Supervised
Learning). This is our proposed method, after both
model translation and semi-supervised learning.
In the semi-supervised learning, we use 6,000
unlabeled target language documents with three
training iterations.

In each experiment, the data consists of 4,000 la-
beled documents and 1,000 test documents (e.g., in
the cross-lingual experiments, we use 4,000 English
annotated documents and 1,000 Chinese or French
test documents). For a given language, the same test
data is used across all experiments.

Table 3 shows the performance of the various
classification methods. The ML (Monolingual)
classifier has the best performance, as it is trained
on labeled data in the target language, so that there
is no information loss and no distribution discrep-
ancy due to a model translation. The MT (ma-
chine translation) based approach scores the lowest
accuracy, probably because the machine translation
software produces only its best translation, which
is often error-prone, thus leading to poor classifi-
cation accuracy. In addition, the direct application
of a classification model from one language to an-

1064



English→ Chinese
Category ML MTS MTT EM SEMI

P R F P R F P R F P R F P R F
sports 96.1 94.3 95.2 80.6 81.7 81.2 81.7 83.8 82.7 83.1 84.7 83.9 92.1 91.8 91.9
health 95.1 93.1 94.1 80.8 81.5 81.2 81.6 83.5 82.6 84.5 85.8 85.2 90.2 91.7 90.9
business 91.6 93.1 92.4 81.3 81.9 81.6 80.7 81.0 80.9 81.6 82.0 81.8 87.3 89.3 88.3
entertainment 88.1 88.3 88.2 76.1 78.8 77.5 75.3 78.9 77.1 76.8 79.7 78.2 83.2 83.8 83.5
education 79.1 82.2 80.6 70.2 72.5 71.8 71.1 72.0 71.6 71.2 73.7 72.5 76.2 79.8 78.0

English→ French
sports 95.8 95.0 95.4 82.8 83.6 83.2 82.1 83.0 82.5 85.3 87.1 86.2 92.5 92.1 92.3
health 94.2 94.5 94.3 82.6 83.9 83.2 81.8 83.0 82.4 86.2 87.2 86.6 92.0 92.2 92.1
business 90.1 92.2 91.1 81.4 82.1 81.7 81.3 81.8 81.8 84.4 84.3 84.4 88.3 89.2 88.8
entertainment 87.4 87.2 87.3 76.6 79.1 77.8 76.0 78.8 77.4 78.9 81.0 80.0 84.3 85.5 84.9
education 78.8 81.8 80.3 72.1 74.8 73.5 72.3 72.7 72.5 73.8 76.2 75.0 76.3 80.1 78.2

Table 3: Comparison of different methods and different language pairs

other does not adapt to the distribution of the sec-
ond language, even if the documents belong to the
same domain. Comparing the two MT alternatives,
we can see that translating the training data (MTS)
has better performance than translating the test data
(MTT). The reason is that when the model is trained
on the translated training data, the model parame-
ters are learned over an entire collection of translated
documents, which is less sensitive to translation er-
rors than translating a test document on which the
classification is performed individually.

Our EM method for translating model features
outperforms the machine translation approach, since
it does not only rely on the best translation by the
machine translation system, but instead takes into
account all possible translations with knowledge
learned specifically from the target language. Ad-
ditionally, the SEMI (semi-supervised) learning is
shown to further improve the classification accuracy.
The semi-supervised learning is able to not only help
adapt the translated model to fit the words distribu-
tion in the target language, but it also compensates
the distortion or information loss during the model
translation process as it can down-weigh the incor-
rectly translated features.

The improvement in performance for both the
EM and the SEMI methods is consistent across
the five different domains, which indicates that the
methods are robust and they are insensitive to the
domain of the data.

The performance of the two language pairs
English-Chinese and English-French shows a dif-
ference as initially hypothesized. In both the EM

and the SEMI models, the classification accuracy
of English-French exceeds that of English-Chinese,
which is probably explained by the fact that there is
less translation ambiguity in similar languages, and
they have more similar distributions. Note that the
monolingual models in French and Chinese perform
comparably, which means the difficulty of the test
data is similar between the two target languages.

5.4 Model Adaptation with Semi-Supervised
Learning

Finally, to gain further insights into our proposed
adaptation method, we run several experiments with
different parameters for the semi-supervised learn-
ing stage. As these experiments are very time con-
suming, we run them only on Chinese.

For each of the five categories, we train a classi-
fication model using the 4,000 training documents
in English and then translate the model into Chinese
with the translation parameters learned with EM on
20,000 unlabeled Chinese documents. Then we fur-
ther train the translated model on a set of unlabeled
Chinese documents using a different number of it-
erations and a different amount of unlabeled docu-
ments. Figures 1 and 2 show the results of these
evaluations.

As the plots show, the use of unlabeled data in
the target language can improve the cross-language
classification by learning new knowledge in the
target language. Larger amounts of unlabeled
data in general help, although the marginal bene-
fit drops with increasing amounts of data. Regard-
ing the number of iterations, the best performance is

1065



 70

 75

 80

 85

 90

 95

 0  1000  2000  3000  4000  5000  6000

C
la

ss
if

ic
at

io
n 

F-
sc

or
e

Size of unlabeled data

sports
health

business
entertainment

education

Figure 1: Change in classification F-score for an increas-
ing amount of unlabeled data in the target language

 70

 75

 80

 85

 90

 95

 0  1  2  3  4  5  6

C
la

ss
if

ic
at

io
n 

F-
sc

or
e

Number of iterations

sports
health

business
entertainment

education

Figure 2: Change in classification F-score for a different
number of iterations

achieved after 3-4 iterations.

6 Conclusions

In this paper, we proposed a novel method for cross-
lingual text classification. Our method ports a clas-
sification model trained in a source language to a tar-
get language, with the translation knowledge being
learned using the EM algorithm. The model is fur-
ther tuned to fit the distribution in the target language
via semi-supervised learning. Experiments on dif-
ferent datasets covering different languages and dif-
ferent domains show significant improvement over
previous methods that rely on machine translation.
Moreover, the cross-lingual classification accuracy
obtained with our method was found to be close to
the one achieved using monolingual text classifica-

tion.

Acknowledgments

The work of the second author has been partially
supported by National Science Foundation awards
#0917170 and #0747340. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the National Science
Foundation.

References

S. Argamon, M. Koppel, and G. Avneri. 1998. Style-
based text categorization: What newspaper am i read-
ing? In AAAI-98 Workshop on Learning for Text Cat-
egorization, Madison.

C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. 2008.
Multilingual subjectivity analysis using machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), Honolulu, Hawaii.

A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.

A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory, Morgan Kaufmann Publishers, June.

P. Brown, S. della Pietra, V. della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: parameter estimation. Computational Linguis-
tics, 19(2).

W. Dai, G. Xue, Q. Yang, and Y. Yu. 2007. Transfer-
ring naive bayes classifiers for text classification. In In
Proceedings of the 22nd AAAI Conference on Artificial
Intell igence, pages 540–545.

A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1).

B. Fortuna and J. Shawe-Taylor. 2005. The use of
machine translation tools for cross-lingual text min-
ing. In Learning With Multiple Views, Workshop at
the 22nd International Conference on Machine Learn-
ing (ICML).

A. Gliozzo and C. Strapparava. 2006. Exploiting com-
parable corpora and bilingual dictionaries for cross-
language text categorization. In Proceedings of the
Conference of the Association for Computational Lin-
guistics, Sydney, Australia.

1066



R. Hwa, P. Resnik, and A. Weinberg. 2005. Bootstrap-
ping parsers via syntactic projection across parallel
texts. Natural Language Engineering. Special issue
on Parallel Texts, editors R. Mihalcea and M. Simard.

T. Joachims. 1997. A probabilistic analysis of the Roc-
chio algorithm with TFIDF for text categorization. In
Proceedings of ICML-97, 14th International Confer-
ence on Machine Learning, Nashville, US.

T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137–142.

P. Koehn and K. Knight. 2000. Estimating word transla-
tion probabilities from unrelated monolingua l corpora
using the em algorithm. In National Conference on
Artificial Intelligence (AAAI 2000) Lang kilde, pages
711–715.

A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.

R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Association for Com-
putational Linguistics, Prague, Czech Republic.

C. Monz and B.J. Dorr. 2005. Iterative translation dis-
ambiguation for cross-language information retrieval.
In Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, Salvador, Brazil.

A. Moschitti. 2003. A study on optimal paramter tun-
ing for Rocchio text classifier. In Proceedings of the
European Conference on Information Retrieval, Italy.

H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting par-
allel texts for word sense disambiguation: An empiri-
cal study. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL
2003), Sapporo, Japan, July.

J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. 1999.
Cross-language information retrieval based on parallel
texts and automatic mining of parallel texts from the
Web. In Proceedings of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval.

K. Nigam and R. Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceed-
ings of the Conference on Information and Knowledge
Management (CIKM 2000), McLean, VA, November.

K. Nigam, J. Lafferty, and A. McCallum. 1999. Using
maximum entropy for text classification. In IJCAI-99
Workshop on Machine Learning for Information Fil-
tering.

J.S. Olsson, D. W. Oard, and J. Hajic. 2005. Cross-
language text classification. In Proceedings of the 28th

Annual international ACM SIGIR Conference on Re-
search and Development in information Retrieval.

B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain, July.

M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz.
1998. A Bayesian approach to filtering junk e-mail.
In AAAI-98 Workshop on Learning for Text Catego-
rization, Madison.

J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender on blogging. In Pro-
ceedings of 2006 AAAI Spring Symposium on Com-
putational Approaches for Analyzing Weblogs, pages
199–204, Stanford.

L. Shi, C. Niu, M. Zhou, and J. Gao. 2006. A dom
tree alignment model for mining parallel data from the
web. In Proceedings of the Annual Meeting of the As-
sociation for Computational Lingusitics (ACL 2006),
Sydney, Australia.

V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.

X. Wan. 2009. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Conference
of the Association of Computational Linguistics and
the International Joint Conference on Natural Lan-
guage Processing, Singapore, August.

1067


