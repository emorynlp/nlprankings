



















































PKUSE at SemEval-2019 Task 3: Emotion Detection with Emotion-Oriented Neural Attention Network


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 287â€“291
Minneapolis, Minnesota, USA, June 6â€“7, 2019. Â©2019 Association for Computational Linguistics

287

PKUSE at SemEval-2019 Task 3: Emotion Detection with
Emotion-Oriented Neural Attention Network

Luyao Ma1,2âˆ—, Long Zhang1,2âˆ—, Wei Ye1, Wenhui Hu1
1National Engineering Research Center for Software Engineering, Peking University

2School of Software and Microeconomics, Peking University
{maluyao, zhanglong418, wye, huwenhui}@pku.edu.cn

Abstract

This paper presents the system in SemEval-
2019 Task 3, â€œEmoContext: Contextual Emo-
tion Detection in Textâ€. We propose a
deep learning architecture with bidirectional
LSTM networks, augmented with an emotion-
oriented attention network that is capable of
extracting emotion information from an ut-
terance. Experimental results show that our
model outperforms its variants and the base-
line. Overall, this system has achieved 75.57%
for the microaveraged F1 score.

1 Introduction

With the rapid development of social media plat-
forms like Twitter, a huge number of textual dia-
logues has increasingly emerged. It is a challenge
for chat bots to generate responses based on user
emotions which can avoid inappropriate conversa-
tions. Emotion detection in text (Chatterjee et al.,
2019) is a research area within Natural Language
Processing which is aim to detect the emotion of
user expressed in text.

Many techniques have been proposed, Wang
et al.,Hasan et al.,Liew and Turtle used feature en-
gineering to extract features manually. In this area,
deep learning-based approches have performed
well in recent years. Some methods (WoÌˆllmer
et al., 2010; Metallinou et al., 2012; Poria et al.,
2017; Chernykh et al., 2017) used recurrent neu-
ral network to model the sequence of utterances
for emotion detection. However, those models did
not highlight the emotion-related parts. We use at-
tention mechanism to locate the parts expressing
emotions in the utterance.

The Task3 in Semeval-2019 is to detect contex-
tual emotions in text. For this task, we propose a
deep learning approach which is a combination of

âˆ—âˆ—These authors have contributed equally to this work.

Long Short-Term Memory network and attention
mechanism.

The rest of the paper is organized as follows:
Section 2 provides system overview. Section 3 de-
scribes our approach in detail. Our experiment is
discussed in Section 4. We conclude our work in
Section 5.

2 System Overview

2.1 Text Preprocessing and Word Embedding

We use word embeddings as input to the model.
Word embeddings are distributed vector presenta-
tions of words (Mikolov et al., 2013), capturing
their syntactic and semantic information. A good
word embedding can get a better classification per-
formance. After comparison, we find that the ef-
fect of the GloVe (Pennington et al., 2014) is the
best, but when we turn the word into a word vec-
tor, we find a lot of cases that are out of vocabu-
lary(oov). In view of that, we preprocess the data
as follows:

â€¢ The emoji used in the chat can better express
human emotions, so we turn them into corre-
sponding emotion words and add them to the
sentence, which not only solves the oov, but
also increases the emotion information in the
sentence

â€¢ Several emoticons are replaced by the tokens
â€œhappyâ€, â€œsadâ€, â€œangryâ€

â€¢ All words are lowercased

2.2 Long Short-Term Memory

LSTM is a special form of threshold RNN
(Hochreiter and Schmidhuber, 1997), which is de-
signed to deal with sequential data by sharing its
internal weights across the sequence. Different
from the structure of RNN, LSTM has three gates:



288

an input gate it, a forget gate ft, an output gate
ot and a memory cell ct. Their effect is to allow
the network to store and retrieve information over
long periods of time.

In our approach, we use the bidirectional LSTM
model to better capture the contextual informa-
tion in sentences. Schuster and Paliwal shows that
the bidirectional structure has better performance
in classification experiments. In order to better
handle the relations among the utterances of a di-
alogue, we use the bc-LSTM architecture(Poria
et al., 2017) to process the dialogue-level classi-
fication. The architecture preserves the sequential
order of utterances when constructing the dialogue
representation.

2.3 Attention Mechanism

The attention mechanism was originally applied
to image recognition (Itti and Koch, 2001; Mnih
et al., 2014), mimicking the focus of the eye mov-
ing on different objects when the person viewed
the image. Similarly, when people read an article,
their attention to each part of the text is different.
The attention mechanism imitates human behav-
ior, giving each feature different weights. With
the weight of a feature being greater, the contribu-
tion of this to current recognition becomes greater.
Neural networks with attention mechanism have
been applied in many tasks of NLP, including ma-
chine translation (Bahdanau et al., 2014; Luong
et al., 2015) text summarization (Rush et al., 2015)
text classification (Yang et al., 2016) sentiment
classification (Chen et al., 2016) and stance classi-
fication (Du et al., 2017).

When learning the representations of text se-
quences, word embeddings are the most effective
intermediate representations for capturing seman-
tic information. We embed the classification label
and word into the same semantic space, and then
construct the semantic relatedness of them accord-
ing to the similarity of word embeddings. Our
model obtains the attention weights of the words
through the emotion-oriented attention network,
which highlights the emotion words, thus improv-
ing the performance of the emotion classification.

3 Model Description

Our model has two steps as follows: 1. Extract the
features of each utterance in the dialogue 2. Con-
struct the representation of the dialogue by the fea-
tures of three utterances for emotion classification.

ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€ğ¿ğ‘†ğ‘‡ğ‘€

Feature extractor

Utterance 1    Utterance2    Utterance3

LSTM

Full-Connected 

Softmax for classification

Figure 1: Architecture for emotion classification.

In the feature extraction step: the embedding of
each utterance is fed into the BiLSTM layer to
construct the word representation of each word;
meanwhile we obtain the attention weight of the
corresponding word by the emotion-oriented at-
tention network. We use the inner product of them
to represent the word, and then feed it into the BiL-
STM layer. Finally, we get the representation of
each utterance after the pooling operation(Fig. 2).

In the classification step: the features of the
three utterances obtained from the previous step
are fed into the LSTM layer as timing information
for emotion classification(Fig. 1).

3.1 Embedding Layer

An input sequence X of length T is composed of
word tokens: X = {x1, ..., xT }. Each token xt is
replaced with the corresponding vocabulary index
V (t). The embedding layer transforms the token
into vector et âˆˆ Rd which is selected from the em-
bedding matrix E according to the index, where d
is the dimensionality of the embedding space.

In order to highlight the emotion words in the
sequence, we append the word embedding vector
of â€œemotionâ€ to the embedding of each word in
original text. The emotion-augmented embedding
of a word t is the concatenation of the embedding
vector et and the emotion representation ez ,

ezt = etâ€–ez (1)

where â€– denotes the concatenation operation,
and then the dimention of ezt is 2d.



289

ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€ â€¦ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€

ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€ â€¦ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€

â€¦

â€¦

â€¦

â€¦

Inner Product

Emotion-Oriented Attention
Embedding

Linear

Softmax

â€¦ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡

â€¦ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡

Embedding

BiLSTM

BiLSTM

PoolFeature extracor

ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€ â€¦ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€

ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€ â€¦ğ¿ğ‘†ğ‘‡ğ‘€ ğ¿ğ‘†ğ‘‡ğ‘€

â€¦

Figure 2: Feature extractor of an utterance with emotion-oriented attention network.

3.2 BiLSTM Layer

The LSTM reads the sequence X only in one di-
rection. We use a bidirectional LSTM to get an-
notations of words by summarizing the contextual
information from both directions. A bidirectional
LSTM consists of a forward LSTM

âˆ’â†’
f that reads

the sentence from x1 to xT and a backward LSTMâ†âˆ’
f that reads the sentence from xT to x1. We ob-
tain the annotation ht for each word xt, by con-
catenating the forward hidden state

âˆ’â†’
h t and the

backward one
â†âˆ’
h t,

ht =
âˆ’â†’
h tâ€–
â†âˆ’
h t (2)

3.3 Emotion-Oriented Attention Network

In the task, the emotion words in the conversa-
tion are vital for classification, which cannot be
captured by the BiLSTM. In order to highlight
the emotion-related words in the utterance, we de-
sign an attention mechanism which increases the
weight of the important words on the basis of the
BiLSTM and contributes more to the classification
decision.

We apply a linear layer to convert the emotion-
augmented embedding of a word ezt to a scalar
value ut, and then get a normalized importance
weight Î±t through a softmax function. This weight
is producted with the word representation ht to get
a weighted word representation vt for each word.

ut =Wue
z
t + bu (3)

Î±t =
eutâˆ‘T
i=1 e

ui
(4)

vt = Î±tht (5)

3.4 Pooling Layer

From the idea of network in network, we use
global maxpooling, global averagepooling and last
tensor for the matrix f output of the BiLSTM
layer. Maxpooling can get the most important fea-
tures of all features (Scherer et al., 2010). Aver-
agepooling can get the most common features of
all features. The last tensor output l of the matrix
f can obtain the semantic information of the sen-
tence in forward and backward through BiLSTM.

The utterance representation z is obtained by
the concatenation of the max vector m, the aver-
age vector a and the last vector l.

z = m||a||l (6)

3.5 Emotion Classification

We use the three utterance representations ob-
tained by feature extractor shown in Figure 2 to
construct the dialogue representation. The three
utterance representations [z1, z2, z3] are fed into
the LSTM, and the last time-step hidden state h3
of the LSTM is regarded as the dialogue repre-
sentation r. We pass it to a fully-connected net-
work with a softmax activation function. This



290

layer obtains a normalized four-dimensional vec-
tor through the nonlinear transformation function
of the input vector.

p = softmax(Wfh3 + bf ) (7)

whereWf and bf are the weights and bias terms
of the fullly-connected layer.

4 Evaluation

4.1 Data
The datasets are provided by Semeval-2019 Task
3. Table 1 gives an overview of the datasets. All
the conversations are collected from twitter. The
conversations consist of user 1â€™s tweet, user 2â€™s re-
sponse to the tweet and user 1â€™s response to user2.
The label is the emotion of the third turn that hu-
man judges mark after considering the context of
three rounds of dialogue.

Dataset Happy Sad Angry Others Total
Training 4243 5463 5506 14948 30160
Validation 425 547 551 1495 3018
Test 284 250 298 4677 5509

Table 1: Datasets for Semeval-2019 Task 3.

4.2 Experiments
The model is implemented using Keras 2.0 (Chol-
let et al., 2017). We experiment with Stanfordâ€™s
GloVe 300 dimensional word embeddings trained
on 840 billion words from Common Crawl. Our
model is trained with Adam Optimizer (Kingma
and Ba, 2014) with initial learning rate of 0.001
and batch size of 64. We use BiLSTMs with hid-
den state size 256, with dropout rate 0.5 on the first
BiLSTM layer and dropout rate 0.3 on the second
one to prevent our neural network from overfitting
(Srivastava et al., 2014).

In our task, the size of samples for each class is
not balanced, which will result in the model tend-
ing to be biased toward the majority class with
poor accuracy for the minority class. For this, we
adjust the parameter â€˜class weightâ€™ to weight the
loss function of each class during training. This
can be useful to tell the model to â€œpay more atten-
tionâ€ to samples from an under-represented class.
In this case, we set the parameter â€˜class weightâ€™
(Happy : 2, Sad : 1, Angry : 1, Others : 4)

4.3 Result and Analysis
In order to evaluate the effect of the emotion-
oriented attention network and the balanced class

weights, we compare our approach with its vari-
ants and the baseline.
Variant1: The variant does not adjust the param-
eter â€˜class weightâ€™.
Variant2: The variant changes the emotion-
oriented attention network with the attention
machanism used in (Yang et al., 2016)
Variant3: The variant removes the emotion-
oriented attention network from the model.

model Happy Sad Angry MicroF1
Baseline 0.5461 0.6149 0.5945 0.5861
Variant1 0.7082 0.7574 0.7175 0.7264
Variant2 0.6967 0.7683 0.7375 0.7330
Variant3 0.7051 0.8113 0.7182 0.7423
Our Model 0.7138 0.8088 0.7500 0.7557

Table 2: Performance of our system and its variants.

Table 2 shows that our model outperforms the
other variants which are all above the baseline
0.5861 for the micro-averaged F1 score. Variant3
has the best performance on â€˜Sadâ€™ class and our
model has the best performance on two classes and
micro-averaged F1 score.

To validate that our model has the ability to cap-
ture the emotion-related parts of an utterance, we
visualize the weights of attention for the following
three dialogues. Figure 3 shows that the emotion
words are highlighted in the dialogues, such as
â€˜Hahaâ€™, â€˜funnyâ€™, â€˜coolâ€™, â€˜likeâ€™, â€˜hateâ€™, â€˜feltâ€™, â€˜badâ€™,
â€˜SORRYâ€™, but the model also highlights some triv-
ial words, such as â€˜Giveâ€™.

haha haha whatâ€˜s so funny ğŸ˜„ give me your number        Happy  

You dont have cool features        i don't like you  I hate you Angry

Did you felt bad yeah me too :( I'm sorry                    Sad

Figure 3: Visualization of attention of examples.

5 Conclusion

In this paper, we proposed an emotion-oriented
neural attention network for Semeval-2019 Task
3. The network use the attention mechanism to
select the emotion-related parts in the utterances.
The classification performance of our model is
better than its variants and the baseline. Mean-
while, the visualization shows that the model has
captured more decision-making information in the
dialogue.



291

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019. Semeval-2019
task 3: Emocontext: Contextual emotion detection
in text. In Proceedings of The 13th International
Workshop on Semantic Evaluation (SemEval-2019),
Minneapolis, Minnesota.

Huimin Chen, Maosong Sun, Cunchao Tu, Yankai Lin,
and Zhiyuan Liu. 2016. Neural sentiment classifi-
cation with user and product attention. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1650â€“1659.

Vladimir Chernykh, Grigoriy Sterling, and Pavel Pri-
hodko. 2017. Emotion recognition from speech
with recurrent neural networks. arXiv preprint
arXiv:1701.08071.

FrancÌ§ois Chollet et al. 2017. Keras https://github.
com/fchollet/keras.

Jiachen Du, Ruifeng Xu, Yulan He, and Lin Gui. 2017.
Stance classification with target-specific neural at-
tention networks. International Joint Conferences
on Artificial Intelligence.

Maryam Hasan, Emmanuel Agu, and Elke Runden-
steiner. 2014. Using hashtags as labels for super-
vised learning of emotions in twitter messages. In
ACM SIGKDD Workshop on Health Informatics,
New York, USA.

Sepp Hochreiter and JuÌˆrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735â€“1780.

Laurent Itti and Christof Koch. 2001. Computational
modelling of visual attention. Nature reviews neu-
roscience, 2(3):194.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jasy Suet Yan Liew and Howard R Turtle. 2016. Ex-
ploring fine-grained emotion detection in tweets. In
Proceedings of the NAACL Student Research Work-
shop, pages 73â€“80.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412â€“1421.

Angeliki Metallinou, Martin Wollmer, Athanasios
Katsamanis, Florian Eyben, Bjorn Schuller, and
Shrikanth Narayanan. 2012. Context-sensitive
learning for enhanced audiovisual emotion classifi-
cation. IEEE Transactions on Affective Computing,
3(2):184â€“198.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111â€“3119.

Volodymyr Mnih, Nicolas Heess, Alex Graves, et al.
2014. Recurrent models of visual attention. In
Advances in neural information processing systems,
pages 2204â€“2212.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532â€“1543.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
Navonil Majumder, Amir Zadeh, and Louis-Philippe
Morency. 2017. Context-dependent sentiment anal-
ysis in user-generated videos. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 873â€“883.

Alexander M Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization. arXiv preprint
arXiv:1509.00685.

Dominik Scherer, Andreas MuÌˆller, and Sven Behnke.
2010. Evaluation of pooling operations in convo-
lutional architectures for object recognition. In In-
ternational conference on artificial neural networks,
pages 92â€“101. Springer.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673â€“2681.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929â€“1958.

Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P Sheth. 2012. Harnessing twitterâ€ big
dataâ€ for automatic emotion identification. In 2012
International Conference on Privacy, Security, Risk
and Trust and 2012 International Confernece on So-
cial Computing, pages 587â€“592. IEEE.

Martin WoÌˆllmer, Angeliki Metallinou, Florian Eyben,
BjoÌˆrn Schuller, and Shrikanth Narayanan. 2010.
Context-sensitive multimodal emotion recognition
from speech and facial expression using bidirec-
tional lstm modeling. In Proc. INTERSPEECH
2010, Makuhari, Japan, pages 2362â€“2365.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480â€“1489.


