



















































Recognizing Social Constructs from Textual Conversation


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1293–1298,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Recognizing Social Constructs from Textual Conversation

Somak Aditya and Chitta Baral and Nguyen H. Vo and Joohyung Lee and Jieping Ye,
Zaw Naung and Barry Lumpkin and Jenny Hastings

Dept. of Computer Science, Arizona State University

Richard Scherl
Dept. of Computer Science,

Monmouth University

Dawn M. Sweet
Dept. of Psychology,
Iowa State University

Daniela Inclezan
Dept. of Computer Science,

Miami University

Abstract

In this paper we present our work on rec-
ognizing high level social constructs such as
Leadership and Status from textual conversa-
tion using an approach that makes use of the
background knowledge about social hierarchy
and integrates statistical methods and sym-
bolic logic based methods. We use a stratified
approach in which we first detect lower level
language constructs such as politeness, com-
mand and agreement that help us to infer inter-
mediate constructs such as deference, close-
ness and authority that are observed between
the parties engaged in conversation. These in-
termediate constructs in turn are used to de-
termine the social constructs Leadership and
Status. We have implemented this system
successfully in both English and Korean lan-
guages and achieved considerable accuracy.

1 Introduction and Related Works

The traditional information extraction paradigm has
seen success in extracting simplistic behaviors or
emotions from text. However, to detect high-level
social constructs such as leadership or status, we re-
quire robustly defined notions about language con-
structs that cannot always be directly inferred from
text. Hence, in this paper we focus on extracting
information from text that requires additional back-
ground knowledge and inference. There are a few
works in this direction, such as (Tari et al., 2010),
however our focus in this paper is to extract infor-
mation pertaining to different social constructs from
textual conversation. The earlier research in ana-
lyzing conversations includes developing annotated

chat corpuses (Shaikh et al., 2010), and developing
a socio-cultural phenomena model from discourse
with a small-scale implementation (Strzalkowski et
al., 2010). Other researchers have focused on auto-
matically annotating social behavior in conversation
using statistical approaches (Mayfield et al., 2013).
The discourse structure of a conversation is modeled
as a Hidden Markov Model in (Stolcke, 2000) to de-
termine dialogue acts such as Statement, Question
and Agreement. In (Prabhakaran et al., 2012) an-
notated email threads are presented for facilitating
detection of social relations.

Among recent works, (Gilbert, 2012) uses lin-
guistic cues to discover workplace hierarchy from
emails. The use of phrases to detect language use
such as “commands” is motivating. However, due
to the lack of logical explanation and robust defi-
nition, the effectiveness of this method decreases in
the semi-formally moderated Wikipedia community,
which has interplay of several different LUs such
as command, politeness and informal language. In
(Danescu-Niculescu-Mizil et al., 2012), the authors
explain how reflection of linguistic styles can shed
light on power differentials; though, a social com-
munity like Wikipedia might not always conform to
the lingustic style coordination assumption. For ex-
ample, two friends who are coordinating on writing
an article may have the same status socially, but dif-
ference in their expertise will drive the conversation.
Other works such as (Gupte et al., 2011) have con-
centrated more on other features of the persons in-
volved in social network, than linguistic cues. Also,
we feel that, the hierarchy depends on the task or
the context. In other words, one person could as-

1293



sume different roles in different context. The above
works do not seem to address this. (Prabhakaran et
al., 2012) achieves a commendable accuracy in de-
tecting overt display of “power”. However, by our
definitions, this is a lower level attribute and is sim-
ilar to authoritative behavior which is a lower level
concept than Leadership or Status. Hence, their re-
sults are not directly comparable to ours.

In this paper, we use a mixture of logic-based and
statistical approaches which better encodes the do-
main knowledge and infers higher-level constructs
from indirect textual cues. The aim of this paper is
to formalize the theory behind our work, highlight
the advantages of integration of statistical and logic-
based approaches and present results from an empir-
ical study.

2 Motivation by a use-case

We start our discussion by presenting a use-case and
explain how results of other traditional methods in-
spired us to come up with an integrated approach.

Consider the following conversation from
Wikipedia where the participants discuss about a
misleading animation that is used under the topic
Convolution.

{D: Put a message on the talk page of the guy who
made it. You’re right; g(t) should be g(tau - t), and
(f*g)(t) should be (f*g)(tau).

T: I don’t think he has a talk page. He’s provided the
code so I can reproduce it. I think he’s right with (f*g)(t),
though?

D: Actually, I think we’re both wrong. You need a vari-
able of integration running opposite directions ....

T: I’ve updated ... I guess it’s not important, but would
be kind of cool. Feel free to suggest improvements to the
animations.}

As we understand, these conversations suggest that
participant D is supposed to hold a higher rank/status
than T . If we analyze manually, we understand that
phrases like Put a message, You’re right, I think we’re
both wrong together supports our conclusion. Consid-
ered separately, the above phrases might be misleading.
To conclude the example, our system outputs :

D has a higher status than T because D demonstrates
more language uses associated with status than T. Confi-
dence: high.

The above example illustrates the degree of context-
sensitivity of our problem. The current statistical liter-
ature suggests methods such as Decision Trees, Boost-
ing methods comprising of a collection of Weak classi-

fiers (basically rules) and probabilistic generative models
(Medhat et al., 2014), (Hutto and Gilbert, 2014), (Vanzo
et al., 2014) and (Saif et al., 2012). While their accuracy
on some datasets is quite satisfactory, it is not clear how
well they do on completely unseen data.

From our experience on such classifiers, we believe
that a higher level of accuracy with explainability can
be achieved by imposing a structure that encodes back-
ground knowledge about the social hierarchy that is ob-
served in nature. With this motivation, we built a system
whose hierarchical architecture robustly defines the so-
cial constructs, the “hidden” concepts that induce them
and their inter-connections. We define notions of inter-
mediate Language Use (LU) and lower level Language
Indicator (LI) categories1. With the help of these ro-
bust definitions, our system properly explains how dif-
ferent emotions and behaviors interact to express status
and leadership among individuals.

3 Social Constructs
Our framework supports determination of various impor-
tant Social Constructs such as Leadership, Status, Group
Cohesion and Sub-Group Formation. However, due to
the length constraints of the paper, we will only discuss
Leadership and Status.

3.1 Definitions and Architecture
We begin by first formally defining the two Social Con-
structs and the different Language Use categories.

Leadership: A leader is someone who guides a group
toward outcomes, controls the group actions, manages in-
teractions between members and members usually recog-
nize the leader.

Status: Status is defined as the social position of one
person with respect to another in a group.

The principal Language Use categories that we detect
are: Deference, Closeness, Authoritative Behavior and
Motivational Behavior. The following intuitions are used
to infer such LUs from text:

Deference is understood when one uses language that
shows respect to another conversational participant or de-
fers to another’s expertise or knowledge or authority.

Closeness is understood when one uses language that
shows familiarity with another conversationalist. It is also
indicated by dialogues where conversationalists refer to
similar events, experiences etc.

Authoritative Behavior is understood when one uses
language that shows power, dominance and control over
a situation.

Motivational Behavior is understood when one uses
language that moves conversational participants toward

1These definitions were proposed as part of the IARPA
Socio-Cultural Content In Language(SCIL) program.

1294



Social  
Constructs 

Language  
Uses 

Language  
Indicators 

Leader Status 

Deference Closeness 
Authority 
Behavior 

Motivational 
Behavior 

Politeness 
Respectful 
Appellation 

Apologetic 
Behavior 

Indexical 
Statements 

Informal 
Language 

Command Expertise 

Negative 
Expertise 

Agenda 
Setting 

Agreement 

Disagreement 

Explanation: 

Means that either the 
presence or absence of A 
contributes  to B A 

B 

A 

B Means that only the 
presence of A indicates B 

Seeking 
Support 

Impoliteness 

Encouragement 

Praise 

Criticism 

Resource 
Allocation 

Informal 
Address 

Figure 1: Social Construct-Language Use-Language In-
dicator hierarchy for English Language

sharing a common goal, collaboration, problem solving
and solidarity.
In Figure 1, we present the entire hierarchy and how the
categories are connected among each other. The arrows
in the figure show which of the LI categories are used to
infer a particular type of LU. It also demonstrates how
each of the LU contributes to the Social Constructs.

4 Behind the Curtain: Our Intuitions
One of the fundamental contributions in this paper is for-
mally describing the hierarchy to determine the Social
Constructs, as shown in Figure 1. To come up with these
interconnections and each of the different pieces of the
puzzle, we went through an iterative process of discus-
sions with many social scientists and linguists to analyze
a large number of example conversations. In this process,
we came up with the aforementioned hierarchy, defini-
tions of SC, LU and LIs and most importantly, the fol-
lowing understanding:

• The Language Indicators as shown in the Figure 1,
suffice for the detection of Leadership and Status.

• Each detected LI is associated with an Intensity
Level that helps us to encode the dissimilar effects
of different words in inferring LIs.

• Each LI is associated with a Signed Language Use.
For example, the LI politeness is associated with the
signed LU positive deference.

• Indicators of an LU with a certain sign are counter-
indicators of the same LU with the opposite sign.

• A signed LU may contribute either favorably or
unfavorably towards its associated SC. For exam-
ple, positive authoritative behavior contributes fa-
vorably towards higher status.

• The signed LUs that contribute towards the SC Sta-
tus are ordered based on their importance. We as-
sume the following ordering exists: authoritative
behavior > motivational behavior > negative defer-
ence > positive deference in the opposite direction
> closeness. However, we do not assume such an
ordering for the SC Leadership.

Our extensive research and successful implementation of
our system for different natural languages leads us to be-
lieve that these notions are universal in application.

5 Fundamentals of the implementation

After we parse each sentence using Stanford Dependency
parser to get the POS tags and mutual dependencies, the
detection of individual LIs and the mapping of LIs, LUs
to SCs are achieved using a combination of statistical and
logic based approach. Many of the ideas and insights
about the detection of LIs and their relations with the LUs
are motivated from (Simon, 1946), (Pennebaker et al.,
2003) , (Bernstein, 2010) , (Brown and Levinson, 1988)
and a few others. Some of our ideas for textual inference
have been inspired by (Scherl et al., 2010).

5.1 Determining the Language Indicators
The process of detection of language indicators from sen-
tences uses a huge ensemble of complex rules. To create
these rules, we borrowed ideas from the researchers of so-
cial science and psychology (Simon, 1946; Pennebaker et
al., 2003).

With the help of POS tags, mutual dependencies and
regular expressions, we create a framework where we de-
tect individual events, verbs, other sentence constituents
and their positive and negative sense. On top of this
framework, we use two different methods to detect lan-
guage indicators. The ideas are similar for all the LIs. We
will only present a few examples for the LI “Command”.

5.1.1 Using Regular Expressions Alone
We use regular expressions of the

form “.*\b[wW]hy don’?t (you|YOU)
(start|read|submit|make|write|get)\s*\b.*” to detect
LIs such as “Command”. We employ a collection of such
expressions to cover several different linguistic styles
which indicates “Command” by an individual.

We achieved a very high recall (close to 1.0) for most
indicators with these rules on test data. However, in few
cases, the frequency of such indicators (such as polite-
ness) were very low deeming the set of regular expres-
sions as incomplete. This observation led us to refine the
regular expressions with Logical rules so that we can in-
corporate our domain knowledge and remove such bias
to the training set.

1295



5.1.2 Using Logical rules on Regular
Expression output and Sentence
constituents

One example of the rules we use to detect “Command”
is: if the subject of the verb is second person and the verb
is associated with a modal verb which indicates a ques-
tion that suggests command, then the LI “Command” is
detected.

Examples of such verbs are “Would you” and “Could
you” etc. It is to be noted that such a verb will denote
both politeness and command depending on the rest of
the sentence. This fascinating inter-dependency is one
reason why we have to collect all such Language Indica-
tors before we infer the higher level Language Uses.

5.2 Mapping of LIs to LUs and LUs to Social
Constructs

Input: To encode one conversation we use a collection
of facts of the form participant(X) and addresses(X, Y,
LI, Level).

These facts essentially encode the identity of the par-
ticipants and the Language Indicators observed in the
overall conversation among a pair of participants.

Output: The module outputs a collection of claim, ev-
idence and confidence mappings.

For example one such mapping is: claim_mapping(X,
"is the leader", "because", X, "demonstrates <language
use>","(Confidence: <confidence level>)"). Here <lan-
guage use> is one of the language uses, <confidence
level> is either low, medium, or high.

Algorithm: We employ statistical and logic-based
procedure in parallel to get the above output. On the
statistical side, we adopt a regression technique to learn
a function that can map the scores associated with LIs
to individual LUs based on annotated training data and
this function is then applied to test data to get confidence
score on LUs. The same procedure is adopted for map-
ping LUs to SCs.

In parallel to this procedure, we also employ a rule-
based technique that uses quantized confidence scores
and outputs confidence levels along with explanations.
As we are able to get the explanation from logical rea-
soning, we use the output confidence scores as votes from
statistical learning to output the final confidence level.

The rules for logical reasoning are explained as defini-
tions and intuitions in the following paragraphs.

Mapping LIs into LUs: A signed LU is said to be
exhibited by participant X towards participant Y with a
certain degree of confidence based on the number of in-
dicators(LI) and counter-indicators(LI) of the signed LU
used by X when addressing Y. The confidence in LU is
directly proportional to the difference between the num-
ber of indicators and counter-indicators.

We categorize LUs according to the number of indi-
cators and apply slight variation to the above rules for
each such category. Also, there are a few LIs that, when
used, automatically override the computed confidence
level for an LU and increase it to high. For example,
“criticism” increases confidence level of positive “moti-
vational behavior” to high.

Mapping LUs to SCs: The relative status of two par-
ticipants is determined based on i) the number of rele-
vant signed LUs exhibited by each participant towards
the other, ii) the ordering of relevant signed LUs and iii)
the confidence level in each exhibited signed LU.

The leader is determined based on the number of ex-
hibited relevant LUs (both favorable and unfavorable).

Mapping LIs to SCs: As shown in Figure 1, we di-
rectly associate some of the LIs to Social Constructs. For
such an association, we again adopt the regression tech-
nique mentioned previously. In this case, the confidence
scores from LIs are directly mapped to the confidence
scores of SCs. We combine this confidence with the
above confidence levels using simplistic rules to output
final social constructs.

It should be noted that the constants used in the rules
are obtained from statistics on annotated conversations.
The annotation process involves labels about SCs, LUs
and LIs for each conversation data.

5.3 Brief Details and Results of the Regression
Technique

In this sub-section, we provide few details of the Sparse
Logistic Regression technique we have used alongside
the logical formulation and present few results from our
experiments with relevant statistical methods. We have
used a similar formulations for mapping LIs to LUs and
LUs to SCs. Here, we provide the example of formulat-
ing the entire problem of detection of Social Constructs
directly in the Classification paradigm.

Status and Leadership can be formulated as a three-
class and two-class problem respectively. For Status,
we had 102 samples with the 38(higher), 26(equal) and
38(lower) samples each for three classes. For Lead-
ership, we had 149 samples with 108(not-leader) and
41(leader) samples for the two classes. For both the tasks,
we extracted 28 textual features. We used the one-vs-rest
scheme for multi-class problem. For each task, we eval-
uated the framework as follows: i. First, we randomly
separate the dataset into training set(p) and test set(1-p).
ii. In the training set, we use 10-fold cross validation to
select proper parameters. iii. We iterate the above proce-
dure for 100 times, and accuracy is evaluated on the pre-
dictions in all iterations. iv. We select different p (from
0.25 to 0.9) and observe the change of accuracy.

We compared the accuracy achieved using Sparse Lo-
gistic Regression with SVM(with RBF Kernel) among

1296



(a) (b)

Figure 2: (a) Training set percentage vs Accuracy graph
for Leadership problem, (b) Training set percentage vs
Accuracy graph for Status classification problem.

others. The accuracy comparison of the SVM(with RBF
kernel) and sparse Logistic Regression is provided in Fig-
ure 2. As we can observe, though the two methods are
comparable, in most cases Sparse Logistic regression per-
forms better.

5.4 Advantages from the integrated approach
The primary advantages are the following:

In general, statistical approaches need a “lot of data”
to attain a certain level of accuracy. As the rules we use
are quite universal and compact, we can achieve a com-
parable(or higher) accuracy with much less training data.

Using the evidence and claim mappings, we give an
“explanation” as to why we detected such a particular SC
in the dialogue. Knowldege of such depth is very hard to
achieve with only statistical approaches.

Explicit representation of “context” specific informa-
tion via rules results in improved accuracy in detection of
LIs such as criticism, praise, command etc.

Statistical modules complement the rule-based ap-
proach where our domain knowledge is “incomplete”.

We use ASP as the Logic Programming language of
our choice as its ability to represent defaults and excep-
tions eases the implementation procedure.

6 Results
We have implemented this system using ASP(Gelfond
and Lifschitz, 1988) and Java. The Wikipedia con-
versations are obtained by parsing the wiki dump from
http://dumps.wikimedia.org/. We also evalu-
ated on the NWTRB (US Nuclear Waste Technical Re-
view Board) dataset. The accuracy and F1 measure are
summarized in Table 1 for approximately two thousand
English and one thousand Korean Wikipedia conversa-
tions. We evaluated two types of questions - i. Yes-No
indicates questions like Is John the leader? and ii. List
indicates questions such as List all the leaders.. Our work

is perhaps unique in determining such social constructs
and evaluating on familiar and unfamiliar datasets. Table

Table 1: Results

SC Q-Type Language Accuracy F1
Task Leader Y-N EN 0.8900 0.6700
Task Leader List EN 0.6700 0.9900
Status Y-N EN 0.4700 0.3457
Status List EN 0.6923 0.5200
Task Leader Y-N KO 0.5667 0.4338
Status Y-N KO 0.4074 0.3900

1 reports evaluations on wikipedia dump. These values
are computed by comparing the results of our systems
with annotated data. Note, in our experiments, we have
performed strict evaluations. For example, the results
are only marked positive if the complete list of leaders
matches with a human-annotated list. Also, we consider
the “explanation” too while performing the evaluation.
The results are true positive only when the detected con-
struct is correct alongwith the explanation provided by
the reasoning module. In general, the previous research
achieves an accuracy of 0.45 in comparable tasks such as
dialog act tagging (Stolcke, 2000).

7 Conclusion
In this paper, we have proposed a novel approach for log-
ically recognizing social constructs from textual conver-
sations. We have used both statistical classification and
logical reasoning to robustly detect status and leadership
as observed in virtual social networks. From our exper-
iments, we show empirically how our approach achieves
a significant accuracy and provides logical explanation of
construct detection.

This research shows the merits of using logical rules
along with statistical techniques to determine Social Con-
structs. As per our understanding, this level of accuracy
and explainability needs integration of both statistical and
logic based methods. Our observations suggest that there
is an increasing need for such integration in various do-
mains. We believe that this work is one of the early steps
in that direction.

8 Acknowledgement
We thank the IARPA SCIL program for supporting this
research. We also thank NSF for the DataNet Federation
Consortium grant OCI-0940841 and ONR for their grant
N00014-13-1-0334 for partially supporting this research.

1297



References

[Bernstein2010] Basil Bernstein. 2010. A public lan-
guage: some sociological implications of a linguistic
form. British Journal of Sociology, pages 53–69.

[Brown and Levinson1988] Penelope Brown and
STEPHEN C. Levinson. 1988. Politeness: Some Uni-
versals in Language Usage (Studies in Interactional
Sociolinguistics 4). Cambridge University Press.

[Danescu-Niculescu-Mizil et al.2012] Cristian Danescu-
Niculescu-Mizil, Lillian Lee, Bo Pang, and Jon Klein-
berg. 2012. Echoes of power: Language effects and
power differences in social interaction. In Proceed-
ings of the 21st International Conference on World
Wide Web, WWW ’12, pages 699–708, New York, NY,
USA. ACM.

[Gelfond and Lifschitz1988] Michael Gelfond and
Vladimir Lifschitz. 1988. The stable model semantics
for logic programming. pages 1070–1080. MIT Press.

[Gilbert2012] Eric Gilbert. 2012. Phrases that signal
workplace hierarchy. In Proceedings of the ACM
2012 Conference on Computer Supported Cooperative
Work, CSCW ’12, pages 1037–1046, New York, NY,
USA. ACM.

[Gupte et al.2011] Mangesh Gupte, Pravin Shankar, Jing
Li, S. Muthukrishnan, and Liviu Iftode. 2011. Find-
ing hierarchy in directed online social networks. In
Proceedings of the 20th International Conference on
World Wide Web, WWW ’11, pages 557–566, New
York, NY, USA. ACM.

[Hutto and Gilbert2014] C. J. Hutto and Eric Gilbert.
2014. Vader: A parsimonious rule-based model for
sentiment analysis of social media text. In ICWSM.

[Mayfield et al.2013] Elijah Mayfield, David Adamson,
and Carolyn Penstein Rosé. 2013. Recognizing rare
social phenomena in conversation: Empowerment de-
tection in support group chatrooms. pages 104–113.

[Medhat et al.2014] W. Medhat, A. Hassan, and H. Ko-
rashy. 2014. Sentiment analysis algorithms and ap-
plications: A survey. Ain Shams Engineering Journal,
5(4):1093 – 1113.

[Pennebaker et al.2003] James W. Pennebaker,
Matthias R. Mehl, and Kate G. Niederhoffer.
2003. Psychological aspects of natural language use:
Our words, our selves. Annual Review of Psychology,
54(1):547.

[Prabhakaran et al.2012] Vinodkumar Prabhakaran,
Huzaifa Neralwala, Owen Rambow, and Mona Diab.
2012. Annotations for power relations on email
threads. In Proceedings of the Eight International
Conference on Language Resources and Evalua-
tion (LREC’12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).

[Saif et al.2012] Hassan Saif, Yulan He, and Harith Alani.
2012. Semantic sentiment analysis of twitter. In Pro-
ceedings of the 11th International Conference on The
Semantic Web - Volume Part I, ISWC’12, pages 508–
524, Berlin, Heidelberg. Springer-Verlag.

[Scherl et al.2010] R. Scherl, D. Inclezan, and M. Gel-
fond. 2010. Automated inference of socio-cultural
information from natural language conversations. In
IEEE International Conference on Social Computing,
pages 480–487, Aug.

[Shaikh et al.2010] Samira Shaikh, Tomek Strzalkowski,
Aaron Broadwell, Jennifer Stromer-Galley, Sarah Tay-
lor, and Nick Webb. 2010. Mpc: A multi-party chat
corpus for modeling social phenomena in discourse.
In Proceedings of the Seventh International Confer-
ence on LREC, may.

[Simon1946] Herbert A. Simon. 1946. The proverbs
of administration. Public Administration Review,
6(1):53–67.

[Stolcke2000] Andreas Stolcke. 2000. Dialogue act mod-
eling for automatic tagging and recognition of conver-
sational speech.

[Strzalkowski et al.2010] Tomek Strzalkowski,
George Aaron Broadwell, Jennifer Stromer-Galley,
Samira Shaikh, Sarah M. Taylor, and Nick Webb.
2010. Modeling socio-cultural phenomena in
discourse. In COLING 2010, 23rd International
Conference on Computational Linguistics, pages
1038–1046.

[Tari et al.2010] Luis Tari, Saadat Anwar, Shanshan
Liang, James Cai, and Chitta Baral. 2010. Discover-
ing drug-drug interactions: a text-mining and reason-
ing approach based on properties of drug metabolism.
Bioinformatics, 26(18).

[Vanzo et al.2014] Andrea Vanzo, Danilo Croce, and
Roberto Basili. 2014. A context based model for sen-
timent analysis in twitter. In Proceedings of COLING
2014, pages 2345–2354, Dublin, Ireland. Dublin City
University and Association for Computational Lin-
guistics.

1298


