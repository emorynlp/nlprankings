










































Inducing Document Plans for Concept-to-Text Generation


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503–1514,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Inducing Document Plans for Concept-to-text Generation

Ioannis Konstas and Mirella Lapata
Institute for Language, Cognition and Computation

School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB

ikonstas@inf.ed.ac.uk, mlap@inf.ed.ac.uk

Abstract

In a language generation system, a content
planner selects which elements must be in-
cluded in the output text and the ordering be-
tween them. Recent empirical approaches per-
form content selection without any ordering
and have thus no means to ensure that the out-
put is coherent. In this paper we focus on
the problem of generating text from a database
and present a trainable end-to-end generation
system that includes both content selection
and ordering. Content plans are represented
intuitively by a set of grammar rules that op-
erate on the document level and are acquired
automatically from training data. We de-
velop two approaches: the first one is inspired
from Rhetorical Structure Theory and repre-
sents the document as a tree of discourse re-
lations between database records; the second
one requires little linguistic sophistication and
uses tree structures to represent global patterns
of database record sequences within a doc-
ument. Experimental evaluation on two do-
mains yields considerable improvements over
the state of the art for both approaches.

1 Introduction

Concept-to-text generation broadly refers to the task
of automatically producing textual output from non-
linguistic input (Reiter and Dale, 2000). Depend-
ing on the application and the domain at hand, the
input may assume various representations including
databases, expert system knowledge bases, simula-
tions of physical systems, or formal meaning rep-
resentations. Generation systems typically follow
a pipeline architecture consisting of three compo-
nents: content planning (selecting and ordering the

parts of the input to be mentioned in the output text),
sentence planning (determining the structure and
lexical content of individual sentences), and surface
realization (verbalizing the chosen content in natu-
ral language). Traditionally, these components are
hand-engineered in order to ensure output of high
quality.

More recently there has been growing interest
in the application of learning methods because of
their promise to make generation more robust and
adaptable. Examples include learning which con-
tent should be present in a document (Duboue and
McKeown, 2002; Barzilay and Lapata, 2005), how it
should be aligned to utterances (Liang et al., 2009),
and how to select a sentence plan among many al-
ternatives (Stent et al., 2004). Beyond isolated com-
ponents, a few approaches have emerged that tackle
concept-to-text generation end-to-end. Due to the
complexity of the task, most models simplify the
generation process, e.g., by treating sentence plan-
ning and surface realization as one component (An-
geli et al., 2010), by implementing content selection
without any document planning (Konstas and Lap-
ata, 2012; Angeli et al., 2010; Kim and Mooney,
2010), or by eliminating content planning entirely
(Belz, 2008; Wong and Mooney, 2007).

In this paper we present a trainable end-to-end
generation system that captures all components of
the traditional pipeline, including document plan-
ning. Rather than breaking up the generation pro-
cess into a sequence of local decisions, each learned
separately (Reiter et al., 2005; Belz, 2008; Chen and
Mooney, 2008; Kim and Mooney, 2010), our model
performs content planning (i.e., document planning
and content selection), sentence planning (i.e., lex-

1503



Database Records

temp(time:6-21, min:9, mean:15, max:21)
wind-spd(time:6-21, min:15, mean:20, max:30)
sky-cover(time:6-9, percent:25-50)
sky-cover(time:9-12, percent:50-75)
wind-dir(time:6-21, mode:SSE)
gust(time:6-21, min:20, mean:30, max:40)

Output Text
Cloudy, with a high around 20. South southeast wind
between 15 and 30 mph. Gusts as high as 40 mph.

(a) WEATHERGOV

Database Records

desktop(cmd:lclick, name:start, type:button)
start(cmd:lclick, name:settings, type:button)
start-target(cmd:lclick, name:control panel, type:button)
win-target(cmd:dblclick, name:users and passwords, type:item)
contMenu(cmd:lclick, name:advanced, type:tab)
action-contMenu(cmd:lclick, name:advanced, type:button)

Output Text
Click start, point to settings, and then click control panel. Double-
click users and passwords. On the advanced tab, click advanced.

(b) WINHELP

Figure 1: Database records and corresponding text for (a) weather forecasting and (b) Windows trou-
bleshooting. Each record has a type (e.g., win-target), and a set of fields. Each field has a value, which
can be categorical (in typewriter), an integer (in bold), or a literal string (in italics).

icalization of input entries), and surface realization
jointly. We focus on the problem of generating text
from a database. The input to our model is a set of
database records and collocated descriptions, exam-
ples of which are shown in Figure 1.

Given this input, we define a probabilistic
context-free grammar (PCFG) that captures the
structure of the database and how it can be verbal-
ized. Specifically, we extend the model of Kon-
stas and Lapata (2012) which also uses a PCFG to
perform content selection and surface realization,
but does not capture any aspect of document plan-
ning. We represent content plans with grammar
rules which operate on the document level and are
embedded on top of the original PCFG. We essen-
tially learn a discourse grammar following two ap-
proaches. The first one is linguistically naive but
applicable to multiple languages and domains; it ex-
tracts rules representing global patterns of record
sequences within a sentence and among sentences
from a training corpus. The second approach learns
document plans based on Rhetorical Structure The-
ory (RST; Mann and Thomson, 1988); it therefore
has a solid linguistic foundation, but is resource in-
tensive as it assumes access to a text-level discourse
parser.

We learn document plans automatically using
both representations and develop a tractable decod-
ing algorithm for finding the best output, i.e., deriva-
tion in our grammar. To the best of our knowledge,
this is the first data-driven model to incorporate doc-
ument planning in a joint end-to-end system. Exper-
imental evaluation on the WEATHERGOV (Liang et
al., 2009) and WINHELP (Branavan et al., 2009) do-

mains shows that our approach improves over Kon-
stas and Lapata (2012) by a wide margin.

2 Related Work

Content planning is a fundamental component in a
natural generation system. Not only does it deter-
mine which information-bearing units to talk about,
but also arranges them into a structure that cre-
ates coherent output. It is therefore not surpris-
ing that many content planners have been based
on theories of discourse coherence (Hovy, 1993;
Scott and de Souza, 1990). Other work has re-
lied on generic planners (Dale, 1988) or schemas
(Duboue and McKeown, 2002). In all cases, con-
tent plans are created manually, sometimes through
corpus analysis. A few researchers recognize that
this top-down approach to planning is too inflexible
and adopt a generate-and-rank architecture instead
(Mellish et al., 1998; Karamanis, 2003; Kibble and
Power, 2004). The idea is to produce a large set
of candidate plans and select the best one according
to a ranking function. The latter is typically devel-
oped manually taking into account constraints relat-
ing to discourse coherence and the semantics of the
domain.

Duboue and McKeown (2001) present perhaps
the first empirical approach to content planning.
They use techniques from computational biology
to learn the basic patterns contained within a plan
and the ordering among them. Duboue and McK-
eown (2002) learn a tree-like planner from an
aligned corpus of semantic inputs and correspond-
ing human-authored outputs using evolutionary al-

1504



gorithms. More recent data-driven work focuses on
end-to-end systems rather than individual compo-
nents, however without taking document planning
into account. For example, Kim and Mooney (2010)
first define a generative model similar to Liang et
al. (2009) that selects which database records to
talk about and then use an existing surface real-
izer (Wong and Mooney, 2007) to render the cho-
sen records in natural language. Their content plan-
ner has no notion of coherence. Angeli et al. (2010)
adopt a more unified approach that builds on top of
the alignment model of Liang et al. (2009). They
break record selection into a series of locally coher-
ent decisions, by first deciding on what records to
talk about. Each choice is based on a history of
previous decisions, which is encoded in the form
of discriminative features in a log-linear model.
Analogously, they choose fields for each record,
and finally verbalize the input using automatically
extracted domain-specific templates from training
data.

Konstas and Lapata (2012) propose a joint model,
which recasts content selection and surface realiza-
tion into a parsing problem. Their model optimizes
the choice of records, fields and words simultane-
ously, however they still select and order records lo-
cally. We replace their content selection mechanism
(which is based on a simple markovized chaining of
records) with global document representations. A
plan in our model is identified either as a sequence
of sentences, each containing a sequence of records,
or as a tree where the internal nodes denote dis-
course information and the leaf nodes correspond to
records.

3 Problem Formulation

The generator takes as input a set of database
records d and outputs a text g that verbalizes some
of these records. Each record token ri ∈ d, with
1 ≤ i ≤ |d|, has a type ri.t and a set of fields f as-
sociated with it. Fields have different values f .v and
types f .t (i.e., integer, categorical, or literal strings).
For example, in Figure 1b, win-target is a record
type with three fields: cmd (denotes the action the
user must perform on an object on their screen,
e.g., left-click), name (denotes the name of the ob-
ject), and type (denotes the type of the object). The
values of these fields are dblclick, users and pass-
words, and item; name is a literal string, the rest are

Grammar Rules
1. S→ R(start)

2. R(ri.t)→ FS(r j,start) R(r j.t) | FS(r j,start)

3. FS(r,r. fi)→ F(r,r. f j) FS(r,r. f j) | F(r,r. f j)

4. F(r,r. f )→W(r,r. f ) F(r,r. f ) |W(r,r. f )

5. W(r,r. f )→ α | g( f .v)

Figure 2: Grammar G of the original model. Paren-
theses denote features, and impose constraints on the
grammar.

categorical.
During training, our algorithm is given a corpus

consisting of several scenarios, i.e., database records
paired with texts w (see Figure 1). For each sce-
nario, the model first decides on a global document
plan, i.e., it selects which types of records belong to
each sentence (or phrase) and how these sentences
(or phrases) should be ordered. Then it selects ap-
propriate record tokens for each type and progres-
sively chooses the most relevant fields; then, based
on the values of the fields, it generates the final text,
word by word.

4 Original Model

Our work builds on the model developed by Kon-
stas and Lapata (2012). The latter is essentially
a PCFG which captures both the structure of the
input database and the way it renders into natural
language. This grammar-based approach lends it-
self well to the incorporation of document planning
which has traditionally assumed tree-like represen-
tations. We first briefly describe the original model
and then present our extensions in Section 5.

Grammar Grammar G in Figure 2 defines a set
of non-recursive CFG rewrite rules that capture the
structure of the database, i.e., the relationship be-
tween records, records and fields, fields and words.
These rules are domain-independent and could be
applied to any database provided it follows the same
structure. Non-terminal symbols are in capitals, the
terminal symbol α corresponds to the vocabulary of
the training set and g( f .v) is a function which gener-
ates integers given the field value f .v. Note that all
non-terminals have features (in parentheses) which

1505



act as constraints and impose non-recursion (e.g., in
rule (2) i 6= j, so that a record cannot emit itself).

Rule (1) defines the expansion from the start sym-
bol S to the first record R of type start. The rules
in (2) implement content selection, by choosing ap-
propriate records from the database and generating
a sequence. R(ri.t) is the source record, R(r j.t) is
the target record and FS(r j.start) is a place-holder
symbol for the set of fields of record token r j. This
method is locally optimal, since it only keeps track
of the previous type of record for each re-write. The
rules in (3) conclude content selection on the field
level, i.e., after we have chosen a record, we select
and order the corresponding fields. Finally, the rules
in (4) and (5) correspond to surface realization. The
former rule binarizes the sequence of words emitted
by a particular field r. f in an attempt to capture local
dependencies between words, such as multi-word
expressions (e.g., right click, radio button). The lat-
ter rule defines the emission of words and integer
numbers1, given a field type and its value. Note that
the original model lexicalizes field values of cate-
gorical and integer type only.

Training The rules of grammar G are associated
with weights that are learned using the EM algo-
rithm (Dempster et al., 1977). During training, the
records, fields and values of database d and the
words w from the associated text are observed, and
the model learns the mapping between them. Notice
that we use w to denote the gold-standard text and
g to refer to the words generated by the model. The
mapping between the database and the observed text
is unknown and thus the weights of the rules define
a hidden correspondence h between records, fields
and their values.

Decoding Given a trained grammar G and an in-
put scenario from a database d, the model generates
text by finding the most likely derivation, i.e., se-
quence of rewrite rules for the input. Although re-
sembling parsing, the generation task is subtly dif-
ferent. In parsing, we observe a string of words and
our goal is to find the most probable syntactic struc-
ture, i.e., hidden correspondence ĥ. In generation,

1The function g( f .v) : Z→ Z, generates an integer in the
following six ways (Liang et al., 2009): identical, rounding
up/down to a multiple of 5, rounding off a multiple of 5 and
adding or subtracting some noise modelled by a geometric dis-
tribution.

however, the string is not observed; instead, we must
find the best text ĝ, by maximizing both over h and g,
where g = g1 . . .gN is a sequence of words licensed
by G. More formally:

ĝ = f
(

argmax
g,h

P
(
(g,h)

))
(1)

where f is a function that takes as input a derivation
tree (g,h) and returns ĝ. Konstas and Lapata (2012)
use a modified version of the CYK parser (Kasami,
1965; Younger, 1967) to find ĝ. Specifically, they
intersect grammar G with a n-gram language model
and calculate the most probable generation ĝ as:

ĝ = f
(

argmax
g,h

p(g) · p(g,h |d)
)

(2)

where p(g,h |d) is the decoding likelihood for a se-
quence of words g = g1 . . .gN of length N and the
hidden correspondence h that emits it, i.e., the likeli-
hood of the grammar for a given database input sce-
nario d. p(g) is a measure of the quality of each out-
put and is provided by the n-gram language model.

5 Extensions

In this section we extend the model of Konstas and
Lapata (2012) by developing two more sophisticated
content selection approaches which are informed by
a global plan of the document to be generated.

5.1 Planning with Record Sequences
Grammar Our key idea is to replace the content
selection mechanism of the original model with a
document plan which essentially defines a gram-
mar on record types. We split a document into
sentences, each terminated by a full-stop. Then a
sentence is further split into a sequence of record
types. Contrary to the original model, we observe a
complete sequence2 of record types, split into sen-
tences. This way we learn domain-specific pat-
terns of frequently occurring record type sequences
among the sentences of a document, as well as more
local structures within a sentence. We thus substitute
rules (1)–(2) in Figure 2 with sub-grammar GRSE
based on record type sequences:

Definition 1 (GRSE grammar)

GRSE = {ΣR, NRSE , PRSE , D}
2Note that a sequence is different from a permutation, as we

may allow repetitions or omissions of certain record types.

1506



where ΣR is a set of terminal symbols R(r.t), and
NRSE is a set of non-terminal symbols:

NRSE = {D, SENT}

where D represents the start symbol and SENT a
sequence of records. PRSE is a set of production rules
of the form:

(a) D→ SENT (ti, . . . , t j) . . . SENT (tl, . . . , tm)

(b) SENT (ti, . . . , t j)→ R(ra.ti) . . . R(rk.t j) ·

where t is a record type, ti, t j, tl and tm may overlap
and ra, rk are record tokens of type ti and t j respec-
tively. The corresponding weights for the production
rules PRSE are:

Definition 2 (GRSE weights)

(a) p(ti, . . . , t j, . . . tl, . . . , tm | D)

(b) p(ti) · ... · p(t j) = 1|s(ti)| · . . . ·
1
|s(t j)|

where s(t) is a function that returns the set of records
with type t (Liang et al., 2009).

Rule (a) defines the expansion from the start sym-
bol D to a sequence of sentences, each represented
by the non-terminal SENT . Similarly to the original
grammar G, we employ the use of features (in paren-
theses) to denote a sequence of record types. The
same record types may recur in different sentences,
but not in the same one. The weight of rule (a) is
simply the joint probability of all the record types
present, ordered and segmented appropriately into
sentences in the document, given the start symbol.

Once record types have been selected (on a per
sentence basis) we move on to rule (b) which de-
scribes how each non-terminal SENT expands to
an ordered sequence of records R, as they are ob-
served within a sentence (see the terminal sym-
bol ‘.’ at the end of the rule). Notice that a record
type ti may correspond to several record tokens ra.
Rules (3)–(5) in grammar G make decisions on these
tokens based on the overall content of the database
and the field/value selection. The weight of this
rule is the product of the weights of each record
type. This is set to the uniform distribution over
{1, ..., |s(t)|} for record type t, where |s(t)| is the
number of records with that type.

Figure 3d shows an example tree for the database
input in Figure 1b, using GRSE and assuming that the
alignments between records and text are given. The

top level of the tree refers to the sequence of record
types as they are observed in the text. The first sen-
tence contains three records with types ‘desktop’,
‘start’ and ‘start-target’, each corresponding to the
textual segments click start, point to settings, and
then click control panel. The next level on the tree,
denotes the choice of record tokens for each sen-
tence, provided that we have decided on the choice
and order of their types (see Figure 3b). In Fig-
ure 3d, the bottom-left sub-tree corresponds to the
choice of the first three records of Figure 1b.

Training A straightforward way to train the ex-
tended model would be to embed the parameters of
GRSE in the original model and then run the EM al-
gorithm using inside-outside at the E-step. Unfortu-
nately, this method will induce a prohibitively large
search space. Rule (a) enumerates all possible com-
binations of record type sequences and the number
grows exponentially even for a few record types and
a small sequence size. To tackle this problem, we ex-
tracted rules for GRSE from the training data, based
on the assumption that there will be far fewer unique
sequences of record types per dataset than exhaus-
tively enumerating all possibilities.

For each scenario, we obtain a word-by-word
alignment between the database records and the cor-
responding text. In our experiments we used Liang
et al.’s (2009) unsupervised model, however any
other semi- or fully supervised method could be
used. As we show in Section 7, the quality of the
alignment inevitably correlates with the quality of
the extracted grammar and the decoder’s output. We
then map the aligned record tokens to their corre-
sponding types, merge adjacent words with the same
type and segment on punctuation (see Figure 3b).
Next, we create the corresponding tree according to
GRSE (Figure 3d) and binarize it. We experimented
both with left and right binarization and adhered to
the latter, as it obtained a more compact set of rules.
Finally, we collectively count the rule weights on the
resulting treebank and extract a rule set, discarding
rules with frequency less than three.

Using the extracted (weighted) GRSE rules, we run
the EM algorithm via inside-outside and learn the
weights for the remaining rules in G. Decoding re-
mains the same as in Konstas and Lapata (2012);
the only requirement is that the extracted grammar
remains binarized in order to guarantee the cubic

1507



desktop1

Click start,

start1

point to settings,

start-target1

and then click control panel.

win-target1

Double-click users and passwords.

contMenu1

On the advanced tab ,p

action-contMenu1

click advanced.

(a) Record token alignments

[
desktop start start-target‖win-target‖contMenu action-contMenu‖

]
(b) Record type segmentation

[Click start,]desktop1.t [point to settings, ]start1.t [and then
click control panel.]start−target1.t [Double-click users and
passwords.]win−target1.t [On the advanced tab,]contMenu1.t [click
advanced.]action−contMenu1.t

(c) Segmentation of text into EDUs

D

SENT(c, a-c)

R(a-c1.t)R(c1.t)

SENT(w-t)

R(w-t1.t)

SENT(d, s, s-t)

R(s-t1.t)R(s1.t)R(d1.t)

(d) Document plan using the GRSE grammar

D

Elaboration[N][S]

Elaboration[N][S]

R(a-c1.t)R(c1.t)

Elaboration[N][S]

R(w-t1.t)Elaboration[N][S]

Joint[N][N]

R(s-t1.t)R(s1.t)

R(d1.t)

(e) Document plan using the GRST grammar

Figure 3: Grammar extraction example from the WINHELP domain using GRSE and GRST . For GRSE , we take
the alignments of records on words and map them to their corresponding types (a); we then segment record
types into sentences (b); and finally, create a tree using grammar GRSE (c). For GRST , we segment the text
into EDUs based on the records they align to (d) and output the discourse tree (omitted here for brevity’s
sake); we build the document plan once we substitute the EDUs with their corresponding record types (e).

bound of the Viterbi search algorithm. Note that the
original grammar is limited to the generation of cat-
egorical and integer values. We extend it to support
the generation of strings. The following rule adds a
simple verbatim lexicalization for string values:

W(r,r. f )→ gen str( f .v, i)
gen str( f .v, i) : V →V, f .v ∈V

where V is the set of words for the fields of type
string, and gen str is a function that takes the value
of a string-typed field f .v, and the position i in
the string, and generates the corresponding word at
that position. For example, gen str(users and pass-
words, 3) = passwords. The weight of this rule is set
to 1.

5.2 Planning with Rhetorical Structure Theory

Grammar RST (Mann and Thompson, 1988) is a
theory of text organization which provides a frame-
work for analyzing text. A basic tenet of the the-
ory is that a text consists of hierarchically organized
text spans or elementary discourse units (EDUs) that

stand in various relations to one another (e.g., Elab-
oration, Attribution). These “rhetorical relations”
hold between two adjacent parts of the text, where
typically, one part is “nuclear” and one a “satellite”.
An analysis of a text consists in identifying the re-
lations holding between successively larger parts of
the text, yielding a natural hierarchical description
of the rhetorical organization of the text. From its
very inception, RST was conceived as a way to char-
acterize text and textual relations for the purpose of
text generation.

In order to create a RST-inspired document plan
for our input (i.e., database records paired with
texts), we make the following assumption: each
record corresponds to a unique non-overlapping
span in the collocated text, and can be therefore
mapped to an EDU. Assuming the text has been seg-
mented and aligned to a sequence of records, we
can create a discourse tree with record types (in
place of their corresponding EDUs) as leaf nodes.
Again, we define a sub-grammar GRST which re-
places rules (1)–(2) from Figure 2:

1508



Definition 3 (GRST grammar)

GRST = {ΣR, NRST , PRST , D}

where ΣR is the alphabet of leaf nodes as de-
fined in Section 5.1, NRST is a set of non-terminals
corresponding to rhetorical relations augmented
with nucleus-satellite information (e.g., Elabora-
tion[N][S] stands for the elaboration relation be-
tween the nucleus EDU left-adjoining with the satel-
lite EDU), PRST is the set of production rules of the
form PRST ⊆ NRST ×{NRST ∪ΣR}×{NRST ∪ΣR} as-
sociated with a weight for each rule, and D ∈ NRST
is the root symbol. Figure 3e gives the discourse tree
for the database input of Figure 1b, using GRST .

Training In order to obtain the weighted produc-
tions of GRST , we use an existing state-of-the-art dis-
course parser3 (Feng and Hirst, 2012) trained on the
RST-DT corpus (Carlson et al., 2001). The latter
contains a selection of 385 Wall Street Journal arti-
cles which have been annotated using the framework
of RST and an inventory of 78 rhetorical relations,
classified into 18 coarse-grained categories (Carl-
son and Marcu, 2001). Figure 4 gives a comparison
of the distribution of relations extracted for the two
datasets we used, against the gold-standard annota-
tion of RST-DT. The statistics for the RST-DT cor-
pus are taken from Williams and Power (2008). The
relative frequencies of relations on both datasets fol-
low closely the distribution of those in RST-DT, thus
empirically supporting the application of the RST
framework to our data.

We segment each document in our training set
into EDUs based on the record-to-text alignments
given by the model of Liang et al. (2009) (see Fig-
ure 3c). We then run the discourse parser on the
resulting EDUs, and retrieve the corresponding dis-
course tree; the internal nodes are labelled with
one of the RST relations. Finally, we replace the
leaf EDUs with their respective terminal symbols
R(r.t) ∈ ΣR (Figure 3e) and collect the resulting
grammar productions; their weights are calculated
via maximum likelihood estimation based on their
collective counts in the parse trees licensed by GRST .
Training and decoding of the extended generation
model (after we embed GRST in the original gram-
mar G) is performed identically to Section 5.1.

3Publicly available from http://www.cs.toronto.edu/
˜weifeng/software.html.

6 Experimental Design

Data Since our aim was to evaluate the planning
component of our model, we used datasets whose
documents are at least a few sentences long. Specif-
ically, we generated weather forecasts and trou-
bleshooting guides for an operating system. For
the first domain (henceforth WEATHERGOV) we used
the dataset of Liang et al. (2009), which consists
of 29,528 weather scenarios for 3,753 major US
cities (collected over four days). The database has
12 record types, each scenario contains on average
36 records, 5.8 out of which are mentioned in the
text. A document has 29.3 words and is four sen-
tences long. The vocabulary is 345 words. We used
25,000 scenarios from WEATHERGOV for training,
1,000 scenarios for development and 3,528 scenar-
ios for testing.

For the second domain (henceforth WINHELP) we
used the dataset of Branavan et al. (2009), which
consists of 128 scenarios. These are articles from
Microsoft’s Help and Support website4 and contain
step-by-step instructions on how to perform tasks on
the Windows 2000 operating system. In its original
format, the database provides a semantic representa-
tion of the textual guide, i.e., it represents the user’s
actions on the operating system’s UI. We semi-
automatically converted this representation into a
schema of records, fields and values, following the
conventions adopted in Branavan et al. (2009).5 The
final database has 13 record types. Each scenario has
9.2 records and each document 51.92 words with 4.3
sentences. The vocabulary is 629 words. We per-
formed 10-fold cross-validation on the entire dataset
for training and testing. Compared to WEATHER-
GOV, WINHELP documents are longer with a larger
vocabulary. More importantly, due to the nature of
the domain, i.e., giving instructions, content selec-
tion is critical not only in terms of what to say but
also in what order.

Grammar Extraction and Parameter Setting
We obtained alignments between database records
and textual segments for both domains and gram-
mars (GRSE and GRST ) using the unsupervised model
of Liang et al. (2009). On WEATHERGOV, we ex-
tracted a GRSE grammar with 663 rules (after bi-

4support.microsoft.com
5The dataset can be downloaded from http://homepages.

inf.ed.ac.uk/ikonstas/index.php?page=resources

1509



E
la

bo
ra

tio
n

A
ttr

ib
ut

io
n

Jo
in

t

C
on

tr
as

t

E
xp

la
na

tio
n

B
ac

kg
ro

un
d

E
na

bl
em

en
t

C
au

se

E
va

lu
at

io
n

C
om

pa
ri

so
n

C
on

di
tio

n

To
pi

c-
C

om
m

en
t

Te
m

po
ra

l

E
xp

la
na

tio
n

Su
m

m
ar

y

To
pi

c
C

ha
ng

e

0

20

40

60 RST-DT
WEATHERGOV

WINHELP

Figure 4: Distribution of RST relations on WEATHERGOV, WINHELP, and the RST-DT (Williams and Power,
2008).

narization). The WINHELP dataset is considerably
smaller, and as a result the procedure described in
Section 5.1 yields a very sparse grammar. To al-
leviate this, we horizontally markovized the right-
hand side of each rule (Collins, 1999; Klein and
Manning, 2003).6 After markovization, we obtained
a GRSE grammar with 516 rules. On WEATHERGOV,
we extracted 434 rules for GRST . On WINHELP we
could not follow the horizontal markovization pro-
cedure, since the discourse trees are already bina-
rized. Instead, we performed vertical markovization,
i.e., annotated each non-terminal with their parent
node (Johnson, 1998) and obtained a GRST grammar
with 419 rules. The model of Konstas and Lapata
(2012) has two parameters, namely the number of
k-best lists to keep in each derivation, and the or-
der of the language model. We tuned k experimen-
tally on the development set and obtained best re-
sults with 60 for WEATHERGOV and 120 for WIN-
HELP. We used a trigram model for both domains,
trained on each training set.

Evaluation We compared two configurations of
our system, one with a content planning compo-
nent based on record type sequences (GRSE) and

6When horizontally markovizing, we can encode an arbi-
trary amount of context in the intermediate non-terminals that
result from this process; in our case we store h=1 horizontal
siblings plus the mother left-hand side (LHS) non-terminal, in
order to uniquely identify the Markov chain. For example,
A→ B C D becomes A→ B 〈A . . .B〉, 〈A . . .B〉 → C 〈A . . .C〉,
〈A . . .C〉 → D.

another one based on RST (GRST ). In both cases
content plans were extracted from (noisy) unsuper-
vised alignments. As a baseline, we used the orig-
inal model of Konstas and Lapata (2012). We also
compared our model to Angeli et al.’s system (2010),
which is state of the art on WEATHERGOV.

System output was evaluated automatically, using
the BLEU modified precision score (Papineni et al.,
2002) with the human-written text as reference. In
addition, we evaluated the generated text by eliciting
human judgments. Participants were presented with
a scenario and its corresponding verbalization and
were asked to rate the latter along three dimensions:
fluency (is the text grammatical?), semantic correct-
ness (does the meaning conveyed by the text corre-
spond to the database input?) and coherence (is the
text comprehensible and logically structured?). Par-
ticipants used a five point rating scale where a high
number indicates better performance. We randomly
selected 12 documents from the test set (for each do-
main) and produced output with the system of Kon-
stas and Lapata (2012) (henceforth K&L), our two
models using GRSE and GRST , respectively, and An-
geli et al. (2010) (henceforth ANGELI). We also in-
cluded the original text (HUMAN) as gold-standard.
We obtained ratings for 60 (12 × 5) scenario-text
pairs for each domain. Examples of the documents
shown to the participants are given in Table 1.

The study was conducted over the Internet us-

1510



WEATHERGOV WINHELP

G
R

SE
Showers before noon. Cloudy, with a high near
38. Southwest wind between 3 and 8 mph.
Chance of precipitation is 55 %.

Right-click my network places, and then click prop-
erties. Right-click local area connection, and click
properties. Click to select the file and printer sharing
for Microsoft networks, and then click ok.

G
R

ST

Showers likely. Mostly cloudy, with a high around
38. South wind between 1 and 8 mph. Chance of
precipitation is 55 %.

Right-click my network places, and then click proper-
ties. Right-click local area connection. Click file and
printer sharing for Microsoft networks, and click ok.

K
&

L

A chance of showers. Otherwise, cloudy, with a
high near 38. Southwest wind between 3 and 8
mph.

Right-click my network places, click properties.
Right-click local area connection. Click to select the
file and printer sharing for Microsoft networks, and
then click ok.

A
N

G
E

L
I A chance of rain or drizzle after 9am. Mostly

cloudy, with a high near 38. Southwest wind be-
tween 3 and 8 mph. Chance of precipitation is 50
%.

Right-click my network places, and then click prop-
erties on the tools menu, and then click proper-
ties. Right-click local area connection, and then click
properties. Click file and printer sharing for Microsoft
networks, and then click ok.

H
U

M
A

N A 50 percent chance of showers. Cloudy, with a
high near 38. Southwest wind between 3 and 6
mph.

Right-click my network places, and then click proper-
ties. Right-click local area connection, and then click
properties. Click to select the file and printer sharing
for Microsoft networks check box. Click ok.

Table 1: Human-authored text and system output on WEATHERGOV and WINHELP.

ing Amazon Mechanical Turk7, and involved 200
volunteers (100 for WEATHERGOV, and 100 for
WINHELP), all self reported native English speak-
ers. For WINHELP, we made sure participants were
computer-literate and familiar with the Windows op-
erating system by administering a short question-
naire prior to the experiment.

7 Results

The results of the automatic evaluation are summa-
rized in Table 2. Overall, our models outperform
K&L’s system by a wide margin on both datasets.
The two content planners (GRSE and GRST ) perform
comparably in terms of BLEU. This suggests that
document plans induced solely from data are of sim-
ilar quality to those informed by RST. This is an
encouraging result given that RST-style discourse
parsers are currently available only for English. AN-
GELI performs better on WEATHERGOV possibly due
to better output quality on the surface level. Their
system defines trigger patterns that specifically lexi-
calize record fields containing numbers. In contrast,
on WINHELP it is difficult to explicitly specify such
patterns, as none of the record fields are numeric; as
a result their system performs poorly compared to

7https://www.mturk.com

the other models.

To assess the impact of the alignment on the
content planner, we also extracted GRSE from
cleaner alignments which we obtained automat-
ically via human-crafted heuristics for each do-
main. The heuristics performed mostly anchor
matching between database records and words in the
text (e.g., the value Lkly of the field rainChance,
matches with the string rain likely in the text).
Using these alignments, GRSE obtained a BLEU
score of 39.23 on WEATHERGOV and 41.35 on WIN-
HELP. These results indicate that improved align-
ments would lead to more accurate grammar rules.
WEATHERGOV seems more sensitive to the align-
ments than WINHELP. This is probably because
the dataset shows more structural variations in the
choice of record types at the document level, and
therefore the grammar extracted from the unsuper-
vised alignments is noisier. Unfortunately, perform-
ing this kind of analysis for GRST would require gold
standard segmentation of our training corpus into
EDUs which we neither have nor can easily approx-
imate via heuristics.

The results of our human evaluation study are
shown in Table 3. We carried out an Analysis of
Variance (ANOVA) to examine the effect of system

1511



Model WEATHERGOV WINHELP
GRSE 35.60 40.92
GRST 36.54 40.65
K&L 33.70 38.26
ANGELI 38.40 32.21

Table 2: Automatic evaluation of system output us-
ing BLEU-4.

WEATHERGOV WINHELP
Model FL SC CO FL SC CO

GRSE 4.25 3.75 4.18 3.59 3.21 3.35
GRST 4.10 3.68 4.10 3.45 3.29 3.22
K&L 3.73 3.25 3.59 3.27 2.97 2.93
ANGELI 3.90 3.44 3.82 3.44 2.79 2.97
HUMAN 4.22 3.72 4.11 4.20 4.41 4.25

Table 3: Mean ratings for fluency (FL), semantic
correctness (SC) and coherence (CO) on system out-
put elicited by humans.

type (GRSE , GRST , K&L, ANGELI, and HUMAN) on
fluency, semantic correctness and coherence ratings.
Means differences of 0.2 or more are significant at
the 0.05 level using a post-hoc Tukey test. Interest-
ingly, we observe that document planning improves
system output overall, not only in terms of coher-
ence. Across all dimensions our models are per-
ceived better than K&L and ANGELI. As far as co-
herence is concerned, the two content planners are
rated comparably (differences in the means are not
significant). Both GRSE and GRST are significantly
better than the comparison systems (ANGELI and
K&L). Table 1 illustrates examples of system out-
put along with the gold standard content selection
for reference, for the WEATHERGOV and WINHELP
domains, respectively.

In sum, we observe that integrating document
planning either via GRSE or GRST boosts perfor-
mance. Document plans induced from record
sequences exhibit similar performance, compared
to those generated using expert-derived linguistic
knowledge. Our systems are consistently better than
K&L both in terms of automatic and human eval-
uation and are close or better than the supervised
model of Angeli et al. (2010). We also show that
feeding the system with a grammar of better qual-
ity can achieve state-of-the-art performance, without

further changes to the model.

8 Conclusions

In this paper, we have proposed an end-to-end sys-
tem that generates text from database input and cap-
tures all components of the traditional generation
pipeline, including document planning. Document
plans are induced automatically from training data
and are represented intuitively by PCFG rules cap-
turing the structure of the database and the way it
renders to text. We proposed two complementary
approaches to inducing content planners. In a first
linguistically naive approach, a document is mod-
elled as a sequence of sentences and each sentence
as a sequence of records. Our second approach
draws inspiration from Rhetorical Structure Theory
(Mann and Thomson, 1988) and represents a docu-
ment as a tree with intermediate nodes correspond-
ing to discourse relations, and leaf nodes to database
records.

Experiments with both approaches demonstrate
improvements over models that do not incorporate
document planning. In the future, we would like to
tackle more challenging domains, such as NFL re-
caps, financial articles and biographies (Howald et
al., 2013; Schilder et al., 2013). Our models could
also benefit from the development of more sophis-
ticated planners either via grammar refinement or
more expressive grammar formalisms (Cohn et al.,
2010).

Acknowledgments

We are grateful to Percy Liang and Gabor Angeli
for providing us with their code and data. Thanks to
Giorgio Satta and Charles Sutton for helpful com-
ments and suggestions. We also thank the members
of the Probabilistic Models reading group at the Uni-
versity of Edinburgh for useful feedback.

References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A

simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 502–512, Cambridge, MA.

Regina Barzilay and Mirella Lapata. 2005. Collec-
tive content selection for concept-to-text generation.
In Proceedings of Human Language Technology and

1512



Empirical Methods in Natural Language Processing,
pages 331–338, Vancouver, British Columbia.

Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 14(4):431–455.

S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
82–90, Suntec, Singapore.

L. Carlson and D. Marcu. 2001. Discourse tagging ref-
erence manual. Technical report, Univ. of Southern
California / Information Sciences Institute.

Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue - Volume 16, SIGDIAL ’01, pages 1–10,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

David L. Chen and Raymond J. Mooney. 2008. Learn-
ing to sportscast: A test of grounded language acqui-
sition. In Proceedings of International Conference on
Machine Learning, pages 128–135, Helsinki, Finland.

Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, 11(November):3053–
3096.

M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.

Robert Dale. 1988. Generating referring expressions in
a domain of objects and processes. Ph.D. thesis, Uni-
versity of Edinburgh.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society, se-
ries B, 39(1):1–38.

Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In Proceedings of the 39th An-
nual Meeting on Association for Computational Lin-
guistics, pages 172–179.

Pablo A. Duboue and Kathleen R. McKeown. 2002.
Content planner construction via evolutionary algo-
rithms and a corpus-based fitness function. In Pro-
ceedings of International Natural Language Genera-
tion, pages 89–96, Ramapo Mountains, NY.

Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In Pro-

ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 60–68, Jeju
Island, Korea.

Eduard Hovy. 1993. Automated discourse generation
using discourse structure relations. Artificial Intelli-
gence, 63:341–385.

Blake Howald, Ravikumar Kondadadi, and Frank
Schilder. 2013. Domain adaptable semantic clustering
in statistical nlg. In Proceedings of the 10th Interna-
tional Conference on Computational Semantics (IWCS
2013) – Long Papers, pages 143–154, Potsdam, Ger-
many, March. Association for Computational Linguis-
tics.

Mark Johnson. 1998. Pcfg models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613–
632, December.

Nikiforos Karamanis. 2003. Entity Coherence for De-
scriptive Text Structuring. Ph.D. thesis, University of
Edinburgh.

Tadao Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Techni-
cal Report AFCRL-65-758, Air Force Cambridge Re-
search Lab, Bedford, MA.

Rodger Kibble and Richard Power. 2004. Optimising
referential coherence in text generation. Computa-
tional Linguistics, 30(4):401–416.

Joohyun Kim and Raymond Mooney. 2010. Generative
alignment and semantic parsing for learning from am-
biguous supervision. In Proceedings of the 23rd Con-
ference on Computational Linguistics, pages 543–551,
Beijing, China.

Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423–430. Association for Computa-
tional Linguistics Morristown, NJ, USA.

Ioannis Konstas and Mirella Lapata. 2012. Unsupervised
concept-to-text generation with hypergraphs. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 752–
761, Montréal, Canada.

Percy Liang, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 91–99, Suntec, Singapore.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.

William C. Mann and Sandra A. Thomson. 1988.
Rhetorical structure theory. Text, 8(3):243–281.

1513



Chris Mellish, Alisdair Knott, Jon Oberlander, and Mick
O’Donnell. 1998. Experiments using stochastic
search for text planning. In Proceedings of Interna-
tional Natural Language Generation, pages 98–107,
New Brunswick, NJ.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia.

Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, New York, NY.

Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Ian
Davy. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137–
169.

Frank Schilder, Blake Howald, and Ravi Kondadadi.
2013. Gennext: A consolidated domain adaptable nlg
system. In Proceedings of the 14th European Work-
shop on Natural Language Generation, pages 178–
182, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.

Donia Scott and Clarisse Sieckenius de Souza. 1990.
Getting the message across in RST-based text gener-
ation. In Robert Dale, Chris Mellish, and Michael
Zock, editors, Current Research in Natural Language
Generation, pages 47–73. Academic Press, New York.

Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of Association for Computational Linguis-
tics, pages 79–86, Barcelona, Spain.

Sandra Williams and Richard Power. 2008. Deriving
rhetorical complexity data from the rst-dt corpus. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC’08), May.

Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Proceedings of the Hu-
man Language Technology and the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172–179, Rochester, NY.

Daniel H Younger. 1967. Recognition and parsing for
context-free languages in time n3. Information and
Control, 10(2):189–208.

1514


