



















































Summarization Through Submodularity and Dispersion


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1014–1022,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Summarization Through Submodularity and Dispersion

Anirban Dasgupta
Yahoo! Labs

Sunnyvale, CA 95054
anirban@yahoo-inc.com

Ravi Kumar
Google

Mountain View, CA 94043
tintin@google.com

Sujith Ravi
Google

Mountain View, CA 94043
sravi@gooogle.com

Abstract
We propose a new optimization frame-
work for summarization by generalizing
the submodular framework of (Lin and
Bilmes, 2011). In our framework the sum-
marization desideratum is expressed as a
sum of a submodular function and a non-
submodular function, which we call dis-
persion; the latter uses inter-sentence dis-
similarities in different ways in order to
ensure non-redundancy of the summary.
We consider three natural dispersion func-
tions and show that a greedy algorithm
can obtain an approximately optimal sum-
mary in all three cases. We conduct ex-
periments on two corpora—DUC 2004
and user comments on news articles—and
show that the performance of our algo-
rithm outperforms those that rely only on
submodularity.

1 Introduction

Summarization is a classic text processing prob-
lem. Broadly speaking, given one or more doc-
uments, the goal is to obtain a concise piece
of text that contains the most salient points in
the given document(s). Thanks to the om-
nipresent information overload facing all of us,
the importance of summarization is gaining; semi-
automatically summarized content is increasingly
becoming user-facing: many newspapers equip
editors with automated tools to aid them in choos-
ing a subset of user comments to show. Summa-
rization has been studied for the past in various
settings—a large single document, multiple docu-
ments on the same topic, and user-generated con-
tent.

Each domain throws up its own set of idiosyn-
crasies and challenges for the summarization task.
On one hand, in the multi-document case (say, dif-
ferent news reports on the same event), the text is

often very long and detailed. The precision/recall
requirements are higher in this domain and a se-
mantic representation of the text might be needed
to avoid redundancy. On the other hand, in the
case of user-generated content (say, comments on
a news article), even though the text is short, one
is faced with a different set of problems: volume
(popular articles generate more than 10,000 com-
ments), noise (most comments are vacuous, lin-
guistically deficient, and tangential to the article),
and redundancy (similar views are expressed by
multiple commenters). In both cases, there is a
delicate balance between choosing the salient, rel-
evant, popular, and diverse points (e.g., sentences)
versus minimizing syntactic and semantic redun-
dancy.

While there have been many approaches to au-
tomatic summarization (see Section 2), our work
is directly inspired by the recent elegant frame-
work of (Lin and Bilmes, 2011). They employed
the powerful theory of submodular functions for
summarization: submodularity embodies the “di-
minishing returns” property and hence is a natural
vocabulary to express the summarization desider-
ata. In this framework, each of the constraints (rel-
evance, redundancy, etc.) is captured as a submod-
ular function and the objective is to maximize their
sum. A simple greedy algorithm is guaranteed to
produce an approximately optimal summary. They
used this framework to obtain the best results on
the DUC 2004 corpus.

Even though the submodularity framework is
quite general, it has limitations in its expressiv-
ity. In particular, it cannot capture redundancy
constraints that depend on pairwise dissimilarities
between sentences. For example, a natural con-
straint on the summary is that the sum or the mini-
mum of pairwise dissimilarities between sentences
chosen in the summary should be maximized; this,
unfortunately, is not a submodular function. We
call functions that depend on inter-sentence pair-

1014



wise dissimilarities in the summary as dispersion
functions. Our focus in this work is on signif-
icantly furthering the submodularity-based sum-
marization framework to incorporate such disper-
sion functions.

We propose a very general graph-based sum-
marization framework that combines a submod-
ular function with a non-submodular dispersion
function. We consider three natural dispersion
functions on the sentences in a summary: sum
of all-pair sentence dissimilarities, the weight of
the minimum spanning tree on the sentences, and
the minimum of all-pair sentence dissimilarities.
These three functions represent three different
ways of using the sentence dissimilarities. We
then show that a greedy algorithm can obtain ap-
proximately optimal summary in each of the three
cases; the proof exploits some nice combinatorial
properties satisfied by the three dispersion func-
tions. We then conduct experiments on two cor-
pora: the DUC 2004 corpus and a corpus of user
comments on news articles. On DUC 2004, we
obtain performance that matches (Lin and Bilmes,
2011), without any serious parameter tuning; note
that their framework does not have the dispersion
function. On the comment corpus, we outperform
their method, demonstrating that value of disper-
sion functions. As part of our methodology, we
also use a new structured representation for sum-
maries.

2 Related Work

Automatic summarization is a well-studied prob-
lem in the literature. Several methods have been
proposed for single- and multi-document summa-
rization (Carbonell and Goldstein, 1998; Con-
roy and O’Leary, 2001; Takamura and Okumura,
2009; Shen and Li, 2010).

Related concepts have also been used in several
other scenarios such as query-focused summariza-
tion in information retrieval (Daumé and Marcu,
2006), microblog summarization (Sharifi et al.,
2010), event summarization (Filatova, 2004), and
others (Riedhammer et al., 2010; Qazvinian et al.,
2010; Yatani et al., 2011).

Graph-based methods have been used for sum-
marization (Ganesan et al., 2010), but in a dif-
ferent context—using paths in graphs to produce
very short abstractive summaries. For a detailed
survey on existing automatic summarization tech-
niques and other related topics, see (Kim et al.,

2011; Nenkova and McKeown, 2012).

3 Framework

In this section we present the summarization
framework. We start by describing a generic ob-
jective function that can be widely applied to sev-
eral summarization scenarios. This objective func-
tion is the sum of a monotone submodular cov-
erage function and a non-submodular dispersion
function. We then describe a simple greedy algo-
rithm for optimizing this objective function with
provable approximation guarantees for three natu-
ral dispersion functions.

3.1 Preliminaries
Let C be a collection of texts. Depending on the
summarization application, C can refer to the set
of documents (e.g., newswire) related to a partic-
ular topic as in standard summarization; in other
scenarios (e.g., user-generated content), it is a col-
lection of comments associated with a news article
or a blog post, etc. For each document c ∈ C,
let S(c) denote the set of sentences in c. Let
U = ∪c∈CS(c) be the universe of all sentences;
without loss of generality, we assume each sen-
tence is unique to a document. For a sentence
u ∈ U , let C(u) be the document corresponding
to u.

Each u ∈ U is associated with a weight w(u),
which might indicate, for instance, how similar u
is to the main article (and/or the query, in query-
dependent settings). Each pair u, v ∈ U is as-
sociated with a similarity s(u, v) ∈ [0, 1]. This
similarity can then be used to define an inter-
sentence distance d(·, ·) as follows: let d′(u, v) =
1 − s(u, v) and define d(u, v) to be the shortest
path distance from u to v in the graph where the
weight of each edge (u, v) is d′(u, v). Note that
d(·, ·) is a metric unlike d′(·, ·), which may not be
a metric. (In addition to being intuitive, d(·, ·) be-
ing a metric helps us obtain guarantees on the al-
gorithm’s output.) For a set S, and a point u 6∈ S,
define d(u, S) = minv∈S d(u, v).

Let k > 0 be fixed. A summary of U is a subset
S ⊆ U, |S| = k. Our aim is to find a summary that
maximizes

f(S) = g(S) + δh(S), (1)

where g(S) is the coverage function that is non-
negative, monotone, and submodular1, h(S) is a

1A function f : U → < is submodular if for every

1015



dispersion function, and δ ≥ 0 is a parameter that
can be used to scale the range of h(·) to be com-
parable to that of g(·).

For two sets S and T , let P be the set of un-
ordered pairs {u, v} where u ∈ S and v ∈ T . Our
focus is on the following dispersion functions: the
sum measure hs(S, T ) =

∑
{u,v}∈P d(u, v), the

spanning tree measure ht(S, T ) given by the cost
of the minimum spanning tree of the set S∪T , and
the min measure hm(S, T ) = min{u,v}∈P d(u, v).
Note that these functions span from consider-
ing the entire set of distances in S to consider-
ing only the minimum distance in S; also it is
easy to construct examples to show that none of
these functions is submodular. Define h?(u, S) =
h?({u}, S) and h?(S) = h?(S, S).

Let O be the optimal solution of the function
f . A summary S̃ is a γ-approximation if f(S̃) ≥
γf(O).

3.2 Algorithm
Maximizing (1) is NP-hard even if δ = 0 or if
g(·) = 0 (Chandra and Halldórsson, 2001). For
the special case δ = 0, since g(·) is submodular,
a classical greedy algorithm obtains a (1 − 1/e)-
approximation (Nemhauser et al., 1978). But if
δ > 0, since the dispersion function h(·) is not
submodular, the combined objective f(·) is not
submodular as well. Despite this, we show that
a simple greedy algorithm achieves a provable ap-
proximation factor for (1). This is possible due to
some nice structural properties of the dispersion
functions we consider.

Algorithm 2 Greedy algorithm, parametrized by
the dispersion function h; here, U, k, g, δ are fixed.
S0 ← ∅; i← 0
for i = 0, . . . , k − 1 do

v ← argmaxu∈U\Si g(Si+u)+δh(Si+u)
Si+1 ← Si ∪ {v}

end for

3.3 Analysis
In this section we obtain a provable approximation
for the greedy algorithm. First, we show that a
greedy choice is well-behaved with respect to the
dispersion function h·(·).
Lemma 1. Let O be any set with |O| = k. If S is
such that |S| = ` < k, then
(i)

∑
u∈O\S hs(u, S) ≥ |O \ S|

`hs(O)
k(k−1) ;

A,B ⊆ U , we have f(A)+f(B) ≥ f(A∪B)+f(A∩B).

(ii)
∑

u∈O\S d(u, S) ≥ 12ht(O)− ht(S); and
(iii) there exists u ∈ O \ S such that hm(u, S) ≥
hm(O)/2.

Proof. The proof for (i) follows directly from
Lemma 1 in (Borodin et al., 2012).

To prove (ii) let T be the tree obtained by adding
all points of O \S directly to their respective clos-
est points on the minimum spanning tree of S. T
is a spanning tree, and hence a Steiner tree, for the
points in set S ∪ O. Hence, cost(T ) = ht(S) +∑

u∈O\S d(u, S). Let smt(S) denote the cost of
a minimum Steiner tree of S. Thus, cost(T ) ≥
smt(O ∪ S). Since a Steiner tree of O ∪ S is also
a Steiner tree of O, smt(O ∪ S) ≥ smt(O). Since
this is a metric space, smt(O) ≥ 12ht(O) (see, for
example, (Cieslik, 2001)). Thus,

ht(S) +
∑

u∈O\S
d(u, S) ≥ 1

2
ht(O)

⇒
∑

u∈O\S
d(u, S) ≥ 1

2
ht(O)− ht(S).

To prove (iii), let O = {u1, . . . , uk}. By def-
inition, for every i 6= j, d(ui, uj) ≥ hm(O).
Consider the (open) ball Bi of radius hm(O)/2
around each element ui. By construction for each
i, Bi ∩ O = {ui} and for each pair i 6= j,
Bi ∩Bj = ∅. Since |S| < k, and there are k balls
Bi, there exists k−` ballsBi such that S∩Bi = ∅,
proving (iii).

We next show that the tree created by the greedy
algorithm for h = ht is not far from the optimum.

Lemma 2. Let u1, . . . , uk be a sequence of points
and let Si = {uj , j ≤ i}. Then, ht(Sk) ≥
1/log k

∑
2≤j≤k d(uj , Sj−1).

Proof. The proof follows by noting that we get a
spanning tree by connecting each ui to its closest
point in Si−1. The cost of this spanning tree is∑

2≤j≤k d(uj , Sj−1) and this tree is also the re-
sult of the greedy algorithm run in an online fash-
ion on the input sequence {u1, . . . , uk}. Using the
result of (Imase and Waxman, 1991), the compet-
itive ratio of this algorithm is log k, and hence the
proof.

We now state and prove the main result about
the quality of approximation of the greedy algo-
rithm.

1016



Theorem 3. For k > 1, there is a polynomial-time
algorithm that obtains a γ-approximation to f(S),
where γ = 1/2 for h = hs, γ = 1/4 for h = hm,
and γ = 1/3 log k for h = ht.

Proof. For hs and ht, we run Algorithm 1 using
a new dispersion function h′, which is a slightly
modified version of h. In particular, for h = hs,
we use h′(S) = 2hs(S). For h = ht, we
abuse notation and define h′ to be a function over
an ordered set S = {u1, . . . , uk} as follows:
h′(S) =

∑
j≤|S| d(uj , Sj−1), where Sj−1 =

{u1, . . . , uj−1}. Let f ′(S) = g(S) + δh′(S).
Consider the ith iteration of the algorithm. By

the submodularity of g(·),
∑

u∈O\Si
g(Si ∪ {u})− g(Si) (2)

≥ g(O ∪ Si)− g(Si) ≥ g(O)− g(Sk),

where we use monotonicity of g(·) to infer g(O ∪
Si) ≥ g(O) and g(Si) ≤ g(Sk).

For h = hs, the proof follows by Lemma 1(i)
and by Theorem 1 in (Borodin et al., 2012).

For ht, using the above argument of submodu-
larity and monotonicity of g, and the result from
Lemma 1(ii), we have

∑

u∈O\Si
g(Si ∪ u)− g(Si) + δd(u, Si)

≥ g(O)− g(Si) + δ(ht(O)/2− ht(Si))
≥ (g(O) + δht(O)/2)− (g(Si) + δht(Si))
≥ f(O)/2− (g(Si) + δht(Si)).

Also, ht(Si) ≤ 2 smt(Si) since this is a met-
ric space. Using the monotonicity of the Steiner
tree cost, smt(Si) ≤ smt(Sk) ≤ ht(Sk). Hence,
ht(Si) ≤ 2ht(Sk). Thus,

∑

u∈O\Si
g(Si ∪ u)− g(Si) + δd(u, Si)

≥ f(O)/2− (g(Si) + δht(Si))
≥ f(O)/2− (g(Sk) + 2δht(Sk))
≥ f(O)/2− 2f(Sk). (3)

By the greedy choice of ui+1,

f ′(Si ∪ ui+1)− f ′(Si)
= g(Si ∪ ui+1)− g(Si) + δd(ui+1, Si)
≥ (f(O)/2− 2f(Sk))/|O \ Si|
≥ 1

k
(f(O)/2− 2f(Sk)).

Summing over all i ∈ [1, k − 1],

f ′(Sk) ≥ (k−1)/k(f(O)/2− 2f(Sk)). (4)

Using Lemma 2 we obtain

f(Sk) = g(Sk) + δht(Sk) ≥
f ′(Sk)
log k

≥ 1−
1/k

log k
(f(O)/2− 2f(Sk)).

By simplifying, we obtain f(Sk) ≥ f(O)/3 log k.
Finally for hm, we run Algorithm 1 twice: once

with g as given and h ≡ 0, and the second
time with g ≡ 0 and h ≡ hm. Let Sg and
Sh be the solutions in the two cases. Let Og
and Oh be the corresponding optimal solutions.
By the submodularity and monotonicity of g(·),
g(Sg) ≥ (1 − 1/e)g(Og) ≥ g(Og)/2. Similarly,
using Lemma 1(iii), hm(Sh) ≥ hm(Oh)/2 since
in any iteration i < k we can choose an ele-
ment ui+1 such that hm(ui+1, Si) ≥ hm(Oh)/2.
Let S = argmaxX∈{Sg ,Sh} f(X). Using an av-
eraging argument, since g and hm are both non-
negative,

f(X) ≥ (f(Sg)+f(Sh))/2 ≥ (g(Og)+δhm(Oh))/4.

Since by definition g(Og) ≥ g(O) and hm(Oh) ≥
hm(O), we have a 1/4-approximation.

3.4 A universal constant-factor
approximation

Using the above algorithm that we used for hm,
it is possible to give a universal algorithm that
gives a constant-factor approximation to each of
the above objectives. By running the Algorithm 1
once for g ≡ 0 and next for h ≡ 0 and taking
the best of the two solutions, we can argue that the
resulting set gives a constant factor approximation
to f . We do not use this algorithm in our exper-
iments, as it is oblivious of the actual dispersion
functions used.

4 Using the Framework

Next, we describe how the framework described
in Section 3 can be applied to our tasks of interest,
i.e., summarizing documents or user-generated
content (in our case, comments). First, we repre-
sent the elements of interest (i.e., sentences within
comments) in a structured manner by using depen-
dency trees. We then use this representation to

1017



generate a graph and instantiate our summariza-
tion objective function with specific components
that capture the desiderata of a given summariza-
tion task.

4.1 Structured representation for sentences

In order to instantiate the summarization graph
(nodes and edges), we first need to model each
sentence (in multi-document summarization) or
comment (i.e., set of sentences) as nodes in the
graph. Sentences have been typically modeled
using standard ngrams (unigrams or bigrams) in
previous summarization work. Instead, we model
sentences using a structured representation, i.e., its
syntax structure using dependency parse trees. We
first use a dependency parser (de Marneffe et al.,
2006) to parse each sentence and extract the set
of dependency relations associated with the sen-
tence. For example, the sentence “I adore tennis”
is represented by the dependency relations (nsubj:
adore, I) and (dobj: adore, tennis).

Each sentence represents a single node u in
the graph (unless otherwise specified) and is com-
prised of a set of dependency relations (or ngrams)
present in the sentence. Furthermore, the edge
weights s(u, v) represent pairwise similarity be-
tween sentences or comments (e.g., similarity be-
tween views expressed in different comments).
The edge weights are then used to define the
inter-sentence distance metric d(u, v) for the dif-
ferent dispersion functions. We identify simi-
lar views/opinions by computing semantic simi-
larity rather than using standard similarity mea-
sures (such as cosine similarity based on ex-
act lexical matches between different nodes in
the graph). For each pair of nodes (u, v) in
the graph, we compute the semantic similarity
score (using WordNet) between every pair of
dependency relation (rel: a, b) in u and v as:
s(u, v) =

∑

reli∈u,relj∈v
reli=relj

WN(ai, aj)×WN(bi, bj),

where rel is a relation type (e.g., nsubj) and a, b
are the two arguments present in the dependency
relation (b does not exist for some relations).
WN(wi, wj) is defined as the WordNet similar-
ity score between words wi and wj .2 The edge
weights are then normalized across all edges in the

2There exists various semantic relatedness measures
based on WordNet (Patwardhan and Pedersen, 2006). In our
experiments, for WN we pick one that is based on the path
length between the two words in the WordNet graph.

graph.
This allows us to perform approximate match-

ing of syntactic treelets obtained from the depen-
dency parses using semantic (WordNet) similar-
ity. For example, the sentences “I adore tennis”
and “Everyone likes tennis” convey the same view
and should be assigned a higher similarity score
as opposed to “I hate tennis”. Using the syntac-
tic structure along with semantic similarity helps
us identify useful (valid) nuggets of information
within comments (or documents), avoid redun-
dancies, and identify similar views in a semantic
space.

4.2 Components of the coverage function
Our coverage function is a linear combination of
the following.

(i) Popularity. One of the requirements for a good
summary (especially, for user-generated content)
is that it should include (or rather not miss) the
popular views or opinions expressed by several
users across multiple documents or comments. We
model this property in our objective function as
follows.

For each node u, we define w(u) as the num-
ber of documents |Curel ⊆ C| from the collection
such that at least one of the dependency relations
rel ∈ u appeared in a sentence within some doc-
ument c ∈ Curel . The popularity scores are then
normalized across all nodes in the graph. We then
add this component to our objective function as
w(S) =

∑
u∈S w(u).

(ii) Cluster contribution. This term captures the
fact that we do not intend to include multiple sen-
tences from the same comment (or document).
Define B to be the clustering induced by the sen-
tence to comment relation, i.e., two sentences in
the same comment belong to the same cluster. The
corresponding contribution to the objective func-
tion is

∑
B∈B |S ∩B|1/2.

(iii) Content contribution. This term promotes the
diversification of content. We look at the graph of
sentences where the weight of each edge is s(u, v).
This graph is then partitioned based on a local
random walk based method to give us clusters
D = {D1, . . . , Dn}. The corresponding contribu-
tion to the objective function is

∑
D∈D |S∩D|1/2.

(iv) Cover contribution. We also measure the
cover of the set S as follows: for each element
s in U first define cover of an element u by a
set S′ as cov(u, S′) =

∑
v∈S′ s(u, v). Then, the

1018



cover value of the set S is defined as cov(S) =∑
u∈S min(cov(u, S), 0.25cov(u, U)).

3

Thus, the final coverage function is: g(S) =
w(S) + α

∑
B∈B |S ∩ B|1/2 + β

∑
D∈D |S ∩

D|1/2 + λcov(S), where α, β, λ are non-negative
constants. By using the monotone submodularity
of each of the component functions, and the fact
that addition preserves submodularity, the follow-
ing is immediate.

Fact 4. g(S) is a monotone, non-negative, sub-
modular function.

We then apply Algorithm 1 to optimize (1).

5 Experiments

5.1 Data

Multi-document summarization. We use the
DUC 2004 corpus4 that comprises 50 clusters (i.e.,
50 different summarization tasks) with 10 docu-
ments per cluster on average. Each document con-
tains multiple sentences and the goal is to produce
a summary of all the documents for a given cluster.

Comments summarization. We extracted a set
of news articles and corresponding user comments
from Yahoo! News website. Our corpus contains a
set of 34 articles and each article is associated with
anywhere from 100–500 comments. Each com-
ment contains more than three sentences and 36
words per sentence on average.

5.2 Evaluation

For each summarization task, we compare the
system output (i.e., summaries automatically pro-
duced by the algorithm) against the human-
generated summaries and evaluate the perfor-
mance in terms of ROUGE score (Lin, 2004), a
standard recall-based evaluation measure used in
summarization. A system that produces higher
ROUGE scores generates better quality summary
and vice versa.

We use the following evaluation settings in our
experiments for each summarization task:

(1) For multi-document summarization, we
compute the ROUGE-15 scores that was the main
evaluation criterion for DUC 2004 evaluations.

3The choice of the value 0.25 in the cover component is
inspired by the observations made by (Lin and Bilmes, 2011)
for the α value used in their cover function.

4http://duc.nist.gov/duc2004/tasks.html
5ROUGE v1.5.5 with options: -a -c 95 -b 665 -m -n 4 -w

1.2

(2) For comment summarization, the collection
of user comments associated with a given arti-
cle is typically much larger. Additionally, indi-
vidual comments are noisy, wordy, diverse, and
informally written. Hence for this task, we use
a slightly different evaluation criterion that is in-
spired from the DUC 2005-2007 summarization
evaluation tasks.

We represent the content within each comment
c (i.e., all sentences S(c) comprising the com-
ment) as a single node in the graph. We then run
our summarization algorithm on the instantiated
graph to produce a summary for each news article.
In addition, each news article and corresponding
set of comments were presented to three human
annotators. They were asked to select a subset of
comments (at most 20 comments) that best rep-
resented a summary capturing the most popular
as well as diverse set of views and opinions ex-
pressed by different users that are relevant to the
given news article. We then compare the auto-
matically generated comment summaries against
the human-generated summaries and compute the
ROUGE-1 and ROUGE-2 scores.6

This summarization task is particularly hard for
even human annotators since user-generated com-
ments are typically noisy and there are several
hundreds of comments per article. Similar to ex-
isting work in the literature (Sekine and Nobata,
2003), we computed inter-annotator agreement for
the humans by comparing their summaries against
each other on a small held-out set of articles. The
average ROUGE-1 F-scores observed for humans
was much higher (59.7) than that of automatic sys-
tems measured against the human-generated sum-
maries (our best system achieved a score of 28.9
ROUGE-1 on the same dataset). This shows that
even though this is a new type of summariza-
tion task, humans tend to generate more consistent
summaries and hence their annotations are reliable
for evaluation purposes as in multi-document sum-
marization.

5.3 Results

Multi-document summarization. (1) Table 1
compares the performance of our system with
the previous best reported system that partici-
pated in the DUC 2004 competition. We also in-
clude for comparison another baseline—a version

6ROUGE v1.5.5 with options: -a -n 2 -x -m -2 4 -u -c 95
-r 1000 -f A -p 0.5 -t 0 -d -l 150

1019



of our system that approximates the submodular
objective function proposed by (Lin and Bilmes,
2011).7 As shown in the results, our best system8

which uses the hs dispersion function achieves a
better ROUGE-1 F-score than all other systems.
(2) We observe that the hm and ht dispersion func-
tions produce slightly lower scores than hs, which
may be a characteristic of this particular summa-
rization task. We believe that the empirical results
achieved by different dispersion functions depend
on the nature of the summarization tasks and there
are task settings under which hm or ht perform
better than hs. For example, we show later how us-
ing the ht dispersion function yields the best per-
formance on the comments summarization task.
Regardless, the theoretical guarantees presented in
this paper cover all these cases.
(3) We also analyze the contributions of individ-
ual components of the new objective function to-
wards summarization performance by selectively
setting certain parameters to 0. Table 2 illustrates
these results. We clearly see that each component
(popularity, cluster contribution, dispersion) indi-
vidually yields a reasonable summarization per-
formance but the best result is achieved by the
combined system (row 5 in the table). We also
contrast the performance of the full system with
and without the dispersion component (row 4 ver-
sus row 5). The results show that optimizing for
dispersion yields an improvement in summariza-
tion performance.
(4) To understand the effect of utilizing syntactic
structure and semantic similarity for constructing
the summarization graph, we ran the experiments
using just the unigrams and bigrams; we obtained
a ROUGE-1 F-score of 37.1. Thus, modeling
the syntactic structure (using relations extracted

7Note that Lin & Bilmes (2011) report a slightly higher
ROUGE-1 score (F-score 38.90) on DUC 2004. This is be-
cause their system was tuned for the particular summarization
task using the DUC 2003 corpus. On the other hand, even
without any parameter tuning our method yields good perfor-
mance, as evidenced by results on the two different summa-
rization tasks. However, since individual components within
our objective function are parametrized it is easy to tune them
for a specific task or genre.

8For the full system, we weight certain parameters per-
taining to cluster contributions and dispersion higher (α =
β = δ = 5) compared to the rest of the objective function
(λ = 1). Lin & Bilmes (2011) also observed a similar find-
ing (albeit via parameter tuning) where weighting the cluster
contribution component higher yielded better performance.
If the maximum number of sentences/comments chosen were
k, we brought both hs and ht to the same approximate scale
as hm by dividing hs by k(k − 1)/2 and ht by k − 1.

from dependency parse tree) along with comput-
ing similarity in semantic spaces (using WordNet)
clearly produces an improvement in the summa-
rization quality (+1.4 improvement in ROUGE-1
F-score). However, while the structured represen-
tation is beneficial, we observed that dispersion
(and other individual components) contribute sim-
ilar performance gains even when using ngrams
alone. So the improvements obtained from the
structured representation and dispersion are com-
plementary.

System ROUGE-1 F
Best system in DUC 2004 37.9

(Lin and Bilmes, 2011), no tuning 37.47

Our algorithm with h = hm 37.5
h = hs 38.5
h = ht 36.8

Table 1: Performance on DUC 2004.

Comments summarization. (1) Table 3 com-
pares the performance of our system against a
baseline system that is constructed by picking
comments in order of decreasing length, i.e., we
first pick the longest comment (comprising the
most number of characters), then the next longest
comment and so on, to create an ordered set of
comments. The intuition behind this baseline is
that longer comments contain more content and
possibly cover more topics than short ones.

From the table, we observe that the new sys-
tem (using either dispersion function) outperforms
the baseline by a huge margin (+44% relative
improvement in ROUGE-1 and much bigger im-
provements in ROUGE-2 scores). One reason be-
hind the lower ROUGE-2 scores for the baseline
might be that while long comments provide more
content (in terms of size), they also add noise and
irrelevant information to the generated summaries.
Our system models sentences using the syntactic
structure and semantics and jointly optimizes for
multiple summarization criteria (including disper-
sion) which helps weed out the noise and identify
relevant, useful information within the comments
thereby producing better quality summaries. The
95% confidence interval scores for the best system
on this task is [36.5–46.9].
(2) Unlike the multi-document summarization,
here we observe that the ht dispersion function
yields the best empirical performance for this
task. This observation supports our claim that the
choice of the specific dispersion function depends

1020



Objective function components ROUGE-1 F
α = β = λ = δ = 0 35.7

w(S) = β = λ = δ = 0 35.1
h = hs, w(S) = α = β = λ = 0 37.1

δ = 0 37.4
w(S), α, β, λ, δ > 0 38.5

Table 2: Performance with different parameters
(DUC).

on the summarization task and that the dispersion
functions proposed in this paper have a wider va-
riety of use cases.
(3) Results showing contributions from individual
components of the new summarization objective
function are listed in Table 4. We observe a sim-
ilar pattern as with multi-document summariza-
tion. The full system using all components out-
perform all other parameter settings, achieving the
best ROUGE-1 and ROUGE-2 scores. The table
also shows that incorporating dispersion into the
objective function yields an improvement in sum-
marization quality (row 4 versus row 5).

System ROUGE-1 ROUGE-2
Baseline (decreasing length) 28.9 2.9
Our algorithm with h = hm 39.2 13.2

h = hs 40.9 15.0
h = ht 41.6 16.2

Table 3: Performance on comments summariza-
tion.

Objective function ROUGE-1 ROUGE-2
components

α = β = λ = δ = 0 36.1 9.4
w(S) = β = λ = δ = 0 32.1 4.9

h = ht, w(S) = α = β = λ = 0 37.8 11.2
δ = 0 38.0 11.6

w(S), α, β, λ, δ > 0 41.6 16.2

Table 4: Performance with different parameters
(comments).

6 Conclusions

We introduced a new general-purpose graph-based
summarization framework that combines a sub-
modular coverage function with a non-submodular
dispersion function. We presented three natural
dispersion functions that represent three different
ways of ensuring non-redundancy (using sentence
dissimilarities) for summarization and proved that
a simple greedy algorithm can obtain an approxi-
mately optimal summary in all these cases. Exper-
iments on two different summarization tasks show

that our algorithm outperforms algorithms that
rely only on submodularity. Finally, we demon-
strated that using a structured representation to
model sentences in the graph improves summa-
rization quality.

For future work, it would be interesting to in-
vestigate other related developments in this area
and perhaps combine them with our approach to
see if further improvements are possible. Firstly,
it would interesting to see if dispersion offers sim-
ilar improvements over a tuned version of the sub-
modular framework of Lin and Bilmes (2011). In a
very recent work, Lin and Bilmes (2012) demon-
strate a further improvement in performance for
document summarization by using mixtures of
submodular shells. This is an interesting exten-
sion of their previous submodular framework and
while the new formulation permits more complex
functions, the resulting function is still submodu-
lar and hence can be combined with the dispersion
measures proposed in this paper. A different body
of work uses determinantal point processes (DPP)
to model subset selection problems and adapt it
for document summarization (Kulesza and Taskar,
2011). Note that DPPs use similarity kernels for
performing inference whereas our measures are
combinatorial and not kernel-representable. While
approximation guarantees for DPPs are open, it
would be interesting to investigate the empiri-
cal gains by combining DPPs with dispersion-like
functions.

Acknowledgments

We thank the anonymous reviewers for their many
useful comments.

References

Allan Borodin, Hyun Chul Lee, and Yuli Ye. 2012.
Max-sum diversification, monotone submodular
functions and dynamic updates. In Proc. PODS,
pages 155–166.

Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proc. SIGIR,
pages 335–336.

Barun Chandra and Magnús Halldórsson. 2001. Facil-
ity dispersion and remote subgraphs. J. Algorithms,
38(2):438–465.

Dietmar Cieslik. 2001. The Steiner Ratio. Springer.

1021



John M. Conroy and Dianne P. O’Leary. 2001. Text
summarization via hidden Markov models. In Proc.
SIGIR, pages 406–407.

Hal Daumé, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proc. COL-
ING/ACL, pages 305–312.

Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 449–454.

Elena Filatova. 2004. Event-based extractive summa-
rization. In Proc. ACL Workshop on Summarization,
pages 104–111.

Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: A graph based approach to abstrac-
tive summarization of highly redundant opinions. In
Proc. COLING.

Makoto Imase and Bernard M. Waxman. 1991. Dy-
namic Steiner tree problem. SIAM J. Discrete Math-
ematics, 4(3):369–384.

Hyun Duk Kim, Kavita Ganesan, Parikshit Sondhi, and
ChengXiang Zhai. 2011. Comprehensive review of
opinion summarization. Technical report, Univer-
sity of Illinois at Urbana-Champaign.

Alex Kulesza and Ben Taskar. 2011. Learning deter-
minantal point processes. In Proc. UAI, pages 419–
427.

Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Proc.
ACL, pages 510–520.

Hui Lin and Jeff Bilmes. 2012. Learning mixtures
of submodular shells with application to document
summarization. In Proc. UAI, pages 479–490.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Workshop on Text
Summarization Branches Out: Proc. ACL Work-
shop, pages 74–81.

G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher.
1978. An analysis of approximations for maximiz-
ing submodular set functions I. Mathematical Pro-
gramming, 14(1):265–294.

Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 43–76. Springer.

Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet-based context vectors to estimate the
semantic relatedness of concepts. In Proc. EACL
Workshop on Making Sense of Sense: Bringing
Computational Linguistics and Psycholinguistics
Together, pages 1–8.

Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Özgür. 2010. Citation summarization through
keyphrase extraction. In Proc. COLING, pages 895–
903.

Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-Tür. 2010. Long story short—Global
unsupervised models for keyphrase based meeting
summarization. Speech Commun., 52(10):801–815.

Satoshi Sekine and Chikashi Nobata. 2003. A survey
for multi-document summarization. In Proc. HLT-
NAACL Workshop on Text Summarization, pages
65–72.

Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. HLT/NAACL, pages 685–688.

Chao Shen and Tao Li. 2010. Multi-document summa-
rization via the minimum dominating set. In Proc.
COLING, pages 984–992.

Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proc. EACL, pages 781–
789.

Koji Yatani, Michael Novati, Andrew Trusty, and
Khai N. Truong. 2011. Review spotlight: A user in-
terface for summarizing user-generated reviews us-
ing adjective-noun word pairs. In Proc. CHI, pages
1541–1550.

1022


