



















































Knowledge Base Completion via Coupled Path Ranking


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1308–1318,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Knowledge Base Completion via Coupled Path Ranking

Quan Wang†, Jing Liu‡, Yuanfei Luo†, Bin Wang†, Chin-Yew Lin‡
†Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China

{wangquan,luoyuanfei,wangbin}@iie.ac.cn
‡Microsoft Research, Beijing 100080, China
{liudani,cyl}@microsoft.com

Abstract

Knowledge bases (KBs) are often greatly
incomplete, necessitating a demand for K-
B completion. The path ranking algorith-
m (PRA) is one of the most promising ap-
proaches to this task. Previous work on
PRA usually follows a single-task learn-
ing paradigm, building a prediction mod-
el for each relation independently with its
own training data. It ignores meaningful
associations among certain relations, and
might not get enough training data for less
frequent relations. This paper proposes a
novel multi-task learning framework for
PRA, referred to as coupled PRA (CPRA).
It first devises an agglomerative clustering
strategy to automatically discover relation-
s that are highly correlated to each other,
and then employs a multi-task learning s-
trategy to effectively couple the prediction
of such relations. As such, CPRA takes in-
to account relation association and enables
implicit data sharing among them. We
empirically evaluate CPRA on benchmark
data created from Freebase. Experimen-
tal results show that CPRA can effective-
ly identify coherent clusters in which rela-
tions are highly correlated. By further cou-
pling such relations, CPRA significantly
outperforms PRA, in terms of both predic-
tive accuracy and model interpretability.

1 Introduction

Knowledge bases (KBs) like Freebase (Bollack-
er et al., 2008), DBpedia (Lehmann et al., 2014),
and NELL (Carlson et al., 2010) are extremely
useful resources for many NLP tasks (Cucerzan,
2007; Schuhmacher and Ponzetto, 2014). They
provide large collections of facts about entities and

their relations, typically stored as (head entity, re-
lation, tail entity) triples, e.g., (Paris, capitalOf,
France). Although such KBs can be impressively
large, they are still quite incomplete and missing
crucial facts, which may reduce their usefulness in
downstream tasks (West et al., 2014; Choi et al.,
2015). KB completion, i.e., automatically infer-
ring missing facts by examining existing ones, has
thus attracted increasing attention. Approaches to
this task roughly fall into three categories: (i) path
ranking algorithms (PRA) (Lao et al., 2011); (ii)
embedding techniques (Bordes et al., 2013; Guo
et al., 2015); and (iii) graphical models such as
Markov logic networks (MLN) (Richardson and
Domingos, 2006). This paper focuses on PRA,
which is easily interpretable (as opposed to em-
bedding techniques) and requires no external logic
rules (as opposed to MLN).

The key idea of PRA is to explicitly use paths
connecting two entities to predict potential rela-
tions between them. In PRA, a KB is encoded as
a graph which consists of a set of heterogeneous
edges. Each edge is labeled with a relation type
that exists between two entities. Given a specific
relation, random walks are first employed to find
paths between two entities that have the given rela-
tion. Here a path is a sequence of relations linking

two entities, e.g., h bornIn−−−−−→ e capitalOf−−−−−−−−→ t.
These paths are then used as features in a bina-
ry classifier to predict if new instances (i.e., entity
pairs) have the given relation.

While KBs are naturally composed of multiple
relations, PRA models these relations separately
during the inference phase, by learning an individ-
ual classifier for each relation. We argue, however,
that it will be beneficial for PRA to model certain
relations in a collective way, particularly when the
relations are closely related to each other. For ex-
ample, given two relations bornIn and livedIn,

1308



there must be a lot of paths (features) that are pre-

dictive for both relations, e.g., h
nationality−−−−−−−−−−→ e

hasCapital−−−−−−−−−→ t. These features make the cor-
responding relation classification tasks highly re-
lated. Numerous studies have shown that learn-
ing multiple related tasks simultaneously (a.k.a.
multi-task learning) usually leads to better predic-
tive performance, profiting from the relevant infor-
mation available in different tasks (Carlson et al.,
2010; Chapelle et al., 2010).

This paper proposes a novel multi-task learning
framework that couples the path ranking of multi-
ple relations, referred to as coupled PRA (CPRA).
The new model needs to answer two critical ques-
tions: (i) which relations should be coupled, and
(ii) in what manner they should be coupled.

As to the first question, it is obvious that not all
relations are suitable to be learned together. For
instance, modeling bornIn together with hasWife
might not bring any real benefits, since there are
few common paths between these two relations.
CPRA introduces a common-path based similarity
measure, and accordingly devises an agglomera-
tive clustering strategy to group relations. Only re-
lations that are grouped into the same cluster will
be coupled afterwards.

As to the second question, CPRA follows the
common practice of multi-task learning (Evgeniou
and Pontil, 2004), and couples relations by using
classifiers with partially shared parameters. Given
a cluster of relations, CPRA builds the classifier-
s upon (i) relation-specific parameters to address
the specifics of individual relations, and (ii) shared
parameters to model the commonalities among d-
ifferent relations. These two types of parameters
are balanced by a coupling coefficient, and learned
jointly for all relations. In this way CPRA couples
the classification tasks of multiple relations, and
enables implicit data sharing and regularization.

The major contributions of this paper are as fol-
lows. (i) We design a novel framework for multi-
task learning with PRA, i.e., CPRA. To the best of
our knowledge, this is the first study on multi-task
PRA. (ii) We empirically verify the effectiveness
of CPRA on a real-world, large-scale KB. Specifi-
cally, we evaluate CPRA on benchmark data creat-
ed from Freebase. Experimental results show that
CPRA can effectively identify coherent clusters
in which relations are highly correlated. By fur-
ther coupling such relations, CPRA substantially
outperforms PRA, in terms of not only predictive

accuracy but also model interpretability. (iii) We
compare CPRA and PRA to the embedding-based
TransE model (Bordes et al., 2013), and demon-
strate their superiority over TransE. As far as we
know, this is the first work that formally compares
PRA-style approaches to embedding-based ones,
on publicly available Freebase data.

In the remainder of this paper, we first review
related work in Section 2, and formally introduce
PRA in Section 3. We then detail the proposed
CPRA framework in Section 4. Experiments and
results are reported in Section 5, followed by the
conclusion and future work in Section 6.

2 Related Work

We first review three lines of related work: (i) KB
completion, (ii) PRA and its extensions, and (iii)
multi-task learning, and then discuss the connec-
tion between CPRA and previous approaches.

KB completion. This task is to automatically
infer missing facts from existing ones. Prior work
roughly falls into three categories: (i) path ranking
algorithms (PRA) which use paths that connect t-
wo entities to predict potential relations between
them (Lao et al., 2011; Lao and Cohen, 2010); (i-
i) embedding-based models which embed entities
and relations into a latent vector space and make
inferences in that space (Nickel et al., 2011; Bor-
des et al., 2013); (iii) probabilistic graphical mod-
els such as the Markov logic network (MLN) and
its variants (Pujara et al., 2013; Jiang et al., 2012).
This paper focuses on PRA, since it is easily inter-
pretable (as opposed to embedding-based models)
and requires no external logic rules (as opposed to
MLN and its variants).

PRA and its extensions. PRA is a random walk
inference technique designed for predicting new
relation instances in KBs, first proposed by Lao
and Cohen (2010). Recently various extension-
s have been explored, ranging from incorporating
a text corpus as additional evidence during infer-
ence (Gardner et al., 2013; Gardner et al., 2014),
to introducing better schemes to generate more
predictive paths (Gardner and Mitchell, 2015; Shi
and Weninger, 2015), or using PRA in a broader
context such as Google’s Knowledge Vault (Dong
et al., 2014). All these approaches are based on
some single-task version of PRA, while our work
explores multi-task learning for it.

Multi-task learning. Numerous studies have
shown that learning multiple related tasks simulta-

1309



neously can provide significant benefits relative to
learning them independently (Caruana, 1997). A
key ingredient of multi-task learning is to model
the notion of task relatedness, through either pa-
rameter sharing (Evgeniou and Pontil, 2004; An-
do and Zhang, 2005) or feature sharing (Argyri-
ou et al., 2007; He et al., 2014). In recent years,
there has been increasing work showing the ben-
efits of multi-task learning in NLP-related tasks,
such as relation extraction (Jiang, 2009; Carlson et
al., 2010) and machine translation (Sennrich et al.,
2013; Cui et al., 2013; Dong et al., 2015). This pa-
per investigates the possibility of multi-task learn-
ing with PRA, in a parameter sharing manner.

Connection with previous methods. Actually,
modeling multiple relations collectively is a com-
mon practice in embedding-based approaches. In
such a method, embeddings are learned jointly for
all relations, over a set of shared latent features
(entity embeddings), and hence can capture mean-
ingful associations among different relations. As
shown by (Toutanova and Chen, 2015), observed
features such as PRA paths usually perform bet-
ter than latent features for KB completion. In this
context, CPRA is designed in a way that gets the
multi-relational benefit of embedding techniques
while keeping PRA-style path features. Nickel et
al. (2014) and Neelakantan et al. (2015) have tried
similar ideas. However, their work focuses on im-
proving embedding techniques with observed fea-
tures, while our approach aims at improving PRA
with multi-task learning.

3 Path Ranking Algorithm

PRA was first proposed by Lao and Cohen (2010),
and later slightly modified in various ways (Gard-
ner et al., 2014; Gardner and Mitchell, 2015). The
key idea of PRA is to explicitly use paths that con-
nect two entities as features to predict potential re-
lations between them. Here a path is a sequence
of relations ⟨r1, r2, · · · , rℓ⟩ that link two entities.
For example, ⟨bornIn, capitalOf⟩ is a path link-
ing SophieMarceau to France, through an inter-
mediate node Paris. Such paths are then used
as features to predict the presence of specific re-
lations, e.g., nationality. A typical PRA model
consists of three steps: feature extraction, feature
computation, and relation-specific classification.

Feature extraction. The first step is to generate
and select path features that are potentially useful
for predicting new relation instances. To this end,

PRA first encodes a KB as a multi-relation graph.
Given a pair of entities (h, t), PRA then finds the
paths by performing random walks over the graph,
recording those starting from h and ending at t
with bounded lengths. More exhaustive strategies
like breadth-first (Gardner and Mitchell, 2015) or
depth-first (Shi and Weninger, 2015) search could
also be used to enumerate the paths. After that a
set of paths are selected as features, according to
some precision-recall measure (Lao et al., 2011),
or simply frequency (Gardner et al., 2014).

Feature computation. Once path features are
selected, the next step is to compute their values.
Given an entity pair (h, t) and a path π, PRA com-
putes the feature value as a random walk proba-
bility p(t|h, π), i.e., the probability of arriving at t
given a random walk starting from h and following
exactly all relations in π. Computing these ran-
dom walk probabilities could be at great expense.
Gardner and Mitchell (2015) recently showed that
such probabilities offer no discernible benefits. So
they just used a binary value to indicate the pres-
ence or absence of each path. Similarly, Shi and
Weninger (2015) used the frequency of a path as it-
s feature value. Besides paths, other features such
as path bigrams and vector space similarities could
also be incorporated (Gardner et al., 2014).

Relation-specific classification. The last step
of PRA is to train an individual classifier for each
relation, so as to judge whether two entities should
be linked by that relation. Given a relation and a
set of training instances (i.e., pairs of entities that
are linked by the relation or not, with features s-
elected and computed as above), one can use any
kind of classifier to train a model. Most previous
work simply chooses logistic regression.

4 Coupled Path Ranking Algorithm

As we can see, PRA (as well as its variants) fol-
lows a single-task learning paradigm, which builds
a classifier for each relation independently with its
own training data. We argue that such a single-task
strategy might not be optimal for KB completion:
(i) by learning the classifiers independently, it fail-
s to discover and leverage meaningful associations
among different relations; (ii) it might not perfor-
m well on less frequent relations for which only a
few training instances are available. This section
presents coupled PRA (CPRA), a novel multi-task
learning framework that couples the path ranking
of multiple relations. Through a multi-task strat-

1310



egy, CPRA takes into account relation association
and enables implicit data sharing among them.

4.1 Problem Formulation
Suppose we are given a KB containing a collection
of triples O = {(h, r, t)}. Each triple is composed
of two entities h, t ∈ E and their relation r ∈ R,
where E is the entity set and R the relation set.
The KB is then encoded as a graph G, with entities
represented as nodes, and triple (h, r, t) a directed
edge from node h to node t. We formally define
KB completion as a binary classification problem.
That is, given a particular relation r, for any entity
pair (h, t) such that (h, r, t) /∈ O, we would like
to judge whether h and t should be linked by r, by
exploiting the graph structure of G. Let R ⊆ R
denote a set of relations to be predicted.

Each relation r ∈ R is associated with a set of
training instances. Here a training instance is an
entity pair (h, t), with a positive label if (h, r, t) ∈
O or a negative label otherwise.1 For each of the
entity pairs, path features could be extracted and
computed using techniques described in Section 3.
We denote by Πr the set of path features extracted
for relation r, and define its training set as Tr =
{(xir, yir)}. Here xir is the feature vector for an
entity pair, with each dimension corresponding to
a path π ∈ Πr, and yir = ±1 is the label. Note
that our primary goal is to verify the possibility
of multi-task learning with PRA. It is beyond the
scope of this paper to further explore better feature
extraction or computation.

Given the relations and their training instances,
CPRA performs KB completion using a multi-task
learning strategy. It consists of two components:
relation clustering and relation coupling. The for-
mer automatically discovers highly correlated re-
lations, and the latter further couples the learning
of these relations, described in detail as follows.

4.2 Relation Clustering
It is obvious that not all relations are suitable to
be coupled. We propose an agglomerative cluster-
ing algorithm to automatically discover relations
that are highly correlated and should be learned to-
gether. Our intuition is that relations sharing more
common paths (features) are probably more simi-
lar in classification, and hence should be coupled.

Specifically, we start with |R| clusters and each
cluster contains a single relation r ∈ R. Here |·| is

1We will introduce the details of generating negative train-
ing instances in Section 5.1.

the cardinality of a set. Then we iteratively merge
the most similar clusters, say Cm and Cn, into a
new cluster C. The similarity between two clusters
is defined as:

Sim(Ci, Cj) =
|ΠCi ∩ΠCj |

min(|ΠCi |, |ΠCj |)
, (1)

where ΠCi is the feature set associated with clus-
ter Ci (if Ci contains a single relation, ΠCi the fea-
ture set associated with that relation). It essential-
ly measures the overlap between two feature sets.
The larger the overlap is, the higher the similari-
ty will be. Once two clusters are merged, we up-
date the feature set associated with the new cluster:
ΠC = ΠCm ∪ΠCn . The algorithm stops when the
highest cluster similarity is below some predefined
threshold δ. This paper empirically sets δ = 0.5.
As such, relations sharing a substantial number of
common paths are grouped into the same cluster.

4.3 Relation Coupling
After clustering, the next step of CPRA is to cou-
ple the path ranking of different relations within
each cluster, i.e., to learn the classification tasks
for these relations simultaneously. We employ a
multi-task classification algorithm similar to (Ev-
geniou and Pontil, 2004), and learn the classifiers
jointly in a parameter sharing manner.

Consider a cluster containing K relations C =
{r1, r2, · · · , rK}. Recall that during the clustering
phase a shared feature set has been generated for
that cluster, i.e., ΠC = Πr1 ∪ · · · ∪ΠrK . We first
reform the training instances for the K relation-
s using this shared feature set, so that all training
data is represented in the same space.2 We denote
by Tk = {(xik, yik)}Nki=1 the reformed training da-
ta associated with the k-th relation. Then our goal
is to jointly learn K classifiers f1, f2, · · · , fK such
that fk(xik) ≈ yik.

We first assume that the classifier for each rela-
tion has a linear form fk(x)=wk · x + bk, where
wk ∈ Rd is the weight vector and bk the bias. To
model associations among different relations, we
further assume that all wk and bk can be written,
for every k ∈ {1, · · · ,K}, as:

wk = w0 + vk and bk = b0. (2)

Here the shared w0 is used to model the common-
alities among different relations, and the relation-
specific vk to address the specifics of individual

2Note that Πrk ⊆ ΠC . We just assign zero values to
features that are contained in ΠC but not in Πrk .

1311



relations. If the relations are closely related (vk ≈
0), they will have similar weights (wt ≈ w0) on
the common paths. We use the same bias b0 for all
the relations.3

We estimate vk, w0, and b0 simultaneously in a
joint optimization problem, defined as follows.

Problem 1 CPRA amounts to solving the general
optimization problem:

min
{vk},w0,b0

K∑
k=1

Nk∑
i=1

ℓ (xik, yik) +
λ1
K

K∑
k=1

∥vk∥22 + λ2 ∥w0∥22 ,

where ℓ (xik, yik) is the loss on a training instance.
It can be instantiated into a logistic regression (L-
R) or support vector machine (SVM) version, by
respectively defining the loss ℓ (xik, yik) as:

ℓ (xik, yik) = log (1 + exp (−yikfk(xik))) ,
ℓ (xik, yik) = [1− yikfk(xik)]+ ,

where fk(xik) = (w0 + vk) · xik + b0. We call them
CPRA-LR and CPRA-SVM respectively.

In this problem, λ1 and λ2 are regularization pa-
rameters. By adjusting their values, we control the
degree of parameter sharing among different rela-
tions. The larger the ratio λ1λ2 is, the more we be-
lieve that all wt should conform to the common
model w0, and the smaller the relation-specific
weight vt will be.

The multi-task learning problem can be directly
linked to a standard single-task learning one, built
on all training data from different relations.

Proposition 1 Suppose the training data associ-
ated with the k-th relation, for every k=1, · · · ,K,
is transformed into:

x̃ik = [
xik√
ρK

,0, · · · ,0︸ ︷︷ ︸
k−1

,xik,0, · · · ,0︸ ︷︷ ︸
K−k

],

where 0 ∈ Rd is a vector whose coordinates are all
zero, and ρ = λ2

λ1
a coupling coefficient. Consider a

linear classifier for the transformed data f̃(x̃) =
w̃ · x̃ + b̃, with w̃ and b̃ constructed as:

w̃ = [
√

ρKw0,v1, · · · ,vK ] and b̃ = b0.

Then the objective function of Problem 1 is equiv-
alent to:

L =
K∑

k=1

Nk∑
i=1

ℓ̃ (x̃ik, yik) + λ̃ ∥w̃∥22 ,

3It implicitly assumes that all the relations have the same
proportion of positive instances. This assumption actually
holds since given any relation we can always generate the
same number of negative instances for each positive one. We
set this number to 4 in our experiments.

where ℓ̃ = log(1 + exp(−yikf̃(x̃ik))) is a logistic loss
for CPRA-LR, and ℓ̃=[1 − yikf̃(x̃ik)]+ a hinge loss
for CPRA-SVM; and λ̃ = λ1

K
.

That means, after transforming data from differ-
ent relations into a unified representation, Prob-
lem 1 is equivalent to a standard single-task learn-
ing problem, built on the transformed data from all
the relations. So it can easily be solved by existing
tools such as LR or SVM.

5 Experiments

In this section we present empirical evaluation of
CPRA in the KB completion task.

5.1 Experimental Setups

We create our data on the basis of FB15K (Bor-
des et al., 2011)4, a relatively dense subgraph of
Freebase containing 1,345 relations and the corre-
sponding triples.

KB graph construction. We notice that in most
cases FB15K encodes a relation and its reverse re-
lation at the same time. That is, once a new fact is
observed, FB15K creates two triples for it, e.g., (x,
film/edited-by, y) and (y, editor/film, x). Re-
verse relations provide no additional knowledge.
They may even hurt the performance of PRA-style
methods. Actually, to enhance graph connectivity,
PRA-style methods usually automatically add an
inverse version for each relation in a KB (Lao and
Cohen, 2010; Lao et al., 2011). That is, for each
observed triple (h, r, t), another triple (t, r−1, h)
is constructed and added to the KB. Consider the
prediction of a relation, say film/edited-by. In
the training phase, we could probably find that ev-
ery two entities connected by this relation are also
connected by the path editor/film−1, and hence
assign an extremely high weight to it.5 However,
in the testing phase, for any entity pair (x, y) such
that (y, editor/film, x) has not been encoded, we
might not even find that path and hence could al-
ways make a negative prediction.6

For this reason, we remove reverse relations in
FB15K. Specifically, we regard r2 to be a reverse
relation of r1 if the triple (t, r2, h) holds whenev-
er (h, r1, t) is observed, and we randomly discard

4https://everest.hds.utc.fr/doku.php?id=en:smemlj12
5For every observed triple (x, film/edited-by, y),

FB15K also encodes (y, editor/film, x), for which (x,
editor/film−1, y) is further constructed.

6Note that such test cases are generally more meaningful:
if we already know (y, editor/film, x), predicting (x,
film/edited-by, y) could be trivial.

1312



one of the two relations.7 As such, we keep 774
out of 1,345 relations in FB15K, covering 14,951
entities and 327,783 triples. Then we build a graph
based on this data and use it as input to CPRA (and
our baseline methods).

Labeled instance generation. We select 171
relations to test our methods. To do so, we pick
10 popular domains, including award, education,
film, government, location, music, olympics, or-
ganization, people, and tv. Relations in these do-
mains with at least 50 triples observed for them are
selected. For each of the 171 relations, we split the
associated triples into roughly 80% training, 10%
validation, and 10% testing. Since the triple num-
ber varies significantly among the relations, we al-
low at most 200 validation/testing triples for each
relation, so as to make the test cases as balanced
as possible. Note that validation and testing triples
are not used for constructing the graph.

We generate positive instances for each relation
directly from these triples. Given a relation r and a
triple (h, r, t) observed for it (training, validation,
or testing), we take the pair of entities (h, t) as a
positive instance for that relation. Then we follow
(Shi and Weninger, 2015; Krompaß et al., 2015) to
generate negative instances. Given each positive
instance (h, t) we generate four negative ones, two
by randomly corrupting the head h, and the other
two the tail t. To make the negative instances as d-
ifficult as possible, we corrupt a position using on-
ly entities that have appeared in that position. That
means, given the relation capitalOf and the pos-
itive instance (Paris, France), we could generate
a negative instance (Paris, UK) but never (Paris,
NBA), since NBA never appears as a tail entity of
the relation. We further ensure that the negative
instances do not overlap with the positive ones.

Feature extraction and computation. Given
the labeled instances, we extract path features for
them using the code provided by Shi and Weninger
(2015)8. It is a depth-first search strategy that enu-
merates all paths between two entities. We set the
maximum path length to be ℓ = 3. There are about
8.2% of the labeled instances for which no path
could be extracted. We remove such cases, giving
on average about 5,250 training, 323 validation,
and 331 testing instances per relation. Then we re-
move paths that appear only once in each relation,
getting 5,515 features on average per relation. We

7We still add an inverse version for the relation kept dur-
ing path extraction.

8https://github.com/nddsg/KGMiner

# Relations 774
# Entities 14,951
# Triples 327,783

# Relations tested 171
# Avg. training instances/relation 5,250
# Avg. validation instances/relation 323
# Avg. testing instances/relation 331
# Avg. features/relation 5,515

Table 1: Statistics of the data.

simply compute the value of each feature as its fre-
quency in an instance. Table 1 lists the statistics of
the data used in our experiments.

Evaluation metrics. As evaluation metrics, we
use mean average precision (MAP) and mean re-
ciprocal rank (MRR), following recent work eval-
uating KB completion performance (West et al.,
2014; Gardner and Mitchell, 2015). Both metrics
evaluate some ranking process: if a method ranks
the positive instances before the negative ones for
each relation, it will get a high MAP or MRR.

Baseline methods. We compare CPRA to tra-
ditional single-task PRA. CPRA first groups the
171 relations into clusters, and then learns classi-
fiers jointly for relations within the same cluster.
We implement two versions of it: CPRA-LR and
CPRA-SVM. As we have shown in Proposition 1,
both of them could be solved by standard classi-
fication tools. PRA learns an individual classifier
for each of the relations, using LR or SVM classi-
fication techniques, denoted by PRA-LR or PRA-
SVM. We use LIBLINEAR (Fan et al., 2008)9 to
solve the LR and SVM classification problems.
For all these methods, we tune the cost c in the
range of {2−5, 2−4, · · · , 24, 25}. And we set the
coupling coefficient ρ = λ2λ1 in CPRA in the range
of {0.1, 0.2, 0.5, 1, 2, 5, 10}.

We further compare CPRA to TransE, a widely
adopted embedding-based method (Bordes et al.,
2013). TransE learns vector representations for
entities and relations (i.e., embeddings), and uses
the learned embeddings to determine the plausibil-
ity of missing facts. Such plausibility can then be
used to rank the labeled instances. We implement
TransE using the code provided by Bordes et al.
(2013)10. To learn embeddings, we take as input
the triples used to construct the graph (from which
CPRA and PRA extract their paths). We tune the
embedding dimension in {20, 50, 100}, the mar-
gin in {0.1, 0.2, 0.5, 1, 2, 5}, and the learning rate

9http://www.csie.ntu.edu.tw/ cjlin/liblinear
10https://github.com/glorotxa/SME

1313



film/casting-director gov-jurisdiction/dist-represent
film/cinematography location/contain
film/costume-design-by location/adjoin
film/art-direction-by us-county/county-seat
film/crewmember county-place/county
film/set-decoration-by location/partially-contain
film/production-design-by region/place-export
film/edited-by
film/written-by
film/story-by

org/place-founded country/divisions
org/headquarter-city country/capital
org/headquarter-state country/fst-level-divisions
org/geographic-scope country/snd-level-divisions
org/headquarter-country admin-division/capital
org/service-location

tv/tv-producer music-group-member/instrument
tv/recurring-writer music-artist/recording-role
tv/program-creator music-artist/track-role
tv/regular-appear-person music-group-member/role
tv/tv-actor

Table 2: Six largest clusters of relations (with the
stopping criterion δ = 0.5).

in {10−4, 10−3, 10−2, 10−1, 1}. For details please
refer to (Bordes et al., 2013). For each of these
methods, we select the optimal configuration that
leads to the highest MAP on the validation set and
report its performance on the test set.

5.2 Relation Clustering Results

We first test the effectiveness of our agglomerative
strategy (Section 4.2) in relation clustering. With
the stopping criterion δ = 0.5, 96 out of the 171
relations are grouped into clusters which contain at
least two relations. Each of these 96 relations will
later be learned jointly with some other relations.
The other 75 relations cannot be merged, and will
still be learned individually. Table 2 shows the six
largest clusters discovered by our algorithm. Rela-
tions in each cluster are arranged in the order they
were merged. The results indicate that our algo-
rithm can effectively identify coherent clusters in
which relations are highly correlated to each other.
For example, the top left cluster describes relations
between a film and its crew members, and the mid-
dle left between an organization and a location.

During clustering we might obtain clusters that
contain too many relations and hence too many
training instances for our CPRA model to learn ef-
ficiently. We split such clusters into sub-clusters,
either according to the domain (e.g., the film clus-
ter and tv cluster) or randomly (e.g., the two loca-
tion clusters on the top right).

5.3 KB Completion Results

We further test the effectiveness of our multi-task
learning strategy (Section 4.3) in KB completion.
Table 3 gives the results on the 96 relations that are
actually involved in multi-tasking learning (i.e.,
grouped into clusters with size larger than one).11

The 96 relations are grouped into 29 clusters, and
relations within the same cluster are learned joint-
ly. Table 3 reports (i) MAP and MRR within each
cluster and (ii) overall MAP and MRR on the 96
relations. Numbers marked in bold type indicate
that CPRA-LR/SVM outperforms PRA-LR/SVM,
within a cluster (with its ID listed in the first col-
umn) or on all the 96 relations (ALL). We judge s-
tatistical significance of the overall improvements
achieved by CPRA-LR/SVM over PRA-LR/SVM
and TransE, using a paired t-test. The average pre-
cision (or reciprocal rank) on each relation is used
as paired data. The symbol “∗∗” indicates a signif-
icance level of p < 0.0001, and “∗” a significance
level of p < 0.05.

From the results, we can see that (i) CPRA
outperforms PRA (using either LR or SVM) and
TransE on the 96 relations (ALL) in both metrics.
All the improvements are statistically significant,
with a significance level of p < 0.0001 for MAP
and a significance level of p < 0.05 for MRR. (i-
i) CPRA-LR/SVM outperforms PRA-LR/SVM in
22/24 out of the 29 clusters in terms of MAP. Most
of the improvements are quite substantial. (iii) Im-
proving PRA-LR and PRA-SVM in terms of MRR
could be hard, since they already get the best per-
formance (MRR = 1) in 19 out of the 29 clusters.
But even so, CPRA-LR/SVM still improves 7/8
out of the remaining 10 clusters. (iv) The PRA-
style methods perform substantially better than the
embedding-based TransE model in most of the 29
clusters and on all the 96 relations. This observa-
tion demonstrates the superiority of observed fea-
tures (i.e., PRA paths) over latent features.

Table 4 further shows the top 5 most discrimina-
tive paths (i.e., features with the highest weights)
discovered by PRA-SVM (left) and CPRA-SVM
(right) for each relation in the 6th cluster.12 The
average precision on each relation is also provid-

11The other 75 relations are still learned individually. So
CPRA and PRA perform the same on these relations. The
MAP values on these 75 relations are 0.6360, 0.6558, 0.6543
for TransE, PRA-LR, and PRA-SVM respectively, and the
MRR values are 0.9049, 0.9033, and 0.9013 respectively.

12This is one of the largest clusters on which CPRA-SVM
improves PRA-SVM substantially.

1314



MAP MRR

TransE PRA-LR CPRA-LR PRA-SVM CPRA-SVM TransE PRA-LR CPRA-LR PRA-SVM CPRA-SVM

1 0.5419 0.5160 0.5408 0.4687 0.5204 0.7500 0.8333 1.0000 0.7778 0.8333
2 0.7480 0.7888 0.7807 0.8010 0.8092 1.0000 1.0000 1.0000 1.0000 1.0000
3 0.4624 0.4625 0.4788 0.4634 0.4560 0.8333 1.0000 1.0000 1.0000 0.8333
4 0.5495 0.5378 0.5423 0.5385 0.5460 0.7667 0.6400 0.7000 0.7167 0.7000
5 0.5164 0.5789 0.6030 0.5891 0.6072 0.8333 0.6667 1.0000 0.8333 1.0000
6 0.6918 0.7733 0.7950 0.7369 0.8084 1.0000 0.8333 0.8333 0.8056 0.9167
7 0.7381 0.7531 0.7754 0.7456 0.7414 0.7500 1.0000 1.0000 1.0000 1.0000
8 0.4258 0.5180 0.5446 0.3162 0.4606 1.0000 1.0000 1.0000 0.3056 0.7500
9 0.6353 0.7879 0.7708 0.7680 0.7685 0.7500 1.0000 1.0000 1.0000 1.0000
10 0.8615 0.7773 0.7738 0.7618 0.7507 1.0000 1.0000 1.0000 1.0000 1.0000
11 0.4549 0.5814 0.6014 0.5717 0.5896 0.8333 1.0000 1.0000 0.8750 1.0000
12 0.6202 0.7187 0.7479 0.7455 0.7457 0.7500 0.5833 1.0000 1.0000 1.0000
13 0.5530 0.6681 0.6716 0.6373 0.6502 0.6667 1.0000 1.0000 1.0000 1.0000
14 0.5082 0.4360 0.5280 0.4715 0.5806 0.3750 0.6667 0.6250 1.0000 1.0000
15 0.9881 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
16 0.5324 0.6818 0.6863 0.6522 0.6705 0.8750 1.0000 1.0000 1.0000 1.0000
17 0.3759 0.3351 0.3593 0.3273 0.3219 0.6111 0.5667 0.7778 0.5111 0.6667
18 0.9423 0.9968 1.0000 0.9947 0.9975 1.0000 1.0000 1.0000 1.0000 1.0000
19 0.7903 0.8376 0.8310 0.8296 0.8328 0.8714 0.9286 0.8571 0.8571 0.8571
20 0.7920 0.8285 0.8746 0.8491 0.8754 1.0000 1.0000 1.0000 1.0000 1.0000
21 0.4885 0.5869 0.5799 0.5554 0.5952 0.6250 1.0000 1.0000 1.0000 1.0000
22 0.7894 0.8371 0.8486 0.8371 0.8374 1.0000 1.0000 1.0000 1.0000 1.0000
23 0.7123 0.7848 0.8191 0.7811 0.7957 0.9500 1.0000 1.0000 1.0000 1.0000
24 0.5982 0.7923 0.8048 0.8204 0.8220 1.0000 1.0000 1.0000 1.0000 1.0000
25 0.6223 0.8723 0.8723 0.7785 0.8109 0.7500 1.0000 1.0000 1.0000 1.0000
26 0.5253 0.5377 0.5685 0.5337 0.5447 0.8750 0.8125 0.8750 0.7083 0.8333
27 0.8763 0.6890 0.8124 0.7014 0.8016 1.0000 0.6667 1.0000 0.7500 1.0000
28 0.7588 0.8131 0.8154 0.8130 0.8146 1.0000 1.0000 1.0000 1.0000 1.0000
29 0.4894 0.5921 0.6543 0.6093 0.6566 0.7500 1.0000 1.0000 1.0000 1.0000

ALL 0.6540 0.7058 0.7254∗∗ 0.6943 0.7162∗∗ 0.8682 0.9061 0.9436∗ 0.8982 0.9358∗

Table 3: KB completion results on the 96 relations that have been grouped into clusters with size larger
than one (with the stopping criterion δ = 0.5), and hence involved in multi-tasking learning.

ed. We can observe that (i) CPRA generally dis-
covers more predictive paths than PRA. Almost all
the top paths discovered by CPRA are easily inter-
pretable and provide sensible reasons for the final
prediction, while some of the top paths discovered
by PRA are hard to interpret and less predictive.
Take org/place-founded as an example. All the
5 CPRA paths are useful to predict the place where
an organization was founded, e.g., the 3rd one tells
that “the organization headquarter in a city which
is located in that place”. However, the PRA path
“common/class→ common/class−1 → film/debut-
venue” is hard to interpret and less predictive. (ii)
For the 1st/4th/6th relation on which PRA gets a
low average precision, CPRA learns almost com-
pletely different top paths and gets a substantially
higher average precision. While for the other re-
lations (2nd/3rd/5th) on which PRA already per-
forms well enough, CPRA learns similar top paths
and gets a comparable average precision. We have
conducted the same analyses with CPRA-LR and
PRA-LR, and observed similar phenomena. All

these observations demonstrate the superiority of
CPRA, in terms of not only predictive accuracy
but also model interpretability.

6 Conclusion

In this paper we have studied the path ranking al-
gorithm (PRA) from the viewpoint of multi-task
learning. We have designed a novel multi-task
learning framework for PRA, called coupled PRA
(CPRA). The key idea of CPRA is to (i) automat-
ically discover relations highly correlated to each
other through agglomerative clustering, and (ii) ef-
fectively couple the prediction of such relations
through multi-task learning. By coupling different
relations, CPRA takes into account relation asso-
ciations and enables implicit data sharing among
them. We have tested CPRA on benchmark data
created from Freebase. Experimental results show
that CPRA can effectively identify coherent clus-
ters in which relations are highly correlated. By
further coupling such relations, CPRA significant-
ly outperforms PRA, in terms of both predictive

1315



org/place-founded (0.4920 vs. 0.6750)

org/headquarter-city location/contain−1

common/class→common/class−1→film/debut-venue org/headquarter-city
common/class→common/class−1→sports-team/location org/headquarter-city→location/contain−1
employer/job-title→employer/job-title−1→location/contain org/headquarter-state→location/contain
music-artist/label−1→person/place-of-birth org/headquarter-city→bibs-location/state
org/headquarter-city (0.9014 vs. 0.9141)

location/contain−1 location/contain−1

org/place-founded org/headquarter-state→location/contain
org/headquarter-state→location/contain org/place-founded
org/child−1→org/child→org/place-founded org/child−1→org/child→org/place-founded
sports-team/location industry/company−1→industry/company→org/place-founded
org/headquarter-state (0.9522 vs. 0.9558)

location/contain−1 location/contain−1

org/headquarter-city→location/contain−1 org/headquarter-city→location/contain−1
org/headquarter-city→bibs-location/state org/headquarter-city→bibs-location/state
org/headquarter-city→county-place/county→location/contain−1 org/headquarter-city
org/headquarter-city→location/contain−1→location/contain−1 org/place-founded
org/geographic-scope (0.5252 vs. 0.6075)

common/class→common/class−1→location/vacationer−1 location/contain−1
common/class→common/class−1→country/languages−1 org/headquarter-city→location/contain−1
common/class→common/class−1→gov-jurisdiction/gov-body−1 location/contain−1→location/contain−1
common/class→common/class−1→region/currency-of-gdp−1 org/place-founded→location/contain−1
politician/party−1→person/nationality→location/adjoins org/headquarter-city→location/contain−1→location/contain−1
org/headquarter-country (0.9859 vs. 0.9938)

org/headquarter-city→airline/city-served−1→org/service-location location/contain−1
org/headquarter-city→admin-area/child−1→region/place-export org/headquarter-city→location/contain−1
org/headquarter-city→country/divisions−1→region/place-export org/headquarter-city→county-place/county→location/contain−1
org/headquarter-city→film/feat-location−1→film/feat-location location/contain−1→location/contain−1
org/headquarter-city→gov-jurisdiction/title→employer/job-title−1 org/place-founded→location/contain−1
org/service-location (0.5644 vs. 0.7044)

org/headquarter-city→country/divisions−1 org/headquarter-city→location/contain−1
org-extra/service-location org/headquarter-city→county-place/county→location/contain−1
film/production-company−1→film/subjects→admin-area/child−1 location/contain−1→location/contain−1
org/legal-structure→entry/taxonomy→entry/taxonomy−1 org/place-founded→location/contain−1
airline/city-served→region/currency→region/currency-of-gdp−1 org-extra/service-location

Table 4: Top paths given by PRA-SVM (left) and CPRA-SVM (right) for each relation in the 6th cluster.

accuracy and model interpretability.

This is the first work that investigates the pos-
sibility of multi-task learning with PRA, and we
just provide a very simple solution. There are still
many interesting topics to study. For instance, the
agglomerative clustering strategy can only identi-
fy highly correlated relations, i.e., those sharing
a lot of common paths. Relations that are only
loosely correlated, e.g., those sharing no common
paths but a lot of sub-paths, will not be identified.
We would like to design new mechanisms to dis-
cover loosely correlated relations, and investigate
whether coupling such relations still provides ben-
efits. Another example is that the current method
is a two-step approach, performing relation clus-
tering first and then relation coupling. It will be in-
teresting to study whether one can merge the clus-

tering step and the coupling step so as to have a
richer inter-task dependent structure. We will in-
vestigate such topics in our future work.

Acknowledgments

We would like to thank Baoxu Shi for providing
the code for path extraction. We would also like to
thank the anonymous reviewers for their valuable
comments and suggestions. This work is support-
ed by the National Natural Science Foundation of
China (grant No. 61402465), the Strategic Priori-
ty Research Program of the Chinese Academy of
Sciences (grant No. XDA06030200), and the Mi-
crosoft Research Asia StarTrack Program. This
work was done when Quan Wang was a visiting
researcher at Microsoft Research Asia.

1316



References

Rie Kubota Ando and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Reseach, 6:1817–1853.

Andreas Argyriou, Theodoros Evgeniou, and Massim-
iliano Pontil. 2007. Multi-task feature learning. In
Advances in Neural Information Processing System-
s, pages 41–48.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247–1250.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured em-
beddings of knowledge bases. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence,
pages 301–306.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Durán, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems, pages 2787–2795.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-
r Settles, Estevam R. Hruschka Jr, and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
24th AAAI Conference on Artificial Intelligence,
pages 1306–1313.

Rich Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41–75.

Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas
Vadrevu, Kilian Weinberger, Ya Zhang, and Belle
Tseng. 2010. Multi-task learning for boosting with
application to web search ranking. In Proceedings
of the 16th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
1189–1198.

Eunsol Choi, Tom Kwiatkowski, and Luke Zettlemoy-
er. 2015. Scalable semantic parsing with partial on-
tologies. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing, pages 1311–1320.

Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 708–
716.

Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu,
Mu Li, and Ming Zhou. 2013. Multi-domain adap-
tation for SMT using multi-task learning. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1055–
1065.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohman-
n, Shaohua Sun, and Wei Zhang. 2014. Knowl-
edge vault: A web-scale approach to probabilistic
knowledge fusion. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 601–610.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, pages
1723–1732.

Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Proceedings of
the 10th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 109–
117.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.

Matt Gardner and Tom Mitchell. 2015. Efficient and
expressive knowledge base completion using sub-
graph feature extraction. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1488–1498.

Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and
inference in a large knowledge-base using laten-
t syntactic cues. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 833–838.

Matt Gardner, Partha Talukdar, Jayant Krishnamurthy,
and Tom Mitchell. 2014. Incorporating vector space
similarity in random walk inference over knowledge
bases. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, pages 397–406.

Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and
Li Guo. 2015. Semantically smooth knowledge
graph embedding. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing, pages 84–94.

Jingrui He, Yan Liu, and Qiang Yang. 2014. Linking
heterogeneous input spaces with pivots for multi-
task learning. In Proceedings of the 2014 SIAM In-
ternational Conference on Data Mining, pages 181–
189.

1317



Shangpu Jiang, Daniel Lowd, and Dejing Dou. 2012.
Learning to refine an automatically extracted knowl-
edge base using markov logic. In Proceedings of the
2012 IEEE International Conference on Data Min-
ing, pages 912–917.

Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 1012–1020.

Denis Krompaß, Stephan Baier, and Volker Tresp.
2015. Type-constrained representation learning in
knowledge graphs. In Proceedings of the 13th Inter-
national Semantic Web Conference, pages 640–655.

Ni Lao and William W. Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine Learning, 81(1):53–67.

Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 529–539.

Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N. Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, Sören Auer, et al. 2014. Dbpedia: A large-
scale, multilingual knowledge base extracted from
wikipedia. Semantic Web Journal.

Arvind Neelakantan, Benjamin Roth, and Andrew M-
cCallum. 2015. Compositional vector space mod-
els for knowledge base completion. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing,
pages 156–166.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings
of the 28th International Conference on Machine
Learning, pages 809–816.

Maximilian Nickel, Xueyan Jiang, and Volker Tresp.
2014. Reducing the rank in relational factorization
models by including observable patterns. In Ad-
vances in Neural Information Processing Systems,
pages 1179–1187.

Jay Pujara, Hui Miao, Lise Getoor, and William Cohen.
2013. Knowledge graph identification. In Proceed-
ings of the 11th International Semantic Web Confer-
ence, pages 542–557.

Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107–136.

Michael Schuhmacher and Simone Paolo Ponzetto.
2014. Knowledge-based graph document modeling.
In Proceedings of the 7th ACM International Con-
ference on Web Search and Data Mining, pages 543–
552.

Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model framework
for statistical machine translation. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, pages 832–840.

Baoxu Shi and Tim Weninger. 2015. Fact checking
in large knowledge graphs: A discriminative predict
path mining approach. In arXiv:1510.05911.

Kristina Toutanova and Danqi Chen. 2015. Observed
versus latent features for knowledge base and tex-
t inference. In Proceedings of the 3rd Workshop on
Continuous Vector Space Models and Their Compo-
sitionality.

Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In Proceedings of the 23rd Interna-
tional Conference on World Wide Web, pages 515–
526.

1318


