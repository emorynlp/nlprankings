



















































XL-NBT: A Cross-lingual Neural Belief Tracking Framework


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 414â€“424
Brussels, Belgium, October 31 - November 4, 2018. cÂ©2018 Association for Computational Linguistics

414

XL-NBT: A Cross-lingual Neural Belief Tracking Framework

Wenhu Chen1, Jianshu Chen2, Yu Su3, Xin Wang1, Dong Yu2
Xifeng Yan1 and William Wang1

University of California, Santa Barbara, CA, USA1

Tencent AI Lab, Bellevue, WA, USA2

The Ohio State University, Columbus, Ohio3

{wenhuchen,xwang,xyan,william}@cs.ucsb.edu, su.809@osu.edu
jianshuchen@tencent.com, dongyu@ieee.org

Abstract

Task-oriented dialog systems are becoming
pervasive, and many companies heavily rely
on them to complement human agents for cus-
tomer service in call centers. With global-
ization, the need for providing cross-lingual
customer support becomes more urgent than
ever. However, cross-lingual support poses
great challengesâ€”it requires a large amount of
additional annotated data from native speak-
ers. In order to bypass the expensive human
annotation and achieve the first step towards
the ultimate goal of building a universal dialog
system, we set out to build a cross-lingual state
tracking framework. Specifically, we assume
that there exists a source language with dia-
log belief tracking annotations while the tar-
get languages have no annotated dialog data
of any form. Then, we pre-train a state tracker
for the source language as a teacher, which
is able to exploit easy-to-access parallel data.
We then distill and transfer its own knowl-
edge to the student state tracker in target lan-
guages. We specifically discuss two types of
common parallel resources: bilingual corpus
and bilingual dictionary, and design different
transfer learning strategies accordingly. Ex-
perimentally, we successfully use English state
tracker as the teacher to transfer its knowl-
edge to both Italian and German trackers and
achieve promising results.

1 Introduction

Over the past few years, we have witnessed the
burgeoning of real-world applications of dialog
systems, with many academic, industrial, and
startup efforts racing to lead the widely-believed
next-generation human-machine interfaces. As
a result, numerous task-oriented dialog systems
such as virtual assistants and customer conversa-
tion services were developed (Wen et al., 2015;
Rojas-Barahona et al., 2017; Bordes and Weston,

2017; Williams et al., 2017; Li et al., 2017), with
Google Duplexbeing the most recent example.

With the rapid process of globalization, more
countries have observed growing populations of
immigrants, and more companies have moved for-
ward to develop their overseas business sectors. To
provide better customer service and bring down
the cost of labor at call centers, the development
of universal dialog systems has become a practi-
cal issue. A straightforward strategy is to sepa-
rately collect training data and train dialog sys-
tems for each language. However, it is not only
tedious but also expensive. Two settings naturally
arise for more efficient usage of the training data:
(1) Multi-lingual setting: we annotate data for
multiple languages and train a single model, with
possible innovations on joint training. (2) Cross-
lingual setting: we annotate data and train a model
for only one (popular) language, and transfer the
learned knowledge to other languages. Here we
are interested in the second case, and the impor-
tant research question we ask is: How can we build
cross-lingual dialog systems that can support less
popular, low- or even zero-resource languages?

As an initial step towards cross-lingual dialog
systems, we focus on the cornerstone of dialog
systems â€“ dialog state tracking (DST), or belief
tracking, a key component for understanding user
inputs and updating belief state, i.e., a systemâ€™s in-
ternal representation of the state of conversation
(Young et al., 2010). Based on the perceived belief
state, the dialog manager can decide which action
to take, and what verbal response to generate (Pre-
cup and Teh, 2017; Bordes and Weston, 2017).

DST models require a considerable amount
of annotated data for training (Henderson et al.,
2014b; Mrksic et al., 2015, 2017). For a common
dialog shown in Figure 1, a typical data acquisition
process (Rojas-Barahona et al., 2017) not only re-
quires two human users to converse for multiple



415

turns but also requires annotators to identify userâ€™s
intention in each turn. Such two-step annotation is
very expensive, especially for rare languages.

We study the novel problem of cross-lingual
DST, where one leverages the annotated data of
a source language to train DST for a target lan-
guage with zero annotated data (Figure 1); no con-
versation dialog or dialog state annotation is avail-
able for the target language. In order to deal with
this zero-resource challenging scenario, we first
decouple the state-of-the-art neural belief tracker
framework (Mrksic et al., 2017) into sub-modules,
namely utterance encoder, context gate, and slot-
value decoder. By introducing a teacher-student
framework, we are able to transfer knowledge
across languages module by module, following
the divide-and-conquer philosophy. Requiring no
target-side dialog data, our method relies on other
easy-to-access parallel resources to understand the
connection between languages. Depending on the
popularity and availability of target language re-
sources, we study two kinds of parallel data: bilin-
gual corpus and bilingual dictionary, and we re-
spectively design two transfer learning strategies.

We use the popular Wizard-of-Oz (Rojas-
Barahona et al., 2017) dataset as our DST bench-
mark to evaluate the effectiveness of our cross-
lingual transfer learning. We specify English as
the source (primary) language and two different
European languages (German and Italian) as our
zero-annotation target languages. Compared with
an array of alternative transfer learning strate-
gies, our cross-lingual DST models consistently
achieve promising results in both scenarios for
both zero-annotation languages. To ensure repro-
ducibility, we release our code, training data and
parallel resources in the github1. Our main contri-
butions are three-fold:
â€¢ Towards building cross-lingual dialog sys-

tems, we are the first to study the cross-
lingual dialog state tracking problem.
â€¢ We systematically study different scenarios

for this problem based on the availability of
parallel data and propose novel transfer learn-
ing methods to tackle the problem.
â€¢ We empirically demonstrate the efficacy of

the proposed methods, showing that our
methods can accurately track dialog states for

1https://github.com/wenhuchen/
Cross-Lingual-NBT

languages with zero annotated data.

2 Related Work

2.1 Dialog State Tracking

Broadly speaking, the dialog belief tracking al-
gorithms can be divided into three families: 1)
hand-crafted rules 2) generative models, and
3) maximum-entropy model (Metallinou et al.,
2013). Later on, many deep learning based dis-
criminative models have surged to replace the
traditional strategies (Henderson et al., 2014a;
Mrksic et al., 2017; Williams et al., 2016)
and achieved state-of-the-art results on various
datasets. Though the discriminative models are re-
ported to achieve fairly high accuracy, their appli-
cations are heavily restricted by the domain, on-
tology, and language. Recently, a pointer network
based algorithm (Xu and Hu, 2018) and another
multi-domain algorithm (Rastogi et al., 2017) have
been proposed to break the ontology and domain
boundary. Besides, (MrksÌŒicÌ et al., 2017) has
proposed an algorithm to train a unified frame-
work to deal with multiple languages with anno-
tated datasets. In contrast, our paper focuses on
breaking the language boundary and transfer DST
knowledge from one language into other zero-
annotation languages.

2.2 Cross-Lingual Transfer Learning

Cross-lingual transfer learning has been a very
popular topic during the years, which can be seen
as a transductive process. In such process, the in-
put domains of the source and target are differ-
ent (Pan and Yang, 2010) since each language has
its own distinct lexicon. By discovering the un-
derlying connections between the source and tar-
get domain, we could design transfer algorithms
for different tasks. Recently, algorithms have been
successfully designed for POS tagging (Zhang
et al., 2016; Kim et al., 2017), NER (Pan et al.,
2017; Ni et al., 2017) as well as image cap-
tioning (Miyazaki and Shimizu, 2016). These
methods first aim at discovering the relatedness
between two languages and separate language-
common modules from language-specific mod-
ules, then resort to external resources to trans-
fer the knowledge across the language boundary.
Our method addresses the transfer learning using
a teacher-student framework and proposes to use
the teacher to gradually guide the student to make
more proper decisions.

https://github.com/wenhuchen/Cross-Lingual-NBT
https://github.com/wenhuchen/Cross-Lingual-NBT


416

Restaurant Price (Preis) Food (Essen) Area	(Bereich) Location (Ort) Telephone (Telefon)

The	House Cheap (Billig) Thai (ThailÃ¤ndisch) Center (Center) 106	Regent	Street 000-00000

Aladdin Expensive (Teuer) Greek (Griechisch) North (Norden) Mesa	Road	100 000-00000

User:	 Iâ€™m	looking	for	a	cheaper Restaurant
Inform(price=cheap)
System:	Sure,	What	kind	â€“and	where?
User:	Thai food,	somewhere	downtown
Inform(price=cheap,food=Thai,area=center)
System:	The	House	serves	cheap	Thai	food
User:	 	Where is	it?
Inform(price=cheap,food=Thai,area=center)
Request(location)
System:	The	House	 is	at	106	Regent	Street

User:	 Ich suche ein teures Restaurant
Inform(preis=?) 
System:	Sicher,	welche	Art	und	wann?
User:	griechisches Essen,	irgendwo	im	Norden
Inform(preis=?,essen=?,ort=?)
System:	The	House	serviert sehr billiges	thailÃ¤ndisches	Essen	
User:	 	Wo ist das?
Inform(preis=?,essen=?,ort=?);Request(?)
System:	Aladdin	befindet sich in	der	Mesa	Road

Transfer

English training dataset German test dataset

Figure 1: Cross-lingual transfer learning for dialog state tracking, where the underlying database (the table above)
is shared across languages. The source language has annotated dialogs and the ground truth states, but the target
language has neither dialogs nor ground truth states (only a testing dataset for evaluation).

3 Problem Definition

Utterance

System acts

Past State

What	do	you	want?

I	want	to	order	Greek	food

food:	none;	area:	center;	price:	none	

food area price Slot

Chinese North Expensive

Va
lu
e

Greek South Middle

Japanese West Cheap

Thai East Donâ€™t	care
Donâ€™t care Donâ€™t care

Slot essen ort preis

Va
lu
e

Chinesisch Norden Teuer

Griechisch SÃ¼d Mittel

Japanisch West Billig

Thai Osten Es ist egal

Es ist egal Es ist egal

DS
T

Source Language ğ’† Target Language ğ’‡

System acts

Past State

Was	wollen	Sie	bestellen

Ich mÃ¶chte chinesisches Essen	bestellen

essen:	none;	ort:	Norden;	preis:	none	

UtteranceD
ST

Figure 2: Cross-lingual DST structure, the ontology
and database between multiple languages are shared.

The dialog states are defined as a set of search
constraints (i.e. informable slots or goals) that the
user specified through the dialog and a set of at-
tribute questions regarding the search results (i.e.
requestable slots or requests). The objective of di-
alog state tracking (DST) is to predict and track
the user intention (i.e., the values of the aforemen-
tioned slots) at each time step based on the cur-
rent user utterance and the entire dialog history.
As shown in Figure 2, for each slot, the DST com-
putes an output distribution of the candidate values
using three inputs: (i) system response at, which
is the sentence generated by the system, (ii) ut-
terance ut, which is the sentence from the user,
and (iii) previous state, which denotes the selected
slot-value pairs. We define the ontology of the di-
alog system to be the set of all the possible words
the dialog slot and value can take. In this pa-
per, we are interested in learning a cross-lingual
DST. Specifically, we assume that the DST for the
source language has access to a human-annotated
training dataset D while the DSTs for the target

languages do not have access to annotated data in
other languages except for testing data. We here
mainly consider two different types of parallel re-
sources to assist the transfer learning:
(1) Bilingual Corpus, where abundant bilingual
corpora exist between the source and the target
languages. This is often the case for common lan-
guage pairs like German, Italian, and French, etc.
(2) Bilingual Dictionary, where public bilingual
dictionaries exist between the source and the tar-
get languages, but large-scaled parallel corpus are
harder to obtain. This can be the case for rarer lan-
guages like Finnish, Bulgarian, etc.
Furthermore, we assume that all the languages
share a common multi-lingual database, whose
column/row names and entry values are stored
via multiple languages (see the database in Fig-
ure 1). That is, the ontology of dialog among dif-
ferent languages is known with a one-to-one map-
ping between them (e.g., greek=griechisch=greco,
food=essen=cibo). Based on that, we could con-
struct a mapping function M to associate the on-
tology terms from different languages with pre-
designed language-agnostic concepts: for exam-
ple, M(foods) = M(Essen) = M(Cibo) =
FOOD. We illustrate our problem definition in Fig-
ure 2.

4 Decoupled Neural Belief Tracker

We design our cross-lingual DST on top
of the state-of-the-art Neural Belief Tracker
(NBT) (Mrksic et al., 2017), which demonstrates
many advantages (no hand-crafted lexicons, no
linguistic knowledge required, etc). These nice
properties are essential for our cross-lingual DST
design because we are pursuing a general and
simple framework regardless of the language
properties. In short, NBT consists of a neural



417

network that computes the matching score for
every candidate slot-value pair (cs, cv) based on
the following three inputs: (i) the system dialog
acts at = (tq, ts, tv),2 (ii) the user utterance
ut, and (iii) the candidate slot-value pair. And
it identifies the user intents by evaluating the
scores for all the slot-value pairs (see Figure 3).
With a slight abuse of notation, we still use
cs, cv, ts, tv, tq âˆˆ RH to denote the vector
representations of themselves, where H is the
embedding dimension. We will use pre-trained
embedding vectors in our cross-lingual NBT, just
like the original NBT and they will be fixed during
training. To enable cross-lingual transfer learning,
we first re-interpret the architecture of the original
NBT by decomposing it into three components:

Utterance Encoding The first component is an
utterance encoder, which maps the utterance ut =
{w1, w2, Â· Â· Â· , wN} of a particular language into
a semantic representation vector r(ut) âˆˆ RH ,
where wi âˆˆ RH is the word vector for the i-th to-
ken and N is the length of the utterance. Note that
the dimension of the semantic vector r(ut) is the
same as that of the word vector. We implement

ğ‘Ÿ

ğ‘”

ğ‘¦

ğ‘¤% ğ‘¤& ğ‘¤' ğ‘¤(

Utterance

ğ‘*ğ‘+

Candidate 

ğ‘¡*ğ‘¡+ğ‘¡-

System acts

Utterance Encoding Slot-value Decoding

Aggregation Gate

Score

Request
gate ğ‘”&

Candidate
gate ğ‘”%

Confirm
gate ğ‘”'

CNN/RNN

Figure 3: Our implementation of baseline NBT,
slightly modified from (Mrksic et al., 2017).

the encoder using the same convolutional neural
network (CNN) as the original NBT, with a slight
modification of adding a top batch normalization
layer. We will explain this change in section 5.

Context Gate The second part is the context
gate, which takes the system acts at = (tq, ts, tv)

2tq represents the system request, ts, tv represents the
system confirmation. If the system wants to request some
information from the user by asking â€œwhatâ€™s your favorite
area?â€, then NBT sets tq=â€œAREAâ€. If the system wants to
confirm some information from a user by asking â€œshould I try
Persian restaurants in the north?â€ then NBT sets ts, tv=â€œarea,
northâ€.

and the candidate slot-value pair (cs, cv) as its in-
puts and filter out the desired information from the
encoded utterance. The context gate g is a sum of
three separate gates:

g(cs, cv, at) = g1 + g2 + g3 (1)

where the individual gates are defined as:
g1 = Ïƒ(W

s
c (cs + cv) + b

s
c)

g2 = (cs Â·W qt tq)ï¿½ [1, Â· Â· Â· , 1]H

g3 = (cs Â·W st ts)(cv Â·W vt tv)ï¿½ [1, Â· Â· Â· , 1]H
(2)

where W sc ,W
q
t ,W

s
t ,W

v
t âˆˆ RHÃ—H are the weight

matrices, and ï¿½ and Â· denote the Hadamard prod-
uct and the inner product, respectively. The three
gates g1 âˆˆ RH , g2 âˆˆ RH , g3 âˆˆ RH model the rel-
evance between the candidate slot and value, the
system request and the system confirms, respec-
tively. The transformation matrices W qt ,W

s
t ,W

v
t

are added to the original NBT to increase the
model flexibility of the gates.

Slot-Value Decoding The final component is a
slot-value decoder, which predicts the score y of a
given slot-value pair using the filtered information
from the utterance representation r as:

y(cs, cv, ut, at) =W
T
y [r(ut)ï¿½ g(cs, cv, at)] (3)

where Wy âˆˆ RHÃ—1 is the weight vector. The
above expression computes the score for the slot-
value pair based on the information from the cur-
rent turn. We combine it with the information
from previous turns to get the final score:

yÌ‚(cv|ut, at, cs) =Î»y(cs, cv, ut, at)+
(1âˆ’ Î»)yÌ‚(cs, cv, utâˆ’1, atâˆ’1)

(4)

here Î» is a combination weight. For each given
slot cs, NBT selects the single highest value for
informable slots and selects all values above a cer-
tain threshold for request slots. Here we replace
the multi-layer perceptron in the orginal NBT by a
linear output layer (to be explained in section 5).

5 Cross-lingual Neural Belief Tracker

In this section, we develop a cross-lingual Neu-
ral Belief Tracker (XL-NBT) that distills knowl-
edge from one NBT to another using a teacher-
student framework. We assume the ontology map-
ping M is known a priori (see Figure 3). XL-
NBT uses language-specific utterance encoder and
context gate for each input language while shar-
ing a common (language-agnostic) slot-value de-
coder across different languages (see Figure 3).



418

The key idea is to optimize the language-specific
components of the student network (NBT of the
target language) so that their outputs are language-
agnostic. This is achieved by making these outputs
close to that of the teacher network (NBT of the
source language), as we detail below.

5.1 Teacher-Student Framework

We are given a well-trained NBT for a source
language e, and we want to learn an NBT for a
target language f without any annotated training
data. Therefore, we cannot learn the target-side
NBT from standard supervised learning. Instead,
we use a teacher-student framework to distill the
knowledge from the source-side NBT (teacher net-
work) into the target-side NBT (student network)
(see Figure 4). Let xe , (ces, c

e
v, u

e
t , a

e
t ) be

the input to the teacher network and let xf ,
(cfs , c

f
v , u

f
t , a

f
t ) be the associated input to the stu-

dent network. The standard teacher-student frame-
work trains the student network by minimizing

JÌ‚1 =
âˆ‘
xe,xf

||y(ces, cev, uet , aet )âˆ’ y(cfs , cfv , uft , a
f
t )||

2

(5)

where y(ces, c
e
v, u

e
t , a

e
t ) and y(c

f
s , c

f
v , u

f
t , a

f
t ) de-

note the scores by the teacher and the student net-
works, respectively, and the slot-value pairs satisfy
M(cfv ) = M(cev) and M(c

f
s ) = M(ces). How-

ever, the target-side inputs (cfs , c
f
v , u

f
t , a

f
t ) parallel

to (ces, c
e
v, u

e
t , a

e
t ) are usually not available in cross-

lingual DST, and, even worse, the target-side ut-
terance uet is not available. We may have to gener-
ate synthetic input data for the student network or
leverage external data sources. It is relatively easy
to use the mapping M(Â·) to generate (cfs , cfv , aft ))
(i.e., the inputs of the target-side context gate)
from the (ces, c

e
v, a

e
t ). But it is more challenging

to obtain the parallel utterance data uft from u
e
t ).

Therefore, we have to leverage external bilingual
data sources to alleviate the problem. However,
the external bilingual data are usually not in the
same domain as the utterance, and hence they are
not aligned with the slot-value pair and system acts
(i.e., (ces, c

e
v, a

e
t ) or (c

f
s , c

f
v , a

f
t )). For this reason,

we cannot perform the knowledge transfer by min-
imizing the cost (5). Instead, we need to develop
a new cost function where the utterance is not re-
quired to be aligned with the slot-value pair and
the system acts. To this end, let ge = ge(ces, c

e
v, a

e
t )

and gf = gf (c
f
s , c

f
v , a

f
t ). And we substitute (3)

into (5) and get:

JÌ‚1 â‰¤ ||Wy||2
âˆ‘
c
f
v ,cev

||re ï¿½ ge âˆ’ rf ï¿½ gf ||2

= ||Wy||2
âˆ‘
c
f
v ,cev

||ge ï¿½ (re âˆ’ rf ) + rf ï¿½ (ge âˆ’ gf )||2

â‰¤ ||Wy||2
âˆ‘
c
f
v ,cev

||ge||2||re âˆ’ rf ||2 + ||rf ||2||ge âˆ’ gf ||2

where re = re(uet ) and rf = rf (u
f
t ). As

we mentioned earlier, the weight Wy in the slot-
value decoder is shared between the student and
the teacher networks and will not be updated.
The teacher-student optimization only adjusts the
weights related to the language-specific parts in
Figure 3 (i.e., utterance encoding and context gat-
ing). Therefore, the shared weight ||Wy|| is seen
as a constant. Furthermore,

âˆ‘
cfv ,cev
||ge||2 can be

seen as a constant since the teacher gate is fixed.
Since we use batch normalization layer to nor-
malize the encoder output (described in Figure 3),
||rf (uft )||2 can also be treated as a constant C2.
Therefore, we formally write the upper bound of
JÌ‚1 as our surrogate cost function J :

J =C1||re(uet )âˆ’ rf (uft )||
2 + C2

âˆ‘
c
f
v ,cev

||ge âˆ’ gf ||2 (6)

The surrogate cost has successfully decoupled ut-
terance encoder with context gate, and we use Jr
and Jg to measure the encoder matching cost and
the gate matching cost, respectively.

Jr = ||re(uet )âˆ’ rf (uft )||
2

Jg =
âˆ‘
c
f
v ,cev

||ge âˆ’ gf ||2 (7)

The encoder cost Jr is optimized to distill the
knowledge from the teacher encoder to student
encoder while gate cost Jg is optimized to dis-
till the knowledge from teacher gate to student
gate. This objective function successfully decou-
ples the optimization of encoder and gate, thus we
are able to optimize Jr and Jg separately from dif-
ferent data sources. Recall that we can easily sim-
ulate the target-side system acts, slot-value pairs
(cfs , c

f
v , af ) by using the ontology mapping M .

Therefore, optimizing Jg is relatively easy. For-
mally, we write the gate matching cost as follows:

Jg =
âˆ‘

aet ,c
e
s,c

e
v

a
f
t ,c

f
s ,c

f
v

||ge(ces, cev, aet )âˆ’ gf (cfs , cfv , aft )||
2

(8)



419

I	want	to	order	Greek	food

What	kind	of	food	do	
you	want?

greek
chinese
thai
japanese

ğ‘²ğ‘³(ğ’‘ğ’†(ğ’„ğ’—|ğ®ğ­, ğšğ­,ğ’„ğ’”)||ğ’‘ğ’‡(ğ’„ğ’—|ğ®ğ­, ğšğ­, ğ’„ğ’”))

ğ’•ğ’’:ğ‘­ğ‘¶ğ‘¶ğ‘«					
ğ’•ğ’”, ğ’•ğ’”:	None,	None	

ğ’„ğ’”ğ’†, ğ’„ğ’—ğ’† :	(FOOD,	greek)

||ğ’“ ğ’–ğ’•ğ’† âˆ’ ğ’“(ğ’–ğ’•
ğ’‡)||

IchmÃ¶chte griechisches Essen

Welche Art	von	Essen	
willst du?

ğ’•ğ’’:ğ‘¬ğ‘ºğ‘ºğ‘¬ğ‘µ
ğ’•ğ’”, ğ’•ğ’”:	None,	None	

ğ’„ğ’”
ğ’‡, ğ’„ğ’—

ğ’‡:	(ESSEN,	grieschisch)

ğ’‚ğ’•
ğ’‡

ğ’–ğ’•
ğ’‡

grieschisch
Chinesisch
thailÃ¤ndisch
japanisch

ğ’‚ğ’•ğ’†

ğ’–ğ’•ğ’†

||ğ’ˆ(ğ’‚ğ’•
ğ’‡, ğ’„ğ’”

ğ’‡, ğ’„ğ’—
ğ’‡) âˆ’ğ’ˆ(ğ’‚ğ’•ğ’†, ğ’„ğ’”ğ’†, ğ’„ğ’—ğ’†)||

+

Figure 4: Teacher-Student Framework for cross-lingual transfer learning. The dotted line denotes the imaginary
utterances, which expresses the same intention as the source side.

In	birds,	life	gains	new	mobility. Bei	VÃ¶geln	erhÃ¤lt	das	Leben	neue	MobilitÃ¤t.

I	can	take	a	look	at	your	records. Ich	kann	mir	deine	Unterlagen	ansehen.

Parallel  Corpora ğ‘«ğ’‘

Student
Encoder

||ğ’“ ğ’–ğ’•ğ’† âˆ’ ğ’“(ğ’–ğ’•
ğ’‡)||

Teacher 
Encoder CNN/RNN CNN/RNN

I	want	to	order	Greek	food

Dialogue Corpora D

Ich want	to	bestellen Greek	food

#sub=2; pos=0, 3 Mixed Language Corpora Dâ€™

Bilingual	Embedding

Replace

Ich mir

order

bestellen ordnen

Bilingual Dictionary ğ‘«ğ‘©I

CNN/RNN CNN/RNN
Student
Encoder

Teacher
Encoder

Figure 5: XL-NBT-C and XL-NBT-D for two scenarios

However, exact optimization of Jr is difficult and
we have to approximate it using external parallel
data. We consider two kinds of external resources
(bilingual corpus and bilingual dictionary) in the
sections 5.2-5.3 (see Figure 5 for the main idea).

5.2 Bilingual Corpus (XL-NBT-C)

In our first scenario, we assume there exists a par-
allel corpus Dp consisting of sentence pairs from
the source language and the target language. In
this case, the cost function (6) is approximated by

J = E
(me,mf )âˆˆDp

||re(me)âˆ’ rf (mf )||2 + Î±Jg (9)

where Î± is the balancing factor and Jg is defined in
(6). The cost function (9) is minimized by stochas-
tic gradient descent. At test time, we switch the
encoder to receive target language inputs.

5.3 Bilingual Dictionary (XL-NBT-D)

In the second scenario, we assume there exists
no parallel corpus but a bilingual dictionary DB
that defines the correspondence between source
words and target words (a one-to-many mapping
{w : MD(w)}). Likewise, it is infeasible to op-
timize the exact encoder cost Jr due to the lack
of target-side utterances. We propose a word re-
placement strategy (to be described later) to gener-
ate synthetic parallel sentence uÌ‚ft of â€œmixedâ€ lan-
guage. Then, we use the generated target parallel
sentences to approximate the cost (6) by

Jr = E
utâˆˆD

||re(uet )âˆ’ rf (uÌ‚ft )||
2 + Î±Jg (10)

where Î± is the balancing factor. For word replace-
ment, we first decide the number of words Nw to
be replaced, then we draw Nw positions randomly
from the source utterance and substitute the corre-
sponding word wi with their target word synonym
from MD(w) based on the context as follows:

jp(Nw = i) =
exp(âˆ’i/Ï„)âˆ‘

iâ€²<N exp(âˆ’iâ€²/Ï„)

p(wÌ‚) =
wÌ‚ Â· hwÌ‚âˆ‘

wâ€²âˆˆM(wi) w
â€² Â· hwÌ‚

(11)

where hwÌ‚ =
âˆ‘2

k=âˆ’2:k 6=0wi+k represents the con-
text vector and N denotes the utterance length.
The context similarity of context and the target-
side synonym can better help us in choosing the
most appropriate candidate from the list. In our
following experiments, we adjust the temperature
of Ï„ to control the aggressiveness of replacement.



420

6 Experiments

6.1 Dataset

The Wizard of Oz (WOZ) (Rojas-Barahona et al.,
2017) dataset is used for training and evaluation,
which consists of user conversations with task-
oriented dialog systems designed to help users
find suitable restaurants around Cambridge, UK.
The corpus contains three informable (i.e. goal-
tracking) slots: FOOD, AREA, and PRICE. The
users can specify values for these slots in order to
find which best meet their criteria. Once the sys-
tem suggests a restaurant, the users can ask about
the values of up to eight requestable slots (PHONE
NUMBER, ADDRESS, etc.). Multilingual WOZ
2.0 (Mrksic et al., 2017) has expanded this dataset
to include more dialogs and more languages. The
train, valid and test datasets for three different lan-
guages (English, German, Italian) are available
online3. We use the English as source language
where 600 dialogs are used for training, 200 for
validation and 400 for testing. We use the German
and Italian as the target language to transfer our
knowledge from English DST system. In the ex-
periments, we do not have access to any training
or validation dataset for German and Italian, and
we only have access to their testing dataset which
is composed of 400 dialogs.

For external resource, we use the IWSLT2014
Ted Talk parallel corpus (Mauro et al., 2012) from
the official website4 for bilingual corpus scenario.
In the IWSLT2014 parallel corpus, we only keep
the sentences between 4 and 40 words and de-
crease the sentence pairs to around 150K. We use
Panlex (Kamholz et al., 2014) as our data source
and crawl translations for all the words appearing
in the dialog datasets to build our bilingual dictio-
nary. We specifically investigate two kinds of pre-
trained embedding, and we use Glove (Pennington
et al., 2014) as the monolingual embedding and
MUSE (Conneau et al., 2017) as the bilingual em-
bedding to see their impacts on the DST perfor-
mance.

We split the raw DST corpus into turn-level ex-
amples. During training, we use the ground truth
previous state Vtâˆ’1 as inputs. At test time, we
use the model searched states as the previous state
to continue tracking intention until the end of the

3https://github.com/nmrksic/
neural-belief-tracker/tree/master/data

4https://wit3.fbk.eu/mt.php?release=
2014-01

dialog. When the dialog terminates, we use two
evaluation metrics introduced in Henderson et al.
(2014a) to evaluate the DST performance: (1)
Goals: the proportion of dialog turns where all the
users search goal constraints were correctly iden-
tified. (2) Requests: similarly, the proportion of
dialog turns where users requests for information
were identified correctly. Our implementation is
based on the NBT5, the details of our system set-
ting are described in the appendix.

6.2 Results

Here we highlight the baselines we use to compare
with our cross-lingual algorithm as follows:
(1) Supervised: this baseline algorithm assumes
the existence of annotated dialog belief tracking
datasets, and it determines the upper bound of the
DST model.
(2) w/o Transfer: this algorithm trains an English
NBT, and then directly feeds target language into
the embedding level as inputs during test time to
evaluate the performance.
(3) Ontology-match: this algorithm directly uses
exact string matching against the utterance to dis-
cover the perceived slot-value pairs, it directly as-
signs a high score to the appearing candidates.
(4) Translation-based: this system pre-trains a
translator on the external bilingual corpus and then
translates the English dialog and ontology into tar-
get language as â€œannotatedâ€ data, which is used
to train the NBT in the target language domain
(more details about the implementation, perfor-
mance and examples are listed in the appendix).
(5) Word-By-Word (WBW): this system trans-
forms the English dialog corpus into target lan-
guage word by word using the bilingual dictionary,
which is used to train the NBT in target side.
We demonstrate the results for our proposed al-
gorithms and other competing algorithms in Ta-
ble 2, from which we can easily conclude that that
(i) our Decoupled NBT does not affect the perfor-
mance, and (ii) our cross-lingual NBT framework
is able to achieve significantly better accuracy for
both languages in both parallel-resource scenarios.

Compare with Translator/WBW. With bilin-
gual corpus, XL-NBT-C with pre-trained bilin-
gual embedding can significantly outperform our
Translator baseline (Klein et al., 2017). This is
intuitive because the translation model requires

5https://github.com/nmrksic/
neural-belief-tracker

https://github.com/nmrksic/neural-belief-tracker/tree/master/data
https://github.com/nmrksic/neural-belief-tracker/tree/master/data
https://wit3.fbk.eu/mt.php?release=2014-01
https://wit3.fbk.eu/mt.php?release=2014-01
https://github.com/nmrksic/neural-belief-tracker
https://github.com/nmrksic/neural-belief-tracker


421

Error Type Examples

Modify
Failure

Machine: I have two options that fit that description, golden wok Chinese restaurant and the Nirala which
serves Indian food, do you have a preference?
User: How about Nirala, whats the address and phone of that?
Previous State: food=Chinese; Prediction: food=none; Groundtruth: food=Indian

Maintain
Failure

Machine: there are $num places with a moderate price range. can you please tell me what kind of food
you would like?
User: well I want to eat in the north, whats up that way?
Previous State: food=expensive; Prediction: food=none; Groundtruth: food=expensive

History
Failure

Machine: Anatolia is located at $num bridge street city center.
User: thank you goodbye!
Previous State: food=Chinese; Prediction: food=Chinese;,Groundtruth: food=Turkish

Table 1: Here we show the frequent error types, the examples are translated to English for better understanding.

Language German (student) Italian (student) English (teacher)

Models Goal Request Goal Request Goal Request

w/ Supervised Dialog
NBT (Mrksic et al., 2017) - - - - 0.84 0.91
Decoupled NBT (mono) 0.79 0.83 0.86 0.91 0.82 0.89
Decoupled NBT (bilingual) 0.80 0.84 0.88 0.91 0.84 0.90

w/o Bilingual Data
w/o Transfer (mono) 0.15 0.10 0.15 0.11 - -
w/o Transfer (bilingual) 0.13 0.13 0.11 0.12 - -
Ontology Matching 0.24 0.21 0.23 0.21 - -

w/ Bilingual Corpus
Translate (Klein et al., 2017) 0.41 0.42 0.48 0.51 - -
XL-NBT-C (mono) 0.48 0.54 0.65 0.60 - -
XL-NBT-C (bilingual) 0.55 0.59 0.72 0.69 - -

w/ Bilingual Dictionary
Word-by-Word 0.22 0.25 0.25 0.27 - -
XL-NBT-D (mono) 0.14 0.15 0.23 0.22 - -
XL-NBT-D (bilingual) 0.51 0.56 0.73 0.63 - -

Table 2: Experimental results for cross-lingual NBT and other baseline algorithms. All results are averaged over
5 runs. Here we use â€œmonoâ€ to refer to the experiments with pre-trained monolingual embedding, â€œbilingualâ€ to
refer to the experiments with pre-trained bilingual embedding.

both source-side encoding and target-side word-
by-word decoding, while our XL-NBT only needs
a bilingual source-encoding to align two vector
space, which averts the compounded decoding er-
rors. With the bilingual dictionary, the word-by-
word translator is very weak and leading to many
broken target sentences, which poses challenges
for DST training. In comparison, our XL-NBT-
D can control the replacement by adjusting its
temperature to maintain the stability of utterance
representation. Furthermore, for both cases, our
teacher-student framework can make use of the
knowledge learned in source-side NBT to assist its
decision making, while translator-based methods
learn from scratch.

Bilingual Corpus vs. Bilingual Dictionary.
From the table, we can easily observe that bilin-
gual corpus is obviously a more informative par-
allel resource to perform cross-lingual transfer
learning. The accuracy of XL-NBT-D is lower
than XL-NBT-C. We conjecture that our replace-

ment strategy to generate â€œmixedâ€ language ut-
terance can sometimes break the semantic coher-
ence and cause additional noises during the trans-
fer process, which remarkably degrades the DST
performance.

Monolingual vs. Bilingual embedding. From
the table, we can observe that the bilingual embed-
ding and monolingual embedding does not make
much difference in supervised training. How-
ever, the gap in the bilingual corpus case is quite
obvious. Monolingual embedding even causes
the transfer to fail in a bilingual dictionary case.
We conjecture that the bilingual word embed-
ding already contain many alignment information
between two languages, which largely eases the
training of encoder matching objective.

German vs. Italian As can be seen, the transfer
learning results for Italian are remarkably higher
than German, especially for the â€œGoalâ€ accuracy.
We conjecture that it is due to German declen-
sion, which can produce many word forms. The



422

very diverse word forms present great challenges
for DST to understand its intention behind. Espe-
cially for the bilingual dictionary, German tends
to have much longer replacement candidate lists
than Italian, which introduces more noises to the
replacement procedure.

Error Analysis Here we showcase the most fre-
quent error types in subsection 6.1. From our
observation, these three types of errors distribute
evenly in the test dialogs. The error mainly comes
from the unaligned utterance space, which leads
to failure in understanding the intention of human
utterance in the target language. This can lead
the system to fail in modifying the dialog state or
maintaining the previous dialog states.

6.3 Discussion

Here we want to further highlight the comparison
between our transfer learning algorithm with the
MT-based approach. Though our approach outper-
forms the standard Translator trained on IWSLT-
2014, it does not necessarily claim that our transfer
algorithm outperforms any translation methods on
any parallel corpus. In our further ablation stud-
ies, we found that using Google Translator 6 can
actually achieve a better score than our transfer al-
gorithm, which is understandable considering the
complexity of Google Translator and the much
larger parallel corpus it leverages. By leverag-
ing more close-to-domain corpus and comprehen-
sive entity recognition/replacement strategy, the
translator model is able to achieve a higher score.
Apparently, we need to trade off the efficiency
for the accuracy. For DST problem, it is an
overkill to introduce a more complex translation
algorithm, what we pursue is a simple yet effi-
cient algorithm to achieve promising scores. It
is also worth mentioning that our XL-NBT al-
gorithm only takes several hours to achieve the
reported score, while the translator model takes
much more time and memory to train depend-
ing on the complexity. Thus, the simplicity and
efficiency makes our model a better fit for rare-
language and limited-budget scenarios.

6.4 Ablation Test

Here we investigate the effectâ€˜ of hyper-parameter
Î±, Ï„ on the evaluation results. The Î± is used to
balance the optimization of encoder constraint and

6https://translate.google.com/

gate constraint, where larger Î± means more opti-
mization on gate constraint. The temperature Ï„ is
used to control the aggressiveness of the replace-
ment XL-NBT-D, where smaller Ï„ means more
source words are replaced by target synonyms.
From the table Table 3, we can observe that the

Î± ablation (Ï„ fixed to 0.1) Ï„ ablation (Î± fixed to 1)

value Goal Request value Goal Request

Î±=0 0.13 0.00 Ï„=0 0.14 0.08
Î±=0.1 0.46 0.54 Ï„=0.03 0.43 0.50
Î±=1 0.51 0.56 Ï„=0.1 0.51 0.56
Î±=5 0.46 0.54 Ï„=0.3 0.47 0.51
Î±=10 0.46 0.52 Ï„=1 0.44 0.52
Î±=100 0.44 0.50 Ï„=10 0.33 0.32

Table 3: Ablation test for hyper-parameter Î± and Ï„ on
English-to-German XL-NBT-D.

experimental results are not very sensitive to Î±,
a dramatic change of Î± will not harm the final
results too much, we simply choose Î± = 1 as
the hyper-parameter. In contrast, the system is
more sensitive to temperature. Too conservative
replacement will lead to weak transfer, while too
aggressive replacement will destroy the utterance
representation. Therefore, we choose the a moder-
ate temperature of Ï„ = 0.1 throughout our experi-
ments. We also draw the learning curve (Precision
vs. Iteration) in the Appendix for both XL-NBT-
C and XL-NBT-D. The learning curves show that
our algorithm is stable and converges quickly, and
the reported results are highly reproducible.

7 Conclusion

In our paper, we propose a novel teacher-student
framework to perform cross-lingual transfer learn-
ing for DST. The key idea of our model is to de-
couple the current DST neural network into two
separate modules and transfer them separately. We
believe our method can be further extended into a
general purpose multi-lingual transfer framework
to resolve other NLP matching or classification
problems.

8 Acknowledgement

We are gratefully supported by a Tencent AI Lab
Rhino-Bird Gift Fund. We are also very thank-
ful for the public belief tracking code and multi-
lingual state-tracking datasets released by Nikola
Mrksic from the University of Cambridge.

https://translate.google.com/


423

References

Antoine Bordes and Jason Weston. 2017. Learning
end-to-end goal-oriented dialog. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR).

Alexis Conneau, Guillaume Lample, Marcâ€™Aurelio
Ranzato, Ludovic Denoyer, and HerveÌ JeÌgou. 2017.
Word translation without parallel data. In Proceed-
ings of the International Conference on Learning
Representations (ICLR).

Matthew Henderson, Blaise Thomson, and Steve
Young. 2014a. Robust dialog state tracking using
delexicalised recurrent neural networks and unsu-
pervised adaptation. In Spoken Language Technol-
ogy Workshop (SLT), 2014 IEEE, pages 360â€“365.
IEEE.

Matthew Henderson, Blaise Thomson, and Steve J.
Young. 2014b. Word-based dialog state tracking
with recurrent neural networks. In Proceedings of
the SIGDIAL 2014 Conference, The 15th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, 18-20 June 2014, Philadelphia, PA,
USA, pages 292â€“299.

David Kamholz, Jonathan Pool, and Susan M Colow-
ick. 2014. Panlex: Building a resource for panlin-
gual lexical translation. In LREC, pages 3145â€“3150.

Joo-Kyung Kim, Young-Bum Kim, Ruhi Sarikaya, and
Eric Fosler-Lussier. 2017. Cross-lingual transfer
learning for pos tagging without cross-lingual re-
sources. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2832â€“2838.

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. ArXiv e-prints.

Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao,
and Asli CÌ§elikyilmaz. 2017. End-to-end task-
completion neural dialogue systems. In Proceedings
of the Eighth International Joint Conference on Nat-
ural Language Processing, IJCNLP 2017, Taipei,
Taiwan, November 27 - December 1, 2017 - Volume
1: Long Papers, pages 733â€“743.

Cettolo Mauro, Girardi Christian, and Federico Mar-
cello. 2012. Wit3: Web inventory of transcribed and
translated talks. In Conference of European Associ-
ation for Machine Translation, pages 261â€“268.

Angeliki Metallinou, Dan Bohus, and Jason Williams.
2013. Discriminative state tracking for spoken di-
alog systems. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
466â€“475.

Takashi Miyazaki and Nobuyuki Shimizu. 2016.
Cross-lingual image caption generation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 1780â€“1790.

Nikola Mrksic, Diarmuid OÌ SeÌaghdha, Blaise Thom-
son, Milica Gasic, Pei-hao Su, David Vandyke,
Tsung-Hsien Wen, and Steve J. Young. 2015. Multi-
domain dialog state tracking using recurrent neural
networks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing, ACL 2015, July 26-
31, 2015, Beijing, China, Volume 2: Short Papers,
pages 794â€“799.

Nikola Mrksic, Diarmuid OÌ SeÌaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve J. Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2017, Vancouver, Canada, July 30 - August
4, Volume 1: Long Papers, pages 1777â€“1788.

Nikola MrksÌŒicÌ, Ivan VulicÌ, Diarmuid OÌ SeÌaghdha, Ira
Leviant, Roi Reichart, Milica GasÌŒicÌ, Anna Korho-
nen, and Steve Young. 2017. Semantic special-
isation of distributional word vector spaces using
monolingual and cross-lingual constraints. arXiv
preprint arXiv:1706.00374.

Jian Ni, Georgiana Dinu, and Radu Florian. 2017.
Weakly supervised cross-lingual named entity
recognition via effective annotation and representa-
tion projection. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 1: Long Papers, pages 1470â€“1480.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345â€“1359.

Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
Nothman, Kevin Knight, and Heng Ji. 2017. Cross-
lingual name tagging and linking for 282 languages.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 1946â€“1958.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532â€“1543.

Doina Precup and Yee Whye Teh, editors. 2017. Pro-
ceedings of the 34th International Conference on
Machine Learning, ICML 2017, Sydney, NSW, Aus-
tralia, 6-11 August 2017, volume 70 of Proceedings
of Machine Learning Research. PMLR.



424

Abhinav Rastogi, Dilek Hakkani-TuÌˆr, and Larry P.
Heck. 2017. Scalable multi-domain dialogue state
tracking. In 2017 IEEE Automatic Speech Recogni-
tion and Understanding Workshop, ASRU 2017, Ok-
inawa, Japan, December 16-20, 2017, pages 561â€“
568.

Lina Maria Rojas-Barahona, Milica Gasic, Nikola
Mrksic, Pei-Hao Su, Stefan Ultes, Tsung-Hsien
Wen, Steve J. Young, and David Vandyke. 2017.
A network-based end-to-end trainable task-oriented
dialogue system. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL 2017, Valen-
cia, Spain, April 3-7, 2017, Volume 1: Long Papers,
pages 438â€“449.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-
hao Su, David Vandyke, and Steve J. Young. 2015.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2015, Lisbon, Portugal, September 17-21, 2015,
pages 1711â€“1721.

Jason Williams, Antoine Raux, and Matthew Hender-
son. 2016. The dialog state tracking challenge se-
ries: A review. Dialogue & Discourse, 7(3):4â€“33.

Jason D. Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: practical and efficient
end-to-end dialog control with supervised and rein-
forcement learning. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30
- August 4, Volume 1: Long Papers, pages 665â€“677.

Puyang Xu and Qi Hu. 2018. An end-to-end approach
for handling unknown slot values in dialogue state
tracking. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2018, Melbourne, Australia, July 15-20,
2018, Volume 1: Long Papers, pages 1448â€“1457.

Steve Young, Milica GasÌŒicÌ, Simon Keizer, FrancÌ§ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for pomdp-based spoken dia-
logue management. Computer Speech & Language,
24(2):150â€“174.

Yuan Zhang, David Gaddy, Regina Barzilay, and
Tommi S. Jaakkola. 2016. Ten pairs to tag - mul-
tilingual POS tagging via coarse mapping between
embeddings. In NAACL HLT 2016, The 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, San Diego California, USA,
June 12-17, 2016, pages 1307â€“1317.


