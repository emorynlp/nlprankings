



















































Morpho Challenge 2005-2010: Evaluations and Results


Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 87–95,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics

Morpho Challenge competition 2005-2010: Evaluations and results

Mikko Kurimo, Sami Virpioja, Ville Turunen, Krista Lagus

Adaptive Informatics Research Centre

Aalto University, Espoo, Finland

Firstname.Lastname@tkk.fi

Abstract

Morpho Challenge is an annual evalu-

ation campaign for unsupervised mor-

pheme analysis. In morpheme analysis,

words are segmented into smaller mean-

ingful units. This is an essential part in

processing complex word forms in many

large-scale natural language processing

applications, such as speech recognition,

information retrieval, and machine trans-

lation. The discovery of morphemes is

particularly important for morphologically

rich languages where inflection, deriva-

tion and composition can produce a huge

amount of different word forms. Morpho

Challenge aims at language-independent

unsupervised learning algorithms that can

discover useful morpheme-like units from

raw text material. In this paper we de-

fine the challenge, review proposed algo-

rithms, evaluations and results so far, and

point out the questions that are still open.

1 Introduction

Many large-scale natural language processing

(NLP) applications, such as speech recognition,

information retrieval and machine translation, re-

quire that complex word forms are analyzed into

smaller, meaningful units. The discovery of these

units called morphemes is particularly important

for morphologically rich languages where the in-

flection, derivation and composition makes it im-

possible to even list all the word forms that are

used. Various tools have been developed for mor-

pheme analysis of word forms, but they are mostly

based on language-specific rules that are not eas-

ily ported to other languages. Recently, the per-

formance of tools based on language-independent

unsupervised learning from raw text material has

improved significantly and rivaled the language-

specific tools in many applications.

The unsupervised algorithms proposed so far in

Morpho Challenge typically first generate various

alternative morphemes for each word and then se-

lect the best ones based on relevant criteria. The

statistical letter successor variation (LSV) analy-

sis (Harris, 1955) and its variations are quite com-

monly used as generation methods. LSV is based

on the observation that the segment borders be-

tween the sub-word units often co-occur with the

peaks of variation for the next letter. One popu-

lar selection approach is to minimize a cost func-

tion that balances between the size of the corpus

when coded by the morphemes and the size of

the morpheme codebook needed. Selection cri-

teria that produce results resembling the linguis-

tic morpheme segmentation include, for example,

the Minimum Description Length (MDL) princi-

ple and maximum a posteriori (MAP) probability

optimization (de Marcken, 1996; Creutz and La-

gus, 2005).

The Morpho Challenge competition was

launched in 2005 to encourage the machine

learning people, linguists and specialists in NLP

applications to study this field and come together

to compare their best algorithms against each

other. The organizers selected evaluation tasks,

data and metric and performed all the evaluations.

Thus, participation was made easy for people

who were not specialists in the chosen NLP

applications. Participation was open to everybody

with no charge. The competition became popular

right from the beginning and has gained new

participants every year.

Although not all the authors of relevant mor-

pheme analysis algorithms have yet submitted

their algorithms for this evaluation campaign,

more than 50 algorithms have already been eval-

uated. After the first five years of Morpho Chal-

lenge, a lot has been learned on the various pos-

sible ways to solve the problem and how the dif-

ferent methods work in various NLP tasks. How-

87



ever, there are still open questions such as: how to

find meaning for the obtained unsupervised mor-

phemes, how to disambiguate among the alterna-

tive analyses of one word, and how to use context

in the analysis. Another recently emerged ques-

tion that is the special topic in 2010 competition

is how to utilize small amounts of labeled data

and semi-supervised learning to further improve

the analysis.

2 Definition of the challenge

2.1 Morphemes and their evaluation

Generally, the morphemes are defined as the

smallest meaningful units of language. Rather

than trying to directly specify which units are

meaningful, the Morpho Challenge aims at find-

ing units that would be useful for various practical

NLP applications. The goal is to find automatic

methods that can discover suitable units using un-

supervised learning directly on raw text data. The

methods should also not be restricted to certain

languages or include many language and applica-

tion dependent parameters that needed to be hand

tuned for each task separately. The following three

goals have been defined as the main scientific ob-

jectives for the challenge: (1) To learn of the phe-

nomena underlying word construction in natural

languages. (2) To discover approaches suitable for

a wide range of languages. (3) To advance ma-

chine learning methodology.

The evaluation tasks, metrics and languages

have been designed based on the scientific objec-

tives of the challenge. It can not be directly ver-

ified how well an obtained analysis reflects the

word construction in natural languages, but intu-

itively, the methods that split everything into let-

ters or pre-specified letter n-grams, or leave the

word forms unanalyzed, would not be very in-

teresting solutions. An interesting thing that can

be evaluated, however, is how close the obtained

analysis is to the linguistic gold standard mor-

phemes that can be obtained from CELEX or

various language-dependent rule-based analyzers.

The exact definition of the morphemes, tags, or

features available in the gold standard to be uti-

lized in the comparison should be decided and

fixed for each language separately.

To verify that a proposed algorithm works in

various languages would, ideally, require running

the evaluations on a large number of languages

that would be somehow representative of various

important language families. However, the re-

sources available for both computing and evalu-

ating the analysis in various applications and lan-

guages are limited. The suggested and applicable

compromise is to select morphologically rich lan-

guages where the morpheme analysis is most use-

ful and those languages where interesting state-of-

the-art evaluation tasks are available. By including

German, Turkish, Finnish and Arabic, many inter-

esting aspects of concatenative morphology have

already been covered.

While the comparison against the linguistic

gold standard morphemes is an interesting sub-

goal, the main interest in running the Morpho

Challenge is to find out how useful the proposed

morpheme analyses are for various practical NLP

applications. Naturally, this is best evaluated

by performing evaluations in several state-of-the-

art application tasks. Due to the limitations of

the resources, the applications have been selected

based on the importance of the morpheme analy-

sis for the application, on the availability of open

state-of-the-art evaluation tasks, and on the effort

needed to run the actual evaluations.

2.2 Unsupervised and semi-supervised

learning

Unsupervised learning is the task of learning with-

out labeled data. In the context of morphology dis-

covery, it means learning without knowing where

morpheme borders are, or which morphemes exist

in which words. Unsupervised learning methods

have many attractive features for morphological

modeling, such as language-independence, inde-

pendence of any particular linguistic theory, and

easy portability to a new language.

Semi-supervised learning can be approached

from two research directions, namely unsuper-

vised and supervised learning. In an essentially

unsupervised learning task there may exist some

labeled (classified) data, or some known links be-

tween data items, which might be utilized by the

(typically generative) learning algorithms. Turned

around, an essentially supervised learning task,

such as classification or prediction, may benefit

also from unlabeled data which is typically more

abundantly available.

In morphology modeling one might consider

the former setup to be the case: the learning task

is essentially that of unsupervised modeling, and

morpheme labels can be thought of as known links

88



between various inflected word forms.

Until 2010 the Morpho Challenge has been de-

fined only as an unsupervised learning task. How-

ever, since small samples of morphologically la-

beled data can be provided already for quite many

languages, also the semi-supervised learning task

has become of interest.

Moreover, while there exists a fair amount of

research and now even books on semi-supervised

learning (Zhu, 2005; Abney, 2007; Zhu, 2010),

it has not been as widely studied for structured

classification problems like sequence segmenta-

tion and labeling (cf. e.g. (Jiao et al., 2006)). The

semi-supervised learning challenge introduced for

Morpho Challenge 2010 can thus be viewed as an

opportunity to strengthen research in both mor-

phology modeling as well as in semi-supervised

learning for sequence segmentation and labeling

in general.

3 Review of Morpho Challenge

competitions so far

3.1 Evaluation tasks, metrics, and languages

The evaluation tasks and languages selected for

Morpho Challenge evaluations are shown in Fig-

ure 1. The languages where evaluations have been

prepared are Finnish (FIN), Turkish (TUR), En-

glish (ENG), German (GER), and Arabic (ARA).

First the morphemes are compared to linguis-

tic gold standards in direct morpheme segmen-

tation (2005) and full morpheme analysis (since

2007). The practical NLP application based eval-

uations are automatic speech recognition (ASR),

information retrieval (IR) and statistical machine

translation (SMT). Morphemes obtained by semi-

supervised learning can be evaluated in parallel

with the unsupervised morphemes. For IR, eval-

uation has also been extended for full sentences,

where the morpheme analysis can based on con-

text. The various suggested and tested evaluations

are defined in this section.

year new languages new tasks

2005 FIN, TUR, ENG segmentation, ASR

2007 GER full analysis, IR

2008 ARA context IR

2009 - SMT

2010 - semi-supervised

Table 1: The evolution of the evaluations. The

acronyms are explained in section 3.1.

3.1.1 Comparisons to linguistic gold standard

The first Morpho Challenge in 2005 (Kurimo et

al., 2006) considered unsupervised segmentation

of words into morphemes. The evaluation was

based on comparing the segmentation boundaries

given by the competitor’s algorithm to the bound-

aries obtained from a gold standard analysis.

From 2007 onwards, the task was changed to

full morpheme analysis, that is, the algorithm

should not only locate the surface forms (i.e., word

segments) of the morphemes, but find also which

surface forms are realizations (allomorphs) of the

same underlying morpheme. This generalizes the

task for finding more meaningful units than just

the realizations of morphemes that may be just in-

dividual letters or even empty strings. In applica-

tions this is useful when it is important to identify

which units carry the same meaning even if they

have different realizations in different words.

As an unsupervised algorithm cannot find the

morpheme labels that would equal to the labels in

the gold standard, the evaluation has to be based

on what word forms share the same morphemes.

The evaluation procedure samples a large num-

ber of word pairs, such that both words in the

pair have at least one morpheme in common, from

both the proposed analysis and the gold standard.

The first version of the method was applied in

2007 (Kurimo et al., 2008) and 2008 (Kurimo et

al., 2009a), and minor modifications were done in

2009 (Kurimo et al., 2009b). However, the orga-

nizers have reported the evaluation results of the

2007 and 2008 submissions also with the new ver-

sion, thus allowing a direct comparison between

them. A summary of these results for English,

Finnish, German and Turkish for the best algo-

rithms is presented in Table 2. The evaluations

in 2008 and 2009 were also performed on Arabic,

but these results and not comparable, because the

database and the gold standard was changed be-

tween the years. The exact annual results for all

participants as well as the details of the evaluation

in each year can be reviewed in the annual evalu-

ation reports (Kurimo et al., 2006; Kurimo et al.,

2008; Kurimo et al., 2009a; Kurimo et al., 2009b).

Already the linguistic evaluation of Morpho

Challenge 2005 applied some principles that have

been used thereafter: (1) The evaluation is based

on a subset of the word forms given as training

data. This not only makes the evaluation proce-

dure lighter, but also allows changing the set when

89



English Finnish

Method P R F Method P R F

2009 2009

Allomorfessor 68.98 56.82 62.31 Monson PMU 47.89 50.98 49.39

Monson PMU 55.68 62.33 58.82 Monson PMM 51.75 45.42 48.38

Lignos 83.49 45.00 58.48 Spiegler PROMODES C 41.20 48.22 44.44

2008 2008

Monson P+M 69.59 65.57 67.52 Monson P+M 65.21 50.43 56.87

Monson ParaMor 63.32 51.96 57.08 Monson ParaMor 49.97 37.64 42.93

Zeman 1 67.13 46.67 55.06 Monson Morfessor 79.76 24.95 38.02

2007 2007

Monson P+M 70.09 67.38 68.71 Bernhard 2 63.92 44.48 52.45

Bernhard 2 67.42 65.11 66.24 Bernhard 1 78.11 29.39 42.71

Bernhard 1 75.61 57.87 65.56 Bordag 5a 72.45 27.21 39.56

German Turkish

Method P R F Method P R F

2009 2009

Monson PMU 52.53 60.27 56.14 Monson PMM 48.07 60.39 53.53

Monson PMM 51.07 57.79 54.22 Monson PMU 47.25 60.01 52.88

Monson PM 50.81 47.68 49.20 Monson PM 49.54 54.77 52.02

2008 2008

Monson P+M 64.06 61.52 62.76 Monson P+M 66.78 57.97 62.07

Monson Morfessor 70.73 38.82 50.13 Monson ParaMor 57.35 45.75 50.90

Monson ParaMor 56.98 42.10 48.42 Monson Morfessor 77.36 33.47 46.73

2007 2007

Monson P+M 69.96 55.42 61.85 Bordag 5a 81.06 23.51 36.45

Bernhard 2 54.02 60.77 57.20 Bordag 5 81.19 23.44 36.38

Bernhard 1 66.82 42.48 51.94 Zeman 77.48 22.71 35.13

Table 2: The summary of the best three submitted methods for years 2009, 2008 and 2007 using the

linguistic evaluation of Morpho Challenge 2009. The complete results tables by the organizers are avail-

able from http://www.cis.hut.fi/morphochallenge2009/. The three columns numbers

are precision (P), recall (R), and F-measure (F). The best F-measure for each language is in boldface,

and the best result that is not based on a direct combination of two other methods is underlined.

the old one is considered to be “overlearned”. (2)

The frequency of the word form plays no role in

evaluation; rare and common forms are equally

likely to be selected, and have equal weight to

the score. (3) The evaluation score is balanced F-

measure, the harmonic mean of precision and re-

call. Precision measures how many of the choices

made by the algorithm are matched in gold stan-

dard; recall measures how many of the choices

in the gold standard are matched in the proposed

analysis. (4) If the linguistic gold standard has

several alternative analysis for one word, for full

precision, it is enough that one of the alternatives

is equivalent to the proposed analysis. The same

holds the other way around for recall.

All of the principles can be also criticized. For

example, evaluation based on the full set would

provide more trustworthy estimates, and common

word forms are more significant in any practical

application. However, the third and the fourth

principle have problems that can be considered to

be more serious.

Balanced F-measure favors methods that are

able to get near-to-equal precision and recall. As

many algorithms can be tuned to give either more

or less morphemes per word than in the default

case, this encourages using developments sets to

optimize the respective parameters. The winning

methods in Challenge 2009—Monson’s ParaMor-

Morfessor Union (PMU) and ParaMor-Morfessor

90



Mimic (PMM) (Monson et al., 2009), and Al-

lomorfessor (Virpioja and Kohonen, 2009)—did

this, more or less explicitly.1 Moreover, it can

be argued that the precision would be more im-

portant than recall in many applications, or, more

generally, that the optimal balance between preci-

sion and recall is application dependent. We see

two solutions for this: Either the optimization for

F-measure should be allowed with a public devel-

opment set, which means moving towards semi-

supervised direction, or precision-recall curves

should be compared, which means more complex

evaluations.

The fourth principle causes problems, if the

evaluated algorithms are allowed to have alterna-

tive analyses for each word. If several alternative

analyses are provided, the obtained precision is

about the average over the individual analyses, but

the recall is based on the best of the alternatives.

This property have been exploited in Challenges

2007 and 2008 by combining the results of two

algorithms as alternative analyses. The method,

Monson’s ParaMor+Morfessor (P+M) holds still

the best position measured in F-measures in all

languages. Combining even better-performing

methods in a similar manner would increase the

scores further. To fix this problem, either the eval-

uation metric should require matching number of

alternative analyses to get the full points, or the

symmetry of the precision and recall measures has

to be removed.

Excluding the methods that combine the anal-

yses of two other methods as alternative ones, we

see that the best F-measure (underlined in Table 2)

is held by Monson’s ParaMor-Morfessor Mimic

from 2009 (Monson et al., 2009) in Turkish and

Bernhard’s method 2 from 2007 (Bernhard, 2006)

in all the other three languages. This means that

except for Turkish, there is no improvement in the

results over the three years. Furthermore, both

of the methods are based purely on segmentation,

and so are all the other top methods presented

in Table 2 except for Bordag’s methods (Bordag,

2006) and Allomorfessor (Virpioja and Kohonen,

2009).

3.1.2 Speech recognition

A key factor in the success of large-vocabulary

continuous speech recognition is the system’s abil-

1Allomorfessor was trained with a pruned data to obtain
a higher recall, whereas ParaMor-Morfessor is explicitly op-
timized for F-measure with a separate Hungarian data set.

ity to limit the search space using a statistical lan-

guage model. The language model provides the

probability of different recognition hypothesis by

using a model of the co-occurence of its words

and morphemes. A properly smoothed n-gram is

the most conventional model. The n-gram should

consist of modeling units that are suitable for the

language, typically words or morphemes.

In Morpho Challenge state-of-the-art large-

vocabulary speech recognizers have been built for

evaluations in Finnish and Turkish (Kurimo et al.,

2006). The various morpheme analysis algorithms

have been compared by measuring the recogni-

tion accuracy with different language models each

trained and optimized based on units from one of

the algorithms. The best results were quite near

to each other, but Bernhard (Bernhard, 2006) and

Morfessor Categories MAP were at the top for

both languages.

3.1.3 Information retrieval

In the information retrieval task, the algorithms

were tested by using the morpheme segmentations

for text retrieval. To return all relevant documents,

it is important to match the words in the queries to

the words in the documents irrespective of which

word forms are used. Typically, a stemming al-

gorithm or a morphological analyzer is used to re-

duce the inflected forms to their stem or base form.

The problem with these methods is that specific

rules need to be crafted for each language. How-

ever, these approaches were also tested for com-

parison purposes. The IR experiments were car-

ried out by replacing the words in the corpora and

queries by the suggested morpheme segmenta-

tions. Test corpora, queries and relevance assess-

ments were provided by Cross-Language Evalua-

tion Forum (CLEF) (Agirre et al., 2008).

To test the effect of the morpheme segmen-

tation, the number of other variables will have

to be minimized, which poses some challenges.

For example, the term weighting method will af-

fect the results and different morpheme analyz-

ers may perform optimally with different weight-

ing approaches. TFIDF and Okapi BM25 term

weighting methods have been tested. In the 2007

Challenge, it was noted that Okapi BM25 suffers

greatly if the corpus contains a lot of frequent

terms. These terms are often introduced when the

algorithms segment suffixes from stems. To over-

come this problem, a method for automatically

generating stop lists of frequent terms was intro-

91



duced. Any term that occurs more times in the cor-

pus than a certain threshold is added to the stop list

and excluded from indexing. The method is quite

simple, but it treats all morpheme analysis meth-

ods equally as it does not require the algorithm

to tag which morphemes are stems and which are

suffixes. The generated stoplists are also reason-

able sized and the results are robust with respect

to the stop list cutoff parameter. With a stop list,

Okapi BM25 clearly outperformed TFIDF rank-

ing method for all algorithms. However, the prob-

lem of choosing the term weighting approach that

treats all algorithms in an optimal way remains

open.

Another challenge is analyzing the results as it

is hard to achieve statistically significant results

with the limited number of queries (50-60) that

were available. In fact, in each language 11-17 of

the best algorithms belonged to the “top group”,

that is, had no statistically different result to the

top performer of the language. To improve the

significance of the results, the number of queries

should be increased. This is a known problem in

the field of IR. However, it is important to test the

methods in a real life application and if an algo-

rithm gives good results across languages, there is

evidence that it is doing something useful.

Some conclusions can be drawn from the re-

sults. The language specific reference methods

(Porter stemming for English, two-layer morpho-

logical analysis for Finnish and German) give the

best results, but the best unsupervised algorithms

are almost at par and the differences are not signif-

icant. For German and Finnish, the best unsuper-

vised methods can also beat in a statistically sig-

nificant way the baseline of not doing any segmen-

tation or stemming. The best algorithms that per-

formed well across languages are ParaMor (Mon-

son et al., 2008), Bernhard (Bernhard, 2006), Mor-

fessor Baseline, andMcNamee (McNamee, 2008).

Comparing the results to the linguistic evalua-

tion (section 3.1.1), it seems that methods that per-

form well at the IR task tend to have good preci-

sion in the linguistic task, with exceptions. Thus,

in the IR task it seems important not to overseg-

ment words. One exception is the method (Mc-

Namee, 2008) which simply splits the words into

equal length letter n-grams. The method gives sur-

prisingly good results in the IR task, given the sim-

plicity, but suffers from low precision in the lin-

guistic task.

3.1.4 Machine translation

In phrase-based statistical machine translation

process there are two stages where morpheme

analysis and segmentation of the words into mean-

ingful sub-word units is needed. The first stage

is the alignment of the parallel sentences in the

source and target language for training the transla-

tion model. The second one is training a statistical

language model for the production of fluent sen-

tences in a morphologically rich target language.

In the machine translation tasks used in the

Morpho Challenge, the focus has so far been in

the alignment problem. In the evaluation tasks in-

troduced in 2009 the language-pairs were Finnish-

English and German-English. To obtain state-of-

the-art results, the evaluation consists of minimum

Bayes risk (MBR) combination of two transla-

tion systems trained on the same data, one us-

ing words and the other morphemes as the ba-

sic modeling units (de Gispert et al., 2009). The

various morpheme analysis algorithms are com-

pared by measuring the translation performance

for different two-model combinations where the

word-based model is always the same, but the

morpheme-based model is trained based on units

from each of the algorithms in turns.

Because the machine translation evaluation has

yet been tried only in 2009, it is difficult to draw

conclusions about the results yet. However, the

Morfessor Baseline algorithm seems to be partic-

ularly difficult to beat both in Finnish-German and

German-English task. The differences between

the best results are small, but the ranking in both

tasks was the same: 1. Morfessor Baseline, 2. Al-

lomorfessor, 3. The linguistic gold standard mor-

phemes (Kurimo et al., 2009b).

3.2 Evaluated algorithms

This section attempts to describe very briefly some

of the individual morpheme analysis algorithms

that have been most successful in the evaluations.

Morfessor Baseline (Creutz and Lagus, 2002):

This is a public baseline algorithm based on jointly

minimizing the size of the morph codebook and

the encoded size of the all the word forms using

the minimum description length MDL cost func-

tion. The performance is above average for all

evaluated tasks in most languages.

Allomorfessor (Kohonen et al., 2009; Virpi-

oja and Kohonen, 2009): The development of

this method was based on the observation that the

92



Finnish German English
0.25

0.3

0.35

0.4

0.45

0.5

0.55

 

 

Morfessor baseline

2007 Bernhard

2008 McNamee 4−gram

2008 Monson P+M

2009 Monson PMU

2009 Lignos

2009 Allomorfessor

Figure 1: Mean Average Precision (MAP) values for some of the best algorithms over the years in the IR

task. The upper horizontal line shows the “goal level” for each language, i.e. the performance of the best

language specific reference method. The lower line shows the baseline reference of doing no stemming

or analysis.

morph level surface forms of one morpheme are

often very similar and the differences occur close

to the morpheme boundary. Thus, the allomor-

phemes could be modeled by simple mutations.

It has been implemented on top of the Morfessor

Baseline using maximum a posteriori (MAP) opti-

mization. This model slightly improves the perfor-

mance in the linguistic evaluation in all languages

(Kurimo et al., 2009b), but in IR and SMT there is

no improvement yet.

Morfessor Categories MAP (Creutz and La-

gus, 2005): In this method hidden Markov models

are used to incorporate morphotactic categories for

theMorfessor Baseline. The structure is optimized

by MAP and yields slight improvements in the lin-

guistic evaluation for most languages, but not for

IR or SMT tasks.

Bernhard (Bernhard, 2006): This has been one

of the best performing algorithms in Finnish, En-

glish and German linguistic evaluation and in IR

(Kurimo et al., 2008). First a list of the most likely

prefixes and suffixes is extracted and alternative

segmentations are generated for the word forms.

Then the best ones are selected based on cost func-

tions that favour most frequent analysis and some

basic morphotactics.

Bordag (Bordag, 2006): This method applies

iterative LSV and clustering of morphs into mor-

phemes. The performance in the linguistic eval-

uation is quite well for Turkish and decent for

Finnish (Kurimo et al., 2008).

ParaMor (Monson et al., 2008): This method

applies an unsupervised model for inflection rules

and suffixation for the stems by building linguisti-

cally motivated paradigms. It has obtained one of

the top performances for all languages when com-

bined with the Morfessor Baseline (Kurimo et al.,

2009a). Various combination methods have been

tested: union, weighted probabilistic average and

proposing both the analyses (Monson et al., 2009).

Lignos (Lignos et al., 2009): This method is

based on the observation that the derivation of

the inflected forms can be modeled as transfor-

mations. The best transformations can be found

by optimizing the simplicity and frequency. This

method performs much better in English than in

the other languages (Kurimo et al., 2009b).

Promodes (Spiegler et al., 2009): This method

presents a probabilistic generative model that ap-

plies LSV and combines multiple analysis using a

committee. It seems to generate a large amount

of short morphemes, which is difficult for many

of the practical applications. However, it obtained

the best performance for the linguistic evaluation

in Arabic 2009 (Kurimo et al., 2009b), but did not

survive as well in other languages, and particularly

not in the IR application.

4 Open questions and challenges

Although more than 50 algorithms have already

been tested in the Morpho Challenge evaluations

and many lessons have been learned from the re-

sults and discussions, many challenges are still

open and untouched. In fact, the attempts to solve

the problem have perhaps produced even more

open questions than there were in the beginning.

93



The main new and open challenges are described

in this section.

What is the best analysis algorithm? Some

of the suggested algorithms have produced good

test results and some even in several tasks and lan-

guages, such as Bernhard (Bernhard, 2006), Mon-

son ParaMor+Morfessor (Monson et al., 2008)

and Allomorfessor (Virpioja and Kohonen, 2009).

However, none of the methods perform really well

in all the evaluation tasks and languages and their

mutual performance differences are often rather

small, even though the morphemes and the al-

gorithmic principles are totally different. Thus,

no dominant morpheme analysis algorithm have

been found. Furthermore, reaching the perfor-

mance level that rivals, or even sometimes domi-

nates, the rule-based and language-dependent ref-

erence methods does not mean that the solutions

are sufficient. Often the limited coverage or un-

suitable level of details in the analysis for the task

in the reference methods just indicates that they

are not sufficient either and better solutions are

needed. Another observation which complicates

the finding and determination of the best algorithm

is that in some tasks, such as statistical language

models for speech recognition, very different al-

gorithms can reach the same performance, because

advanced modelling methods can compensate for

unsuitable morpheme analysis.

What is the meaning of the morphemes? In

some of the fundamental applications of mor-

pheme analysis, such as text understanding, mor-

pheme segmentation alone is only part of the solu-

tion. Even more important is to find the meaning

for the obtained morphemes. The extension of the

segmentation of words into smaller units to iden-

tification of the units that correspond to the same

morpheme is a step taken to this direction, but the

question of the meaning of the morpheme is still

open. However, in the unsupervised way of learn-

ing, solutions to this may be so tightly tied to the

applications that much more complex evaluations

would be needed.

How to evaluate the alternative analyses? It

is clear that when a word form is separated from

the sentence context where it was used, the mor-

pheme analysis easily becomes ambiguous. In the

Morpho Challenge evaluations this has been taken

into account by allowing multiple alternative anal-

yses. However, in some evaluations, for exam-

ple, in the measurement of the recall of the gold

standard morphemes, this leads to unwanted re-

sults and may favour methods that always provide

a large number of alternative analysis.

How to improve the analysis using context?

A natural way to disambiguate the analysis in-

volves taking the sentence context into account.

Some of the Morpho Challenge evaluations, for

example, the information retrieval, allow this op-

tion when the source texts and queries are given.

However, this has not been widely tried yet by

the participants, probably because of the increased

computational complexity of the modelling task.

How to effectively apply semi-supervised

learning? In semi-supervised learning, a small set

of labeled data in the form of gold standard anal-

ysis for the word forms are provided. This data

can be used for improving the unsupervised solu-

tions based on unlabeled data in several ways: (1)

The labeled data is used for tuning some learning

parameters, followed by an unsupervised learning

process for the unlabeled data. (2) The labeled

morphemes are used as an ideal starting point

to bootstrap the learning on the unlabeled words

(self-training). (3) Using the EM algorithm for es-

timating a generative model, the unlabeled cases

can be treated as missing data.

The best and most practical way of using the

partly labeled data will be determined in future

when the semi-supervised task has been evaluated

in the future Morpho Challenge evaluations. For

the first time this task will be evaluated in the on-

going Morpho Challenge 2010.

Acknowledgments

We are grateful to the University of Leipzig,

University of Leeds, Computational Linguistics

Group at University of Haifa, Stefan Bordag,

Ebru Arisoy, Nizar Habash, Majdi Sawalha, Eric

Atwell, and Mathias Creutz for making the data

and gold standards in various languages available

to the Challenge. This work was supported by the

Academy of Finland in the project Adaptive In-

formatics, the graduate schools in Language Tech-

nology and Computational Methods of Informa-

tion Technology, in part by the GALE program of

the Defense Advanced Research Projects Agency,

Contract No. HR0011-06-C-0022, and in part by

the IST Programme of the European Community,

under the FP7 project EMIME (213845) and PAS-

CAL Network of Excellence.

94



References

Steven Abney. 2007. Semisupervised Learning
for Computational Linguistics. Chapman and
Hall/CRC.

Eneko Agirre, Giorgio M. Di Nunzio, Nicola Ferro,
Thomas Mandl, and Carol Peters. 2008. CLEF
2008: Ad hoc track overview. In Working Notes for
the CLEF 2008 Workshop.

Delphine Bernhard. 2006. Unsupervised morpholog-
ical segmentation based on segment predictability
and word segments alignment. In Proc. PASCAL
Challenge Workshop on Unsupervised segmentation
of words into morphemes, Venice, Italy. PASCAL
European Network of Excellence.

Stefan Bordag. 2006. Two-step approach to unsuper-
vised morpheme segmentation. In Proc. of the PAS-
CAL Challenge Workshop on Unsupervised segmen-
tation of words into morphemes, Venice, Italy. PAS-
CAL European Network of Excellence.

Mathias Creutz and Krista Lagus. 2002. Unsu-
pervised discovery of morphemes. In Proc. SIG-
PHON/ACL’02, pages 21–30.

Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proc. AKRR’05, pages 106–
113.

Adria de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum bayes risk
combination of translation hypothesis from alter-
native morphological decompositions. In Proc.
NAACL’09, pages 73-76.

C. G. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT.

Zellig S. Harris. 1955. From phoneme to morpheme.
Language, 31(2):190–222. Reprinted 1970 in Pa-
pers in Structural and Transformational Linguistics,
Reidel Publishing Company, Dordrecht, Holland.

Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proc.
ACL’06, pages 209–216.

Oskar Kohonen, Sami Virpioja, and Mikaela Klami.
2009. Allomorfessor: Towards unsupervised mor-
pheme analysis. In Evaluating systems for Mul-
tilingual and MultiModal Information Access, 9th
Workshop of the Cross-Language Evaluation Forum,
CLEF 2008, Revised Selected Papers, Lecture Notes
in Computer Science , Vol. 5706. Springer.

Mikko Kurimo, Mathias Creutz, and Krista Lagus.
2006. Unsupervised segmentation of words into
morphemes - challenge 2005, an introduction and
evaluation report. In Proc. PASCAL Challenge
Workshop on Unsupervised segmentation of words

into morphemes, Venice, Italy. PASCAL European
Network of Excellence.

Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.
2008. Morpho Challenge evaluation using a linguis-
tic Gold Standard. In Advances in Multilingual and
MultiModal Information Retrieval, 8th Workshop of
the Cross-Language Evaluation Forum, CLEF 2007,
Revised Selected Papers, Lecture Notes in Computer
Science , Vol. 5152, pages 864–873. Springer.

Mikko Kurimo, Ville Turunen, and Matti Varjokallio.
2009a. Overview of Morpho Challenge 2008.
In Evaluating systems for Multilingual and Mul-
tiModal Information Access, 9th Workshop of the
Cross-Language Evaluation Forum, CLEF 2008,
Revised Selected Papers, Lecture Notes in Computer
Science , Vol. 5706. Springer.

Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009b.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece.

Constantine Lignos, Erwin Chan, Mitchell P. Marcus,
and Charles Yang. 2009. A rule-based unsupervised
morphology learning framework. In Working Notes
for the CLEF 2009 Workshop, Corfu, Greece.

Paul McNamee. 2008. Retrieval experiments at mor-
pho challenge 2008. InWorking Notes for the CLEF
2008 Workshop, Aarhus, Denmark, September.

Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. ParaMor: Finding paradigms
across morphology. In Advances in Multilingual
and MultiModal Information Retrieval, 8th Work-
shop of the Cross-Language Evaluation Forum,
CLEF 2007, Revised Selected Papers, Lecture Notes
in Computer Science , Vol. 5152. Springer.

Christian Monson, Kristy Hollingshead, and Brian
Roard. 2009. Probabilistic paraMor. In Working
Notes for the CLEF 2009 Workshop, Corfu, Greece,
September.

Sebastian Spiegler, Bruno Golenia, and Peter Flach.
2009. PROMODES: A probabilistic generative
model for word decomposition. In Working Notes
for the CLEF 2009 Workshop, Corfu, Greece,
September.

Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme discovery with Allomorfessor. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.

Xiaojin Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.

Xiaojin Zhu. 2010. Semi-supervised learning. In En-
cyclopedia of Machine Learning. To appear.

95


