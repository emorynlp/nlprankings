










































Online Learning for Interactive Statistical Machine Translation


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 546–554,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Online Learning for Interactive Statistical Machine Translation

Daniel Ortiz-Martı́nez

Dpto. de Sist. Inf. y Comp.

Univ. Politéc. de Valencia

46071 Valencia, Spain

dortiz@dsic.upv.es

Ismael Garcı́a-Varea

Dpto. de Informática

Univ. de Castilla-La Mancha

02071 Albacete, Spain

ivarea@info-ab.uclm.es

Francisco Casacuberta

Dpto. de Sist. Inf. y Comp.

Univ. Politéc. de Valencia

46071 Valencia, Spain

fcn@dsic.upv.es

Abstract

State-of-the-art Machine Translation (MT)

systems are still far from being perfect. An al-

ternative is the so-called Interactive Machine

Translation (IMT) framework. In this frame-

work, the knowledge of a human translator is

combined with a MT system. The vast ma-

jority of the existing work on IMT makes use

of the well-known batch learning paradigm.

In the batch learning paradigm, the training of

the IMT system and the interactive translation

process are carried out in separate stages. This

paradigm is not able to take advantage of the

new knowledge produced by the user of the

IMT system. In this paper, we present an ap-

plication of the online learning paradigm to

the IMT framework. In the online learning

paradigm, the training and prediction stages

are no longer separated. This feature is par-

ticularly useful in IMT since it allows the user

feedback to be taken into account. The online

learning techniques proposed here incremen-

tally update the statistical models involved in

the translation process. Empirical results show

the great potential of online learning in the

IMT framework.

1 Introduction

Information technology advances have led to the

need for more efficient translation methods. Current

MT systems are not able to produce ready-to-use

texts. Indeed, MT systems usually require human

post-editing to achieve high-quality translations.

One way of taking advantage of MT systems is to

combine them with the knowledge of a human trans-

lator in the IMT paradigm, which is a special type of

the computer-assisted translation paradigm (Isabelle

and Church, 1997). An important contribution to

IMT technology was pioneered by the TransType

project (Foster et al., 1997; Langlais et al., 2002)

where data driven MT techniques were adapted for

their use in an interactive translation environment.

Following the TransType ideas, Barrachina et

al. (2009) proposed a new approach to IMT, in which

fully-fledged statistical MT (SMT) systems are used

to produce full target sentence hypotheses, or por-

tions thereof, which can be partially or completely

accepted and amended by a human translator. Each

partial, correct text segment is then used by the

SMT system as additional information to achieve

improved suggestions. Figure 1 illustrates a typical

IMT session.

source(f ): Para ver la lista de recursos

reference(ê): To view a listing of resources

inter.-0
ep

es To view the resources list

inter.-1

ep To view

k a

es list of resources

inter.-2

ep To view a list

k list i

es list i ng resources

inter.-3

ep To view a listing

k o

es o f resources

accept ep To view a listing of resources

Figure 1: IMT session to translate a Spanish sen-

tence into English. In interaction-0, the system sug-

gests a translation (es). In interaction-1, the user

moves the mouse to accept the first eight characters

”To view ” and presses the a key (k), then the sys-

tem suggests completing the sentence with ”list of

resources” (a new es). Interactions 2 and 3 are sim-

ilar. In the final interaction, the user accepts the cur-

rent suggestion.

In this paper, we also focus on the IMT frame-

work. Specifically, we present an IMT system that is

able to learn from user feedback. For this purpose,

we apply the online learning paradigm to the IMT

framework. The online learning techniques that we

propose here allow the statistical models involved in

the translation process to be incrementally updated.

546



Figure 2 (inspired from (Vidal et al., 2007)) shows

a schematic view of these ideas. Here, f is the in-

put sentence and e is the output derived by the IMT

system from f . By observing f and e, the user inter-

acts with the IMT system until the desired output ê is

produced. The input sentence f and its desired trans-

lation ê can be used to refine the models used by the

system. In general, the model is initially obtained

through a classical batch training process from a pre-

viously given training sequence of pairs (fi,ei) from

the task being considered. Now, the models can be

extended with the use of valuable user feedback.

e

f

e

k

f

       Interactive 

     SMT System

 Batch

Learning

 Online

Learning

 . . .

 f  , e
 2    2

 f  , e
 1    1

feedback/interactions

e

f ^

^

Incremental

Models

Figure 2: An Online Interactive SMT system

2 Interactive machine translation

IMT can be seen as an evolution of the SMT frame-

work. Given a sentence f from a source lan-

guage F to be translated into a target sentence e
of a target language E , the fundamental equation of
SMT (Brown et al., 1993) is the following:

ê = argmax
e

{Pr(e | f)} (1)

= argmax
e

{Pr(f | e)Pr(e)} (2)

where Pr(f | e) is approximated by a translation
model that represents the correlation between the

source and the target sentence and where Pr(e) is
approximated by a language model representing the

well-formedness of the candidate translation e.

State-of-the-art statistical machine translation

systems follow a loglinear approach (Och and Ney,

2002), where direct modelling of the posterior prob-

ability Pr(e | f) of Equation (1) is used. In this case,
the decision rule is given by the expression:

ê = argmax
e

{

M
∑

m=1

λmhm(e, f)

}

(3)

where each hm(e, f) is a feature function represent-
ing a statistical model and λm its weight.
Current MT systems are based on the use of

phrase-based models (Koehn et al., 2003) as transla-

tion models. The basic idea of Phrase-based Trans-

lation (PBT) is to segment the source sentence into

phrases, then to translate each source phrase into a

target phrase, and finally to reorder the translated

target phrases in order to compose the target sen-

tence. If we summarize all the decisions made dur-

ing the phrase-based translation process by means of

the hidden variable ãK1 , we obtain the expression:

Pr(f |e) =
∑

K,ãK
1

Pr(f̃K1 , ã
K
1 | ẽ

K
1 ) (4)

where each ãk ∈ {1 . . .K} denotes the index of the
target phrase ẽ that is aligned with the k-th source
phrase f̃k, assuming a segmentation of length K.
According to Equation (4), and following a max-

imum approximation, the problem stated in Equa-

tion (2) can be reframed as:

ê ≈ arg max
e,a

{

p(e) · p(f ,a | e)
}

(5)

In the IMT scenario, we have to find an extension

es for a given prefix ep. To do this we reformulate

Equation (5) as follows:

ês ≈ arg max
es,a

{

p(es | ep) · p(f ,a | ep, es)
}

(6)

where the term p(ep) has been dropped since it does
depend neither on es nor on a.

Thus, the search is restricted to those sentences e

which contain ep as prefix. It is also worth mention-

ing that the similarities between Equation (6) and

Equation (5) (note that epes ≡ e) allow us to use
the same models whenever the search procedures are

adequately modified (Barrachina et al., 2009).

Following the loglinear approach stated in Equa-

tion (3), Equation (6) can be rewriten as:

ês = argmax
es,a

{

M
∑

m=1

λmhm(e,a, f)

}

(7)

547



which is the approach that we follow in this work.

A common problem in IMT arises when the user

sets a prefix (ep) which cannot be found in the

phrase-based statistical translation model. Differ-

ent solutions have been proposed to deal with this

problem. The use of word translation graphs, as a

compact representation of all possible translations

of a source sentence, is proposed in (Barrachina

et al., 2009). In (Ortiz-Martı́nez et al., 2009), a

technique based on the generation of partial phrase-

based alignments is described. This last proposal has

also been adopted in this work.

3 Related work

In this paper we present an application of the online

learning paradigm to the IMT framework. In the on-

line learning setting, models are trained sample by

sample. Our work is also related to model adapta-

tion, although model adaptation and online learning

are not exactly the same thing.

The online learning paradigm has been previ-

ously applied to train discriminative models in

SMT (Liang et al., 2006; Arun and Koehn, 2007;

Watanabe et al., 2007; Chiang et al., 2008). These

works differ from the one presented here in that we

apply online learning techniques to train generative

models instead of discriminative models.

In (Nepveu et al., 2004), dynamic adaptation of

an IMT system via cache-based model extensions to

language and translation models is proposed. The

work by Nepveu et al. (2004) constitutes a domain

adaptation technique and not an online learning

technique, since the proposed cache components re-

quire pre-existent models estimated in batch mode.

In addition to this, their IMT system does not use

state-of-the-art models.

To our knowledge, the only previous work on on-

line learning for IMT is (Cesa-Bianchi et al., 2008),

where a very constrained version of online learn-

ing is presented. This constrained version of online

learning is not able to extend the translation models

due to technical problems with the efficiency of the

learning process. In this paper, we present a purely

statistical IMT systemwhich is able to incrementally

update the parameters of all of the different models

that are used in the system, including the transla-

tion model, breaking with the above mentioned con-

straints. What is more, our system is able to learn

from scratch, that is, without any preexisting model

stored in the system. This is demonstrated empiri-

cally in section 5.

4 Online IMT

In this section we propose an online IMT system.

First, we describe the basic IMT system involved

in the interactive translation process. Then we in-

troduce the required techniques to incrementally up-

date the statistical models used by the system.

4.1 Basic IMT system

The basic IMT system that we propose uses a log-

linear model to generate its translations. According

to Equation (7), we introduce a set of seven feature

functions (from h1 to h7):

• n-gram language model (h1)

h1(e) = log(
∏|e|+1

i=1 p(ei|e
i−1
i−n+1)),

1 where

p(ei|e
i−1
i−n+1) is defined as follows:

p(ei|e
i−1
i−n+1) =

max{cX(e
i
i−n+1)−Dn, 0}

cX(e
i−1
i−n+1)

+

Dn

cX(e
i−1
i−n+1)

N1+(e
i−1
i−n+1•) · p(ei|e

i−1
i−n+2) (8)

where Dn =
cn,1

cn,1+2cn,2
is a fixed discount (cn,1

and cn,2 are the number of n-grams with one
and two counts respectively),N1+(e

i−1
i−n+1•) is the

number of unique words that follows the history

ei−1i−n+1 and cX(e
i
i−n+1) is the count of the n-gram

eii−n+1, where cX(·) can represent true counts
cT (·) or modified counts cM (·). True counts are
used for the higher order n-grams and modified
counts for the lower order n-grams. Given a cer-
tain n-gram, its modified count consists in the
number of different words that precede this n-
gram in the training corpus.

Equation (8) corresponds to the probability given

by an n-gram language model with an interpolated
version of the Kneser-Ney smoothing (Chen and

Goodman, 1996).

1|e| is the length of e, e0 denotes the begin-of-sentence sym-
bol, e|e|+1 denotes the end-of-sentence symbol, e

j
i ≡ ei...ej

548



• target sentence-length model (h2)
h2(e, f) = log(p(|f | | |e|)) = log(φ|e|(|f |+0.5)−
φ|e|(|f | − 0.5)), where φ|e|(·) denotes the cumula-
tive distribution function (cdf) for the normal dis-

tribution (the cdf is used here to integrate the nor-

mal density function over an interval of length 1).
We use a specific normal distribution with mean

µ|e| and standard deviation σ|e| for each possible
target sentence length |e|.

• inverse and direct phrase-based models (h3, h4)
h3(e,a, f) = log(

∏K
k=1 p(f̃k|ẽãk)), where

p(f̃k|ẽãk) is defined as follows:

p(f̃k|ẽãk) = β · pphr(f̃k|ẽãk) +

(1− β).phmm(f̃k|ẽãk) (9)

In Equation (9), pphr(f̃k|ẽãk) denotes the proba-
bility given by a statistical phrase-based dictionary

used in regular phrase-based models (see (Koehn

et al., 2003) for more details). phmm(f̃k|ẽãk) is
the probability given by an HMM-based (intra-

phrase) alignment model (see (Vogel et al., 1996)):

phmm(f̃ |ẽ) = ǫ
∑

a
|f̃ |
1

|f̃ |
∏

j=1

p(f̃j |ẽaj ) · p(aj |aj−1, |ẽ|)

(10)

The HMM-based alignment model probability is

used here for smoothing purposes as described

in (Ortiz-Martı́nez et al., 2009).

Analogously h4 is defined as:
h4(e,a, f) = log(

∏K
k=1 p(ẽãk |f̃k))

• target phrase-length model (h5)
h5(e,a, f) = log(

∏K
k=1 p(|ẽk|)), where p(|ẽk|) =

δ(1− δ)|ẽk|. h5 implements a target phrase-length
model by means of a geometric distribution with

probability of success on each trial δ. The use of a
geometric distribution penalizes the length of tar-

get phrases.

• source phrase-length model (h6)
h6(e,a, f) = log(

∏K
k=1 p(|f̃k| | |ẽãk |)),

where p(|f̃k| | |ẽãk |) = δ(1− δ)
abs(|f̃k|−|ẽãk |) and

abs(·) is the absolute value function. A geometric
distribution is used to model this feature (it penal-

izes the difference between the source and target

phrase lengths).

• distortion model (h7)
h7(a) = log(

∏K
k=1 p(ãk|ãk−1)), where

p(ãk|ãk−1) = δ(1 − δ)
abs(bãk−lãk−1 ), bãk

denotes the beginning position of the source

phrase covered by ãk and lãk−1 denotes the last
position of the source phrase covered by ãk−1.
A geometric distribution is used to model this

feature (it penalizes the reorderings).

The log-linear model, which includes the above

described feature functions, is used to generate the

suffix es given the user-validated prefix ep. Specif-

ically, the IMT system generates a partial phrase-

based alignment between the user prefix ep and a

portion of the source sentence f , and returns the suf-

fix es as the translation of the remaining portion of

f (see (Ortiz-Martı́nez et al., 2009)).

4.2 Extending the IMT system from user

feedback

After translating a source sentence f , a new sen-

tence pair (f , e) is available to feed the IMT system
(see Figure 1). In this section we describe how the

log-linear model described in section 4.1 is updated

given the new sentence pair. To do this, a set of suf-

ficient statistics that can be incrementally updated is

maintained for each feature function hi(·). A suffi-
cient statistic for a statistical model is a statistic that

captures all the information that is relevant to esti-

mate this model.

Regarding feature function h1 and according to
equation (8), we need to maintain the following data:

ck,1 and ck,2 given any order k, N1+(·), and cX(·)
(see section 4.1 for the meaning of each symbol).

Given a new sentence e, and for each k-gram eii−k+1
of e where 1 ≤ k ≤ n and 1 ≤ i ≤ |e|+1, we mod-
ify the set of sufficient statistics as it is shown in Al-

gorithm 1. The algorithm checks the changes in the

counts of the k-grams to update the set of sufficient
statistics. Sufficient statistics forDk are updated fol-
lowing the auxiliar procedure shown in Algorithm 2.

Feature function h2 requires the incremental cal-
culation of the mean µ|e| and the standard deviation
σ|e| of the normal distribution associated to a target
sentence length |e|. For this purpose the procedure
described in (Knuth, 1981) can be used. In this pro-

cedure, two quantities are maintained for each nor-

mal distribution: µ|e| and S|e|. Given a new sentence

549



input : n (higher order), eii−k+1 (k-gram),
S = {∀j(cj,1, cj,2), N1+(·), cX(·)}
(current set of sufficient statistics)

output : S (updated set of sufficient statistics)
begin

if cT (e
i
i−k+1) = 0 then

if k − 1 ≥ 1 then

updD(S,k-1,cM (e
i−1
i−k+2),cM (e

i−1
i−k+2)+1)

if cM (e
i−1
i−k+2) = 0 then

N1+(e
i−1
i−k+2) = N1+(e

i−1
i−k+2) + 1

cM (e
i−1
i−k+2) = cM (e

i−1
i−k+2) + 1

cM (e
i
i−k+2) = cM (e

i
i−k+2) + 1

if k = n then

N1+(e
i−1
i−k+1) = N1+(e

i−1
i−k+1) + 1

if k = n then
updD(S,k,cT (e

i
i−k+1),cT (e

i
i−k+1) + 1)

cT (e
i−1
i−k+1)=cT (e

i−1
i−k+1) + 1

cT (e
i
i−k+1)=cT (e

i
i−k+1) + 1

end

Algorithm 1: Pseudocode for updating the suf-

ficient statistics of a given k-gram

input : S (current set of sufficient statistics),k
(order), c (current count), c′ (new count)

output : (ck,1, ck,2) (updated sufficient statistics)
begin

if c = 0 then
if c′ = 1 then ck,1 = ck,1 + 1
if c′ = 2 then ck,2 = ck,2 + 1

if c = 1 then
ck,1 = ck,1 − 1
if c′ = 2 then ck,2 = ck,2 + 1

if c = 2 then ck,2 = ck,2 − 1
end

Algorithm 2: Pseudocode for the updD proce-

dure

pair (f , e), the two quantities are updated using a re-
currence relation:

µ|e| = µ
′

|e| + (|f | − µ
′

|e|)/c(|e|) (11)

S|e| = S
′

|e| + (|f | − µ
′

|e|)(|f | − µ|e|) (12)

where c(|e|) is the count of the number of sentences
of length |e| that have been seen so far, and µ

′

|e| and

S
′

|e| are the quantities previously stored (µ|e| is ini-
tialized to the source sentence length of the first sam-

ple and S|e| is initialized to zero). Finally, the stan-

dard deviation can be obtained from S as follows:
σ|e| =

√

S|e|/(c(|e|)− 1).

Feature functions h3 and h4 implement inverse
and direct smoothed phrase-based models respec-

tively. Since phrase-based models are symmetric

models, only an inverse phrase-based model is main-

tained (direct probabilities can be efficiently ob-

tained using appropriate data structures, see (Ortiz-

Martı́nez et al., 2008)). The inverse phrase model

probabilities are estimated from the phrase counts:

p(f̃ |ẽ) =
c(f̃ , ẽ)

∑

f̃ ′
c(f̃ ′, ẽ)

(13)

According to Equation (13), the set of suffi-

cient statistics to be stored for the inverse phrase

model consists of a set of phrase counts (c(f̃ , ẽ) and
∑

f̃ ′
c(f̃ ′, ẽ) must be stored separately). Given a

new sentence pair (f , e), the standard phrase-based
model estimation method uses a word alignment ma-

trix between f and e to extract the set of phrase pairs

that are consistent with the word alignment ma-

trix (see (Koehn et al., 2003) for more details). Once

the consistent phrase pairs have been extracted, the

phrase counts are updated. The word alignment ma-

trices required for the extraction of phrase pairs are

generated by means of the HMM-based models used

in the feature functions h3 and h4.

Inverse and direct HMM-based models are used

here for two purposes: to smooth the phrase-based

models via linear interpolation and to generate word

alignment matrices. The weights of the interpola-

tion can be estimated from a development corpus.

Equation (10) shows the expression of the probabil-

ity given by an inverse HMM-based model. The

probability includes lexical probabilities p(fj |ei)
and alignment probabilities p(aj |aj−1, l). Since the
alignment in the HMM-based model is determined

by a hidden variable, the EM algorithm is required

to estimate the parameters of the model (see (Och

and Ney, 2003)). However, the standard EM algo-

rithm is not appropriate to incrementally extend our

HMM-based models because it is designed to work

in batch training scenarios. To solve this problem,

we apply the incremental view of the EM algorithm

described in (Neal and Hinton, 1998). According

to (Och and Ney, 2003), the lexical probability for a

550



pair of words is given by the expression:

p(f |e) =
c(f |e)

∑

f ′ c(f
′|e)

(14)

where c(f |e) is the expected number of times that
the word e is aligned to the word f . The alignment
probability is defined in a similar way:

p(aj |aj−1, l) =
c(aj |aj−1, l)

∑

a′j
c(a′j |aj−1, l)

(15)

where c(aj |aj−1, l) denotes the expected number of
times that the alignment aj has been seen after the
previous alignment aj−1 given a source sentence
composed of l words.
Given the equations (14) and (15), the set of suf-

ficient statistics for the inverse HMM-based model

consists of a set of expected counts (numerator and

denominator values are stored separately). Given a

new sentence pair (f , e), we execute a new iteration
of the incremental EM algorithm on the new sample

and collect the contributions to the expected counts.

The parameters of the direct HMM-based model

are estimated analogously to those of the inverse

HMM-based model. Once the direct and the inverse

HMM-based model parameters have been modified

due to the presentation of a new sentence pair to the

IMT system, both models are used to obtain word

alignments for the new sentence pair. The resulting

direct and inverse word alignment matrices are com-

bined by means of the symmetrization alignment op-

eration (Och and Ney, 2003) before extracting the

set of consistent phrase pairs.

HMM-based alignment models are used here

because, according to (Och and Ney, 2003)

and (Toutanova et al., 2002), they outperform IBM 1

to IBM 4 alignment models while still allowing the

exact calculation of the likelihood for a given sen-

tence pair.

The δ parameters of the geometric distributions
associated to the feature functions h5, h6 and h7 are
left fixed. Because of this, there are no sufficient

statistics to store for these feature functions.

Finally, the weights of the log-linear combination

are not modified due to the presentation of a new

sentence pair to the system. These weights can be

adjusted off-line by means of a development corpus

and well-known optimization techniques.

5 Experiments

This section describes the experiments that we car-

ried out to test our online IMT system.

5.1 Experimental setup

The experiments were performed using the XE-

ROX XRCE corpus (SchlumbergerSema S.A. et

al., 2001), which consists of translations of Xe-

rox printer manuals involving three different pairs

of languages: French-English, Spanish-English, and

German-English. The main features of these cor-

pora are shown in Table 1. Partitions into training,

development and test were performed. This corpus

is used here because it has been extensively used in

the literature on IMT to report results.

IMT experiments were carried out from English

to the other three languages.

5.2 Assessment criteria

The evaluation of the techniques presented in this

paper were carried out using the Key-stroke and

mouse-action ratio (KSMR) measure (Barrachina

et al., 2009). This is calculated as the number of

keystrokes plus the number of mouse movements

plus one more count per sentence (aimed at simulat-

ing the user action needed to accept the final transla-

tion), the sum of which is divided by the total num-

ber of reference characters. In addition to this, we

also used the well-known BLEU score (Papineni et

al., 2001) to measure the translation quality of the

first translation hypothesis produced by the IMT sys-

tem for each source sentence (which is automatically

generated without user intervention).

5.3 Online IMT results

To test the techniques proposed in this work, we

carried out experiments in two different scenarios.

In the first one, the first 10 000 sentences extracted
from the training corpora were interactively trans-

lated by means of an IMT system without any pre-

existent model stored in memory. Each time a new

sentence pair was validated, it was used to incremen-

tally train the system. Figures 3a, 3b and 3c show the

evolution of the KSMR with respect to the number

of sentence pairs processed by the IMT system; the

results correspond to the translation from English to

Spanish, French and German, respectively. In addi-

551



En Sp En Fr En Ge

Train

Sent. pairs 55761 52844 49376

Running words 571960 657172 542762 573170 506877 440682

Vocabulary 25627 29565 24958 27399 24899 37338

Dev.

Sent. pairs 1012 994 964

Running words 12111 13808 9480 9801 9162 8283

Perplexity (3-grams) 46.2 34.0 96.2 74.1 68.4 124.3

Sent. pairs 1125 984 996

Test

Running words 7634 9358 9572 9805 10792 9823

Perplexity (3-grams) 107.0 59.6 192.6 135.4 92.8 169.2

Table 1: XEROX corpus statistics for three different language pairs (from English (En) to Spanish (Sp),

French (Fr) and German (Ge))

tion, for each language pair we interactively trans-

lated the original portion of the training corpus and

the same portion of the original corpus after being

randomly shuffled.

As these figures show, the results clearly demon-

strate that the IMT system is able to learn from

scratch. The results were similar for the three lan-

guages. It is also worthy of note that the obtained

results were better in all cases for the original cor-

pora than for the shuffled ones. This is because,

in the original corpora, similar sentences appear

more or less contiguosly (due to the organization of

the contents of the printer manuals). This circum-

stance increases the accuracy of the online learning,

since with the original corpora the number of lat-

eral effects ocurred between the translation of sim-

ilar sentences is decreased. The online learning of

a new sentence pair produces a lateral effect when

the changes in the probability given by the models

not only affect the newly trained sentence pair but

also other sentence pairs. A lateral effect can cause

that the system generates a wrong translation for a

given source sentence due to undesired changes in

the statistical models.

The accuracy were worse for shuffled corpora,

since shuffling increases the number of lateral ef-

fects that may occur between the translation of sim-

ilar sentences (because they no longer appear con-

tiguously). A good way to compare the quality of

different online IMT systems is to determine their

robustness in relation to sentence ordering. How-

ever, it can generally be expected that the sentences

to be translated in an interactive translation session

will be in a non-random order.

Alternatively, we carried out experiments in a dif-

ferent learning scenario. Specifically, the XEROX

 30

 40

 50

 60

 70

 80

 90

 100

 0  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000
K
S
M
R

#Sentences

original
shuffled

(a) English-Spanish

 40

 50

 60

 70

 80

 90

 100

 0  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000

K
S
M
R

#Sentences

original
shuffled

(b) English-French

 40

 50

 60

 70

 80

 90

 100

 0  1000  2000  3000  4000  5000  6000  7000  8000  9000 10000

K
S
M
R

#Sentences

original
shuffled

(c) English-German

Figure 3: KSMR evolution translating a portion of

the training corpora

test corpora were interactively translated from the

English language to the other three languages, com-

paring the performance of a batch IMT system with

552



that of an online IMT system. The batch IMT sys-

tem is a conventional IMT system which is not able

to take advantage of user feedback after each transla-

tion while the online IMT system uses the new sen-

tence pairs provided by the user to revise the sta-

tistical models. Both systems were initialized with

a log-linear model trained in batch mode by means

of the XEROX training corpora. The weights of the

log-linear combination were adjusted for the devel-

opment corpora by means of the downhill-simplex

algorithm. Table 2 shows the obtained results. The

table shows the BLEU score and the KSMR for the

batch and the online IMT systems (95% confidence
intervals are shown). The BLEU score was calcu-

lated from the first translation hypothesis produced

by the IMT system for each source sentence. The ta-

ble also shows the average online learning time (LT)

for each new sample presented to the system2. All

the improvements obtained with the online IMT sys-

tem were statistically significant. Also, the average

learning times clearly allow the system to be used in

a real-time scenario.

IMT system BLEU KSMR LT (s)

En-Sp
batch 55.1± 2.3 18.2± 1.1 -
online 60.6± 2.3 15.8± 1.0 0.04

En-Fr
batch 33.7± 2.0 33.9± 1.3 -
online 42.2± 2.2 27.9± 1.3 0.09

En-Ge
batch 20.4± 1.8 40.3± 1.2 -
online 28.0± 2.0 35.0± 1.3 0.07

Table 2: BLEU and KSMR results for the XEROX

test corpora using the batch and the online IMT sys-

tems. The average online learning time (LT) in sec-

onds is shown for the online system

Finally, in Table 3 a comparison of the KSMR re-

sults obtained by the online IMT system with state-

of-the-art IMT systems is reported (95% confidence

intervals are shown). We compared our system with

those presented in (Barrachina et al., 2009): the

alignment templates (AT), the stochastic finite-state

transducer (SFST), and the phrase-based (PB) ap-

proaches to IMT. The results were obtained using

the same Xerox training and test sets (see Table 1)

for the four different IMT systems. Our system out-

performed the results obtained by these systems.

2All the experiments were executed on a PC with a 2.40 Ghz

Intel Xeon processor with 1GB of memory.

AT PB SFST Online

En-Sp 23.2±1.3 16.7±1.2 21.8±1.4 15.8± 1.0

En-Fr 40.4±1.4 35.8±1.3 43.8±1.6 27.9± 1.3

En-Ge 44.7±1.2 40.1±1.2 45.7±1.4 35.0± 1.3

Table 3: KSMR results comparison of our system

and three different state-of-the-art batch systems

6 Conclusions

We have presented an online IMT system. The pro-

posed system is able to incrementally extend the sta-

tistical models involved in the translation process,

breaking technical limitations encountered in other

works. Empirical results show that our techniques

allow the IMT system to learn from scratch or from

previously estimated models.

One key aspect of the proposed system is the use

of HMM-based alignment models trained by means

of the incremental EM algorithm.

The incremental adjustment of the weights of the

log-linear models and other parameters have not

been tackled here. For the future we plan to incor-

porate this functionality into our IMT system.

The incremental techniques proposed here can

also be exploited to extend SMT systems (in fact,

our proposed IMT system is based on an incremen-

tally updateable SMT system). For the near future

we plan to study possible aplications of our tech-

niques in a fully automatic translation scenario.

Finally, it is worthy of note that the main ideas

presented here can be used in other interactive ap-

plications such as Computer Assisted Speech Tran-

scription, Interactive Image Retrieval, etc (see (Vi-

dal et al., 2007) for more information). In conclu-

sion, we think that the online learning techniques

proposed here can be the starting point for a new

generation of interactive pattern recognition systems

that are able to take advantage of user feedback.

Acknowledgments

Work supported by the EC (FEDER/FSE), the

Spanish Government (MEC, MICINN, MITyC,

MAEC, ”Plan E”, under grants MIPRCV ”Con-

solider Ingenio 2010” CSD2007-00018, iTrans2

TIN2009-14511, erudito.com TSI-020110-2009-

439), the Generalitat Valenciana (grant Prome-

teo/2009/014), the Univ. Politécnica de Valencia

(grant 20091027) and the Spanish JCCM (grant

PBI08-0210-7127).

553



References

A. Arun and P. Koehn. 2007. Online learning methods

for discriminative training of phrase based statistical

machine translation. In Proc. of the MT Summit XI,

pages 15–20, Copenhagen, Denmark, September.

S. Barrachina, O. Bender, F. Casacuberta, J. Civera,

E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomás,

and E. Vidal. 2009. Statistical approaches to

computer-assisted translation. Computational Lin-

guistics, 35(1):3–28.

Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della

Pietra, and R. L. Mercer. 1993. The mathematics of

statistical machine translation: Parameter estimation.

Computational Linguistics, 19(2):263–311.

N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008.

Online learning algorithms for computer-assisted

translation. Deliverable D4.2, SMART: Stat. Multi-

lingual Analysis for Retrieval and Translation, Mar.

S.F. Chen and J. Goodman. 1996. An empirical study of

smoothing techniques for language modeling. In Proc.

of the ACL, pages 310–318, San Francisco.

D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-

margin training of syntactic and structural translation

features. In Proc. of EMNLP.

George Foster, Pierre Isabelle, and Pierre Plamondon.

1997. Target-text mediated interactive machine trans-

lation. Machine Translation, 12(1):175–194.

P. Isabelle and K. Church. 1997. Special issue on

new tools for human translators. Machine Translation,

12(1–2).

D.E. Knuth. 1981. Seminumerical Algorithms, volume 2

of The Art of Computer Programming. Addison-

Wesley, Massachusetts, 2nd edition.

P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical

phrase-based translation. In Proc. of the HLT/NAACL,

pages 48–54, Edmonton, Canada, May.

P. Langlais, G. Lapalme, and M. Loranger. 2002.

Transtype: Development-evaluation cycles to boost

translator’s productivity. Machine Translation,

15(4):77–98.

P. Liang, A. Bouchard-Côté, D. Klein, and B. Taskar.

2006. An end-to-end discriminative approach to ma-

chine translation. In Proc. of the 44th ACL, pages 761–

768, Morristown, NJ, USA.

R.M. Neal and G.E. Hinton. 1998. A view of the EM

algorithm that justifies incremental, sparse, and other

variants. In Proceedings of the NATO-ASI on Learning

in graphical models, pages 355–368, Norwell, MA,

USA.

L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004.

Adaptive language and translation models for interac-

tive machine translation. In Proc. of EMNLP, pages

190–197, Barcelona, Spain, July.

Franz Josef Och and Hermann Ney. 2002. Discrimina-

tive Training and Maximum Entropy Models for Sta-

tistical Machine Translation. In Proc. of the 40th ACL,

pages 295–302, Philadelphia, PA, July.

Franz Josef Och and Hermann Ney. 2003. A system-

atic comparison of various statistical alignment mod-

els. Computational Linguistics, 29(1):19–51, March.

D. Ortiz-Martı́nez, I. Garcı́a-Varea, and Casacuberta F.

2008. The scaling problem in the pattern recognition

approach to machine translation. Pattern Recognition

Letters, 29:1145–1153.

Daniel Ortiz-Martı́nez, Ismael Garcı́a-Varea, and Fran-

cisco Casacuberta. 2009. Interactive machine trans-

lation based on partial statistical phrase-based align-

ments. In Proc. of RANLP, Borovets, Bulgaria, sep.

Kishore A. Papineni, Salim Roukos, Todd Ward, and

Wei-Jing Zhu. 2001. Bleu: a method for auto-

matic evaluation of machine translation. Technical

Report RC22176 (W0109-022), IBM Research Divi-

sion, Thomas J. Watson Research Center, Yorktown

Heights, NY, September.

SchlumbergerSema S.A., ITI Valencia, RWTH Aachen,

RALI Montreal, Celer Soluciones, Société Gamma,

and XRCE. 2001. TT2. TransType2 - computer as-

sisted translation. Project Tech. Rep.

Kristina Toutanova, H. Tolga Ilhan, and Christopher

Manning. 2002. Extensions to hmm-based statistical

word alignment models. In Proc. of EMNLP.

E. Vidal, L. Rodrı́guez, F. Casacuberta, and I. Garcı́a-

Varea. 2007. Interactive pattern recognition. In Proc.

of the 4th MLMI, pages 60–71. Brno, Czech Republic,

28-30 June.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.

1996. HMM-based word alignment in statistical trans-

lation. In Proc. of COLING, pages 836–841, Copen-

hagen, Denmark, August.

T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.

2007. Online large-margin training for statistical ma-

chine translation. In Proc. of EMNLP and CoNLL,

pages 764–733, Prage, Czeck Republic.

554


