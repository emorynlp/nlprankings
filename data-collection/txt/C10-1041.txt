Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366,

Beijing, August 2010

358

A Large Scale Ranker-Based System  
for Search Query Spelling Correction 

 

Jianfeng Gao 

Xiaolong Li 

Microsoft Research, Redmond 

Microsoft Corporation 

Daniel Micol 

Microsoft Corporation 

jfgao@microsoft.com 

xiaolong.li@microsoft.com 

danielmi@microsoft.com 

Chris Quirk 

Microsoft Research, Redmond 
chrisq@microsoft.com 

Xu Sun 

University of Tokyo 

xusun@mist.i.u-tokyo.ac.jp 

 

 

Abstract 

This paper makes three significant extensions to a 
noisy  channel  speller  designed  for  standard  writ-
ten text to target the challenging domain of search 
queries.  First,  the  noisy  channel  model  is  sub-
sumed  by  a  more  general ranker,  which allows  a 
variety  of  features  to  be  easily  incorporated.  Se-
cond,  a  distributed  infrastructure  is  proposed  for 
training and applying Web scale n-gram language 
models. Third, a new phrase-based error model is 
presented.  This model  places  a  probability  distri-
bution  over  transformations  between  multi-word 
phrases,  and  is  estimated  using  large  amounts  of 
query-correction  pairs  derived  from  search  logs. 
Experiments  show  that  each  of  these  extensions 
leads  to  significant  improvements  over  the  state-
of-the-art baseline methods. 

1 

Introduction 

Search  queries  present  a  particular  challenge  for 
traditional  spelling  correction  methods.  New 
search  queries  emerge  constantly.  As  a  result, 
many  queries contain valid search terms, such as 
proper  nouns  and  names,  which  are  not  well  es-
tablished  in  the  language.  Therefore,  recent  re-
search  has  focused  on  the  use  of  Web  corpora 
and search logs, rather than human-compiled lex-
icons,  to  infer  knowledge  about  spellings  and 
word  usages  in  search  queries  (e.g.,  Whitelaw  et 
al., 2009; Cucerzan and Brill, 2004).  

The  spelling  correction  problem  is  typically 
formulated  under  the  framework  of  the  noisy 
input  query     
channel  model.  Given  an 
          , we want to find the best spelling correc-
tion                among all candidates: 

           

       

 

Applying Bayes' Rule, we have 

           

           

 

(1) 

(2) 

where  the  error  model        models  the  trans-
formation  probability  from  C  to  Q,  and  the  lan-
guage  model  (LM)       models  the  likelihood 
that C is a correctly spelled query. 

This paper extends a noisy channel speller de-
signed  for  regular  text  to  search  queries  in  three 
ways: using a ranker (Section 3), using Web scale 
LMs  (Section  4),  and  using  phrase-based  error 
models (Section 5). 

First of all, we propose a ranker-based speller 
that  covers  the  noisy  channel  model  as  a  special 
case.  Given  an  input  query,  the  system  first  gen-
erates  a  short  list  of  candidate  corrections  using 
the noisy channel model. Then a feature vector is 
computed  for  each  query  and  candidate  correc-
tion  pair.  Finally,  a  ranker  maps  the  feature  vec-
tor  to  a  real-valued  score,  indicating  the  likeli-
hood that this candidate  is a desirable correction. 
We will demonstrate that ranking provides a flex-
ible  modeling  framework  for  incorporating  a 
wide variety of features that would be difficult to 
model under the noisy channel framework. 

that  significant 

Second, we explore the use of Web scale LMs 
for  query  spelling  correction.  While  traditional 
LM  research  focuses  on  how  to  make  the  model 
“smarter” via how to better estimate the probabil-
ity  of unseen  words (Chen and Goodman, 1999); 
and  how  to  model  the  grammatical  structure  of 
language  (e.g.,  Charniak,  2001),  recent  studies 
show 
improvements  can  be 
achieved  using  “stupid”  n-gram  models  trained 
on  very  large  corpora  (e.g.,  Brants  et  al.,  2007). 
We adopt the latter strategy in this study. We pre-
sent a distributed infrastructure to efficiently train 
and  apply  Web  scale  LMs.  In  addition,  we  ob-
serve  that  search  queries  are  composed  in  a  lan-
guage style different from that of regular text. We 
thus  train  multiple  LMs  using  different  texts  as-
sociated with Web corpora and search queries. 

Third, we propose a phrase-based  error model 
that  captures  the  probability  of  transforming  one 

359

multi-term phrase into another multi-term phrase. 
Compared to traditional error models that account 
for  transformation  probabilities  between  single 
characters  or  substrings  (e.g.,  Kernighan  et  al., 
1990;  Brill  and  Moore,  2000),  the  phrase-based 
error  model  is  more  effective  in  that  it  captures 
inter-term  dependencies  crucial  for  correcting 
real-word  errors,  prevalent  in  search  queries.  We 
also  present  a  novel  method  of  extracting  large 
amounts  of  query-correction  pairs  from  search 
logs. These pairs, implicitly judged by millions of 
users, are used for training the error models. 

Experiments  show  that  each  of  the  extensions 
leads  to  significant  improvements  over  its  base-
line  methods  that  were  state-of-the-art  until  this 
work, and that the combined method yields a sys-
tem  which  outperforms  the  noisy  channel  speller 
by a large margin: a 6.3% increase in accuracy on 
a human-labeled query set. 

2  Related Work 

Prior  research  on  spelling  correction  for  regular 
text  can  be  grouped  into  two  categories:  correct-
ing  non-word  errors  and  real-word  errors.  The 
former focuses on the development of error mod-
els based on different edit distance functions (e.g., 
Kucich,  1992;  Kernighan  et  al.,  1990;  Brill  and 
Moore, 2000; Toutanova and Moore, 2002). Brill 
and  Moore’s  substring-based  error  model,  con-
sidered to be state-of-the-art among these models, 
acts  as  the  baseline  against  which  we  compare 
our models. On the other hand, real-word spelling 
correction  tries  to  detect  incorrect  usages  of  a 
valid  word  based  on  its  context,  such  as  "peace" 
and  "piece"  in the context "a _ of cake". N-gram 
LMs  and  naïve  Bayes  classifiers  are  commonly 
used  models  (e.g.,  Golding  and  Roth,  1996; 
Mangu and Brill, 1997; Church et al., 2007). 

While  almost  all  of  the  spellers  mentioned 
above  are  based  on  a  pre-defined  dictionary  (ei-
ther  a  lexicon  against  which  the  edit  distance  is 
computed, or a set of real-word confusion pairs), 
recent  research  on  query  spelling  correction  fo-
cuses on exploiting noisy Web corpora and query 
logs to infer knowledge about spellings and word 
usag in queries (Cucerzan and Brill 2004; Ahmad 
and  Kondrak,  2005;  Li  et  al.,  2006;  Whitelaw  et 
al., 2009).  Like those spellers designed for regu-
lar text, most of these  query spelling systems are 
also based on the noisy channel framework. 

3  A Ranker-Based Speller 

The  noisy  channel  model  of  Equation  (2)  does 
not  have  the  flexibility  to  incorporate  a  wide  va-
riety  of  features  useful  for  spelling  correction, 
e.g.,  whether  a  candidate  appears as  a  Wikipedia 
document  title.  We  thus  generalize  the  speller  to 
a  ranker-based  system.  Let  f  be  a  feature  vector 
of  a  query  and  candidate  correction  pair  (Q,  C). 
The ranker maps f to a real value y that indicates 
how likely C is a desired correction. For example, 
a linear ranker maps f to y with a weight vector w 
such as          , where w is optimized for accu-
racy  on  human-labeled          pairs.  Since  the 
logarithms  of  the  LM  and  error  model  probabili-
ties can be included as features, the ranker covers 
the noisy channel model as a special case. 

For efficiency, our speller operates in two dis-
tinct stages: candidate generation and re-ranking. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. For each term 
q,  we  consult  a  lexicon  to  identify  a  list  of 
spelling suggestions c whose edit distance from q 
is  lower  than  some  threshold.  Our  lexicon  con-
tains  around  430,000  high  frequency  query  uni-
gram and bigrams collected from 1 year of query 
logs. These suggestions are stored in a lattice.  

We  then  use  a  decoder  to  identify  the  20-best 
candidates from the lattice according to Equation 
(2),  where  the  LM  is  a  backoff  bigram  model 
trained  on  1  year  of  query  logs,  and  the  error 
model is approximated by weighted edit distance:  

                            

(3) 

The  decoder  uses  a  standard  two-pass algorithm. 
The  first  pass  uses  the  Viterbi  algorithm  to  find 
the  best  C  according  to  the  model  of  Equations 
(2)  and  (3).    The  second  pass  uses  the  A-star  al-
gorithm to find the 20-best corrections, using the 
Viterbi  scores  computed  at  each  state  in  the  first 
pass as heuristics. 

The  core  component  in  the  second  stage  is  a 
ranker, which re-ranks the 20-best candidate cor-
rections  using  a  set  of  features  extracted  from 
      .  If  the  top  C  after  re-ranking  is  different 
from  Q,  C  is  proposed  as  the  correction.  We  use 
96  features  in  this  study.  In  addition  to  the  two 
features  derived  from  the  noisy  channel  model, 
the  rest  of  the  features  can  be  grouped  into  the 
following 5 categories. 

1.  Surface-form  similarity  features,  which 
check whether C and Q differ in certain patterns, 

360

e.g., whether C is transformed from Q by adding 
an  apostrophe,  or  by  adding  a  stop  word  at  the 
beginning or end of Q. 

2.  Phonetic-form  similarity  features,  which 
check whether the edit distance between the met-
aphones  (Philips,  1990)  of  a  query  term  and  its 
correction candidate is below some thresholds. 

3.  Entity  features,  which  check  whether  the 
original query is likely to be a proper noun based 
on an in-house named entity recognizer. 

4.  Dictionary  features,  which  check  whether 
a query term  or a candidate correction are in one 
or more human-compiled dictionaries, such as the 
extracted Wiki, MSDN, and ODP dictionaries. 

5.  Frequency  features,  which  check  whether 
the frequency of a query term or a candidate cor-
rection  is  above  certain  thresholds  in  different 
datasets, such as query logs and Web documents. 

4  Web Scale Language Models 

An  n-gram  LM  assigns  a  probability  to  a  word 
string   

                 according to  

 

 

    

     ∏  (  |  

   )

  ∏  (  |      

   

)

 

(4) 

   

   

where  the  approximation  is  based  on  a  Markov 
assumption that each word depends only upon the 
immediately  preceding  n-1  words.  In  a  speller, 
the log of n-gram LM probabilities of an original 
query  and  its  candidate  corrections  are  used  as 
features in the ranker.  

While  recent  research  reports  the  benefits  of 
large LMs trained on Web corpora on a variety of 
applications (e.g. Zhang et al., 2006; Brants et al., 
2007), it is also clear that search queries are com-
posed  in  a  language  style  different  from  that  of 
the body or title of a Web document. Thus, in this 
study  we  developed a set of  large LMs from dif-
ferent  text  streams  of  Web  documents  and  query 
logs.  Below,  we  first  describe  the  n-gram  LM 
collection  used  in  this  study,  and  then  present  a 
distributed  n-gram  LM  platform  based  on  which 
these LMs are built and served for the speller. 

4.1  Web Scale Language Models 

Table  1  summarizes  the  data  sets and  Web  scale 
n-gram LMs used in this study. The collection  is 
built  from  high  quality  English  Web  documents 
containing trillions of tokens, served by a popular 
commercial  search  engine.  The  collection  con-

Title 
257.2B 
150M 
1.1B 
3.1B 
5.1B 
395GB 

Body  Anchor 
11.0B 
1.3T 
60.3M 
1.2B 
464.1M 
11.7B 
1.4B 
60.0B 
148.5B 
2.3B 
183GB 
12.8TB 

Dataset 
Total tokens 
Unigrams 
Bigrams 
Trigrams 
4-grams 
Size on disk# 
# N-gram entries as well as other model parameters are 
stored. 
Table 1: Statistics of the Web n-gram LMs collection (count 
cutoff = 0 for all models). These models will be accessible at 
Microsoft (2010). 

Query 
28.1B 
251.5M 
1.3B 
3.1B 
4.6B 
393GB 

sists of several data sets built from different Web 
sources,  including  the  different  text  fields  from 
the  Web  documents  (i.e.,  body,  title,  and  anchor 
texts)  and  search  query  logs.  The  raw  texts  ex-
tracted  from  these  different  sources  were  pre- 
processed  in  the  following  manner:  texts  are  to-
kenized based on white-space and upper case let-
ters  are  converted  to  lower  case.  Numbers  are 
retained,  and  no  stemming/inflection  is  per-
formed. The n-gram LMs are word-based backoff 
models,  where  the  n-gram  probabilities  are  esti-
mated  using  Maximum  Likelihood  Estimation 
with  smoothing.  Specifically,  for  a  trigram  mod-
el, the smoothed probability is computed as 

                 

(5) 

                 (             )
{

           

                     
 

                     

         

where      is the count of the n-gram in the train-
ing  corpus  and   is  a  normalization  factor.      
is  a  discount  function  for  smoothing.  We  use 
modified absolute  discounting (Gao et al., 2001), 
whose  parameters  can  be  efficiently  estimated 
and performance converges to that of more elabo-
rate  state-of-the-art  techniques  like  Kneser-Ney 
smoothing in large data (Nguyen et al. 2007).  

4.2  Distributed N-gram LM Platform 

The  platform  is  developed  on  a  distributed  com-
puting system designed for storing and analyzing 
massive  data  sets,  running  on  large  clusters  con-
sisting  of  hundreds  of  commodity  servers  con-
nected via high-bandwidth network.  

We use the SCOPE (Structured Computations 
Optimized  for  Parallel  Execution)  programming 
model  (Chaiken  et  al.,  2008)  to  train  the  Web 
scale n-gram LMs shown in Table 1. The SCOPE 
scripting  language  resembles  SQL  which  many 
programmers  are  familiar  with.  It  also  supports 

361

C: 
S: 
T: 
M: 
Q: 

“disney theme park” 

[“disney”, “theme park”] 
[“disnee”, “theme part”] 

(1  2, 2 1) 

“theme part disnee” 

correct query 
segmentation 
translation 
permutation 
misspelled query 

Figure 2: Example demonstrating the generative procedure 
behind the phrase-based error model. 

Figure 1. Distributed 5-gram counting. 

 

C#  expressions  so  that  users  can  easily  plug-in 
customized  C#  classes.  SCOPE  supports  writing 
a  program  using  a series  of  simple  data  transfor-
mations so that users can simply write a script to 
process  data  in  a  serial  manner  without  wonder-
ing how to achieve parallelism  while the SCOPE 
compiler and  optimizer are responsible for trans-
lating  the  script  into  an  efficient,  parallel  execu-
tion  plan.  We  illustrate  the  usage  of  SCOPE  for 
building  LMs  using  the  following  example  of 
counting  5-grams  from  the  body  text  of  English 
Web pages. The flowchart is shown in Figure 1.  
The  program  is  written  in  SCOPE  as  a  step-
by- step of computation, where a command takes 
the output of the previous command as its input. 

ParsedDoc=SELECT docId, TokenizedDoc 

FROM @”/shares/…/EN_Body.txt” 
USING DefaultTextExtractor; 

NGram=PROCESS ParsedDoc 

PRODUCE NGram, NGcount 
USING NGramCountProcessor(-stream       
TokenizedDoc -order 5 –bufferSize 
20000000); 

NGramCount=REDUCE NGram 

ON NGram 
PRODUCE NGram, NGcount 
USING NGramCountReducer; 

 
OUTPUT TO @”Body-5-gram-count.txt”; 

The  first  SCOPE  command  is  a  SELECT 
statement that extracts parsed Wed body text. The 
second  command  uses  a  build-in  Processor 
(NGramCountProcessor)  to  map  the  parsed  doc-
uments  into  separate  n-grams  together  with  their 
counts.  It  generates  a  local  hash  at  each  node 
(i.e., a core in a multi-core server) to store the (n-
gram,  count)  pairs.  The  third  command  (RE-
DUCE)  aggregates  counts  from  different  nodes 
according  to  the  key  (n-gram  string).  The  final 
command (OUTPUT) writes out the resulting to a 
data file. 

The  smoothing  method  can  be  implemented 
similarly  by  the  customized  smoothing  Proces-
sor/Reducer.  They  can  be  imported  from  the  ex-
isting C# codes (e.g., developed for building LMs 
in a single machine) with minor changes.  

It is straightforward to apply the built LMs for 
the  ranker  in  the  speller.  The  n-gram  platform 
provides  a  DLL  for  n-gram  batch  lookup.  In  the 
server,  an  n-gram  LM  is  stored  in  the  form  of 
multiple lists of key-value pairs, where the key is 
the  hash  of  an  n-gram  string  and  the  value  is  ei-
ther the n-gram probability or backoff parameter.  

5  Phrase-Based Error Models 

The goal  of an error model is to transform a cor-
rectly spelled query C into a misspelled query Q. 
Rather  than  replacing  single  words  in  isolation, 
the  phrase-based  error  model  replaces  sequences 
of  words  with  sequences  of  words,  thus  incorpo-
rating  contextual  information.  The  training  pro-
cedure  closely  follows  Sun  et  al.  (2010).  For  in-
stance,  we  might  learn  that  “theme  part”  can  be 
replaced  by  “theme  park”  with  relatively  high 
probability,  even  though  “part”  is  not  a  mis-
spelled  word.  We  use  this  generative  story:  first 
the  correctly  spelled  query  C  is  broken  into  K 
non-empty word sequences c1, …, ck, then each is 
replaced  with  a  new  non-empty  word  sequence 
q1, …, qk, finally these phrases are permuted and 
concatenated  to  form  the  misspelled  Q.  Here,  c 
and q denote consecutive sequences of words. 

To formalize this generative process, let S de-
note the segmentation of C into K phrases c1…cK, 
and  let  T  denote  the  K  replacement  phrases 
q1…qK  –  we  refer  to  these  (ci,  qi)  pairs  as  bi-
phrases. Finally, let M denote a permutation of K 
elements  representing  the  final  reordering  step. 
Figure 2 demonstrates the generative procedure. 

Next  let  us  place  a  probability  distribution 
over rewrite pairs. Let B(C, Q) denote the set of S, 
T, M triples that transform C into Q. Assuming a 
uniform  probability  over  segmentations, 
the 
phrase-based probability can be defined as: 

Node 1

Node 2

…...

Node N

Web Pages

Web Pages

Web Pages

Parsing

Parsing

Parsing

Tokenize

Tokenize

Tokenize

Counting

Counting

Counting

Local 
Hash

Local 
Hash

Local 
Hash

…...

Recursive 

Reducer

Output

362

         ∑                         

  (6) 

              

As is common practice in SMT, we use the max-
imum approximation to the sum:  

        

   

              

                          (7) 

5.1  Forced Alignments 

Although we have defined a generative model for 
transforming  queries,  our  goal  is  not  to  propose 
new  queries,  but  rather  to  provide  scores  over 
existing Q and C pairs that will act as features for 
the  ranker.  Furthermore,  the  word-level  align-
ments between Q and C can most often be identi-
fied  with  little  ambiguity.  Thus  we  restrict  our 
attention  to  those  phrase  transformations  con-
sistent with a good word-level alignment. 

Let J be the length of Q, L be the length of C, 
and A = a1…aJ  be a hidden variable representing 
the  word  alignment  between  them.  Each  ai  takes 
on a value ranging from 1 to L indicating its cor-
responding  word  position  in  C,  or  0  if  the  ith 
word  in  Q  is  unaligned.  The  cost  of  assigning  k 
to  ai  is  equal  to  the  Levenshtein  edit  distance 
(Levenshtein,  1966)  between  the  ith  word  in  Q 
and the kth word in C, and the cost of assigning 0 
to  ai  is  equal  to  the  length  of  the  ith  word  in  Q. 
The  least  cost  alignment  A*  between  Q  and  C  is 
computed efficiently using the A-star algorithm. 
When  scoring  a  given  candidate  pair,  we  fur-
ther  restrict  our  attention  to  those  S,  T,  M  triples 
that  are  consistent  with  the  word  alignment, 
which we denote as B(C, Q, A*). Here, consisten-
cy  requires  that  if  two  words  are  aligned  in  A*, 
then  they  must  appear  in  the  same  bi-phrase  (ci, 
qi).  Once  the  word  alignment  is  fixed,  the  final 
permutation  is  uniquely  determined,  so  we  can 
safely discard that factor. Thus we have: 

            

        
         

          

(8) 

For  the  sole  remaining  factor  P(T|C,  S),  we 
make the assumption that a segmented query  T = 
q1…  qK  is  generated  from  left  to  right  by  trans-
forming each phrase c1…cK independently: 

            ∏         
, 

 
   

(9) 

where          is  a  phrase  transformation  prob-
ability, the  estimation  of which  will be described 
in Section 5.2.  

Google: 

http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+part&aq=f&oq=&aqi= 

http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 

Yahoo: 

http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+part& 
fr2=sb-top&fr=yfp-t-701&sao=1 

http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 

Bing: 

http://www.bing.com/search? 
q=harrypotter+sheme+part&form=QBRE&qs=n 

http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 

Figure 3.  A sample of query reformulation sessions from 3 
popular search engines. These sessions show that a user first 
issues the query "harrypotter sheme part", and then clicks on 
the resulting spell suggestion "harry potter theme park". 

To  find  the  maximum  probability  assignment 
efficiently,  we  use  a  dynamic  programming  ap-
proach,  similar  to  the  monotone  decoding  algo-
rithm described in Och (2002).  

5.2  Training the Error Model  

Given  a  set  of  (Q,  C)  pairs  as  training  data,  we 
follow  a  method  commonly  used  in  SMT  (Och 
and  Ney,  2004)  to  extract  bi-  phrases  and  esti-
mate  their  replacement  probabilities.  A  detailed 
description is discussed in Sun et al. (2010). 

We now  describe  how (Q, C) pairs are gener-
ated  automatically  from  massive  query  reformu-
lation sessions of a commercial Web browser. 

A  query  reformulation  session  contains  a  list 
of  URLs  that  record  user  behaviors  that relate  to 
the  query  reformulation  functions,  provided  by  a 
Web  search  engine.  For  example,  most  commer-
cial  search  engines  offer  the  "did  you  mean" 
function, suggesting a possible alternate interpre-
tation or spelling of a user-issued query. Figure 3 
shows  a  sample  of  the  query  reformulation  ses-
sions  that  record  the  "did  you  mean"  sessions 
from  three  of  the  most  popular  search  engines. 
These sessions encode the same user behavior: A 
user  first  queries  for  "harrypotter  sheme  part", 

363

and  then  clicks  on  the  resulting  spelling  sugges-
tion  "harry  potter  theme  park".  We  can  "reverse-
engineer" the parameters from the URLs of these 
sessions, and deduce  how each search engine  en-
codes both a query and the fact that a user arrived 
at  a  URL  by  clicking  on  the  spelling  suggestion 
of  the  query  –  an  strong  indication  that  the 
spelling suggestion is desired. In this study, from 
1  year  of  sessions,  we  extracted  ~120  million 
pairs.  We  found  the  data  set  very  clean  because 
these  spelling  corrections  are  actually  clicked, 
and thus judged implicitly, by many users. 

In addition to the "did  you  mean" functionali-
ty, recently some search  engines have introduced 
two new spelling suggestion functions. One is the 
"auto-correction"  function,  where  the  search  en-
gine  is  confident  enough  to  automatically  apply 
the spelling correction to the query and execute it 
to  produce  search  results.  The  other  is  the  "split 
pane"  result  page,  where  one  half  portion  of  the 
search  results  are  produced  using  the  original 
query, while the other half, usually visually sepa-
rate  portion  of  results,  are  produced  using  the 
auto-corrected query. 

In neither of these functions does the user ever 
receive  an  opportunity  to  approve  or  disapprove 
of  the  correction.  Since  our  extraction  approach 
focuses  on  user-approved  spelling  suggestions, 
we  ignore  the  query  reformulation  sessions  re-
cording  either  of  the  two  functions.  Although  by 
doing  so  we  could  miss  some  basic,  obvious 
spelling  corrections,  our  experiments  show  that 
the  negative  impact  on  error  model  training  is 
negligible.  One  possible  reason  is  that  our  base-
line  system,  which  does  not  use  any  error  model 
learned  from  the  session  data,  is  already  able  to 
correct  these  basic,  obvious  spelling  mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 

We  found  that  the  error  models  trained  using 
the  data  directly  extracted  from  the  query  refor-
mulation sessions suffer from the problem  of un-
derestimating  the  self-transformation  probability 
of a query P(Q2=Q1|Q1), because we only includ-
ed in the training data the pairs where the query is 
different  from  the  correction.  To  deal  with  this 
problem,  we  augmented  the  training  data  by  in-
cluding  correctly  spelled  queries,  i.e.,  the  pairs 
(Q1, Q2) where Q1 = Q2.  First, we extracted a set 
of  queries  from  the  sessions  where  no  spell  sug-
gestion  is  presented  or  clicked  on.  Second,  we 

removed from the set those queries that were rec-
ognized  as  being  auto-corrected  by  a  search  en-
gine.  We  do  so  by  running  a  sanity  check  of  the 
queries  against  our  baseline  noisy  channel 
speller,  which  will  be  described  in  Section  6.  If 
the  system  consider  a  query  misspelled,  we  as-
sumed it an obvious misspelling, and removed it. 
The  remaining  queries  were  assumed  to  be  cor-
rectly spelled and were added to the training data. 

6  Experiments 

We  perform  the  evaluation  using  a  manually  an-
notated  data  set  containing  24,172  queries  sam-
pled from  one year’s query logs from a commer-
cial  search  engine.  The  spelling  of  each  query  is 
manually  corrected  by  four  independent  annota-
tors.  The  average  length  of  queries  in  the  data 
sets  is  2.7  words.  We  divided  the  data  set  into 
non-overlapped  training  and  test  data  sets.  The 
training  data  contain  8,515        pairs,  among 
which  1,743  queries  are  misspelled  (i.e.      ). 
The  test  data  contain  15,657        pairs,  among 
which 2,960 queries are misspelled.  

The  speller  systems  we  developed  in  this 

study are evaluated using the following metrics. 

 Accuracy:  The  number  of  correct  outputs 
generated  by  the  system  divided  by  the  total 
number of queries in the test set. 

 Precision:  The  number  of  correct  spelling 
corrections  for  misspelled  queries  generated 
by the system  divided by the total number of 
corrections generated by the system. 

  Recall:  The  number  of  correct  spelling  cor-
rections  for  misspelled  queries  generated  by 
the  system  divided  by  the  total  number  of 
misspelled queries in the test set. 

We  also  perform  a  significance  test,  a  t-test 

with a significance level of 0.05. 

In our experiments, all the speller systems are 
ranker-based. Unless otherwise stated, the ranker 
is  a  two-layer  neural  net  with  5  hidden  nodes. 
The  free  parameters  of  the  neural  net  are  trained 
to  optimize  accuracy  on  the  training  data  using 
the  back  propagation  algorithm  (Burges  et  al., 
2005) .  

6.1  System Results 

Table  1  summarizes  the  main  results  of  different 
spelling  systems.  Row  1  is  the  baseline  speller 
where  the  noisy  channel  model  of  Equations  (2) 

364

Accuracy 

#  System 
1  Noisy channel 
2  Linear ranker 
3  Nonlinear ranker 
4 
5 
6 
Table 1. Summary of spelling correction results. 

Precision  Recall 
35.9 
42.8 
49.6 
58.2 
58.7 
63.9 

3 + PBEM 
3 + WLMs 
3 + PBEM + WLMs  

85.3 
88.0 
89.0 
90.7 
90.4 
91.6 

72.1 
74.0 
74.1 
78.7 
75.1 
79.1 

and  (3)  is  used.  The  error  model  is  based  on  the 
weighted  edit  distance  function  and  the  LM  is  a 
backoff bigram  model trained on 1 year of  query 
logs,  with  count  cutoff  30.  Row  2  is  the  speller 
using  a  linear  ranker  to  incorporate  all  ranking 
features  described  in  Section  3.  The  weights  of 
the  linear  ranker  are  optimized  using  the  Aver-
aged Perceptron algorithm (Freund and Schapire, 
1999).  Row  3  is  the  speller  where  a  nonlinear 
ranker (i.e., 2-layer neural net) is trained atop the 
features. Rows 4, 5 and 6 are systems that incor-
porate  the  additional  features  derived  from  the 
phrase-based  error  model  (PBEM)  described  in 
Section  5  and  the  four  Web  scale  LMs  (WLMs) 
listed in Table 1. 

The  results  show  that  (1)  the  ranker  is  a  very 
flexible  modeling  framework  where  a  variety  of 
fine-grained  features  can  be  easily  incorporated, 
and  a  ranker-based  speller  outperforms  signifi-
cantly  (p  <  0.01)  the  traditional  system  based  on 
the  noisy  channel  model  (Row  2  vs.  Row  1);  (2) 
the  speller  accuracy  can  be  further  improved  by 
using  more  sophisticated  rankers  and  learning 
algorithms  (Row  3  vs.  Row  2);  (3)  both  WLMs 
and  PBEM  bring  significant 
improvements 
(Rows  4 and  5  vs.  Row  3); and  (4)  interestingly, 
the  gains  from  WLMs  and  PBEM  are  additive 
and  the  combined  leads  to  a  significantly  better 
speller  (Row  6  vs.  Rows  4  and  5)  than  that  of 
using either of them individually. 

In what follows, we investigate in detail how 
the  WLMs  and  PBEM  trained  on  massive  Web 
content  and  search  logs  improve  the  accuracy  of 
the  speller  system.  We  will  compare  our  models 
with  the  state-of-the-art  models  proposed  previ-
ously.  From  now  on,  the  system  listed  in  Row  3 
of Table 1 will be used as baseline. 

6.2  Language Models 

The quality of n-gram LMs depends on the order 
of  the  model,  the  size  of  the  training  data,  and 
how  well  the  training  data  match  the  test  data. 
Figure  4  illustrates  the  perplexity  results  of  the 

 

Figure 4. Perplexity results on test queries, using n-
gram LMs with different orders, derived from differ-
ent data sources. 
 

four LMs trained  on different  data sources tested 
on  a  random  sample  of  733,147  queries.  The  re-
sults  show  that  (1)  higher  order  LMs  produce 
lower  perplexities,  especially  when  moving  be-
yond unigram  models; (2) as expected, the query 
LMs  are  most  predictive  for  the  test  queries, 
though  they  are  from  independent  query  log 
snapshots; (3) although the body LMs are trained 
on much larger amounts of data than the title and 
anchor LMs, the former lead to much higher per-
plexity  values,  indicating  that  both  title  and  an-
chor texts are quantitatively much more similar to 
queries than body texts. 

Table  2  summarizes  the  spelling  results  using 
different LMs. For comparison, we also built a 4-
gram  LM  using  the  Google  1T  web  5-gram  cor-
pus  (Brants  and  Franz,  2006).  This  model  is  re-
ferred  to  as  the  G1T  model,  and  is  trained  using 
the “stupid backoff” smoothing method (Brants et 
al.,  2007).  Due  to  the  high  count  cutoff  applied 
by  the  Google  corpus  (i.e.,  n-grams  must  appear 
at least 40 times to be included in the corpus), we 
found  the  G1T  model  results  to  a  higher  OOV 
rate (i.e., 6.5%) on our test data than that of the 4 
Web scale LMs (i.e., less than 1%). 

The  results  in  Table  2  are  more  or  less  con-
sistent  with  the  perplexity  results:  the  query  LM 
is  the  best  performer;  there  is  no  significant  dif-
ference  among  the  body,  title  and  anchor  LMs 
though  the  body  LM  is  trained  on  a  much  larger 
amount of data; and all the 4 Web scale LMs out-
perform  the  G1T  model  substantially  due  to  the 
significantly lower OOV rates. 

6.3  Error Models 

This  section  compares  the  phrase-based  error 
model  (PBEM)  described  in  Section  5,  with  one 
of  the  state-of-the-art  error  models,  proposed  by 
Brill and Moore (2000), henceforth referred to as 

365

Accuracy 

#  System 
1  Baseline 
2 
3 
4 
5 
6 
Table 2. Spelling correction results using different LMs 
trained on different data sources. 

Precision  Recall 
49.6 
56.3 
54.4 
54.7 
55.6 
51.5 

1+ query 4-gram 
1 + body 4-gram 
1 + title 4-gram 
1 + anchor 4-gram 
1 + G1T 4-gram 

89.0 
90.1 
89.9 
89.8 
89.9 
89.4 

74.1 
75.6 
75.7 
75.4 
75.1 
75.1 

Accuracy 

#  System 
1  Baseline w/o EM 
2  Baseline 
3 
4 
5 
6 
Table 3. Spelling correction results using different error 
models. 

Precision  Recall 
47.0 
49.6 
50.1 
51.0 
55.6 
58.1 

1 + B&M, L=1 
1 + B&M, L=3 
1 + PBEM, L=1 
1 + PBEM, L=3 

88.6 
89.0 
89.0 
89.2 
90.1 
90.7 

72.0 
74.1 
73.3 
73.7 
76.7 
78.5 

the B&M  model. B&M is a substring  error mod-
el. It estimates        as 

   

            
    

           

∏         
   

  

(10) 

where R is a partitioning of correction term c into 
adjacent  substrings,  and  T  is  a  partitioning  of 
query term q, such that |T|=|R|. The partitions are 
thus  in  one-to-one  alignment.  To  train  the  B&M 
model,  we  extracted  1  billion  term-correction 
pairs         from  the  set  of  120  million  query-
correction  pairs       ,  derived  from  the  search 
logs as described in Section 5.2.  

Table  3  summarizes  the  comparison  results. 
Rows  1 and  2  are  our  ranker-based  baseline  sys-
tems with and without the  error model (EM) fea-
ture.  The  error  model  is  based  on  weighted  edit 
distance of Eq. (3), where the weights are learned 
on  some  manually  annotated  word-correction 
pairs  (which  is  not  used  in  this  study).  Rows  3 
and 4 are the B&M models using different maxi-
mum  substring  lengths,  specified  by  L.  L=1  re-
duces  B&M  to  the  weighted  edit  distance  model 
in Row 2. Rows 5 and 6 are PBEMs with  differ-
ent maximum phrase lengths. L=1 reduces PBEM 
to  a  word-based  error  model.  The  results  show 
the  benefits  of  capturing  context  information  in 
error  models.  In  particular,  the  significant  im-
provements  resulting  from  PBEM  demonstrate 
that  the  dependencies  between  words  are  far 
more  effective  than  that  between  characters 
(within  a  word)  for  spelling  correction.  This  is 
largely  due  to  the  fact  that  there  are  many  real-
word  spelling  errors  in  search  queries.  We  also 
notice that PBEM is a more powerful model  than   

Precision 

Accuracy 

#  # of word pairs 
1  Baseline w/o EM 
2  1M 
3  10M 
4  100M 
5  1B 
Table  4.  The  performance  of  B&M  error  model  (L=3)  as  a 
function of the size of training data (# of word pairs). 

Recall 
46.97 
50.74 
50.92 
51.06 
50.99 

88.55 
89.15 
89.22 
89.20 
89.21 

71.95 
73.71 
74.11 
73.60 
73.72 

Precision 

#  # of (Q, C) pairs  Accuracy 
1  Baseline w/o EM 
2  5M 
3  15M 
4  45M 
5  120M 
Table  5.  The  performance  of  PBEM  (L=3)  as a  function  of 
the size of training data (# of (Q, C) pairs). 

Recall 
46.97 
52.34 
56.67 
57.02 
58.12 

88.55 
89.59 
90.23 
90.45 
90.70 

71.95 
77.01 
77.87 
78.56 
78.49 

B&M in that it can benefit more from increasing-
ly larger training data. As shown in Tables 4 and 
5,  whilst  the  performance  of  B&M  saturates 
quickly with the increase of training data, the per-
formance  of  PBEM  does  not  appear  to  have 
peaked – further improvements are likely given a 
larger data set. 

7  Conclusions and Future Work 

This paper explores the use of  massive Web cor-
pora  and  search  logs  for  improving  a  ranker- 
based  search  query  speller.  We  show  significant 
improvements  over a noisy channel speller using 
fine-grained  features,  Web  scale  LMs,  and  a 
phrase-based  error  model  that  captures  intern- 
word  dependencies.  There  are  several  techniques 
we  are  exploring  to  make  further  improvements. 
First,  since  a  query  speller  is  developed  for  im-
proving the Web search results, it is natural to use 
features from search results in ranking, as studied 
in Chen et al. (2007). The challenge is efficiency. 
Second,  in  addition  to  query  reformulation  ses-
sions,  we  are  exploring  other  search  logs  from 
which  we  might  extract  more        pairs  for  er-
ror  model  training.  One  promising  data  source  is 
clickthrough  data  (e.g.,  Agichtein  et  al,  2006; 
Gao  et  al.,  2009).  For  instance,  we  might  try  to 
learn  a  transformation  from  the  title  or  anchor 
text of a document to the query that led to a click 
on that document. Finally, the phrase-based error 
model  is inspired by phrase-based SMT systems. 
We  are  introducing  more  SMT  techniques  such 
as  alignment  and  translation  rule  exaction.  In  a 
broad sense, spelling correction can be viewed as 
a  monolingual  MT  problem  where  we  translate 
bad English queries into good ones. 

366

Acknowledgments 

The  authors  would  like  to  thank  Andreas  Bode, 
Mei  Li,  Chenyu  Yan  and  Kuansan  Wang  for  the 
very  helpful  discussions  and  collaboration.  The 
work  was  done  when  Xu  Sun  was  visiting  Mi-
crosoft Research Redmond. 

References 

Agichtein, E., Brill, E. and Dumais, S. 2006. Improv-

ing web search ranking by incorporating user be-
havior information. In SIGIR, pp. 19-26. 

Ahmad, F., and Kondrak, G. 2005. Learning a spelling 

error model from search query logs. In HLT-
EMNLP, pp. 955-962. 

Brants, T., and Franz, A. 2006. Web 1T 5-gram corpus 

version 1.1. Technical report, Google Research. 

Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. 
2007. Large language models in machine translation. 
In EMNLP-CoNLL, pp. 858 - 867. 

Brill, E., and Moore, R. C. 2000. An improved error 

model for noisy channel spelling correction. In ACL, 
pp. 286-293. 

Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In ICML, 
pp. 89-96.  

 Chaiken, R., Jenkins, B., Larson, P., Ramsey, B., 

Shakib, D., Weaver, S., and Zhou, J. 2008. SCOPE: 
easy and efficient parallel processing f massive data 
sets. In Proceedings of the VLDB Endowment, pp. 
1265-1276. 

Charniak, E. 2001. Immediate-head parsing for lan-

guage models. In ACL/EACL, pp. 124-131. 

Chen, S. F., and Goodman, J. 1999. An empirical 

study of smoothing techniques for language model-
ing. Computer Speech and Language, 13(10):359-
394. 

Chen, Q., Li, M., and Zhou, M. 2007. Improving que-

ry spelling correction using web search results. In 
EMNLP-CoNLL, pp. 181-189. 

Church, K., Hard, T., and Gao, J. 2007. Compressing 

trigram language models with Golomb coding. In 
EMNLP-CoNLL, pp. 199-207. 

Cucerzan, S., and Brill, E. 2004. Spelling correction as 

an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 

Freund, Y. and Schapire, R. E. 1999. Large margin 
classification using the perceptron algorithm. In 
Machine Learning, 37(3): 277-296. 

guistics and Chinese Language Processing, 
6(1):27–60, 2001.  

Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 

2009. Smoothing clickthrough data for web search 
ranking. In SIGIR, pp. 355-362.  

Golding, A. R., and Roth, D. 1996. Applying winnow 

to context-sensitive spelling correction. In ICML, pp. 
182-190. 

Joachims, T. 2002. Optimizing search engines using 

clickthrough data. In SIGKDD, pp. 133-142. 

Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 

phrase-based translation. In HLT/NAACL, pp. 127-
133. 

Kucich, K. 1992. Techniques for automatically 

correcting words in text. ACM Computing Surveys, 
24(4):377-439. 

Levenshtein, V. I. 1966. Binary codes capable of cor-

recting deletions, insertions and reversals. Soviet 
Physics Doklady, 10(8):707-710. 

Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. Ex-
ploring distributional similarity based models for 
query spelling correction. In ACL, pp. 1025-1032. 

Mangu, L., and Brill, E. 1997. Automatic rule acquisi-
tion for spelling correction. In ICML, pp. 187-194. 

Microsoft Microsoft web n-gram services. 2010. 

http://research.microsoft.com/web-ngram 

Nguyen, P., Gao, J., and Mahajan, M. 2007. MSRLM: 

a scalable language modeling toolkit. Technical re-
port TR-2007-144, Microsoft Research. 

Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 

Och, F., and Ney, H. 2004. The alignment template 

approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 

Philips, L. 1990. Hanging on the metaphone. Comput-

er Language Magazine, 7(12):38-44. 

Sun, X., Gao, J., Micol, D., and Quirk, C. 2010. 

Learning phrase-based spelling error models from 
clickthrough data. In ACL.  

Toutanova, K., and Moore, R. 2002. Pronunciation 

modeling for improved spelling correction. In ACL, 
pp. 144-151.  

Whitelaw, C., Hutchinson, B., Chung, G. Y., and Ellis, 

G. 2009. Using the web for language independent 
spellchecking and autocorrection. In EMNLP, pp. 
890-899. 

Gao, J., Goodman, J., and Miao, J. 2001. The use of 

clustering techniques for language modeling -
application to Asian languages. Computational Lin-

Zhang, Y., Hildebrand, Al. S., and Vogel, S. 2006. 
Distributed language modeling for n-best list re-
ranking. In EMNLP, pp. 216-233. 

