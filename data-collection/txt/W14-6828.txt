



















































Introduction to BIT Chinese Spelling Correction System at CLP 2014 Bake-off


Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 179â€“185,
Wuhan, China, 20-21 October 2014

Introduction to BIT Chinese Spelling Correction 

System at CLP 2014 Bake-off 

Min Liu 

School of Computer Science 

and Technology, Beijing In-

stitute of Technology 

luis328@foxmail.com 

 

 

Ping Jian 

School of Computer Science 

and Technology, Beijing In-

stitute of Technology 

pjian@bit.edu.cn 

Heyan Huang 

School of Computer Science 

and Technology, Beijing In-

stitute of Technology 

 hhy63@bit.edu.cn 

Abstract 

This paper describes the Chinese spelling 

correction system submitted by BIT at 

CLP Bake-off 2014 task 2. The system 

mainly includes two parts: 1) N-gram 

model is adopted to retrieve the 

non-words which are wrongly separated 

by word segmentation. The non-words 

are then corrected in terms of word fre-

quency, pronunciation similarity, shape 

similarity and POS (part of speech) tag. 

2) For wrong words, abnormal POS tag 

is used to indicate their location and de-

pendency relation matching is employed 

to correct them. Experiment results 

demonstrate the effectiveness of our 

system. 

1. Introduction 

Spelling check, which is an automatic mecha-

nism to detect and correct human spelling errors, 

is a common task in every written language. The 

number of people learning Chinese as a Foreign 

Language (CFL) is booming in recent decades 

and this number is expected to become even 

larger for the years to come. However, unlike 

English learning environment where many 

learning techniques have been developed, tools to 

support CFL learners are relatively rare, espe-

cially those that could automatically detect and 

correct Chinese spelling and grammatical errors. 

For example, Microsoft WordÂ® has not yet sup-

ported these functions for Chinese, although it 

supports English for years. In CLP Bake-off 2014, 

essays written by CFL learners were collected for 

developing automatic spelling checkers. The 

aims are that through such evaluation campaigns, 

more innovative computer assisted techniques 

will be developed, more effective Chinese 

learning resources will be built, and the 

state-of-art NLP techniques will be advanced for 

the educational applications. 

By analyzing the training data released by the 

CLP 2014 Bake-off task21 and the test data used 

in SIGHAN Bake-off 20132 , we find that the 

main errors focus on two types: One is wrong 

characters which result in â€œnon-wordsâ€ that are 

similar to OOV (out-of-vocabulary). For example, 

the writer may misspell â€œèº«é‚Šâ€ as â€œç”Ÿé‚Šâ€, and 

â€œæ ¹æ“šâ€ as â€œæ ¹è™•â€ (The former appears because 
of the wordsâ€™ similar pronunciation and the latter 

comes up due to their similar shape). These are 

even not words and of course do not exist in the 

vocabulary. The other type is words which are 

correct in the dictionary but incorrect in the sen-

tence. Some of them may be misspelled, like â€œæƒ…

æ„›â€ in phrase â€œæƒ…æ„›çš„ç‹å®œå®¶â€, which is a mis-

spelling of word â€œè¦ªæ„›â€. But we can find â€œæƒ…æ„›â€ 
in the dictionary and it is not a non-word. Others 

are words which are not used correctly. This 

usually happens when the writer does not under-

stand their meaning clearly. For example,  writ-

ers often confuse â€œåœ¨â€ and â€œå†â€, such as â€œé«˜é›„æ˜¯

å†å°ç£å—éƒ¨ä¸€å€‹ç¾ä»£åŒ–åŸå¸‚â€. Here, it is â€œåœ¨â€ 

but not â€œå† â€ the right one. Different from 
non-words, we call these words â€œwrong wordsâ€.   

According to the statistics obtained from the 

training data of CLP 2014 Back-off, there are 

nearly 3,400 wrong words which are about twice 

more than non-words, 1,800 ones.  

Spelling check and correction is a traditional 

task in natural language processing. Pollock and 

Zamora (1984) built a misspelling dictionary for 

spelling check. Chang (1995) adopted a bi-gram 

language model to substitute the confusing 

character. Zhang et al. (2000) proposed an ap-

proximate word matching method to detect and 

correct spelling errors. Liu et al. (2011) 

 
1 http://www.cipsc.org.cn/clp2014/webpage/cn/four_ 

bakeoffs/Bakeoff2014cfp_ChtSpellingCheck_cn.htm 
2 http://tm.itc.ntnu.edu.tw/CNLP/?q=node/27 

179



 
Figure 1: System architecture 

extended the principles of decomposing Chinese 

characters with the Cangjie codes to judge the 

visual similarity between Chinese characters. 

SIGHAN Bake-off 2013 for Chinese spelling 

check inspired a variety of spelling check and 

correction techniques (Wu et al., 2013). Typical 

statistical approaches such as maximum entropy 

model and machine translation model performed 

well assisted by rule based model and other lan-

guage analysis techniques. 

Compared with the test data in SIGHAN 

Bake-off 2013, there are more wrong words and 

the text is more colloquial in the current Bake-off, 

which make the correction task more challeng-

ing. 

2. System Architecture 

In terms of the error types of the task, our system 

is mainly composed by two stages: non-word 

correction and wrong word correction. In detail, 

stage one consists of several parts: word seg-

mentation, non-word detection, POS (part of 

speech) tagging and non-word correction. The 

second stage is conducted by heuristic rules cor-

rection, POS tagging & parsing, and wrong word 

detection & correction. The figure 1 shows the 

architecture of our system. 

2.1 Preparations 

To cater to the need of error correction system 

for linguistic resources, three dictionaries/bases 

are constructed: a dictionary, a word-POS base 

and a dependency relation base.  

We use Tsai's list of Chinese words3 collect-

ed by Chih-Hao Tsai as a basic dictionary and 

make use of Sinica Corpus4 to add frequency 

for each word in it. Considering that Pinyin5 can 

be useful in pronunciation similarity spelling 

error detection and correction, we add it to each 

word in the dictionary with the help of TagPin-

yin6 developed by International R&D Center for 

Chinese Education. Since this tool can only tag 

Pinyin for simplified Chinese, we use OpenCC7 

to make the conversion between traditional Chi-

nese and simplified Chinese. By this way, we 

obtain the dictionary like the example below: 

 
 

 

There are more than 239,000 words totally in 

the dictionary. The words have the same first 

character are put in one line and they are indexed 

by their first character to boost the efficiency of 

searching. Each item consists of three parts: the 

word (â€œæ…šæ„§â€), the Pinyin (â€œcan kuiâ€), and the 
frequency (â€œ58â€). 

Penn Chinese Treebank7.0 (CTB7.0) (Xue et 

al., 2005) is employed to build the word-POS 

base and the dependency relation base. In this 

way, the word category information and candi-

dates for correct words are provided. Taking 

domain and area stuff into consideration, we 

extract the mz (news magazine from Sinorama), 

bc (broadcast conversation from New Tang 

Dynasty TV etc.) and wb (weblogs) parts of 

CTB7.0, which form a dependency corpus in-

cluding 30,861 sentences. The simplified char-

acters in the corpus are also converted by 

OpenCC. We get about 42,000 items in the 

word-POS base and the format is as following: 

 

 

 

 

 
3 http://technology.chtsai.org/wordlist/ 
4 http://app.sinica.edu.tw/kiwi/mkiwi/ 
5 Pinyin is the standard system of romanized spelling for 

transliterating Chinese. 
6 http://nlp.blcu.edu.cn/downloads/download-tools/ 
7 http://code.google.com/p/opencc/ 

èƒ› èƒ› jia 1 èƒ›éª¨ jia gu 1 

æ…š æ…š can 3 æ…šè‰² can se 1 æ…šæ„§ can kui  58 

æ­éœ²  JJ 1 NN 1 VV 16 

æ®  VV 4 

æ®ä¹‹ä¸å» VV 2 

180



In the example above, the first column is the 

word and the following are all the POSes and 

their frequencies by counting the corpus.  

The dependency relation base is made up of 

dependency relations extracted from the CTB 

corpus. It includes one word with all its head 

words and the corresponding frequencies in each 

line. The following is an example: 

 

 

Here, â€œæŠ—è­°â€ is headed by â€œROOTâ€ which 
means that it is the root word in the sentence. By 

this way, more than 300,000 dependency rela-

tions were extracted from the corpus. 

Originally, we considered Sinica Treebank8, 

which is a traditional Chinese corpus in nature, 

as the more proper one to generate the POS and 

dependency base. However, the POS category 

and the dependency relation type of the bank are 

too trivial. In addition, the parsing unit in Sinica 

Treebank is not a natural sentence but segments 

divided by punctuations, which results in lack of 

dependency types. Many relations between 

segments degenerate to â€œROOTâ€ in the Tree-

bank. 

2.2 Non-word Correction 

This stage mainly includes non-word detection 

& correction stage and it starts from the seg-

mentation of raw error sentences. When seg-

mentation is done for the input file, we find that 

the words involving misspelled character might 

be separated into serial characters. For example, 

sentence â€œé€™å€‹å­¸æœŸå·²ç¶“éäº†å…©å€‹è£¡æ‹œäº†ã€‚â€ 

will be segmented into â€œé€™ å€‹ å­¸æœŸ å·²ç¶“ éäº† 

å…©å€‹ è£¡ æ‹œ äº† ã€‚â€ and potential non-word â€œè£¡

æ‹œâ€ is impossible to be found as a word. Dic-
tionary based non-word detection would not 

work in this case. We utilize a simple n-gram 

model here to retrieve the missing words. The 

method in detail is described as following: The 

uncommon co-occurrence of adjacent characters 

after segmentation can be found by pre-trained 

character n-gram model. The retrieving begins at 

the first single-character word with low proba-

bility, and combines it with one single-character 

word before or after it. To further confirm 

whether the combination is reasonable or not, 

we traverse the dictionary to find if there is a 

â€œdependableâ€ candidate word which can make 

sure that the retrieved non-word can be substi-

 
8 http://rocling.iis.sinica.edu.tw/CKIP/engversion/ 

treebank.htm 

tuted by a real word in the dictionary. For sim-

plicity, we only consider words who have the 

same Pinyin form with the retrieved word as the 

â€œdependableâ€ words.  

After the non-word retrieval, a dictionary 

matching is competent to detect the non-words 

in the sentences. In the step of non-word correc-

tion, the word which cannot be matched from the 

dictionary completely will be substituted by a 

word in the dictionary. A weighted voting ap-

proach is employed here to select the most pos-

sible candidate word. 

 ï¿½Ì‚ï¿½(ğ‘¤non) = arg max
ğ‘¤ğ‘–âˆˆDic

ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ (ğ‘¤ğ‘–, ğ‘¤non) Ì‡ ,  (1) 

ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘¤, ğ‘¤non) = ğ‘™ğ‘œğ‘”ğ¹ğ‘Ÿğ‘¤ + ğ‘†ğ‘–ğ‘š(ğ‘¤, ğ‘¤non), (2) 

   ğ‘†ğ‘–ğ‘š(ğ‘¤, ğ‘¤non) = ğ›¼1ğ‘†ğ‘–ğ‘špro + ğ›¼2ğ‘†ğ‘–ğ‘šshap 

                             + ğ›¼3ğ‘†ğ‘–ğ‘šPOS ,            (3) 

    ğ‘†ğ‘–ğ‘špro = {
1        same pronunciation
0                           otherwise

 ,  (4) 

   ğ‘†ğ‘–ğ‘šshap = {
1                        same shape
0                           otherwise

 ,  (5) 

     ğ‘†ğ‘–ğ‘šPOS = {
1                    same category
0                            otherwise

 ,  (6) 

where, wnon represents the non-word to be sub-

stituted while w is the candidate word. Fr in the 

formulation indicates the frequency of the word 

in the dictionary. Besides the frequency, three 

types of similarity measures are considered in 

our system: the pronunciation similarity, the 

shape similarity and the lexical category similar-

ity. If the candidate word in the dictionary has 

the same or similar pronunciation with the target 

word, Simpro is set 1, else it is set 0. The setting 

of Simshap is the same. Because characters of 

similar pronunciations are the most common 

source of errors in the training set, the weight 

coefficient Î±1 is set 2 and Î±2 and Î±3 are both set 1 

in our system. The similar pronunciation and 

similar shape character set offered by SIGHAN 

Bake-off 2013 are employed to scope the candi-

dates. 

As for the category similarity, it is known that 

there is no lexical category for an 

out-of-dictionary word. To predict the probable 

class of the target non-word (more precisely, itâ€™s 

the class of the location where the non-word lo-

cates), a sequential labeling POS tagger is ap-

plied. We believe that the tagger will label a 

known word depending more on the word itself 

but label an unknown word relying more on its 

context. Experiments and analyses on the train-

ing data show that about 80% non-words are 

æŠ—è­°  ROOT 4 äº‹ä»¶ 3 ä»¥ç¤º 1 â€¦ 

181



specified with the category which is valid for the 

corresponding correct words. For instance, sen-

tence â€œæˆ‘ å·²ç¶“ *å…¶å¾… äº† å¾ˆä¹…â€ is tagged as 
following: 

 

 

For the non-word â€œ*å…¶å¾…â€, tag â€œVVâ€ is marked, 
which indicates that it needs a verb there in ac-

cordance with the context. â€œVVâ€ is also one of 

the possible categories of the word â€œæœŸå¾…â€, 
which is the word that there was supposed to be. 

In the weighted voting module, candidates who 

own the same POS tag with the target word are 

preferred to be selected. 

In consideration of all the measures, candidate 

word with the highest score will be chosen as the 

correction result. 

2.3 Wrong Word Correction 

After all the non-words are substituted by in dic-

tionary words, several heuristic rules are utilized 

to deal with some phenomena with strong regu-

larity. These rules include:  

ï¬ Replace â€œé–€â€ by â€œå€‘â€: if there is any word in 
a predefined set or its first-class similar 

words in HIT-CIR TongyiciCilin (Extend-

ed)9  (Che et al., 2010) appearing before 

â€œé–€â€, it should be â€œå€‘â€. The set used in the 
task is:  

{æˆ‘, ä½ , å¦³, ä»–, å¥¹, äºº, åŒå­¸, å…„å¼Ÿ, è¦ªäºº, 

å®¢äºº, å°æ‰‹, æˆå“¡, å…¬å¸, å·¥å» , ä¼æ¥­}. 

ï¬ Correction of interjections: if  â€œé˜¿â€, â€œæŠŠâ€, 

â€œå·´â€, â€œæ‹‰â€ and â€œéº»â€ etc. locate before a dot 

mark (ã€‚ï¼Ÿï¼ ï¼Œã€ ï¼› ï¼š) and segmented as 

a single character word, it should be â€œå•Šâ€, 

â€œå§â€, â€œå•¦â€ or â€œå˜›â€. 

ï¬ The gender related correction: correct â€œä»–â€, 

â€œå¥¹â€, â€œä½ â€, â€œå¦³â€ into the one appears more 
frequently in the context (within the sen-

tences owning the same Pid). Here is an 

example: â€œå¦³â€ will be corrected by â€œä½ â€ in 

sentence â€œæˆ‘å¸Œæœ›ï¼Œä½ æœƒå¦³è‡ªå·±ç™¼ç¾æ€éº¼åšã€‚

å¯æ˜¯æˆ‘è¦ºå¾—ä½ å¾—å•æœ‹å‹æ€éº¼è¾¦ã€‚æ‰€ä»¥æˆ‘è¦º

å¾—ä½ ä¸Šèª²çš„æ™‚å€™ä¸æ‡‰è©²å–é…’ã€‚è€Œä¸”å–é…’å°

ä½ çš„èº«é«”ä¸å¥½ï¼Œå®³ä½ å¾ˆå®¹æ˜“æ„Ÿå†’ã€‚â€ 
ï¬ Correction of â€œDeâ€ (â€œDeâ€ refers to one of the 

word â€œçš„â€, â€œåœ°â€ and â€œå¾—â€): which â€œDeâ€ will 
be used depends on the category of its head 

word located after it in the sentence.  

If the category of the head word is adjective 

or adverb, it should be â€œå¾—â€.  
 

9 http://www.ltp-cloud.com/ 

If the one is noun or punctuation, it should be 

â€œçš„â€.  
If the one is verb, it should be â€œåœ°â€. 
To make use of the dependency structure this 

rule should be carried out after the POS tag-

ging and parsing step. 

For common errors, a novel method compris-

ing abnormal POS detection and dependency 

relation matching is designed.  

It is found that the POS tag of some words in a 

sentence may look strange when there is a wrong 

word in the sentence. Two examples are as fol-

lowing: 

 

 

 

 

The existence of wrong word â€œéå¤±â€ and â€œå†â€ 
confuses the sequential POS tagger and abnor-

mal labeling comes up. Sometimes it happens on 

the wrong words themselves, such as â€œéå¤±â€ 
being labeled with an impossible class â€œVVâ€ 

(verb); sometimes other words around are af-

fected by the wrong word, such as â€œå°ç£â€ being 
tagged as a verb due to the wrongly used word 

â€œå†â€ before it.  
To locate and correct these wrong words, a 

dependency parsing is carried out following the 

POS retagging and all the dependency pairs in-

volving the abnormal word are extracted to be 

examined. The left side in Figure 2 shows the 

dependency pairs related with â€œéå¤±â€. Distinct 
with the first one, POS tagging at this stage is 

conducted on the sentence where the non-words 

has been replaced by in-dictionary ones. This is 

hoped to achieve a higher tagging precision. 

By traversing the dependency base, if there is 

no exact matching of these dependencies but 

similar ones (by pronunciation or by shape) in 

the base, we have reason to believe that the 

matched similar pairs imply the answer we ex-

pect. The right side of Figure 2 exhibits the 

matched pairs in the dependency base. In the 

example, the wrong word â€œ éå¤± â€ is to be 

changed with â€œéä¸–â€. In the same way, â€œå† å°

ç£â€ will be corrected by â€œåœ¨ å°ç£â€ since the 
latter is frequent in the base. 

 

 

 

 

 

 

æˆ‘_PN  å·²ç¶“_AD  *å…¶å¾…_VV  äº†_AS  å¾ˆä¹…_NN 

ä»–_PN éå¤±_VV å·²ç¶“_AD ä¸‰_CD å¹´_M å¤š_AD 

äº†_SP 

å†_AD å°ç£_VV ç”Ÿæ´»_NN æ€éº¼æ¨£_VA 

ä»– éä¸– 3 

äº† éä¸– 6 

ä»– éå¤± 

äº† éå¤± 

Dependency relations 
in the error sentence 

Dependency relations in 
the dependency base 

Figure 2: Wrong word correction via 

dependency relation matching 

182



 

3. Experiments 

In this section, several experiments are conducted 

to verify the proposed methods described in 

Section 2. The final official provided test dataset 

consists of 1,062 sentences with or without 

spelling errors in traditional Chinese. Since the 

released training data are hardly employed to 

train models in our system, we regard it as a de-

velopment set where some parameters are set-

tled.  

3.1 Training N-gram, Word Segmentation, 

POS Tagging and Parsing Models 

Sinica Corpus was used to train the CRF based 

word segmentation model implemented by 

CRF++10, while the final test sets released by 

SIGHAN Bake-off 2013 were used to train the 

single character word n-gram model. The POS 

tagger and parser were trained at the extracted 

part of CTB7.0 (the same part where the de-

pendency base is built). The texts were convert-

ed into tradition Chinese by OpenCC. Like word 

segmentation, CRF based sequential labeling 

model is utilized for the POS tagging. It can 

achieve an accuracy more than 93% when 

trained and tested at CTB. Dependency trees of 

the test sentences were obtained by a fast parser, 

the Layer-based dependency parser 11 , which 

considers hierarchical parsing as sequence la-

beling (Jian and Zong, 2009). 

3.2 Metrics 

The criteria for judging correctness are: 

(1) Detection level: all locations of incorrect 

characters in a given passage should be com-

pletely identical with the gold standard. 

(2) Correction level: all locations and corre-

sponding corrections of incorrect characters 

should be completely identical with the gold 

standard. The following metrics are measured in 

both levels with the help of the confusion ma-

trix. 

ï¬ False Positive Rate (FPR) = FP / (FP+TN) 

ï¬ Accuracy=(TP+TN)/ (TP+TN+FP+FN) 

ï¬ Precision= TP/(TP+FP) 

ï¬ Recall=TP/(TP+FN) 

ï¬ F1-Score=2*Precision*Recall/(Precision+ 

Recall) 

 
10 http://crfpp.googlecode.com/svn/trunk/doc/index. 

html?source=navbar 
11 http://www.openpr.org.cn/index.php/NLP-Toolkit- 

for-Natural-Language-Processing/30-Layer-Based-Depende
ncy-Parser/View-details.html 

Table 1: Confusion Matrix 

3.3 Experiment Design 

There are some different settings in our previous 

experiments on the development set (the re-

leased training data) and we apply three of them 

to the final test file. 

BIT Run1: All modules are employed except 

the abnormal POS detection and dependency 

relation matching. The threshold of the n-gram 

transfer probability at non-word retrieval step is 

set as 0.008. The frequency threshold of the 

â€œdependableâ€ word is set as 80. That is to say 

the quasi non-word will not be retrieved if its 

â€œdependableâ€ word appears less than 80 times in 

the dictionary. 

BIT Run2: Abnormal POS detection and de-

pendency relation matching are included. 

BIT Run3: â€œDeâ€ is a frequently used word in 

Chinese texts. Due to the low parsing accuracy, 

plenty of â€œDeâ€ were wrongly replaced in our 

experiments. To avoid this type of noise, the 

heuristic rules about the correction of â€œDeâ€ are 

removed in Run3. Moreover, the transfer proba-

bility and the frequency threshold is changed to 

0.001 and 100 respectively to tighten the re-

trieval. 

3.4 Final Results 

We get three evaluation results (shown in Table 

2 and Table 3) from the organizer. Run1 and 

Run2 are the ones submitted to the Bake-off. 

Considering that nearly two-thirds of the errors 

are wrong word errors, Run1 which doesnâ€™t em-

ploy any wrong word detection strategies per-

forms poorly on recall. Another reason of the low 

recall is that the non-word detection module in 

our system lies on the assumption that there is no 

more than one wrong character in a non-word. In 

this way, words such as â€œå‹åˆ€â€ (å˜®å¨) and â€œèŠ±é‘½

å“â€ (â€œåŒ–å¦å“â€) are missed. 

 

 

 

 

Confusion Ma-

trix 

System Result 

Positive 

(With Errors) 

Negative 

(Without Errors) 

Gold 

Stand- 

ard 

Positive 
TP 

(True Positive) 
FN 

(False Negative) 

Negative 
FP 

(False Positive) 
TN 

(True Negative) 

183



 Approaches  Resources and knowledge Toolkits 

Word segmentation CRFs based sequential labeling Sinica corpus TagPinyin 

OpenCC 

CRF++ 

LDPar 

POS tagging CRFs based sequential labeling Part of Penn CTB7.0 

Parsing Layer-based dependency parsing Part of Penn CTB7.0 

Non-word detection Word segmentation 

n-gram based non-word retrieval 

SIGHAN Bake-off 2013 test set 

Word base (Sinica corpus and 

Tsai's list of Chinese words) 

Training data released 

Non-word correction Weighted votes Word base 

Pinyin 

similar pronunciation character set 

similar shape character set 

POS tag 

Heuristic rules Rule-based correction Training set 

HIT-CIR Tongyici Cilin (Extended) 

Wrong word detection 

& correction 

POS tagging 

Dependency parsing 

Abnormal POS detection 

Dependency relation matching 

Word-POS base 

Dependency relation base 

Table 4: A summary of approaches and resources employed in our correction system 

 

BIT Run1 

FPR Accuracy Precision Recall F1-Score 

0.3352 

Detection Level 

0.4313 0.3710 0.1977 0.2580 

Correction Level 

0.4115 0.3206 0.1582 0.2119 

BIT Run2 

FPR Accuracy Precision Recall F1-Score 

0.3277 

Detection Level 

0.4482 0.4061 0.2241 0.2888 

Correction Level 

0.4303 0.3650 0.1883 0.2484 

Table 2: The results of the submitted two runs 

BIT Run3 

FPR Accuracy Precision Recall F1-Score 

0.1582 

Detection Level 

0.5245 0.5670 0.2072 0.3034 

Correction Level 

0.5122 0.5359 0.1827 0.2725 

Table 3: The results of Run3 

According to the results of Run2, wrong word 

correction based on the knowledge of POS tag 

and dependency relation shows positive effects 

both on precision and recall. Since only the POS 

tag is adopted to detect possible wrong words in 

the current strategy, the misusage of words 

which are in the same category will escape. â€œå“ª

è£¡â€ and â€œé‚£è£¡â€ is an typical example. Both of 
them act as pronouns at most of the time. A 

broader context and more complex semantic 

knowledge are required to distinguish them. 

Management of the auxiliary word â€œDeâ€ is not 

given enough attention in our system. Although 

the corresponding rules designed are delicate 

and clear, many unexpected cases and poor per-

formance of Chinese language analysis tech-

niques make it not work well in practice. Results 

of Run3 reveal that the accuracy and precision 

are improved a lot when heuristic rules for cor-

rection of â€œDeâ€ are removed, although the recall 

decreases to some extent. 

Results of Run3 also illustrate that the stricter 

thresholds for retrieval in non-word detection 

are helpful to improve the performance. This 

implies that the perplexity of non-words in this 

task is not very high and it is not a big problem 

to differentiate them from correct ones. 

4. Conclusion 

In this paper we propose a hybrid system for 

Chinese spelling mistake correction. The n-gram 

based non-word retrieval, abnormal POS tag 

based wrong word detection and dependency 

relation matching based wrong word correction 

are the key techniques of our system. All the 

approaches, linguistic resources and toolkits in-

volved are gathered in Table 4.  

To further improve the performance of our 

system, we will try to extend our work in the 

following aspects: 1) Make full use of the  

184



training data, such as modeling the correct and 

the incorrect syntactic structures of the data; 2) 

Apply semantic collocations to elevate the 

wrong word detection and correction precision. 

Acknowledgments 

Heyan Huang was supported by the National 

Program on Key Basic Research Project (973 

Program) (No.2013CB329303) and the National 

Natural Science Foundation of China (No. 

61132009). Ping Jian was supported by the Na-

tional Natural Science Foundation of China (No. 

61202244). 

References 

Chao-Huang Chang. 1995. A new approach for au-

tomatic Chinese spelling correction. In Proceed-

ings of Natural Language Processing Pacific Rim 

Symposium, pages 278-283, Seoul, Korea. 

Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. 

LTP: A Chinese Language Technology Platform. 

In Proceedings of the Coling 2010: Demonstration 

Volume, pages 13-16, Beijing, China.  

Ping Jian and Chengqing Zong. 2009. Layer-based 

Dependency Parsing. In Proceedings of the 23rd 

Pacific Asia Conference on Language, Information 

and Computation (PACLIC 23), pages 230-239, 

Hong Kong.  

Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, 

Yi-Hsuan Chuang, Shih-Hung Wu, and Chia-Ying 

Lee. 2011. Visually and phonologically similar 

characters in incorrect Chinese words: Analyses, 

identification, and applications. In ACM Transac-

tions on Asian Language Information Processing 

(TALIP), 10(2): 1-39. 

Joseph J. Pollock and Antonio Zamora. 1984. Auto-

matic spelling correction in scientific and scholarly 

text. In Communications of the ACM, 27(4): 

358-368.  

Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee. 

2013. Chinese Spelling Check Evaluation at 

SIGHAN Bake-off 2013. In Proceedings of the 7th 

SIGHAN Workshop on Chinese Language Pro-

cessing, pages 35-42. 

Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 

Palmer. 2005. The Penn Chinese TreeBank: Phrase 

Structure Annotation of a Large Corpus. Natural 

Language Engineering, 11(2): 207-238. 

Lei Zhang, Changning Huang, Ming Zhou, and Hai-

hua Pan. 2000. Automatic detecting/correcting er-

rors in Chinese text by an approximate 

word-matching algorithm. In Proceedings of the 

38th Annual Meeting on Association for Computa-

tional Linguistics, pages 248-254. 

185


