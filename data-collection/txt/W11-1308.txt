










































Detecting Compositionality Using Semantic Vector Space Models Based on Syntactic Context. Shared Task System Description


Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 43–47,

Portland, Oregon, 24 June 2011. c©2011 Association for Computational Linguistics

Detecting compositionality using semantic
vector space models based on syntactic context.

Shared task system description∗

Guillermo Garrido
NLP & IR Group at UNED

Madrid, Spain
ggarrido@lsi.uned.es

Anselmo Peñas
NLP & IR Group at UNED

Madrid, Spain
anselmo@lsi.uned.es

Abstract

This paper reports on the participation of the
NLP GROUP at UNED in the DiSCo’2011
compositionality evaluation task. The aim of
the task is to predict compositionality judge-
ments assigned by human raters to candidate
phrases, in English and German, from three
common grammatical relations: adjective-
noun, subject-verb and subject-object.

Our participation is restricted to adjective-
noun relations in English. We explore the
use of syntactic-based contexts obtained from
large corpora to build classifiers that model
the compositionality of the semantics of such
pairs.

1 Introduction

This paper reports on the NLP GROUP at UNED ’s
participation in DiSCo’2011 Shared Task. We at-
tempt to model the notion of compositionality from
analyzing language use in large corpora. In doing
this, we are assuming the distributional hypothesis:
words that occur in similar contexts tend to have
similar meanings (Harris, 1954). For a review of
the field, see (Turney and Pantel, 2010).

1.1 Approach
In previous approaches to compositionality detec-
tion, different kinds of information have been used:
morphological, lexical, syntactic, and distributional.

∗ This work has been partially supported by the Spanish
Ministry of Science and Innovation, through the project Holo-
pedia (TIN2010-21128-C02), and the Regional Government of
Madrid, through the project MA2VICMR (S2009/TIC1542).

For our participation, we are interested in exploring,
exclusively, the reach of pure syntactic information
to explain semantics.

Our approach draws from the Background
Knowledge Base representation of texts introduced
in (Peñas and Hovy, 2010). We hypothesize that
behind syntactic dependencies in natural language
there are semantic relations; and that syntactic con-
texts can be leveraged to represent meaning, particu-
larly of nouns. A system could learn these semantic
relations from large quantities of natural language
text, to build an independent semantic resource, a
Background Knowledge Base (BKB) (Peñas and
Hovy, 2010).

From a dependency-parsed corpus, we automat-
ically harvest meaning-bearing patterns, matching
the dependency trees to a set of pre-specified syn-
tactic patterns, similarly to (Pado and Lapata, 2007).
Patterns are matched to dependency trees to produce
propositions, carriers of minimal semantic units.
Their frequency in the collection is the fundamen-
tal source of our representation.

Our participation, due to time constraints, is re-
stricted to adjective-noun pairs in English.

2 System Description

Our hypothesis can be spelled out as: words (or
word compounds) with similar syntactic contexts are
semantically similar.

The intuition behind our approach is that non-
compositional compounds are units of meaning.
Then, the meaning of an adjective-noun combina-
tion that is not compositional should be different
from the meaning of the noun alone; for similar

43



approaches, see (Baldwin et al., 2003; Katz and
Giesbrecht, 2006; Mitchell and Lapata, 2010). We
propose studying the distributional semantics of a
adjective-noun compound; in particular, we will rep-
resent it via its syntactic contexts.

2.1 Adjective-noun compounds
Given a particular adjective-noun compound, de-
noted 〈a, n〉, we want to measure its composition-
ality by comparing its syntactic contexts to those of
the noun: 〈n〉. After exploring the dataset we real-
ized that considering nouns alone introduced noise,
as contexts of the target and different meanings of
the noun might be hard to separate; in order to soften
this problem we decided to compare the occurrences
of the 〈a, n〉 pair to those of the noun with a different
adjective.

Given a dependency-parsed corpus C, we denote
N the set of all nouns occurring in C and A the set of
all adjectives. An adjective-noun pair, 〈a, n〉, is an
occurrence in the dependency parse of the sentence
of an arc (a, n), where n is the governor of an adjec-
tival relation, with a as modifier. We define the com-
plementary of 〈a, n〉 as the set of all adjective-noun
pairs with the same noun but a different adjective:

〈ac, n〉 = {〈b, n〉 such that b ∈ A, b 6= a}

In order to detect compositionality, we compare
the semantics of 〈a, n〉 to those of its complemen-
tary 〈ac, n〉. We use syntactic context as the repre-
sentation of these compounds’ semantics.

We call target pairs those 〈a, n〉 in which we are
interested, as they appear in the training, validation,
or test sets for the task. For each of them, its com-
plementary target is: 〈ac, n〉.

We model the syntactic contexts of any 〈a, n〉 pair
as a set of vectors in a set of vector spaces defined as
follows. After inspection of the corpus, and its de-
pendency parse annotation layer, we manually spec-
ified a few syntactic relations, which we consider
codify the relevant syntactic relations in which an
〈a, n〉 takes part. For each of these syntactic rela-
tions, we built a vector space model, and we repre-
sented as a vector in it each of the target patterns,
and each of their respective complementary targets.
To compute compositionality of a target, we calcu-
lated the cosine similarity between the target vec-
tor and the target’s complementary vector. So, for

each syntactic relation, and for each target, we have
a value of its similarity to the complementary tar-
get. These similarity values are considered features,
from which to learn the compositionality of targets.

For results comparability, we used the PukWaC
corpus1 as dataset. PukWaC adds to UkWaC a layer
of syntactic dependency annotation. The corpus has
been POS-tagged and lemmatized with the TreeTag-
ger2. The dependency parse was done with Malt-
Parser (Nivre and Scholz, 2004).

2.2 Implementation details

We defined a set of 19 syntactic patterns that define
interesting relations in which an 〈a, n〉 pair might
take part, trying to exploit the dependencies pro-
duced by the MaltParser (Nivre and Scholz, 2004),
including:
• Relations to a verb, other than the auxiliary to

be and to have: subject; object; indirect object;
subject of a passive construction; logical sub-
ject of a passive construction.
• The relations defined in the previous point, en-

riched with a noun that acts as the other element
of a [subject-verb-object] or [subject-passive
verb-logical subject] construction.
• Collapsed prepositional complexes.
• Noun complexes.
• As subject or object of the verb to be.
• Modified by a second adjective.
• As modifier of a possessive.
The paths were defined manually to match our in-

tuitions of which are the paths that best describe the
context of an 〈a, n〉pair, similarly to (Pado and Lap-
ata, 2007). For each of the patterns, the set of words
that are related through it to the target 〈a, n〉 define
the target’s context.

For most of our processing, we used simple pro-
grams implemented in Prolog and Python. We im-
plemented Prolog programs to model the depen-
dency parsed sentences of the full PUkWaC corpus,
and to match and extract these patterns from them.
After an aggregating step, where proper nouns, num-
bers and dates are substituted by place-holder vari-

1Available at http://wacky.sslmit.unibo.
it

2http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html

44



ables, they amount to over 16 million instances,
representing the syntactic relations in which every
〈a, n〉 pair in the corpus takes part. In further pro-
cessing, only those that affect the target pairs, or the
nouns in them, have to be taken into account.

As described above, each pattern we have defined
yields a vector space, where each target and its com-
plementary are represented as a vector. The base
vectors of the vector space model for a pattern are
the words that are syntactic contexts, with that syn-
tactic pattern, of any target in the target set3.

The value of the coordinate for a target and a base
vector is the frequency of the context word as related
to the target by the pattern. All frequencies were
locally scaled using logarithms4.

For each syntactic pattern, and for each target
and complementary, we have two vectors, represent-
ing their meanings in the vector space distributional
model. The complementary vector, in particular,
represents the centroid (average) of the meanings of
all 〈b, n〉 pairs, that share the noun with the target
but have a different adjective, b

We propose that a target will be more composi-
tional if its meaning is more similar to the meaning
of the centroid of its complementary, that codifies
the general meaning of that noun (whenever it ap-
pears with a different adjective).

For each syntactic pattern and target, we can com-
pute the cosine similarity to the complementary tar-
get, and obtain a value to use as a feature of the com-
positionality of the target. Those features will be
used to train a classifier, being the compositionality
score of each sample the label to be learnt.

We used RapidMiner5 (Mierswa et al., 2006) as
our Machine Learning framework. The classifiers
we have used, that are described below, are the im-
plementations available in RapidMiner.

3It would have been possible to consider a common vector
space, using all patterns as base vectors. We decided not to do
so after realising that a single similarity value for a target and
its complementary was not by itself a signal strong enough to
predict the compositionality score. A second objective was to
assess the relative importance of different syntactic contexts for
the task.

4We did not attempt any global weighting. We leave this for
future work.

5http://rapid-i.com

2.3 Feature selection

From the 19 original features, inspection of the cor-
relation to the compositionality score label showed
that some of them were not to be expected to have
much predictive power, while some of them were
too sparse in the collection.

We decided to perform feature selection previ-
ous to all subsequent learning steps. We used
RapidMiner genetic algorithm for feature selection6.
Among the patterns which features were not selected
were those where the 〈a, n〉 pair appears in prepo-
sitional complexes, in noun complexes, as indirect
object, as subject or object of the verb to be, and as
subject of a possessive. Among those selected were
subject and objects of both active and passive con-
structions, and the object of possessives.

2.4 Runs description

Numeric scores For the numeric evaluation task,
we built a regression model by means of a SVM
classifier. We used RapidMiner’s implementation
of mySVMLearner (Rüping, 2000), that is based on
the optimization algorithm of SVM-light (Joachims,
1998). We used the default parameters for the clas-
sifier. A simple dot product kernel seemed to ob-
tain the best results in 10-fold cross validation over
the union of the provided train and validation re-
sults. For the three runs, we used identical settings,
optimizing different quality measures in each run:
absolute error (RUN SCORE-1), Pearson’s correla-
tion coefficient (RUN SCORE-2), and Spearman’s
rho (RUN SCORE-3). The choice of a SVM classifier
was motivated by the objective of learning a good
parametric classifier model. In initial experiments,
SVM showed to perform better than other possible
choices, like logistic regression. In hindsight, the
relatively small size of the dataset might be a reason
for the relatively poor results. Experimenting with
other approaches is left for future work.

Coarse scores For the coarse scoring, we decided
to build a different set of classifiers, that would learn
the nominal 3-valued compositionality label. The
classifiers built in our initial experiments turned out

6The mutation step switches features on and off, while the
crossover step interchanges used features. Selection is done
randomly. The algorithm used to evaluate each of the feature
subsets was a SVM identical as the one described below.

45



Run avg4 r ρ
RUN-SCORE-1 16.395 0.483 0.487
RUN-SCORE-2 15.874 0.475 0.463
RUN-SCORE-3 16.318 0.494 0.486

baseline 17.857 – –

Table 1: TRAINING. Numeric score runs results on 10-fold
cross-validation for the training set. avg4: average absolute
error; r: Pearson’s correlation;ρ: Spearman’s rho.

Run avg4 r ρ
RUN-SCORE-1 17.016 0.237 0.267
RUN-SCORE-2 17.180 0.217 0.219
RUN-SCORE-3 17.289 0.180 0.189

baseline 17.370 – –

Table 2: TEST. Numeric score runs for the test set. Only
for the en-ADJ-NN samples. avg4: average absolute error; r:
Pearson’s correlation;ρ: Spearman’s rho.

to lazily choose the most frequent class (“high”) for
most of the test samples. In an attempt to overcome
this situation and possibly learn non linearly separa-
ble classes, we tried neural network classifiers7. In
hindsight, from seeing the very poor performance of
this classifiers on the test set, it is clear that any per-
formance gains were due to over-fitting on the train-
ing set.

For RUN COARSE-2, we binned the numeric
scores obtained in RUN-SCORE-1, dividing the score
space in three equal sized parts; we decided not to
assume the same distribution of the three labels for
the training and test sets. The results were worse
than the numeric scores, due to the fact that the 3
classes are not equally sized.

2.5 Results

Results in the training phase For all our training,
we performed 10-fold cross validation. For refer-
ence, we report the results as evaluated by averag-
ing over the 10 splits of the union of the provided
training and validation set in Table 1. We compared
against a dummy baseline: return as constant score
the average of the scores in the training and valida-

7For RUN COARSE-1, we used AutoMLP (Breuel and
Shafait, 2010), an algorithm that learns a neural network, op-
timizing both the learning rate and number of hidden nodes of
the network. For RUN COARSE-3, we learnt a simple neural net-
work model, by means of a feed-forward neural network trained
by a backpropagation algorithm (multi-layer perceptron), with
a hidden layer with sigmoid type and size 8.

tion sample sets.
Disappointingly, the resulting classifiers seemed

to be quite lazy, yielding values significatively close
to the average of the compositionality label in the
training and validation set.

The AutoMNLP and neural network seemed to
perform reasonably, and better than other classifiers
we tried (e.g., SVM based). We were wary, though,
of the risk of having learnt an over-fitted model; un-
fortunately, the results on the test set confirmed that:
for instance, the accuracy of RUN-SCORE-3 for the
training set was 0.548, but for the test set it was only
0.327.

Results in the test phase After the task results
were distributed, we verified that our numeric score
runs, for the subtask en-ADJ-NN performed quite
well: fifth among the 17 valid submissions for the
subtask, using the average point difference as quality
measure. Nevertheless, in terms of ranking correla-
tion scores, our system performs presumably worse,
although separate correlation results for the en-ADJ-
NN subtask were not available to us at the time of
writing this report.

Our naive baseline turns out to be strong in terms
of average point score. Of course, the ranking corre-
lation of such a baseline is none; using ranking cor-
relation as quality measure would be more sensible,
given that it discards such a baseline.

3 Conclusions

We obtained modest results in the task. Our three
numeric runs obtained results very similar to each
other. Only taking part in the en-ADJ-NN subtask,
we obtained the 5th best of a total of 17 valid sys-
tems in average point difference. Nevertheless, in
terms ranking correlation scores, our systems seem
to perform worse. The modifications we tried to spe-
cialize for coarse scoring were unsuccessful, yield-
ing poor results.

A few conclusions we can draw at this moment
are: our system could benefit from global frequency
weighting schemes that we did not try but that have
shown to be successful in the past; the relatively
small size of the dataset has not allowed us to learn a
better classifier; finally, we believe the ranking cor-
relation quality measures are more sensible than the
point difference for this particular task.

46



References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and

Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions: analysis, acquisition and treatment - Volume 18,
MWE ’03, pages 89–96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Thomas Breuel and Faisal Shafait. 2010. Automlp: Sim-
ple, effective, fully automated learning rate and size
adjustment. In The Learning Workshop. Online, 4.

Zellig S. Harris. 1954. Distributional structure. Word,
pages 146–162.

Thorsten Joachims. 1998. Making large-scale svm learn-
ing practical. LS8-Report 24, Universität Dortmund,
LS VIII-Report.

Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
’06, pages 12–19, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin
Scholz, and Timm Euler. 2006. Yale: rapid prototyp-
ing for complex data mining tasks. In KDD’06, pages
935–940.

Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.

Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. COLING ’04.

Sebastian Pado and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models. Com-
putational Linguistics, 33(2):161–199, jun.

Anselmo Peñas and Eduard Hovy. 2010. Semantic en-
richment of text with background knowledge. pages
15–23, jun.

Stefan Rüping. 2000. mySVM-Manual.
http://www-ai.cs.uni-dortmund.de

/SOFTWARE/MYSVM/.
Peter D. Turney and Patrick Pantel. 2010. From fre-

quency to meaning: Vector space models of semantics.
J. Artif. Intell. Res. (JAIR), 37:141–188.

47


