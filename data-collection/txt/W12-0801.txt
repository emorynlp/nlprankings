










































Preservation of Recognizability for Weighted Linear Extended Top-Down Tree Transducers


Proc. EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing, pages 1–10,
Avignon, France, April 24 2012. c©2012 Association for Computational Linguistics

Preservation of Recognizability for
Weighted Linear Extended Top-Down Tree Transducers∗

Nina Seemann and Daniel Quernheim and Fabienne Braune and Andreas Maletti
University of Stuttgart, Institute for Natural Language Processing

{seemanna,daniel,braunefe,maletti}@ims.uni-stuttgart.de

Abstract

An open question in [FÜLÖP, MALETTI,
VOGLER: Weighted extended tree trans-
ducers. Fundamenta Informaticae 111(2),
2011] asks whether weighted linear ex-
tended tree transducers preserve recogniz-
ability in countably complete commuta-
tive semirings. In this contribution, the
question is answered positively, which is
achieved with a construction that utilizes
inside weights. Due to the completeness
of the semiring, the inside weights always
exist, but the construction is only effective
if they can be effectively determined. It is
demonstrated how to achieve this in a num-
ber of important cases.

1 Introduction

Syntax-based statistical machine translation
(Knight, 2007) created renewed interest in tree
automata and tree transducer theory (Fülöp
and Vogler, 2009). In particular, it sparked
research on extended top-down tree transduc-
ers (Graehl et al., 2009), which are top-down
tree transducers (Rounds, 1970; Thatcher, 1970)
in which the left-hand sides can contain several
(or no) input symbols. A recent contribution
by Fülöp et al. (2011) investigates the theoretical
properties of weighted extended tree transduc-
ers over countably complete and commutative
semirings (Hebisch and Weinert, 1998; Golan,
1999). Such semirings permit sums of countably
many summands, which still obey the usual
associativity, commutativity, and distributivity
laws. We will use the same class of semirings.

∗ All authors were financially supported by the EMMY
NOETHER project MA / 4959 / 1-1 of the German Research
Foundation (DFG).

Input→ Parser → TM → LM → Output

Figure 1: Syntax-based machine translation pipeline.

Extended top-down tree transducers are used as
translation models (TM) in syntax-based machine
translation. In the standard pipeline (see Figure 1;
LM is short for language model) the translation
model is applied to the parses of the input sen-
tence, which can be represented as a recogniz-
able weighted forest (Fülöp and Vogler, 2009).
In practice, only the best or the n-best parses are
used, but in principle, we can use the recogniz-
able weighted forest of all parses. In either case,
the translation model transforms the input trees
into a weighted forest of translated output trees.
A class of transducers preserves recognizability
if for every transducer of the class and each rec-
ognizable weighted forest, this weighted forest
of translated output trees is again recognizable.
Fülöp et al. (2011) investigates which extended
top-down tree transducers preserve recognizabil-
ity under forward (i.e., the setting previously de-
scribed) and backward application (i.e., the set-
ting, in which we start with the output trees and
apply the inverse of the translation model), but the
question remained open for forward application
of weighted linear extended top-down tree trans-
ducers [see Table 1 for an overview of the exist-
ing results for forward application due to Engel-
friet (1975) in the unweighted case and Fülöp et
al. (2010) and Fülöp et al. (2011) for the weighted
case]. In conclusion, Fülöp et al. (2011) ask: “Are
there a commutative semiring S that is count-
ably complete wrt.

∑
, a linear wxttM [weighted

extended top-down tree transducer with regular
look-ahead; see Section 4], and a recognizable

1



model preserves regularity

unweighted

ln-XTOP 3
l-XTOP 3
l-XTOPR 3
XTOP 7

weighted

ln-XTOP 3
l-XTOP 3
l-XTOPR 3
XTOP 7

Table 1: Overview of the known results due to Engel-
friet (1975) and Fülöp et al. (2011) and our results in
boxes.

weighted tree language ϕ such that M(ϕ) [for-
ward application] is not recognizable? Or even
harder, are there S and M with the same prop-
erties such that M(1̃) [1̃ is the weighted forest
in which each tree has weight 1] is not recogniz-
able?”

In this contribution, we thus investigate preser-
vation of recognizability (under forward applica-
tion) for linear extended top-down tree transduc-
ers with regular look-ahead (Engelfriet, 1977),
which are equivalent to linear weighted extended
tree transducers by Fülöp et al. (2011). We show
that they always preserve recognizability, thus
confirming the implicit hypothesis of Fülöp et al.
(2011). The essential tool for our construction is
the inside weight (Lari and Young, 1990; Graehl
et al., 2008) of the states of the weighted tree
grammar (Alexandrakis and Bozapalidis, 1987)
representing the parses. The inside weight of a
state q is the sum of all weights of trees accepted
in this state. In our main construction (see Sec-
tion 5) we first compose the input weighted tree
grammar with the transducer (input restriction).
This is particularly simple since we just abuse
the look-ahead of the initial rules. In a second
step, we normalize the obtained transducer, which
yields the standard product construction typically
used for input restriction. Finally, we project to
the output by basically eliminating the left-hand
sides. In this step, the inside weights of states
belonging to deleted subtrees are multiplied to
the production weight. Due to the completeness
of the semiring, the inside weights always ex-
ist, but the infinite sums have to be computed ef-
fectively for the final step of the construction to

be effective. This problem is addressed in Sec-
tion 6, where we show several methods to effec-
tively compute or approximate the inside weights
for all states of a weighted tree grammar.

2 Notation

Our weights will be taken from a commuta-
tive semiring (A,+, ·, 0, 1), which is an algebraic
structure of two commutative monoids (A,+, 0)
and (A, ·, 1) such that · distributes over + and
0 · a = 0 for all a ∈ A. An infinitary sum opera-
tion

∑
is a family (

∑
I)I where I is a countable

index set and
∑

I : A
I → A. Given f : I → A,

we write
∑

i∈I f(i) instead of
∑

I f . The semi-
ring together with the infinitary sum operation

∑
is countably complete (Eilenberg, 1974; Hebisch
and Weinert, 1998; Golan, 1999; Karner, 2004) if
for all countable sets I and ai ∈ A with i ∈ I
•
∑

i∈I ai = am + an if I = {m,n},
•
∑

i∈I ai =
∑

j∈J
(∑

i∈Ij ai
)

if I =
⋃
j∈J Ij

for countable sets J and Ij with j ∈ J such
that Ij ∩ Ij′ = ∅ for all different j, j′ ∈ J ,
and
• a ·

(∑
i∈I ai

)
=
∑

i∈I(a · ai) for all a ∈ A.
For such a semiring, we let a∗ =

∑
i∈N a

i for
every a ∈ A. In the following, we assume that
(A,+, ·, 0, 1) is a commutative semiring that is
countably complete with respect to

∑
.

Our trees have node labels taken from an al-
phabet Σ and leaves might also be labeled by el-
ements of a set V . Given a set T , we write Σ(T )
for the set

{σ(t1, . . . , tk) | k ∈ N, σ ∈ Σ, t1, . . . , tk ∈ T} .

The set TΣ(V ) of Σ-trees with V -leaves is defined
as the smallest set T such that V ∪ Σ(T ) ⊆ T .
We write TΣ for TΣ(∅). For each tree t ∈ TΣ(V )
we identify nodes by positions. The root of t has
position ε and the position iw with i ∈ N and
w ∈ N∗ addresses the position w in the i-th di-
rect subtree at the root. The set of all positions
in t is pos(t). We write t(w) for the label (taken
from Σ ∪ V ) of t at position w ∈ pos(t). Sim-
ilarly, we use t|w to address the subtree of t that
is rooted in position w, and t[u]w to represent the
tree that is obtained from replacing the subtree t|w
at w by u ∈ TΣ(V ). For a given set L ⊆ Σ ∪ V
of labels, we let

posL(t) = {w ∈ pos(t) | t(w) ∈ L}

2



be the set of all positions whose label belongs
to L. We also write posl(t) instead of pos{l}(t).

We often use the set X = {x1, x2, . . . } of vari-
ables and its finite subsets Xk = {x1, . . . , xk}
for every k ∈ N to label leaves. Let V
be a set potentially containing some variables
of X . The tree t ∈ TΣ(V ) is linear if
|posx(t)| ≤ 1 for every x ∈ X . Moreover,
var(t) = {x ∈ X | posx(t) 6= ∅} collects all
variables that occur in t. Given a finite set Q and
T ⊆ TΣ(V ), we let

Q[T ] = {q(t) | q ∈ Q, t ∈ T} .

We will treat elements ofQ[T ] (in which elements
ofQ are always used as unary symbols) as special
trees of TΣ∪Q(V ). A substitution θ is a mapping
θ : X → TΣ(V ). When applied to t ∈ TΣ(V ),
it returns the tree tθ, which is obtained from t
by replacing all occurrences of x ∈ X (in par-
allel) by θ(x). This can be defined recursively
by xθ = θ(x) for all x ∈ X , vθ = v for all
v ∈ V \X , and σ(t1, . . . , tk)θ = σ(t1θ, . . . , tkθ)
for all σ ∈ Σ and t1, . . . , tk ∈ TΣ(V ).

3 Weighted Tree Grammars

In this section, we will recall weighted tree
grammars (Alexandrakis and Bozapalidis, 1987)
[see (Fülöp and Vogler, 2009) for a modern treat-
ment and a complete historical account]. In gen-
eral, weighted tree grammars (WTGs) offer an ef-
ficient representation of weighted forests, which
are sets of trees such that each individual tree
is equipped with a weight. The representation
is even more efficient than packed forests (Mi et
al., 2008) and moreover can represent an infinite
number of weighted trees. To avoid confusion
between the nonterminals of a parser, which pro-
duces the forests considered here, and our WTGs,
we will refer to the nonterminals of our WTG as
states.

Definition 1. A weighted tree grammar (WTG) is
a system (Q,Σ, q0, P ) where
• Q is a finite set of states (nonterminals),
• Σ is the alphabet of symbols,
• q0 ∈ Q is the starting state, and
• P is a finite set of productions q a→ t, where
q ∈ Q, a ∈ A, and t ∈ TΣ(Q).

Example 2. We illustrate our notation on the
WTG Gex = (Q,Σ, qs, P ) where
• Q = {qs, qnp, qprp, qn, qadj},

• Σ contains “S”, “NP”, “VP”, “PP”, “DT”,
“NN”, “N”, “VBD”, “PRP”, “ADJ”, “man”,
“hill”, “telescope”, “laughs”, “the”, “on”,
“with”, “old”, and “young”, and
• P contains the productions

qs
1.0→ S(qnp,VP(VBD(laughs))) (ρ1)

qnp
0.4→ NP(qnp,PP(qprp, qnp))

qnp
0.6→ NP(DT(the), qn) (ρ2)

qprp
0.5→ PRP(on)

qprp
0.5→ PRP(with)

qn
0.3→ N(qadj , qn)

qn
0.3→ NN(man) (ρ3)

qn
0.2→ NN(hill)

qn
0.2→ NN(telescope)

qadj
0.5→ ADJ(old)

qadj
0.5→ ADJ(young)

It produces a weighted forest representing sen-
tences about young and old men with telescopes
on hills.

In the following, let G = (Q,Σ, q0, P ) be a
WTG. For every production ρ = q a→ t in P , we
let wtG(ρ) = a. The semantics of G is defined
with the help of derivations. Let ξ ∈ TΣ(Q) be
a sentential form, and let w ∈ posQ(ξ) be such
that w is the lexicographically smallest Q-labeled
position in ξ. Then ξ ⇒ρG ξ[t]w if ξ(w) = q. For
a sequence ρ1, . . . , ρn ∈ P of productions, we
let wtG(ρ1 · · · ρn) =

∏n
i=1 wtG(ρi). For every

q ∈ Q and t ∈ TΣ(Q), we let

wtG(q, t) =
∑

ρ1,...,ρn∈P
q⇒ρ1G ···⇒

ρn
G t

wtG(ρ1 · · · ρn) .

The WTG G computes the weighted forest
LG : TΣ → A such that LG(t) = wtG(q0, t) for
every t ∈ TΣ. Two WTGs are equivalent if they
compute the same weighted forest. Since produc-
tions of weight 0 are useless, we often omit them.

Example 3. For the WTG Gex of Example 2 we
display a derivation with weight 0.18 for the sen-
tence “the man laughs” in Figure 2.

The notion of inside weights (Lari and Young,
1990) is well-established, and Maletti and Satta

3



qs ⇒ρ1G

S

qnp VP

VBD

laughs

⇒ρ2G

S

NP

DT

the

qn

VP

VBD

laughs

⇒ρ3G

S

NP

DT

the

NN

man

VP

VBD

laughs

Figure 2: Derivation with weight 1.0 · 0.6 · 0.3.

(2009) consider them for WTGs. Let us recall the
definition.

Definition 4. The inside weight of state q ∈ Q is

inG(q) =
∑
t∈TΣ

wtG(q, t) .

In Section 6 we demonstrate how to compute
inside weights. Finally, let us introduce WTGs in
normal form. The WTG G is in normal form if
t ∈ Σ(Q) for all its productions q a→ t in P . The
following theorem was proven by Alexandrakis
and Bozapalidis (1987) as Proposition 1.2.

Theorem 5. For every WTG there exists an
equivalent WTG in normal form.

Example 6. The WTG Gex of Example 2 is not
normalized. To illustrate the normalization step,
we show the normalization of the production ρ2,
which is replaced by the following three produc-
tions:

qnp
0.6→ NP(qdt, qn) qdt

1.0→ DT(qt)

qt
1.0→ the .

4 Weighted linear extended tree
transducers

The model discussed in this contribution is an ex-
tension of the classical top-down tree transducer,
which was introduced by Rounds (1970) and
Thatcher (1970). Here we consider a weighted
and extended variant that additionally has regular
look-ahead. The weighted top-down tree trans-
ducer is discussed in (Fülöp and Vogler, 2009),
and extended top-down tree transducers were
studied in (Arnold and Dauchet, 1982; Knight and

Graehl, 2005; Knight, 2007; Graehl et al., 2008;
Graehl et al., 2009). The combination (weighted
extended top-down tree transducer) was recently
investigated by Fülöp et al. (2011), who also con-
sidered (weighted) regular look-ahead, which was
first introduced by Engelfriet (1977) in the un-
weighted setting.

Definition 7. A linear extended top-down
tree transducer with full regular look-ahead
(l-XTOPRf ) is a system (S,Σ,∆, s0, G,R) where
• S is a finite set of states,
• Σ and ∆ are alphabets of input and output

symbols, respectively,
• s0 ∈ S is an initial state,
• G = (Q,Σ, q0, P ) is a WTG, and
• R is a finite set of weighted rules of the form
`
a→µ r where
– a ∈ A is the rule weight,
– ` ∈ S[TΣ(X)] is the linear left-hand

side,
– µ : var(`)→ Q is the look-ahead, and
– r ∈ T∆(S[var(`)]) is the linear right-

hand side.

In the following, let M = (S,Σ,∆, s0, G,R)
be an l-XTOPRf . We assume that the WTG G
contains a state > such that wtG(>, t) = 1 for
every t ∈ TΣ. In essence, this state represents
the trivial look-ahead. If µ(x) = > for every
rule ` a→µ r ∈ R and x ∈ var(r) (respectively,
x ∈ var(`)), then M is an l-XTOPR (respectively,
l-XTOP). l-XTOPR and l-XTOP coincide exactly
with the models of Fülöp et al. (2011), and in the
latter model we drop the look-ahead component µ
and the WTG G completely.

Example 8. The rules of our running example
l-XTOP Mex (over the input and output alpha-
bet Σ, which is also used by the WTG Gex of Ex-
ample 2) are displayed in Figure 3.

Next, we present the semantics. Without loss
of generality, we assume that we can distin-
guish states from input and output symbols (i.e.,
S ∩ (Σ ∪ ∆) = ∅). A sentential form of M is a
tree of SF(M) = T∆(Q[TΣ]). Let ρ = `

a→µ r be
a rule of R. Moreover, let ξ, ζ ∈ SF(M) be sen-
tential forms and w ∈ N∗ be the lexicographically
smallest position in posQ(ξ). We write ξ

b⇒M,ρ ζ
if there exists a substitution θ : X → TΣ such that
• ξ = ξ[`θ]w,
• ζ = ξ[rθ]w, and
• b = a ·

∏
x∈var(`) wtG(µ(x), θ(x)).

4



s0

S

NP

x1 x2

VP

x3

→ 0.6

S

NP

s1

x1

s2

x2

VP

s3

x3

∣∣∣ 0.4
S

s1

x1

VP

s3

x3

s2

N

ADJ

x1

x2
→ 0.7

N

ADJ

s5

x1

s2

x2

∣∣∣ 0.3 s2
x2

s1

NP

x1 x2

→ 0.5

NP

s1

x1

s2

x2

∣∣∣ 0.5 s1
x1

s1

DT

the

→ 1.0
DT

the

s3

VBD

laughs

→ 1.0
VBD

laughs

s2

PP

x1 x2

→ 1.0

PP

s4

x1

s1

x2

s2

NN

man /
hill /

telescope

→ 1.0

NN

man /
hill /

telescope

s4

PRP

on /
with

→ 1.0
PRP

on /
with

Figure 3: Example rules of an l-XTOP. We collapsed rules with the same left-hand side as well as several lexical
items to save space.

s0

S

NP

NP

DT

the

NN

man

PP

PRP

on

NP

DT

the

NN

hill

VP

VBD

laughs

0.4⇒M

S

s1

NP

DT

the

NN

man

VP

s3

VBD

laughs

0.5⇒M

S

NP

s1

DT

the

s2

NN

man

VP

s3

VBD

laughs

⇒∗M

S

NP

DT

the

NN

man

VP

VBD

laughs

Figure 4: Derivation with weight 0.4 · 0.5 · 1.0 (rules omitted).

The tree transformation τM computed byM is de-
fined by

τM (t, u) =
∑

ρ1,...,ρn∈R
s0(t)

a1⇒M,ρ1 ···
an⇒M,ρnu

a1 · . . . · an

for every t ∈ TΣ and u ∈ T∆.
Example 9. A sequence of derivation steps of the
l-XTOP Mex is illustrated in Figure 4. The trans-
formation it computes is capable of deleting the
PP child of every NP-node with probability 0.4 as
well as deleting the ADJ child of every N-node
with probability 0.3.

A detailed exposition to unweighted l-XTOPR

is presented by Arnold and Dauchet (1982) and
Graehl et al. (2009).

5 The construction

In this section, we present the main construction
of this contribution, in which we will construct a

WTG for the forward application of another WTG
via an l-XTOPR. Let us first introduce the main
notions. Let L : TΣ → A be a weighted forest
and τ : TΣ×T∆ → A be a weighted tree transfor-
mation. Then the forward application of L via τ
yields the weighted forest τ(L) : T∆ → A such
that (τ(L))(u) =

∑
t∈TΣ L(t) · τ(t, u) for ev-

ery u ∈ T∆. In other words, to compute the
weight of u in τ(L), we consider all input trees t
and multiply their weight in L with their trans-
lation weight to u. The sum of all those prod-
ucts yields the weight for u in τ(L). In the par-
ticular setting considered in this contribution, the
weighted forest L is computed by a WTG and the
weighted tree transformation τ is computed by an
l-XTOPR. The question is whether the resulting
weighted forest τ(L) can be computed by a WTG.

Our approach to answer this question con-
sists of three steps: (i) composition, (ii) nor-
malization, and (iii) range projection, which
we address in separate sections. Our input is

5



qs ⇒
S

qnp qvp
⇒

S

NP

qnp qpp

qvp ⇒2

S

NP

qnp qpp

VP

VBD

qv

qs ⇒
S

qnp qvp
⇒

S

NP

qdt qn

qvp ⇒2

S

NP

qdt qn

VP

VBD

qv

Figure 5: Two derivations (without production and
grammar decoration) with weight 0.4 [top] and
0.6 [bottom] of the normalized version of the
WTG Gex (see Example 10).

the WTG G′ = (Q′,Σ, q′0, P
′), which com-

putes the weighted forest L = LG′ , and
the l-XTOPR M = (S,Σ,∆, s0, G,R) with
G = (Q,Σ, q0, P ), which computes the weighted
tree transformation τ = τM . Without loss of gen-
erality, we suppose thatG andG′ contain a special
state > such that wtG(>, t) = wtG′(>, t) = 1
for all t ∈ TΣ. Moreover, we assume that the
WTG G′ is in normal form. Finally, we assume
that s0 is separated, which means that the initial
state of M does not occur in any right-hand side.
Our example l-XTOP Mex has this property. All
these restrictions can be assumed without loss of
generality. Finally, for every state s ∈ S, we let

Rs = {`
a→µ r ∈ R | `(ε) = s} .

5.1 Composition
We combine the WTG G′ and the l-XTOPR M
into a single l-XTOPRf M

′ that computes

τM ′(t, u) = LG′(t) · τM (t, u) = L(t) · τ(t, u)

for every t ∈ TΣ and u ∈ T∆. To this end, we
construct

M ′ = (S,Σ,∆, s0, G×G′, (R \Rs0) ∪R′)

such that G × G′ is the classical product WTG
[see Proposition 5.1 of (Berstel and Reutenauer,
1982)] and for every rule ` a→µ r in Rs0 and
θ : var(`)→ Q′, the rule

`
a·wtG′ (q′0,`θ)−−−−−−−−→µ′ r

is in R′, where µ′(x) = 〈µ(x), θ(x)〉 for every
x ∈ var(`).

Example 10. Let us illustrate the construction on
the WTG Gex of Example 2 and the l-XTOP Mex
of Example 8. According to our assumptions,
Gex should first be normalized (see Theorem 5).
We have two rules in Rs0 and they have the same
left-hand side `. It can be determined easily that
wtG′ex(qs, `θ) 6= 0 only if
• θ(x1)θ(x2)θ(x3) = qnpqppqv or
• θ(x1)θ(x2)θ(x3) = qdtqnqv.

Figure 5 shows the two corresponding derivations
and their weights. Thus, the s0-rules are replaced
by the 4 rules displayed in Figure 6.

Theorem 11. For every t ∈ TΣ and u ∈ T∆, we
have τM ′(t, u) = L(t) · τ(t, u).

Proof. We prove an intermediate property for
each derivation of M . Let

s0(t)
b1⇒M,ρ1 · · ·

bn⇒M,ρn u

be a derivation of M . Let ρ1 = `
a1→µ r be the

first rule, which trivially must be in Rs0 . Then for
every θ : var(`)→ Q′, there exists a derivation

s0(t)
c1⇒M ′,ρ′1 ξ2

b2⇒M ′,ρ2 · · ·
bn⇒M ′,ρn u

in M ′ such that

c1 = b1·wtG′(q′0, `θ)·
∏

x∈var(`)

wtG′(θ(x), θ′(x)) ,

where θ′ : var(`) → TΣ is such that t = `θ′.
Since we sum over all such derivations and∑
θ : var(`)→Q′

wtG′(q′0, `θ) ·
∏

x∈var(`)

wtG′(θ(x), θ′(x))

= wtG′(q′0, t) = LG′(t)

by a straightforward extension of Lemma 4.1.8
of (Borchardt, 2005), we obtain that the deriva-
tions in M ′ sum to LG′(t) · b1 · . . . · bn as desired.
The main property follows trivially from the in-
termediate result.

5.2 Normalization
Currently, the weights of the input WTG are
only on the initial rules and in its look-ahead.
Next, we use essentially the same method as
in the previous section to remove the look-
ahead from all variables that are not deleted.
Let M ′ = (S,Σ,∆, s0, G × G′, R) be the
l-XTOPRf constructed in the previous section and

6



s0

S

NP

x1 x2

VP

x3

→µ

0.6 · c

S

NP

s1

x1

s2

x2

VP

s3

x3

∣∣∣
0.4 · c

S

s1

x1

VP

s3

x3

Figure 6: 4 new l-XTOPRf rules, where µ and c are
either (i) µ(x1)µ(x2)µ(x3) = qnpqppqv and c = 0.4
or (ii) µ(x1)µ(x2)µ(x3) = qdtqnqv and c = 0.6 (see
Example 10).

s0

S

NP

x1 x2

VP

x3

→µ

0.4 · 0.4

S

〈s1, qnp〉

x1

VP

〈s3, qv〉

x3

∣∣∣
0.4 · 0.6

S

〈s1, qdt〉

x1

VP

〈s3, qv〉

x3

Figure 7: New l-XTOPR rules, where µ(x2) = qpp
[left] and µ(x2) = qn [right] (see Figure 6).

ρ = ` a→µ r ∈ R be a rule with µ(x) = 〈>, q′〉
for some q′ ∈ Q′ \ {>} and x ∈ var(r). Note
that µ(x) = 〈>, q′〉 for some q′ ∈ Q′ for all
x ∈ var(r) since M is an l-XTOPR. Then we
construct the l-XTOPRf M

′′

(S ∪ S ×Q′,Σ,∆, s0, G×G′, (R \ {ρ}) ∪R′)

such that R′ contains the rule ` a→µ′ r′, where

µ′(x′) =

{
〈>,>〉 if x = x′

µ(x′) otherwise

for all x′ ∈ var(`) and r′ is obtained from r by re-
placing the subtree s(x) with s ∈ S by 〈s, q′〉(x).
Additionally, for every rule `′′ a

′′
→µ′′ r′′ in Rs and

θ : var(`′′)→ Q′, the rule

`′′
a′′·wtG′ (q′,`′′θ)−−−−−−−−−→µ′′′ r′′

is in R′, where µ′′′(x) = 〈µ′′(x), θ(x)〉 for ev-
ery x ∈ var(`). This procedure is iterated until
we obtain an l-XTOPR M ′′. Clearly, the iteration
must terminate since we do not change the rule
shape, which yields that the size of the potential
rule set is bounded.

Theorem 12. The l-XTOPR M ′′ and the
l-XTOPRf M

′ are equivalent.

〈s2, qn〉

N

ADJ

x1

x2

→µ

0.32 · 0.5

〈s2, qn〉

x2

〈s1, qnp〉

NP

x1 x2

→µ′|µ′′
0.5 · 0.4

〈s1, qnp〉

x1

∣∣∣
0.5 · 0.6

〈s1, qdt〉

x1

Figure 8: New l-XTOPR rules, where µ(x1) is either
qold or qyoung , µ′(x2) = qpp, and µ′′(x2) = qn.

Proof. It can be proved that the l-XTOPRf con-
structed after each iteration is equivalent to its
input l-XTOPRf in the same fashion as in Theo-
rem 11 with the only difference that the rule re-
placement now occurs anywhere in the derivation
(not necessarily at the beginning) and potentially
several times. Consequently, the finally obtained
l-XTOPR M ′′ is equivalent to M ′.

Example 13. Let us reconsider the l-XTOPRf con-
structed in the previous section and apply the nor-
malization step. The interesting rules (i.e., those
rules l a→µ r where var(r) 6= var(l)) are dis-
played in Figures 7 and 8.

5.3 Range projection
We now have an l-XTOPR M ′′ with rules R′′

computing τM ′′(t, u) = L(t) · τ(t, u). In the fi-
nal step, we simply disregard the input and project
to the output. Formally, we want to construct a
WTG G′′ such that

LG′′(u) =
∑
t∈TΣ

τM ′′(t, u) =
∑
t∈TΣ

L(t) · τ(t, u)

for every u ∈ T∆. Let us suppose that G is the
WTG inside M ′′. Recall that the inside weight of
state q ∈ Q is

inG(q) =
∑
t∈TΣ

wtG(q, t) .

We construct the WTG

G′′ = (S ∪ S ×Q′,∆, s0, P ′′)

such that `(ε) c→ r′ is in P ′′ for every rule
`
a→µ r ∈ R′′, where

c = a ·
∏

x∈var(`)\var(r)

inG(µ(x))

and r′ is obtained from r by removing the vari-
ables of X . If the same production is constructed
from several rules, then we add the weights. Note
that the WTG G′′ can be effectively computed if
inG(q) is computable for every state q.

7



qs qprp

qnp qn qadj

Figure 9: Dependency graph of the WTG Gex.

Theorem 14. For every u ∈ T∆, we have

LG′′(u) =
∑
t∈TΣ

L(t) · τ(t, u) = (τ(L))(u) .

Example 15. The WTG productions for the rules
of Figures 7 and 8 are

s0
0.4·0.4→ S(〈s1, qnp〉,VP(〈s3, qv〉))

s0
0.4·0.6→ S(〈s1, qdt〉,VP(〈s3, qv〉))

〈s2, qn〉
0.3·0.3→ 〈s2, qn〉

〈s1, qnp〉
0.5·0.4→ 〈s1, qnp〉

〈s1, qnp〉
0.5·0.6→ 〈s1, qdt〉 .

Note that all inside weights are 1 in our exam-
ple. The first production uses the inside weight
of qpp, whereas the second production uses the in-
side weight of qn. Note that the third production
can be constructed twice.

6 Computation of inside weights

In this section, we address how to effectively com-
pute the inside weight for every state. If the WTG
G = (Q,Σ, q0, P ) permits only finitely many
derivations, then for every q ∈ Q, the inside
weight inG(q) can be computed according to Def-
inition 4 because wtG(q, t) = 0 for almost all
t ∈ TΣ. If P contains (useful) recursive rules,
then this approach does not work anymore. Our
WTG Gex of Example 2 has the following two re-
cursive rules:

qnp
0.4→ NP(qnp,PP(qprp, qnp)) (ρ4)

qn
0.3→ N(qadj , qn) . (ρ5)

The dependency graph of Gex, which is shown in
Figure 9, has cycles, which yields that Gex per-
mits infinitely many derivations. Due to the com-
pleteness of the semiring, even the infinite sum of
Definition 4 is well-defined, but we still have to
compute it. We will present two simple methods
to achieve this: (a) an analytic method and (b) an
approximation in the next sections.

6.1 Analytic computation
In simple cases we can compute the inside weight
using the stars a∗, which we defined in Section 2.
Let us first list some interesting countably com-
plete semirings for NLP applications and their
corresponding stars.

• Probabilities: (R∞≥0,+, ·, 0, 1) where R∞≥0
contains all nonnegative real numbers
and ∞, which is bigger than every real
number. For every a ∈ R∞≥0 we have

a∗ =

{
1

1−a if 0 ≤ a < 1
∞ otherwise

• VITERBI: ([0, 1],max, ·, 0, 1) where [0, 1] is
the (inclusive) interval of real numbers be-
tween 0 and 1. For every 0 ≤ a ≤ 1 we have
a∗ = 1.

• Tropical: (R∞≥0,min,+,∞, 0) where
a∗ = 0 for every a ∈ R∞≥0.

• Tree unification: (2TΣ(X1),∪,t, ∅, {x1})
where 2TΣ(X1) = {L | L ⊆ TΣ(X1)} and
t is unification (where different occurrences
of x1 can be replaced differently) extended
to sets as usual. For every L ⊆ TΣ(Xk) we
have L∗ = {x1} ∪ (L t L).

We can always try to develop a regular expres-
sion (Fülöp and Vogler, 2009) for the weighted
forest recognized by a certain state, in which we
then can drop the actual trees and only compute
with the weights. This is particularly easy if our
WTG has only left- or right-recursive productions
because in this case we obtain classical regular
expressions (for strings). Let us consider produc-
tion ρ5. It is right-recursive. On the string level,
we obtain the following unweighted regular ex-
pression for the string language generated by qn:

L(qadj)∗(man | hill | telescope)

where L(qadj) = {old, young} is the set of strings
generated by qadj . Correspondingly, we can de-
rive the inside weight by replacing the generated
string with the weights used to derive them. For
example, the production ρ5, which generates the
state qadj , has weight 0.3. We obtain the expres-
sion

inG(qn) = (0.3 · inG(qadj))∗ · (0.3 + 0.2 + 0.2) .

8



Example 16. If we calculate in the probability
semiring and inG(qadj) = 1, then

inG(qn) =
1

1− 0.3
· (0.3 + 0.2 + 0.2) = 1 ,

as expected (since our productions induce a prob-
ability distribution on all trees generated from
each state).

Example 17. If we calculate in the tropical semi-
ring, then we obtain

inG(qn) = min(0.3, 0.2, 0.2) = 0.2 .

It should be stressed that this method only
allows us to compute inG(q) in very simple
cases (e.g., WTG containing only left- or right-
recursive productions). The production ρ4 has
a more complicated recursion, so this simple
method cannot be used for our full example WTG.

However, for extremal semirings the inside
weight always coincides with a particular deriva-
tion. Let us also recall this result. The semiring is
extremal if a+ a′ ∈ {a, a′} for all a, a′ ∈ A. The
VITERBI and the tropical semiring are extremal.
Recall that

inG(q) =
∑
t∈TΣ

wtG(q, t)

=
∑
t∈TΣ

∑
ρ1,...,ρn∈P
q⇒ρ1G ···⇒

ρn
G t

wtG(ρ1 · · · ρn) ,

which yields that inG(q) coincides with the
derivation weight wtG(ρ1 · · · ρn) of some deriva-
tion q ⇒ρ1G · · · ⇒

ρn
G t for some t ∈ TΣ. In

the VITERBI semiring this is the highest scor-
ing derivation and in the tropical semiring it is
the lowest scoring derivation (mind that in the
VITERBI semiring the production weights are
multiplied in a derivation, whereas they are added
in the tropical semiring). There are efficient algo-
rithms (Viterbi, 1967) that compute those deriva-
tions and their weights.

6.2 Numerical Approximation
Next, we show how to obtain a numerical ap-
proximation of the inside weights (up to any
desired precision) in the probability semiring,
which is the most important of all semirings
discussed here. A similar approach was used
by Stolcke (1995) for context-free grammars. To
keep the presentation simple, let us suppose that

G = (Q,Σ, q0, P ) is in normal form (see The-
orem 5). The method works just as well in the
general case.

We first observe an important property of the
inside weights. For every state q ∈ Q

inG(q) =
∑

q
a→σ(q1,...,qn)∈P

a · inG(q1) · . . . · inG(qn) ,

which can trivially be understood as a system of
equations (where each inG(q) with q ∈ Q is a
variable). Since there is one such equation for
each variable inG(q) with q ∈ Q, we have a
system of |Q| non-linear polynomial equations in
|Q| variables.

Several methods to solve non-linear systems of
equations are known in the numerical calculus lit-
erature. For example, the NEWTON-RAPHSON
method allows us to iteratively compute the roots
of any differentiable real-valued function, which
can be used to solve our system of equations be-
cause we can compute the JACOBI matrix for our
system of equations easily. Given a good starting
point, the NEWTON-RAPHSON method assures
quadratic convergence to a root. A good start-
ing point can be obtained, for example, by bisec-
tion (Corliss, 1977). Another popular root-finding
approximation is described by Brent (1973).

Example 18. For the WTG of Example 2 we ob-
tain the following system of equations:

inG(qs) = 1.0 · inG(qnp)

inG(qnp) = 0.4 · inG(qnp) · inG(qprp) · inG(qnp)
+ 0.6 · inG(qn)

inG(qn) = 0.3 · inG(qadj) · inG(qn)
+ 0.3 + 0.2 + 0.2

inG(qadj) = 0.5 + 0.5

inG(qprp) = 0.5 + 0.5 .

Together with inG(qn) = 1, which we already
calculated in Example 16, the only interesting
value is

inG(qs) = inG(qnp) = 0.4 · inG(qnp)2 + 0.6 ,

which yields the roots inG(qnp) = 1 and
inG(qnp) = 1.5. The former is the desired solu-
tion. As before, this is the expected solution.

9



References
Athanasios Alexandrakis and Symeon Bozapalidis.

1987. Weighted grammars and Kleene’s theorem.
Inf. Process. Lett., 24(1):1–4.

André Arnold and Max Dauchet. 1982. Morphismes
et bimorphismes d’arbres. Theoret. Comput. Sci.,
20(1):33–93.

Jean Berstel and Christophe Reutenauer. 1982. Rec-
ognizable formal power series on trees. Theoret.
Comput. Sci., 18(2):115–148.

Björn Borchardt. 2005. The Theory of Recognizable
Tree Series. Ph.D. thesis, Technische Universität
Dresden.

Richard P. Brent. 1973. Algorithms for Minimization
without Derivatives. Series in Automatic Computa-
tion. Prentice Hall, Englewood Cliffs, NJ, USA.

George Corliss. 1977. Which root does the bisection
algorithm find? SIAM Review, 19(2):325–327.

Samuel Eilenberg. 1974. Automata, Languages, and
Machines — Volume A, volume 59 of Pure and Ap-
plied Math. Academic Press.

Joost Engelfriet. 1975. Bottom-up and top-down tree
transformations — a comparison. Math. Systems
Theory, 9(3):198–231.

Joost Engelfriet. 1977. Top-down tree transducers
with regular look-ahead. Math. Systems Theory,
10(1):289–303.

Zoltán Fülöp and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Hand-
book of Weighted Automata, EATCS Monographs
on Theoret. Comput. Sci., chapter 9, pages 313–
403. Springer.

Zoltán Fülöp, Andreas Maletti, and Heiko Vogler.
2010. Preservation of recognizability for syn-
chronous tree substitution grammars. In Proc. 1st
Workshop Applications of Tree Automata in Natu-
ral Language Processing, pages 1–9. Association
for Computational Linguistics.

Zoltán Fülöp, Andreas Maletti, and Heiko Vogler.
2011. Weighted extended tree transducers. Fun-
dam. Inform., 111(2):163–202.

Jonathan S. Golan. 1999. Semirings and their Appli-
cations. Kluwer Academic, Dordrecht.

Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391–427.

Jonathan Graehl, Mark Hopkins, Kevin Knight, and
Andreas Maletti. 2009. The power of extended
top-down tree transducers. SIAM J. Comput.,
39(2):410–430.

Udo Hebisch and Hanns J. Weinert. 1998. Semirings
— Algebraic Theory and Applications in Computer
Science. World Scientific.

Georg Karner. 2004. Continuous monoids and semi-
rings. Theoret. Comput. Sci., 318(3):355–372.

Kevin Knight and Jonathan Graehl. 2005. An over-
view of probabilistic tree transducers for natural
language processing. In Proc. 6th Int. Conf. Com-
putational Linguistics and Intelligent Text Process-
ing, volume 3406 of LNCS, pages 1–24. Springer.

Kevin Knight. 2007. Capturing practical natural
language transformations. Machine Translation,
21(2):121–133.

Karim Lari and Steve J. Young. 1990. The esti-
mation of stochastic context-free grammars using
the inside-outside algorithm. Computer Speech and
Language, 4(1):35–56.

Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Workshop Parsing Technologies, pages 1–12. Asso-
ciation for Computational Linguistics.

Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. 46th Ann. Meeting of
the ACL, pages 192–199. Association for Computa-
tional Linguistics.

William C. Rounds. 1970. Mappings and grammars
on trees. Math. Systems Theory, 4(3):257–287.

Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Comput. Linguist., 21(2):165–201.

James W. Thatcher. 1970. Generalized2 sequential
machine maps. J. Comput. System Sci., 4(4):339–
367.

Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymptotically optimum de-
coding algorithm. IEEE Trans. Inform. Theory,
13(2):260–269.

10


