



















































TCDSCSS: Dimensionality Reduction to Evaluate Texts of Varying Lengths - an IR Approach


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 619–623,
Dublin, Ireland, August 23-24, 2014.

TCDSCSS: Dimensionality Reduction to Evaluate Texts of Varying
Lengths - an IR Approach

Arun Jayapal
Dept of Computer Science

Trinity College Dublin
jayapala@cs.tcd.ie

Martin Emms
Dept of Computer Science

Trinity College Dublin
martin.emms@cs.tcd.ie

John D.Kelleher
School of Computing

Dublin Institute of Technology
john.d.kelleher@dit.ie

Abstract

This paper provides system description of
the cross-level semantic similarity task for
the SEMEVAL-2014 workshop. Cross-
level semantic similarity measures the de-
gree of relatedness between texts of vary-
ing lengths such as Paragraph to Sen-
tence and Sentence to Phrase. Latent Se-
mantic Analysis was used to evaluate the
cross-level semantic relatedness between
the texts to achieve above baseline scores,
tested on the training and test datasets. We
also tried using a bag-of-vectors approach
to evaluate the semantic relatedness. This
bag-of-vectors approach however did not
produced encouraging results.

1 Introduction

Semantic relatedness between texts have been
dealt with in multiple situations earlier. But it is
not usual to measure the semantic relatedness of
texts of varying lengths such as Paragraph to Sen-
tence (P2S) and Sentence to Phrase (S2P). This
task will be useful in natural language process-
ing applications such as paraphrasing and summa-
rization. The working principle of information re-
trieval system is the motivation for this task, where
the queries are not of equal lengths compared to
the documents in the index. We attempted two
ways to measure the semantic similarity for P2S
and S2P in a scale of 0 to 4, 4 meaning both texts
are similar and 0 being dissimilar. The first one
is Latent Semantic Analysis (LSA) and second, a
bag-of-vecors (BV) approach. An example of tar-
get similarity ratings for comparison type S2P is
provided in table 1.

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails: http://creativecommons.org/licenses/
by/4.0/

Sentence: Schumacher was undoubtedly one of
the very greatest racing drivers there has ever
been, a man who was routinely, on every lap, able
to dance on a limit accessible to almost no-one
else.

Score Phrase
4 the unparalleled greatness

of Schumachers driving abilities
3 driving abilities
2 formula one racing
1 north-south highway
0 orthodontic insurance

Table 1: An Example - Sentence to Phrase simi-
larity ratings for each scale

2 Data

The task organizers provided training data, which
included 500 pairs of P2S, S2P, Phrase to Word
(P2W) and their similarity scores. The training
data for P2S and S2P included text from different
genres such as Newswire, Travel, Metaphoric and
Reviews. In the training data for P2S, newswire
text constituted 36% of the data, while reviews
constituted 10% of the data and rest of the three
genres shared 54% of the data.

Considering the different genres provided in the
training data, a chunk of data provided for NIST
TAC’s Knowledge Base Population was used for
building a term-by-document matrix on which
to base the LSA method. The data included
newswire text and web-text, where the web-text
included data mostly from blogs. We used 2343
documents from the NIST dataset1, which were
available in eXtended Markup Language format.

Further to the NIST dataset, all the paragraphs
in the training data2 of paragraph to sentence were
added to the dataset. To add these paragraphs to
the dataset, we converted each paragraph into a

1Distributed by LDC (Linguistic Data Consortium)
2provided by the SEMEVAL task-3 organizers

619



new document and the documents were added to
the corpus. The unique number of words identi-
fied in the corpus were approximately 40000.

3 System description

We tried two different approaches for evaluating
the P2S and S2P. Latent Semantic Analysis (LSA)
using SVD worked better than the Bag-of-Vectors
(BV) approach. The description of both the ap-
proaches are discussed in this section.

3.1 Latent Semantic Analysis
LSA has been used for information retrieval al-
lowing retrieval via vectors over latent, arguably
conceptual, dimensions, rather than over surface
word dimensions (Deerwester et al., 1990). It was
thought this would be of advantage for comparison
of texts of varying length.

3.1.1 Representation
The data corpus was converted into a mxn term-
by-document matrix, A, where the counts (cm,n)
of all terms (wm) in the corpus are represented
in rows and the respective documents (dn) in
columns:

A =


d1 d2 · · · dn

w1 c1,1 c1,2 · · · c1,n
w2 c2,1 c2,2 · · · c2,n
...

...
...

. . .
...

wm cm,1 cm,2 · · · cm,n


The document indexing rules such as text tok-
enization, case standardization, stop words re-
moval, token stemming, and special characters and
punctuations removal were followed to get the ma-
trix A.

Singular Value Decomposition (SVD) decom-
poses the matrix into U , Σ and V matrices (ie.,
A = UΣV T ) such that U and V are orthonormal
matrices and Σ is a diagonal matrix with singular
values. Retaining just the first k columns of U and
V , gives an approximation of A

A ≈ Ak = UkΣkV Tk (1)
According to LSA, the columns of Uk are thought
of as representing latent, semantic dimensions,
and an arbitrary m-dimensional vector #»v can be
projected onto this semantic space by taking the
dot-product with each column of Uk; we will call
the result #      »vsem .

In the experiments reported later, the m-
dimensional vector #»v is sometimes a vector of

word counts, and sometimes a thresholded or
‘boolean’ version, mapping all non-zero numbers
to 1.

3.1.2 Similarity Calculation
To evaluate the similarity of a paragraph, p, and a
sentence, s, first these are represented as vectors of
word counts, #»p and #»s , then these are projected in
the latent semantic space, to give #      »psem and #      »ssem ,
and then between these the cosine similarity met-
ric is calculated:

cos( #      »psem . #      »ssem ) =
#      »psem .

#      »ssem
| #      »psem |.| #      »ssem | (2)

The cosine similarity metric provides a similarity
value in the range of 0 to 1, so to match the target
range of 0 to 4, the cosine values were multiplied
by 4. Exactly the same procedure is used for the
sentence to phrase comparison.

Further, the number of retained dimensions of
Uk was varied, giving different dimensionalities
of the LSA space. The results of testing at the re-
duced dimensions are discussed in 4.1

3.2 Bag-of-Vectors
Another method we experimented on could be
termed a ‘bag-of-vectors’ (BV) approach: each
word in an item to be compared is replaced by a
vector representing its co-occurrence behavior and
the obtained bags of vectors enter into the compar-
ison process.

3.2.1 Representation
For the BV approach, the same data sources as was
used for the LSA approach is turned into a m×m
term-by-term co-occurrence matrix C:

C =



w1 w2 · · · wm
w1 c1,1 c1,2 · · · c1,m
w2 c2,1 c2,2 · · · c2,m
...

...
...

. . .
...

wm cm,1 cm,2 · · · cm,m


The same preprocessing steps as for the LSA ap-
proach applied (text tokenization, case standard-
ization, stop words removal, special characters and
punctuations removal). Via C, if one has a bag-
of-words representing a paragraph, sentence or
phrase, one can replace it by a bag-of-vectors, re-
placing each word wi by the corresponding row of
C – we will call these rows word-vectors.

620



3.2.2 Similarity Calculation
For calculating P2S similarity, the procedure is
as follows. The paragraph and sentence are tok-
enized, and stop-words were removed and are rep-
resented as two vectors #»p and #»s .

For each word pi from #»p , its word vector from
C is found, and this is compared to the word vector
for each word si in #»s , via the cosine measure. The
highest similarity score for each word pi in #»p is
stored in a vector

# »

Sp shown in (3). The overall
semantic similarity score between paragraph and
sentence is then the mean value of the vector

# »

Sp ×
4 – see (4).

Sp =
[
Sp1 Sp2 · · · Spi

]
(3)

Ssim =
∑n

i=1 Spi
n

× 4 (4)

Exactly corresponding steps are carried out for the
S2P similarity. Although experiments were car-
ried out this particular BV approach, the results
were not encouraging. Details of the experiments
carried out are explained in 4.2.

4 Experiments

Different experiments were carried out using LSA
and BV systems described in sections 3.1 and 3.2
on the dataset described in section 2. Pearson
correlation and Spearman’s rank correlation were
the metrics used to evaluate the performance of
the systems. Pearson correlation provides the de-
gree of similarity between the system’s score for
each pair and the gold standard’s score for the said
pair while Spearman’s rank correlation provides
the degree of similarity between the rankings of
the pairs according to similarity.

4.1 LSA
The LSA model was used to evaluate the semantic
similarity between P2S and S2P.

4.1.1 Paragraph to Sentence
An initial word-document matrix A was built by
extracting tokens just based on spaces, stop words
removed and tokens sorted in alphabetical order.
As described in 3.1.1, via the SVD of A, a ma-
trix Uk is obtained which can be used to project an
m dimensional vector into a k dimensional one.
In one setting the paragraph and sentence vec-
tors which are projected into the LSA space have
unique word counts for their dimensions. In an-
other setting before projection, these vectors are

Dimensions 100% 90% 50% 30% 10%

Basic word-doc representation 0.499 - 0.494 0.484 0.426
Evaluation-boolean counts 0.548 - 0.533 0.511 0.420
Constrained tokenization 0.368 0.564 0.540 0.516 0.480

Added data 0.461 0.602 0.568 0.517 0.522

Table 2: Pearson scores at different dimensions -
Paragraph to Sentence

thresholded into ‘boolean’ versions, with 1 for ev-
ery non-zero count.

The Pearson scores for these settings are in the
first and second rows of table 2. They show the
variation with the number of dimensions of the
LSA representation (that is the number of columns
of U that are kept)3. An observation is that the
usage of boolean values instead of word counts
showed improved results.

Further experiments were conducted, retaining
the boolean treatment of the vectors to be pro-
jected. In a new setting, further improvements
were made to the pre-processing step, creating a
new word-document matrix A using constrained
tokenization rules, removing unnecessary spaces
and tabs, and tokens stemmed4. The performance
of the similarity calculation is shown as the third
row of Table 2: there is a trend of increase in cor-
relation scores with respect to the increase in di-
mensionality up to a maximum of 0.564, reached
at 90% dimension.

0 20 40 60 80 100
0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

Percent Dimensions maintained

S
e
m

a
n
ti
c
 s

im
ila

ri
ty

 

 

Basic word−doc representation

Evaluation with Boolean values

Constrained Tokenization

Added data representation

Figure 1: Paragraph to Sentence - Pearson corre-
lation scores for four different experiments at dif-
ferent dimensions3 (represented in percent) of Uk

Not convinced with the pearson scores, more

3Here, the dimension X% means k = (X/100) × N ,
where N is the total number of columns in A in the unreduced
SVD.

4Stemmed using Porter Stemmer module availabe from
http://tartarus.org/∼martin/PorterStemmer/

621



documents were added to the dataset to build a
new word-document matrix representation A. The
documents included all the paragraphs from the
training set. Each paragraph provided in the train-
ing set was added to the dataset as a separate docu-
ment. The experiment was performed maintaining
the settings from the previous experiment and the
results are shown in the fourth row of table 2. The
increase in trend of correlation scores with respect
to the increase in dimensionality is followed by the
new U produced from A after applying SVD. Fig-
ure 2 provides the distribution of similarity scores
evaluated at 90% dimension of the model with re-
spect to the gold standard.

Further to compare the performance of different
experiments, all the experiment results are plotted
in Figure 1. It can be observed that every subse-
quent model built has shown improvements in per-
formance. The first two experiments shown in the
first two rows of table 2 are shown in red and blue
lines in the figure. It can be observed that in both
the settings, the pearson correlation scores were
increasing as the the number of dimensions main-
tained also increased, whereas in the other two set-
tings, the pearson correlation scores reached their
maximum at 90% and came down at 100% di-
mension, which is unexpected and so is not jus-
tified. It is observed from Figure 2 that the scores

0 100 200 300 400 500
0

0.5

1

1.5

2

2.5

3

3.5

4

Training data Examples

S
im

ila
ri
ty

 s
co

re
s

Figure 2: Semantic similarity scores - Gold stan-
dard (Line plot) vs System scores (Scatter plot) for
examples in training data

of the system in scatter plot are not always clus-
tered around the gold standard scores, plotted as a
line. As the gold standard score goes up, the sys-
tem prediction accuracy has come down. One rea-
son for this pattern can be attributed to the train-
ing set which had data mostly data from Newswire

Dimensions 100% 90% 70% 50% 30% 10%

Basic word-doc
representation 0.493 - - 0.435 0.423 0.366

Evaluation
boolean counts 0.472 - - 0.449 0.430 0.363

Constrained
tokenization 0.498 0.494 0.517 0.485 0.470 0.434

Added
data 0.493 0.504 0.498 0.498 0.488 0.460

Table 3: Pearson scores at different dimensions3-
Sentence to Phrase

and webtext. Therefore, during evaluation all the
words from paragraph and/or sentence would not
have got a position while getting projected on the
latent semantic space, which we believe has pulled
down the accuracy.

4.1.2 Sentence to Phrase

The experiments carried out for P2S provided in
4.1.1 were conducted for S2P examples as well.
The pearson scores produced by different experi-
ments at different dimensions are provided in ta-
ble 3. This table shows that the latest word-
document representation made with added docu-
ments, did not have any impact on the correlation
scores, while the earlier word-document represen-
tation provided in 3rd row, which used the original
dataset preprocessed with constrained tokeniza-
tion rules, removing unnecessary spaces and tabs,
and tokens stemmed, provided better correlation
score at 70% dimension. Further the comparison
of different experiments carried out at different
settings are plotted in Figure 3.

0 20 40 60 80 100
0.35

0.4

0.45

0.5

0.55

Percent Dimensions maintained

S
e
m

a
n
ti
c
 s

im
ila

ri
ty

 

 

Basic word−doc representation

Evaluation with Boolean values

Constrained Tokenization

Added data representation

Figure 3: Sentence to Phrase - Pearson correlation
scores for four different experiments at different
dimensions3 (represented in percentage) of Uk

622



4.2 Bag of Vectors

BV was tested in two different settings. The
first representation was created with bi-gram co-
occurance count as mentioned in section 3.2.1 and
experiments were carried out as mentioned in sec-
tion 3.2.2. This produced negative Pearson corre-
lation scores for P2S and S2P. Then we created an-
other representation by getting co-occurance count
in a window of 6 words in a sentence, on evalua-
tion produced correlation scores of 0.094 for P2S
and 0.145 for S2P. As BV showed strong negative
results, we did not continue using the method for
evaluating the test data. But we strongly believe
that the BV approach can produce better results if
we could compare the sentence to the paragraph
rather than the paragraph to the sentence as men-
tioned in section 3.2.2. During similarity calcula-
tion, when comparing sentence to the paragraph,
for each word in the sentence, we look for the best
semantic match from the paragraph, which would
increase the mean value by reducing the number of
divisions representing the number of words in the
sentence. In the current setting, it is believed that
while computing the similarity for the paragraph
to sentence, the words in the paragraph (longer
text) will consider a few words in the sentence to
be similar multiple times. This could not be right
when we compare the texts of varying lengths.

5 Conclusion and Discussion

On manual verification, it was identified that the
dataset used to build the representation did not
have documents related to the genres Metaphoric,
CQA and Travel. The original dataset mostly had
documents from Newswire text and blogs which
included reviews as well. Further, it can be identi-
fied from tables 2 and 3, the word-document rep-
resentation with added documents from the train-
ing set improved Pearson scores. This allowed to
assume that the dataset did not have completely
relevant set of documents to evaluate the training
set which included data from different genres. For
evaluation of the model on test data, we submitted
two runs and best of them reported Pearson score
of 0.607 and 0.552 on P2S and S2P respectively.
In the future work, we should be able to experi-
ment with more relevant data to build the model
using LSI and also use statistically strong unsu-
pervised classifier pLSI (Hofmann T, 2001) for the
same task. Further to this, as discussed in 4.2 we
would be able to experiment with the BV approach

by comparing the sentence to the paragraph, which
we believe will yield promising results to compare
the texts of varying lengths.

References
Scott Deerwester, Susan T. Dumais, George W. Fur-

nas, Thomas K. Landauer and Richard Harshman
1990. Indexing by latent semantic analysis Jour-
nal of the American society for information science,
41(6):391–401

Thomas Hofmann 2001. Unsupervised Learning
by Probabilistic Latent Semantic Analysis Journal
Machine Learning, Volume 42 Issue 1-2, January-
February 2001 Pages 177 - 196

623


