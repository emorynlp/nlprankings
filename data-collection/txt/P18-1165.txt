



















































Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1777–1788
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1777

Reliability and Learnability of Human Bandit Feedback
for Sequence-to-Sequence Reinforcement Learning

Julia Kreutzer1 and Joshua Uyheng3∗ and Stefan Riezler1,2
1Computational Linguistics & 2IWR, Heidelberg University, Germany

{kreutzer,riezler}@cl.uni-heidelberg.de
3Departments of Psychology & Mathematics, Ateneo de Manila University, Philippines

juyheng@ateneo.edu

Abstract

We present a study on reinforcement learn-
ing (RL) from human bandit feedback
for sequence-to-sequence learning, exem-
plified by the task of bandit neural ma-
chine translation (NMT). We investigate
the reliability of human bandit feedback,
and analyze the influence of reliability on
the learnability of a reward estimator, and
the effect of the quality of reward esti-
mates on the overall RL task. Our anal-
ysis of cardinal (5-point ratings) and ordi-
nal (pairwise preferences) feedback shows
that their intra- and inter-annotator α-
agreement is comparable. Best reliabil-
ity is obtained for standardized cardinal
feedback, and cardinal feedback is also
easiest to learn and generalize from. Fi-
nally, improvements of over 1 BLEU can
be obtained by integrating a regression-
based reward estimator trained on cardinal
feedback for 800 translations into RL for
NMT. This shows that RL is possible even
from small amounts of fairly reliable hu-
man feedback, pointing to a great potential
for applications at larger scale.

1 Introduction

Recent work has received high attention by suc-
cessfully scaling reinforcement learning (RL) to
games with large state-action spaces, achieving
human-level (Mnih et al., 2015) or even super-
human performance (Silver et al., 2016). This
success and the ability of RL to circumvent the
data annotation bottleneck in supervised learning
has led to renewed interest in RL in sequence-
to-sequence learning problems with exponential

∗The work for this paper was done while the second au-
thor was an intern in Heidelberg.

output spaces. A typical approach is to com-
bine REINFORCE (Williams, 1992) with poli-
cies based on deep sequence-to-sequence learn-
ing (Bahdanau et al., 2015), for example, in ma-
chine translation (Bahdanau et al., 2017), seman-
tic parsing (Liang et al., 2017), or summarization
(Paulus et al., 2017). These RL approaches fo-
cus on improving performance in automatic eval-
uation by simulating reward signals by evalua-
tion metrics such as BLEU, F1-score, or ROUGE,
computed against gold standards. Despite coming
from different fields of application, RL in games
and sequence-to-sequence learning share firstly
the existence of a clearly specified reward func-
tion, e.g., defined by winning or losing a game, or
by computing an automatic sequence-level evalu-
ation metric. Secondly, both RL applications rely
on a sufficient exploration of the action space, e.g.,
by evaluating multiple game moves for the same
game state, or various sequence predictions for the
same input.

The goal of this paper is to advance the state-
of-the-art of sequence-to-sequence RL, exempli-
fied by bandit learning for neural machine trans-
lation (NMT). Our aim is to show that successful
learning from simulated bandit feedback (Sokolov
et al., 2016b; Kreutzer et al., 2017; Nguyen et al.,
2017; Lawrence et al., 2017) does in fact carry
over to learning from actual human bandit feed-
back. The promise of bandit NMT is that human
feedback on the quality of translations is easier
to obtain in large amounts than human references,
thus compensating the weaker nature of the signals
by their quantity. However, the human factor en-
tails several differences to the above sketched sim-
ulation scenarios of RL. Firstly, human rewards
are not well-defined functions, but complex and
inconsistent signals. For example, in general ev-
ery input sentence has a multitude of correct trans-
lations, each of which humans may judge differ-



1778

ently, depending on many contextual and personal
factors. Secondly, exploration of the space of pos-
sible translations is restricted in real-world scenar-
ios where a user judges one displayed translation,
but cannot be expected to rate an alternative trans-
lation, let alone large amounts of alternatives.

In this paper we will show that despite the fact
that human feedback is ambiguous and partial in
nature, a catalyst for successful learning from hu-
man reinforcements is the reliability of the feed-
back signals. The first deployment of bandit NMT
in an e-commerce translation scenario conjectured
lacking reliability of user judgments as the rea-
son for disappointing results when learning from
148k user-generated 5-star ratings for around 70k
product title translations (Kreutzer et al., 2018).
We thus raise the question of how human feed-
back can be gathered in the most reliable way,
and what effect reliability will have in downstream
tasks. In order to answer these questions, we
measure intra- and inter-annotator agreement for
two feedback tasks for bandit NMT, using car-
dinal feedback (on a 5-point scale) and ordinal
feedback (by pairwise preferences) for 800 trans-
lations, conducted by 16 and 14 human raters,
respectively. Perhaps surprisingly, while relative
feedback is often considered easier for humans
to provide (Thurstone, 1927), our investigation
shows that α-reliability (Krippendorff, 2013) for
intra- and inter-rater agreement is similar for both
tasks, with highest inter-rater reliability for stan-
dardized 5-point ratings.

In a next step, we address the issue of machine
learnability of human rewards. We use deep learn-
ing models to train reward estimators by regres-
sion against cardinal feedback, and by fitting a
Bradley-Terry model (Bradley and Terry, 1952) to
ordinal feedback. Learnability is understood by
a slight misuse of the machine learning notion of
learnability (Shalev-Shwartz et al., 2010) as the
question how well reward estimates can approx-
imate human rewards. Our experiments reveal
that rank correlation of reward estimates with TER
against human references is higher for regression
models trained on standardized cardinal rewards
than for Bradley-Terry models trained on pairwise
preferences. This emphasizes the influence of the
reliability of human feedback signals on the qual-
ity of reward estimates learned from them.

Lastly, we investigate machine learnability of
the overall NMT task, in the sense of Green et al.

(2014) who posed the question of how well an MT
system can be tuned on post-edits. We use an RL
approach for tuning, where a crucial difference of
our work to previous work on RL from human re-
wards (Knox and Stone, 2009; Christiano et al.,
2017) is that our RL scenario is not interactive, but
rewards are collected in an offline log. RL then can
proceed either by off-policy learning using logged
single-shot human rewards directly, or by using es-
timated rewards. An expected advantage of esti-
mating rewards is to tackle a simpler problem first
— learning a reward estimator instead of a full
RL task for improving NMT — and then to de-
ploy unlimited feedback from the reward estimator
for off-policy RL. Our results show that significant
improvements can be achieved by training NMT
from both estimated and logged human rewards,
with best results for integrating a regression-based
reward estimator into RL. This completes the ar-
gumentation that high reliability influences quality
of reward estimates, which in turn affects the qual-
ity of the overall NMT task. Since the size of our
training data is tiny in machine translation propor-
tions, this result points towards a great potential
for larger-scaler applications of RL from human
feedback.

2 Related Work

Function approximation to learn a “critic” instead
of using rewards directly has been embraced in
the RL literature under the name of “actor-critic”
methods (see Konda and Tsitsiklis (2000), Sut-
ton et al. (2000), Kakade (2001), Schulman et al.
(2015), Mnih et al. (2016), inter alia). In differ-
ence to our approach, actor-critic methods learn
online while our approach estimates rewards in an
offline fashion. Offline methods in RL, with and
without function approximation, have been pre-
sented under the name of “off-policy” or “coun-
terfactual” learning (see Precup et al. (2000), Pre-
cup et al. (2001), Bottou et al. (2013), Swami-
nathan and Joachims (2015a), Swaminathan and
Joachims (2015b), Jiang and Li (2016), Thomas
and Brunskill (2016), inter alia). Online actor-
critic methods have been applied to sequence-
to-sequence RL by Bahdanau et al. (2017) and
Nguyen et al. (2017). An approach to off-policy
RL under deterministic logging has been pre-
sented by Lawrence et al. (2017). However, all
these approaches have been restricted to simulated
rewards.



1779

RL from human feedback is a growing area.
Knox and Stone (2009) and Christiano et al.
(2017) learn a reward function from human feed-
back and use that function to train an RL system.
The actor-critic framework has been adapted to
interactive RL from human feedback by Pilarski
et al. (2011) and MacGlashan et al. (2017). These
approaches either update the reward function from
human feedback intermittently or perform learn-
ing only in rounds where human feedback is pro-
vided. A framework that interpolates a human cri-
tique objective into RL has been presented by Ju-
dah et al. (2019). None of these works system-
atically investigates the reliability of the feedback
and its impact of the down-stream task.

Kreutzer et al. (2018) have presented the first
application of off-policy RL for learning from
noisy human feedback obtained for determinis-
tic logs of e-commerce product title translations.
While learning from explicit feedback in the form
of 5-star ratings fails, Kreutzer et al. (2018) pro-
pose to leverage implicit feedback embedded in
a search task instead. In simulation experiments
on the same domain, the methods proposed by
Lawrence et al. (2017) succeeded also for neural
models, allowing to pinpoint the lack of reliabil-
ity in the human feedback signal as the reason for
the underwhelming results when learning from hu-
man 5-star ratings. The goal of showing the effect
of highly reliable human bandit feedback in down-
stream RL tasks was one of the main motivations
for our work.

For the task of machine translation, estimat-
ing human feedback, i.e. quality ratings, is re-
lated to the task of sentence-level quality estima-
tion (sQE). However, there are crucial differences
between sQE and the reward estimation in our
work: sQE usually has more training data, often
from more than one machine translation model. Its
gold labels are inferred from post-edits, i.e. cor-
rections of the machine translation output, while
we learn from weaker bandit feedback. Although
this would in principle be possible, sQE predic-
tions have not (yet) been used to directly reinforce
predictions of MT systems, mostly because their
primary purpose is to predict post-editing effort,
i.e. give guidance how to further process a trans-
lation. State-of-the-art models for sQE such as
(Martins et al., 2017) and (Kim et al., 2017) are
unsuitable for the direct use in this task since they
rely on linguistic input features, stacked architec-

Figure 1: Rating interface for 5-point ratings.

Figure 2: Rating interface for pairwise ratings.

tures or post-edit or word-level supervision. Sim-
ilar to approaches for generative adversarial NMT
(Yu et al., 2017; Wu et al., 2017) we prefer a sim-
pler convolutional architecture based on word em-
beddings for the human reward estimation.

3 Human MT Rating Task

3.1 Data

We translate a subset of the TED corpus with
a general-domain and a domain-adapted NMT
model (see §6.2 for NMT and data), post-
process the translations (replacing special charac-
ters, restoring capitalization) and filter out identi-
cal out-of-domain and in-domain translations. In
order to compose a homogeneous data set, we first
select translations with references of length 20 to
40, then sort the translation pairs by difference in
character n-gram F-score (chrF, β = 3) (Popović,
2015) and length, and pick the top 400 translation
pairs with the highest difference in chrF but lowest
difference in length. This yields translation pairs
of similar length, but different quality.

3.2 Rating Task

The pairs were treated as 800 separate transla-
tions for a 5-point rating task. From the orig-
inal 400 translation pairs, 100 pairs (or 200 in-
dividual translations) were randomly selected for



1780

Inter-rater Intra-rater
Type α Mean α Stdev. α

5-point 0.2308
0.4014 0.1907

5-point norm. 0.2820

5-point norm. part. 0.5059 0.5527 0.0470
5-point norm. trans. 0.3236 0.3845 0.1545
Pairwise 0.2385 0.5085 0.2096

Pairwise filt. part. 0.3912 0.7264 0.0533
Pairwise filt. trans. 0.3519 0.5718 0.2591

Table 1: Inter- and intra-reliability measured by
Krippendorff’s α for 5-point and pairwise ratings
of 1,000 translations of which 200 translations are
repeated twice. The filtered variants are restricted
to either a subset of participants (part.) or a subset
of translations (trans.).

repetition. This produced a total of 1,000 indi-
vidual translations, with 600 occurring once, and
200 occurring twice. The translations were shuf-
fled and separated into five sections of 200 trans-
lations, each with 120 translations from the unre-
peated pool, and 80 translations from the repeated
pool, ensuring that a single translation does not oc-
cur more than once in each section. For a pair-
wise task, the same 100 pairs were repeated from
the original 400 translation pairs. This produced
a total of 500 translation pairs. The translations
were also shuffled and separated into five sections
of 100 translation pairs, each with 60 translation
pairs from the unrepeated pool, and 40 translation
pairs from the repeated pool. None of the pairs
were repeated within each section.

We recruited 14 participants for the pairwise
rating task and 16 for the 5-point rating task. The
participants were university students with fluent
or native language skills in German and English.
The rating interface is shown in Figures 1 and 2.
Rating instructions are given in the supplementary
material. Note that no reference translations were
presented since the objective is to model a realistic
scenario for bandit learning.1

4 Reliability of Human MT Ratings

4.1 Inter-rater and Intra-rater Reliability
In the following, we report inter- and intra-rater re-
liability of the cardinal and ordinal feedback tasks
described in §3 with respect to Krippendorff’s α

1The collection of ratings can be downloaded
from http://www.cl.uni-heidelberg.de/
statnlpgroup/humanmt/.

(Krippendorff, 2013) evaluated at interval and or-
dinal scale, respectively.

As shown in Table 1, measures of inter-rater
reliability show small differences between the 5-
point and pairwise task. The inter-rater reliabil-
ity in the 5-point task (α = 0.2308) is roughly the
same as that of the pairwise task (α = 0.2385).
Normalization of ratings per participant (by stan-
dardization to Z-scores), however, shows a marked
improvement of overall inter-rater reliability for
the 5-point task (α = 0.2820). A one-way
analysis of variance taken over inter-rater reli-
abilities between pairs of participants suggests
statistically significant differences across tasks
(F (2, 328) = 6.399, p < 0.01), however, a post
hoc Tukey’s (Larsen and Marx, 2012) honest sig-
nificance test attributes statistically significant dif-
ferences solely between the 5-point tasks with and
without normalization. These scores indicate that
the overall agreement between human ratings is
roughly the same, regardless of whether partici-
pants are being asked to provide cardinal or ordi-
nal ratings. Improvement in inter-rater reliability
via participant-level normalization suggests that
participants may indeed have individual biases to-
ward certain regions of the 5-point scale, which
the normalization process corrects.

In terms of intra-rater reliability, a better mean
was observed among participants in the pair-
wise task (α = 0.5085) versus the 5-point task
(α = 0.4014). This suggests that, on average, hu-
man raters provide more consistent ratings with
themselves in comparing between two translations
versus rating single translations in isolation. This
may be attributed to the fact that seeing multi-
ple translations provides raters with more cues
with which to make consistent judgments. How-
ever, at the current sample size, a Welch two-
sample t-test (Larsen and Marx, 2012) between
5-point and pairwise intra-rater reliabilities shows
no significant difference between the two tasks
(t (26.92) = 1.4362, p = 0.1625). Thus, it re-
mains difficult to infer whether one task is defini-
tively superior to the other in eliciting more con-
sistent responses. Intra-rater reliability is the same
for the 5-point task with and without normaliza-
tion, as participants are still compared against
themselves.

http://www.cl.uni-heidelberg.de/statnlpgroup/humanmt/
http://www.cl.uni-heidelberg.de/statnlpgroup/humanmt/


1781

Figure 3: Improvements in inter-rater reliability
using intra-rater consistency filter.

Figure 4: Improvements in inter-rater reliability
using item variance filter.

4.2 Rater and Item Variance

The succeeding analysis is based on two assump-
tions: first, that human raters vary in that they do
not provide equally good judgments of translation
quality, and second, rating items vary in that some
translations may be more difficult to judge than
others. This allows to investigate the influence of
rater variance and item variance on inter-rater re-
liability by an ablation analysis where low-quality
judges and difficult translations are filtered out.

Using intra-rater reliability as an index of how
well human raters judge translation quality, Fig-
ure 3 shows a filtering process whereby human
raters with α scores lower than a moving thresh-
old are dropped from the analysis. As the relia-
bility threshold is increased from 0 to 1, overall
inter-rater reliability is measured. Figure 4 shows
a similar filtering process implemented using vari-
ance in translation scores. Item variances are nor-
malized on a scale from 0 to 1 and subtracted from

1 to produce an item variance threshold. As the
threshold increases, overall inter-rater reliability is
likewise measured as high-variance items are pro-
gressively dropped from the analysis.

As the plots demonstrate, inter-rater reliability
generally increases with consistency and variance
filtering. For consistency filtering, Figure 3 shows
how the inter-rater reliability of the 5-point task
experiences greater increases than the pairwise
task with lower filtering thresholds, especially in
the normalized case. This may be attributed to the
fact that more participants in the 5-point task had
low intra-rater reliability. Pairwise tasks, on the
other hand, require higher thresholds before large
gains are observed in overall inter-rater reliabil-
ity. This is because more participants in the pair-
wise task had relatively high intra-rater reliability.
In the normalized 5-point task, selecting a thresh-
old of 0.49 as a cutoff for intra-rater reliability re-
tains 8 participants with an inter-rater reliability of
0.5059. For the pairwise task, a threshold of 0.66
leaves 5 participants with an inter-rater reliability
of 0.3912.

The opposite phenomenon is observed in the
case of variance filtering. As seen in Figure 4,
the overall inter-rater reliability of the pairwise
task quickly overtakes that of the 5-point task,
with and without normalization. This may be at-
tributed to how, in the pairwise setup, more items
can be a source of disagreement among human
judges. Ambiguous cases, that will be discussed
in §4.3, may result in higher item variance. This
problem is not as pronounced in the 5-point task,
where judges must simply judge individual trans-
lations. It may be surmised that this item variance
accounts for why, on average, judges in the pair-
wise task demonstrate higher intra-rater reliabil-
ity than those in the 5-point task, yet the overall
inter-rater reliability of the pairwise task is lower.
By selecting a variance threshold such that at least
70% of items are retained in the analysis, the im-
proved inter-rater reliabilities were 0.3236 for the
5-point task and 0.3519 for the pairwise task.

4.3 Qualitative Analysis

On completion of the rating task, we asked the par-
ticipants for a subjective judgment of difficulty on
a scale from 1 (very difficult) to 10 (very easy). On
average, the pairwise rating task (mean 5.69) was
perceived slightly easier than the 5-point rating
task (mean 4.8). They also had to state which as-



1782

pects of the tasks they found difficult: The biggest
challenge for 5-point ratings seemed to be the
weighing of different error types and the rating of
long sentences with very few, but essential errors.
For pairwise ratings, difficulties lie in distinguish-
ing between similar, or similarly bad translations.
Both tasks showed difficulties with ungrammatical
or incomprehensible sources.

Comparing items with high and low agreement
across raters allows conclusions about objective
difficulty. We assume that high inter-rater agree-
ment indicates an ease of judgment, while diffi-
culties in judgment are manifested in low agree-
ment. A list of examples is given in the supple-
mentary material. For 5-point ratings, difficul-
ties arise with ungrammatical sources and omis-
sions, whereas obvious mistakes in the target, such
as over-literal translations, make judgment easier.
Preference judgments tend to be harder when both
translations contain errors and are similar. When
there is a tie, the pairwise rating framework does
not allow to indicate whether both translations are
of high or low quality. Since there is no normaliza-
tion strategy for pairwise ratings, individual biases
or rating schemes can hence have a larger negative
impact on the inter-rater agreement.

5 Learnability of a Reward Estimator
from MT Ratings

5.1 Learning a Reward Estimator

The numbers of ratings that can be obtained di-
rectly from human raters in a reasonable amount
of time is tiny compared to the millions of sen-
tences used for standard NMT training. By learn-
ing a reward estimator on the collection of human
ratings, we seek to generalize to unseen transla-
tions. The model for this reward estimator should
ideally work without time-consuming feature ex-
traction so it can be deployed in direct interaction
with a learning NMT system, estimating rewards
on the fly, and most importantly generalize well so
it can guide the NMT towards good local optima.

Learning from Cardinal Feedback. The inputs
to the reward estimation model are sources x and
their translations y. Given cardinal judgments for
these inputs, a regression model with parameters
ψ is trained to minimize the mean squared error
(MSE) for a set of n predicted rewards r̂ and judg-

ments r:

LMSE(ψ) = 1
n

n∑
i=1

(r(yi)− r̂ψ(yi))2.

In simulation experiments, where all translations
can be compared to existing references, r may be
computed by sentence-BLEU (sBLEU). For our
human 5-point judgments, we first normalize the
judgments per rater as described in §4, then aver-
age the judgments across raters and finally scale
them linearly to the interval [0.0, 1.0].

Learning from Pairwise Preference Feedback.
When pairwise preferences are given instead of
cardinal judgments, the Bradley-Terry model al-
lows us to train an estimator of r. Following Chris-
tiano et al. (2017), let P̂ψ[y1 � y2] be the proba-
bility that any translation y1 is preferred over any
other translation y2 by the reward estimator:

P̂ψ[y
1 � y2] =

exp r̂ψ(y
1)

exp r̂ψ(y1) + exp r̂ψ(y2)
.

Let Q[y1 � y2] be the probability that translation
y1 is preferred over translation y2 by a gold stan-
dard, e.g. the human raters or in comparison to a
reference translation. With this supervision signal
we formulate a pairwise (PW) training loss for the
reward estimation model with parameters ψ:

LPW (ψ) = − 1
n

n∑
i=1

Q[y1i � y2i ] log P̂ψ[y1i � y2i ]

+Q[y2i � y1i ] log P̂ψ[y2i � y1i ].

For simulation experiments — where we lack a
genuine supervision for preferences — we com-
pute Q comparing the sBLEU scores for both
translations, i.e. translation preferences are mod-
eled according to their difference in sBLEU:

Q[y1 � y2] = exp sBLEU(y
1)

exp sBLEU(y1) + exp sBLEU(y2)
.

When obtaining preference jugdments directly
from raters, Q[y1 � y2] is simply the relative fre-
quency of y1 being preferred over y2 by a rater.

5.2 Experiments
Data. The 1,000 ratings collected as described
in §3 are leveraged to train regression models and
pairwise preference models. In addition, we train
models on simulated rewards (sBLEU) for a com-
parison with arguably “clean” feedback for the



1783

Model Feedback ρ

MSE Simulated -0.2571
PW Simulated -0.1307

MSE Human -0.2193
PW Human -0.1310

MSE Human filt. -0.2341
PW Human filt. -0.1255

Table 2: Spearman’s rank correlation ρ between
estimated rewards and TER for models trained
with simulated rewards and human rewards (also
filtered subsets).

same set of translations. In order to augment this
very small collection of ratings, we leverage the
available out-of-domain bitext as auxiliary train-
ing data. We sample translations for a subset of the
out-of-domain sources and store sBLEU scores
as rewards, collecting 90k out-of-domain training
samples in total (see supplementary material for
details). During training, each mini-batch is sam-
pled from the auxiliary data with probability paux,
from the original training data with probability
1 − paux. Adding this auxiliary data as a regu-
larization through multi-task learning prevents the
model from overfitting to the small set of human
ratings. In the experiments paux was tuned to 0.8.

Architecture. We choose the following neu-
ral architecture for the reward estimation (details
see supplementary material): Inputs are padded
source and target subword embeddings, which are
each processed with a biLSTM (Hochreiter and
Schmidhuber, 1997). Their outputs are concate-
nated for each time step, further fed to a 1D-
convolution with max-over-time pooling and sub-
sequently a leaky ReLU (Maas et al., 2013) output
layer. This architecture can be seen as a biLSTM-
enhanced bilingual extension to the convolutional
model for sentence classification proposed by Kim
(2014). It has the advantage of not requiring any
feature extraction but still models n-gram features
on an abstract level.

Evaluation Method. The quality of the reward
estimation models is tested by measuring Spear-
man’s ρ with TER on a held-out test set of 1,314
translations following the standard in sQE eval-
uations. Hyperparameters are tuned on another
1,200 TED translations.

Results. Table 2 reports the results of reward es-
timators trained on simulated and human rewards.
When trained from cardinal rewards, the model
of simulated scores performs slightly better than
the model of human ratings. This advantage is
lost when moving to preference judgments, which
might be explained by the fact that the softmax
over sBLEUs with respect to a single reference
is just not as expressive as the preference proba-
bilities obtained from several raters. Filtering by
participants (retaining 8 participants for cardinal
rewards and 5 for preference jugdments, see Sec-
tion 4) improves the correlation further for cardi-
nal rewards, but slightly hurts for preference judg-
ments. The overall correlation scores are relatively
low — especially for the PW models — which
we suspect is due to overfitting to the small set
of training data. From these experiments we con-
clude that when it comes to estimating translation
quality, cardinal human jugdments are more useful
than pairwise preference jugdments.

6 Reinforcement Learning from Direct
and Estimated Rewards in MT

6.1 NMT Objectives

Supervised Learning. Most commonly, NMT
models are trained with Maximum Likelihood Es-
timation (MLE) on a parallel corpus of source and
target sequences D = {(x(s),y(s))}Ss=1:

LMLE(θ) =
S∑
s=1

log pθ(y
(s)|x(s)).

The MLE objective requires reference translations
and is agnostic to rewards. In the experiments it is
used to train the out-of-domain baseline model as
a warm start for reinforcement learning from in-
domain rewards.

Reinforcement Learning from Estimated or
Simulated Direct Rewards. Deploying NMT in
a reinforcement learning scenario, the goal is to
maximize the expectation of a reward r over all
source and target sequences (Wu et al., 2016),
leading to the following REINFORCE (Williams,
1992) objective:

RRL(θ) =Ep(x)pθ(y|x) [r(y)] (1)

≈
S∑
s=1

k∑
i=1

pτθ(ỹ
(s)
i |x

(s)) r(ỹi) (2)



1784

The reward r can either come from a reward esti-
mation model (estimated reward) or be computed
with respect to a reference in a simulation setting
(simulated direct reward). In order to counteract
high variance in the gradient updates, the running
average of rewards is subtracted from r for learn-
ing. In practice, Equation 1 is approximated with
k samples from pθ(y|x) (see Equation 2). When
k = 1, this is equivalent to the expected loss
minimization in Sokolov et al. (2016a,b); Kreutzer
et al. (2017), where the system interactively learns
from online bandit feedback. For k > 1 this is
similar to the minimum-risk training for NMT pro-
posed in Shen et al. (2016). Adding a tempera-
ture hyper-parameter τ ∈ (0.0,∞] to the softmax
over the model output o allows us to control the
sharpness of the sampling distribution pτθ(y|x) =
softmax(o/τ), i.e. the amount of exploration dur-
ing training. With temperature τ < 1, the model’s
entropy decreases and samples closer to the one-
best output are drawn. We seek to keep the explo-
ration low to prevent the NMT to produce samples
that lie far outside the training domain of the re-
ward estimator.

Off-Policy Learning from Direct Rewards.
When rewards can not be obtained for samples
from a learning system, but were collected for a
static deterministic system (e.g. in a production
environment), we are in an off-policy learning sce-
nario. The challenge is to improve the MT sys-
tem from a log L = {(x(h),y(h), r(y(h)))}Hh=1 of
rewarded translations. Following Lawrence et al.
(2017) we define the following off-policy learning
(OPL) objective to learn from logged rewards:

ROPL(θ) = 1
H

H∑
h=1

r(y(h)) p̄θ(y
(h)|x(h)),

with reweighting over the current mini-batch B:
p̄θ(y

(h)|x(h)) = pθ(y
(h)|x(h))∑B

b=1 pθ(y
(b)|x(b))

.2 In contrast to

the RL objective, only logged translations are re-
inforced, i.e. there is no exploration in learning.

6.2 Experiments

Data. We use the WMT 2017 data3 for training
a general domain (here: out-of-domain) model for

2Lawrence et al. (2017) propose reweighting over the
whole log, but this is infeasible for NMT. Here B � H .

3Pre-processed data available at http://www.
statmt.org/wmt17/translation-task.html.

WMT TED
Model BLEU METEOR BEER BLEU METEOR BEER

WMT 27.2 31.8 60.08 27.0 30.7 59.48
TED 26.3 31.3 59.49 34.3 34.6 64.94

Table 3: Results on test data for in- and out-of-
domain fully-supervised models. Both are trained
with MLE, the TED model is obtained by fine-
tuning the WMT model in TED data.

translations from German to English. The train-
ing data contains 5.9M sentence pairs, the devel-
opment data 2,999 sentences (WMT 2016 test set)
and the test data 3,004 sentences. For in-domain
data, we choose the translations of TED talks4

as used in IWSLT evaluation campaigns. The
training data contains 153k, the development data
6,969, and the test data 6,750 parallel sentences.

Architecture. Our NMT model is a standard
subword-based encoder-decoder architecture with
attention (Bahdanau et al., 2015). An encoder Re-
current Neural Network (RNN) reads in the source
sentence and a decoder RNN generates the tar-
get sentence conditioned on the encoded source.
We implemented RL and OPL objectives in Neu-
ral Monkey (Helcl and Libovický, 2017).5 The
NMT has a bidirectional encoder and a single-
layer decoder with 1,024 GRUs each, and subword
embeddings of size 500 for a shared vocabulary
of subwords obtained from 30k byte-pair merges
(Sennrich et al., 2016). For model selection we
use greedy decoding, for test set evaluation beam
search with a beam of width 10. We sample k = 5
translations for RL models and set the softmax
temperature τ = 0.5. Further hyperparameters are
given in the supplementary material.

Evaluation Method. Trained models are eval-
uated with respect to BLEU (Papineni et al.,
2002), METEOR (Denkowski and Lavie, 2011)
using MULTEVAL (Clark et al., 2011) and BEER
(Stanojević and Sima’an, 2014) to cover a diverse
set of automatic measures for translation quality.6

We test for statistical significance with approxi-
mate randomization (Noreen, 1989).

4Pre-processing and data splits as described in https:
//github.com/rizar/actor-critic-public/
tree/master/exp/ted.

5The code is available in the Neural Monkey
fork https://github.com/juliakreutzer/
bandit-neuralmonkey/tree/acl2018.

6Since tendencies of improvement turn out to be consis-
tent across metrics, we only discuss BLEU in the text.

http://www.statmt.org/wmt17/translation-task.html
http://www.statmt.org/wmt17/translation-task.html
https://github.com/rizar/actor-critic-public/tree/master/exp/ted
https://github.com/rizar/actor-critic-public/tree/master/exp/ted
https://github.com/rizar/actor-critic-public/tree/master/exp/ted
https://github.com/juliakreutzer/bandit-neuralmonkey/tree/acl2018
https://github.com/juliakreutzer/bandit-neuralmonkey/tree/acl2018


1785

Model Rewards BLEU METEOR BEER

Baseline - - 27.0 30.7 59.48

RL D S 32.5?±0.01 33.7
?
±0.01 63.47

?
±0.10

OPL D S 27.5? 30.9? 59.62?

RL+MSE E S 28.2?±0.09 31.6
?
±0.04 60.23

?
±0.14

RL+PW E S 27.8?±0.01 31.2
?
±0.01 59.83

?
±0.04

OPL D H 27.5? 30.9? 59.72?

RL+MSE E H 28.1?±0.01 31.5
?
±0.01 60.21

?
±0.12

RL+PW E H 27.8?±0.09 31.3
?
±0.09 59.88

?
±0.23

RL+MSE E F 28.1?±0.20 31.6
?
±0.10 60.29

?
±0.13

Table 4: Results on TED test data for training with
estimated (E) and direct (D) rewards from simula-
tion (S), humans (H) and filtered (F) human rat-
ings. Significant (p ≤ 0.05) differences to the
baseline are marked with ?. For RL experiments
we show three runs with different random seeds,
mean and standard deviation in subscript.

The out-of-domain model is trained with MLE
on WMT. The task is now to improve the gener-
alization of this model to the TED domain. Ta-
ble 3 compares the out-of-domain baseline with
domain-adapted models that were further trained
on TED in a fully-supervised manner (super-
vised fine-tuning as introduced by Freitag and Al-
Onaizan (2016); Luong and Manning (2015)). The
supervised domain-adapted model serves as an up-
per bound for domain adaptation with human re-
wards: if we had references, we could improve up
to 7 BLEU. What if references are not available,
but we can obtain rewards for sample translations?

Results for RL from Simulated Rewards. First
we simulate “clean” and deterministic rewards by
comparing sample translations to references using
GLEU (Wu et al., 2016) for RL, and smoothed
sBLEU for estimated rewards and OPL. Table 4
lists the results for this simulation experiment in
rows 2-5 (S). If unlimited clean feedback was
given (RL with direct simulated rewards), im-
provements of over 5 BLEU can be achieved.
When limiting the amount of feedback to a log of
800 translations, the improvements over the base-
line are only marginal (OPL). When replacing the
direct reward by the simulated reward estimators
from §5, i.e. having unlimited amounts of approx-
imately clean rewards, however, improvements of
1.2 BLEU for MSE estimators (RL+MSE) and
0.8 BLEU for pairwise estimators (RL+PW) are
found. This suggests that the reward estimation

model helps to tackle the challenge of generaliza-
tion over a small set of ratings.

Results for RL from Human Rewards. Know-
ing what to expect in an ideal setting with non-
noisy feedback, we now move to the experiments
with human feedback. OPL is trained with the
logged normalized, averaged and re-scaled human
reward (see §5). RL is trained with the direct re-
ward provided by the reward estimators trained on
human rewards from §5. Table 4 shows the re-
sults for training with human rewards in rows 6-
8: The improvements for OPL are very similar to
OPL with simulated rewards, both suffering from
overfitting. For RL we observe that the MSE-
based reward estimator (RL+MSE) leads to sig-
nificantly higher improvements as a the pairwise
reward estimator (RL+PW) — the same trend as
for simulated ratings. Finally, the improvement
of 1.1 BLEU over the baseline showcases that we
are able to improve NMT with only a small num-
ber of human rewards. Learning from estimated
filtered 5-point ratings, does not significantly im-
prove over these results, since the improvement of
the reward estimator is only marginal (see § 5).

7 Conclusion

In this work, we sought to find answers to the
questions of how cardinal and ordinal feedback
differ in terms of reliability, learnability and ef-
fectiveness for RL training of NMT, with the goal
of improving NMT with human bandit feedback.
Our rating study, comparing 5-point and prefer-
ence ratings, showed that their reliability is com-
parable, whilst cardinal ratings are easier to learn
and to generalize from, and also more suitable for
RL in our experiments.

Our work reports improvements of NMT lever-
aging actual human bandit feedback for RL, leav-
ing the safe harbor of simulations. Our experi-
ments show that improvements of over 1 BLEU
are achievable by learning from a dataset that is
tiny in machine translation proportions. Since
this type of feedback, in contrast to post-edits and
references, is fast and cheap to elicit from non-
professionals, our results bear a great potential for
future applications on larger scale.

Acknowledgments.

This work was supported in part by DFG Research
Grant RI 2221/4-1, and by an internship program
of the IWR at Heidelberg University.



1786

References
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,

Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. In Proceedings
of the International Conference on Learning Repre-
sentations (ICLR). Toulon, France.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR). San Diego, CA, USA.

Léon Bottou, Jonas Peters, Joaquin Quiñonero-
Candela, Denis X. Charles, D. Max Chickering,
Elon Portugaly, Dipanakar Ray, Patrice Simard, and
Ed Snelson. 2013. Counterfactual reasoning and
learning systems: The example of computational ad-
vertising. Journal of Machine Learning Research
14:3207–3260.

Ralph Allan Bradley and Milton E. Terry. 1952. Rank
analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika 39(3-4):324–
345.

Paul F. Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. In
Advances in Neural Information Processing Systems
(NIPS). Long Beach, CA, USA.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT). Portland, OR, USA.

Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation (WMT). Edinburgh, Scotland.

Markus Freitag and Yaser Al-Onaizan. 2016. Fast
domain adaptation for neural machine translation.
CoRR abs/1612.06897.

Spence Green, Sida I. Wang, Jason Chuang, Jeffrey
Heer, Sebastian Schuster, and Christopher D. Man-
ning. 2014. Human effort and machine learnabil-
ity in computer aided translation. In Proceedings
the onference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Doha, Qatar.

Jindřich Helcl and Jindřich Libovický. 2017. Neural
Monkey: An Open-source Tool for Sequence Learn-
ing. The Prague Bulletin of Mathematical Linguis-
tics (107):5–17.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Nan Jiang and Lihong Li. 2016. Doubly robust off-
policy value evaluation for reinforcement learning.
In Proceedings of the 33rd International Conference
on Machine Learning (ICML). New York, NY, USA.

Kshitij Judah, Saikat Roy, Alan Fern, and Thomas G.
Dietterich. 2019. Reinforcement learning via prac-
tice and critique advice. In Proceedings of the 24th
AAAI Conference on Artificial Intelligence. Atlanta,
GA, USA.

Sham Kakade. 2001. A natural policy gradient. In
Advances in Neural Information Processing Systems
(NIPS). Vancouver, Canada.

Hyun Kim, Jong-Hyeok Lee, and Seung-Hoon Na.
2017. Predictor-estimator using multilevel task
learning with stack propagation for neural quality
estimation. In Proceedings of the Second Confer-
ence on Machine Translation (WMT). Copenhagen,
Denmark.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Doha, Qatar.

W. Bradley Knox and Peter Stone. 2009. Interac-
tively shaping agents via human reinforcement: The
TAMER framework. In Proceedings of the Interna-
tional Conference on Knowledge Capture (K-CAP).
Redondo Beach, CA, USA.

Vijay R. Konda and John N. Tsitsiklis. 2000. Actor-
critic algorithms. In Advances in Neural In-
formation Processing Systems (NIPS). Vancouver,
Canada.

Julia Kreutzer, Shahram Khadivi, Evgeny Matusov,
and Stefan Riezler. 2018. Can neural machine trans-
lation be improved with user feedback? In Pro-
ceedings of the 16th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
- Industry Track (NAACL-HLT). New Orleans, LA,
USA.

Julia Kreutzer, Artem Sokolov, and Stefan Riezler.
2017. Bandit structured prediction for neural
sequence-to-sequence learning. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (ACL). Vancouver, Canada.

Klaus Krippendorff. 2013. Content Analysis. An Intro-
duction to Its Methodology. Sage, third edition.

Richard Larsen and Morris Marx. 2012. An Introduc-
tion to Mathematical Statistics and Its Applications.
Prentice Hall, fifth edition.

Carolin Lawrence, Artem Sokolov, and Stefan Riezler.
2017. Counterfactual learning from bandit feedback
under deterministic logging: A case study in statisti-
cal machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Copenhagen, Denmark.



1787

Chen Liang, Jonathan Berant, Quoc Le, Kenneth D.
Forbus, and Ni Lao. 2017. Neural symbolic ma-
chines: Learning semantic parsers on freebase with
weak supervision. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (ACL). Vancouver, Canada.

Minh-Thang Luong and Christopher D. Manning.
2015. Stanford neural machine translation systems
for spoken language domains. In Proceedings of the
International Workshop on Spoken Language Trans-
lation (IWSLT). Da Nang, Vietnam.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectifier nonlinearities improve neural net-
work acoustic models. In ICML Workshop on Deep
Learning for Audio, Speech and Language Process-
ing. Atlanta, GA, USA.

James MacGlashan, Mark K. Ho, Robert Loftin, Bei
Peng, Guan Wang, David L. Roberts, Matthew E.
Taylor, and Michael L. Littman. 2017. Interactive
learning from policy-dependent human feedback. In
Proceedings of the 34th International Conference on
Machine Learning (ICML). Sydney, Australia.

André Martins, Marcin Junczys-Dowmunt, Fabio Ke-
pler, Ramón Astudillo, Chris Hokamp, and Roman
Grundkiewicz. 2017. Pushing the limits of transla-
tion quality estimation. Transactions of the Associ-
ation for Computational Linguistics (TACL) 5:205–
218.

Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi
Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu.
2016. Asynchronous methods for deep reinforce-
ment learning. In Proceedings of the 33rd Inter-
national Conference on Machine Learning (ICML).
New York, NY, USA.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidje-
land, Georg Ostrovski, Stig Petersen, Charles Beat-
tie, Amir Sadik, Ioannis Antonoglou, Helen King,
Dharshan Kumaran, Daan Wierstra, Shane Legg,
and Demis Hassabis. 2015. Human-level con-
trol through deep reinforcement learning. Nature
518:529–533.

Khanh Nguyen, Hal Daumé, and Jordan Boyd-Graber.
2017. Reinforcement learning for bandit neural ma-
chine translation with simulated feedback. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP). Copen-
hagen, Denmark.

Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL). Philadelphia, PA, USA.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. CoRR abs/1705.04304.

Patrick M. Pilarski, Michael R. Dawson, Thomas De-
gris, Farbod Fahimi, Jason P. Carey, and Richard S.
Sutton. 2011. Online human training of a myoelec-
tric prosthesis controller via actor-critic reinforce-
ment learning. In Proceedings of the IEEE In-
ternational Conference on Rehabilitation Robotics.
Zürich, Switzerland.

Maja Popović. 2015. chrf: character n-gram f-score
for automatic mt evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation
(WMT). Lisbon, Portugal.

Doina Precup, Richard S. Sutton, and Sanjoy Dasgupta.
2001. Off-policy temporal-difference learning with
function approximation. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing (ICML). Williams College, MA, USA.

Doina Precup, Richard S. Sutton, and Satinder P. Singh.
2000. Eligibility traces for off-policy policy eval-
uation. In Proceedings of the Seventeenth Inter-
national Conference on Machine Learning (ICML).
San Francisco, CA, USA.

John Schulman, Sergey Levine, Philipp Moritz,
Michael I. Jordan, and Pieter Abbeel. 2015. Trust
region policy optimization. In Proceedings of the
31st International Conferene on Machine Learning
(ICML). Lille, France.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (ACL). Berlin, Germany.

Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro,
and Karthik Sridharan. 2010. Learnability, stabil-
ity and uniform convergence. Journal of Machine
Learning Research 11:2635–2670.

Shiqi Shen, Yong Cheng, Zongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL). Berlin,
Germany.

David Silver, Aja Huang, Chris J. Maddison, Arthur
Guez, Laurent Sifre, George van den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, Sander Dieleman, Do-
minik Grewe, John Nham, Nal Kalchbrenner, Ilya
Sutskever, Timothy Lillicrap, Madeleine Leach, Ko-
ray Kavukcuoglu, Thore Graepel, and Demis Has-
sabis. 2016. Mastering the game of go with deep
neural networks and tree search. Nature 529:484–
489.



1788

Artem Sokolov, Julia Kreutzer, Christopher Lo, and
Stefan Riezler. 2016a. Learning structured predic-
tors from bandit feedback for interactive NLP. In
Proceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL). Berlin,
Germany.

Artem Sokolov, Julia Kreutzer, Christopher Lo, and
Stefan Riezler. 2016b. Stochastic structured predic-
tion under bandit feedback. In Advances in Neural
Information Processing Systems (NIPS). Barcelona,
Spain.

Miloš Stanojević and Khalil Sima’an. 2014. Fit-
ting sentence level translation evaluation with many
dense features. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Doha, Qatar.

Richard S. Sutton, David McAllester, Satinder Singh,
and Yishay Mansour. 2000. Policy gradient methods
for reinforcement learning with function approxima-
tion. In Advances in Neural Information Process-
ings Systems (NIPS). Vancouver, Canada.

Adith Swaminathan and Thorsten Joachims. 2015a.
Counterfactual risk minimization: Learning from
logged bandit feedback. In International Confer-
ence on Machine Learning (ICML). Lille, France.

Adith Swaminathan and Thorsten Joachims. 2015b.
The self-normalized estimator for counterfactual
learning. In Advances in Neural Information Pro-
cessing Systems (NIPS). Montreal, Canada.

Philip S. Thomas and Emma Brunskill. 2016. Data-
efficient off-policy policy evaluation for reinforce-
ment learning. In Proceedings of the 33nd Inter-
national Conference on Machine Learning (ICML).
New York, NY, USA.

Louis Leon Thurstone. 1927. A law of comparative
judgement. Psychological Review 34:278–286.

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning 8:229–256.

Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin,
Jianhuang Lai, and Tie-Yan Liu. 2017. Adversarial
neural machine translation. CoRR abs/1704.06933.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR
abs/1609.08144.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial
nets with policy gradient. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelli-
gence (AAAI). San Francisco, CA, USA.


