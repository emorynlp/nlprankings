










































Driving Semantic Parsing from the World's Response


Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 18–27,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Driving Semantic Parsing from the World’s Response

James Clarke Dan Goldwasser Ming-Wei Chang Dan Roth
Department of Computer Science

University of Illinois
Urbana, IL 61820

{clarkeje,goldwas1,mchang21,danr}@illinois.edu

Abstract

Current approaches to semantic parsing,
the task of converting text to a formal
meaning representation, rely on annotated
training data mapping sentences to logi-
cal forms. Providing this supervision is
a major bottleneck in scaling semantic
parsers. This paper presents a new learn-
ing paradigm aimed at alleviating the su-
pervision burden. We develop two novel
learning algorithms capable of predicting
complex structures which only rely on a
binary feedback signal based on the con-
text of an external world. In addition we
reformulate the semantic parsing problem
to reduce the dependency of the model on
syntactic patterns, thus allowing our parser
to scale better using less supervision. Our
results surprisingly show that without us-
ing any annotated meaning representations
learning with a weak feedback signal is ca-
pable of producing a parser that is compet-
itive with fully supervised parsers.

1 Introduction

Semantic Parsing, the process of converting text
into a formal meaning representation (MR), is one
of the key challenges in natural language process-
ing. Unlike shallow approaches for semantic in-
terpretation (e.g., semantic role labeling and in-
formation extraction) which often result in an in-
complete or ambiguous interpretation of the natu-
ral language (NL) input, the output of a semantic
parser is a complete meaning representation that
can be executed directly by a computer program.

Semantic parsing has mainly been studied in the
context of providing natural language interfaces
to computer systems. In these settings the target
meaning representation is defined by the seman-
tics of the underlying task. For example, provid-

ing access to databases: a question posed in nat-
ural language is converted into a formal database
query that can be executed to retrieve information.
Example 1 shows a NL input query and its corre-
sponding meaning representation.

Example 1 Geoquery input text and output MR
“What is the largest state that borders Texas?”
largest(state(next to(const(texas))))

Previous works (Zelle and Mooney, 1996; Tang
and Mooney, 2001; Zettlemoyer and Collins,
2005; Ge and Mooney, 2005; Zettlemoyer and
Collins, 2007; Wong and Mooney, 2007) employ
machine learning techniques to construct a seman-
tic parser. The learning algorithm is given a set of
input sentences and their corresponding meaning
representations, and learns a statistical semantic
parser — a set of rules mapping lexical items and
syntactic patterns to their meaning representation
and a score associated with each rule. Given a sen-
tence, these rules are applied recursively to derive
the most probable meaning representation. Since
semantic interpretation is limited to syntactic pat-
terns identified in the training data, the learning
algorithm requires considerable amounts of anno-
tated data to account for the syntactic variations
associated with the meaning representation. An-
notating sentences with their MR is a difficult,
time consuming task; minimizing the supervision
effort required for learning is a major challenge in
scaling semantic parsers.

This paper proposes a new model and learning
paradigm for semantic parsing aimed to alleviate
the supervision bottleneck. Following the obser-
vation that the target meaning representation is to
be executed by a computer program which in turn
provides a response or outcome; we propose a re-
sponse driven learning framework capable of ex-
ploiting feedback based on the response. The feed-
back can be viewed as a teacher judging whether
the execution of the meaning representation pro-
duced the desired response for the input sentence.

18



This type of supervision is very natural in many
situations and requires no expertise, thus can be
supplied by any user.

Continuing with Example 1, the response gen-
erated by executing a database query would be
used to provide feedback. The feedback would be
whether the generated response is the correct an-
swer for the input question or not, in this case New
Mexico is the desired response.

In response driven semantic parsing, the learner
is provided with a set of natural language sen-
tences and a feedback function that encapsulates
the teacher. The feedback function informs the
learner whether its interpretation of the input sen-
tence produces the desired response. We consider
scenarios where the feedback is provided as a bi-
nary signal, correct +1 or incorrect −1.

This weaker form of supervision poses a chal-
lenge to conventional learning methods: semantic
parsing is in essence a structured prediction prob-
lem requiring supervision for a set of interdepen-
dent decisions, while the provided supervision is
binary, indicating the correctness of a generated
meaning representation. To bridge this difference
we propose two novel learning algorithms suited
to the response driven setting.

Furthermore, to account for the many syntac-
tic variations associated with the MR, we propose
a new model for semantic parsing that allows us
to learn effectively and generalize better. Cur-
rent semantic parsing approaches extract parsing
rules mapping NL to their MR, restricting pos-
sible interpretations to previously seen syntactic
patterns. We replace the rigid inference process
induced by the learned parsing rules with a flex-
ible framework. We model semantic interpreta-
tion as a sequence of interdependent decisions,
mapping text spans to predicates and use syntac-
tic information to determine how the meaning of
these logical fragments should be composed. We
frame this process as an Integer Linear Program-
ming (ILP) problem, a powerful and flexible in-
ference framework that allows us to inject rele-
vant domain knowledge into the inference process,
such as specific domain semantics that restrict the
space of possible interpretations.

We evaluate our learning approach and model
on the well studied Geoquery domain (Zelle and
Mooney, 1996; Tang and Mooney, 2001), a
database consisting of U.S. geographical informa-
tion, and natural language questions. Our experi-

mental results show that our model with response
driven learning can outperform existing models
trained with annotated logical forms.

The key contributions of this paper are:

Response driven learning for semantic parsing
We propose a new learning paradigm for learn-
ing semantic parsers without any annotated mean-
ing representations. The supervision for learning
comes from a binary feedback signal based a re-
sponse generated by executing a meaning repre-
sentation. This type of supervision signal is nat-
ural to produce and can be acquired from non-
expert users.

Novel training algorithms Two novel train-
ing algorithms are developed within the response
driven learning paradigm. The training algorithms
are applicable beyond semantic parsing and can be
used in situations where it is possible to obtain bi-
nary feedback for a structured learning problem.

Flexible semantic interpretation process We
propose a novel flexible semantic parsing model
that can handle previously unseen syntactic varia-
tions of the meaning representation.

2 Semantic Parsing

The goal of semantic parsing is to produce a func-
tion F : X → Z that maps from the space natural
language input sentences, X , to the space of mean-
ing representations, Z . This type of task is usu-
ally cast as a structured output prediction problem,
where the goal is to obtain a model that assigns the
highest score to the correct meaning representa-
tion given an input sentence. However, in the task
of semantic parsing, this decision relies on identi-
fying a hidden intermediate representation (or an
alignment) that captures the way in which frag-
ments of the text correspond to the meaning repre-
sentation. Therefore, we formulate the prediction
function as follows:

ẑ = Fw(x) = arg max
y∈Y ,z∈Z

w
T Φ(x,y, z) (1)

Where Φ is a feature function that describes the
relationships between an input sentence x, align-
ment y and meaning representation z. w is the
weight vector which contains the parameters of the
model. We refer to the arg max above as the in-
ference problem. The feature function combined
with the nature of the inference problem defines
the semantic parsing model. The key to producing

19



What is the largest Texas?

largest( const(texas))))

New Mexico

x:

y:

z:

r:

that bordersstate

state( next_to(

Figure 1: Example input sentence, meaning repre-
sentation, alignment and answer for the Geoquery
domain

a semantic parser involves defining a model and a
learning algorithm to obtain w.

In order to exemplify these concepts we con-
sider the Geoquery domain. Geoquery contains a
query language for a database of U.S. geograph-
ical facts. Figure 1 illustrates concrete examples
of the terminology introduce. The input sentences
x are natural language queries about U.S. geog-
raphy. The meaning representations z are logical
forms which can be executed on the database to
obtain a response which we denote with r. The
alignment y captures the associations between x
and z.

Building a semantic parser involves defining the
model (feature function Φ and inference problem)
and a learning strategy to obtain weights (w) as-
sociated with the model. We defer discussion of
our model until Section 4 and first focus on our
learning strategy.

3 Structured Learning with Binary
Feedback

Previous approaches to semantic parsing have
assumed a fully supervised setting where
a training set is available consisting of ei-
ther: input sentences and logical forms
{(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins,
2005)) or input sentences, logical forms
and a mapping between their constituents
{(xl,yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)).
Given such training examples a weight vector w
can be learned using structured learning methods.
Obtaining, through annotation or other means, this
form of training data is an expensive and difficult
process which presents a major bottleneck for
semantic parsing.

To reduce the burden of annotation we focus
on a new learning paradigm which uses feedback
from a teacher. The feedback signal is binary
(+1,−1) and informs the learner whether a pre-
dicted logical form ẑ when executed on the target

Algorithm 1 Direct Approach (Binary Learning)

Input: Sentences {xl}Nl=1,
Feedback : X ×Z → {+1, 1},
initial weight vector w

1: Bl ← {} for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: ŷ, ẑ = arg max

y,z w
T Φ(xl,y, z)

5: f = Feedback (xl, ẑ)
6: add (Φ(xl, ŷ, ẑ)/|xl|, f) to Bl
7: end for
8: w← BinaryLearn(B) where B = ∪lBl
9: until no Bl has new unique examples

10: return w

domain produces the desired response or outcome.
This is a very natural method for providing super-
vision in many situations and requires no exper-
tise. For example, a user can observe the response
and provide a judgement. The general form of
the teacher’s feedback is provided by a function
Feedback : X × Z → {+1,−1}.

For the Geoquery domain this amounts to
whether the logical form produces the correct re-
sponse r for the input sentence. Geoquery has the
added benefit that the teacher can be automated
if we have a dataset consisting of input sentences
and response pairs {(xl, rl)}Nl=1. Feedback eval-
uates whether a logical form produces a response
matching r:

Feedback (xl, z) =

{

+1 if execute(z) = rl

−1 otherwise

We are now ready to present our learning
with feedback algorithms that operate in situations
where input sentences, {xl}Nl=1, and a teacher
feedback mechanism, Feedback , are available. We
do not assume the availability of logical forms.

3.1 Direct Approach (Binary Learning)

In general, a weight vector can be considered
good if when used in the inference problem (Equa-
tion (1)) it scores the correct logical form and
alignment (which may be hidden) higher than all
other logical forms and alignments for a given in-
put sentence. The intuition behind the direct ap-
proach is that the feedback function can be used to
subsample the space of possible structures (align-
ments and logical forms (Y × Z)) for a given in-
put x. The feedback mechanism indicates whether
the structure is good (+1) or bad (−1). Using this

20



intuition we can cast the problem of learning a
weight vector for Equation (1) as a binary classifi-
cation problem where we directly consider struc-
tures the feedback assigns +1 as positive examples
and those assigned −1 as negative.

We represent the input to the binary classifier
as the feature vector Φ(x,y, z) normalized by the
size of the input sentence1 |x|, and the label as the
result from Feedback (x, z).

Algorithm 1 outlines the approach in detail. The
first stage of the algorithm iterates over all the
training input sentences and computes the best
logical form ẑ and alignment ŷ by solving the in-
ference problem (line 4). The feedback function
is queried (line 5) and a training example for the
binary predictor created using the normalized fea-
ture vector from the triple containing the sentence,
alignment and logical form as input and the feed-
back as the label. This training example is added
to the working set of training examples for this in-
put sentence (line 6). All the feedback training ex-
amples are used to train a binary classifier whose
weight vector is used in the next iteration (line 8).
The algorithm repeats until no new unique training
examples are added to any of the working sets for
any input sentence. Although the number of possi-
ble training examples is very large, in practice the
algorithm is efficient and converges quickly. Note
that this approach is capable of using a wide va-
riety of linear classifiers as the base learner (line
8).

A policy is required to specify the nature of
the working set of training examples (Bl) used for
training the base classifier. This is pertinent in line
6 of the algorithm. Possible policies include: al-
lowing duplicates in the working set (i.e., Bl is
a multiset), disallowing duplicates (Bl is a set),
or only allowing one example per input sentence
(‖Bl‖ = 1). We adopt the first approach in this
paper.2

3.2 Aggressive Approach (Structured
Learning)

There is important implicit information which
the direct approach ignores. It is implicit that
when the teacher indicates an input paired with
an alignment and logical form is good (+1 feed-

1Normalization is required to ensure that each sentence
contributes equally to the binary learning problem regardless
of the sentence’s length.

2The working set Bl for each sentence may contain multi-
ple positive examples with the same and differing alignments.

Algorithm 2 Aggressive Approach (Structured
Learning)

Input: Sentences {xl}Nl=1,
Feedback : X ×Z → {+1, 1},
initial weight vector w

1: Sl ← ∅ for all l = 1, . . . , N
2: repeat
3: for l = 1, . . . , N do
4: ŷ, ẑ = arg max

y,z w
T Φ(xl,y, z)

5: f = Feedback (xl, ẑ)
6: if f is +1 then
7: Sl ← {(x

l, ŷ, ẑ)}
8: end if
9: end for

10: w← StructLearn(S,Φ) where S = ∪lSl
11: until no Sl has changed
12: return w

back) that in order to repeat this behavior all other
competing structures should be made suboptimal
(or bad). To leverage this implicit information
we adopt a structured learning strategy in which
we consider the prediction as the optimal structure
and all others as suboptimal. This is in contrast to
the direct approach where only structures that have
explicitly received negative feedback are consid-
ered subopitmal.

When a structure is found with positive feed-
back it is added to the training pool for a struc-
tured learner. We consider this approach aggres-
sive as the structured learner implicitly considers
all other structures as being suboptimal. Negative
feedback indicates that the structure should not be
added to the training pool as it will introduce noise
into the learning process.

Algorithm 2 outlines the learning in more detail.
As before, ŷ and ẑ are predicted using the cur-
rent weight vector and feedback received (lines 4
and 5). When positive feedback is received a new
training instance for a structured learner is created
from the input sentence and prediction (line 7) this
training instance replaces any previous instance
for the input sentence. When negative feedback
is received the training pool Sl is not updated. A
weight vector is learned using a structured learner
where the training data S contains at most one ex-
ample per input sentence. In the first iteration of
the outer loop the training data S will contain very
few examples. In each subsequent iteration the
newly learned weight vector allows the algorithm
to acquire new examples. This is repeated until no

21



new examples are added or changed in S.
Like the direct approach, this learning frame-

work is makes very few assumptions about the
type of structured learner used as a base learner
(line 10).3

4 Model

Semantic parsing is the process of converting a
natural language input into a formal logic repre-
sentation. This process is performed by associat-
ing lexical items and syntactic patterns with logi-
cal fragments and composing them into a complete
formula. Existing approaches rely on extracting
a set of parsing rules, mapping text constituents
to a logical representation, from annotated train-
ing data and applying them recursively to obtain
the meaning representation. Adapting to new data
is a major limitation of these approaches as they
cannot handle inputs containing syntactic patterns
which were not observed in the training data. For
example, assume the training data produced the
following set of parsing rules:

Example 2 Typical parsing rules
(1) NP [λx.capital(x)]→ capital

(2) PP [ const(texas)]→ of Texas

(3) NNP [ const(texas)]→ Texas

(4) NP [capital(const(texas))]→

NP[λx.capital(x)] PP [ const(texas)]

At test time the parser is given the sentences in
Example 3. Despite the lexical similarity in these
examples, the semantic parser will correctly parse
the first sentence but fail to parse the second be-
cause the lexical items belong to different a syn-
tactic category (i.e., the word Texas is not part of a
preposition phrase in the second sentence).

Example 3 Syntactic variations of the same MR
Target logical form: capital(const(texas))

Sentence 1: “What is the capital of Texas?”

Sentence 2: “What is Texas’ capital?”

The ability to adapt to unseen inputs is one
of the key challenges in semantic parsing. Sev-
eral works (Zettlemoyer and Collins, 2007; Kate,
2008) have addressed this issue explicitly by man-
ually defining syntactic transformation rules that
can help the learned parser generalize better. Un-
fortunately these are only partial solutions as a

3Mistake driven algorithms that do not enforce margin
constraints may not be able to generalize using this proto-
col since they will repeat the same prediction at training time
and therefore will not update the model.

manually constructed rule set cannot cover the
many syntactic variations.

Given the previous example, we observe
that it is enough to identify that the function
capital(·) and the constant const(texas)
appear in the target MR, since there is only a single
way to compose these entities into a single formula
— capital(const(texas)).

Motivated by this observation we define our
meaning derivation process over the rules of the
MR language and use syntactic information as a
way to bias the MR construction process. That
is, our inference process considers the entire space
of meaning representations irrespective of the pat-
terns observed in the training data. This is possi-
ble as the MRs are defined by a formal language
and formal grammar.4 The syntactic information
present in the natural language is used as soft ev-
idence (features) which guides the inference pro-
cess to good meaning representations.

This formulation is a major shift from existing
approaches that rely on extracting parsing rules
from the training data. In existing approaches
the space of possible meaning representations is
constrained by the patterns in the training data
and syntactic structure of the natural language in-
put. Our formulation considers the entire space of
meaning representations and allows the model to
adapt to previously unseen data and always pro-
duce a semantic interpretation by using the pat-
terns observed in the input.

We frame our semantic interpretation process
as a constrained optimization process, maximiz-
ing the objective function defined by Equation 1
which relies on extracting lexical and syntactic
features instead of parsing rules. In the remain-
der of this section we explain the components of
our inference model.

4.1 Target Meaning Representation

Following previous work, we capture the se-
mantics of the Geoquery domain using a sub-
set of first-order logic consisting of typed con-
stants and functions. There are two types: en-
tities E in the domain and numeric values N .
Functions describe a functional relationship over
types (e.g., population : E → N ). A com-
plete logical form is constructed through func-
tional composition; in our formalism this is per-

4This is true for all meaning representations designed to
be executed by a computer system.

22



formed by the substitution operator. For ex-
ample, given the function next to(x) and
the expression const(texas), substitution re-
places the occurrence of the free variable x, with
the expression, resulting in a new logical form:
next to(const(texas)). Due to space lim-
itations we refer the reader to (Zelle and Mooney,
1996) for a detailed description of the Geoquery
domain.

4.2 Semantic Parsing as Constrained
Optimization

Recall that the goal of semantic parsing is to pro-
duce the following function (Equation (1)):

Fw(x) = arg max
y,z

w
T Φ(x,y, z)

However, given that y and z are complex struc-
tures it is necessary to decompose the structure
into a set of smaller decisions to facilitate efficient
inference.

In order to define our decomposition we intro-
duce additional notation: c is a constituent (or
word span) in the input sentence x and D is the
set of all function and constant symbols in the do-
main. The alignment y is defined as a set of map-
pings between constituents and symbols in the do-
main y = {(c, s)} where s ∈ D.

We decompose the construction of an alignment
and logical form into two types of decisions:
First-order decisions. A mapping between con-
stituents and logical symbols (functions and con-
stants).
Second-order decisions. Expressing how logi-
cal symbols are composed into a complete logical
interpretation. For example, whether next to
and state forms next to(state(·)) or
state(next to(·)).

Note that for all possible logical forms and
alignments there exists a one-to-one mapping to
these decisions.

We frame the inference problem as an Integer
Linear Programming (ILP) problem (Equation (2))
in which the first-order decisions are governed by
αcs, a binary decision variable indicating that con-
stituent c is aligned with logical symbol s. And
βcs,dt capture the second-order decisions indicat-
ing the symbol t (associated with constituent d)
is an argument to function s (associated with con-

stituent c).

Fw(x) = arg max
α,β

∑

c∈x

∑

s∈D

αcs ·w
T Φ1(x, c, s)

+
∑

c,d∈x

∑

s,t∈D

βcs,dt ·w
T Φ2(x, c, s, d, t) (2)

It is clear that there are dependencies between
the α-variables and β-variables. For example,
given that βcs,dt is active, the corresponding α-
variables αcs and αdt must also be active. In order
to ensure a consistent solution we introduce a set
of constraints on Equation (2). In addition we add
constraints which leverage the typing information
inherent in the domain to eliminate logical forms
that are invalid in the Geoquery domain. For ex-
ample, the function length only accepts river
types as input. The set of constraints are:

• A given constituent can be associated with
exactly one logical symbol.

• βcs,dt is active if and only if αcs and αdt are
active.

• If βcs,dt is active, s must be a function and
the types of s and t should be consistent.

• Functional composition is directional and
acyclic.

The flexibility of ILP has previously been advan-
tageous in natural language processing tasks (Roth
and Yih, 2007) as it allows us to easily incorporate
such constraints.

4.3 Features

The inference problem defined in Equation (2)
uses two feature functions: Φ1 and Φ2.

First-order decision features Φ1 Determining
if a logical symbol is aligned with a specific con-
stituent depends mostly on lexical information.
Following previous work (e.g., (Zettlemoyer and
Collins, 2005)) we create a small lexicon, mapping
logical symbols to surface forms.5 This lexicon is
small and only used as a starting point. Existing
approaches rely on annotated logical forms to ex-
tend the lexicon. However, in our setting we do
not have access to annotated logical forms, instead
we rely on external knowledge to supply further

5The lexicon contains on average 1.42 words per func-
tion and 1.07 words per constant. For example the function
next to has the lexical entries: borders, next, adjacent and
the constant illinois the lexical item illinois.

23



information. We add features which measure the
lexical similarity between a constituent and a logi-
cal symbol’s surface forms (as defined by the lexi-
con). Two metrics are used: stemmed word match
and a similarity metric based on WordNet (Miller
et al., 1990) which allows our model to account
for words not in the lexicon. The WordNet met-
ric measures similarity based on synonymy, hy-
ponymy and meronymy (Do et al., 2010). In the
case where the constituent is a preposition, which
are notorious for being ambiguous, we add a fea-
ture that considers the current lexical context (one
word to the left and right) in addition to word sim-
ilarity.

Second-order decision features Φ2 Determin-
ing how to compose two logical symbols relies on
syntactic information, in our model we use the de-
pendency tree (Klein and Manning, 2003) of the
input sentence. Given a second-order decision
βcs,dt, the dependency feature takes the normal-
ized distance between the head words in the con-
stituents c and d. A set of features also indicate
which logical symbols are usually composed to-
gether, without considering their alignment to text.

5 Experiments

In this section we describe our experimental setup,
which includes the details of the domain, re-
sources and parameters.

5.1 Domain and Corpus

We evaluate our system on the Geoquery domain
as described previously. The domain consists of
a database and Prolog query language for U.S.
geographical facts. The corpus contains of 880
natural language queries paired with Prolog log-
ical form queries ((x, z) pairs). We follow previ-
ous approaches and transform these queries into a
functional representation. We randomly select 250
sentences for training and 250 sentences for test-
ing.6 We refer to the training set as Response 250
(R250) indicating that each example x in this data
set has a corresponding desired database response
r. We refer the testing set as Query 250 (Q250)
where the examples only contain the natural lan-
guage queries.

6Our inference problem is less constrained than previous
approaches thus we limit the training data to 250 examples
due to scalability issues. We also prune the search space by
limiting the number of logical symbol candidates per word
(on average 13 logical symbols per word).

Precision and recall are typically used as eval-
uation metrics in semantic parsing. However, as
our model inherently has the ability to map any
input sentence into the space of meaning repre-
sentations the trade off between precision and re-
call does not exist. Thus, we report accuracy: the
percentage of meaning representations which pro-
duce the correct response. This is equivalent to
recall in previous work (Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007).

5.2 Resources and Parameters

Feedback Recall that our learning framework
does not require meaning representation annota-
tions. However, we do require a Feedback func-
tion that informs the learner whether a predicted
meaning representation when executed produces
the desired response for a given input sentence.
We automatically generate a set of natural lan-
guage queries and response pairs {(x, r)} by exe-
cuting the annotated logical forms on the database.
Using this data we construct an automatic feed-
back function as described in Section 3.

Domain knowledge Our learning approaches
require an initial weight vector as input. In or-
der to provide an initial starting point, we initialize
the weight vector using a similar procedure to the
one used in (Zettlemoyer and Collins, 2007) to set
weights for three features and a bias term. The
weights were developed on the training set using
the feedback function to guide our choices.

Underlying Learning Algorithms In the direct
approach the base linear classifier we use is a lin-
ear kernel Support Vector Machine with squared-
hinge loss. In the aggressive approach we de-
fine our base structured learner to be a structural
Support Vector Machine with squared-hinge loss
and use hamming distance as the distance func-
tion. We use a custom implementation to op-
timize the objective function using the Cutting-
Plane method, this allows us to parrallelize the
learning process by solving the inference problem
for multiple training examples simultaneously.

6 Results

Our experiments are designed to answer three
questions:

1. Is it possible to learn a semantic parser with-
out annotated logical forms?

24



Algorithm R250 Q250
NOLEARN 22.2 —
DIRECT 75.2 69.2
AGGRESSIVE 82.4 73.2
SUPERVISED 87.6 80.4

Table 1: Accuracy of learned models on R250 data and
Q250 (testing) data. NOLEARN: using initialized weight
vector, DIRECT: using feedback with the direct approach,
AGGRESSIVE: using feedback with the aggressive approach,
SUPERVISED: using gold 250 logical forms for training.
Note that none of the approaches use any annotated logical
forms besides the SUPERVISED approach.

Algorithm # LF Accuracy
AGGRESSIVE — 73.2
SUPERVISED 250 80.4
W&M 2006 ∼ 310 ∼ 60.0
W&M 2007 ∼ 310 ∼ 75.0
Z&C 2005 600 79.29
Z&C 2007 600 86.07
W&M 2007 800 86.59

Table 2: Comparison against previously published results.
Results show that with a similar number of logical forms
(# LF) for training our SUPERVISED approach outperforms
existing systems, while the AGGRESSIVE approach remains
competitive without using any logical forms.

2. How much performance do we sacrifice by
not restricting our model to parsing rules?

3. What, if any, are the differences in behaviour
between the two learning with feedback ap-
proaches?

We first compare how well our model performs
under four different learning regimes. NOLEARN
uses a manually initialized weight vector. DIRECT
and AGGRESSIVE use the two response driven
learning approaches, where a feedback function
but no logical forms are provided. As an up-
per bound we train the model using a fully SU-
PERVISED approach where the input sentences are
paired with hand annotated logical forms.

Table 1 shows the accuracy of each setup. The
model without learning (NOLEARN) gives a start-
ing point with an accuracy of 22.2%. The re-
sponse driven learning methods perform substan-
tially better than the starting point. The DIRECT
approach which uses a binary learner reaches an
accuracy of 75.2% on the R250 data and 69.2% on
the Q250 (testing) data. While the AGGRESSIVE
approach which uses a structured learner sees a
bigger improvement, reaching 82.4% and 73.2%
respectively. This is only 7% below the fully SU-
PERVISED upper bound of the model.

To answer the second question, we compare a
supervised version of our model to existing se-
mantic parsers. The results are in Table 2. Al-
though the numbers are not directly comparable
due to different splits in the data7, we can see that
with a similar number of logical forms for train-
ing our SUPERVISED approach outperforms ex-
isting systems (Wong and Mooney, 2006; Wong
and Mooney, 2007), while the AGGRESSIVE ap-
proach remains competitive without using any log-
ical forms. Our SUPERVISED model is still very
competitive with other approaches (Zettlemoyer
and Collins, 2007; Wong and Mooney, 2007),
which used considerably more annotated logical
forms in the training phase.

In order to answer the third question, we turn
our attention to the differences between the two
response driven learning approaches. The DIRECT
and AGGRESSIVE approaches use binary feedback
to learn, however they utilize the signal differently.
DIRECT uses the signal directly to learn a bi-
nary classifier capable of replicating the feedback,
whereas AGGRESSIVE learns a structured predic-
tor that can repeatedly obtain the logical forms
for which positive feedback was received. Thus,
although the AGGRESSIVE outperforms the DI-
RECT approach the concepts each approach learns
may be different. Analysis over the training data
shows that in 66.8% examples both approaches
predict a logical form that gives the correct an-
swer. While AGGRESSIVE correctly answers an
additional 16% which DIRECT gets incorrect. In
the opposite direction, DIRECT correctly answers
8.8% that AGGRESSIVE does not. Leaving only
8.4% of the examples that both approaches pre-
dict incorrect logical forms. This suggests that an
approach which combines DIRECT and AGGRES-
SIVE may be able to improve even further.

Figure 2 shows the accuracy on the entire train-
ing data (R250) at each iteration of learning. We
see that the AGGRESSIVE approach learns to cover
more of the training data and at a faster rate than
DIRECT. Note that the performance of the DI-
RECT approach drops at the first iteration. We hy-
pothesize this is due to imbalances in the binary
feedback dataset (too many negative examples) in
the first iteration.

7It is relatively difficult to compare different approaches
in the Geoquery domain given that many existing papers do
not use the same data split.

25



70 1 2 3 4 5 6

90

0

10

20

30

40

50

60

70

80

Learning Iterations

A
c
c
u
ra
c
y
 o
n
 R
e
s
p
o
n
s
e
 2
5
0

Direct Approach

Aggressive Approach

Initialization

Figure 2: Accuracy on training set as number of learning
iterations increases.

7 Related Work

Learning to map sentences to a meaning repre-
sentation has been studied extensively in the NLP
community. Early works (Zelle and Mooney,
1996; Tang and Mooney, 2000) employed induc-
tive logic programming approaches to learn a se-
mantic parser. More recent works apply statisti-
cal learning methods to the problem. In (Ge and
Mooney, 2005; Nguyen et al., 2006), the input to
the learner consists of complete syntactic deriva-
tions for the input sentences annotated with logi-
cal expressions. Other works (Wong and Mooney,
2006; Kate and Mooney, 2006; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009) try to alleviate the
annotation effort by only taking sentence and log-
ical form pairs to train the models. Learning is
then defined over hidden patterns in the training
data that associate logical symbols with lexical
and syntactic elements.

In this work we take an additional step to-
wards alleviating the difficulty of training seman-
tic parsers and present a world response based
training protocol. Several recent works (Chen and
Mooney, 2008; Liang et al., 2009; Branavan et
al., 2009) explore using an external world context
as a supervision signal for semantic interpretation.
These works operate in settings different to ours as
they rely on an external world state that is directly
referenced by the input text. Although our frame-
work can also be applied in these settings we do
not assume that the text can be grounded in a world
state. In our experiments the input text consists of
generalized statements which describe some infor-
mation need that does not correspond directly to a

grounded world state.
Our learning framework closely follows recent

work on learning from indirect supervision. The
direct approach resembles learning a binary clas-
sifier over a latent structure (Chang et al., 2010a);
while the aggressive approach has similarities with
work that uses labeled structures and a binary
signal indicating the existence of good structures
to improve structured prediction (Chang et al.,
2010b).

8 Conclusions

In this paper we tackle one of the key bottlenecks
in semantic parsing — providing sufficient super-
vision to train a semantic parser. Our solution is
two fold, first we present a new training paradigm
for semantic parsing that relies on natural, hu-
man level supervision. Second, we suggest a new
model for semantic interpretation that does not
rely on NL syntactic parsing rules, but rather uses
the syntactic information to bias the interpretation
process. This approach allows the model to gener-
alize better and reduce the required amount of su-
pervision. We demonstrate the effectiveness of our
training paradigm and interpretation model over
the Geoquery domain, and show that our model
can outperform fully supervised systems.

Acknowledgements We are grateful to Rohit Kate and
Raymond Mooney for their help with the Geoquery dataset.

Thanks to Yee Seng Chan, Nick Rizzolo, Shankar Vembu

and the three anonymous reviewers for their insightful com-

ments. This material is based upon work supported by the

Air Force Research Laboratory (AFRL) under prime contract

no. FA8750-09-C-0181 and by DARPA under the Bootstrap

Learning Program. Any opinions, findings, and conclusion or

recommendations expressed in this material are those of the

authors and do not necessarily reflect the views of the AFRL

or DARPA.

References

S.R.K. Branavan, H. Chen, L. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement learning for map-
ping instructions to actions. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).

M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010a. Discriminative learning over constrained la-
tent representations. In Proc. of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).

26



M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010b. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).

D. Chen and R. Mooney. 2008. Learning to sportscast:
a test of grounded language acquisition. In Proc. of
the International Conference on Machine Learning
(ICML).

Q. Do, D. Roth, M. Sammons, Y. Tu, and V.G. Vydis-
waran. 2010. Robust, Light-weight Approaches to
compute Lexical Similarity. Computer Science Re-
search and Technical Reports, University of Illinois.
http://hdl.handle.net/2142/15462.

R. Ge and R. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).

R. Kate and R. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics (ACL).

R. Kate. 2008. Transforming meaning representation
grammars to improve semantic parsing. In Proc. of
the Annual Conference on Computational Natural
Language Learning (CoNLL).

D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Proc. of the Conference on Advances in
Neural Information Processing Systems (NIPS).

P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).

G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography.

L. Nguyen, A. Shimazu, and X. Phan. 2006. Semantic
parsing with structured svm ensemble classification
models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).

D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning.

L. Tang and R. Mooney. 2000. Automated construc-
tion of database interfaces: integrating statistical and
relational learning for semantic parsing. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP).

L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming
for semantic parsing. In Proc. of the European Con-
ference on Machine Learning (ECML).

Y.-W. Wong and R. Mooney. 2006. Learning for
semantic parsing with statistical machine transla-
tion. In Proc. of the Annual Meeting of the North
American Association of Computational Linguistics
(NAACL).

Y.-W. Wong and R. Mooney. 2007. Learning
synchronous grammars for semantic parsing with
lambda calculus. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

J. M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic proramming.
In Proc. of the National Conference on Artificial In-
telligence (AAAI).

L. Zettlemoyer and M. Collins. 2005. Learning to map
sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proc. of
the Annual Conference in Uncertainty in Artificial
Intelligence (UAI).

L. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to logical
form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Pro-
cessing and on Computational Natural Language
Learning (EMNLP-CoNLL).

L. Zettlemoyer and M. Collins. 2009. Learning
context-dependent mappings from sentences to log-
ical form. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).

27


