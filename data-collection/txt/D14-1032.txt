



















































Learning Abstract Concept Embeddings from Multi-Modal Data: Since You Probably Can't See What I Mean


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255–265,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Learning Abstract Concept Embeddings from Multi-Modal Data:
Since You Probably Can’t See What I Mean

Felix Hill
Computer Laboratory

University of Cambridge
felix.hill@cl.cam.ac.uk

Anna Korhonen
Computer Laboratory

University of Cambridge
anna.korhonen@cl.cam.ac.uk

Abstract

Models that acquire semantic represen-
tations from both linguistic and percep-
tual input are of interest to researchers
in NLP because of the obvious parallels
with human language learning. Perfor-
mance advantages of the multi-modal ap-
proach over language-only models have
been clearly established when models are
required to learn concrete noun concepts.
However, such concepts are comparatively
rare in everyday language. In this work,
we present a new means of extending
the scope of multi-modal models to more
commonly-occurring abstract lexical con-
cepts via an approach that learns multi-
modal embeddings. Our architecture out-
performs previous approaches in combin-
ing input from distinct modalities, and
propagates perceptual information on con-
crete concepts to abstract concepts more
effectively than alternatives. We discuss
the implications of our results both for op-
timizing the performance of multi-modal
models and for theories of abstract con-
ceptual representation.

1 Introduction

Multi-modal models that learn semantic represen-
tations from both language and information about
the perceptible properties of concepts were orig-
inally motivated by parallels with human word
learning (Andrews et al., 2009) and evidence that
many concepts are grounded in perception (Barsa-
lou and Wiemer-Hastings, 2005). The perceptual
information in such models is generally mined di-
rectly from images (Feng and Lapata, 2010; Bruni
et al., 2012) or from data collected in psychologi-
cal studies (Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013).

By exploiting the additional information en-
coded in perceptual input, multi-modal models
can outperform language-only models on a range
of semantic NLP tasks, including modelling sim-
ilarity (Bruni et al., 2014; Kiela et al., 2014) and
free association (Silberer and Lapata, 2012), pre-
dicting compositionality (Roller and Schulte im
Walde, 2013) and concept categorization (Silberer
and Lapata, 2014). However, to date, these pre-
vious approaches to multi-modal concept learning
focus on concrete words such as cat or dog, rather
than abstract concepts, such as curiosity or loyalty.
However, differences between abstract and con-
crete processing and representation (Paivio, 1991;
Hill et al., 2013; Kiela et al., 2014) suggest that
conclusions about concrete concept learning may
not necessarily hold in the general case. In this pa-
per, we therefore focus on multi-modal models for
learning both abstract and concrete concepts.

Although concrete concepts might seem more
basic or fundamental, the vast majority of open-
class, meaning-bearing words in everyday lan-
guage are in fact abstract. 72% of the noun or
verb tokens in the British National Corpus (Leech
et al., 1994) are rated by human judges1 as more
abstract than the noun war, for instance, a con-
cept many would already consider to be quite
abstract. Moreover, abstract concepts by defi-
nition encode higher-level (more general) princi-
ples than concrete concepts, which typically re-
side naturally in a single semantic category or do-
main (Crutch and Warrington, 2005). It is there-
fore likely that abstract representations may prove
highly applicable for multi-task, multi-domain or
transfer learning models, which aim to acquire
‘general-purpose’ conceptual knowledge without
reference to a specific objective or task (Collobert
and Weston, 2008; Mesnil et al., 2012).

In a recent paper, Hill et al. (2014) investigate
whether the multi-modal models cited above are

1Contributors to the USF dataset (Nelson et al., 2004).

255



effective for learning concepts other than concrete
nouns. They observe that representations of cer-
tain abstract concepts can indeed be enhanced in
multi-modal models by combining perceptual and
linguistic input with an information propagation
step. Hill et al. (2014) propose ridge regression as
an alternative to the nearest-neighbour averaging
proposed by Johns and Jones (2012) for such prop-
agation, and show that it is more robust to changes
in the type of concept to be learned. However, both
methods are somewhat inelegant, in that they learn
separate linguistic and ‘pseudo-perceptual’ repre-
sentations, which must be combined via a separate
information combination step. Moreover, for the
majority of abstract concepts, the best performing
multi-modal model employing these techniques
remains less effective than conventional text-only
representation learning model.

Motivated by these observations, we introduce
an architecture for learning both abstract and con-
crete representations that generalizes the skipgram
model of Mikolov et al. (2013) from text-based to
multi-modal learning. Aspects of the model de-
sign are influenced by considering the process of
human language learning. The model moderates
the training input to include more perceptual infor-
mation about commonly-occurring concrete con-
cepts and less information about rarer concepts.
Moreover, it integrates the processes of combin-
ing perceptual and linguistic input and propagat-
ing information from concrete to abstract concepts
into a single representation update process based
on back-propagation.

We train our model on running-text language
and two sources of perceptual descriptors for con-
crete nouns: the ESPGame dataset of annotated
images (Von Ahn and Dabbish, 2004) and the
CSLB set of concept property norms (Devereux
et al., 2013). We find that our model combines in-
formation from the different modalities more ef-
fectively than previous methods, resulting in an
improved ability to model the USF free associa-
tion gold standard (Nelson et al., 2004) for con-
crete nouns. In addition, the architecture propa-
gates the extra-linguistic input for concrete nouns
to improve representations of abstract concepts
more effectively than alternative methods. While
this propagation can effectively extend the advan-
tage of the multi-modal approach to many more
concepts than simple concrete nouns, we observe
that the benefit of adding perceptual input appears

to decrease as target concepts become more ab-
stract. Indeed, for the most abstract concepts of
all, language-only models still provide the most
effective learning mechanism.

Finally, we investigate the optimum quantity
and type of perceptual input for such models. Be-
tween the most concrete concepts, which can be
effectively represented directly in the perceptual
modality, and the most abstract concepts, which
cannot, we identify a set of concepts that cannot
be represented effectively directly in the percep-
tual modality, but still benefit from perceptual in-
put propagated in the model via concrete concepts.

The motivation in designing our model and ex-
periments is both practical and theoretical. Taken
together, the empirical observations we present are
potentially important for optimizing the learning
of representations of concrete and abstract con-
cepts in multi-modal models. In addition, they of-
fer a degree of insight into the poorly understood
issue of how abstract concepts may be encoded in
human memory.

2 Model Design

Before describing how our multi-modal architec-
ture encodes and integrates perceptual informa-
tion, we first describe the underlying corpus-based
representation learning model.

Language-only Model Our multi-modal archi-
tecture builds on the continuous log-linear skip-
gram language model proposed by Mikolov et
al. (2013). This model learns lexical representa-
tions in a similar way to neural-probabilistic lan-
guage models (NPLM) but without a non-linear
hidden layer, a simplification that facilitates the
efficient learning of large vocabularies of dense
representations, generally referred to as embed-
dings (Turian et al., 2010). Embeddings learned
by the model achieve state-of-the-art performance
on several evaluations including sentence comple-
tion and analogy modelling (Mikolov et al., 2013).

For each word type w in the vocabulary V , the
model learns both a ‘target-embedding’ rw ∈ Rd
and a ‘context-embedding’ r̂w ∈ Rd such that,
given a target word, its ability to predict nearby
context words is maximized. The probability of
seeing context word c given target w is defined as:

p(c|w) = e
r̂c·rw∑

v∈V er̂v ·rw

256



  

w
n

Target Representation

Score: p(c|w)

Context Representations  Information Source
w

n+2

pw
n+2

w
n+1

pw
n+1

w
n-1

pw
n-1

w
n-2

pw
n-2

Linguistic
Text8 Corpus

Perceptual
P

ESP  
P

CSBL

Figure 1: Our multi-modal model architecture. Light boxes are elements of the original Mikolov et
al. (2013) model. For target words wn in the domain of P (concrete concepts), the model updates its
representations based on corpus context wordswn±i, then on words pwn±i in perceptual pseudo-sentences.
For wn not in the domain of P (abstract concepts), updates are based solely on the wn±i.

The model learns from a set of target-word,
context-word pairs, extracted from a corpus of
sentences as follows. In a given sentence S (of
length N ), for each position n ≤ N , each word
wn is treated in turn as a target word. An inte-
ger t(n) is then sampled from a uniform distribu-
tion on {1, . . . k}, where k > 0 is a predefined
maximum context-window parameter. The pair to-
kens {(wn, wn+j) : −t(n) ≤ j ≤ t(n), wi ∈ S}
are then appended to the training data. Thus, tar-
get/context training pairs are such that (i) only
words within a k-window of the target are selected
as context words for that target, and (ii) words
closer to the target are more likely to be selected
than those further away.

The training objective is then to maximize the
sum of the log probabilities T across of all such
examples from S and across all sentences in the
corpus, where T is defined as follows:

T =
1
N

N∑
n=1

∑
−t(n)≤j≤t(n),j 6=0

log(p(wn+j |wn))

The model free parameters (target-embeddings
and context-embeddings of dimension d for each
word in the corpus with frequency above a certain
threshold f ) are updated according to stochastic
gradient descent and backpropation, with learning
rate controlled by Adagrad (Duchi et al., 2011).
For efficiency, the output layer is encoded as a
hierarchical softmax function based on a binary
Huffman tree (Morin and Bengio, 2005).

As with other distributional architectures, the
model captures conceptual semantics by exploit-
ing the fact that words appearing in similar lin-
guistic contexts are likely to have similar mean-
ings. Informally, the model adjusts its embeddings

to increase the ‘probability’ of seeing the language
in the training corpus. Since this probability in-
creases with the p(c|w), and the p(c|w) increase
with the dot product r̂c · rw, the updates have the
effect of moving each target-embedding incremen-
tally ‘closer’ to the context-embeddings of its col-
locates. In the target-embedding space, this results
in embeddings of concept words that regularly oc-
cur in similar contexts moving closer together.

Multi-modal Extension We extend the Mikolov
et al. (2013) architecture via a simple means of in-
troducing perceptual information that aligns with
human language learning. Based on the assump-
tion that frequency in domain-general linguistic
corpora correlates with the likelihood of ‘experi-
encing’ a concept in the world (Bybee and Hop-
per, 2001; Chater and Manning, 2006), perceptual
information is introduced to the model whenever
designated concrete concepts are encountered in
the running-text linguistic input. This has the ef-
fect of introducing more perceptual input for com-
monly experienced concrete concepts and less in-
put for rarer concrete concepts.

To implement this process, perceptual informa-
tion is extracted from external sources and en-
coded in an associative array P, which maps (typ-
ically concrete) words w to bags of perceptual fea-
tures b(w). The construction of this array depends
on the perceptual information source; the process
for our chosen sources is detailed in Section 2.1.

Training our model begins as before on running-
text. When a sentence Sm containing a word w in
the domain of P is encountered, the model finishes
training on Sm and begins learning from a per-
ceptual pseudo-sentence Ŝm(w). Ŝm(w) is con-
structed by alternating the token w with a fea-

257



Ŝ(crocodile) = Crocodile legs crocodile teeth crocodile
teeth crocodile scales crocodile green crocodile.

Ŝ(screwdriver) = Screwdriver handle screwdriver flat
screwdriver long screwdriver handle screwdriver head.

Figure 2: Example pseudo-sentences generated by
our model.

ture sampled at random from b(w) until Ŝm(w)
is the same length as Sm (see Figure 2). Because
we want the ensuing perceptual learning process
to focus on how w relates to its perceptual prop-
erties (rather than how those properties relate to
each other), we insert multiple instances of w into
Ŝm(w). This ensures that the majority of train-
ing cases derived from Ŝm(w) are instances of (w,
feature) rather than (feature, feature) pairs. Once
training on Ŝm(w) is complete, the model reverts
to the next ‘genuine’ (linguistic) sentence Sm+1,
and the process continues. Thus, when a concrete
concept is encountered in the corpus, its embed-
ding is first updated based on language (moved in-
crementally closer to concepts appearing in sim-
ilar linguistic contexts), and then on perception
(moved incrementally closer to concepts with the
same or similar perceptual features).

For greater flexibility, we introduce a parameter
α reflecting the raw quantity of perceptual infor-
mation relative to linguistic input. When α = 2,
two pseudo-sentences are generated and inserted
for every corpus occurrence of a token from the
domain of P. For non-integral α, the number of
sentences inserted is bαc, and a further sentence is
added with probability α− bαc.

In all experiments reported in the following sec-
tions we set the window size parameter k = 5 and
the minimum frequency parameter f = 3, which
guarantees that the model learns embeddings for
all concepts in our evaluation sets. While the
model learns both target and context-embeddings
for each word in the vocabulary, we conduct our
experiments with the target embeddings only. We
set the dimension parameter d = 300 as this pro-
duces high quality embeddings in the language-
only case (Mikolov et al., 2013).

2.1 Information Sources

We construct the associative array of perceptual
information P from two sources typical of those
used for multi-modal semantic models.

ESPGame Dataset The ESP-Game dataset
(ESP) (Von Ahn and Dabbish, 2004) consists of
100,000 images, each annotated with a list of lex-
ical concepts that appear in that image.

For any concept w identified in an ESP im-
age, we construct a corresponding bag of features
b(w). For each ESP image I that contains w, we
append the other concept tokens identified in I to
b(w). Thus, the more frequently a concept co-
occurs with w in images, the more its correspond-
ing lexical token occurs in b(w). The array PESP
in this case then consists of the (w,b(w)) pairs.

CSLB Property Norms The Centre for Speech,
Language and the Brain norms (CSLB) (Devereux
et al., 2013) is a recently-released dataset contain-
ing semantic properties for 638 concrete concepts
produced by human annotators. The CSLB dataset
was compiled in the same way as the McRae et
al. (2005) property norms used widely in multi-
modal models (Silberer and Lapata, 2012; Roller
and Schulte im Walde, 2013); we use CSLB be-
cause it contains more concepts. For each concept,
the proportion of the 30 annotators that produced
a given feature can also be employed as a measure
of the strength of that feature.

When encoding the CSLB data in P, we first
map properties to lexical forms (e.g. is green
becomes green). By directly identifying percep-
tual features and linguistic forms in this way,
we treat features observed in the perceptual data
as (sub)concepts to be acquired via the same
multi-modal input streams and stored in the same
domain-general memory as the evaluation con-
cepts. This design decision in fact corresponds
to a view of cognition that is sometimes disputed
(Fodor, 1983). In future studies we hope to com-
pare the present approach to architectures with
domain-specific conceptual memories.

For each concept w in CSLB, we then con-
struct a feature bag b(w) by appending lexical
forms to b(w) such that the count of each fea-
ture word is equal to the strength of that feature
for w. Thus, when features are sampled from
b(w) to create pseudo-sentences (as detailed pre-
viously) the probability of a feature word occur-
ring in a sentence reflects feature strength. The
array PCSLB then consists of all (w,b(w)) pairs.

Linguistic Input The linguistic input to all
models is the 400m word Text8 Corpus2 of

2From http://mattmahoney.net/dc/textdata.html

258



ESPGame CSLB
Image 1 Image 2 Crocodile Screwdriver

red wreck has 4 legs (7) has handle (28)

chihuaua cyan has tail (18) has head (5)

eyes man has jaw (7) is long (9)

little crash has scales (8) is plastic (18)

ear accident has teeth (20) is metal (28)

nose street is green (10)

small is large (10)

Table 1: Concepts identified in images in the ESP
Game (left) and features produced for concepts by
human annotators in the CSLB dataset (with fea-
ture strength, max=30).

Concept 1 Concept 2 Assoc.
abdomen (6.83) stomach (6.04) 0.566
throw (4.05) ball (6.08) 0.234
hope (1.18) glory (3.53) 0.192
egg (5.79) milk (6.66) 0.012

Table 2: Example concept pairs (with mean con-
creteness rating) and free-association scores from
the USF dataset.

Wikipedia text, split into sentences and with punc-
tuation removed.

2.2 Evaluation

We evaluate the quality of representations by how
well they reflect free association scores, an em-
pirical measure of cognitive conceptual proxim-
ity. The University of South Florida Norms
(USF) (Nelson et al., 2004) contain free associa-
tion scores for over 40,000 concept pairs, and have
been widely used in NLP to evaluate semantic rep-
resentations (Andrews et al., 2009; Feng and La-
pata, 2010; Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013). Each concept that we
extract from the USF database has also been rated
for conceptual concreteness on a Likert scale of
1-7 by at least 10 human annotators. Following
previous studies (Huang et al., 2012; Silberer and
Lapata, 2012), we measure the (Spearman ρ) cor-
relation between association scores and the cosine
similarity of vector representations.

We create separate abstract and concrete con-
cept lists by ranking the USF concepts accord-
ing to concreteness and sampling at random from
the first and fourth quartiles respectively. We also
introduce a complementary noun/verb dichotomy,

Concept Type List Pairs Examples
concrete nouns 541 1418 yacht, cup
abstract nouns 100 295 fear, respect
all nouns 666 1815 fear, cup
concrete verbs 50 66 kiss, launch
abstract verbs 50 127 differ, obey
all verbs 100 221 kiss, obey

Table 3: Details the subsets of USF data used in
our evaluations, downloadable from our website.

on the intuition that information propagation may
occur differently from noun to noun or from noun
to verb (because of their distinct structural rela-
tionships in sentences). POS-tags are not assigned
as part of the USF data, so we draw the noun/verb
distinction based on the majority POS-tag of USF
concepts in the lemmatized British National Cor-
pus (Leech et al., 1994). The abstract/concrete
and noun/verb dichotomies yield four distinct con-
cept lists. For consistency, the concrete noun list
is filtered so that each concrete noun concept w
has a perceptual representation b(w) in both PESP
and PCSLB. For the four resulting concept lists
C (concrete/abstract, noun/verb), a correspond-
ing set of evaluation pairs {(w1, w2) ∈ USF :
w1, w2 ∈ C} is extracted (see Table 3 for details).

3 Results and Discussion

Our experiments were designed to answer four
questions, outlined in the following subsec-
tions: (1) Which model architectures perform best
at combining information pertinent to multiple
modalities when such information exists explicitly
(as common for concrete concepts)? (2) Which
model architectures best propagate perceptual in-
formation to concepts for which it does not exist
explicitly (as is common for abstract concepts)?
(3) Is it preferable to include all of the perceptual
input that can be obtained from a given source, or
to filter this input stream in some way? (4) How
much perceptual vs. linguistic input is optimal for
learning various concept types?

3.1 Combining information sources

To evaluate our approach as a method of in-
formation combination we compared its perfor-
mance on the concrete noun evaluation set against
three alternative methods. The first alternative
is simple concatenation of these perceptual vec-
tors with linguistic vectors embeddings learned

259



by the Mikolov et al. (2013) model on the Text8
Corpus. In the second alternative (proposed
for multi-modal models by Silberer and Lapata
(2012)), canonical correlation analysis (CCA)
(Hardoon et al., 2004) was applied to the vec-
tors of both modalities. CCA yields reduced-
dimensionality representations that preserve un-
derlying inter-modal correlations, which are then
concatenated. The final alternative, proposed by
Bruni et al. (2014) involves applying Singular
Value Decomposition (SVD) to the matrix of con-
catenated multi-modal representations, yielding
smoothed representations.3

When implementing the concatenation, CCA
and SVD methods, we first encoded the percep-
tual input directly into sparse feature vectors, with
coordinates for each of the 2726 features in CSLB
and for each of the 100,000 images in ESP. This
sparse encoding matches the approach taken by
Silberer and Lapata (2012), for CCA and concate-
nation, and by Hill et al. (2014) for the ridge re-
gression method of propagation (see below).

We compare these alternatives to our proposed
model with α = 1. In The CSLB and ESP models,
all training pseudo-sentences are generated from
the arrays PCSLB and PESP respectively. In the
models classed as CSLB&ESP, a random choice
between PCSLB and PESP is made every time
perceptual input is included (so that the overall
quantity of perceptual information is the same).

As shown in Figure 2 (left side), the embed-
dings learned by our model achieve a higher cor-
relation with the USF data than simple concatena-
tion, CCA and SVD regardless of perceptual input
source. With the optimal perceptual source (ESP
only), for instance, the correlation is 11% higher
that the next best alternative method, CCA.

One possible factor behind this improvement
is that, in our model, the learned representations
fully integrate the two modalities, whereas for
both CCA and the concatenation method each rep-
resentation feature (whether of reduced dimension
or not) corresponds to a particular modality. This
deeper integration may help our architecture to
overcome the challenges inherent in information
combination such as inter-modality differences in
information content and representation sparsity. It
is also important to note that Bruni et al. (2014) ap-

3CCA was implemented using the CCA package in
R. SVD was implemented using SVDLIBC (http://
tedlab.mit.edu/˜dr/SVDLIBC/), with truncation
factor k = 1024 as per (Bruni et al., 2014).

plied their SVD method with comparatively dense
perceptual representations extracted from images,
whereas our dataset-based perceptual vectors were
sparsely-encoded.

3.2 Propagating input to abstract concepts

To test the process of information propagation in
our model, we evaluated the learned embeddings
of more abstract concepts. We compared our
approach with two recently-proposed alternative
methods for inferring perceptual features when ex-
plicit perceptual information is unavailable.

Johns and Jones In the method of Johns and
Jones (2012), pseudo-perceptual representations
for target concepts without a perceptual repre-
sentations (uni-modal concepts) are inferred as a
weighted average of the perceptual representations
of concepts that do have such a representation (bi-
modal concepts).

In the first step of their two-step method, for
each uni-modal concept k, a quasi-perceptual rep-
resentation is computed as an average of the
perceptual representations of bi-modal concepts,
weighted by the proximity between each of these
concepts and k

kp =
∑
c∈C̄

S(kl, cl)λ · cp

where C̄ is the set of bi-modal concepts, cp and kp

are the perceptual representations for c and k re-
spectively, and cl and kl the linguistic representa-
tions. The exponent parameter λ reflects the learn-
ing rate.

In step two, the initial quasi-perceptual repre-
sentations are inferred for a second time, but with
the weighted average calculated over the percep-
tual or initial quasi-perceptual representations of
all other words, not just those that were originally
bi-modal. As with Johns and Jones (2012), we set
the learning rate parameter λ to be 3 in the first
step and 13 in the second.

Ridge Regression An alternative, proposed for
the present purpose by Hill et al. (2014), uses ridge
regression (Myers, 1990). Ridge regression is a
variant of least squares regression in which a reg-
ularization term is added to the training objective
to favor solutions with certain properties.

For bi-modal concepts of dimension np, we ap-
ply ridge regression to learn np linear functions

260



fi : Rnl → R that map the linguistic represen-
tations (of dimension nl) to a particular percep-
tual feature i. These functions are then applied
together to map the linguistic representations of
uni-modal concepts to full quasi-perceptual repre-
sentations.

Following Hill et al. (2014), we take the Euclid-
ian l2 norm of the inferred parameter vector as the
regularization term. This ensures that the regres-
sion favors lower coefficients and a smoother so-
lution function, which should provide better gen-
eralization performance than simple linear regres-
sion. The objective for learning the fi is then to
minimize

‖aX − Yi‖22 + ‖a‖22
where a is the vector of regression coefficients, X
is a matrix of linguistic representations and Yi a
vector of the perceptual feature i for the set of bi-
modal concepts.

Comparisons We applied the Johns and Jones
method and ridge regression starting from linguis-
tic embeddings acquired by the Mikolov et al.
(2013) model on the Text8 Corpus, and concate-
nated the resulting pseudo-perceptual and linguis-
tic representations. As with the implementation
of our model, the perceptual input for these alter-
native models was limited to concrete nouns (i.e.
concrete nouns were the only bi-modal concepts
in the models).

Figure 3 (right side) shows the propagation per-
formance of the three models. While the corre-
lations overall may seem somewhat low, this is
a consequence of the difficulty of modelling the
USF data. In fact, the performance of both the
language-only model and our multi-modal exten-
sion across the concept types (from 0.18 to 0.36) is
equal to or higher than previous models evaluated
on the same data (Feng and Lapata, 2010; Silberer
and Lapata, 2012; Silberer et al., 2013).

For learning representations of concrete verbs,
our approach achieves a 69% increase in perfor-
mance over the next best alternative. The perfor-
mance of the model on abstract verbs is marginally
inferior to Johns and Jones’ method. Neverthe-
less, the clear advantage for concrete verbs makes
our model the best choice for learning represen-
tations of verbs in general, as shown by perfor-
mance on the set all verbs, which also includes
mixed abstract-concrete pairs.

Our model is also marginally inferior to alterna-
tive approaches in learning representations of ab-

stract nouns. However, in this case, no method
improves on the linguistic-only baseline. It is
possible that perceptual information is simply so
removed from the core semantics of these con-
cepts that they are best acquired via the linguis-
tic medium alone, regardless of learning mecha-
nism. The moderately inferior performance of our
method in such cases is likely caused by its greater
inherent inter-modal dependence compared with
methods that simply concatenate uni-modal rep-
resentations. When the perceptual signal is of
low quality, this greater inter-modal dependence
allows the linguistic signal to be obscured.

The trade-off, however, is generally higher-
quality representations when the perceptual signal
is stronger, exemplified by the fact that our pro-
posed approach outperforms alternatives on pairs
generated from both abstract and concrete nouns
(all nouns). Indeed, the low performance of the
Johns and Jones method on all nouns is strik-
ing given that: (a) It performs best on abstract
nouns (ρ = .282), and (b) For concrete nouns it
reverts to simple concatenation, which also per-
forms comparatively well (ρ = .249). The poor
performance of the Jobns and Jones method on
all nouns must therefore derive its comparisons
of mixed abstract-concrete or concrete-abstract
pairs. This suggests that the pseudo-perceptual
representations inferred by this method for ab-
stract concepts method may not be compatible
with the directly-encoded perceptual representa-
tions of concrete concepts, rendering the compar-
ison computation between items of differing con-
creteness inaccurate.

3.3 Direct representation vs. propagation

Although property norm datasets such as the
CSLB data typically consist of perceptual fea-
ture information for concrete nouns only, image-
based datasets such as ESP do contain informa-
tion on more abstract concepts, which was omit-
ted from the previous experiments. Indeed, im-
age banks such as Google Images contain millions
of photographs portraying quite abstract concepts,
such as love or war. On the other hand, encod-
ings or descriptions of abstract concepts are gen-
erally more subjective and less reliable than those
of concrete concepts (Wiemer-Hastings and Xu,
2005). We therefore investigated whether or not
it is preferable to include this additional informa-
tion as model input or to restrict perceptual input

261



0.203
0.22

0.15

0.239

0.259
0.271

0.256

0.301

0.249
0.24

0.231

0.296

0.0

0.1

0.2

0.3

0.4

CSLB ESP CSLB & ESP
Concrete nouns − information combination

C
or

re
la

tio
n

Combination Method
Vector Concatenation

CCA
SVD

Our Model (α=1)

0.282
0.265

0.25

0.07

0.236

0.364

0.06

0.116

0.197

0.177 0.172 0.175 0.167
0.175

0.225

0.0

0.1

0.2

0.3

0.4

abstract nouns all nouns concrete verbs abstract verbs all verbs
More abstract concepts − information propagation (CSLB & ESP

C
or

re
la

tio
n

Propagation Method
Johns and Jones

Ridge Regression
Our Model (α=1)

Figure 3: The proposed approach compared with other methods of information combination (left) and
propagation. Dashed lines indicate language-only model baseline. For brevity we include both perceptual
input sources ESP and CSLB when comparing means of propagation; results with individual information
sources were similar.

to concrete nouns as previously.

Of our evaluation sets, it was possible to con-
struct from ESP (and add to PESP) representa-
tions for all of the concrete verbs, and for ap-
proximately half of the abstract verbs and abstract
nouns. Figure 4 (top), shows the performance of
a our model trained on all available perceptual in-
put versus the model in which the perceptual input
was restricted to concrete nouns.

The results reflect a clear manifestation of the
abstract/concrete distinction. Concrete verbs be-
have similarly to concrete nouns, in that they can
be effectively represented directly from perceptual
information sources. The information encoded in
these representations is beneficial to the model and
increases performance. In contrast, constructing
‘perceptual’ representations of abstract verbs and
abstract nouns directly from perceptual informa-
tion sources is clearly counter-productive (to the
extent that performance also degrades on the com-
bined sets all nouns and all verbs). It appears in
these cases that the perceptual input acts to ob-
scure or contradict the otherwise useful signal in-
ferred from the corpus.

As shown in the previous section, the inclusion
of any form of perceptual input inhibits the learn-
ing of abstract nouns. However, this is not the case
for abstract verbs. Our model learns higher qual-
ity representations of abstract verbs if perceptual
input is restricted to concrete nouns than if no per-
ceptual input is included at all and when percep-
tual input is included for both concrete nouns and
abstract verbs. This supports the idea of a grad-
ual scale of concreteness: The most concrete con-
cepts can be effectively represented directly in the

perceptual modality; somewhat more abstract con-
cepts cannot be represented directly in the percep-
tual modality, but have representations that are im-
proved by propagating perceptual input from con-
crete concepts via language; and the most abstract
concepts are best acquired via language alone.

3.4 Source and quantity of perceptual input

For different concept types, we tested the effect of
varying the proportion of perceptual to linguistic
input (the parameter α). Perceptual input was re-
stricted to concrete nouns as in Sections 3.1-3.2.

As shown in Figure 4, performance on concrete
nouns improves (albeit to a decreasing degree) as
α increases. When learning concrete noun rep-
resentations, linguistic input is apparently redun-
dant if perceptual input is of sufficient quality and
quantity. For the other concept types, in each case
there is an optimal value for α in the range .5–2,
above which perceptual input obscures the linguis-
tic signal and performance degrades. The prox-
imity of these optima to 1 suggests that for op-
timal learning, when a concrete concept is experi-
enced approximately equal weight should be given
to available perceptual and linguistic information.

4 Conclusions

Motivated by the notable prevalence of abstract
concepts in everyday language, and their likely
importance to flexible, general-purpose represen-
tation learning, we have investigated how abstract
and concrete representations can be acquired by
multi-modal models. In doing so, we presented a
simple and easy-to-implement architecture for ac-
quiring semantic representations of both types of

262



0.1

0.2

0.3

0 1 2 3 4 5
α

C
or

re
la

tio
n

Concrete Nouns

0.1

0.2

0.3

0 1 2 3 4 5
α

Abstract Nouns

0.1

0.2

0.3

0 1 2 3 4 5
α

Concrete Verbs

0.1

0.2

0.3

0 1 2 3 4 5
α

Perceptual Input

CSLB

ESP

CSLB & 
 ESP

Text−only

Abstract Verbs

0.267
0.295

0.136

0.249

0.335
0.364

0.337

0.176

0.087

0.166
0.201

0.225

0.0

0.1

0.2

0.3

0.4

concrete 
 nouns

abstract 
 nouns

all nouns concrete 
 verbs

abstract 
 verbs

all verbs

Concept Type

C
or

re
la

tio
n

Perceptual Information Source Direct representation Propagation

Our Model α = 1

Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual in-
formation where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of
increasing α on correlation with USF pairs (Spearman ρ) for each concept type. Horizontal dashed lines
indicate language-only model baseline.

concept from linguistic and perceptual input.
While neuro-probabilistic language models

have been applied to the problem of multi-modal
representation learning previously (Srivastava and
Salakhutdinov, 2012; Wu et al., 2013; Silberer and
Lapata, 2014) our model and experiments develop
this work in several important ways. First, we ad-
dress the problem of learning abstract concepts.
By isolating concepts of different concreteness
and part-of-speech in our evaluation sets, and sep-
arating the processes of information combination
and propagation, we demonstrate that the multi-
modal approach is indeed effective for some, but
perhaps not all, abstract concepts. In addition, our
model introduces a clear parallel with human lan-
guage learning. Perceptual input is introduced pre-
cisely when concrete concepts are ‘experienced’
by the model in the corpus text, much like a lan-
guage learner experiencing concrete entities via
sensory perception.

Taken together, our findings indicate the utility
of distinguishing three concept types when learn-
ing representations in the multi-modal setting.

Type I Concepts that can be effectively repre-
sented directly in the perceptual modality. For

such concepts, generally concrete nouns or con-
crete verbs, our proposed approach provides a sim-
ple means of combining perceptual and linguistic
input. The resulting multi-modal representations
are of higher quality than those learned via other
approaches, resulting in a performance improve-
ment of over 10% in modelling free association.

Type II Concepts, including abstract verbs, that
cannot be effectively represented directly in the
perceptual modality, but whose representations
can be improved by joint learning from linguis-
tic input and perceptual information about related
concepts. Our model can effectively propagate
perceptual input (exploiting the relations inferred
from the linguistic input) from Type I concepts to
enhance the representations of Type II concepts
above the language-only baseline. Because of the
frequency of abstract concepts, such propagation
extends the benefit of the multi-modal approach to
a far wider range of language than models based
solely in the concrete domain.

Type III Concepts that are more effectively
learned via language-only models than multi-
modal models, such as abstract nouns. Neither

263



our proposed approach nor alternative propagation
methods achieve an improvement in representa-
tion quality for these concepts over the language-
only baseline. Of course, it is an empirical ques-
tion whether a multi-modal approach could ever
enhance the representation learning of these con-
cepts, one with potential implications for cognitive
theories of grounding (a topic of much debate in
psychology (Grafton, 2009; Barsalou, 2010)).

Additionally, we investigated the optimum type
and quantity of perceptual input for learning con-
cepts of different types. We showed that too much
perceptual input can result in degraded represen-
tations. For concepts of type I and II, the op-
timal quantity resulted from setting α = 1; i.e.
whenever a concrete concept was encountered, the
model learned from an equal number of language-
based and perception-based examples. While we
make no formal claims here, such observations
may ultimately provide insight into human lan-
guage learning and semantic memory.

In future we will address the question of
whether Type III concepts can ever be enhanced
via multi-modal learning, and investigate multi-
modal models that optimally learn concepts of
each type. This may involve filtering the percep-
tual input stream for concepts according to con-
creteness, and possibly more elaborate model ar-
chitectures that facilitate distinct representational
frameworks for abstract and concrete concepts.

Acknowledgements

Thanks to the Royal Society and St John’s College
for supporting this research, and to Yoshua Bengio
and Diarmuid Ó Séaghdha for helpful discussions.

References
Mark Andrews, Gabriella Vigliocco, and David Vin-

son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463.

Lawrence W Barsalou and Katja Wiemer-Hastings.
2005. Situating abstract concepts. Grounding Cog-
nition: The Role of Perception and Action in Mem-
ory, Language, and Thought, pages 129–163.

Lawrence W Barsalou. 2010. Grounded cognition:
past, present, and future. Topics in Cognitive Sci-
ence, 2(4):716–724.

Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-

ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145. Asso-
ciation for Computational Linguistics.

Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.

Joan L Bybee and Paul J Hopper. 2001. Frequency and
the Emergence of Linguistic Structure, volume 45.
John Benjamins Publishing.

Nick Chater and Christopher D Manning. 2006. Prob-
abilistic models of language processing and acquisi-
tion. Trends in Cognitive Sciences, 10(7):335–344.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160–167. ACM.

Sebastian J Crutch and Elizabeth K Warrington. 2005.
Abstract and concrete concepts have structurally
different representational frameworks. Brain,
128(3):615–627.

Barry J Devereux, Lorraine K Tyler, Jeroen Geertzen,
and Billi Randall. 2013. The centre for speech, lan-
guage and the brain (cslb) concept property norms.
Behavior Research Methods, pages 1–9.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91–99. Asso-
ciation for Computational Linguistics.

Jerry A Fodor. 1983. The modularity of mind: An
essay on faculty psychology. MIT press.

Scott T Grafton. 2009. Embodied cognition and the
simulation of action to understand others. Annals of
the New York Academy of Sciences, 1156(1):97–117.

David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639–2664.

Felix Hill, Anna Korhonen, and Christian Bentz.
2013. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive Science.

Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Multi-modal models for abstract and concrete con-
cept semantics. Transactions of the Association for
Computational Linguistics.

264



Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.

Brendan T Johns and Michael N Jones. 2012. Per-
ceptual inference through global lexical similarity.
Topics in Cognitive Science, 4(1):103–120.

Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of the annual meeting of the
Association for Computational Linguistics. ACL.

Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the British National
Corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622–
628. Association for Computational Linguistics.

Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547–
559.

Grégoire Mesnil, Yann Dauphin, Xavier Glorot, Salah
Rifai, Yoshua Bengio, Ian J Goodfellow, Erick
Lavoie, Xavier Muller, Guillaume Desjardins, David
Warde-Farley, et al. 2012. Unsupervised and trans-
fer learning challenge: a deep learning approach.
Journal of Machine Learning Research-Proceedings
Track, 27:97–110.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international Workshop on Arti-
ficial Intelligence and Statistics, pages 246–252.

Raymond H Myers. 1990. Classical and Modern
Regression with Applications, volume 2. Duxbury
Press Belmont, CA.

Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, & Com-
puters, 36(3):402–407.

Allan Paivio. 1991. Dual coding theory: Retrospect
and current status. Canadian Journal of Psychology,
45(3):255.

Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the

2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146–1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.

Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433. As-
sociation for Computational Linguistics.

Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of the annual meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.

Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with vi-
sual attributes. In Proceedings of the 51th Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria, August.

Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In NIPS, pages 2231–2239.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.

Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on human factors in computing
systems, pages 319–326. ACM.

Katja Wiemer-Hastings and Xu Xu. 2005. Content
differences for abstract and concrete concepts. Cog-
nitive Science, 29(5):719–736.

Pengcheng Wu, Steven CH Hoi, Hao Xia, Peilin Zhao,
Dayong Wang, and Chunyan Miao. 2013. On-
line multimodal deep similarity learning with ap-
plication to image retrieval. In Proceedings of the
21st ACM International Conference on Multimedia,
pages 153–162. ACM.

265


