















































Tailoring Neural Architectures for Translating from Morphologically Rich Languages


Proceedings of the 27th International Conference on Computational Linguistics, pages 3134â€“3145
Santa Fe, New Mexico, USA, August 20-26, 2018.

3134

Tailoring Neural Architectures for Translating from
Morphologically Rich Languages

Peyman Passban, Andy Way, Qun Liu
ADAPT Centre

School of Computing
Dublin City University, Ireland

firstname.lastname@adaptcentre.ie

Abstract

A morphologically complex word (MCW) is a hierarchical constituent with meaning-preserving
subunits, so word-based models which rely on surface forms might not be powerful enough
to translate such structures. When translating from morphologically rich languages (MRLs), a
source word could be mapped to several words or even a full sentence on the target side, which
means an MCW should not be treated as an atomic unit. In order to provide better translations
for MRLs, we boost the existing neural machine translation (NMT) architecture with a double-
channel encoder and a double-attentive decoder. The main goal targeted in this research is to
provide richer information on the encoder side and redesign the decoder accordingly to benefit
from such information. Our experimental results demonstrate that we could achieve our goal as
the proposed model outperforms existing subword- and character-based architectures and showed
significant improvements on translating from German, Russian, and Turkish into English.

Title and Abstract in Farsi (Persian)

	P @ Ã©Ã”g
.
Q
ï¿½
K Ã¸@QK. ÃºÃ¦

.
Â’Â« Ã¸AÃ« Ã©ÂºJ.

ï¿½
Âƒ QK. Ãº

	
Ã¦
ï¿½
JJ.Ã“ Ã¸AÃ«ÃˆYÃ“ Ã¸PAÃ’ÂªÃ“ XÃ±J.Ã®E.

Ã¨YJ
jï¿½ ï¿½
Kï¿½ ÃºÂ¾K

ï¿½PÃ±Ã‹Ã±

	
PÌ„Ã±Ã“ PA

ï¿½
J

	
kAÂƒAK. Ã¸AÃ«

	
Ã AK.

	P

(
ï¿½

Â
	
mï¿½'. QK


	P) Yg@Ã° AK. ÃºÃ¦
.

ï¿½
K @QÃ“ Ã©ÃŠÂ‚ÃŠÂƒ Ã¸AÃ«PA

ï¿½
J

	
kAÂƒ Y

	
Jï¿½

	
K AÃ’Ã« ÃºÂ¾K


ï¿½PÃ±Ã‹Ã±
	
PÌ„Ã±Ã“ Ã¨YJ
jï¿½ ï¿½
Kï¿½ Ã¸AÃ«Ã±Ã‚Ã‹@ AK.

ï¿½
HAÃ’ÃŠÂ¿

Q
	

Â¢
	
ï¿½ PX QK


	
YKï¿½A

	
K Ã©K


	Qm.
ï¿½
ï¿½
' Yg@Ã° Â¹K
 @P Ã©Ã’ÃŠÂ¿ QÃ« Ã©Â»

ï¿½
HAÃ’ÃŠÂ¿ QK. Ãº

	
Ã¦
ï¿½
JJ.Ã“ Ã¸AÃ«ÃˆYÃ“ @

	
YÃ‹ , Y

	
J
ï¿½
ï¿½Â‚Ã« P@XA

	
JÂªÃ“ Ã¸AÃ«

ÃAÃ†
	
JÃ« Ã©K. . Y

	
J

ï¿½
ÂƒAK. Ã¹Ã–

	
ÃŸ ÃºÃ¦

.

ï¿½
K @QÃ“ Ã©ÃŠÂ‚ÃŠÂƒ Ã¸AÃ«PA

ï¿½
J

	
kAÂƒ

	Ã¡ï¿½

	
Jkï¿½

ï¿½
Â€ 	P@XQKï¿½ Ã¸@QK. ÃºÃ¦

.
ÂƒA

	
JÃ“ Ã¸AÃ« Ã©

	
JK


	QÃƒ Y
	
KQï¿½
ÃƒÃºÃ—

PA
ï¿½
J

	
kAÂƒ Q

	
Â¢

	
ï¿½ 	P@ Q

ï¿½
K Ã¨XAÂƒ

	
Ã AK.

	P Â¹K
 Ã©K. (ÃºÂ»Q
ï¿½
K) ÃºÂ¾K


ï¿½PÃ±Ã‹Ã±
	
PÌ„Ã±Ã“ Ã¨YJ
jï¿½ ï¿½
Kï¿½ Ã¸AÃ«PA

ï¿½
J

	
kAÂƒ AK.

	
Ã AK.

	P Â¹K

	P@ Ã©Ã”g

.
Q
ï¿½
K

YÂ’
ï¿½
Â®Ã“

	
Ã AK.

	P PX Ã‰Ã“AÂ¿ Ã©ÃŠÃ”g
.

Â¹K
 Ãº
ï¿½
Ã¦k AK
 Ã° Ã©Ã’ÃŠÂ¿

	Ã¡K
Y
	
Jkï¿½ Ã©K. Y

	
K @Ã±

ï¿½
KÃºÃ— Ã¨YJ
jï¿½ ï¿½
Kï¿½ Ã©Ã’ÃŠÂ¿ Â¹K
 ,(ÃºÃ¦Â„J
ÃŠÃ†

	
K @)

YK
AJ.
	
K Ã¨YJ
jï¿½ ï¿½
Kï¿½

ï¿½
HAÃ’ÃŠÂ¿ Ã©Â»

ï¿½
IÂƒ@

ï¿½
HPÃ°QÃ¥

	
Â• 	Ã¡K
@ Ã¨Y

	
Jï¿½

	
JÂ» YJ
ï¿½K
A

ï¿½
K Ã¨YK
YKï¿½

	Ã¡K
@ Ã° , XÃ±
ï¿½

Âƒ ( Ã©Ã”g
.
Q
ï¿½
K)

ï¿½
I

ï¿½
ÂƒAÃ†

	
K

AK. Ã¸AÃ« Ã©Ã”
g
.
Q
ï¿½
K YJ
Ã‹Ã±

ï¿½
K PÃ±

	
Â¢

	
JÃ“ Ã©K. ,

ï¿½
ÂÃ«Ã° ï¿½QKï¿½

	Ã¡K
@ PX . Y
	
KÃ±

ï¿½
Âƒ Ã©

ï¿½
J
	
QÌ„Ãƒ Q

	
Â¢

	
ï¿½PX QK


	
YKï¿½A

	
K Ã©K


	Qm.
ï¿½
ï¿½
' Ã¸AÃ«Yg@Ã° Y

	
Jï¿½

	
K AÃ’Ã«

Ãº
	

Ã¦J

ï¿½

ÂƒAÃ“ Ã©Ã”g
.
Q
ï¿½
K Ã¸AÃ«ÃˆYÃ“ PX (Encoder-Decoder) ÃˆÃ°@Yï¿½JÃ“ Ã¸PAÃ’ÂªÃ“ Aï¿½K Ã•Ã§'
XQÂ» Ã¹ÂªÂƒ AÃ“ ,Q

ï¿½
ï¿½Ã®E.

ï¿½
IJ


	
Â®J
Â»

PXÃ±Âº
	
K@

ï¿½
IÃ’Â‚

ï¿½
Â¯ PX ÃˆA

	
KAÂ¿ Â¹K
 Ã¸Am.

ï¿½'
. Ã¸XAÃ®

	
D

ï¿½
Â‚ï¿½
Kï¿½ Ã¸PAÃ’ÂªÃ“ PX . Ã•Ã¦


Ã«X Qï¿½
ï¿½J

	
Âª

ï¿½
K @P ÃºÃ¦

.
Â’Â« Ã¸AÃ« Ã©ÂºJ.

ï¿½
Âƒ QK. Ãº

	
Ã¦
ï¿½
JJ.Ã“

PX ; Y
	
Jï¿½

	
JÂ» Ã‘Ã«@Q

	
Â¯ @P @YJ.Ã“

ï¿½
IÃ–ÃÂ… 	P@ Ã¸Q

ï¿½
KÃº

	
Ã¦

	
Â«

ï¿½
HAÂ«CÂ£@ A

ï¿½
K Y

	
J
ï¿½
ï¿½Â‚Ã« ÃˆAÂª

	
Â¯

	
Ã AÃ“ 	QÃ’Ã«

ï¿½
HPÃ±Â“ Ã©K. ÃˆA

	
K AÂ¿ Ã°X

Q
ï¿½
K Ã¨YJ
jï¿½ ï¿½
Kï¿½ PA

ï¿½
J

	
kAÂƒ AK. Ã¸QÃ‚K
X Yg@Ã° AK. XÃ±k. Ã±Ã“ (Attention mechanism) 	QÂ»QÃ–

ï¿½
ÃŸ Yg@Ã°

	Qï¿½

	
K PXÃ±Â»X

ï¿½
IÃ–ÃÂ…

Ã¹ï¿½K
AÃ–
	

ÃŸ 	PAK.
	
Ã XPÃ°

ï¿½
@

ï¿½
IÂƒYK. Ã° Ã©Ã”

g
.
Q
ï¿½
K

ï¿½
IJ


	
Â®J
Â» XÃ±J.Ã®E. Ã¨

ï¿½PÃ°QKï¿½
	Ã¡K
@ ÃAm.

ï¿½
	
' @ 	P @ ÃºÃÂ“@

	
Â¬YÃ« .

ï¿½
IÂƒ@ Ã¨Y

ï¿½
Âƒ Am.

ï¿½'
. Ag.

l .
ï¿½'
A

ï¿½
J
	
K . Y

	
J
ï¿½
ï¿½Â‚Ã« XPÃ°A

ï¿½
JÂƒX 	Ã¡K
@ YK



Ã±Ã“ Ã¨Y

ï¿½
Âƒ ÃAm.

ï¿½
	
' @

ï¿½
HA

ï¿½
Â‚ï¿½
AÃ“

	P
ï¿½
@ l .

ï¿½'
A
ï¿½
J
	
K Ã©Â» XÃ±K. @YJ.Ã“

ï¿½
IÃ–ÃÂ… 	P@ Ã¸Q

ï¿½
ï¿½Ã®E.

XÃ±k. Ã±Ã“ Ã¸AÃ«ÃˆYÃ“ QÃ‚K
X
	P@ Ã¸Q

ï¿½
ï¿½Ã®E. XQÂºÃŠÃ”

Â« ï¿½IÂƒ@ PXA
ï¿½
Â¯ Ã¸XAÃ®

	
D

ï¿½
Â‚ï¿½
Kï¿½ ÃˆYÃ“ Ã©Â» YÃ«XÃºÃ—

	
Ã A

ï¿½
Â‚

	
ï¿½

ï¿½
HAÂªÃ‹AÂ¢Ã“

ÃºÃ¦Â„J
ÃŠÃ†
	
K @ Ã©K. ÃºÂ»Q

ï¿½
K Ã° , ÃºÃ¦Â…Ã°P , Ãº

	
GAÃ–Ã

ï¿½
@ Ãº

	
Ã¦ÂªK
 AÃ“ Ã©ÂªÃ‹AÂ¢Ã“ XPÃ±Ã“

	
Ã AK.

	P Ã©Âƒ 	P@ Ã©Ã”g
.
Q
ï¿½
K PX Ã° Y

ï¿½
ÂƒAK. Ã©

ï¿½
J

ï¿½
Âƒ@X

. Y
	
JÂ»ÃºÃ— Ã‰Ã”Â« Ã¸Q

ï¿½
ï¿½

ï¿½
Â‚ï¿½
K.

ï¿½
I

ï¿½
Â¯X AK. Ã° Q

ï¿½
K

ï¿½
Â‡

	
Â¯Ã±Ã“



3135

1 Introduction

In this paper we propose a neural architecture which is designed to deal with morphological complexities
on the source side. In such scenarios the neural model and specifically the encoder should be able
to handle morphologically complex inputs. It is hard (or even impossible) to benefit from all subunit
information within an MCW when it is treated as an atomic unit, e.g. the Turkish word â€˜ev.de.ki.ler.de.kiâ€™
meaning â€˜one of those (things/people) at homeâ€™ clearly demonstrates that there are meaningful subunits
within the word which should be processed separately. Expecting a neural model to learn such intra-
word relations by itself complicates the whole process drastically as it requires powerful neural structures
which may not lead to the desired result. Accordingly, we believe that there should be an architectural
design (manipulation) inspired by the nature of this particular problem, which is able to decompose
MCWs or process them in decomposed forms.

Subword-level NMT models are the most preferred alternatives to translate from MRLs, which can
be discussed in two main categories. One group of models does not change the neural architecture
but manipulates data by decomposing words into their subunits. In this approach either linguistically
motivated morphological analyzers such as Morfessor (Smit et al., 2014) or purely statistical models
such as the byte-pair encoding (bpe) model (Sennrich et al., 2016) are applied to process words. This
approach, by its very nature, seems to be a promising solution as it changes the sparse surface form-
based vocabulary set into a much smaller set of fundamental subunits. The size of a set including atomic
subunits, especially for MRLs, is considerably smaller than that of the vocabulary set. Moreover, this
solution partially solves the out-of-vocabulary word problem, as the chance of facing an unknown surface
form is much higher than the chance of facing an unknown subunit.

The aforementioned approach could help generate better translations, but there is a potential problem
with its single encoder shared among all different types of subunits. In the Morfessor-based segmentation
(or any other linguistically motivated segmentation models) the NMT model is fed by stems and affixes
instead of words, but all those subunits are processed with a single encoder. Clearly, this is not the best
way of learning/representing source-side information, as stems and affixes are different constituents and
should be processed separately. The same problem also applies to statistical models such as bpe, because
although the difference between different subunits might not be obvious (as that of stems and affixes),
it does not mean that they are the same types and should be treated equally. Furthermore, there could
be another issue raised by the type of the neural architecture. The encoder-decoder architecture (Cho
et al., 2014; Sutskever et al., 2014) employed in this approach was originally designed to work at the
word level but is instead used for subwords, which might produce some problems, e.g. a sequence of
subwords can be considerably longer than a sequence of words. Long sequences are usually hard to
remember for neural models and this can affect NMT modelâ€™s performance. Considering all positive and
negative aspects of subword-based models, we still believe that such models are truly beneficial but our
main concern is their architecture which we demonstrate can be improved to work with subwords.

As we previously mentioned, there are two main categories of models proposed to tackle the problem
of translating from MRLs. We already explained the first group and its advantages and disadvantages.
Unlike the first group, the second one focuses more on the neural architecture rather than data prepro-
cessing (word segmentation), and boosts the network with character-based encoders. In such models
words are consumed character by character. Usually, there is a convolutional module after the input layer
to read characters, connect them to each other, and extract inter-character relations. This approach is
helpful as it requires no preprocessing step, but there are two main problems with that. Similar to the
previous group, the length of the input sequences could be an issue as by segmenting words into char-
acters an input sequence with a limited number of words is changed to a long sequence of characters.
Furthermore, the convolutional computation over characters could be quite costly, e.g. Lee et al. (2017)
use 200 convolutional filters of width 1 along with 200 filters of width 2, 250 filters of width 3, and con-
tinue up to a filter size of 300 with width 8 to extract useful information on the source side. This amount
of computation is carried out only in one layer (for one word), so in addition there are max-pooling

This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/



3136

layers followed by 4 highway layers (Srivastava et al., 2015), and then recurrent layers on top. This is a
complex architecture which is determined by the type of the input data (a sequence of characters).

Considering both groups of approaches, we assert that there is a need for a third one, for two main
reasons: i) models from the first approach work at the subword level but the neural architecture they use
is not designed for this purpose; and ii) systems from the second approach use complex architectures
to be able to learn character-level information but (it seems) the complex mechanism is not completely
effective as results reported from different studies demonstrate that character-based models usually fail
to compete with other architectures.1 Our experimental results also confirm this (see Table 2). The
main issue we try to point out by studying these approaches is that existing models may provide state-
of-the-art results but they are not designed based on the nature of the problem at hand. Rather, they
are just powerful models with recurrent, convolutional, and highway layers which work with simplified
(segmented) inputs. They might improve translation quality or partially address the problem, but do not
resolve, in a meaningful way, the challenge of handling morphological (subword-level) complexities on
the source side.

In this paper we propose a new architecture to address the shortcomings of these previous approaches
which is our main contribution. We prefer to work with subwords not characters, because we might lose
useful information when we segment structures into characters, or might not be able to extract desired
information via the character-based architecture. Related characters are already grouped and connected
to each other in subwords, so there is no need to decompose and then glue them back together. Therefore,
our basic unit for translation is a morpheme (subword). As mentioned earlier, it seems that existing
architectures can be improved further to work with subwords. Accordingly, we propose a novel neural
architecture which relies on subwords and is supposed to outperform other character- and subword-
based counterparts. In the next section, we first review similar models which tackle the same problem
with different approaches, then we explain our own solution. The comparison between our model and
others confirms that our hypothesis on designing the neural architecture according to the nature of the
problem is true and directly affects performance.

2 Related Work

The problem of morphology and dealing with MCWs is an important issue in the field of sequence
modeling as it directly affects the modelâ€™s architecture and its performance. There is a fair amount
of research carried out to address this problem, which can be discussed in two main categories. One
group of models tries to cope with the problem on the encoder side where the goal is to understand
the rich morphology of the source language. Kim et al. (2016) proposed a convolutional module to
process complex inputs for the problem of language modeling. Costa-jussaÌ€ and Fonollosa (2016) and
Lee et al. (2017) adapted the same convolutional encoder to NMT. Luong and Manning (2016) designed
a hybrid character- and word-based encoder to try to solve the out-of-vocabulary problem. Vylomova
et al. (2016) tackled the problem by comparing the impact of different representation schemes on the
encoder. Similarly, Burlot et al. (2017) investigated the impact of different word representation models
in the context of factored NMT. Our work is also an example of models which try to provide richer
information when the source side is an MRL.

Models reviewed so far address the problem of morphology on the source side. In contrast, there is a
group of models which study the same problem for the target side. Huck et al. (2017) compared different
word-segmentation models, including linguistically motivated as well as statistical techniques, to find the
most appropriate segmentation scheme when translating into MRLs. Chung et al. (2016) tried to design
a suitable architecture when the target language is an MRL. They benefit from using a character-based
decoder which partially resolves the problem. Passban et al. (2018) proposed a similar approach in which
they equipped the character-based decoder with an additional morphology table to inform the decoder
with the target languageâ€™s morphological structures.

1Comparing translation results generated by different word-, subword-, and character-based architectures on different lan-
guage pairs shows that there is no character-based model which is able to outperform its word- and/or subword-based counter-
parts. Please see results at http://matrix.statmt.org/?mode=all



3137

Apart from these models, there are others that do not directly address the problem of morphology
but their solutions could be quite useful to translate MRLs. Jean et al. (2015) proposed a model to
handle very large vocabularies. Luong et al. (2015) addressed the problem of rare and unseen words
with the help of a post-translation phase to exchange unknown tokens with their potential translations.
Dalvi et al. (2017) did not propose a new model but studied the impact of morphological information
in NMT. They evaluated the behaviour of an encoder-decoder model to see what sort of morphological
information is learned via the model and how the model deals with complex structures. Passban (2018)
extensively discussed the problem of morphology at the word and sequence level and proposed solutions
for modeling and translating sequences in monolingual and bilingual settings.

3 Proposed Approach

We propose an NMT architecture with a double-source encoder and double-attentive decoder for trans-
lating from MRLs. Our neural architecture is inspired by models proposed in Firat et al. (2016) and
Zoph and Knight (2016). It takes inputs from two different channels: one channel which is referred to as
the main channel sends stem information (main input), and the other one (the auxiliary channel) sends
affix information. If the input is w0, w1, ..., wn for the (original) encoder-decoder model, our proposed
architecture takes two sequences of Ï‚0, Ï‚1, ..., Ï‚n and Ï„0, Ï„1, ..., Ï„n through the main and auxiliary chan-
nels, respectively, where wi shows the surface form of a word whose stem is Ï‚i, and affix information
associated with wi is given by Ï„i.

Our new neural architecture is based on a hypothesis which assumes the core semantic unit is the stem.
The translation generated on the target side could appear in different surface forms but it should convey
the core meaning dictated by source-side stems. Therefore, to generate a translation the minimum re-
quirement is stem information. Defining the translation process based on stems considerably simplifies
the problem, because we no longer need to work with complex surface forms. The sentence represen-
tation generated based on stems provides the decoder with high-level source-side information. While
it could steer the decoder toward potentially correct target tokens, the decoder needs more than this to
generate precise translations. Accordingly, stem information is accompanied with auxiliary information
supplied by the affix channel.

There are two main motivations underpinning this type of modelling. We can have different words
on the target side (morphologically naive) which could be translations of a single stem. The reason
they differ from one another and appear in different forms is because the source-side stem collocates
with different affixes. By having different input channels we can model these combinations to help
the decoder with the generation of the translation. The double-channel encoder can understand such
combinations better, as it separately processes the stem and its affixes. This is the first motivation for
our design. Moreover, the neural model is not totally able to extract all information preserved in MCWs
when it works with surface forms, but we can simplify the process by explicitly showing subword units
to the network, which is a novel point in our architecture. This is the second reason why we process
stems and affixes separately.

3.1 The Double-Channel Architecture
Our neural model is an extension to the encoder-decoder model of Cho et al. (2014) with some funda-
mental changes. In our architecture, given an input sequence w1, w2, ..., wn, words are segmented into
their stems (Ï‚) and affix tokens (Ï„ ). Regarding the structure of the input data, our encoder should process
two tokens (stem and affix) instead of one (word) at each time step, so the neural architecture is adapted
to the new data structure. To this end, we have two encoders where the stem encoder reads stems one
after another and the other one (affix encoder) reads affix tokens. The process can be formulated as in
(1):

st = f(stâˆ’1, Ï‚
e
t )

at = f(atâˆ’1, Ï„
e
t )

(1)

where st and at are the hidden states of the stem and affix encoders, respectively, at time step t. Ï‚et
indicates the embedding of the t-th wordâ€™s stem, and Ï„ et is the embedding of the affix token for the same



3138

word.
When the final time step is reached for an input sentence of length n, we have two vectors that represent

the summary of stem and affix sequences. The decoder should be informed about source-side information
to start sampling/generation, for which we place a fully connected layer with a transformation matrix
between the encoder and decoder to map both source vectors to the first hidden state of the decoder, as
in (2):

h0 = tanh
(
WÏ‚Ï„ [sn â€¢ an]

)
(2)

where h0 is the decoderâ€™s hidden state at the beginning, tanh applies non-linearity, WÏ‚Ï„ âˆˆ
R(|sn|+|an|)Ã—|h0| is the transformation matrix, and â€¢ indicates the concatenation operation.

At each time step the decoder samples a token from the target vocabulary. In the simple encoder-
decoder model, the decoder conditions the prediction on its hidden state ht, the last predicted token ytâˆ’1,
and a dedicated context vector ct generated by the attention mechanism (Bahdanau et al., 2014) over the
encoderâ€™s hidden states. In our case, we have two sets of source-side hidden states and the decoder pays
attention to both (instead of one). This process is simply formulated as in (3):

yt = g(ht, ytâˆ’1, c
Ï‚
t , c

Ï„
t ) (3)

Our decoder benefits from a double-attentive mechanism instead of the simple attention model. cÏ‚t is
the stem-based context vector and provides information about source stems, and cÏ„t is the affix context
vector which informs the decoder about morphological properties of the input sequence.

In order to construct the stem-based context vector cÏ‚t , we use exactly the same attention model pro-
posed by Bahdanau et al. (2014), as in (4):

cÏ‚t =

nâˆ‘
tâ€²=1

Î±ttâ€²stâ€²

Î±ttâ€² =
exp(eÏ‚ttâ€²)âˆ‘n
k=1 exp(e

Ï‚
tk)

eÏ‚ttâ€² =aÏ‚(stâ€² , htâˆ’1)

(4)

where aÏ‚ is a simple feed-forward connection. The equation shows that the decoder tries to select relevant
stems at each time step which can help it sample better tokens. cÏ‚t summarizes all source stems and
informs the decoder about the impact of each stem. For the affix context vector, the attention model has
a slightly different mechanism whose detail is shown in (5):

cÏ„t =
nâˆ‘

j=tâ€²

Î²ttâ€²atâ€²

Î²ttâ€² =
exp(eÏ„ttâ€²)âˆ‘n
k=1 exp(e

Ï„
tk)

eÏ„ttâ€² =aÏ„ (atâ€² , [htâˆ’1 â€¢ cÏ‚t ])

(5)

Affix tokens are weighted given the hidden state of the decoder together with the stem context vector.
We know that affixes associate with their stems, so to select the best affix we need to consider both the
impact of stems and the decoderâ€™s information. Experimental results show that this combination obtains
better performance (see Section 4). Figures 1 and 2 visualize our double-attentive attention module.
They provide a comparison between simple and double-attentive attention models. It should be noted
that for simplicity our figures only show connections associated with the attention module (and exclude
others).



3139

st+1 st 

attention 

 
 

 
 

 
 

 
 ht-1 ht 

st+1 st 

attentions 

 
 

 
 

 
 

 
 ht-1 ht 

at+1 at 
 

 
 

 

attentiona 

ct 

ğ‘ğ‘¡ğ‘  

ğ‘ğ‘¡ğ‘ 

Figure 1: This figure shows the original attention mechanism where the source side-encoder consumes
one word at each time step and updates its hidden state st. The decoder constructs the context vector
ct based on the decoderâ€™s previous hidden state and all source-side hidden states. The context vector is
used along with the (current) hidden state ht to generate the output.

st+1 st 

attention 

 
 

 
 

 
 

 
 ht-1 ht 

st+1 st 

attentions 

 
 

 
 

 
 

 
 ht-1 ht 

at+1 at 
 

 
 

 

attentiona 

ct 

ğ‘ğ‘¡ğ‘  

ğ‘ğ‘¡ğ‘ 

Figure 2: This figure illustrates the double-source encoder, where the stem encoder takes one stem at
each time step and updates st, and the affix encoder updates at after consuming the t-th affix token.
The stem-based context vector is depicted by cst . The affix-based context vector is constructed using
information provided by all of the affix-encoderâ€™s hidden states, the decoderâ€™s previous hidden state, and
the stem context vector.

4 Experimental Study

Our NMT model is an encoder-decoder model with gated recurrent units (GRUs) (Cho et al., 2014). On
the encoder side we have two encoders, one for stems and the other for affix tokens. Both encoders in-
clude two GRU layers where the first layer is a bidirectional layer and the second layer is a unidirectional
layer. Stems and affix tokens are represented with 512-dimensional embeddings. On the decoder side,
we have the double-attentive attention module and two GRU layers. The GRU size for both the encoder
and decoder is 1024. Similar to input tokens, target tokens are also represented with 512-dimensional
embeddings. The network was trained using stochastic gradient descent with Adam (Kingma and Ba,
2015). The mini-batch size is 80, the beam search width is 20, and the norm of the gradient is clipped
with the threshold 1.0.

4.1 Data Preparation

Our models are trained to translate from German (De), Russian (Ru), and Turkish (Tr) into En-
glish (En). For German and Russian, we used WMT-15 datasets,2 where the Deâ€“En corpus in-
cludes 4.5M parallel sentences and the size of the Ruâ€“En corpus is 2.1M. The newstest-2013 and
newstest-2015 datasets are used as the development and test sets, respectively. For Trâ€“En, we used
the OpenSubtitle2016 collection (Lison and Tiedemann, 2016).3 We randomly selected 3K sen-
tences for each of the development and test sets, and 4M for training.

2http://www.statmt.org/wmt15/translation-task.html
3http://opus.nlpl.eu/OpenSubtitles2016.php



3140

Englishâ€“German Englishâ€“Russian Englishâ€“Turkish

En De En Ru En Tr

Sentence 4.2M 2.1M 4.0M
Token 103, 692, 553 96, 235, 845 43, 944, 989 39, 694, 475 24, 875, 286 17, 915, 076
Type 103, 574 143, 329 70, 376 119, 258 108, 699 178, 672
Stem 21, 223 26, 301 15, 964 19, 557 15, 714 17, 419
Affix 13, 410 24, 054 9, 320 17, 542 7, 112 8, 041
Prefix 3, 104 5, 208 2, 959 3, 324 1, 211 753
Suffix 3, 285 4, 974 2, 219 3, 460 1, 766 2, 701
Char 355 302 360 323 189 231

Table 1: Sentence is the number of parallel sentences in the corpus. Token is the number of words. Type
shows the number of unique words. Stem, Prefix, Suffix, and Affix show the number of unique stems,
prefixes, suffixes, and affix tokens in the corpus. Char is the number of unique characters appeared in
the corpus.

We segment input words with Morfessor. The longest subunit is selected as the stem. What appears
before the stem is the prefix and what follows the stem is considered as the suffix. We do not postprocess
Morfessorâ€™s output, e.g. Turkish is a suffix-based language with no prefixes, but Morfessor extracted
some prefixes for this language. We use segmented units as they are and do not change them. Table 1
summarizes some statistics about our training corpora.

It is clear what the stem encoder takes as its input, but for a word we may have none (null), one or
many prefixes and suffixes, whose combination constructs the affix token in our setting. For the affix
encoder we have a look-up table which includes prefix and suffix embeddings. The encoder retrieves
associated prefix and suffix embeddings at each time step, combines them (via vector summation) and
sends the combined vector to the encoder. This architecture is referred to as pre+suf in our experiments.
If a word has more than one prefix/suffix the encoder retrieves all of them, and if it has none the encoder
uses a null embedding.

In the pre+suf setting, what the affix encoder receives as its input is a vector which has some infor-
mation about morphological properties of the associated stem (word), but it is possible to provide the
encoder with richer information, so we propose the affix extensions (affix-cÏ„ and affix-cÏ‚cÏ„ ).4 In this
new setting we combine the surface form of all prefixes and suffixes to generate a new affix token, e.g if
wordi = prefix1i .stemi.suffix

1
i .suffix

2
i , the affix token Ï„i would be prefix

1
i .suffix

1
i .suffix

2
i . We assign a unique

embedding for this new combination. Clearly, the embedding of the new affix token is different from the
summation of the embeddings of the prefix(es) and suffix(es) and introduces a new token.

A real example can show the difference between the pre+suf and affix extensions. If the given word
is pre.process.ing, in the first extension pre+suf, the stem encoder takes the embedding of process and
the affix encoder takes the summation of the embeddings of pre and ing. In the second extension (either
affix-cÏ„ or affix-cÏ‚cÏ„ ), the stem token is the same but the affix encoder takes an embedding which is
exclusively defined for pre.ing.

The embedding of the affix token is a new unit which preserves information about: i) morphological
properties of its associated word, ii) morphological properties of other words appearing in the sentence,
and iii) the collocation and order of different prefixes and suffixes, at the word and sentence level. It
seems that the simple combination of prefix and suffix embeddings as in the pre+suf extension cannot
preserve this amount of information, because the role of each suffix/prefix is clear and their summation
will provide us with specific information, while in the second extension we can expect the neural network
to learn our desired information. Basically, by defining the affix token we try to introduce an additional
cell of memory, in which the NMT model and the affix encoder store the useful information it has learned

4In these two extensions everything is the same but the attention mechanism. Equations (5) and (6) explain the attention
mechanisms for affix-cÏ‚cÏ„ and affix-cÏ„ , respectively.



3141

(in addition to stem).

4.2 Experimental Results

The results obtained from our experiments are reported in Table 2. The numbers in the table are BLEU
scores (Papineni et al., 2002) of different neural models. We compare our models to all existing models
which translate from MRLs or reported experimental results on our datasets. The first row of the table
shows an encoder-decoder model where the decoder works at the character level and uses the architecture
proposed in Chung et al. (2016). The first row can be considered as a baseline for all other models
reported in the table, as it does not use any complicated neural architecture on the source side. For
each word it simply sums stem, prefix, and suffix embeddings together and sends the summed vector
as the word-level representation to the GRU-based encoder. Although the model is quite simple, it is
able to generate comparable results to other more complex architectures, which reinforces our claim
that existing neural architectures are not suitable to work at the subword level. If we find a better way
to provide the neural model with subword information, we will be able to improve translation quality
still further. For all other models we use the same neural architecture where we keep the character-
based decoder unchanged and only change the encoder to compare the capacity of different encoders in
modeling morphological information.

Model Source Target Deâ†’En Ruâ†’En Trâ†’En

Baseline Ï‚+pre+suf char 22.11 22.79 22.98

Costa-jussaÌ€ and Fonollosa (2016)
word word 18.83 - -
char word 21.40 - -

Firat et al. (2016) bpe bpe 24.00 22.44 -

Lee et al. (2017)
bpe char 25.27 22.83 -
char char 25.83* 22.73 -

Sennrich and Haddow (2016)? Ï‚ â€¢ Ï„ char 22.41 23.01* 23.14*
Our model

pre+suf [Ï‚]1 [pre+suf]2 char 26.11 22.95 23.62
affix-cÏ‚cÏ„ [Ï‚]1 [Ï„ ]2 char 26.74 23.44 23.81
affix-cÏ„ [Ï‚]1 [Ï„ ]2 char 26.29 23.40 23.74

Table 2: Source and Target indicate the data type for the encoder and decoder, respectively. Ï‚ is the stem,
pre is the prefix and suf is the suffix. Ï„ is the affix token. The bold-faced score is the best score for the
direction and the score with * shows the best performance reported by other existing models. According
to paired bootstrap re-sampling (Koehn, 2004) with p = 0.05, the bold-faced number is significantly
better than the score with *. Brackets show different channels and the + mark indicates the summation,
e.g. [Ï‚]1 [pre+suf]2 means the first channel takes a stem at each step and the second channel takes the
summation of the prefix and suffix of the associated stem. â€¢ is the concatenation operation and ? indicates
that results were produced in our experimental setting using our re-implementation of the original model.

The second row shows the model proposed by Costa-jussaÌ€ and Fonollosa (2016), in which a compli-
cated convolutional module is used to model the relation between characters and build the word-level
representation. It shows that the character-level modeling works better than the word-level model but
the simple baseline engine still outperforms both settings. The third row belongs to Firat et al. (2016)
which is a multi-way multilingual NMT model. The fourth row shows a fully character-level NMT model
(Lee et al., 2017) where both the encoder and decoder are designed based on characters. As previously
discussed, this model has quite a complicated architecture.

The fifth row reports results from the model of Sennrich and Haddow (2016) that is a suitable model to
cope with the problem of complex structures on the source side. The idea behind the model is to enrich
input words via additional annotations such as part-of-speech and/or morphological tags. The embedding
of the additional annotation(s) is concatenated to the embedding of the word/stem. The idea of attaching



3142

auxiliary information to words/stems was successful and our experimental results also confirm this. In
their original model the training data is tagged with external tools which show the need for annotated
training sets. We borrowed the same idea and tried to evaluate that architecture in our setting. In the
original model, proper morphological tags are defined for input words/stems whereas in our case we
consider the affix token as the morphological tag of each stem and we do not need a morphological
tagger. The embedding of the affix token is concatenated to the stem embedding and sent to the encoder.
Accordingly, the NMT engine is an extension to the baseline model in which the encoder is fed by
concatenated structures and the decoder still has the same character-based architecture. This simple
concatenation provides better results than the summation applied in the baseline model, which approves
affix and subword level information is useful but we only need to find an optimal way to explore it.

The last block shows 3 variations of our model. In the pre+suf model the encoder has two encoding
channels, one for stems and the other for prefixes and suffixes. At each time step, the stem encoder takes
one stem embedding. On the other channel, the affix encoder takes an embedding which is the summation
of the prefix and suffix embeddings of the word whose stem is processed by the stem encoder at the same
time step. pre+suf employs our novel double-attentive attention mechanism. This new architecture
enables the NMT engine to perform better than the complicated fully character-based model, which is an
indication that the character-level representation does not necessarily provide a better representation. We
think that our model outperforms the character-based model because its architecture is more compatible
with the nature of MRLs. The impact of the compatible neural architecture becomes more important
when we compare the baseline and pre+suf models. The input format is exactly the same for both, but
the baseline model simply sums stem, prefix, and suffix embeddings, whereas pre+suf has two separate
channels to process stem and affix information.

The affix-cÏ‚cÏ„ variation is the best model among our double-channel models. As previously discussed,
assigning a unique embedding to the combination of prefixes and suffixes instead of summing their
embeddings generates better results and provides richer information.

The third and last variation, affix-cÏ„ , shows the impact of our architecture on the decoder side. As we
modeled in Equation (5), attention weights assigned to the affix-encoderâ€™s hidden states are computed
based on the decoderâ€™s hidden state and the stem context vector. As affix tokens provide complementary
information to stem information, the affix context vector should be aware of the content of the stem
context vector, so we proposed the model explained in Equation (5). If we only consider the decoderâ€™s
information to compute affix weights, the equation will be revised as in (6):

cÏ„t =
nâˆ‘

j=tâ€²

Î²ttâ€²atâ€²

Î²ttâ€² =
exp(eÏ„ttâ€²)âˆ‘n
k=1 exp(e

Ï„
tk)

eÏ„ttâ€² =aÏ„ (atâ€² , htâˆ’1)

(6)

In the affix-cÏ‚cÏ„ version, the energy between the decoderâ€™s (t-1)-th state and the affix-encoderâ€™s tâ€²-th
state was computed by eÏ„ttâ€² = aÏ„ (atâ€² , [htâˆ’1 â€¢ c

Ï‚
t ]), whereas this version simplifies the computation by

estimating eÏ„ttâ€² with aÏ„ (atâ€² , htâˆ’1). The model described in Equation (6) is implemented in the affix-c
Ï„

extension. Results obtained from this extension show that, although the architecture is also successful
compared to other exiting models, its performance is worse than the model which (additionally) involves
the stem context vector to compute affix weights.

5 Conclusion and Future Work

In this paper we focused on morphological complexities on the source side in the NMT task and equipped
the encoder to handle complex inputs. The proposed model includes two separate channels to consume
both stem and affix tokens. This novel two-channel solution can improve performance in two ways,
which can be discussed from linguistic and computational perspectives. In terms of linguistic issues,



3143

when the first channel deals with stems instead of words, it is asked to process considerably fewer con-
stituents as the number of stems are much fewer than the number of words. The network is trained to
translate simple stem forms instead of complex words. This simplification is usually helpful when trans-
lating (from/into) MRLs (Sennrich and Haddow, 2016). Furthermore, there is another channel which
preserves morphological information. Every time that the first channel is not able to provide precise
translations or needs auxiliary information, the second channel completes it with morphological infor-
mation.

The proposed architecture can also be studied from mathematical and computational points of view.
Defining a secondary channel means adding extra neural modules and parameters (weights). Increasing
the number of neural parameters increases the learning capacity of the network and gives a higher pos-
sibility to achieve better results. Experimental results confirm that this happens in practice and the extra
channel is helpful.

Existing subword-based models segment MCWs using Morfessor, bpe, and other models, and feed
the translation engine with simplified units. Character-based models also try to facilitate the process by
working with characters. Both approaches are useful and improve performance but the improvement is
somewhat random, as we do not exactly know what/where the origin of this improvement is. By propos-
ing our double-channel architecture we address this problem and feed the network in a more structured
way, in the expectation that we can represent the impact of morphological information. While we are
still far from the goal of modeling morphological information for NMT, this research is an important step
toward our goal.

Along with these useful properties of our approach, it also has some deficiencies which we plan to
address in future work. Our affix information is extracted via Morfessor which could be noisy. Affix
tokens are supposed to provide morphological information but we are not sure that our interpretation
matches what actually happens in reality. According to experimental results, affix tokens are useful
but we are not sure if they preserve real morphological information. In our future work we plan to
linguistically enrich our pipeline with more precise and relevant information. As another plan, we also
tend to focus on the decoder.

Acknowledgment

We thank our anonymous reviewers for their valuable feedback, as well as the Irish centre for high end
computing (www.ichec.ie) for providing computational infrastructures. This work has been supported
by the ADAPT Centre for Digital Content Technology which is funded under the SFI Research Centres
Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning

to align and translate. In Proceedings of the International Conference on Learning Representations, Banff,
Canada.

Franck Burlot, Mercedes GarcÄ±Ìa-MartÄ±Ìnez, LoÄ±Ìˆc Barrault, Fethi Bougares, and FrancÌ§ois Yvon. 2017. Word rep-
resentations in factored neural machine translation. In Proceedings of the Second Conference on Machine
Translation, pages 20â€“31, Copenhagen, Denmark.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning phrase representations using rnn encoderâ€“decoder for statistical machine
translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 1724â€“1734, Doha, Qatar.

Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. 2016. A character-level decoder without explicit seg-
mentation for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1693â€“1703, Berlin, Germany.

Marta R. Costa-jussaÌ€ and JoseÌ A. R. Fonollosa. 2016. Character-based neural machine translation. In Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages
357â€“361, Berlin, Germany, August.



3144

Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, and Stephan Vogel. 2017. Understanding and
improving morphological learning in the neural machine translation decoder. In Proceedings of the Eighth
International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages
142â€“151.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation
with a shared attention mechanism. In Proceedings of the 2016 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pages 866â€“875, San Diego,
California, June.

Matthias Huck, Simon Riess, and Alexander Fraser. 2017. Target-side word segmentation strategies for neural ma-
chine translation. In Proceedings of the Second Conference on Machine Translation, pages 56â€“67, Copenhagen,
Denmark.

SeÌbastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume
1: Long Papers), pages 1â€“10, Beijing, China, July.

Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016. Character-aware neural language mod-
els. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16), pages 2741â€“2749,
Phoenix, Arizona, USA.

Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference
on Learning Representations (ICLR), San Diego, USA.

Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 388â€“395, Barcelona, Spain, July.

Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2017. Fully character-level neural machine translation with-
out explicit segmentation. Transactions of the Association for Computational Linguistics, vol. 5:365â€“378.

Pierre Lison and JoÌˆrg Tiedemann. 2016. Opensubtitles2016: Extracting large parallel corpora from movie and tv
subtitles. In Proceedings of the 10th International Conference on Language Resources and Evaluation, pages
923â€“929, PortorozÌŒ, Slovenia.

Minh-Thang Luong and Christopher D. Manning. 2016. Achieving open vocabulary neural machine translation
with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages 1054â€“1063, Berlin, Germany, August.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word
problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume
1: Long Papers), pages 11â€“19, Beijing, China, July.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation
of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics,
pages 311â€“318, Pennsylvania, PA., USA.

Peyman Passban, Qun Liu, and Andy Way. 2018. Improving character-based decoding using target-side mor-
phological information for neural machine translation. In Proceedings of the Human Language Technology
Conference of the NAACL, New Orleans, Louisiana, USA, Jun.

Peyman Passban. 2018. Machine Translation of Morphologically Rich Languages Using Deep Neural Networks.
Ph.D. thesis, School of Computing, Dublin City University, Ireland.

Rico Sennrich and Barry Haddow. 2016. Linguistic input features improve neural machine translation. In Pro-
ceedings of the First Conference on Machine Translation, pages 83â€“91, Berlin, Germany, August.

Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword
units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 1715â€“1725, Berlin, Germany, August.

Peter Smit, Sami Virpioja, Stig-Arne GroÌˆnroos, and Mikko Kurimo. 2014. Morfessor 2.0: Toolkit for statistical
morphological segmentation. In Proceedings of the Demonstrations at the 14th Conference of the European
Chapter of the Association for Computational Linguistics, pages 21â€“24, Gothenburg, Sweden.



3145

Rupesh Kumar Srivastava, Klaus Greff, and JuÌˆrgen Schmidhuber. 2015. Highway networks. In Proceedings of
the ICML Deep Learning Workshop, Lille, France.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks.
In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Proceedings of the
Confrence on Advances in Neural Information Processing Systems (NIPS), pages 3104â€“3112. Montreal, Canada.

Ekaterina Vylomova, Trevor Cohn, Xuanli He, and Gholamreza Haffari. 2016. Word representation models for
morphologically rich languages in neural machine translation. CoRR, abs/1606.04217.

Barret Zoph and Kevin Knight. 2016. Multi-source neural translation. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 30â€“34, San Diego, California, June.


