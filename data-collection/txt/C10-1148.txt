Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1317–1325,

Beijing, August 2010

1317

Paraphrasing with Search Engine Query Logs

Shiqi Zhao†‡, Haifeng Wang†, and Ting Liu‡

†Baidu Inc.

‡HIT Center for Information Retrieval, Harbin Institute of Technology

{zhaoshiqi, wanghaifeng}@baidu.com, tliu@ir.hit.edu.cn

Abstract

This paper proposes a method that extracts
paraphrases from search engine query
logs. The method ﬁrst extracts paraphrase
query-title pairs based on an assumption
that a search query and its correspond-
ing clicked document titles may mean the
same thing.
It then extracts paraphrase
query-query and title-title pairs from the
query-title paraphrases with a pivot ap-
proach. Paraphrases extracted in each step
are validated with a binary classiﬁer. We
evaluate the method using a query log
from Baidu1, a Chinese search engine.
Experimental results show that the pro-
posed method is effective, which extracts
more than 3.5 million pairs of paraphrases
with a precision of over 70%. The results
also show that the extracted paraphrases
can be used to generate high-quality para-
phrase patterns.
1 Introduction
The use of paraphrases is ubiquitous in hu-
man languages, which also presents a challenge
for natural language processing (NLP). Previous
studies have shown that paraphrasing can play im-
portant roles in plenty of areas, such as machine
translation (MT) (Callison-Burch et al., 2006;
Kauchak and Barzilay, 2006), question answer-
ing (QA) (Duboue and Chu-Carroll, 2006; Riezler
et al., 2007), natural language generation (NLG)
(Iordanskaja et al., 1991), and so on. As a result,
the research on paraphrasing and its applications
have attracted signiﬁcant interest.

1www.baidu.com

This paper proposes a method that uses search
engine query logs for extracting paraphrases,
which is illustrated in Figure 1. Speciﬁcally, three
kinds of paraphrases can be extracted with our
method, which include (1) query-title (Q-T): a
query and a document title that users clicked on;
(2) query-query (Q-Q): two queries, for which
users clicked on the same document title; (3) title-
title (T-T): two titles that users clicked on for the
same query. We train a classiﬁer for each kind to
ﬁlter incorrect pairs and reﬁne the paraphrases.

Extracting paraphrases using query logs has
many advantages. First, query logs keep growing,
which have no scale limitation. Second, query
logs reﬂect web users’ real needs, hence the ex-
tracted paraphrases may be more useful than that
from other kinds of corpora. Third, paraphrases
extracted from query logs can be directed applied
in search engines for query suggestion and doc-
ument reranking.
In addition, we ﬁnd that both
queries and titles contain a good many question
sentences, which can be useful in developing QA
systems.

We conduct experiments using a query log of
a commercial Chinese search engine Baidu, from
which we extracted about 2.7 million pairs of
paraphrase Q-T, 0.4 million pairs of paraphrase Q-
Q, and 0.4 million pairs of paraphrase T-T. The
precision of the paraphrases is above 70%.
In
addition, we generate paraphrase patterns using
the extracted paraphrases. The results show that
73,484 pairs of paraphrase patterns have been gen-
erated, with a precision of over 78%.

In the rest of the paper, we ﬁrst review related
work in Section 2. Section 3 describes our method
in detail. Section 4 presents the evaluation and re-

1318

Figure 1: Illustration of the proposed method.

sults. Section 5 concludes the paper and discusses
future directions.

2 Related Work
In this section, we brieﬂy review previous studies
on paraphrase extraction and query log mining in
information retrieval (IR).

2.1 Paraphrase Extraction
A variety of data resources have been exploited
for paraphrase extraction. For example, some re-
searchers extract paraphrases from multiple trans-
lations of the same foreign novel (Barzilay and
McKeown, 2001; Ibrahim et al., 2003), while
some others make use of comparable news arti-
cles that report on the same event within a small
time interval (Shinyama et al., 2002; Barzilay and
Lee, 2003; Dolan et al., 2004). Besides the mono-
lingual corpora, bilingual parallel corpora have
also been used for extracting paraphrases (Ban-
nard and Callison-Burch, 2005; Callison-Burch,
2008; Zhao et al., 2008). Their basic assumption
is that phrases that align with the same foreign
phrase may have the same meaning.

The above methods have achieved promising
results. However, their performances are usually
constrained due to the scale and domain limita-
tion. As an alternative, researchers have tried
to acquire paraphrases from large-scale web cor-
pora (Lin and Pantel, 2001; Pas¸ca and Dienes,
2005; Bhagat and Ravichandran, 2008) or directly
based on web mining (Ravichandran and Hovy,

2002). These methods are guided by an extended
version of distributional hypothesis, namely, if
two phrases often occur in similar contexts, their
meanings tend to be similar. The disadvantage
of these methods is that the underlying assump-
tion does not always hold. Phrases with opposite
meanings can also occur in similar contexts, such
as “X solves Y” and “X worsens Y” (Lin and Pan-
tel, 2001). In addition, the extracted paraphrases
are generally short fragments with two slots (vari-
ables) at both ends.

2.2 Query Log Mining in IR
Query logs are widely used in the IR commu-
nity, especially for mining similar queries. For ex-
ample, Wen et al. (2002) clustered queries based
on user click information. Their basic idea is
that if some queries result in similar user clicks,
the meanings of these queries should be similar.
Such methods have also been investigated in (Gao
et al., 2007) for cross-lingual query suggestion
and (Zhao et al., 2007) for synonymous questions
identiﬁcation. This paper is partly inspired by
their studies. However, we do not simply use click
information as clues for mining similar queries.
Instead, we mine paraphrases across queries and
clicked document titles.

For instance, Cui et al.

In addition, query logs can be used for query
expansion.
(2002)
extract probabilistic correlations between query
terms and document terms by analyzing query
logs, which are then used to select high-quality

paraphrase Q-T extraction

paraphrase Q-Q extraction

paraphrase T-T extraction

query

title

both query and title

paraphrase relation

1319

H1:

H2:

H3:

If a query q hits a title t, then q and
t are likely to be paraphrases.
If queries q1 and q2 hit the same title t,
q1 and q2 are likely to be paraphrases.
If a query q hits titles t1 and t2, then
t1 and t2 are likely to be paraphrases.

Table 1: Hypotheses for extracting paraphrases.

expansion terms for new queries. Note that the
expansion terms are merely related terms of the
queries, not necessarily paraphrases.

There are other studies that use query logs
for constructing ontologies (Sekine and Suzuki,
2007),
learning named entities (Pas¸ca, 2007),
building user proﬁles (Richardson, 2008), correct-
ing spelling errors (Ahmad and Kondrak, 2005),
and so forth.

3 The Proposed Method
3.1 Basic Idea
Nowadays, more and more users tend to search
long queries with search engines. Many users
even directly search questions to get exact an-
swers. By analyzing our query log that records
rich information including user queries, clicked
urls, titles, etc., we ﬁnd that most titles of clicked
documents are highly related with search queries.
Especially, paraphrases can be easily found from
long queries and the corresponding clicked ti-
tles. This motivates us to extract paraphrases from
query-title pairs. Here we introduce a concept hit
that will be frequently used: given a query q, a
web document d, and d’s title t, if there exist some
users that click on d when searching q, then we
say q hits t.

The hypothesis for extracting paraphrase Q-T
In addition, we ﬁnd
is shown in Table 1 (H1).
that when several queries hit the same title, the
queries are likely to be paraphrases of each other.
The other way round, when a query hits several
titles, paraphrases can also be found among the ti-
tles. We therefore further extract paraphrase Q-Q
and T-T from the paraphrase Q-T. The underly-
ing hypotheses can be found in Table 1 (H2 and

INPUT: Q: query space, T : title space
OUTPUT: Pqt: the set of paraphrase Q-T,
Pqq: the set of paraphrase Q-Q,
Ptt: the set of paraphrase T-T,
P araSet: the set of paraphrases

IF IsP araphrase(q, t)

IF q hits t

1. FOR any q ∈ Q and t ∈ T
2.
3.
4.
5.
END IF
6.
7. END FOR

Add ⟨q, t⟩ to Pqt

END IF

IF ⟨q1, t⟩ ∈ Pqt and ⟨q2, t⟩ ∈ Pqt

IF IsP araphrase(q1, q2)

8. FOR any q1, q2 ∈ Q and t ∈ T
9.
10.
11.
12.
END IF
13.
14. END FOR

Add ⟨q1, q2⟩ to Pqq

END IF

IF ⟨q, t1⟩ ∈ Pqt and ⟨q, t2⟩ ∈ Pqt

15. FOR any t1, t2 ∈ T and q ∈ Q
16.
IF IsP araphrase(t1, t2)
17.
18.
19.
END IF
20.
21. END FOR

Add ⟨t1, t2⟩ to Ptt

END IF

22. RETURN P araSet = Pqt ∪ Pqq ∪ Ptt
Table 2: Algorithm for extracting paraphrases.

H3). Note that, based on H2 and H3, paraphrase
Q-Q and T-T can be directly extracted from raw
Q-T pairs. However, in consideration of preci-
sion, we extract them from paraphrase Q-T. We
call our paraphrase Q-Q and T-T extraction ap-
proach as a pivot approach, since we use titles as
pivots (queries as targets) when extracting para-
phrase Q-Q and use queries as pivots (titles as tar-
gets) when extracting paraphrase T-T.

3.2 Algorithm
Our paraphrase extraction algorithm is shown in
Table 2.
In particular, lines 1∼7 extract para-

1320

phrase Q-T from the query log. Lines 8∼14 and
15∼21 extract paraphrase Q-Q and T-T, respec-
tively. Line 22 combines the paraphrase Q-T, Q-
Q, and T-T together. To ﬁlter noise, the extracted
Q-T, Q-Q, and T-T pairs are all validated using
a function IsP araphrase(s1, s2). In this work,
we recast paraphrase validation as a binary clas-
siﬁcation problem. Any pair of ⟨s1, s2⟩ is classi-
ﬁed as 1 (paraphrase) or 0 (non-paraphrase) with
a support vector machine (SVM) classiﬁer. The
features used for classiﬁcation will be detailed in
Section 3.3.

In practice, we exploit a query log that contains
287 million Q-T pairs, which are then ﬁltered us-
ing the following constraints:
(1) exclude Q-T
pairs that are too short, i.e., either query q or tittle
t contains less than three terms; (2) exclude Q-T
pairs where q subsumes t or vice versa, e.g., “牛
肉 (beef)” and “牛肉 的 做法 (cooking method of
beef)”; (3) exclude Q-T pairs in which the similar-
ity between q and t is below a predeﬁned threshold
T 2; (4) exclude Q-T pairs whose t contains fre-
quent internet terms, such as “主页 (home page)”,
“网站 (web site)”, “在线 (online)”, since such ti-
tles are mostly organization home pages, online
videos, downloadable resources, etc., which are
useless for our purpose of paraphrase extraction.

3.3 Features for Paraphrase Validation
Given a pair of candidate paraphrases ⟨s1, s2⟩, in
which s1 and s2 can be either a query or a title, we
exploit the following features in the classiﬁcation-
based paraphrase validation.

• Frequency Feature FF . FF is deﬁned based
on each ⟨s1, s2⟩’s frequency. We expect that more
frequent ⟨s1, s2⟩ should be more reliable.

c(s1,s2)

FF (s1, s2) = {

1

C

if c(s1, s2) < C
if c(s1, s2) ≥ C

(1)
where c(s1, s2) denotes the number of times that
the ⟨s1, s2⟩ pair occurs in the corpus. C is a nor-
malizing factor (C = 10 in our experiments).

2The similarity is computed based on word overlap rate,
which will be described in detail in section 3.3. We set T =
0.6 in the experiments.

• Length Rate Feature FLR:

FLR(s1, s2) =

min{cw(s1), cw(s2)}
max{cw(s1), cw(s2)}

(2)

where cw(s) denotes the number of words in s.
• Word Overlap Rate Feature FW OR:
cw(s1 ∩ s2)

FW OR(s1, s2) =

(3)

max{cw(s1), cw(s2)}

where “s1 ∩ s2” is the intersection of s1 and s2.
• Character Overlap Rate Feature FCOR. Chi-
nese words are composed of characters. It is quite
often that words with similar characters share
similar meanings, such as “爽快 (comfortable)”
and “痛快 (comfortable)”, “出售 (sell)” and “销
售 (sell)”. Here we use FCOR to measure the sim-
ilarity between s1 and s2 at the character level.
Detailedly, we segment s1 and s2 into sets of
characters and compute the overlap rate based on
Equation (3)3.
• Cosine Similarity Feature FCS. In FCS, both
s1 and s2 are represented as vectors and their co-
sine similarity is computed as:

FCS(s1, s2) =

vecw(s1) · vecw(s2)

∥vecw(s1)∥ × ∥vecw(s2)∥

(4)

where vecw(s) is the vector of words in s, “·” de-
notes the dot product of two vectors, ∥vecw(s)∥
is the norm of a vector. Here, the weight of each
word w in a vector is computed using a heuristic
similar to tf-idf:

W (w) = tf (w) × log(

N

c(w)

+ 0.1)

(5)

where tf (w) is the frequency of w in the given s,
c(w) is the number of times that w occurs in the
corpus, N = maxw c(w).
• Edit Distance Feature FED. Let ED(s1, s2)
be the edit distance at the word level between s1
and s2, we compute FED as follows:

ED(s1, s2)

FED(s1, s2) = 1 −
3In FCOR, cw(s) of Equation (3) denotes the number of

max{cw(s1), cw(s2)}

(6)

characters in s.

1321

• Named Entity (NE) Similarity Feature FN E.
NE information is critical in paraphrase identiﬁca-
tion (Shinyama et al., 2002). We therefore com-
pute the NE similarity between s1 and s2 and take
it as a feature. We employ a Chinese NE recog-
nition tool that can recognize person names, loca-
tions, organizations, and numerals. The NE simi-
larity is computed as:

FN E(s1, s2) =

cne(s1 ∩ s2) + 1

max{cne(s1), cne(s2)} + 1

(7)

where cne(s) denotes the number of NEs in s.
Equation (7) guarantees FN E = 1 if there are no
NEs in either s1 or s2.
• Pivot Fertility Feature FP F : FP F is a fea-
ture specially designed for paraphrase Q-Q and
T-T extraction, which are based on the pivot ap-
proach4. Speciﬁcally, we deﬁne fertility of a pivot
as the number of targets it corresponds to. Our ob-
servation indicates that the larger the fertility of a
pivot is, the more noisy the targets are. Hence we
deﬁne FP F as:

FP F (s1, s2) = max

p

1

f (p)

(8)

where s1 = q1, s2 = q2, p = t when classifying
Q-Q, while s1 = t1, s2 = t2, p = q when classi-
fying T-T. f (p) denotes the fertility of the pivot p.
The value is maximized over p if s1 and s2 can be
extracted with multiple pivots.

3.4 Generating Paraphrase Patterns
A key feature of our method is that the extracted
paraphrases are particularly suitable for generat-
ing paraphrase patterns, especially for the hot do-
mains that are frequently searched. For example,
there are quite a few paraphrases concerning the
therapy of various diseases, from which we can
easily induce patterns expressing the meaning of
“How to treat [X] disease”, such as “[X] 病 如
何 治疗”, “怎么 治疗 [X] 病”, and “[X] 病 的
治疗 方法”. Therefore, in this work, we try to
generate paraphrase patterns using the extracted
paraphrases.

In our preliminary experiments, we only induce
paraphrase patterns from paraphrases that contain

4FP F is not used in paraphrase Q-T validation.

percent (%)

SAME RELA DIFF
55.92

44.08

-

Table 3: Human labeling of candidate Q-T.

no more than 6 words. In addition, only one slot
is allowed in each pair of paraphrase patterns. Let
s1 and s2 be a pair of paraphrases extracted above.
If there exist words w ∈ s1 and v ∈ s2 that satisfy
(1) w = v, (2) w and v are not stop words, then
we can induce a pair of paraphrase patterns by re-
placing w in s1 and v in s2 with a slot “[X]”. It is
obvious that several pairs of paraphrase patterns
may be induced from one pair of paraphrases.

4 Experiments
We experiment with a query log that contains a
total of 284,316,659 queries. Statistics reveal that
170,315,807 queries (59.90%) lead to at least one
user click, each having 1.69 clicks on average. We
extract 287,129,850 raw Q-T pairs using the query
log, from which 4,448,347 pairs of candidate Q-
T are left after ﬁltering as described in Section
3.2. Almost all queries and titles are written in
Chinese, though some of them contain English or
Japanese words. The preprocessing of candidate
Q-T includes Chinese word segmentation (WSeg)
and NE recognition (NER). Our WSeg tool is im-
plemented based on forward maximum matching,
while the NER tool is based on a NE dictionary
mined from the web.

4.1 Evaluation of Candidate Q-T
We ﬁrst evaluate candidate Q-T without valida-
tion. To this end, we randomly sampled 5000
pairs of candidate Q-T and labeled them manu-
ally. Each pair is labeled into one of the 3 classes:
SAME - q and t have the same meaning; RELA - q
and t have related meanings; DIFF - q and t have
clearly different meanings. The labeling results
are listed in Table 3. We can see that no candidate
Q-T is in the DIFF class. This is not surprising,
since users are unlikely to click on web pages un-
related to their queries.

To gain a better insight into the data, we ana-
lyzed the subtle types of candidate Q-T in both
SAME and RELA classes. In detail, we sampled

1322

1000 pairs of candidate Q-T from the 5000 pairs
labeled above, in which 563 are in the SAME
class, while the other 437 are in the RELA class.
Our analysis suggests that candidate Q-T in the
SAME class can be divided into 4 subtle types:

• Trivial change (12.61%): changes of punctu-
ation or stop words, such as “考研 失败 怎
么 办” and “考研 失败 怎么 办 ？”.

• Word or phrase replacement (68.38%): re-
placements of synonymous words or phrases,
such as “咖 啡 斑 的 治 疗 多 少 钱 (how
mach is ...)” and “咖啡 斑 的 治疗 费用
是 多少 (what is the price of ...)”.

• Structure change (7.10%): changes of both
words and word orders, such as “减肥 中 水
果 可以 吃 什么 (what fruit can I eat on a
diet)” and “吃 什么 水果 可以 瘦身 (what
fruit can help loss weight)”.

• Others (11.90%): candidate Q-T that cannot

be classiﬁed into the 3 types above.

The above analysis reveals that more than two
thirds of candidate Q-T in the SAME class are in
the “word or phrase replacement” type, while the
ones with structure changes are slightly more than
7%. We believe this is mainly because queries
and titles are relatively short and their structures
are simple. Thus structure rewriting can hardly be
conducted. This distribution is in line with that
reported in (Zhao et al., 2008).

As for the RELA class, we ﬁnd that 42.33% of
such candidate Q-T share a problem of named en-
tity mismatch, such as “美 国 (US) 大 型 水 利
工 程” and “中 国 (China) 急 需 大 型 水 利 工
程”. This indicates that the NE similarity feature
is necessary in paraphrase validation.

4.2 Evaluation of Paraphrase Q-T
The candidate Q-T extracted above are classiﬁed
with a SVM classiﬁer5 under its default setting.
To evaluate the classiﬁer, we run 5-fold cross val-
idation with the 5000 human annotated data, in
which we use 4000 for training and the rest 1000
for testing in each run. The evaluation criteria are
5We use libsvm-2.82 toolkit, which can be downloaded

from http://www.csie.ntu.edu.tw/ cjlin/libsvm/

precision (P), recall (R), and f-measure (F), which
are deﬁned as follows:

P = ∥Sa ∩ Sm∥

∥Sa∥

R = ∥Sa ∩ Sm∥

∥Sm∥

2 × P × R
P + R

F =

(9)

(10)

(11)

where Sa is the set of paraphrases automatically
recognized with the classiﬁer, Sm is the set of
paraphrases manually annotated. Precision, re-
call, and f-measure are averaged over 5 runs in
the 5-fold cross validation.

Figure 2 (a) shows the classiﬁcation results
(dark bars). For comparison, we also show the
precision, recall6, and f-measure of the candidate
Q-T (light bars). As can be seen, the precision is
improved from 0.5592 to 0.7444 after classiﬁca-
tion. F-measure is also evidently enhanced. This
result indicates that the classiﬁcation-based para-
phrase validation is effective. We then use all of
the 5000 annotated data to train a classiﬁer and
classify all the candidate Q-T. Results show that
2,762,291 out of 4,448,347 pairs of candidate Q-
T are classiﬁed as paraphrases.

4.3 Evaluation of Paraphrase Q-Q and T-T
From the paraphrase Q-T, we further extracted
934,758 pairs of candidate Q-Q and 438,954 pairs
of candidate T-T (without validation). We ran-
domly sampled 5000 from each for human an-
notation. The results show that the precisions of
candidate Q-Q and T-T are 0.4672 and 0.6860, re-
spectively. As can be seen, the precision of can-
didate Q-Q is much lower than that of candidate
T-T. Our analysis reveals that it is mainly because
candidate Q-Q are more noisy, since user queries
contain quite a lot of spelling mistakes and infor-
mal expressions.

The candidate Q-Q and T-T are also reﬁned
based on classiﬁcation. We ﬁrst evaluate the clas-
siﬁcation performance using the 5000 human la-
beled data. The experimental setups for Q-Q and

6We assume all possible paraphrases are included in the

candidates, thus its recall is 100%.

1323

Figure 2: Classiﬁcation precision (P), recall (R), and f-measure (F).

T-T classiﬁcation are the same as that of Q-T clas-
siﬁcation, in which we run 5-fold cross validation
with a SVM classiﬁer using its default parameters.
Figure 2 (b) and (c) give the classiﬁcation results
(dark bars) as well as the precision, recall, and f-
measure of the candidates (light bars).

We can see that the precision of Q-Q is signiﬁ-
cantly enhanced from 0.4672 to 0.7345 after clas-
siﬁcation, which suggests that a substantial part
of errors and noise are removed. The increase of
f-measure demonstrates the effectiveness of clas-
siﬁcation despite the decrease of recall. Mean-
while, the quality of candidate T-T is not clearly
improved after classiﬁcation. The reason should
be that the precision of candidate T-T is already
pretty high. We then use all 5000 human labeled
data to train a classiﬁer for Q-Q and T-T respec-
tively and classify all candidate Q-Q and T-T. Re-
sults show that 390,920 pairs of paraphrase Q-Q
and 415,539 pairs of paraphrase T-T are extracted
after classiﬁcation.

[X] 文件 怎么 打开
p1
p2 如何 打开 [X] 文件
(how to open [X] ﬁle)
7z; ashx; aspx; bib; cda; cdfs; cmp;
cpi; csf; csv; cur; dat; dek...

slot

p1 关于 [X] 的 诗词
p2 有关 [X] 的 诗歌
(poems about [X])

slot 草原 (prairies); 长江 (Yangtze River);
泰山 (Mount Tai); 乡愁 (nostalgia)...

Table 4: Examples of paraphrase patterns.

ize that the method currently used for inducing
paraphrase patterns is simple. Hence we will im-
prove the method in our following experiments.
Speciﬁcally, multiple slots will be allowed in a
pair of patterns.
In addition, we will try to ap-
ply the alignment techniques in the generation of
paraphrase patterns, as Zhao et al. (2008) did.

4.4 Evaluation of Paraphrase Patterns
Using the method introduced in Section 3.4, we
have generated 73,484 pairs of paraphrase pat-
terns that appear at least two times in the cor-
pus. We randomly selected 500 pairs and labeled
them manually. The results show that the preci-
sion is 78.4%. Two examples are shown in Ta-
ble 4, in which p1 and p2 are paraphrase patterns.
Some slot ﬁllers are also listed below. We real-

4.5 Analysis
Feature Contribution. To investigate the contri-
butions of different features used in classiﬁcation,
we tried different feature combinations for each of
our three classiﬁers. The results are shown in Ta-
ble 5, in which “+” means the feature has contri-
bution to the corresponding classiﬁer. As can be
seen, the character overlap rate feature (FCOR),
cosine similarity feature (FCS), and NE similarity

(a) Q-T classification

(b) Q-Q classification

(c) T-T classification

1.2

1

0.8

0.6

0.4

0.2

0

P

cand.

0.5592

R

1

1.2

1

0.8

0.6

0.4

0.2

0

P

F

0.7173

cand.

0.4672

1.2

1

0.8

0.6

0.4

0.2

0

P

F

0.6369

cand.

0.686

R

1

R

1

F

0.8138

para.

0.7444

0.8391

0.7887

para.

0.7345

0.6575

0.6938

para.

0.7056

0.9776

0.8196

1324

+

+

Feature Q-T Q-Q T-T
FF
FLR
FW OR
FCOR
FCS
FED
FN E
FP F

+
+
+
+
+

+
+

+
+

+

+

Table 5: Feature contribution.

feature (FN E) are the most useful, which play im-
portant roles in all the three classiﬁers. The other
features are useful in some of the classiﬁers ex-
cept the word overlap rate feature (FW OR). The
classiﬁcation results reported in prior sections are
all achieved with the optimal feature combination.
Analysis of the Paraphrases. We combine the
extracted paraphrase Q-T, Q-Q and T-T and get
a total of 3,560,257 pairs of unique paraphrases.
Statistics show that only 8380 pairs (0.24%) are
from more than one source, which indicates that
the intersection among the three sets is very small.
Further statistics show that the average length of
the queries and titles in the paraphrases is 6.69
(words).

To have a detailed analysis of the extracted
paraphrases, we randomly selected 1000 pairs and
manually labeled the precision, types, and do-
mains. It is found that more than 43% of the para-
phrases are paraphrase questions, in which how
(36%), what (19%), and yes/no (14%) questions
are the most common. In addition, we ﬁnd that
the precision of paraphrase questions (84.26%)
is evidently higher than non-question paraphrases
(65.14%). Those paraphrase questions are useful
in question analysis and expansion in QA, which
can hardly be extracted from other kinds of cor-
pora.

As expected, the paraphrases we extract cover
a variety of domains. However, around 50% of
them are in the 7 most popular domains7, includ-
ing:
(1) health and medicine, (2) documentary
download, (3) entertainment, (4) software, (5) ed-

7Note that pornographic queries have been ﬁltered from

the query log beforehand.

ucation and study, (6) computer game, (7) econ-
omy and ﬁnance. This analysis reﬂects what web
users are most concerned about. These domains,
especially (4) and (6), are not well covered by the
parallel and comparable corpora previously used
for paraphrase extraction.

5 Conclusions and Future Directions
In this paper, we put forward a novel method that
extracts paraphrases from search engine query
logs. Our contribution is that we, for the ﬁrst
time, propose to extract paraphrases from user
queries and the corresponding clicked document
titles. Speciﬁcally, three kinds of paraphrases
are extracted, which can be (1) a query and a
hit title, (2) two queries that hit the same title,
and (3) two titles hit by the same query. The
extracted paraphrases are reﬁned based on clas-
siﬁcation. Using the proposed method, we ex-
tracted over 3.5 million pairs of paraphrases from
a query log of Baidu. Human evaluation results
show that the precision of the paraphrases is above
70%. The results also show that we can gener-
ate high-quality paraphrase patterns from the ex-
tracted paraphrases.

Our future research will be conducted along the
following directions. Firstly, we will use a much
larger query log for paraphrase extraction, so as to
enhance the coverage of paraphrases. Secondly,
we plan to have a deeper study of the transitivity
of paraphrasing. Simply speaking, we want to ﬁnd
out whether we can extract ⟨s1, s3⟩ as paraphrases
given that ⟨s1, s2⟩ and ⟨s2, s3⟩ are paraphrases.
6 Acknowledgments
We would like to thank Wanxiang Che, Hua Wu,
and the anonymous reviewers for their useful
comments on this paper.

References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a Spelling Error Model from Search Query
Logs. In Proceedings of HLT/EMNLP, pages 955-
962.

Colin Bannard and Chris Callison-Burch. 2005. Para-
In Pro-

phrasing with Bilingual Parallel Corpora.
ceedings of ACL, pages 597-604.

1325

De-Kang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question Answering. In Natu-
ral Language Engineering 7(4): 343-360.

Marius Pas¸ca and P´eter Dienes. 2005. Aligning Nee-
dles in a Haystack: Paraphrase Acquisition Across
the Web.
In Proceedings of IJCNLP, pages 119-
130.

Marius Pas¸ca. 2007. Weakly-supervised Discovery
In

of Named Entities using Web Search Queries.
Proceedings of CIKM, pages 683-690.

Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. In Proceedings of ACL, pages 41-47.

Matthew Richardson. 2008. Learning about the World
through Long-Term Query Logs. In ACM Transac-
tions on the Web 2(4): 1-27.

Stefan Riezler, Alexander Vasserman,

Ioannis
Tsochantaridis, Vibhu Mittal and Yi Liu.
2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of ACL,
pages 464-471.

Satoshi Sekine and Hisami Suzuki. 2007. Acquiring
Ontological Knowledge from Query Logs. In Pro-
ceedings of WWW, pages 1223-1224.

Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
Automatic Paraphrase Acquisition from
In Proceedings of HLT, pages 40-

2002.
News Articles.
46.

Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query Clustering Using User Logs. In ACM
Transactions on Information Systems 20(1): 59-81,
2002.

Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-08:HLT, pages 780-788.

Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning
Question Paraphrases for QA from Encarta Logs. In
Proceedings of IJCAI, pages 1795-1800.

Regina Barzilay and Lillian Lee.

2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment.
In Proceedings of
HLT-NAACL, pages 16-23.

Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting Paraphrases from a Parallel Corpus. In
Proceedings of ACL/EACL, pages 50-57.

Rahul Bhagat and Deepak Ravichandran. 2008. Large
Scale Acquisition of Paraphrases for Learning Sur-
face Patterns.
In Proceedings of ACL-08: HLT,
pages 674-682.

Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL, pages 17-24.

Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP, pages 196-205.

Hang Cui, Ji-Rong Wen, Jian-Yun Nie, Wei-Ying Ma.
2002. Probabilistic Query Expansion Using Query
Logs In Proceedings of WWW, pages 325-332.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised Construction of Large Paraphrase
Exploiting Massively Parallel News
Corpora:
Sources.
In Proceedings of COLING, pages 350-
356.

Pablo Ariel Duboue and Jennifer Chu-Carroll. 2006.
Answering the Question You Wish They Had
Asked: The Impact of Paraphrasing for Question
Answering. In Proceedings of HLT-NAACL, pages
33-36.

Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian
Hu, Kam-Fai Wong, and Hsiao-Wuen Hon. 2007.
Cross-Lingual Query Suggestion Using Query Logs
of Different Languages. In Proceedings of SIGIR,
pages 463-470.

Ali Ibrahim, Boris Katz, Jimmy Lin. 2003. Extract-
ing Structural Paraphrases from Aligned Monolin-
gual Corpora. In Proceedings of IWP, pages 57-64.

Lidija Iordanskaja, Richard Kittredge, and Alain
Polgu`ere. 1991. Lexical Selection and Paraphrase
in a Meaning-Text Generation Model. In C´ecile L.
Paris, William R. Swartout, and William C. Mann
(Eds.): Natural Language Generation in Artiﬁcial
Intelligence and Computational Linguistics, pages
293-312.

David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of HLT-NAACL, pages 455-462.

