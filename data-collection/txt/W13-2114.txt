










































Generating Natural Language Questions to Support Learning On-Line


Proceedings of the 14th European Workshop on Natural Language Generation, pages 105–114,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

Generating Natural Language Questions to Support Learning On-Line

David Lindberg Fred Popowich
School of Computing Science

Simon Fraser University
Burnaby, BC, CANADA

dll4,popowich@sfu.ca

John Nesbit Phil Winne
Faculty of Education

Simon Fraser University
Burnaby, BC, CANADA

nesbit,winne@sfu.ca

Abstract
When instructors prepare learning materi-
als for students, they frequently develop
accompanying questions to guide learn-
ing. Natural language processing technol-
ogy can be used to automatically generate
such questions but techniques used have
not fully leveraged semantic information
contained in the learning materials or the
full context in which the question genera-
tion task occurs. We introduce a sophisti-
cated template-based approach that incor-
porates semantic role labels into a system
that automatically generates natural lan-
guage questions to support online learn-
ing. While we have not yet incorporated
the full learning context into our approach,
our preliminary evaluation and evaluation
methodology indicate our approach is a
promising one for supporting learning.

1 Introduction

Ample research (e.g., Callender and McDaniel,
2007) shows that learners learn more, and more
deeply, if they are prompted to examine their
learning materials while and after they study. Of-
ten, these prompts consist of questions related to
the learning materials. After reading a given pas-
sage or section of text, learners are familiar with
learning exercises which consist of questions they
need to answer.

Questioning is one of the most common and in-
tensively studied instructional strategies used by
teachers (Rus and Graesser, 1989). Questions em-
bedded in text, or presented while learners are
studying text, are hypothesized to promote self-
explanation which is known to increase compre-
hension and enhance transfer of learning (e.g.,
Rittle-Johnson, 2006).

Traditionally, these questions have been con-
structed by educators. Recent research, though,

has investigated how natural language processing
techniques can be used to automatically generate
these questions (Kalady et al., 2010; Varga and
Ha, 2010; Ali et al., 2010; Mannem et al., 2010).
While the automated approaches have generally
focussed on syntactic features, we propose an ap-
proach that also takes semantic features into ac-
count, in conjunction with domain dependent and
domain independent templates motivated by ed-
ucational research. After introducing our ques-
tion generation system, we will provide a prelimi-
nary analysis of the performance of the system on
educational material, and then outline our future
plans to tailor the questions to the needs of spe-
cific learners and specific learning outcomes.

2 Question Generation from Text

The task of question generation (QG) from text
can be broadly divided into three (not entirely dis-
joint) categories: syntax-based, semantics-based,
and template-based. Systems in the syntactic cat-
egory often use elements of semantics and vice-
versa. A system we would call template-based
must to some extent use syntactic and/or seman-
tic information. Regardless of the approach taken,
systems must perform at least four tasks:

1. content selection: picking spans of source
text (typically single sentences) from which
questions can be generated

2. target identification: determining which spe-
cific words and/or phrases should be asked
about

3. question formulation: determining the appro-
priate question(s) given the content identified

4. surface form generation: producing the final
surface-form realization

Task 2 need not always precede task 3; target
identification can drive question formulation and

105



vice-versa. A system constrained to generating
specific kinds of questions will select only the tar-
gets appropriate for those kinds of questions. Con-
versely, a system with broader generation capa-
bilities might pick targets more freely and (ide-
ally) generate only the questions that are appro-
priate for those targets. We consider the methods
used in performing tasks 2 and 4 to be the pri-
mary discriminators in determining the category
into which a given method is best placed. This is
not the only way one might classify a QG system.
However, we believe this method allows us to best
compare and contrast our approach with previous
approaches.

Syntax-based methods comprise a large portion
of the existing literature. Kalady et al. (2012),
Varga and Ha (2010), Wolfe (1976), and Ali et
al. (2010) provide a sample of these methods. Al-
though each of these efforts has differed on a few
details, they have followed the same basic strat-
egy: parse sentences using a syntactic parser, sim-
plify complex sentences, identify key phrases, and
apply syntactic transformation rules and question
word replacement.

The methods we have labeled “semantics-
based” use method(s) of target identification (task
2) that are primarily semantic, using techniques
such as semantic role labeling (SRL). Given a sen-
tence, a semantic role labeler identifies the pred-
icates (relations and actions) along with the se-
mantic entities associated with each predicate. Se-
mantic roles, as defined in PropBank (Palmer et
al., 2005), include Arg0, Arg1, ..., Arg5, and
ArgA. A set of modifiers is also defined and in-
cludes ArgM-LOC (location), ArgM-EXT (ex-
tent), ArgM-DIS (discourse), ArgM-ADV (adver-
bial), ArgM-NEG (negation), ArgM-MOD (modal
verb), ArgM-CAU (cause), ArgM-TMP (time),
ArgM-PNC (purpose), ArgM-MNR (manner), and
ArgM-DIR (direction). We adopt the shorter
CoNLL SRL shared task naming conventions
(Carreras and Màrquez, 2005) (e.g., A0 and AM-
LOC).

Mannem et al. (2010), for example, introduce a
semantics-based system that combines SRL with
syntactic transformations. In the content selec-
tion stage, a single sentence is first parsed with
a semantic role labeler to identify potential tar-
gets. Targets are selected using simple selec-
tion criteria. Any of the predicate-specific se-
mantic arguments (A0-A5), if present, are consid-

ered valid targets. Mannem et al. further iden-
tify modifiers AM-MNR, AM-PUNC, AM-CAU,
AM-TMP, AM-LOC, and AM-DIS as potential
targets. These roles are used to generate addi-
tional questions that cannot be attained using only
the A0-A5 roles. For example, AM-LOC can be
used to generate a where question, and an AM-
TMP can be used to generate a when question. Af-
ter targets have been identified, these, along with
the complete SRL parse of the sentence are passed
to the question formulation stage. Two heuristics
are used to rank the generated questions. Ques-
tions are ranked first by the depth of their predi-
cate in the dependency parse of the original ques-
tion. This is based on the assumption that ques-
tions arising from main clauses are more desir-
able than those generated from deeper predicates.
In the second stage, questions with the same rank
are re-ranked according to the number of pronouns
they contain, with questions with fewer pronouns
having higher rank.

One limitation of the syntax and semantics-
based methods is that they generate questions by
rearranging the surface form of sentences. Ques-
tion templates offer the ability to ask questions that
are not so tightly-coupled to the exact wording of
the source text. A question template is any pre-
defined text with placeholder variables to be re-
placed with content from the source text. Ques-
tion templates allow question generation systems
to leverage human expertise in language genera-
tion.

The template-based system of Cai et al. (2006)
uses Natural Language Generation Markup Lan-
guage (NLGML), a language that can be used to
generate not only questions but any natural lan-
guage expression. NLGML uses syntactic pattern
matching and semantic features for content selec-
tion and question templates to guide question for-
mulation and surface-form realization. Note that
a pattern need not specify a complete syntax tree.
Additionally, patterns can impose semantic con-
straints. However, simple “copy and paste” tem-
plates are not a panacea for surface-form real-
ization. Mechanisms for changing capitalization
of words and changing verb conjugation (when
source sentence verbs are to appear in the output
text) need to be provided: NLGML provides some
such functions.

106



3 Our Approach

We develop a template-based framework for QG.
The primary motivation for this decision is the
ability of a template-based approach to generate
questions that are not merely declarative to in-
terrogative transformations. We aim to address
some of the limitations of the existing approaches
outlined in the previous section while leveraging
some of their strengths in novel ways. We com-
bine the benefits of a semantics-based approach,
the most important of which is not being tightly-
constrained by syntax, with the surface-form flex-
ibility of a template-based approach.

The data used to develop our approach was ob-
tained from a collection of 25 documents prepared
for educational research purposes within the Fac-
ulty of Education at SFU. All hand-coded rules
we describe below were motivated by patterns ob-
served in this development data. This collection
was modeled after a high-school science curricu-
lum on global warming, with vocabulary and dis-
course appropriate for learners in that age group.
Although the collection included a glossary of key
terms and their definitions, this resource was used
only for evaluation purposes as described in Sec-
tion 4.

3.1 Semantic-based templates

Previous template-based methods have used syn-
tactic pattern matching, which does provide a
great deal of flexibility in specifying sentences
appropriate for generating certain types of ques-
tions. However, this flexibility comes at the ex-
pense of generality. As seen in Wyse and Piwek
(2009), who use Stanford Tregex (Levy and An-
drew, 2006) for pattern matching, the specificity of
syntactic patterns can make it difficult to specify
a syntactic pattern of the desired scope. Further-
more, semantically similar entities can span dif-
ferent syntactic structures, and matching these re-
quires either multiple patterns (in the case of Cai
et al., 2006) or a more complicated pattern (in the
case of Wyse and Piwek, 2009).

If we want to develop templates that are se-
mantically motivated, more flexible in terms of
the content they successfully match, and more ap-
proachable for non-technical users, we need to
move away from syntactic pattern matching. In-
stead, we match semantic patterns. We define a
semantic pattern as the SRL parse of a sentence
and the named entities (if any) contained within

the span of each semantic role. We use Stanford
NER (Finkel et al., 2005) for named entity recog-
nition. Figure 1 shows a sentence and its corre-
sponding semantic pattern. Notice this sentence
has two predicates, each with its own semantic ar-
guments. Each of these predicate-argument struc-
tures is a distinct predicate frame.

Figure 1: A sentence and its semantic pattern

Even the shallow semantics of SRL can identify
the semantically interesting portions of a sentence,
and these semantically-meaningful substrings can
span a range of syntactic patterns. Figure 2
shows a clear example of this phenomenon. In
this example, we see two sentences expressing
the same semantic relationship between two con-
cepts, namely, the fact that trapped heat causes
the Earth’s temperature to increase. In one case,
this causation is expressed in an adjective phrase,
while the other uses a sentence-initial preposi-
tional phrase. The parse trees are generated using
the Stanford Parser (Klein and Manning, 2003).
The AM-CAU semantic role captures the cause in
both sentences. It is impossible to accomplish the
same feat with a single NLGML pattern. However,
it is possible to capture both with a single Tregex
pattern.

The principle advantage of semantic pattern
matching is that a single semantic pattern casts a
narrow semantic net while casting a large syntactic
net. This means fewer patterns need to be defined
by the template author, and the patterns are more
compact.

Our templates have three components: plain-
text, slots, and slot options. Plaintext forms the

107



Figure 2: Two different syntax subtrees subsumed
by a single semantic role

skeleton into which semantically-meaningful sub-
strings of a source sentence are inserted to create a
question. The only restrictions on the plaintext is
that it cannot contain any text that looks like a slot
but is not intended as one, and it cannot contain
the character sequence used to delineate the plain-
text from the slots appearing outside the plaintext.
Aside from these restrictions, any desired text is
valid.

Slots facilitate sentence and template matching.
They accept specific semantic arguments, and can
appear inside or outside the plaintext. These pro-
vide the semantic pattern against which a source
sentence is matched. A slot inside the plaintext
acts as a variable to be replaced by the correspond-
ing semantic role text from a matching sentence,
while any slots appearing outside the plaintext
serve only to provide additional pattern match-
ing criteria. The template author does not need
to specify the complete semantic pattern in each
template. Instead, only the portions relevant to the
desired question need to be specified. This is an
important point of contrast between our template-
based approach vs. syntax and semantics-based
approaches. We can choose to generate questions
that do not include any predicates from the source

sentence but instead ask more abstract or general
questions about other semantic constituents. We
believe these kinds of questions are better able to
escape the realm of the factoid, because they are
not constrained to the actions and relations de-
scribed by predicates.

Slot options function much like NLGML func-
tions and are of two types: modifiers and filters.
Modifiers apply transformations to the role text in-
serted into a slot, and filters enforce finer-grained
matching criteria. Predicate slots have their own
distinct set of options, while the other semantic
roles share a common set of options. A template’s
slots and filters describe the necessary conditions
for the template to be matched with a source sen-
tence semantic pattern.

3.2 Predicate slot options

The predicate filter options restrict the predicates
that can match a predicate slot. With no filter
options specified, any predicate is considered a
match. Table 1 shows the complete list of filters.

Filter Description
be predicate lemma must not be “be”
!be predicate lemma must be “be”
!have predicate lemma must not be “have”

Table 1: Predicate filters

The selection of predicate filters might at first
seem oddly limited. Failing to consider the func-
tional differences between various types of verbs
(particularly auxiliary and copula) would indeed
produce low-quality questions and should in fact
be ignored in most cases. For example, consider
the sentence “Dinosaurs, along with many other
animals, became extinct approximately 65 mil-
lion years ago.” A question such as “What did
dinosaurs, along with many other animals, be-
come?” is not particularly useful. We can rec-
ognize copula predicates by their surrounding se-
mantic pattern, so in the broad sense, we do not
need to adopt any copula-specific rules.

The one exception to the above rule is any cop-
ula whose lemma is be. The be and !be filters
allow the presence or absence of such a predicate
to be detected. This capability is useful for two
reasons. First, the presence of such a predicate
gives us an inexpensive way to generate defini-
tion questions, even if the source text is not writ-
ten in the form of a definition. Although this will
over-generate definition questions, non-predicate

108



filters can be used to add additional mitigating
constraints. Second, requiring the absence of such
a predicate allows us to actively avoid generat-
ing certain kinds of ungrammatical or meaning-
less questions. Whether using one of these predi-
cates results in ungrammatical questions depends
on the wording of the underlying template, so we
provide the !be filter for the template author to
use as needed. Consider the sentence “El Nino
is caused when the westerly winds are unusually
weak.” Without the !be filter, one of our tem-
plates would generate the question “When can El
Nino be?” Applying the !be filter prevents this
question from being generated.

Like copula, auxiliary verbs are often not suit-
able for question generation. Fortunately, many
auxiliary verbs are also modal and are assigned
the label AM-MOD and so do not form predi-
cate frames of their own. Instead, they are in-
cluded in the frame of the predicate they modify.
In other cases auxiliary verbs are not modal, such
as in the sentence “So far, scientists have not been
able to predict the long term effects of this wob-
ble.” In this case, the auxiliary have is treated as
a separate predicate, but importantly, the span of
its A1 includes the predicate been. We provide
a non-predicate filter to prevent generation when
this overlap is present.

The !have filter is motivated by the observa-
tion that the predicate have can appear as a full,
non-copula predicate (with an A0 and A1) but of-
ten does not yield high-quality questions. For ex-
ample, consider the sentence “This effect can have
a large impact on the Earth’s climate.” Without the
!have filter, one of our templates would gener-
ate the question “What can this effect have?” With
the !have filter, that template does not yield any
questions from the given sentence.

Predicate modifiers allow the template author to
explicitly force a change in conjugation. See Ta-
ble 2 for the complete set of predicate modifiers,
where fps is an abbreviation for first person sin-
gular, sps for second person singular, and so on.
The lemma modifier can appear on its own. How-
ever, all other conjugation changes must specify
both a tense and a person. If no modifiers are used,
the predicate is copied as-is from the source sen-
tence. Although perfect is an aspect rather than a
tense, MorphAdorner1, which we use to conjugate
predicates, defines it as a tense, so we have imple-

1http://morphadorner.northwestern.edu

mented it as a tense filter.

Modifier Tense Modifier
lemma lemma (dictionary form) fps
pres present sps
prespart present participle tps
past past fpp
pastpart past participle spp
perf perfect tpp
pastperf past perfect
pastperfpart past perfect participle

Table 2: Predicate modifiers

3.3 Non-predicate slot options
The filters for non-predicate slots impose addi-
tional syntactic and named entity restrictions on
any matching role text. As with predicates, the
absence of any non-predicate filters results in the
mere presence of the corresponding semantic role
being sufficient for matching. See Table 3 for the
complete list of non-predicate filters describing re-
strictions on the role text (RT), role span (RS), and
predicate frame (PF) in terms of the semantic type
of named entities (and in some cases in terms of
non-semantic features).

Filter Description
null PF must not contain this semantic role.
!nv RS must not contain a predicate
dur RT must contain DURATION
date RT must contain DATE
!date RT must not contain a DATE
loc RT must contain a LOCATION.
ne RT must contain a named entity
misc RT must contain a MISC
comp RT must contain a comparison
!comma RT must not contain a comma
singular RT must be singular
plural RT must be plural

Table 3: Non-predicate filters

The choice of filters again requires some expla-
nation. The null and !nv filters were foreshad-
owed above. For slots appearing outside the tem-
plate’s plaintext, the null filter explicitly requires
that the corresponding semantic role not be present
in a source sentence semantic pattern. An A0 slot
paired with the null filter is the mechanism al-
luded to earlier that allows for the recognition of
copula predicates without the need to examine the
predicate itself. The !nv filter can be used to pre-
vent ungrammatical questions. We observe that
if a role span does include a predicate, resulting
questions are often ungrammatical due to the con-
jugation of that predicate. Applying this filter to

109



the A1 of a predicate prevents generation from a
predicate frame whose predicate is a non-modal
auxiliary verb.

The named entity filters (dur, !dur, date,
loc, ne, and misc) are those most relevant to
the corpus we have used to evaluate our approach
and thus the easiest to experiment with effectively.
Because named entities are used only for filtering,
expanding the set of named entity filters is a trivial
task.

The filters comp, !comma, singular, and
plural are syntax-based filters. With the ex-
ception of !comma, these filters force the exam-
ination of the part-of-speech (POS) tags to de-
tect the desired features. The singular and
plural filters let templates be tailored to singu-
lar and plural arguments in any desired way, be-
yond simply selecting appropriate auxiliary verbs.
The type of comparison we search for when the
comp filter is used is quite specific. We search
for phrases that describe conditions that are atypi-
cal. These can be seen in phrases such “unusually
weak,” “unseasonably warm,” “strangely absent,”
and so on. These phrases are present when a word
whose POS tag is RB (adverb) is followed by a
word whose tag is JJ (adjective). Consider a sen-
tence such as “El Nino is caused when the westerly
winds are unusually weak.” The comp filter allows
us to generate questions such as “What data would
indicate El Nino?” or “How do the conditions that
cause El Nino differ from normal conditions?” Al-
though this heuristic does produce both false pos-
itives and false negatives, other syntactic features
such as comparative adverbs and comparative ad-
jectives are less semantically constrained. Further
investigation is needed to determine more flexible
ways to recognize descriptions of atypical condi-
tions.

We see two situations in which a comma ap-
pears within the span of a single semantic role.
The first situation occurs when a list of nouns is
serving the role, such as in “Climate change in-
cludes changes in precipitation, temperature, and
pressure.” Here, “changes in precipitation, temper-
ature, and pressure” is the A1 of the predicate in-
cludes. In cases where a question is only appro-
priate for single concept (e.g. temperature) rather
than a set of concepts, the !comma filter pre-
vents such a question from being generated from
the sentence above. This has implications for role
text containing appositives, the second situation in

which a comma appears within a single role span.
Such roles are rejected when !comma is used.
This is not ideal, as removing appositives does not
cause semantic roles to be lost from a semantic
pattern. Future work will address this problem.

The non-predicate modifiers (Table 4) serve two
purposes: to create more fluent questions and to
remove non-essential text. Note that the -tpp,
which forces the removal of trailing prepositional
phrases, can have undesired results when applied
to certain modifier roles, such as AM-LOC, AM-
MNR, and AM-TMP, when they appear in the tem-
plate plaintext. These modifiers often contain only
a prepositional phrase, and in such cases, -tpp
will result in an empty string being placed into the
template.

Modifier Effect
-lp If initial token is prep, remove it
-tpp If RT ends with PP, remove PP
-ldt If initial token is determiner, remove it

Table 4: Non-predicate modifiers

3.4 Our QG system
Figure 3 shows the architecture and data flow of
our QG system. One of the most important things
to observe about this architecture is that the tem-
plates are an external input. They are in no way
coupled to the system and can be modified as
needed without any system modifications.

Compared to most other approaches, we per-
form very little pre-processing. Syntax-based
methods in particular have been motivated to per-
form sentence simplification, because their meth-
ods are more likely to generate meaningful ques-
tions from short, succinct sentences. We have cho-
sen not to perform any sentence simplification.
This decision was motivated by the observation
that common methods of sentence simplification
can eliminate useful semantic content. For exam-
ple, Kalady et al. (2010) claim that prepositional
phrases are often not fundamental to the meaning
of a sentence, so they remove them when simpli-
fying a sentence. However, as Figure 4 shows,
a prepositional phrase can contain important se-
mantic information. In that example, removing the
prepositional phrase causes temporal information
to be lost.

One pre-processing step we do perform is
pronominal anaphora resolution (Charniak and El-
sner, 2009). Even though we do not split com-

110



Figure 3: System architecture and data flow

Figure 4: Semantic information can be lost dur-
ing sentence simplification. Removing the prepo-
sitional phrase from the first sentence leaves the
simpler second sentence, but the AM-TMP modi-
fier is lost.

plex sentences and therefore do not create new
sentences in which pronouns are separated from
their antecedents, this kind of anaphora resolution
remains an important step in limiting the number
of vague questions.

Each source sentence is tokenized and anno-
tated with POS tags, named entities, lemmata, and
its SRL parse. SRL is the cornerstone of our ap-
proach. We generate the SRL parse (Collobert
et al., 2011) in order to extract a set of predicate
frames. Questions are generated from individ-
ual predicate frames rather than entire sentences
(unless the sentence contains only one predicate
frame). Given a sentence, the semantic pattern of
each of its predicate frames is compared against
that of each template. Algorithm 1 describes the
process of matching a single predicate frame (pf )
to a single template (t). Although it is not stated in
Algorithm 1, the sentence-level tokenization, lem-
mata, named entities and POS tags are checked
as needed according to the template’s slot filters.
If a predicate frame and template are matched,
they are passed to Algorithm 2, which fills tem-
plate slots with role text to produce a question.
Even in the absence of modifiers, all role text re-
ceives some additional processing before being in-
serted into its corresponding slot. These additional
steps include the removal of colons and the things
they introduce and the removal of text contained
in parentheses. We observe that these extra steps
lead to questions that are more meaningful.

Algorithm 1 patternsMatch(pf ,t)
for all slot ∈ t do

if pf does not have slot.role then
if null 6∈ slot.filters then

return false
end if

else
for all filter ∈ slot.filters do

if pf.role does not match filter then
return false

end if
end for

end if
end for
return true

Because we generate questions from predicate
frames rather than entire sentences, two sentences
describing the same semantic entities might result
in duplicate questions. To avoid duplicates we
keep only the first occurrence of a question.

Using slots and filters, we can now create some
interesting templates and see the questions they

111



Algorithm 2 fillTemplate(t,pf )
question text← t.plaintext
for all slot ∈ t.plaintext slots do

role text← pf.role(slot.role).text
for all modifier ∈ slot.modifiers do

applyModifier(role text,modifier)
end for
In question text, replace slot with role text

end for
return question text

yield. Table 5 shows some templates (T) that
match the sentence in Figure 1 and the questions
(Q) that result. Although the questions that are
generated are not answerable from the original
sentence, they were judged answerable from the
source document in our evaluation. The full set of
templates is provided in (Lindberg, 2013).

As recently as 12,500 years ago, the Earth was in the
midst of a glacial age referred to as the Last Ice Age.
T: How would you describe [A2 -lp misc]?
Q: How would you describe the Last Ice Age?
T: Summarize the influence of [A1 -lp !comma !nv] on
the environment.
Q: Summarize the influence of a glacial age on the envi-
ronment.
T: What caused [A2 -lp !nv misc]? ## [A0 null]
Q: What caused the Last Ice Age?

Table 5: A few sample templates and questions

4 Evaluation

There remains no standard set of evaluation met-
rics for assessing the quality of question gener-
ation output. Some present no evaluation at all
(Wyse and Piwek, 2009; Stanescu et al., 2008).
Among those who do perform an evaluation, there
does appear to be a consensus that some form
of human evaluation is necessary. Despite this
agreement in principle, approaches tend to diverge
thereafter. There are differences in the evaluation
criteria and the evaluation procedure.

Most previous efforts in QG have not gone be-
yond manual evaluation. While some have gone
a step further and built models for ranking based
on the probability of a question being acceptable
(Heilman and Smith, 2010), these models have not
had a strong basis in pedagogy. While a question
that is both syntactically and semantically well-
formed is considered acceptable in some evalua-
tion schemes, such questions can greatly outnum-
ber the questions that we can reasonably expect a
student would want or have time to answer. We
implement a classifier that attempts to identify the

questions that are the most pedagogically useful.

For our initial evaluation of the performance of
our QG system, we selected a subset of 10 doc-
uments from the collection described in the previ-
ous section. On average, each document contained
25 sentences. From the 10 documents, our system
generated 1472 questions in total, an average of
5.9 questions per sentence. Due to the educational
nature of this material, we needed evaluators with
educational training rather than naive ones. Ac-
cordingly, the questions we generated were evalu-
ated by a graduate student from the Faculty of Ed-
ucation. She was asked to give binary judgements
for grammaticality, semantic validity, vagueness,
answerability, and learning value. For each ques-
tion, two aspects of answerability were evaluated.
The first aspect was whether the question was an-
swerable from the source sentence from which
it was generated. The second was whether the
question was answerable given the source docu-
ment as a whole. The evaluator was given no pre-
determined guidelines regarding the relationships
among the evaluation criteria (e.g., the influence
of vagueness and answerability on learning value).
This aspect of the evaluation was left to her dis-
cretion as an educator. She found that 85% of the
questions were grammatical, with 66% of them ac-
tually making sense. It was determined that 14%
of the questions were answerable from the sen-
tence used to generate them, while 20% of them
were answerable from the document. Finally, she
determined that 17% of the questions had learn-
ing value according to the prescribed learning out-
comes for the curriculum being modeled. Aside
from performing this evaluation, the evaluator was
not involved in this research.

Given this evaluation, we then built a classi-
fier which used logistic regression (L2 regular-
ized log-likelihood) to classify on learning value.
We used length, language model, SRL, named en-
tity, glossary, and syntax features. Length and
language model features measure the token count
and grammaticality of a question and the sentence
from which it was generated. SRL features in-
clude the token count of each semantic role in the
generating predicate frame, whether each role is
required by the matching template, and whether
each role’s text is used. Named entity features
indicate the presence of each of nine named en-
tity types in both the source sentence and gener-
ated question. Glossary features note the number

112



of glossary terms that appear in a sentence and
question and a measure of the average importance
of each term, which we calculated from a sim-
ple in-terms-of graph (Winne and Hadwin, 2013)
we constructed from the glossary. This graph has
directed edges between each glossary term and
the terms that appear in its gloss. Syntax fea-
tures identify the depth of the generating predi-
cate frame in the source sentence and the POS tag
of its predicate. Without adding noise, the train-
ing set had 217 questions with learning value and
1101 questions without learning value. The clas-
sifier obtained precision and recall scores of 0.47
and 0.22 respectively for questions with learning
value, along with scores of 0.79 and 0.92 for ques-
tions with no learning value. We then added noise
to the training set by relabelling any grammati-
cal question that made sense as having learning
value. This relabelling resulted in a training set
of 778 questions with learning value and only 540
questions without learning value. The classifier
trained on this noisy set showed a precision score
on learning value questions decreased to 0.29 but
a dramatic increase in recall to 0.81. For questions
with no learning value, the precision increased
slightly to 0.86 which was offset by a dramatic de-
crease in recall to 0.38. So when the system gener-
ates a poor quality question, we have a high prob-
ability of knowing that it is a poor question which
allows us to then filter or discard it.

5 Conclusions

We have shown how a template-based method,
using predominately semantic information, can
be used to generate natural language questions
for use in an on-line learning system. Our tem-
plates are based on semantic patterns, which cast
a wide syntactic net and a narrow semantic net.
The template mechanism supports rich selectional
and generational capabilities, generating a large
pool from which questions for learners can be
selected. A simple automated technique for se-
lecting questions with learning value was intro-
duced. Although this automated technique shows
promise for some applications, future investiga-
tion into what constitutes a useful question in the
context of a specific task and an individual learner
is needed. Some might argue that it is risky to
generate questions that cannot be answered from
the source sentence from which they were gener-
ated. Although some questions are generated that

are not answered elsewhere in a document, there
is a benefit in learners being able to recognize that
a particular question is not answerable. Our future
work will expand both on the types of potential
questions generated, and on the selection from the
set of potential questions based on the information
an individual learner (a) knows, (b) has available
in a “library” of saved sources, (c) has operated
on while studying online (e.g., tagged), and (d)
might find in the Internet. To facilitate this further
research, we will be integrating question genera-
tion into the nStudy system (Hadwin et al., 2010;
Winne and Hadwin, 2013). We will also be per-
forming thorough user studies which will evalu-
ate the generated questions from the learner’s per-
spective in addition to the educator’s perspective.

Acknowledgments

This research was supported by an Insight De-
velopment Grant (#430-2012-044) from the So-
cial Sciences and Humanities Research Council
of Canada and a Discovery Grant from the Nat-
ural Sciences and Engineering Research Council
of Canada. The authors are extremely grateful to
Kiran Bisra from the Faculty of Education for pro-
viding information for the evaluation. Finally, spe-
cial thanks to the reviewers for their comments and
suggestions.

References
Husam Ali, Yllias Chali, and Sadid A Hasan. 2010.

Automation of question generation from sentences.
In Proceedings of QG2010: The Third Workshop on
Question Generation, pages 58–67.

Zhiqiang Cai, Vasile Rus, Hyun-Jeong Joyce Kim,
Suresh C. Susarla, Pavan Karnam, and Arthur C.
Graesser. 2006. Nlgml: A markup language
for question generation. In Thomas Reeves and
Shirley Yamashita, editors, Proceedings of World
Conference on E-Learning in Corporate, Govern-
ment, Healthcare, and Higher Education 2006,
pages 2747–2752, Honolulu, Hawaii, USA, Octo-
ber. AACE.

Aimee A. Callender and Mark A. McDaniel. 2007.
The benefits of embedded question adjuncts for low
and high structure builders. Journal Of Educational
Psychology (2007), pages 339–348.

Xavier Carreras and Lluı́s Màrquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning, pages
152–164. Association for Computational Linguis-
tics.

113



Eugene Charniak and Micha Elsner. 2009. Em works
for pronoun anaphora resolution. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 148–156. Association for Computational Lin-
guistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.

A.F. Hadwin, M. Oshige, C.L.Z. Gress, and P.H.
Winne. 2010. Innovative ways for using nstudy
to orchestrate and research social aspects of self-
regulated learning. Computers in Human Behaviour
(2010), pages 794–805.

Michael Heilman and Noah A Smith. 2010. Good
question! statistical ranking for question genera-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 609–617. Association for Computational Lin-
guistics.

Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das.
2010. Natural language question generation using
syntax and keywords. In Proceedings of QG2010:
The Third Workshop on Question Generation, pages
1–10.

Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.

Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC 2006.

David Lindberg. 2013. Automatic question generation
from text for self-directed learning. Master’s thesis,
Simon Fraser University, Canada.

Prashanth Mannem, Rashmi Prasad, and Aravind Joshi.
2010. Question generation from paragraphs at
upenn: Qgstec system description. In Proceedings
of QG2010: The Third Workshop on Question Gen-
eration, pages 84–91.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.

Bethany Rittle-Johnson. 2006. Promoting transfer:
Effects of self-explanation and direct instruction.
Child Development (2006), pages 1–15.

Vasile Rus and Arthur C Graesser. 1989. Classroom
questioning. In School improvement research series.

Liana Stanescu, Cosmin Stoica Spahiu, Anca Ion, and
Andrei Spahiu. 2008. Question generation for
learning evaluation. In Computer Science and In-
formation Technology, 2008. IMCSIT 2008. Interna-
tional Multiconference on, pages 509–513. IEEE.

Andrea Varga and Le An Ha. 2010. Wlv: A ques-
tion generation system for the qgstec 2010 task b.
In Proceedings of QG2010: The Third Workshop on
Question Generation, pages 80–83.

Philip H Winne and Allyson F Hadwin. 2013. nstudy:
Tracing and supporting self-regulated learning in the
internet. In International handbook of metacog-
nition and learning technologies, pages 293–308.
Springer.

John H Wolfe. 1976. Automatic question gener-
ation from text-an aid to independent study. In
ACM SIGCUE Outlook, volume 10, pages 104–112.
ACM.

Brendan Wyse and Paul Piwek. 2009. Generating
questions from openlearn study units. In AIED
2009 Workshop Proceedings Volume 1: The 2nd
Workshop on Question Generation, 6-9 July 2009,
Brighton, UK.

114


