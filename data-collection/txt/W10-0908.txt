










































Empirical Studies in Learning to Read


Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 61–69,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Empirical Studies in Learning to Read 

Marjorie Freedman, Edward Loper, Elizabeth Boschee, Ralph Weischedel 

BBN Raytheon Technologies 

10 Moulton St 

Cambridge, MA 02139 

{mfreedma, eloper, eboschee, weischedel}@bbn.com 

Abstract 

In this paper, we present empirical results on 

the challenge of learning to read. That is, giv-

en a handful of examples of the concepts and 

relations in an ontology and a large corpus, 

the system should learn to map from text to 

the concepts/relations of the ontology. In this 

paper, we report contrastive experiments on 

the recall, precision, and F-measure (F) of the 

mapping in the following conditions: (1) em-

ploying word-based patterns, employing se-

mantic structure, and combining the two; and 

(2) fully automatic learning versus allowing 

minimal questions of a human informant. 

1 Introduction 

This paper reports empirical results with an algo-

rithm that “learns to read” text and map that text 

into concepts and relations in an ontology specified 

by the user. Our approach uses unsupervised and 

semi-supervised algorithms to harness the diversity 
and redundancy of the ways concepts and relations 

are expressed in document collections. Diversity 

can be used to automatically generate patterns and 

paraphrases for new concepts and relations to 

boost recall. Redundancy can be exploited to au-

tomatically check and improve the accuracy of 

those patterns, allowing for system learning with-

out human supervision.  

For example, the system learns how to recog-

nize a new relation (e.g. invent), starting from 5-20 
instances (e.g. Thomas Edison + the light bulb). 

The system iteratively searches a collection of 

documents to find sentences where those instances 

are expressed (e.g. “Thomas Edison’s patent for 
the light bulb”), induces patterns over textual fea-

tures found in those instances (e.g. pa-

tent(possessive:A, for:B)), and repeats the cycle by 
applying the generated patterns to find additional 

instances followed by inducing more patterns from 

those instances. Unsupervised measures of redun-

dancy and coverage are used to estimate the relia-

bility of the induced patterns and learned instances; 

only the most reliable are added, which minimizes 

the amount of noise introduced at each step.  

There have been two approaches to evaluation 

of mapping text to concepts and relations: Auto-

matic Content Extraction (ACE)
1
 and Knowledge 

Base Population (KBP)
2
. In ACE, complete ma-

nual annotation for a small corpus (~25k words) 

was possible; thus, both recall and precision could 

be measured across every instance in the test set. 

This evaluation can be termed micro reading in 
that it evaluates every concept/relation mention in 

the corpus. In ACE, learning algorithms had 

roughly 300k words of training data.  

By contrast, in KBP, the corpus of documents 

in the test set was too large for a complete answer 

key. Rather than a complete answer key, relations 

were extracted for a list of entities; system output 

was pooled and judged manually. This type of 

reading has been termed macro reading3, since 
finding any instance of the relation in the 1.3M 

document corpus is measured success, rather than 

finding every instance. Only 118 queries were pro-

vided, though several hundred were created and 

distributed by participants.  

In the study in this paper, recall, precision, and 

F are measured for 11 relations under the following 

contrastive conditions 

                                                          
1 http://www.nist.gov/speech/tests/ace/ 
2 http://apl.jhu.edu/~paulmac/kbp.html  
3 See http://rtw.ml.cmu.edu/papers/mitchell-iswc09.pdf   

61



1. Patterns based on words vs. predicate-

argument structure vs. combining both. 

2. Fully automatic vs. a few periodic res-

ponses by humans to specific queries. 

Though many prior studies have focused on 

precision, e.g., to find any text justification to an-

swer a question, we focus equally on recall and 

report recall performance as well as precision. This 

addresses the challenge of finding information on 

rarely mentioned entities (no matter how challeng-

ing the expression). We believe the effect will be 

improved technology overall. We evaluate our sys-

tem in a micro-reading context on 11 relations. In a 

fully automatic configuration, the system achieves 

an F of .48 (Recall=.37, Precision=.68). With li-

mited human intervention, F rises to .58 (Re-

call=.49, Precision=.70). We see that patterns 

based on predicate-argument structure (text 

graphs) outperform patterns based on surface 

strings with respect to both precision and recall. 

Section 2 describes our approach; section 3, 

some challenges; section 4, the implementation; 

section 5, evaluation; section 6, empirical results 

on extraction type; section 7, the effect of periodic, 

limited human feedback; section 8, related work; 

and section 9, lessons learned and conclusions. 

2 Approach 

Our approach for learning patterns that can be used 

to detect relations is depicted in Figure 1. Initially, 

a few instances of the relation tuples are provided, 

along with a massive corpus, e.g., the web or the 

gigaword corpus from the Linguistic Data Consor-

tium (LDC). The diagram shows three inventor-

invention pairs, beginning with Thomas Edi-
son…light bulb. From these, we find candidate 

sentences in the massive corpus, e.g., Thomas Edi-
son invented the light bulb. Features extracted from 

the sentences retrieved, for example features of the 

text-graph (the predicate-argument structure con-

necting the two arguments), provide a training in-

stance for pattern induction. The induced patterns 

are added to the collection (database) of patterns.

Running the extended pattern collection over the 

corpus finds new, previously unseen relation 

tuples. From these new tuples, additional sentences 

which express those tuples can be retrieved, and 

the cycle of learning can continue. 

There is an analogous cycle of learning con-

cepts from instances and the large corpus; the ex-

periments in this paper do not report on that paral-

lel learning cycle. 

Figure 1: Approach to Learning Relations 

At the i
th
 iteration, the steps are 

1. Given the set of hypothesized instances of the 

relation (triples HTi), find instances of such 

triples in the corpus. (On the first iteration, 

“hypothesized” triples are manually-generated 

seed examples.) 

2. Induce possible patterns. For each proposed 

pattern P: 

a. Apply pattern P to the corpus to generate a 

set of triples TP
b. Estimate precision as the confidence-

weighted average of the scores of the 

triples in TP. Reduce precision score by the 

percentage of triples in TP that violate us-

er-specified relation constraints (e.g. arity 

constraints described in 4.3)  

c. Estimate recall as the confidence-weighted 

percentage of triples in HTi found by the 

pattern 

3. Identify a set of high-confidence patterns HPi
using cutoffs automatically derived from rank-

based curves for precision, recall, and F-

measure (α=0.7) 

4. Apply high-confidence patterns to a Web-scale 

corpus to hypothesize new triples. For each 

proposed triple T 

a. Estimate score(T) as the expected proba-
bility that T is correct, calculated by com-

bining the respective precision and recall 

scores of all of the patterns that did or did 

not return it (using the Naïve Bayes as-

sumption that all patterns are independent) 

b. Estimate confidence(T) as the percentage 
of patterns in HPi by which T was found 

5. Identify a set of high-confidence triples HTi+1

��������

����	�
�

�����
��

��
����


�����
���

�������


�����
��

����


������������	�


����

������������	� 
����

����
�

������

�������

�������

������

	
�

������

������

�	
�

�	�

��������	
������ 
�������

�����	������������� ��������

���������
���� 
����
�����	

Edison invented the light bulb

Bell built the first telephone

Edison was granted a U.S. patent 

for the light bulb

Franklin invented the lightning rod

���	��������

�����
�

62



using cutoffs automatically derived from rank

based curves; use these triples to begin the 

next iteration 

3 Challenges 

The iterative cycle of learning we describe above 

has most frequently been applied for 

reading tasks. However, here we are interested in 
measuring performance for micro-

are several reasons for wanting to measure perfo

mance in a micro-reading task:  

• For information that is rare (e.g. relations 

about infrequently mentioned entities), a m

cro-reading paradigm may more accurately 

predict results. 

• For some tasks or domains micro

be all that we can do-- the actual corpus of i

terest may not be macro-scaled.

• Macro-reading frequently utilizes statistics of 

extraction targets from the whole corpus to 

improve its precision. Therefore, i

micro-reading could improve the precision of 

macro-reading.  

• Because we are measuring performance in a 

micro-reading context, recall at the instance 

level is as important as precision.

ly, our learning system must learn to predict

patterns that incorporate nominal and pron

minal mentions.  

• Furthermore, while the approach we describe 

makes use of corpus-wide statistics during the 

learning process, during pattern application we 

limit ourselves to only information from within 

a single document (and in practice primarily 

within a single sentence). Our evaluation 

measures performance at the instance

4 Implementation 

4.1 Pattern Types 

Boschee et al.(2008) describes two types of pa

terns: patterns that rely on surface strings and pa

terns that rely only on two types of syntactic

structure. We diverge from that early work by a

                                                          
4Our evaluation measures only performance in extracti

relation: that is if the text of sentence implies to an annot

that the relation is present, then the annotator ha

structed to mark the sentence as correct (regardles

or not outside knowledge contradicts this fact).

using cutoffs automatically derived from rank-

rves; use these triples to begin the 

The iterative cycle of learning we describe above 

has most frequently been applied for macro-
However, here we are interested in 

-reading. There 

several reasons for wanting to measure perfor-

For information that is rare (e.g. relations 

about infrequently mentioned entities), a mi-

reading paradigm may more accurately 

For some tasks or domains micro-reading may 

the actual corpus of in-

scaled.

reading frequently utilizes statistics of 

targets from the whole corpus to 

Therefore, improving 

the precision of 

Because we are measuring performance in a 

reading context, recall at the instance 

level is as important as precision. Consequent-

learn to predict

patterns that incorporate nominal and prono-

Furthermore, while the approach we describe 

wide statistics during the 

learning process, during pattern application we 

limit ourselves to only information from within 

a single document (and in practice primarily 

Our evaluation 

measures performance at the instance-level
4
.  

two types of pat-

patterns that rely on surface strings and pat-

two types of syntactic-

early work by al-

ur evaluation measures only performance in extracting the 

that is if the text of sentence implies to an annotator 

that the relation is present, then the annotator has been in-

structed to mark the sentence as correct (regardless of whether 

contradicts this fact).  

lowing more expressive surface-string patterns: our 

surface-string patterns can include wild

lowing the system to make match

omitting words). For example for

tim), the system learns the pattern 
assassinated <VICTIM>, which correctly match
Booth, with hopes of a resurgent Confederacy in 

mind, cruelly assassinated Lincoln
We also diverge from the earlier 

making use of patterns based on the 

predicate-argument structure and not dependency 

parses. The normalized predicate

tures (text-graphs) are built by performing a set of 

rule-based transformations on the syntactic parse 

of a sentence. These transformations include fi

ing the logical subject and object for each verb, 

resolving some traces, identifying temporal arg

ments, and attaching other verb arguments with 

lexicalized roles (e.g. ‘of’ in Figure 

ing graphs allow both noun and verb predicates.

Manually created patterns using

have been successfully used for event detection 

and template-based question answering.

graph structures have also served as useful feature

in supervised, discriminative models for relation 

and event extraction. While 

the experiments described 

here do not include depen-

dency tree paths, we do 

allow arbitrarily large text 

graph patterns.  

4.2 Co-Reference 

Non-named mentions are essential for a

high instance-level recall. In certain cases, a 

tion is most clearly and frequently e

pronouns and descriptions (e.g 

relation child).5 Because non-named instances a

pear in different constructions than named i

stances, we need to learn patterns that will appear

in non-named contexts. Thus, c

mation is used during pattern induction to extract 

patterns from sentences where the hypothesized 

triples are not explicitly mentioned. In particular

any mention that is co-referent with the desired 

entity can be used to induce a pattern.

for 7 types of entities is produced by SERIF, a 

                                                          
5 The structure of our noun-predicates allows us to learn lex

calized patterns in cases like this. For her father 

induce the pattern n:father:<ref>PARENT, 

Figure 2: 

string patterns: our 

string patterns can include wild-cards (al-

matches which require 

for kill(agent, vic-

system learns the pattern <AGENT> <...> 
, which correctly matches 

Booth, with hopes of a resurgent Confederacy in 

mind, cruelly assassinated Lincoln.  
earlier work by only 

making use of patterns based on the normalized 

argument structure and not dependency 

parses. The normalized predicate-argument struc-

graphs) are built by performing a set of 

based transformations on the syntactic parse 

These transformations include find-

ing the logical subject and object for each verb, 

, identifying temporal argu-

ments, and attaching other verb arguments with 

Figure 2). The result-

ing graphs allow both noun and verb predicates.  

using this structure 

have been successfully used for event detection 

based question answering. The text 

graph structures have also served as useful features 

in supervised, discriminative models for relation 

named mentions are essential for allowing 

In certain cases, a rela-

tion is most clearly and frequently expressed with 

pronouns and descriptions (e.g her father for the 
named instances ap-

pear in different constructions than named in-

stances, we need to learn patterns that will appear 

. Thus, co-reference infor-

mation is used during pattern induction to extract 

patterns from sentences where the hypothesized 

triples are not explicitly mentioned. In particular, 

referent with the desired 

entity can be used to induce a pattern. Co-reference 

7 types of entities is produced by SERIF, a 

                  
predicates allows us to learn lexi-

her father we would 

:father:<ref>PARENT, <pos>CHILD. 

: Text Graph Pattern 

63



state of the art information extraction engine.

manually determined confidence threshold is used 

to discard mentions where co-reference certainty is 

too low. 

During pattern matching, co-reference is used 

to find the “best string” for each element of the 

matched triple. In particular, pronouns and descri

tors are replaced by their referents; and abbreviat

names are replaced by full names.

cannot be resolved to a description or a name, or i

the co-reference threshold falls below a manually 

determined threshold, then the match is discarded.

Pattern scoring requires that we compare i

stances of triples across the whole corpus.  If the

instances were compared purely on the basis of 

strings, in many cases the same entity would a

pear as distinct (e.g. US, United States)
would interfere with the arity constraints describe

below.  To alleviate this challenge, we u

base of name strings that have been shown to be 

equivalent with a combination of edit

extraction statistics (Baron & Freedman

Thus, for triple(tP) and hypothesized triples (HT

if tP∉HTi, but can be mapped via the equivalent 

names database to some triple tP’

score and confidence are adjusted

tP’, weighted by the confidence of the equivalence.

4.3 Relation Set and Constraints

We ran experiments using 11 relation types. The 

relation types were selected as a subset of the rel

tions that have been proposed for DARPA’s m

chine reading program. In addition to seed 

examples, we provided the learning system with 

three types of constraints for each relation: 

Symmetry: For relations where R(X,Y)

R(Y,X), the learning (and scoring process), norm

lizes instances of the relation so that R(X,Y

R(Y,X) are equivalent.  

Arity: For each argument of the relation, pr

vide an expected maximum number of instances 

per unique instance of the other argument.

numbers are intentionally higher than the expected 

true value to account for co-referen

Patterns that violate the arity constraint (e.g 

v:accompanied(<obj>=<X>, <sub>=<Y>
pattern for spouse) are penalized.  This is one way 
of providing negative feedback during the uns

pervised training process.  

Argument Type: For each argument, specify 

state of the art information extraction engine. A 

manually determined confidence threshold is used 

reference certainty is 

reference is used 

nd the “best string” for each element of the 

In particular, pronouns and descrip-

tors are replaced by their referents; and abbreviated 

If any pronoun 

cannot be resolved to a description or a name, or if 

reference threshold falls below a manually 

determined threshold, then the match is discarded.

Pattern scoring requires that we compare in-

stances of triples across the whole corpus.  If these 

instances were compared purely on the basis of 

y cases the same entity would ap-

US, United States). This 
would interfere with the arity constraints described 

below.  To alleviate this challenge, we use a data-

name strings that have been shown to be 

tion of edit-distance and 

& Freedman, 2008). 

) and hypothesized triples (HTi), 

, but can be mapped via the equivalent 

’∈HTi, then its 

towards that of 

’, weighted by the confidence of the equivalence.

Relation Set and Constraints

We ran experiments using 11 relation types. The 

relation types were selected as a subset of the rela-

tions that have been proposed for DARPA’s ma-

program. In addition to seed 

examples, we provided the learning system with 

three types of constraints for each relation: 

: For relations where R(X,Y) = 

R(Y,X), the learning (and scoring process), norma-

lizes instances of the relation so that R(X,Y) and 

For each argument of the relation, pro-

vide an expected maximum number of instances 

per unique instance of the other argument. These 

numbers are intentionally higher than the expected 

reference mistakes. 

Patterns that violate the arity constraint (e.g 

v:accompanied(<obj>=<X>, <sub>=<Y> as a 
This is one way 

of providing negative feedback during the unsu-

For each argument, specify 

the expected class of entities for this argument.

Entity types are one of the 7 ACE types 

Organization, Geo-political entity, Location, Faci

ity, Weapon, Vehicle) or Date.

tem only allows instance proposals when the types 

are correct. Potentially, the system could use pa

tern matches that violate type constraints as an a

ditional type of negative example.

implementation would need to account for the fact 

that in some cases, potentially too general pa

are quite effective when the type constraints are 

met. For example, for the relation 

ployed(PERSON, ORGANIZATION)
<PER> is a fairly precise pattern, despite clearly 

being overly general without the type constraints.

In our relation set, only two relations (

and spouse) are symmetric. Table 
the other constraints. ACE types/dates are in co

umns labeled with the first letter of t

type (A is arity). We have only included those 

types that are an argument for some relation

Table 1: Argument types of the test relations

4.4 Corpus and Seed Examples

While many other experiments using this approach

have used web-scale corpora, we chose to include 

Wikipedia articles as well as Gigaword

vide additional instances of information (e.g. 

birthDate and sibling) that is uncommon in news.
For each relation type, 20 seed

selected randomly from the corpus by using a 

combination of keyword search and an ACE e

traction system to identify passages that were lik

ly to contain the relations of interest.

seed example was guaranteed to occur at least once 

in a context that indicated the relation was present.

5 Evaluation Framework

To evaluate system performance, we ran two sep

the expected class of entities for this argument.

Entity types are one of the 7 ACE types (Person, 

political entity, Location, Facil-

Currently the sys-

sals when the types 

are correct. Potentially, the system could use pat-

tern matches that violate type constraints as an ad-

ditional type of negative example. Any 

implementation would need to account for the fact 

too general patterns 

are quite effective when the type constraints are 

met. For example, for the relation em-

ployed(PERSON, ORGANIZATION), <ORG>’s
<PER> is a fairly precise pattern, despite clearly 

being overly general without the type constraints.   

only two relations (sibling

Table 1 below includes 

ACE types/dates are in col-

umns labeled with the first letter of the name of the 

. We have only included those 

types that are an argument for some relation. 

: Argument types of the test relations  

Corpus and Seed Examples

While many other experiments using this approach

scale corpora, we chose to include 

Wikipedia articles as well as Gigaword-3 to pro-

vide additional instances of information (e.g. 

is uncommon in news.

For each relation type, 20 seed-examples were 

from the corpus by using a 

combination of keyword search and an ACE ex-

traction system to identify passages that were like-

ly to contain the relations of interest. As such, each 

seed example was guaranteed to occur at least once 

he relation was present.  

performance, we ran two sepa-

64



rate annotation-based evaluations, the first mea

ured precision, and the second measure

To measure overall precision, we ran each sy

tem’s patterns over the web-scale corpora, and 

randomly sampled 200 of the instances it found

These instances were then manually 

whether they conveyed the intended relation or not.

The system precision is simply the percentage of 

instances that were judged to be correct.

To measure recall, we began by randomly s

lecting 20 test-examples from the corpus, using the 

same process that we used to select the training 

seeds (but guaranteed to be distinct from the trai

ing seeds). We then searched the web

for sentences that might possibly link these test 

examples together (whether directly or via co

reference). We randomly sampled this set of se

tences, choosing 10 sentences for each test

example, to form a collection of 200 sentences 

which were likely to convey the desired relation.

These sentences were then manually annotated to 

indicate which sentences actually convey the d

sired relation; this set of sentences forms the 

test set. Once a recall set had been created for each 

relation, a system’s recall could be

running that system’s patterns over the documents 

in the recall set, and checking what percentage of 

the recall test sentences it correctly identified.

We intentionally chose to sample 10 sentences 

from each test example, rather than sampling from 

the set of all sentences found for any of the test

examples, in order to prevent one or two very 

common instances from dominating the recall set.

As a result, the recall test set is somewhat biased

away from “true” recall, since it places a higher 

weight on the “long tail” of instances.

we believe that this gives a more accurate indic

tion of the system’s ability to find novel instance

of a relation (as opposed to novel ways of talking 

about known instances).   

6 Effect of Pattern Type 

As described in 4.1, our system is capable of lear

ing two classes of patterns: surface

text-graphs. We measured our system’s perfo

mance on each of the relation types after twenty 

                                                          
6 While the system provides estimated precision for each pa

tern, we do not evaluate over the n-best matches. All patterns 

with estimated confidence above 50%  are treated eq

sample from the set of matches produced by these pa

based evaluations, the first meas-

precision, and the second measured recall.  

, we ran each sys-

scale corpora, and 

pled 200 of the instances it found
6
. 

These instances were then manually assessed as to 

the intended relation or not.

The system precision is simply the percentage of 

o be correct.

we began by randomly se-

examples from the corpus, using the 

same process that we used to select the training 

(but guaranteed to be distinct from the train-

We then searched the web-scale corpus 

for sentences that might possibly link these test 

examples together (whether directly or via co-

We randomly sampled this set of sen-

tences, choosing 10 sentences for each test-

example, to form a collection of 200 sentences 

convey the desired relation.

These sentences were then manually annotated to 

indicate which sentences actually convey the de-

set of sentences forms the recall 
Once a recall set had been created for each 

could be evaluated by 

running that system’s patterns over the documents 

in the recall set, and checking what percentage of 

the recall test sentences it correctly identified.

We intentionally chose to sample 10 sentences 

ple, rather than sampling from 

the set of all sentences found for any of the test

examples, in order to prevent one or two very 

common instances from dominating the recall set.

As a result, the recall test set is somewhat biased 

nce it places a higher 

weight on the “long tail” of instances. However, 

we believe that this gives a more accurate indica-

tion of the system’s ability to find novel instances 

(as opposed to novel ways of talking 

, our system is capable of learn-

surface-strings and 

graphs. We measured our system’s perfor-

ance on each of the relation types after twenty 

precision for each pat-

best matches. All patterns 

with estimated confidence above 50%  are treated equally. We  

sample from the set of matches produced by these patterns.  

iterations. In each iteration, the system can learn 

multiple patterns of either type.

no penalty for learning overlapping pattern types. 

For example, in the first iteration for the relation

killed(), the system learns both the surface
pattern <AGENT> killed <VICTIM
graph pattern: v:killed(<sub>=<AGENT>, 

<obj>=<VICTIM>). During decoding, if multiple 
patterns match the same relation instance, the sy

tem accepts the relation instance, but does not 

make use of the additional information that there 

were multiple supporting patterns. 

Figure 3: Precision of Pattern Types by Relation

Figure 4: Recall of Pattern Type by 

Figure 5: F-Score of Pattern Type by Relation

Figure 3, Figure 4, and Figure 5 plot precision, 

recall, and F-score for each of the 11 relations 

showing performance of all patterns vs. only text

graph patterns vs. only surface-string patterns.

• For most relations, the text-graph patterns pr

vide both higher precision and higher recall 

than the surface-string patterns. 

cision of the text-graph patterns for 

the result of the system learning a number of 

overly general patterns that correlate with a

tacks, but do not themselves indicate the pre

In each iteration, the system can learn 

There is currently 

no penalty for learning overlapping pattern types. 

or example, in the first iteration for the relation

(), the system learns both the surface-string 

<AGENT> killed <VICTIM> and the text-
v:killed(<sub>=<AGENT>, 

During decoding, if multiple 

patterns match the same relation instance, the sys-

n instance, but does not 

make use of the additional information that there 

were multiple supporting patterns. 

: Precision of Pattern Types by Relation

Relation

Score of Pattern Type by Relation

plot precision, 

score for each of the 11 relations 

mance of all patterns vs. only text-

string patterns.

graph patterns pro-

vide both higher precision and higher recall 

string patterns. The lower pre-

graph patterns for attackOn is 

the result of the system learning a number of 

overly general patterns that correlate with at-

tacks, but do not themselves indicate the pres-

65



ence of an attack.  For instance, the system 

learns patterns with predicates said 

Certainly, governments often make statements 

on the date of an attack and troops arrive in a 

location before attacking, but both patterns will 

produce a large number of spurious instances.

• While text-graph patterns typically have

precision than the combined pattern set, su

face-string patterns provide enough improv

ment in recall that typically the all

score is higher than the text-graph F

Figure 6: Text-Graph and Surface-String 

A partial explanation for the higher recall and 

precision of the text-graph patterns is illustrated in 

Figure 6 which presents a simple surface

pattern and a simple text-graph pattern that appear 

to represent the same information. On the right of 

the figure are three sentences. The text

tern correctly identifies the agent
each sentence. However, the surface

misses the killed() relation in the first sentence and 
misidentifies the victim in the second sentence. The 
false-alarm in the second sentence would have

been avoided by a system that restricted itself to 

matching named instances, but as described above 

in section 4.2, for the micro-reading task described 

here, detecting relations with pronouns is critical. 

While we allowed the creation of text

patterns with arbitrarily long paths between the 

arguments, in practice, the system rarely learned 

such patterns. For the relation killed(Agent, Vi
tim), we learned 8 patterns that have more than one 

predicate (compared to 22 that only have a single 

predicate). For the relation 

tion(Victim, Location), the system learned 28 pa

terns with more than 1 predicate (compared with 

20 containing only 1 predicate). In

precision of the longer patterns was higher, but 

their recall was significantly lower.

killedInLocation, none of the longer path patterns 

matched any of the examples in our recall set.

One strength of text-graph patterns is

for intelligent omission of overly specific text

example, ignoring ‘during a buglary’

For instance, the system 

said and arrive.

Certainly, governments often make statements 

and troops arrive in a 

location before attacking, but both patterns will 

produce a large number of spurious instances.

graph patterns typically have higher 

precision than the combined pattern set, sur-

string patterns provide enough improve-

ment in recall that typically the all-pattern F-

graph F-score. 

String Patterns 

A partial explanation for the higher recall and 

graph patterns is illustrated in 

which presents a simple surface-string 

graph pattern that appear 

to represent the same information. On the right of 

the figure are three sentences. The text-graph pat-

agent and victim in 
each sentence. However, the surface-string pattern, 

() relation in the first sentence and 

in the second sentence. The 

alarm in the second sentence would have

been avoided by a system that restricted itself to 

matching named instances, but as described above 

reading task described 

ecting relations with pronouns is critical. 

While we allowed the creation of text-graph 

patterns with arbitrarily long paths between the 

arguments, in practice, the system rarely learned 

killed(Agent, Vic-
patterns that have more than one 

predicate (compared to 22 that only have a single 

predicate). For the relation killedInLoca-
, the system learned 28 pat-

more than 1 predicate (compared with 

In both cases, the 

precision of the longer patterns was higher, but 

their recall was significantly lower. In the case of 

none of the longer path patterns 

matched any of the examples in our recall set.  

graph patterns is allowing 

overly specific text, for 

during a buglary’ in Figure 6. 

Surface string patterns can include 

for surface string patterns, the omission

tactically defined. Approximately 30% of surface 

string patterns included one wild

tional 17% included two. Figure 

aged precision and recall for text

surface-string patterns. The final three columns 

break the surface-string patterns down by the nu

ber of wild-cards. It appears that with one, the pa

terns remain reasonably precise, but the addition o

a second wild-card drops precision by more than 

50%. The presence of wild-card

recall, but surface-string patterns do not reach the 

level of recall of text-graph patterns.
 Text Graph Surface String

All No

Precision 0.75 0.61 0.72

Recall 0.32 0.22 0.16

Figure 7: Performance by Number of 

7 Effect of Human Review

In addition to allowing the system to self

completely unsupervised manner, we ran a parallel 

set of experiments where the system was given 

limited human guidance. At the end of 

5, 10, and 20, a person provided 

of feedback (on average 5 minutes)

was presented with five patterns

matched instances for each pattern.

able to provide two types of feedback:

• The pattern is correct/incorrect (e.g. 

<EMPLOYEE> said <ORGANIZATION> is 

an incorrect pattern for employ(

• The matched instances are correct/incorrect 

(e.g. ‘Bob received a diploma from 

correct instance, even if the pattern that pr

duced it is debatable (e.g. v:<received> 
subj:PERSON, from:ORGANIZATION

rect instance can also produce a new

to-be correct seed.  

Pattern judgments are stored in the database and 

incorporated as absolute truth. Instance judgments 
provide useful input into pattern scoring.

were selected for annotation using 

combines their estimated f-measure; the

cy; and their dissimilarity to patterns that

previously chosen for annotation.

instances for each pattern are randomly sampled, to 

ensure that the resulting annotation 

derive an unbiased precision estima

include wild-cards, but 

the omission is not syn-

Approximately 30% of surface 

wild-card. An addi-

Figure 7 presents aver-

for text-graph and 

The final three columns 

string patterns down by the num-

It appears that with one, the pat-

terns remain reasonably precise, but the addition of 

precision by more than 

patterns improves 

string patterns do not reach the 

graph patterns.  
Surface String

No-* 1-* 2-* 

0.72 0.69 0.30 

0.16 0.10 0.09 

by Number of WildCards (*) 

Effect of Human Review

In addition to allowing the system to self-train in a 

completely unsupervised manner, we ran a parallel 

set of experiments where the system was given 

At the end of iterations 1, 

, a person provided under 10 minutes 

of feedback (on average 5 minutes). The person 

patterns, and five sample 

matched instances for each pattern. The person was 

able to provide two types of feedback:

The pattern is correct/incorrect (e.g. 

<ORGANIZATION> is 

an incorrect pattern for employ(X,Y)) 

The matched instances are correct/incorrect 

received a diploma from MIT’ is a 

correct instance, even if the pattern that pro-

v:<received> 
subj:PERSON, from:ORGANIZATION). A cor-

rect instance can also produce a new known-

Pattern judgments are stored in the database and 

Instance judgments 

provide useful input into pattern scoring. Patterns 

using a score that

easure; their frequen-

cy; and their dissimilarity to patterns that were 

hosen for annotation. The matched 

randomly sampled, to 

ensure that the resulting annotation can be used to 

derive an unbiased precision estimate. 

66



Figure 8, Figure 9, and Figure 10

recall, and F-score at iterations 5 and 20 for the 

system running in a fully unsupervised manner and 

one allowing human intervention.  

Figure 8: Precision at Iterations 5 and 20 for the Uns

pervised System and the System with Intervention

Figure 9: Recall at Iterations 5 and 20 for 

vised System and the System with Intervention

Figure 10: F-Score at Iterations 5 and 20 for the Uns

pervised System and the System with Intervention

For two relations: child and sibling

proved dramatically with human intervention. By 

inspecting the patterns produced by the system, we 

see that in case of sibling without intervention, the 

system only learned the relation ‘brother’
the relation ‘sister.’ The limited feedback from a 

person was enough to allow the system to learn 

patterns for sister as well, causing the significantly 

improved recall. We see smaller, but frequently 

significant improvements in recall in a number of 

other relations. Interestingly, for different relat

the recall improvements are seen at different iter

tions. For sibling, the jump in recall appears within 

the first five iterations. Contrastingly, for 

School, there is a minor improvement in recall a

10 plot precision, 

score at iterations 5 and 20 for the 

fully unsupervised manner and 

: Precision at Iterations 5 and 20 for the Unsu-

pervised System and the System with Intervention

and 20 for the Unsuper-

System with Intervention

Score at Iterations 5 and 20 for the Unsu-

pervised System and the System with Intervention

sibling, recall im-

atically with human intervention. By 

inspecting the patterns produced by the system, we 

without intervention, the 

brother’ and not 
The limited feedback from a 

nough to allow the system to learn 

as well, causing the significantly 

improved recall. We see smaller, but frequently 

significant improvements in recall in a number of 

other relations. Interestingly, for different relations, 

the recall improvements are seen at different itera-

the jump in recall appears within 

the first five iterations. Contrastingly, for attend-
, there is a minor improvement in recall af-

ter iteration 5, but a much larger improvement afte

iteration 20. For child, there is actually a small d

crease in recall after 5 iterations, but after 20 iter

tions, the system has dramatically improved. 

The effect on precision is similarly varied. For 

9 of the 11, human intervention improves prec

sion; but the improvement is never as dramatic as 

the improvement in recall. For precision, the 

strongest improvements in performance appear in 

the early iterations. It is unclear whether this mer

ly reflects that bootstrapping is likely to become 

less precise over time (as it learns

or if early feedback is truly better for improving 

precision.  

In the case of attackOn, even with 
vention, after iteration 10, the system begins to 

learn very general patterns of the type described in 

the previous section (e.g. <said in:LOCATION 
on:DATE> as a pattern indicating an attack

patterns may be correlated with experiencing an 

attack but are not themselves evidence of an attack

Because the overly general patterns do in fact co

relate with the presence of an attack, the positive

examples provided by human intervention may in 

fact produce more such patterns. 

There is an interaction between improved prec

sion and improved recall. If a system is very i

precise at iteration n, the additional instances that it 
proposes may not reflect the relation and be so di

ferent from each other that the system becomes 

unable to produce good patterns that improve r

call. Conversely, if recall at iteration 

produce a sufficiently diverse set of instances, it

will be difficult for the system to generate 

stances that are used to estimate pattern precision. 

8 Related Work 

Much research has been done on concept and 

relation detection using large amounts of supe

vised training. This is the typical approach in pr

grams like Automatic Content Extraction (ACE), 

which evaluates system performance 

fixed set of concepts and relations in text. In ACE

all participating researchers are given access to a

substantial amount of supervised training, e.g., 

250k words of annotated data. Researchers have 

typically used this data to incorporate a great deal 

of structural syntactic information in their models

(e.g. Ramshaw 2001), but the obvious weakness of 

these approaches is the resulting reliance on the 

ter iteration 5, but a much larger improvement after 

there is actually a small de-

recall after 5 iterations, but after 20 itera-

tions, the system has dramatically improved. 

The effect on precision is similarly varied. For 

human intervention improves preci-

sion; but the improvement is never as dramatic as 

For precision, the 

strongest improvements in performance appear in 

It is unclear whether this mere-

ly reflects that bootstrapping is likely to become 

less precise over time (as it learns more patterns), 

truly better for improving 

even with human inter-

vention, after iteration 10, the system begins to 

of the type described in 

<said in:LOCATION 
ndicating an attack. These 

may be correlated with experiencing an 

attack but are not themselves evidence of an attack.

Because the overly general patterns do in fact cor-

relate with the presence of an attack, the positive 

intervention may in 

fact produce more such patterns. 

here is an interaction between improved preci-

If a system is very im-

, the additional instances that it 

proposes may not reflect the relation and be so dif-

ferent from each other that the system becomes 

produce good patterns that improve re-

Conversely, if recall at iteration n does not 
produce a sufficiently diverse set of instances, it 

will be difficult for the system to generate in-

are used to estimate pattern precision.    

Much research has been done on concept and 

relation detection using large amounts of super-

vised training. This is the typical approach in pro-

grams like Automatic Content Extraction (ACE), 

luates system performance in detecting a 

fixed set of concepts and relations in text. In ACE, 

all participating researchers are given access to a 

substantial amount of supervised training, e.g., 

250k words of annotated data. Researchers have 

this data to incorporate a great deal 

of structural syntactic information in their models 

(e.g. Ramshaw 2001), but the obvious weakness of 

these approaches is the resulting reliance on the 

67



manually annotated examples, which are expensive 

and time-consuming to create. 

Co-training circumvents this weakness by play-

ing off two sufficiently different views of a data set 

to leverage large quantities of unlabeled data 

(along with a few examples of labeled data), in 

order to improve the performance of a learning 

algorithm (Mitchell and Blum, 1998). Co-training 

will offer our approach to simultaneously learn the 

patterns of expressing a relation and its arguments. 

Other researchers have also previously explored 

automatic pattern generation from unsupervised 

text, classically in (Riloff & Jones 1999). Ravi-

chandran and Hovy (2002) reported experimental 

results for automatically generating surface pat-

terns for relation identification; others have ex-

plored similar approaches (e.g. Agichtein & 

Gravano 2000 or Pantel & Pennacchiotti, 2006). 

More recently (Mitchell et al., 2009) has shown 

that for macro-reading, precision and recall can be 

improved by learning a large set of interconnected 

relations and concepts simultaneously.  

We depart from this work by learning patterns 

that use the structural features of text-graph pat-

terns and our particular approach to pattern and 

pair scoring and selection.  

Most approaches to automatic pattern genera-

tion have focused on precision, e.g., Ravichandran 

and Hovy report results in the Text Retrieval Con-

ference (TREC) Question Answering track, where 

extracting one instance of a relation can be suffi-

cient, rather than detecting all instances. This study 

has also emphasized recall. Information about an 

entity may only be mentioned once, especially for 

rarely mentioned entities. A primary focus on pre-

cision allows one to ignore many instances that 

require co-reference or long-distance dependen-

cies; one primary goal of our work is to measure 

system performance in exactly those areas. 

9 Conclusion 

We have shown that bootstrapping approaches can 

be successfully applied to micro-reading tasks. 

Most prior work with this approach has focused on 

macro-reading, and thus emphasized precision.  

Clearly, the task becomes much more challenging 

when the system must detect every instance. De-

spite the challenge, with very limited human inter-

vention, we achieved F-scores of >.65 on 6 of the 

11 relations (average F on the relation set was .58).  

We have also replicated an earlier preliminary 

result (Boschee, 2008) showing that for a micro-

reading task, patterns that utilize seman-

tic/syntactic information outperform patterns that 

make use of only surface strings. Our result covers 

a larger inventory of relation types and attempts to 

provide a more precise measure of recall than the 

earlier preliminary study.  

Analysis of our system’s output provides in-

sights into challenges that such a system may face.  

One challenge for bootstrapping systems is that 

it is easy for the system to learn just a subset of 

relations. We observed this in both sibling where 
we learned the relation brother and for employed

where we only learned patterns for leaders of an 

organization. For sibling human intervention al-

lowed us to correct for this mistake. However for 

employed even with human intervention, our recall 
remains low. The difference between these two 

relations may be that for sibling there are only two 
sub-relations to learn, while there is a rich hie-

rarchy of potential sub-relations under the general 

relation employed. The challenge is quite possibly 
exacerbated by the fact that the distribution of em-

ployment relations in the news is heavily biased 

towards top officials, but our recall test set inten-

tionally does not reflect this skew.  

Another challenge for this approach is continu-

ing to learn in successive iterations. As we saw in 

the figures in Section 7, for many relations perfor-

mance at iteration 20 is not significantly greater 

than performance at iteration 5. Note that seeing 

improvements on the long tail of ways to express a 

relation may require a larger recall set than the test 

set used here. This is exemplified by the existence 

of the highly precise 2-predicate patterns which in 

some cases never fired in our recall test set.  

In future, we wish to address the subset prob-

lem and the problem of stalled improvements. Both 

could potentially be addressed by improved inter-

nal rescoring. For example, the system scoring 

could try to guarantee coverage over the whole 

seed-set thus promoting patterns with low recall, 

but high value for reflecting different information. 

A complementary set of improvements could ex-

plore improved uses of human intervention.  

Acknowledgments 

This work was supported, in part, by BBN under 

AFRL Contract FA8750-09-C-179. 

68



References 
E. Agichtein and L. Gravano. Snowball: extracting rela-

tions from large plain-text collections. In Proceed-

ings of the ACM Conference on Digital Libraries, pp. 

85-94, 2000.  

A. Baron and M. Freedman, “Who is Who and What is 

What: Experiments in Cross Document Co-

Reference”. Empirical Methods in Natural Language 
Processing. 2008.  

A. Blum and T. Mitchell. Combining Labeled and Un-

labeled Data with Co-Training. In Proceedings of the 

1998 Conference on Computational Learning 

Theory, July 1998.  

E. Boschee, V. Punyakanok, R. Weischedel. An Explo-

ratory Study Towards ‘Machines that Learn to Read’. 

Proceedings of AAAI BICA Fall Symposium, No-

vember 2008.  

M.  Collins and Y Singer. Unsupervised Models for 

Named Entity Classification. EMNLP/VLC. (1999). 

M Mintz, S Bills, R Snow, and D Jurafsky.. Distant su-

pervision for relation extraction without labeled data. 

Proceedings of ACL-IJCNLP 200. 2009.. 

T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and 

R. Wang. “Populating the Semantic Web by Macro-

Reading Internet Text. Invited paper, Proceedings of 
the 8th International Semantic Web Conference 

(ISWC 2009).  

P. Pantel and M. Pennacchiotti. Espresso: Leveraging 

Generic Patterns for Automatically Harvesting Se-

mantic Relations. In Proceedings of Conference on 

Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120. 

Sydney, Australia, 2006.  

L. Ramshaw , E. Boschee, S. Bratus, S. Miller, R. 

Stone, R. Weischedel, A. Zamanian, “Experiments in 

multi-modal automatic content extraction”, Proceed-

ings of Human Technology Conference, March 2001.  

D. Ravichandran and E. Hovy. Learning surface text 

patterns for a question answering system. In Pro-

ceedings of the 40th Annual Meeting of the Associa-

tion for Computational Linguistics (ACL 2002), 

pages 41–47, Philadelphia, PA, 2002.  

E. Riloff. Automatically generating extraction patterns 

from untagged text. In Proceedings of the Thirteenth 

National Conference on Artificial Intelligence, pages 

1044-1049, 1996.   

 E. Rilof and Jones, R  "Learning Dictionaries for In-

formation Extraction by Multi-Level Bootstrapping",  

Proceedings of the Sixteenth National Conference on 

Artificial Intelligence (AAAI-99) , 1999, pp. 474-

479. 1999. 

R Snow, D Jurafsky, and A Y. Ng.. Learning syntactic 

patterns for automatic hypernym discovery . Proceed-

ings of NIPS 17. 2005. 

69


