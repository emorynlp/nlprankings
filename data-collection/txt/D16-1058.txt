



















































Attention-based LSTM for Aspect-level Sentiment Classification


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606â€“615,
Austin, Texas, November 1-5, 2016. cÂ©2016 Association for Computational Linguistics

Attention-based LSTM for Aspect-level Sentiment Classification

Yequan Wang and Minlie Huang and Li Zhao* and Xiaoyan Zhu
State Key Laboratory on Intelligent Technology and Systems

Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China

*Microsoft Research Asia
wangyequan@live.cn, aihuang@tsinghua.edu.cn
lizo@microsoft.com, zxy-dcs@tsinghua.edu.cn

Abstract

Aspect-level sentiment classification is a fine-
grained task in sentiment analysis. Since it
provides more complete and in-depth results,
aspect-level sentiment analysis has received
much attention these years. In this paper, we
reveal that the sentiment polarity of a sentence
is not only determined by the content but is
also highly related to the concerned aspect.
For instance, â€œThe appetizers are ok, but the
service is slow.â€, for aspect taste, the polar-
ity is positive while for service, the polarity
is negative. Therefore, it is worthwhile to ex-
plore the connection between an aspect and
the content of a sentence. To this end, we
propose an Attention-based Long Short-Term
Memory Network for aspect-level sentiment
classification. The attention mechanism can
concentrate on different parts of a sentence
when different aspects are taken as input. We
experiment on the SemEval 2014 dataset and
results show that our model achieves state-of-
the-art performance on aspect-level sentiment
classification.

1 Introduction

Sentiment analysis (Nasukawa and Yi, 2003), also
known as opinion mining (Liu, 2012), is a key
NLP task that receives much attention these years.
Aspect-level sentiment analysis is a fine-grained
task that can provide complete and in-depth results.
In this paper, we deal with aspect-level sentiment
classification and we find that the sentiment polar-
ity of a sentence is highly dependent on both con-
tent and aspect. For example, the sentiment polarity

of â€œStaffs are not that friendly, but the taste covers
all.â€ will be positive if the aspect is food but neg-
ative when considering the aspect service. Polarity
could be opposite when different aspects are consid-
ered.

Neural networks have achieved state-of-the-art
performance in a variety of NLP tasks such as ma-
chine translation (Lample et al., 2016), paraphrase
identification (Yin et al., 2015), question answer-
ing (Golub and He, 2016) and text summariza-
tion (Rush et al., 2015). However, neural net-
work models are still in infancy to deal with aspect-
level sentiment classification. In some works, tar-
get dependent sentiment classification can be ben-
efited from taking into account target information,
such as in Target-Dependent LSTM (TD-LSTM)
and Target-Connection LSTM (TC-LSTM) (Tang et
al., 2015a). However, those models can only take
into consideration the target but not aspect informa-
tion which is proved to be crucial for aspect-level
classification.

Attention has become an effective mechanism to
obtain superior results, as demonstrated in image
recognition (Mnih et al., 2014), machine transla-
tion (Bahdanau et al., 2014), reasoning about entail-
ment (RocktaÌˆschel et al., 2015) and sentence sum-
marization (Rush et al., 2015). Even more, neural
attention can improve the ability to read comprehen-
sion (Hermann et al., 2015). In this paper, we pro-
pose an attention mechanism to enforce the model
to attend to the important part of a sentence, in re-
sponse to a specific aspect. We design an aspect-to-
sentence attention mechanism that can concentrate

606



on the key part of a sentence given the aspect.
We explore the potential correlation of aspect and

sentiment polarity in aspect-level sentiment classifi-
cation. In order to capture important information in
response to a given aspect, we design an attention-
based LSTM. We evaluate our approach on a bench-
mark dataset (Pontiki et al., 2014), which contains
restaurants and laptops data.

The main contributions of our work can be sum-
marized as follows:

â€¢ We propose attention-based Long Short-Term
memory for aspect-level sentiment classifica-
tion. The models are able to attend differ-
ent parts of a sentence when different aspects
are concerned. Results show that the attention
mechanism is effective.

â€¢ Since aspect plays a key role in this task, we
propose two ways to take into account aspect
information during attention: one way is to
concatenate the aspect vector into the sentence
hidden representations for computing attention
weights, and another way is to additionally ap-
pend the aspect vector into the input word vec-
tors.

â€¢ Experimental results indicate that our ap-
proach can improve the performance compared
with several baselines, and further examples
demonstrate the attention mechanism works
well for aspect-level sentiment classification.

The rest of our paper is structured as follows:
Section 2 discusses related works, Section 3 gives a
detailed description of our attention-based propos-
als, Section 4 presents extensive experiments to jus-
tify the effectiveness of our proposals, and Section 5
summarizes this work and the future direction.

2 Related Work

In this section, we will review related works on
aspect-level sentiment classification and neural net-
works for sentiment classification briefly.

2.1 Sentiment Classification at Aspect-level

Aspect-level sentiment classification is typically
considered as a classification problem in the liter-

ature. As we mentioned before, aspect-level sen-
timent classification is a fine-grained classification
task. The majority of current approaches attempt to
detecting the polarity of the entire sentence, regard-
less of the entities mentioned or aspects. Traditional
approaches to solve those problems are to manu-
ally design a set of features. With the abundance of
sentiment lexicons (Rao and Ravichandran, 2009;
Perez-Rosas et al., 2012; Kaji and Kitsuregawa,
2007), the lexicon-based features were built for sen-
timent analysis (Mohammad et al., 2013). Most of
these studies focus on building sentiment classifiers
with features, which include bag-of-words and sen-
timent lexicons, using SVM (Mullen and Collier,
2004). However, the results highly depend on the
quality of features. In addition, feature engineering
is labor intensive.

2.2 Sentiment Classification with Neural
Networks

Since a simple and effective approach to learn dis-
tributed representations was proposed (Mikolov et
al., 2013), neural networks advance sentiment anal-
ysis substantially. Classical models including Re-
cursive Neural Network (Socher et al., 2011; Dong
et al., 2014; Qian et al., 2015), Recursive Neu-
ral Tensor Network (Socher et al., 2013), Recur-
rent Neural Network (Mikolov et al., 2010; Tang
et al., 2015b), LSTM (Hochreiter and Schmidhuber,
1997) and Tree-LSTMs (Tai et al., 2015) were ap-
plied into sentiment analysis currently. By utilizing
syntax structures of sentences, tree-based LSTMs
have been proved to be quite effective for many NLP
tasks. However, such methods may suffer from syn-
tax parsing errors which are common in resource-
lacking languages.

LSTM has achieved a great success in various
NLP tasks. TD-LSTM and TC-LSTM (Tang et
al., 2015a), which took target information into con-
sideration, achieved state-of-the-art performance
in target-dependent sentiment classification. TC-
LSTM obtained a target vector by averaging the
vectors of words that the target phrase contains.
However, simply averaging the word embeddings of
a target phrase is not sufficient to represent the se-
mantics of the target phrase, resulting a suboptimal
performance.

607



Despite the effectiveness of those methods, it is
still challenging to discriminate different sentiment
polarities at a fine-grained aspect level. Therefore,
we are motivated to design a powerful neural net-
work which can fully employ aspect information for
sentiment classification.

3 Attention-based LSTM with Aspect
Embedding

3.1 Long Short-term Memory (LSTM)

Recurrent Neural Network(RNN) is an extension of
conventional feed-forward neural network. How-
ever, standard RNN has the gradient vanishing
or exploding problems. In order to overcome
the issues, Long Short-term Memory network
(LSTM) was developed and achieved superior per-
formance (Hochreiter and Schmidhuber, 1997). In
the LSTM architecture, there are three gates and a
cell memory state. Figure 1 illustrates the architec-
ture of a standard LSTM.

LSTM LSTM LSTMâ€¦

softmax

ğ‘¤1 ğ‘¤2 ğ‘¤ğ‘

â„1 â„2 â„ğ‘

Figure 1: The architecture of a standard LSTM.
{w1, w2, . . . , wN} represent the word vector in a sen-
tence whose length is N . {h1, h2, . . . , hN} is the hidden
vector.

More formally, each cell in LSTM can be com-
puted as follows:

X =

[
htâˆ’1
xt

]
(1)

ft = Ïƒ(Wf Â· X + bf ) (2)
it = Ïƒ(Wi Â· X + bi) (3)
ot = Ïƒ(Wo Â· X + bo) (4)
ct = ft âŠ™ ctâˆ’1 + it âŠ™ tanh(Wc Â· X + bc) (5)
ht = ot âŠ™ tanh(ct) (6)

where Wi,Wf ,Wo âˆˆ RdÃ—2d are the weighted ma-
trices and bi, bf , bo âˆˆ Rd are biases of LSTM to be
learned during training, parameterizing the transfor-
mations of the input, forget and output gates respec-
tively. Ïƒ is the sigmoid function and âŠ™ stands for
element-wise multiplication. xt includes the inputs
of LSTM cell unit, representing the word embed-
ding vectors wt in Figure 1. The vector of hidden
layer is ht.

We regard the last hidden vector hN as the rep-
resentation of sentence and put hN into a softmax
layer after linearizing it into a vector whose length is
equal to the number of class labels. In our work, the
set of class labels is {positive, negative, neutral}.

3.2 LSTM with Aspect Embedding
(AE-LSTM)

Aspect information is vital when classifying the po-
larity of one sentence given aspect. We may get op-
posite polarities if different aspects are considered.
To make the best use of aspect information, we pro-
pose to learn an embedding vector for each aspect.

Vector vai âˆˆ Rda is represented for the embed-
ding of aspect i, where da is the dimension of aspect
embedding. A âˆˆ RdaÃ—|A| is made up of all aspect
embeddings. To the best of our knowledge, it is the
first time to propose aspect embedding.

3.3 Attention-based LSTM (AT-LSTM)

The standard LSTM cannot detect which is the im-
portant part for aspect-level sentiment classification.
In order to address this issue, we propose to de-
sign an attention mechanism that can capture the
key part of sentence in response to a given aspect.
Figure 2 represents the architecture of an Attention-
based LSTM (AT-LSTM).

Let H âˆˆ RdÃ—N be a matrix consisting of hid-
den vectors [h1, . . . , hN ] that the LSTM produced,
where d is the size of hidden layers and N is the
length of the given sentence. Furthermore, va rep-
resents the embedding of aspect and eN âˆˆ RN is a
vector of 1s. The attention mechanism will produce
an attention weight vector Î± and a weighted hidden

608



LSTM LSTM LSTM

Word Representation

â€¦

Attention

Aspect Embedding

H

ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘

â„2 â„3 â„ğ‘

ğ‘£ğ‘ ğ‘£ğ‘ ğ‘£ğ‘

LSTM

ğ‘¤1

â„1

ğ‘£ğ‘

r

ğ›¼

Figure 2: The Architecture of Attention-based LSTM. The aspect embeddings have been used to decide the attention weights
along with the sentence representations. {w1, w2, . . . , wN} represent the word vector in a sentence whose length is N . va
represents the aspect embedding. Î± is the attention weight. {h1, h2, . . . , hN} is the hidden vector.

representation r.

M = tanh(

[
WhH

Wvva âŠ— eN

]
) (7)

Î± = softmax(wT M) (8)

r = HÎ±T (9)

where, M âˆˆ R(d+da)Ã—N , Î± âˆˆ RN , r âˆˆ Rd.
Wh âˆˆ RdÃ—d, Wv âˆˆ RdaÃ—da and w âˆˆ Rd+da are
projection parameters. Î± is a vector consisting of
attention weights and r is a weighted representation
of sentence with given aspect. The operator in 7 (a
circle with a multiplication sign inside, OP for short
here) means: vaâŠ—eN = [v; v; . . . ; v], that is, the op-
erator repeatedly concatenates v for N times, where
eN is a column vector with N 1s. Wvva âŠ— eN is
repeating the linearly transformed va as many times
as there are words in sentence.

The final sentence representation is given by:

hâˆ— = tanh(Wpr + WxhN ) (10)

where, hâˆ— âˆˆ Rd, Wp and Wx are projection param-
eters to be learned during training. We find that this
works practically better if we add WxhN into the fi-
nal representation of the sentence, which is inspired
by (RocktaÌˆschel et al., 2015).

The attention mechanism allows the model to
capture the most important part of a sentence when
different aspects are considered.

hâˆ— is considered as the feature representation of
a sentence given an input aspect. We add a linear
layer to convert sentence vector to e, which is a real-
valued vector with the length equal to class number
|C|. Then, a softmax layer is followed to trans-
form e to conditional probability distribution.

y = softmax(Wsh
âˆ— + bs) (11)

where Ws and bs are the parameters for softmax
layer.

3.4 Attention-based LSTM with Aspect
Embedding (ATAE-LSTM)

The way of using aspect information in AE-LSTM
is letting aspect embedding play a role in com-
puting the attention weight. In order to better
take advantage of aspect information, we append
the input aspect embedding into each word input
vector. The structure of this model is illustrated
in 3. In this way, the output hidden representa-
tions (h1, h2, ..., hN ) can have the information from
the input aspect (va). Therefore, in the following
step that compute the attention weights, the inter-

609



LSTM LSTM LSTM

Word Representation

â€¦

Attention

Aspect Embedding

H

ğ‘¤2 ğ‘¤3 ğ‘¤ğ‘

â„2 â„3 â„ğ‘

ğ‘£ğ‘ ğ‘£ğ‘ ğ‘£ğ‘

LSTM

ğ‘¤1

â„1

ğ‘£ğ‘

ğ‘£ğ‘ ğ‘£ğ‘ ğ‘£ğ‘ğ‘£ğ‘
Aspect Embedding

ğ‘Ÿ

ğ›¼

Figure 3: The Architecture of Attention-based LSTM with Aspect Embedding. The aspect embeddings have been take as input
along with the word embeddings. {w1, w2, . . . , wN} represent the word vector in a sentence whose length is N . va represents the
aspect embedding. Î± is the attention weight. {h1, h2, . . . , hN} is the hidden vector.

dependence between words and the input aspect can
be modeled.

3.5 Model Training

The model can be trained in an end-to-end way by
backpropagation, where the objective function (loss
function) is the cross-entropy loss. Let y be the tar-
get distribution for sentence, yÌ‚ be the predicted sen-
timent distribution. The goal of training is to mini-
mize the cross-entropy error between y and yÌ‚ for all
sentences.

loss = âˆ’
âˆ‘

i

âˆ‘

j

yji logyÌ‚
j
i + Î»||Î¸||2 (12)

where i is the index of sentence, j is the index of
class. Our classification is three way. Î» is the L2 -
regularization term. Î¸ is the parameter set.

Similar to standard LSTM, the parameter set
is {Wi, bi,Wf , bf ,Wo, bo,Wc, bc,Ws, bs}. Fur-
thermore, word embeddings are the parameters
too. Note that the dimension of Wi, Wf ,Wo,Wc
changes along with different models. If the aspect
embeddings are added into the input of the LSTM

cell unit, the dimension of Wi, Wf ,Wo,Wc will be
enlarged correspondingly. Additional parameters
are listed as follows:

AT-LSTM: The aspect embedding A is added
into the set of parameters naturally. In addition,
Wh,Wv,Wp,Wx, w are the parameters of atten-
tion. Therefore, the additional parameter set of AT-
LSTM is {A,Wh,Wv,Wp,Wx, w}.

AE-LSTM: The parameters include the as-
pect embedding A. Besides, the dimension of
Wi, Wf ,Wo,Wc will be expanded since the aspect
vector is concatenated. Therefore, the additional pa-
rameter set consists of {A}.

ATAE-LSTM: The parameter set consists of
{A, Wh,Wv, Wp,Wx, w}. Additionally, the dimen-
sion of Wi,Wf ,Wo,Wc will be expanded with the
concatenation of aspect embedding.

The word embedding and aspect embedding are
optimized during training. The percentage of out-
of-vocabulary words is about 5%, and they are ran-
domly initialized from U(âˆ’Ïµ, Ïµ), where Ïµ = 0.01.

In our experiments, we use AdaGrad (Duchi et
al., 2011) as our optimization method, which has

610



improved the robustness of SGD on large scale
learning task remarkably in a distributed environ-
ment (Dean et al., 2012). AdaGrad adapts the learn-
ing rate to the parameters, performing larger updates
for infrequent parameters and smaller updates for
frequent parameters.

4 Experiment

We apply the proposed model to aspect-level sen-
timent classification. In our experiments, all word
vectors are initialized by Glove1 (Pennington et al.,
2014). The word embedding vectors are pre-trained
on an unlabeled corpus whose size is about 840 bil-
lion. The other parameters are initialized by sam-
pling from a uniform distribution U(âˆ’Ïµ, Ïµ). The
dimension of word vectors, aspect embeddings and
the size of hidden layer are 300. The length of at-
tention weights is the same as the length of sentence.
Theano (Bastien et al., 2012) is used for implement-
ing our neural network models. We trained all mod-
els with a batch size of 25 examples, and a momen-
tum of 0.9, L2-regularization weight of 0.001 and
initial learning rate of 0.01 for AdaGrad.

4.1 Dataset
We experiment on the dataset of SemEval 2014 Task
42 (Pontiki et al., 2014). The dataset consists of
customers reviews. Each review contains a list of
aspects and corresponding polarities. Our aim is to
identify the aspect polarity of a sentence with the
corresponding aspect. The statistics is presented in
Table 1.

4.2 Task Definition
Aspect-level Classification Given a set of pre-
identified aspects, this task is to determine the
polarity of each aspect. For example, given a
sentence, â€œThe restaurant was too expensive.â€,
there is an aspect price whose polarity is negative.
The set of aspects is {food, price, service, ambi-
ence, anecdotes/miscellaneous}. In the dataset of
SemEval 2014 Task 4, there is only restaurants
data that has aspect-specific polarities. Table 2

1Pre-trained word vectors of Glove can be obtained from
http://nlp.stanford.edu/projects/glove/

2The introduction about SemEval 2014 can be obtained
from http://alt.qcri.org/semeval2014/

Asp.
Positive Negative Neural

Train Test Train Test Train Test
Fo. 867 302 209 69 90 31
Pr. 179 51 115 28 10 1
Se. 324 101 218 63 20 3
Am. 263 76 98 21 23 8
An. 546 127 199 41 357 51
Total 2179 657 839 222 500 94

Table 1: Aspects distribution per sentiment class. {Fo., Pr.,
Se, Am., An.} refer to {food, price, service, ambience, anec-
dotes/miscellaneous}. â€œAsp.â€ refers to aspect.

Models Three-way Pos./Neg.
LSTM 82.0 88.3
TD-LSTM 82.6 89.1
TC-LSTM 81.9 89.2
AE-LSTM 82.5 88.9
AT-LSTM 83.1 89.6
ATAE-LSTM 84.0 89.9

Table 2: Accuracy on aspect level polarity classification about
restaurants. Three-way stands for 3-class prediction. Pos./Neg.

indicates binary prediction where ignoring all neutral instances.

Best scores are in bold.

illustrates the comparative results.

Aspect-Term-level Classification For a given set
of aspects term within a sentence, this task is to de-
termine whether the polarity of each aspect term is
positive, negative or neutral. We conduct experi-
ments on the dataset of SemEval 2014 Task 4. In
the sentences of both restaurant and laptop datasets,
there are the location and sentiment polarity for
each occurrence of an aspect term. For example,
there is an aspect term fajitas whose polarity is neg-
ative in sentence â€œI loved their fajitas.â€.

Experiments results are shown in Table 3 and Ta-
ble 4. Similar to the experiment on aspect-level
classification, our models achieve state-of-the-art
performance.

4.3 Comparison with baseline methods

We compare our model with several baselines, in-
cluding LSTM, TD-LSTM, and TC-LSTM.

LSTM: Standard LSTM cannot capture any as-
pect information in sentence, so it must get the same

611



ğœ¶

(a) the aspect of this sentence: service

ğœ¶

(b) the aspect of this sentence: food

Figure 4: Attention Visualizations. The aspects of (a) and (b) are service and food respectively. The color depth expresses the
importance degree of the weight in attention vector Î±. From (a), attention can detect the important words from the whole sentence

dynamically even though multi-semantic phrase such as â€œfastest delivery timesâ€ which can be used in other areas. From (b),

attention can know multi-keypoints if more than one keypoint existing.

Models Three-way Pos./Neg.
LSTM 74.3 -
TD-LSTM 75.6 -
AE-LSTM 76.6 89.6
ATAE-LSTM 77.2 90.9

Table 3: Accuracy on aspect term polarity classification about
restaurants. Three-way stands for 3-class prediction. Pos./Neg.

indicates binary prediction where ignoring all neutral instances.

Best scores are in bold.

Models Three-way Pos./Neg.
LSTM 66.5 -
TD-LSTM 68.1 -
AE-LSTM 68.9 87.4
ATAE-LSTM 68.7 87.6

Table 4: Accuracy on aspect term polarity classification about
laptops. Three-way stands for 3-class prediction. Pos./Neg. in-

dicates binary prediction where ignoring all neutral instances.

Best scores are in bold.

sentiment polarity although given different aspects.
Since it cannot take advantage of the aspect infor-
mation, not surprisingly the model has worst per-
formance.

TD-LSTM: TD-LSTM can improve the perfor-
mance of sentiment classifier by treating an aspect
as a target. Since there is no attention mechanism in

TD-LSTM, it cannot â€œknowâ€ which words are im-
portant for a given aspect.

TC-LSTM: TC-LSTM extended TD-LSTM by
incorporating a target into the representation of a
sentence. It is worth noting that TC-LSTM per-
forms worse than LSTM and TD-LSTM in Table 2.
TC-LSTM added target representations, which was
obtained from word vectors, into the input of the
LSTM cell unit.

In our models, we embed aspects into another
vector space. The embedding vector of aspects can
be learned well in the process of training. ATAE-
LSTM not only addresses the shortcoming of the
unconformity between word vectors and aspect em-
beddings, but also can capture the most important
information in response to a given aspect. In ad-
dition, ATAE-LSTM can capture the important and
different parts of a sentence when given different
aspects.

4.4 Qualitative Analysis
It is enlightening to analyze which words decide the
sentiment polarity of the sentence given an aspect.
We can obtain the attention weight Î± in Equation 8
and visualize the attention weights accordingly.

Figure 4 shows the representation of how atten-
tion focuses on words with the influence of a given
aspect. We use a visualization tool Heml (Deng

612



The   appetizers   are   ok,   but   the   service   is   slow.

I  highly  recommend  it  for  not  just  its  superb  cuisine,  but  also  for  its  friendly  owners  and  staff.

The  service,  however,  is  a  peg  or  two  below  the  quality  of  food  (horrible  bartenders),  and  

the  clientele,  for  the  most  part,  are  rowdy,  loud-mouthed  commuters  (this  could  explain  the  

bad  attitudes  from  the  staff)  getting  loaded  for  an  AC/DC  concert  or  a  Knicks  game.

aspect: service; polarity: negativeaspect: food; polarity: neutral

(a)

aspect: food; polarity: positive aspect: food; polarity: positive

(b)

aspect: food; polarity: positive aspect: service; polarity: positive aspect: ambience; polarity: negative

(c)

Figure 5: Examples of classification. (a) is an instance with different aspects. (b) represents that our model can focus on where
the keypoints are and not disturbed by the privative word not. (c) stands for long and complicated sentences. Our model can obtain

correct sentiment polarity.

et al., 2014) to visualize the sentences. The color
depth indicates the importance degree of the weight
in attention vector Î±, the darker the more important.
The sentences in Figure 4 are â€œI have to say they
have one of the fastest delivery times in the city .â€
and â€œThe fajita we tried was tasteless and burned
and the mole sauce was way too sweet.â€. The corre-
sponding aspects are service and food respectively.
Obviously attention can get the important parts from
the whole sentence dynamically. In Figure 4 (a),
â€œfastest delivery timesâ€ is a multi-word phrase, but
our attention-based model can detect such phrases
if service can is the input aspect. Besides, the atten-
tion can detect multiple keywords if more than one
keyword is existing. In Figure 4 (b), tastless and too
sweet are both detected.

4.5 Case Study

As we demonstrated, our models obtain the state-of-
the-art performance. In this section, we will further
show the advantages of our proposals through some
typical examples.

In Figure 5, we list some examples from the test
set which have typical characteristics and cannot be
inferred by LSTM. In sentence (a), â€œThe appetiz-
ers are ok, but the service is slow.â€, there are two

aspects food and service. Our model can discrimi-
nate different sentiment polarities with different as-
pects. In sentence (b), â€œI highly recommend it for
not just its superb cuisine, but also for its friendly
owners and staff.â€, there is a negation word not. Our
model can obtain correct polarity, not affected by
the negation word who doesnâ€™t represent negation
here. In the last instance (c), â€œThe service, however,
is a peg or two below the quality of food (horri-
ble bartenders), and the clientele, for the most part,
are rowdy, loud-mouthed commuters (this could ex-
plain the bad attitudes from the staff) getting loaded
for an AC/DC concert or a Knicks game.â€, the sen-
tence has a long and complicated structure so that
existing parser may hardly obtain correct parsing
trees. Hence, tree-based neural network models
are difficult to predict polarity correctly. While our
attention-based LSTM can work well in those sen-
tences with the help of attention mechanism and as-
pect embedding.

5 Conclusion and Future Work

In this paper, we have proposed attention-based
LSTMs for aspect-level sentiment classification.
The key idea of these proposals are to learn aspect

613



embeddings and let aspects participate in computing
attention weights. Our proposed models can con-
centrate on different parts of a sentence when dif-
ferent aspects are given so that they are more com-
petitive for aspect-level classification. Experiments
show that our proposed models, AE-LSTM and
ATAE-LSTM, obtain superior performance over the
baseline models.

Though the proposals have shown potentials for
aspect-level sentiment analysis, different aspects are
input separately. As future work, an interesting
and possible direction would be to model more than
one aspect simultaneously with the attention mech-
anism.

Acknowledgments

This work was partly supported by the National
Basic Research Program (973 Program) under
grant No.2012CB316301/2013CB329403, the Na-
tional Science Foundation of China under grant
No.61272227/61332007, and the Beijing Higher
Education Young Elite Teacher Project. The work
was also supported by Tsinghua University Beijing
Samsung Telecom R&D Center Joint Laboratory for
Intelligent Media Computing.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

FreÌdeÌric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian Goodfellow, Arnaud Bergeron,
Nicolas Bouchard, David Warde-Farley, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. arXiv preprint arXiv:1211.5590.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,
Matthieu Devin, Mark Mao, Andrew Senior, Paul
Tucker, Ke Yang, Quoc V Le, et al. 2012. Large scale
distributed deep networks. In Advances in Neural
Information Processing Systems, pages 1223â€“1231.

Wankun Deng, Yongbo Wang, Zexian Liu, Han Cheng,
and Yu Xue. 2014. Hemi: a toolkit for illustrating
heatmaps. PloS one, 9(11):e111988.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment classi-
fication. In ACL (2), pages 49â€“54.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121â€“2159.

David Golub and Xiaodong He. 2016. Character-level
question answering with attention. arXiv preprint
arXiv:1604.00727.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In Advances in Neural Information
Processing Systems, pages 1684â€“1692.

Sepp Hochreiter and JuÌˆrgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735â€“
1780.

Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building
lexicon for sentiment analysis from massive collection
of html documents. In EMNLP-CoNLL, pages 1075â€“
1083.

Guillaume Lample, Miguel Ballesteros, Sandeep Subra-
manian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Bing Liu. 2012. Sentiment analysis and opinion mining.
Synthesis lectures on human language technologies,
5(1):1â€“167.

Tomas Mikolov, Martin KarafiaÌt, Lukas Burget, Jan
CernockyÌ€, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model. In
INTERSPEECH, volume 2, page 3.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111â€“3119.

Volodymyr Mnih, Nicolas Heess, Alex Graves, et al.
2014. Recurrent models of visual attention. In
Advances in Neural Information Processing Systems,
pages 2204â€“2212.

Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. arXiv preprint
arXiv:1308.6242.

Tony Mullen and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse infor-
mation sources. In EMNLP, volume 4, pages 412â€“
418.

Tetsuya Nasukawa and Jeonghee Yi. 2003. Sen-
timent analysis: Capturing favorability using natu-
ral language processing. In Proceedings of the 2nd
international conference on Knowledge capture, pages
70â€“77. ACM.

614



Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of the Empiricial Methods
in Natural Language Processing (EMNLP 2014),
12:1532â€“1543.

Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in spanish. In
LREC, volume 12, page 73.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Ion Androutsopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the
8th international workshop on semantic evaluation
(SemEval 2014), pages 27â€“35.

Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan
Zhu, and Xiaoyan Zhu. 2015. Learning tag embed-
dings and tag-specific composition functions in re-
cursive neural network. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing, volume 1, pages
1365â€“1374.

Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, pages
675â€“682. Association for Computational Linguistics.

Tim RocktaÌˆschel, Edward Grefenstette, Karl Moritz Her-
mann, TomaÌsÌŒ KocÌŒiskyÌ€, and Phil Blunsom. 2015. Rea-
soning about entailment with neural attention. arXiv
preprint arXiv:1509.06664.

Alexander M Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization. arXiv preprint
arXiv:1509.00685.

Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 151â€“161. Association for
Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In EMNLP, volume 1631, page 1642. Citeseer.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting
Liu. 2015a. Target-dependent sentiment classifica-

tion with long short term memory. arXiv preprint
arXiv:1512.01100.

Duyu Tang, Bing Qin, and Ting Liu. 2015b. Docu-
ment modeling with gated recurrent neural network
for sentiment classification. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1422â€“1432.

Wenpeng Yin, Hinrich SchuÌˆtze, Bing Xiang, and Bowen
Zhou. 2015. Abcnn: Attention-based convolutional
neural network for modeling sentence pairs. arXiv
preprint arXiv:1512.05193.

615


