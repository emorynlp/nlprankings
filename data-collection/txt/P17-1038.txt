



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 409–419
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1038

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 409–419
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1038

Automatically Labeled Data Generation for Large Scale Event Extraction

Yubo Chen1,2, Shulin Liu1,2, Xiang Zhang1, Kang Liu1 and Jun Zhao1,2
1 National Laboratory of Pattern Recognition, Institute of Automation,

Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China
{yubo.chen, shulin.liu, xiang.zhang, kliu, jzhao}@nlpr.ia.ac.cn

Abstract

Modern models of event extraction for
tasks like ACE are based on supervised
learning of events from small hand-labeled
data. However, hand-labeled training da-
ta is expensive to produce, in low cov-
erage of event types, and limited in size,
which makes supervised methods hard to
extract large scale of events for knowledge
base population. To solve the data label-
ing problem, we propose to automatically
label training data for event extraction vi-
a world knowledge and linguistic knowl-
edge, which can detect key arguments and
trigger words for each event type and em-
ploy them to label events in texts auto-
matically. The experimental results show
that the quality of our large scale automat-
ically labeled data is competitive with e-
laborately human-labeled data. And our
automatically labeled data can incorporate
with human-labeled data, then improve the
performance of models learned from these
data.

1 Introduction

Event Extraction (EE), a challenging task in In-
formation Extraction, aims at detecting and typ-
ing events (Event Detection), and extracting ar-
guments with different roles (Argument Identifi-
cation) from natural-language texts. For exam-
ple, in the sentence shown in Figure 1, an EE
system is expected to identify an Attack even-
t triggered by threw and extract the correspond-
ing five augments with different roles: Yesterday
(Role=Time), demonstrators (Role=Attacker), s-
tones (Role=Instrument), soldiers (Role=Target),
and Israeli (Role=Place).

To this end, so far most methods (Nguyen et al.,

In Baghdad ,   a cameraman died when   an American tank fired on   the Palestine  Hotel.

prep_in

det nsubj nsubj

advcl
advmod

det
amod

prep_on

det
nn

In Baghdad ,   a cameraman died when   an American tank fired on   the Palestine  Hotel.

Victim

Place InstrumentDie Attack Target

Target
Place

Michelle Obama and Barack Obama were on October 3, 1992.MarryPerson Person Time

Marry

Victim

Place
Instrument

Die Attack Target

Target

Place

In Baghdad ,   a cameraman died when   an American tank fired on   the Palestine  Hotel.

Victim

Place InstrumentDie Attack Target

Target
Place

Figure 1: This sentence expresses an Attack event
triggered by threw and containing five arguments.

2016; Chen et al., 2015; Li et al., 2014; Hong et al.,
2011; Ji and Grishman, 2008) usually adopted su-
pervised learning paradigm which relies on elab-
orate human-annotated data, such as ACE 20051,
to train extractors. Although this paradigm was
widely studied, existing approaches still suffer
from high costs for manually labeling training da-
ta and low coverage of predefined event types. In
ACE 2005, all 33 event types are manually pre-
defined and the corresponding event information
(including triggers, event types, arguments and
their roles) are manually annotated only in 599
English documents since the annotation process
is extremely expensive. As Figure 2 shown, n-
early 60% of event types in ACE 2005 have less
than 100 labeled samples and there are even three
event types which have less than ten labeled sam-
ples. Moreover, those predefined 33 event types
are in low coverage for Natural Language Process-
ing (NLP) applications on large-scale data.

Therefore, for extracting large scale events, e-
specially in open domain scenarios, how to auto-
matically and efficiently generate sufficient train-
ing data is an important problem. This paper aim-
s to automatically generate training data for EE,
which involves labeling triggers, event types, ar-
guments and their roles. Figure 1 shows an ex-
ample of labeled sentence. Recent improvements
of Distant Supervision (DS) have been proven to
be effective to label training data for Relation Ex-
traction (RE), which aims to predict semantic re-

1http://projects.ldc.upenn.edu/ace/

409

https://doi.org/10.18653/v1/P17-1038
https://doi.org/10.18653/v1/P17-1038


0

200

400

600

800

1000

1200

1400

1600

Figure 2: Statistics of ACE 2005 English Data.

lations between pairs of entities, formulated as
(entity1, relation, entity2). And DS for RE as-
sumes that if two entities have a relationship in
a known knowledge base, then all sentences that
mention these two entities will express that rela-
tionship in some way (Mintz et al., 2009). How-
ever, when we use DS for RE to EE, we meet fol-
lowing challenges:

Triggers are not given out in existing knowl-
edge bases. EE aims to detect an event instance
of a specific type and extract their arguments and
roles, formulated as (event instance, event type;
role1, argument1; role2, argument2; ...; rolen,
argumentn), which can be regarded as a kind
of multiple or complicated relational data. In
Figure 3, the right part shows an example of
spouse of relation between Barack Obama and
Michelle Obama, where two rectangles repre-
sent two entities and the edge connecting them
represents their relation. DS for RE uses t-
wo entities to automatically label training da-
ta; In comparison, the left part in Figure 3
shows a marriage event of Barack Obama
and Michelle Obama, where the dash cir-
cle represents the marriage event instance of
Barack Obama and Michelle Obama, rectan-
gles represent arguments of the event instance, and
each edge connecting an argument and the event
instance expresses the role of the argument. For
example, Barack Obama plays a Spouse role
in this marriage event instance. It seems that we
could use an event instance and an argument to
automatically generate training data for argumen-
t identification just like DS for RE. However, an
event instance is a virtual node in existing knowl-
edge bases and mentioned implicitly in texts. For
example, in Freebase, the aforementioned mar-
riage event instance is represented as m.02nqglv
(see details in Section 2). Thus we cannot direct-
ly use an event instance and an argument, like
m.02nqglv and Barack Obama, to label back

MarriageMichelle Obama

10/03/1992

Trinity United Church of Christ

Null

Spouse Spouse

location of    ceremony

time_from time_to

Barack Obama Michelle ObamaSpouse_ofVS
An example of marriage event An example of spouse_of relation

MarriageMichelle Obama

10/03/1992

Trinity United Church of Christ

Null

Spouse Spouse

location of    ceremony

time_from time_to

Barack Obama Michelle ObamaSpouse_of
An example of marriage event An example of spouse_of relation

MarriageMichelle Obama

10/03/1992

Trinity United Church of Christ

Null

Spouse Spouse

location of    ceremony

time_from time_to

Barack Obama
Michelle Obama

Spouse_of

An example of marriage event An example of spouse_of relation

Michelle Obama

Figure 3: A comparison of events and relations.

in sentences. In ACE event extraction program,
an event instance is represented as a trigger word,
which is the main word that most clearly repre-
sents an event occurrence in sentences, like threw
in Figure 1. Following ACE, we can use trig-
ger words to represent event instance, like married
for people.marriage event instance. Unfortunate-
ly, triggers are not given out in existing knowledge
bases.

To resolve the trigger missing problem men-
tioned above, we need to discover trigger words
before employing distant supervision to automati-
cally label event arguments. Following DS in RE,
we could naturally assume that a sentence con-
tains all arguments of an event in the knowledge
base tend to express that event, and the verbs oc-
cur in these sentences tend to evoke this type of
events. However, arguments for a specific event
instance are usually mentioned in multiple sen-
tences. Simply employing all arguments in the
knowledge base to label back in sentences will
generate few sentences as training samples. As
shown in Table 1, only 0.02% of instances can find
all argument mentions in one sentence.

Event Type EI# A# S#
education.education 530,538 8 0
film.film crew gig 252,948 3 8
people.marriage 152,276 5 0

... ... ... ...
military.military service 27,933 6 0

olympics.olympic medal honor 20,790 5 4
sum of the selected 21 events 3,870,492 100 798

Table 1: Statistics of events in Freebase. EI# de-
notes number of event instances in Freebase. A#
denotes number of arguments for each event type-
s, and S# indicates number of sentences contain all
arguments of each event type in Wikipedia.

To solve above problems, we propose an ap-
proach to automatically generate labeled data for
large scale EE by jointly using world knowledge
(Freebase) and linguistic knowledge (FrameNet).
At first, we put forward an approach to prioritize

410



arguments and select key or representative argu-
ments (see details in Section 3.1) for each event
type by using Freebase; Secondly, we merely use
key arguments to label events and figure out trig-
ger words; Thirdly, an external linguistic knowl-
edge resource, FrameNet, is employed to filter
noisy trigger words and expand more triggers; Af-
ter that, we propose a Soft Distant Supervision
(SDS) for EE to automatically label training da-
ta, which assumes that any sentence containing al-
l key arguments in Freebase and a corresponding
trigger word is likely to express that event in some
way, and arguments occurring in that sentence are
likely to play the corresponding roles in that event.
Finally, we evaluate the quality of the automatical-
ly labeled training data by both manual and auto-
matic evaluations. In addition, we employ a CNN-
based EE approach with multi-instance learning
for the automatically labeled data as a baseline for
further research on this data. In summary, the con-
tributions of this paper are as follows:

• To our knowledge, it is the first work to au-
tomatically label data for large scale EE via
world knowledge and linguistic knowledge.
All the labeled data in this paper have been
released and can be downloaded freely2.

• We propose an approach to figure out key ar-
guments of an event by using Freebase, and
use them to automatically detect events and
corresponding trigger words. Moreover, we
employ FrameNet to filter noisy triggers and
expand more triggers.

• The experimental results show that the qual-
ity of our large scale automatically labeled
data is competitive with elaborately human-
annotated data. Also, our automatically la-
beled data can augment traditional human-
annotated data, which could significantly im-
prove the extraction performance.

2 Background

In this paper, we respectively use Freebase as our
world knowledge containing event instance and
FrameNet as the linguistic knowledge containing
trigger information. The articles in Wikipedia are
used as unstructured texts to be labeled. To under-
stand our method easily, we first introduce them as
follows:

2https://github.com/acl2017submission/event-data

Freebase is a semantic knowledge base (Bol-
lacker et al., 2008), which makes use of medi-
ators (also called compound value types, CVT-
s) to merge multiple values into a single value.
As shown in Figure 3, people.marriage is one
type of CVTs. There are many instances of peo-
ple.marriage and the marriage of Barack Obama
and Michelle Obama is numbered as m.02nqglv.
Spouse, from, to and location of ceremony are
roles of the people.marriage CVTs. Barack Oba-
ma, Michelle Obama, 10/3/1992 and Trinity U-
nited Church of Christ are the values of the in-
stances. In this paper, we regard these CVTs as
events, type of CVTs as event type, CVT instances
as event instances, values in CVTs as arguments
in events and roles of CVTs as the roles of ar-
guments play in the event, respectively. Accord-
ing to the statistics of the Freebase released on
23th April, 2015, there are around 1885 CVTs and
around 14 million CVTs instances. After filtering
out useless and meaningless CVTs, such as CVT-
s about user profiles and website information, we
select 21 types of CVTs with around 3.8 million
instances for experiments, which mainly involves
events about education, military, sports and so on.

FrameNet3 is a linguistic resource storing in-
formation about lexical and predicate argument se-
mantics (Baker et al., 1998). FrameNet contains
more than 1, 000 frames and 10, 000 Lexical Unit-
s (LUs). Each frame of FrameNet can be taken as
a semantic frame of a type of events (Liu et al.,
2016). Each frame has a set of lemmas with part
of speech tags that can evoke the frame, which are
called LUs. For example, appoint.v is a LU of Ap-
pointing frame in FrameNet, which can be mapped
to people.appointment events in Freebase. And a
LUs of the frame plays a similar role as the trig-
ger of an event. Thus we use FrameNet to detect
triggers in our automatically data labeling process.

Wikipedia4 that we used was released on Jan-
uary, 2016. All 6.3 million articles in it are used in
our experiments. We use Wikipedia because it is
relatively up-to-date, and much of the information
in Freebase is derived from Wikipedia.

3 Method of Generating Training Data

Figure 4 describes the architecture of automati-
cally labeling data, which primarily involves the
following four components: (i) Key argument de-

3http://framenet.icsi.berkeley.edu
4https://www.wikipedia.org/

411



Wikipedia

TrainingDS

Figure 4: The architecture of automatically label-
ing training data for large scale event extraction.

tection, which prioritizes arguments of each even-
t type and selects key arguments for each type
of event; (ii) Trigger word detection, which uses
key arguments to label sentences that may express
events preliminarily, and then detect triggers; (iii)
Trigger word filtering and expansion, which uses
FrameNet to filter noisy triggers and expand trig-
gers; (iv) Automatically labeled data generation,
which uses a SDS to label events in sentences.

3.1 Key Argument Detection
This section illustrates how to detect key argu-
ments for each event type via Freebase. Intuitively,
arguments of a type of event play different roles.
Some arguments play indispensable roles in an
event, and serve as vital clues when distinguishing
different events. For example, compared with ar-
guments like time, location and so on, spouses are
key arguments in a marriage event. We call these
arguments as key arguments. We propose to use
Key Rate (KR) to estimate the importance of an
argument to a type of event, which is decided by
two factors: Role Saliency and Event Relevance.

Role Saliency (RS) reflects the saliency of an
argument to represent a specific event instance of
a given event type. If we tend to use an argumen-
t to distinguish one event instance form other in-
stances of a given event type, this argument will
play a salient role in the given event type. We de-
fine RS as follows:

RSij =
Count(Ai, ETj)

Count(ETj)
(1)

where RSij is the role saliency of i-th argument
to j-th event type, Count(Ai , ETj) is the num-
ber of Arguemnti occurring in all instances of
eventTypej in Freebase and Count(ETj) is the
number of instances of eventTypej in Freebase.

Event Relevance (ER) reflects the ability in
which an argument can be used to discriminate d-

ifferent event types. If an argument occurs in ev-
ery event type, the argument will have a low event
relevance. We propose to compute ER as follows:

ERi = log
Sum(ET )

1 + Count(ETCi)
(2)

where ERi is the event relevance of i-th argumen-
t, Sum (ET ) is the number of all event types in
knowledge base and Count(ETCi) is the number
of event types containing i-th argument. Finally,
KR is computed as follows:

KRij = RSij ∗ ERi (3)
We compute KR for all arguments of each even-

t type, and sort them according to KR. Then we
choose top K arguments as key arguments.

3.2 Trigger Word Detection

After detecting key arguments for every even-
t types, we use these key arguments to label sen-
tences that may express events in Wikipedia. At
first, we use Standford CoreNLP tool5 to converts
the raw Wikipedia texts into a sequence of sen-
tences, attaches NLP annotations (POS tag, NER
tag). Finally, we select sentences that contains all
key arguments of an event instance in Freebase as
sentences expressing corresponding events. Then
we use these labeled sentences to detect triggers.

In a sentence, a verb tend to express an occur-
rence of an event. For example, in ACE 2005 En-
glish data, there are 60% of events triggered by
verbs. As shown in Figure 1, threw is a trigger
of Attack event. Intuitively, if a verb occurs more
times than other verbs in the labeled sentences of
one event type , the verb tends to trigger this type
of event; and if a verb occurs in sentences of ev-
ery event types, like is, the verb will have a low
probability to trigger events. Thus we propose
Trigger Candidate Frequency (TCF) and Trigger
Event Type Frequency (TETF) to evaluate above t-
wo aspects. Finally we employ Trigger Rate (TR),
which is the product of TCF and TETF to estimate
the probability of a verb to be a trigger, which is
formulated as follows:

TRij = TCFij ∗ TETFi (4)

TCFij =
Count(Vi, ETSj)

Count(ETSj)
(5)

5http://stanfordnlp.github.io/CoreNLP/

412



TETFi = log
Sum(ET )

1 + Count(ETIi)
(6)

where TRij is the trigger rate of i-th verb to j-
th event type, Count(Vi, ETSj) is the number
of sentences, which express j-th type of even-
t and contain i-th verb, Count(ETSj) is the
number of sentences expressing j-th event type,
Count(ETIi) is the number of event types, which
have the labeled sentences containing i-th verb.
Finally, we choose verbs with high TR values as
the trigger words for each event type.

3.3 Trigger Word Filtering and Expansion
We can obtain an initial verbal trigger lexicon by
above trigger word detection. However, this initial
trigger lexicon is noisy and merely contains verbal
triggers. The nominal triggers like marriage are
missing. Because the number of nouns in one sen-
tence is usually larger than that of verbs, it is hard
to use TR to find nominal triggers. Thus, we pro-
pose to use linguistic resource FrameNet to filter
noisy verbal triggers and expand nominal triggers.
As the success of word embedding in capturing se-
mantics of words (Turian et al., 2010), we employ
word embedding to map the events in Freebase to
frames in FrameNet. Specifically, we use the aver-
age word embedding of all words in i-th Freebase
event type name ei and word embedding of k-th
lexical units of j-th frame ej,k to compute the se-
mantic similarity. Finally, we select the frame con-
tains max similarity of ei and ej,k as the mapped
frame, which can be formulated as follows:

frame(i) = argmax
j

(similarity(ei, ej,k)) (7)

Then, we filter the verb, which is in initial ver-
bal trigger word lexicon and not in the mapping
frame. And we use nouns with high confidence in
the mapped frame to expand trigger lexicon.

3.4 Automatically labeled data generation
Finally, we propose a Soft Distant Supervision
and use it to automatically generate training da-
ta, which assumes that any sentence containing al-
l key arguments in Freebase and a corresponding
trigger word is likely to express that event in some
way, and arguments occurring in that sentence are
likely to play the corresponding roles in that event.

4 Method of Event Extraction

In this paper, event extraction is formulated as
a two-stage, multi-class classification task. The

first stage is called Event Classification, which
aims to predict whether the key argument can-
didates participate in a Freebase event. If the
key arguments participate a Freebase event, the
second stage is conducted, which aims to assign
arguments to the event and identify their corre-
sponding roles. We call this stage as argumen-
t classification. We employ two similar Dynam-
ic Multi-pooling Convolutional Neural Network-
s with Multi-instance Learning (DMCNNs-MIL)
for above two stages. The Dynamic Multi-pooling
Convolutional Neural Networks (DMCNNs) is the
best reported CNN-based model for event extrac-
tion (Chen et al., 2015) by using human-annotated
training data. However, our automatically labeled
data face a noise problem, which is a intrinsic
problem of using DS to construct training data
(Hoffmann et al., 2011; Surdeanu et al., 2012). In
order to alleviate the wrong label problem, we use
Multi-instance Learning (MIL) for two DMCNNs.
Because the second stage is more complicated and
limited in space, we take the MIL used in argu-
ments classification as an example and describes
as follows:

We define all of the parameters for the stage
of argument classification to be trained in DM-
CNNs as θ. Suppose that there are T bags
{M1,M2, ...,MT } and that the i-th bag contains
qi instances (sentences) Mi =

{
m1i ,m

2
i , ...,m

qi
i

}
,

the objective of multi-instance learning is to pre-
dict the labels of the unseen bags. In stage of ar-
gument classification, we take sentences contain-
ing the same argument candidate and triggers with
a same event type as a bag and all instances in a
bag are considered independently. Given an in-
put instance mji , the network with the parameter
θ outputs a vector O, where the r-th component
Or corresponds to the score associated with argu-
ment role r. To obtain the conditional probability
p(r|mji , θ), we apply a softmax operation over all
argument role types:

p(r|mji , θ) =
eor

n∑
k=1

eok
(8)

where, n is the number of roles. And the objective
of multi-instance learning is to discriminate bags
rather than instances. Thus, we define the objec-
tive function on the bags. Given all (T ) training
bags (Mi, yi), we can define the objective function

413



Event Type FreebaseSize
Sentences

(KA)
Sentences
(KA+T) Examples of argument roles sorted by KR Examples of triggers

people.marriage 152,276 56,837 26,349 spouse, spouse, from, to, location marriage, marry, wed, wedding, couple,..., wife
music.group membership 239,813 90,617 20,742 group, member, start, role, end musician, singer, sing, sang, sung, concert,..., play

education.education 530,538 26,966 11,849 student, institution, degree,..., minor educate, education, graduate, learn, study,..., student
organization.leadership 43,610 5,429 3,416 organization, person, title,..., to CEO, charge, administer, govern, rule, boss,..., chair

olympics.olympic medal honor 20,790 4,056 2,605 medalist, olympics, event,..., country win, winner, tie, victor, gold, silver,..., bronze
... ... ... ... ... ...

sum of 21 selected events 3,870,492 421,602 72,611 argument1, argument2 ,..., argumentN trigger1, trigger2, trigger3, ... , triggerN

Table 2: The statistics of five largest automatically labeled events in selected 21 Freebase events, with
their size of instances in Freebase, sentences labeled with key argument (KA) and KA + Triggers(T),
examples of arguments roles sorted by KR and examples of triggers.

using cross-entropy at the bag level as follows:

J (θ) =
T∑

i=1

log p(yi|mji , θ) (9)

where j is constrained as follows:

j∗ = argmax
j
p(r|mji , θ) 1 ≤ j ≤ qi (10)

To compute the network parameter θ, we max-
imize the log likelihood J (θ) through stochas-
tic gradient descent over mini-batches with the
Adadelta (Zeiler, 2012) update rule.

5 Experiments

In this section, we first manually evaluate our auto-
matically labeled data. Then, we conduct automat-
ic evaluations for our labeled data based on ACE
corpus and analyze effects of different approach-
es to automatically label training data. Finally, we
shows the performance of DMCNNs-MIL on our
automatically labeled data.

5.1 Our Automatically Labeled Data
By using the proposed methods, a large set of la-
beled data could be generated automatically. Ta-
ble 2 shows the statistics of the five largest auto-
matically labeled events among selected 21 Free-
base events. Two hyper parameters, the number
of key arguments and the value of TR in our au-
tomatically data labeling, are set as 2 and 0.8, by
grid search respectively. When we merely use t-
wo key arguments to label data, we will obtain
421, 602 labeled sentences. However, these sen-
tences miss labeling triggers. Thus, we leverage
these rough labeled data and FrameNet to find trig-
gers and use SDS to generate labeled data. Finally,
72, 611 labeled sentences are generated automat-
ically. Compared with nearly 6, 000 human an-
notated labeled sentence in ACE, our method can
automatically generate large scale labeled training
data.

5.2 Manual Evaluations of Labeled Data 
 

 

##001  He is the uncle of [Amal Clooney], [wife] of the actor [George Clooney].   

Trigger: wife Event Type: Marriage  MannalAnotate[Y/N]: 

Argument: Amal Clooney  Role:Spouse  MannalAnotate[Y/N]: 

Argument: George Clooney Role:Spouse  MannalAnotate[Y/N]: 

 

##002  She was [married] to the cinematographer [Theo Nischwitz] and was 

sometimes credited as [Gertrud Hinz-Nischwitz].  

Trigger: married  Event Type: Marriage  MannalAnotate[Y/N]: 

Argument: Theo Nischwitz  Role:Spouse  MannalAnotate[Y/N]: 

Argument: Gertrud Hinz-Nischwitz Role:Spouse  MannalAnotate[Y/N]: 

 

 Figure 5: Examples of manual evaluations.

We firstly manually evaluate the precision of
our automatically generated labeled data. We ran-
domly select 500 samples from our automatically
labeled data. Each selected sample is a sentence
with a highlighted trigger, labeled arguments and
corresponding event type and argument roles. Fig-
ure 5 gives some samples. Annotators are asked
to assign one of two labels to each sample. “Y”:
the word highlighted in the given sentence indeed
triggers an event of the corresponding type or the
word indeed plays the corresponding role in that
event. Otherwise “N” is labeled. It is very easy to
annotate a sample for annotators, thus the annotat-
ed results are expected to be of high quality. Each
sample is independently annotated by three anno-
tators6 (including one of the authors and two of
our colleagues who are familiar with event extrac-
tion task) and the final decision is made by voting.

Stage Average Precision
Trigger Labeling 88.9

Argument Labeling 85.4

Table 3: Manual Evaluation Results

We repeat above evaluation process on the final
72, 611 labeled data three times and the average
precision is shown in Table 3. Our automatically
generated data can achieve a precision of 88.9 and
85.4 for trigger labeling and argument labeling re-

6The inter-agreement rate is 87.5%

414



Methods TriggerIdentification(%)
Trigger Identification
+ Classification(%)

Argument
Identification(%)

Argument
Role(%)

P R F P R F P R F P R F
Li’s structure trained with ACE 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7
Chen’s DMCNN trained with ACE 80.4 67.7 73.5 75.6 63.6 69.1 68.8 51.9 59.1 62.2 46.9 53.5
Nguyen’s JRNN trained with ACE 68.5 75.7 71.9 66.0 73.0 69.3 61.4 64.2 62.8 54.2 56.7 55.4
DMCNN trained with ED Only 77.6 67.7 72.3 72.9 63.7 68.0 64.9 51.7 57.6 58.7 46.7 52.0
DMCNN trained with ACE+ED 79.7 69.6 74.3 75.7 66.0 70.5 71.4 56.9 63.3 62.8 50.1 55.7

Table 4: Overall performance on ACE blind test data

spectively, which demonstrates that our automati-
cally labeled data is of high quality.

5.3 Automatic Evaluations of Labeled Data

To prove the effectiveness of the proposed ap-
proach automatically, we add automatically gen-
erated labeled data into ACE dataset to expand
the training sets and see whether the performance
of the event extractor trained on such expanded
training sets is improved. In our automatically
labeled data, there are some event types that can
correspond to those in ACE dataset. For exam-
ple, our people.marriage events can be mapped
to life.marry events in ACE2005 dataset. We
mapped these types of events manually and we
add them into ACE training corpus in two ways.
(1) we delete the human annotated ACE data for
these mapped event types in ACE dataset and add
our automatically labeled data to remainder ACE
training data. We call this Expanded Data (ED) as
ED Only. (2) We directly add our automatically la-
beled data of mapped event types to ACE training
data and we call this training data as ACE+ED.
Then we use such data to train the same even-
t extraction model (DMCNN) and evaluate them
on the ACE testing data set. Following (Nguyen
et al., 2016; Chen et al., 2015; Li et al., 2013), we
used the same test set with 40 newswire articles
and the same development set with 30 documents
and the rest 529 documents are used for ACE train-
ing set. And we use the same evaluation metric P,
R, F as ACE task defined. We select three base-
lines trained with ACE data. (1) Li’s structure,
which is the best reported structured-based system
(Li et al., 2013). (2) Chen’s DMCNN, which is
the best reported CNN-based system (Chen et al.,
2015). (3) Nguyen’s JRNN, which is the state-of-
the-arts system (Nguyen et al., 2016).

The results are shown in Table 4. Compared
with all models, DMCNN trained with ACE+ED
achieves the highest performance. This demon-
strates that our automatically generated labeled

data could expand human annotated training data
effectively. Moreover, compared with Chen’s DM-
CNN trained with ACE, DMCNN trained with ED
Only achieves a competitive performance. This
demonstrates that our large scale automatically la-
beled data is competitive with elaborately human-
annotated data.

5.4 Discussion

Impact of Key Rate

In this section, we prove the effectiveness of KR to
find key arguments and explore the impact of dif-
ferent numbers of key arguments to automatically
generate data. We specifically select two methods
as baselines for comparison with our KR method:
ER and RS, which use the event relevance and role
salience to sort arguments of each type of events
respectively. Then we choose the same number of
key arguments in all methods and use these key
arguments to label data. After that we evaluate
these methods by using above automatic evalua-
tions based on ACE data. Results are shown in Ta-
ble 5. ACE+KR achieve the best performance in
both stages. This demonstrates the effectiveness
of our KR methods.

Feature Trigger Argument
F1 F1

ACE 69.1 53.5
ACE + RS 70.1 55.3
ACE + ER 69.5 54.2
ACE + KR 70.5 55.7

Table 5: Effects of ER, RS and KR

To explore the impact of different numbers of
key arguments, we sort all arguments of each type
of events according to KR value and select top k
arguments as the key arguments. Examples are
shown in Table 2. Then we automatically eval-
uate the performance by using automatic evalua-
tions proposed above. Figure 6 shows the results,
when we set k = 2, the method achieves a best

415



Figure 6: Effects of the number of key arguments

performance in both stages. Then, the F1 value re-
duces as k grows. The reason is that the heuristics
for data labeling are stricter as k grows. As a re-
sult, less training data is generated. For example,
if k = 2, we will get 25, 797 sentences labeled
as people.marriage events and we will get 534 la-
beled sentences, if k = 3. However, when we set
k = 1, although more labeled data are generated,
the precision could not be guaranteed.

Impact of Trigger Rate and FrameNet

In this section, we prove the effectiveness of TR
and FrameNet to find triggers. We specifically s-
elect two methods as baselines: TCF and TETF.
TCF, TETF and TR respectively use the trigger
candidate frequency, trigger event type frequen-
cy and trigger rate to sort trigger candidates of
each type of events. Then we generate initial trig-
ger lexicon by using all trigger candidates with
high TCF value, TETF value or TR value. We
set these hyper parameters as 0.8, 0.9 and 0.8, re-
spectively, which are determined by grid search
from (0.5, 0.6, 0.7, 0.8, 0.9, 1.0). FrameNet was
used to filter noisy verbal triggers and expand
nominal triggers. Trigger examples generated by
TR+Framenet are shown in Table 2. Then we
evaluate the performance of these methods by
using above automatic evaluations. Results are
shown in Table 6, Compared with ACE+TCF and
ACE+TETF, ACE+TR gains a higher performance
in both stages. It demonstrates the effectiveness of
our TR methods. When we use FrameNet to gen-
erate triggers, compared with ACE+TR, we get a
1.0 improvement on trigger classification and a 1.7
improvement on argument classification. Such im-
provements are higher than improvements gained
by other methods (TCF, IEF, TR), which demon-
strates the effectiveness of the usage of FrameNet.

Feature Trigger Argument
F1 F1

ACE 69.1 53.5
ACE + TCF 69.3 53.8

ACE + TETF 69.2 53.7
ACE + TR 69.5 54.0

ACE + TR + FrameNet 70.5 55.7

Table 6: Effects of TCF, TETF,TR and FrameNet

5.5 Performance of DMCNN-MIL
Following previous work (Mintz et al., 2009) in
distant supervised RE, we evaluate our method in
two ways: held-out and manual evaluation.

Held-out Evaluation
In the held-out evaluation, we hold out part of the
Freebase event data during training, and compare
newly discovered event instances against this held-
out data. We use the following criteria to judge
the correctness of each predicted event automati-
cally: (1) An event is correct if its key arguments
and event type match those of an event instance in
Freebase; (2) An argument is correctly classified
if its event type and argument role match those of
any of the argument instance in the corresponding
Freebase event. Figure 7 and Figure 8 show the
precision-recall (P-R) curves for each method in
the two stages of event extraction respectively. We
can see that multi-instance learning is effective to
alleviate the noise problem in our distant super-
vised event extraction.

Figure 7: P-R curves for
event classification.

Figure 8: P-R curves for
argument classification.

Human Evaluation
Because the incomplete nature of Freebase, held-
out evaluation suffers from false negatives prob-
lem. We also perform a manual evaluation to e-
liminate these problems. In the manual evaluation,
we manually check the newly discovered event in-
stances that are not in Freebase. Because the num-
ber of these event instances in the test data is un-
known, we cannot calculate the recall in this case.

416



Instead, we calculate the precision of the top n ex-
tracted event instances. The human evaluation re-
sults are presented in Table 7. We can see that
DMCNNs-MIL achieves the best performance.

Methods Event ClassificaitonTop 100 Top 300 Top 500 Average
DMCNNs 58.7 54.3 52.9 55.3

DMCNNs+MIL 70.6 67.2 64.3 67.4

Methods Argument ClassificaitonTop 100 Top 300 Top 500 Average
DMCNNs 43.5 40.6 36.7 40.3

DMCNNs+MIL 50.8 45.6 43.5 46.6

Table 7: Precision for top 100, 300, and 500 events

6 Related Work

Most of previous event extraction work focused
on supervised learning paradigm and trained even-
t extractors on human-annotated data which yield
relatively high performance. (Ahn, 2006; Ji and
Grishman, 2008; Hong et al., 2011; McClosky
et al., 2011; Li et al., 2013, 2014; Chen et al.,
2015; Nguyen and Grishman, 2015; Nguyen et al.,
2016). However, these supervised methods de-
pend on the quality of the training data and la-
beled training data is expensive to produce. Un-
supervised methods can extract large numbers of
events without using labeled data (Chambers and
Jurafsky, 2011; Cheung et al., 2013; Huang et al.,
2016). But extracted events may not be easy to be
mapped to events for a particular knowledge base.

Distant supervision have been used in relation
extraction for automatically labeling training da-
ta (Mintz et al., 2009; Hinton et al., 2012; Krause
et al., 2012; Krishnamurthy and Mitchell, 2012;
Berant et al., 2013; Surdeanu et al., 2012; Zeng
et al., 2015). But DS for RE cannot directly use
for EE. For the reasons that an event is more com-
plicated than a relation and the task of EE is more
difficult than RE. The best reported supervised RE
and EE system got a F1-score of 88.0% (Wang
et al., 2016) and 55.4% (Nguyen et al., 2016) re-
spectively. Reschke et al. (2014) extended the
distant supervision approach to fill slots in plane
crash. However, the method can only extract ar-
guments of one plane crash type and need flight
number strings as input. In other words, the ap-
proach cannot extract whole event with different
types automatically.

7 Conclusion and Future Work

In this paper, we present an approach to automati-
cally label training data for EE. The experimental

results show the quality of our large scale auto-
matically labeled data is competitive with elabo-
rately human-annotated data. Also, we provide a
DMCNN-MIL model for this data as a baseline
for further research. In the future, we will use
the proposed automatically data labeling method
to more event types and explore more models to
extract events by using automatically labeled data.

Acknowledgments

This work was supported by the Natural Science
Foundation of China (No. 61533018) and the
National Basic Research Program of China (No.
2014CB340503). And this research work was al-
so supported by Google through focused research
awards program.

References
David Ahn. 2006. The stages of event extraction.

In Proceedings of the Workshop on Annotating
and Reasoning About Time and Events. pages 1–8.
http://dl.acm.org/citation.cfm?id=1629235.1629236.

Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th Internation-
al Conference on Computational Linguistics. Asso-
ciation for Computational Linguistics, pages 86–90.
http://aclweb.org/anthology/C98-1013.

Jonathan Berant, Andrew Chou, Roy Frostig, and
Percy Liang. 2013. Semantic parsing on free-
base from question-answer pairs. In Proceed-
ings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1533–1544.
http://aclweb.org/anthology/D13-1160.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a
collaboratively created graph database for struc-
turing human knowledge. In Proceedings of
the 2008 ACM SIGMOD international confer-
ence on Management of data. pages 1247–1250.
http://doi.acm.org/10.1145/1376616.1376746.

Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without
the templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies. As-
sociation for Computational Linguistics, pages
976–986. http://aclweb.org/anthology/P11-1098.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dy-
namic multi-pooling convolutional neural network-
s. In Proceedings of the 53rd Annual Meet-

417



ing of the Association for Computational Lin-
guistics and the 7th International Joint Confer-
ence on Natural Language Processing. Associa-
tion for Computational Linguistics, pages 167–176.
https://doi.org/10.3115/v1/P15-1017.

Kit Jackie Chi Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics, pages
837–846. http://aclweb.org/anthology/N13-1104.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint arX-
iv:1207.0580 https://arxiv.org/pdf/1207.0580.

Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and S. Daniel Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies. Association for Computational Linguistic-
s, pages 541–550. http://aclweb.org/anthology/P11-
1055.

Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Us-
ing cross-entity inference to improve event extrac-
tion. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies. Association
for Computational Linguistics, pages 1127–1136.
http://aclweb.org/anthology/P11-1113.

Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng Ji,
Clare R. Voss, Jiawei Han, and Avirup Sil. 2016.
Liberal event extraction and event schema induction.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics, pages 258–268.
http://www.aclweb.org/anthology/P16-1025.

Heng Ji and Ralph Grishman. 2008. Refining
event extraction through cross-document inference.
In Proceedings of ACL-08: HLT . Association
for Computational Linguistics, pages 254–262.
http://aclweb.org/anthology/P08-1030.

Sebastian Krause, Hong Li, Hans Uszkoreit, and
Feiyu Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from
the web. In Proceedings of International Se-
mantic Web Conference, Springer, pages 263–
278. http://link.springer.com/chapter/10.1007/978-
3-642-35176-1 17.

Jayant Krishnamurthy and Tom Mitchell. 2012. Weak-
ly supervised training of semantic parsers. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and

Computational Natural Language Learning. Associ-
ation for Computational Linguistics, pages 754–765.
http://aclweb.org/anthology/D12-1069.

Qi Li, Heng Ji, Yu Hong, and Sujian Li. 2014. Con-
structing information networks using one single
model. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 1846–1851. https://doi.org/10.3115/v1/D14-
1198.

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics, pages 73–
82. http://aclweb.org/anthology/P13-1008.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and
Jun Zhao. 2016. Leveraging framenet to im-
prove automatic event detection. In Proceed-
ings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics. Association
for Computational Linguistics, pages 2134–2143.
http://www.aclweb.org/anthology/P16-1201.

David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies. Association
for Computational Linguistics, pages 1626–1635.
http://aclweb.org/anthology/P11-1163.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. Asso-
ciation for Computational Linguistics, pages 1003–
1011. http://aclweb.org/anthology/P09-1113.

Huu Thien Nguyen and Ralph Grishman. 2015. Event
detection and domain adaptation with convolution-
al neural networks. In Proceedings of the 53rd An-
nual Meeting of the Association for Computation-
al Linguistics and the 7th International Joint Con-
ference on Natural Language Processing. Associa-
tion for Computational Linguistics, pages 365–371.
https://doi.org/10.3115/v1/P15-2060.

Thien Huu Nguyen, Kyunghyun Cho, and Ralph
Grishman. 2016. Joint event extraction via re-
current neural networks. In Proceedings of the
2016 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. Associa-
tion for Computational Linguistics, pages 300–309.
http://www.aclweb.org/anthology/N16-1034.

Kevin Reschke, Martin Jankowiak, Mihai Surdeanu,
Christopher D Manning, and Daniel Jurafsky.

418



2014. Event extraction using distant supervi-
sion. In Proceedings of the Ninth Internation-
al Conference on Language Resources and E-
valuation. pages 4527–4531. http://www.lrec-
conf.org/proceedings/lrec2014/pdf/1127 Paper.pdf.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and D. Christopher Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning. Associ-
ation for Computational Linguistics, pages 455–465.
http://aclweb.org/anthology/D12-1042.

Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and gen-
eral method for semi-supervised learning. In Pro-
ceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics. Associa-
tion for Computational Linguistics, pages 384–394.
http://aclweb.org/anthology/P10-1040.

Linlin Wang, Zhu Cao, Gerard de Melo, and
Zhiyuan Liu. 2016. Relation classification vi-
a multi-level attention cnns. In Proceedings
of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics. Association
for Computational Linguistics, pages 1298–1307.
http://www.aclweb.org/anthology/P16-1123.

Matthew D Zeiler. 2012. Adadelta: An adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701
https://arxiv.org/pdf/1212.5701.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction vi-
a piecewise convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1753–1762.
https://doi.org/10.18653/v1/D15-1203.

419


	Automatically Labeled Data Generation for Large Scale Event Extraction

