214

Coling 2010: Poster Volume, pages 214–222,

Beijing, August 2010

Hybrid Decoding: Decoding with Partial Hypotheses Combination over

Multiple SMT Systems∗

Lei Cui†, Dongdong Zhang‡, Mu Li‡, Ming Zhou‡, and Tiejun Zhao†

†School of Computer Science and Technology

Harbin Institute of Technology

{cuilei,tjzhao}@mtlab.hit.edu.cn

‡Microsoft Research Asia

{dozhang,muli,mingzhou}@microsoft.com
Abstract

In this paper, we present hybrid decod-
ing — a novel statistical machine transla-
tion (SMT) decoding paradigm using mul-
tiple SMT systems.
In our work, in ad-
dition to component SMT systems, sys-
tem combination method is also employed
in generating partial translation hypothe-
ses throughout the decoding process, in
which smaller hypotheses generated by
each component decoder and hypotheses
combination are used in the following de-
coding steps to generate larger hypothe-
ses. Experimental results on NIST evalu-
ation data sets for Chinese-to-English ma-
chine translation (MT) task show that our
method can not only achieve signiﬁcant
improvements over individual decoders,
but also bring substantial gains compared
with a state-of-the-art word-level system
combination method.
Introduction

1
In recent years, system combination for SMT has
been known to be quite effective with translation
consensus information built from multiple SMT
systems. The combination approaches can be
classiﬁed into two types. One is the combination
with each system’s outputs, which can be seen as
full hypotheses combination. The other is the par-
tial hypotheses (PHS) combination during the de-
coding phase.

A lot of impressive work has been done to im-
prove the performance of the SMT systems by uti-
∗This work has been done while the ﬁrst author was vis-

iting Microsoft Research Asia.

lizing consensus statistics which come from sin-
gle system or multiple systems. For example,
Minimum Bayes Risk (MBR) (Kumar and Byrne,
2004) decoding over n-best list ﬁnds a translation
that has lowest expected loss with all the other hy-
potheses, and it shows that improvement over the
Maximum a Posteriori (MAP) decoding. Several
word-based methods (Rosti et al., 2007a; Sim et
al., 2007) have also been proposed. Usually, these
methods take n-best list from different SMT sys-
tems as inputs, and construct a confusion network
for second-pass decoding. There are also a lot of
research work to advance the confusion network
construction by ﬁnding better alignment between
the skeleton and the other hypotheses (He et al.,
2008; Ayan et al., 2008). Typically, all the ap-
proaches above only use full hypotheses but have
no access to the PHS information.

Moreover, some dedicated efforts have been
tried by manipulating PHS between multiple MT
systems. Collaborative decoding (co-decoding)
(Li et al., 2009) leverages translation consensus
by exchanging partial translation results and re-
ranking both full and partial hypotheses explored
in decoding. However, no new PHS are generated
compared to the individual decoding but only the
ranking is affected. Liu et al.
(2009) proposes
joint decoding, a method that integrates multiple
translation models in one decoder. Although joint
decoding is able to generate new translations com-
pared to single decoder, it has to use the PHS
existed in one of its component decoder at each
step. Different from their work, we propose a
new perspective which leverages outputs from lo-
cal word-level combination. This will potentially
bring much beneﬁt of performance since word-

215

level combination can produce more promising
PHS.

The word-level system combination method is
employed to generate partial translation hypothe-
ses in our hybrid decoding framework.
In this
sense, full hypotheses word-level combination
(FH-Comb) method(Rosti et al., 2007a; Sim et al.,
2007; He et al., 2008; Ayan et al., 2008) can be
considered as a special case of hybrid decoding,
where their combinations are only performed on
the largest hypotheses. Similar with FH-Comb,
hybrid decoding also uses word alignment infor-
mation. However, challenge exists in hybrid de-
coding as word alignment needs to be carefully
conducted through the decoding process. Obvi-
ously, document-level word alignment methods
such as GIZA++(Och and Ney, 2000) are quite
time consuming and unpractical to be embedded
into hybrid decoding. We propose a heuristic
method that can conduct word alignment of par-
tial hypotheses based on word alignment informa-
tion of phrase pairs learnt automatically from the
model training process. In this way, more PHS are
generated and the search space is enlarged sub-
stantially, which brings better translation results.
The rest of the paper is organized as follows:
Section 2 gives a formal description of hybrid
decoding, including framework overview, word-
level PHS combination and parameter estimation.
We conduct experiments with different settings
and make comparison between our method and
baseline, as well as a state-of-the-art word-level
system combination method in Section 3. Exper-
imental results discussion is presented in Section
4. Section 5 concludes the paper.

2 Hybrid Decoding
2.1 Overview
Different system combination methods (Li et al.,
2009; Liu et al., 2009) offer different frameworks
to coordinate multiple SMT decoders. Hybrid de-
coding provides a new scheme to organize mul-
tiple decoders to work synchronously. As the
decoding algorithms may differ in multiple de-
coders1, hybrid decoding has some difﬁculty in

1In the SMT area, some decoders use left-right decod-
ing to generate the hypothesis and “Pharaoh”(Koehn et al.,

integrating different decoding algorithms. With-
out loss of generality, we assume that bottom-up
CKY-based decoding is adopted in each individ-
ual decoder, which is the same as co-decoding
(Li et al., 2009) and joint decoding (Liu et al.,
2009). Hybrid decoding collects n-best PHS of a
source span2 from multiple decoders, then results
from word-level PHS combination of that span are
given back to each decoder, mixed with the origi-
nal PHS. After that, we re-rank the hybrid list and
continue the decoding.
In an example with two
decoders, parts of the whole decoding process are
illustrated in Figure 1 and can be summarized as
follows:

Figure 1: Hybrid decoding with two decoders,
where the string “s-e”means the source span
starts from position s and ends at position e. The
blank rectangles represent the n-best partial trans-
lations of each decoder, and the shaded rectan-
gles illustrate the n-best local combination out-
puts. The ovals denote bottom-up CKY-based de-
coding results.

2003) is one of them, while others adopt bottom-up decoding
which is represented by “Hiero”(Chiang, 2007).

2The word “span”is used to represent translation unit
in CKY-based decoders, which denotes one or more consec-
utive words in the source sentence.

Decoder1

Decoder2

Local decoding layer

Local combination layer

1-6

1-6

1-6

Local decoding layer

1-6

1-6

1-2

3-6

3-6

3-6

1-2

Local combination layer

Local decoding layer

3-6

3-6

3-5

6-6

3-4

5-6

216

1. Individual decoding. Each individual de-
coder should maintain the n-best PHS of
each span from the bottom. After all the in-
dividual decoders ﬁnish translating the same
span, they feed their own partial translations
into a public container which can be used for
word-level PHS combination, then get back
the partial combination outputs for step 3.

2. Local word-level combination. After fed
with PHS from multiple decoders, a confu-
sion network is built and word-level combi-
nation for PHS is conducted. The obtained
new partial translations are given back to
each individual decoder to continue the de-
coding.

3. Mix new PHS with the original ones. The
span in each individual decoder will receive
the corresponding new PHS from the local
combination outputs. The feature space of
the new PHS is not exactly the same with that
of the original ones. It has to be mapped in
some way then the mixed hypotheses are re-
ranked.

In the following sub-sections, we ﬁrst present
the background of word-level combination for
PHS, then introduce hybrid decoding algorithm in
detail, as well as the feature deﬁnition and param-
eter estimation.

2.2 Word-Level Combination for Partial

Hypotheses

Most word-level system combination methods are
based on confusion network decoding.
In con-
fusion network construction, one hypothesis has
to be selected as the skeleton which determines
the word order of the combination results. Other
hypotheses are aligned against the skeleton. Ei-
ther votes or some word conﬁdence scores are as-
signed to each word in the network.

Most of the research on confusion network con-
struction focuses on seeking better word align-
ment between the skeleton and the other hypothe-
ses. So far, several word alignment procedures are
used for SMT system combination, which mainly
are GIZA++ alignments (Matusov et al., 2006),
TER alignments (Sim et al., 2007) and IHMM

ʍ||| political ||| 0-0
ʍ ||| political and economic ||| 0-0 1-2
||| economic ||| 0-0
 ᑭL||| economic interests ||| 0-0 1-1
ʍ [X1] ||| political and [X1] ||| 0-0 1-2

Figure 2: The example of translation alignment
from phrase-table and rule-table

alignments (He et al., 2008). Similar with general
word-level system combination method, word-
level PHS combination also uses word alignment
information. However, in hybrid decoding, it is
quite time-consuming and impractical to conduct
word alignment like GIZA++ for each span. For-
tunately, unit hypotheses word alignment can be
obtained from the model training process, which
is shown in Figure 2. We devise a heuristic
approach for PHS alignment that leverages the
translation derivations from the sub-phrases. The
derivation information ultimately comes from the
phrase table in phrase-based systems (Koehn et
al., 2003; Xiong et al., 2006) or the rule table in
syntactic-based systems (Chiang, 2007; Liu et al.,
2007; Galley et al., 2006).

The derivation is built in a phrase-based sys-
tem as follows. For example, we have two phrase
translations “ ᡃ  Ḅ ||| our ||| 0-0 1-0”and
“ ᑭL ||| economic interests ||| 0-0 1-1”,
where string “m-n”means the mth word in the
source phrase is aligned to the nth word in the tar-
get phrase. When combining the two phrases for
generating “ᡃ Ḅ  ᑭL”, we obtain
the translation hypothesis as “our economic in-
terests”and also integrate the alignment fragment
to get “0-0 1-0 2-1 3-2”. The case is similar in
syntactic-based system for non-terminal substitu-
tion, which we will not discuss further here.

Next, we introduce the skeleton-to-hypothesis
word alignment algorithm in detail. With the
translation derivations, the skeleton-to-hypothesis
(sk2hy) word alignment can be performed based
on the source-to-skeleton (so2sk) and source-to-
hypothesis (so2hy) word alignment as they share
the same source sentence. The basic idea is to
construct the sk2hy word alignment with the min-
imum correspondence subsets (MCS). A MCS is
deﬁned as a triple < SK, HY, SO > where the

217

repeat

Fetch out a source word to SO
SO1 = SO2 = SO
repeat

SO=UNION(SO1, SO2)
SK=GETALIGN(SO, so2sk)
HY =GETALIGN(SO, so2hy)
SO1=GETALIGN(SK, so2sk)
SO2=GETALIGN(HY, so2hy)

SK is the subset of skeleton words, HY is the
subset of the hypothesis words, and SO is the
minimum source word set that all target words in
both SK and HY are aligned to. Figure 3 shows
the algorithm for skeleton-to-hypothesis align-
ment. Most of the pseudo-code is self-explained
except for some subroutines, which are listed in
Table 1.
1: procedure SKEHYPALIGN(so2sk, so2hy)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end procedure

until |SO1| == |SO2| == |SO|
simmax = −inf inity
for all sk ∈ SK do
for all hy ∈ HY do

sim =SIM(sk, hy)
if sim ≥ simmax then

end for
ADDALIGN(skmax, hymax)

simmax = sim
skmax = sk
hymax = hy

until all the source words are fetched out

end for

end if

Figure 3: Algorithm for skeleton-to-hypothesis
alignment

Subroutines
UNION(A,B)
GETALIGN(S,align)

SIM(w1,w2)

ADDALIGN(w1,w2)

Description
the union of set A and set B
get the words aligned to
S based on align
similarity between w1 and w2,
we use edit distance here
align w1 with w2

Table 1: Description for subroutines

Due to the variety of the word order in n-
best outputs, skeleton selection becomes essen-

tial in confusion network construction. The sim-
plest way is to use the top-1 PHS from any indi-
vidual decoder with the best performance under
some criteria. However, this cannot always lead
to better performance on some evaluation met-
rics (Rosti et al., 2007a). An alternative would
be MBR method with some loss function such as
TER (Snover et al., 2006) or BLEU (Papineni et
al., 2002). We show the experimental results of
two skeleton selection methods for PHS combina-
tion in Section 3.

2.3 Hybrid Decoding Model
For a given source sentence f, any individual de-
coder in hybrid decoding ﬁnds the best transla-
tion e∗ among the possible translation hypotheses
Φ(f ) in terms of a ranking function F :

e∗ = argmaxe∈Φ(f )F(e)

(1)

Suppose we have n individual decoders. The
ranking function Fn of the nth decoder can be
written as:

Fn(e) =

mXi=1

λn,ihn,i(f, e)

(2)

where each hn,i(f, e) is a feature function of the
nth decoder, and λn,i is the corresponding feature
weight. m is the number of features in each de-
coder.

The ﬁnal result of hybrid decoder is the top-
1 translation from the confusion network, which
is constructed on multiple decoders with the last
layer’s output of CKY-based decoding.

2.4 Hybrid Decoding Algorithm
The hybrid decoder acts as a control unit which
controls the synchronization of multiple individ-
ual decoders. The algorithm is fully demonstrated
in Figure 4. The hybrid decoder pushes the same
span f j
i to different decoders and gets back the n-
best PHS (lines 2-6). When the span’s length is
too small, both word alignment and partial com-
bination results are not accurate. We predeﬁne a
ﬁxed threshold δ which is used for determining the
start-up of combination (line 7). When the length
condition holds, the n-best PHS of each individual

218

decoder are stored in container G (lines 8). Con-
fusion network is constructed and new PHS can be
extracted from it and are further mixed and sorted
with the original ones (lines 11-15).

1 , D)

nbest =DECODING(d, i, j)
if j − i ≥ δ then
ADD(G, nbest)
end if

for all i, j s.t. j − i = l do

for l ← 1...n do
G ← ∅
for all d ∈ D do

1: procedure HYBRIDDECODING(f n
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end procedure

end for
cn =CONNETBUILD(G)
nbest0 =GETPARHYP(cn)
for all d ∈ D do
end for

MIXSORT(nbestd, nbest0)

end for

end for

Figure 4: Hybrid decoding algorithm

2.5 Hybrid Decoding Features
Next we present the PHS word-level combination
feature functions for hybrid decoding. Following
(Rosti et al., 2007b), four features are utilized to
model the PHS as:
Word Conﬁdence Feature hwc(e)

hwc(e) = Pn

The word conﬁdence feature is computed as
i=1 µiciw, where n is the num-
ber of the systems, µi is the system conﬁ-
dence of system i, and ciw is the word conﬁ-
dence of word w in system i.

Word Penalty Feature hwp(e)

Word penalty feature is the number of words
in the partial hypothesis (PH).

Null Penalty Feature hnp(e)

For null penalty feature, we mean the number
of NULL links along the PH when extracted
from the confusion network.

Language Model Feature hlm(e)

Different from the above three combination

features, which can be obtained during the
confusion network construction or hypothe-
ses extraction, the language model feature
cannot be summed up on the ﬂy.
Instead,
it must be re-computed when building each
new PH.

2.6 Feature Space Mapping
The features used in hybrid decoding can be clas-
siﬁed into two categories: features for individual
decoders (FID) and features for PHS word-level
combination (FComb), and they are independent.
When mixing the new PHS with the original ones
of individual decoders, FComb space has to be
mapped to a FID space. However, several features
in FID are not deﬁned in FComb, such as source
to target (S2T) phrase probability, target to source
(T2S) phrase probability, S2T lexical probability,
T2S lexical probability and other model speciﬁc
features. A mapping function H needs to be de-
ﬁned as follows:

Ff id = H(Ff comb)

(3)

where Ff comb denotes the feature vector from
FComb space, while Ff id is the feature vector
from FID space.

An easy mapping function is implemented with
an intuitive motivation: PHS combination results
are better than the ones in individual decoder and
we prefer not to disorder the original search space.
Thus, the undeﬁned feature values of PHS from
FComb space are assigned by corresponding fea-
ture values of the top-1 PH in original decoder.
Experiments show that our method is not only
practical but also quite effective.

2.7 Parameter Estimation
Minimum Error Rate Training (MERT) (Och,
2003) algorithm is adopted to estimate feature
weights for hybrid decoding. As hybrid decoder
makes use of PHS from both individual decoders
and combination results as a whole, we devise
a new feature vector representation. The feature
vectors from FID space and FComb space are sim-
ply concatenated to form a longer vector without
overlapping. The weights are tuned simultane-
ously in order to reach a relatively global optima.

219

3 Experiment

3.1 Data and Metric

We conducted our experiments on the test data
of NIST 2005 and NIST 2006 Chinese-to-English
machine translation tasks. The NIST 2003 test
data is used as the development data to tune the
parameters. Statistics of the data sets are shown in
Table 2. Translation performances are measured
with case-insensitive BLEU4 score (Papineni et
al., 2002). Statistical signiﬁcance test is per-
formed using the bootstrap re-sampling method
proposed by Koehn (2004).

The bilingual

training corpora we used are
listed in Table 3, which contains 498K sentence
pairs, 12.1M Chinese words and 13.8M English
words after pre-processing. Word alignment is
performed by GIZA++ (Och and Ney, 2000) in
both directions with the default setting, and the
intersect-diag-grow method is used to reﬁne the
symmetric word alignment.

Data Set
NIST 2003(dev)
NIST 2005(test)
NIST 2006(test)

# Sentences

919
1,082
1,664

Table 2: Statistics of test/dev data sets

Description

FBIS Multilanguage Texts

LDC ID
LDC2003E07 Ch/En Treebank Par Corpus
LDC2003E14
LDC2005T06 Ch News Translation Text Part 1
LDC2005T10 Ch/En News Magazine Par Text
LDC2005E83 GALE Y1 Q1 Release - Translations
LDC2006E26 GALE Y1 - En/Ch Par Financial News
LDC2006E34 GALE Y1 Q2 Release - Translations

V2.0

LDC2006E85 GALE Y1 Q3 Release - Translations
LDC2006E92 GALE Y1 Q4 Release - Translations

Table 3: Training corpora for Chinese-English
translation

The language model used for hybrid decoding
and all the baseline systems is a 5-gram model
trained with the Xinhua portion of LDC English
Gigaword Version 3.0 plus the English part of
bilingual training data.

Implementation

3.2
We use two baseline systems. The ﬁrst one
(SYS1) is re-implementation of Hiero, a hi-
erarchical phrase-based system (Chiang, 2007)
based on Synchronous Context Free Grammar
(SCFG). Phrasal translation rules and hierarchi-
cal translation rules with nonterminals are ex-
tracted from all
the bilingual sentence pairs.
The second one (SYS2) is a phrase-based sys-
tem (Xiong et al., 2006) based on Bracketing
Transduction Grammar (Wu, 1997) with a lex-
icalized reordering model (Zhang et al., 2007)
under maximum entropy framework, where the
translation rules are exactly the same
phrasal
with that of SYS1.
The lexicalized reorder-
ing model is trained using the MaxEnt toolkit
(Zhang, 2006) where the training instances are
extracted from subset of the training corpora,
which contains LDC2003E07, LDC2003E14,
LDC2005T06, LDC2005T10. Both systems use
the bottom-up CKY-based decoding with cube-
pruning (Chiang, 2007) and the beam size is set
to 10 for decoding efﬁciency.

δ

set

For hybrid decoder, we

to be
sentence.length − 3, meaning that the PHS of
individual decoders only perform local combi-
nation in the last three layers. The reason why
we adopt this setting is because we ﬁnd that
starting local combination on short spans hurts
the performance badly on test data. Experimental
results are shown in the next section.

3.3 Translation Results
We present the overall results of hybrid decod-
ing over two baseline systems on both test sets.
We also implement an IHMM-based word-level
system combination method (He et al., 2008) to
make comparison with hybrid decoding system,
and the n-best candidates used for IHMM-based
word-level system combination is set to 10. Pa-
rameters for all the systems are tuned on NIST
2003 test set. The results are shown in Table 4.

In Table 4, we ﬁnd that the hybrid decoding per-
forms signiﬁcantly better than SYS1 and SY2 on
both test sets. Besides, compared to IHMM word-
level system combination method, hybrid decod-
ing also brings substantial gains with 0.63% and
0.92% points respectively.

220

SYS1
SYS2

IHMM Word-Comb

Hybrid

NIST 2005 NIST 2006
0.3745
0.3699
0.3821∗
0.3884∗+

0.3346
0.3296
0.3421∗
0.3513∗+

Table 4: Hybrid decoding results on test sets,
*:signiﬁcantly better than SYS1 and SYS2 with
p<0.01, +:signiﬁcantly better than IHMM Word-
Comb with p<0.01

We also try different layers for determining
the start-up of local word-level PHS combination.
Figure 5 gives the intuitive BLEU results.

Figure 6: Performance of hybrid decoding with
different beam sizes on NIST 2005 test set

increases, the performance gap is getting narrow.
One intuitive observation is that hybrid decoding
performs slightly worse than IHMM Word-Comb
when the beam size is set to 100. One possible
reason for this phenomenon is that, the alignment
noise may be introduced to hybrid decoding since
we have to generate monolingual alignments with
many poor translation derivations.

The confusion network for PHS of each system
can be built independently. We would like to eval-
uate the performance of single system hybrid de-
coding. Table 5 gives the results on both Hiero
and BTG decoders.

NIST 2005
SYS2
0.3699
0.3758∗

SYS1
0.3745
0.3770

NIST 2006
SYS2
0.3296
0.3355∗

SYS1
0.3346
0.3358

baseline
self-comb

Table 5: Hybrid decoding for single system,
*:signiﬁcantly better than baseline with p<0.05

Table 5 shows that BTG decoder (SYS2) has
more potential for so-called “self-boosting”.
The self-combination of BTG decoder improves
the performance substantially over the baseline.
However, we did not observe any signiﬁcant im-
provement for Hiero decoder (SYS1).

Finally, we examine the impacts of skeleton se-
lection for PHS in hybrid decoding. The results in
Table 6 demonstrate that, compared to the top-1
selection method, translation performance can be
improved signiﬁcantly with MBR-based skeleton
selection method. It strongly suggests that choos-
ing the skeleton with more consistent word order

Figure 5: Performance of hybrid decoding with
different start-up settings on NIST 2005 test set,
where the ”lastM” means to conduct local word-
level PHS combination in the last M layers from
the perspective of CKY-based decoding.

As shown in Figure 5, the performance drops
drastically if we start to conduct word-level PHS
combination too early. After considering about ef-
ﬁciency and performance, we determine to do that
in the last three layers.

We then investigate the effects on hybrid de-
coding with different beam sizes, and compare the
trend with two baseline systems and IHMM-based
word-level system combination method as well.
The results are illustrated in Figure 6.

From what we see in Figure 6, the performance
of each system is monotonically increasing as the
beam size becomes larger. Hybrid decoding per-
forms consistently better than IHMM Word-Comb
when the beam size is small, and the largest im-
provement (+0.63% points) is obtained when the
beam size is set to 10. However, as the beam size

0.4
0.39
0.38
0.37
0.36
0.35
0.34
0.33
0.32

 

0.4
0.395
0.39
0.385
0.38
0.375
0.37
0.365
0.36
0.355

SYS1

SYS2

IHMM

Hybrid

10

20

50

100

 

221

will lead to better translation results.

NIST 2005 NIST 2006
0.3817
0.3884∗

0.3415
0.3513∗

Top-1
MBR

Table 6: Skeleton selection in hybrid decoding,
*:signiﬁcantly better than top-1 skeleton selection
method with p<0.01

4 Discussion
System combination methods have been widely
used in SMT to improve the performance. For
example, in (Rosti et al., 2007a), several combi-
nation methods have been proposed to make use
of different kinds of consensus information.
In
(He et al., 2008), better word alignment method is
adopted to advance the word-level system combi-
nation. Our method is different from these meth-
ods in the sense that we do not exclusively rely
on the n-best full hypotheses from each individual
decoder, but emphasize the importance of word-
level combination for PHS. Thus, it enlarges the
search space and is more prone to ﬁnd better trans-
lations. Experimental results have shown the ef-
fectiveness of our method.

The idea of multiple systems collaborative de-
coding (Li et al., 2009) works well on re-ranking
the outputs of each system using n-gram agree-
ment statistics. However, no new translation re-
sults are generated compared to individual decod-
ing. Our method takes advantage of confusion
network to give PHS which cannot be seen before.
Although (Liu et al., 2009) also work on PHS,
we explore the cooperation of multiple systems
from a new perspective. They use translation
derivations from different decoders jointly as a
bridge to connect different models. Different from
their work, we devise a heuristic method to ob-
tain word alignment information from the deriva-
tion of each decoder, which can be embedded
for word-level PHS combination easily and efﬁ-
ciently.

5 Conclusion and Future Work
In this paper, we propose a new SMT decoding
framework named hybrid decoding, in which mul-
tiple decoders work synchronously to conduct lo-

cal decoding and local word-level PHS combina-
tion in turn. We also devise a heuristic method to
obtain word alignment information directly from
the translation derivations, which is both intuitive
and efﬁcient. Experimental results show that with
hybrid decoding the overall performance can be
improved signiﬁcantly over both the individual
baseline decoder and the state-of-the-art system
combination method.

In the future, we will involve more individual
SMT decoders into hybrid decoding. In addition,
we would like to keep on this work in two direc-
tions. On the one hand, start-up threshold of PHS
combination will be explored in detail to ﬁnd its
underlying impact on hybrid decoding. On the
other hand, we will try to employ a more theoreti-
cally sound approach to conduct the feature space
mapping from the feature space of confusion net-
work to that of individual decoders.

References
Ayan, Necip Fazil, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 33-40

Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation.
In Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 263-270

Chiang, David.

translation.
pages 201-228

2007. Hierarchical phrase-based
Computational Linguistics, 33(2):

Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models.
In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961-968

He, Xiaodong, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis for combining outputs from ma-
chine translation systems.
In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 98-107

Koehn, Phillip, Franz J. Och, and Daniel Marcu. 2003.
In Proceed-

Statistical phrase-based translation.

222

Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems.
In Proceedings of the
2007 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 228-235

Rosti, Antti-Veikko, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312-319

Sim, K.C., W. Byrne, M. Gales, H. Sahbi, and P.
Woodland.
2007. Consensus network decoding
for statistical machine translation combinnation. In
32nd IEEE International Conference on Acoustics,
Speech, and Signal Processing

Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annota-
tion.
In the 7th conference of the Association for
Machine Translation in the Americas, pages 223-
231

Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3): pages 377-
404

Xiong, Deyi, Qun Liu, and Shouxun Lin.

2006.
Maximum entropy based phrase reordering model
for statistical machine translation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
521-528

Zhang, Dongdong, Mu Li, Chi-Ho Li, Ming Zhou.
2007. Phrase Reordering Model Integrating Syn-
tactic Knowledge for SMT.
In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 533-540

Zhang, Le.

2006.
for python and c++.

Maximum entropy model-
available at

ing toolkit
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.

ings of the 2003 Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 48-54

Koehn, Phillip. 2004. Statistical signiﬁcance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388-395

Kumar, Shankar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine trans-
lation.
In Proceedings of the 2004 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 169-176

Li, Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009. Collaborative decoding: par-
tial hypothesis re-ranking using translation consen-
sus between decoders. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 585-
592

Liu, Yang, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
704-711

Liu, Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 576-584

Matusov, Evgeny, Nicola Uefﬁng, and Hermann Ney.
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced
hypotheses alignment.
In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 33-40

Och, Franz Josef. and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 440-447

Och, Franz Josef. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160-167

Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation.
In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311-318

