










































SRL-Based Verb Selection for ESL


Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068–1076,
MIT, Massachusetts, USA, 9-11 October 2010. c©2010 Association for Computational Linguistics

 
 

Abstract 

In this paper we develop an approach to tackle 

the problem of verb selection for learners of 

English as a second language (ESL) by using 

features from the output of Semantic Role La-

beling (SRL). Unlike existing approaches to 

verb selection that use local features such as 

n-grams, our approach exploits semantic fea-

tures which explicitly model the usage context 

of the verb. The verb choice highly depends 

on its usage context which is not consistently 

captured by local features. We then combine 

these semantic features with other local fea-

tures under the generalized perceptron learn-

ing framework. Experiments on both in-

domain and out-of-domain corpora show that 

our approach outperforms the baseline and 

achieves state-of-the-art performance.
1
 

1 Introduction 

Verbs in English convey actions or states of being. 

In addition, they also communicate sentiments and 

imply circumstances, e.g., in “He got [gained] the 

scholarship after three interviews.”, the verb 

“gained” may indicate that the “scholarship” was 

competitive and required the agent’s efforts; in 

contrast, “got” sounds neutral and less descriptive. 

                                                           
* This work has been done while the author was visiting Mi-

crosoft Research Asia. 

Since verbs carry multiple important functions, 

misusing them can be misleading, e.g., the native 

speaker could be confused when reading “I like 

looking [reading] books”. Unfortunately, accord-

ing to (Gui and Yang, 2002; Yi et al., 2008), more 

than 30% of the errors in the Chinese Learner Eng-

lish Corpus (CLEC) are verb choice errors. Hence, 

it is useful to develop an approach to automatically 

detect and correct verb selection errors made by 

ESL learners. 

However, verb selection is a challenging task 

because verbs often exhibit a variety of usages and 

each usage depends on a particular context, which 

can hardly be adequately described by convention-

al n-gram features. For instance, both “made” and 

“received” can complete “I have __ a telephone 

call.”, where the usage context can be represented 

as “made/received a telephone call”; however, in 

“I have __ a telephone call from my boss”, the 

prepositional phrase “from my boss” becomes a 

critical part of the context, which now cannot be 

described by n-gram features, resulting in only 

“received” being suitable. 

Some researchers (Tetreault and Chodorow, 

2008) exploited syntactic information and n-gram 

features to represent verb usage context. Yi et al. 

(2008) introduced an unsupervised web-based 

proofing method for correcting verb-noun colloca-

tion errors. Brockett et al. (2006) employed phrasal 

Statistical Machine Translation (SMT) techniques 

to correct countability errors. None of their meth-

ods incorporated semantic information. 

SRL-based Verb Selection for ESL 

1,2
Xiaohua Liu, 

3
Bo Han

*
, 

4
Kuan Li

*
, 

5
Stephan Hyeonjun Stiller and 

2
Ming Zhou 

1
School of Computer Science and Technology 

Harbin Institute of Technology 
2
Microsoft Research Asia 

3
Department of Computer Science and Software Engineering 

The University of Melbourne 
4
College of Computer Science 

Chongqing University 
5
Computer Science Department 

Stanford University 
{xiaoliu,  mingzhou, v-kuli}@microsoft.com 

 b.han@pgrad.unimelb.edu.au 
sstiller@stanford.edu 

1068



Unlike the other papers, we derive features from 

the output of an SRL (Màrquez, 2009) system to 

explicitly model verb usage context. SRL is gener-

ally understood as the task of identifying the argu-

ments of a given verb and assigning them semantic 

labels describing the roles they play. For example, 

given a sentence “I want to watch TV tonight” and 

the target predicate “watch”, the output of SRL 

will be something like “I [A0] want to watch [tar-

get predicate] TV [A1] tonight [AM-TMP].”, 

meaning that the action “watch” is conducted by 

the agent “I”, on the patient “TV”, and the action 

happens “tonight”. 

We believe that SRL results are excellent fea-

tures for characterizing verb usage context for 

three reasons: (i) Intuitively, the predicate-

argument structures generated by SRL systems 

capture major relationships between a verb and its 

contextual participants and consequently largely 

determine whether or not the verb usage is proper. 

For example, in “I want to watch a match tonight.”, 

“match” is the patient of “watch”, and “watch … 

match” forms a collocation, suggesting “watch” is 

appropriately used. (ii) Predicate-argument struc-

tures abstract away syntactic differences in sen-

tences with similar meanings, and therefore can 

potentially filter out lots of noise from the usage 

context. For example, consider “I want to watch a 

football match on TV tonight”: if “match” is suc-

cessfully identified as the agent of “watch”, 

“watch … football”, which is unrelated to the us-

age of “watch” in this case, can be easily excluded 

from the usage context. (iii) Research on SRL has 

made great achievements, including human-

annotated training corpora and state-of-the-art sys-

tems, which can be directly leveraged. 

Taking an English sentence as input, our method 

first generates correction candidates by replacing 

each verb with verbs in its pre-defined confusion 

set; then for every candidate, it extracts SRL-

derived features; finally our method scores every 

candidate using a linear function trained by the 

generalized perceptron learning algorithm (Collins, 

2002) and selects the best candidate as output. 

Experimental results show that SRL-derived fea-

tures are effective in verb selection, but we also 

observe that noise in SRL output adversely in-

creases feature space dimensions and the number 

of false suggestions. To alleviate this issue, we use 

local features, e.g., n-gram-related features, and 

achieve state-of-the-art performance when all fea-

tures are integrated. 

Our contributions can be summarized as follows: 

1. We propose to exploit SRL-derived fea-
tures to explicitly model verb usage con-

text. 

2. We propose to use the generalized percep-
tron framework to integrate SRL-derived 

(and other) features  and achieve state-of-

the-art performance on both in-domain and 

out-of-domain test sets. 

Our paper is organized as follows: In the next 

section, we introduce related work. In Section 3, 

we describe our method. Experimental results and 

analysis on both in-domain and out-of-domain cor-

pora are presented in Section 4. Finally, we con-

clude our paper with a discussion of future work in 

Section 5. 

2 Related Work 

SRL results are used in various tasks. Moldovan et 

al. (2004) classify the semantic relations of noun 

phrases based on SRL. Ye and Baldwin (2006) 

apply semantic role–related information to verb 

sense disambiguation. Narayanan and Harabagiu 

(2004) use semantic role structures for question 

answering. Surdeanu et al. (2003) employ predi-

cate-argument structures for information extrac-

tion. 

However, in the context of ESL error detection 

and correction, little study has been carried out on 

clearly exploiting semantic information. Brockett 

et al. (2006) propose the use of the phrasal statisti-

cal machine translation (SMT) technique to identi-

fy and correct ESL errors. They devise several 

heuristic rules to generate synthetic data from a 

high-quality newswire corpus and then use the syn-

thetic data together with their original counterparts 

for SMT training. The SMT approach on the artifi-

cial data set achieves encouraging results for cor-

recting countability errors. Yi et al. (2008) use web 

frequency counts to identify and correct determiner 

and verb-noun collocation errors. Compared with 

these methods, our approach explicitly models 

verb usage context by leveraging the SRL output. 

The SRL-based semantic features are integrated, 

along with the local features, into the generalized 

perceptron model. 
 

1069



3 Our Approach 

Our method can be regarded as a pipeline consist-

ing of three steps. Given as input an English sen-

tence written by ESL learners, the system first 

checks every verb and generates correction candi-

dates by replacing each verb with its confusion set. 

Then a feature vector that represents verb usage 

context is derived from the outputs of an SRL sys-

tem and then multiplied with the feature weight 

vector trained by the generalized perceptron. Final-

ly, the candidate with the highest score is selected 

as the output. 

3.1 Formulation 

We formulate the task as a process of generating 

and then selecting correction candidates: 

           
 

 sScores
sGENs '

maxarg*



                      (1) 

Here 
's  denotes the input sentence for proofing, 

 'sGEN  is the set of correction candidates, and 
 sScore  is the linear model trained by the percep-

tron learning algorithm, which will be discussed in 

section 3.4. 

We call every target verb in 
's  a checkpoint. 

For example, “sees” is a checkpoint in “Jane sees 

TV every day.”. Correction candidates are generat-

ed by replacing each checkpoint with its confu-

sions. Table 1 shows a sentence with one 

checkpoint and the corresponding correction can-

didates. 
 

Input Jane sees TV every day. 

Candidates Jane watches TV every day. 

Jane looks TV every day. 

… 

Table 1. Correction candidate list. 

One state-of-the-art SRL system (Riedel and 

Meza-Ruiz, 2008) is then utilized to extract predi-

cate-argument structures for each verb in the input, 

as illustrated in Table 2. 

Semantic features are generated by combining 

the predicate with each of its arguments; e.g., 

“watches_A0_Jane”, “sees_A0_Jane”, “watch-

es_A1_TV” and “sees_A1_TV” are semantic fea-

tures derived from the semantic roles listed in Ta-

ble 2. 

 

Sentence Semantic roles 

Jane sees TV every day Predicate: sees; 

A0: Jane; 

A1: TV; 

Jane watches TV every 

day 

Predicate: watches; 

A0: Jane; 

A1: TV; 

Table 2. Examples of SRL outputs. 

At the training stage, each sentence is labeled by 

the SRL system. Each correction candidate s  is 

represented as a feature vector 
dRs  )( , where 

d  is the total number of features. The feature 

weight vector is denoted as 
dRw


, and  sScore  

is computed as follows: 

               wssScore  )(                        (2) 

Finally,  sScore  is applied to each candidate, 

and *s , the one with the highest score, is selected 

as the output, as shown in Table 3. 
 

 Correction candidate Score 
*s  Jane watches TV every day. 10.8 

 Jane looks TV every day. 0.8 

 Jane reads TV every day. 0.2 

 … … 

Table 3.  Correction candidate scoring. 

In the above framework, the basic idea is to 

generate correction candidates with the help of pre-

defined confusion sets and apply the global linear 

model to each candidate to compute the degree of 

its fitness to the usage context that is represented 

as features derived from SRL results. 

To make our idea practical, we need to solve the 

following three subtasks: (i) generating the confu-

sion set that includes possible replacements for a 

given verb; (ii) representing the context with se-

mantic features and other complementary features; 

and (iii) training the feature weight. We will de-

scribe our solutions to those subtasks in the rest of 

this section. 

1070



3.2 Generation of Verb Confusion Sets 

Verb confusion sets are used to generate correction 

candidates. Due to the great number of verbs and 

their diversified usages, manually collecting all 

verb confusions in all scenarios is prohibitively 

time-consuming. To focus on the study of the ef-

fectiveness of semantic role features, we restrict 

our research scope to correcting verb selection er-

rors made by Chinese ESL learners and select fifty 

representative verbs which are among the most 

frequent ones and account for more than 50% of 

ESL verb errors in the CLEC data set. For every 

selected verb we manually compile a confusion set 

using the following data sources: 

1. Encarta treasures. We extract all the syno-

nyms of verbs from the Microsoft Encarta Diction-

ary, and this forms the major source for our 

confusion sets. 

2. English-Chinese Dictionaries. ESL learners 

may get interference from their mother tongue (Liu 

et al., 2000). For example, some Chinese people 

mistakenly say “see newspaper”, partially because 

the translation of “see” co-occurs with “newspa-

per” in Chinese. Therefore English verbs in the 

dictionary sharing more than two Chinese mean-

ings are collected. For example, “see” and “read” 

are in a confusion set because they share the mean-

ings of both “看” (“to see”, “to read”) and “领会” 
(“to grasp”) in Chinese. 

3. An SMT translation table. We extract para-

phrasing verb expressions from a phrasal SMT 

translation table learnt from parallel corpora (Och 

and Ney, 2004). This may help us use the implicit 

semantics of verbs that SMT can capture but a dic-

tionary cannot, such as the fact that the verb  

Note that verbs in any confusion set that we are 

not interested in are dropped, and that the verb it-

self is included in its own confusion set. We leave 

it to our future work to automatically construct 

verb confusions. 

3.3 Verb Usage Context Features 

The verb usage context
1
 refers to its surrounding 

text, which influences the way one understands the 

expression. Intuitively, verb usage context can take 

the form of a collocation, e.g., “watch … TV” in “I 

saw [watched] TV yesterday.” ; it can also simply 

be idioms, e.g., we say “kick one’s habit” instead 

of “remove one’s habit”.  

We use features derived from the SRL output to 

represent verb usage context. The SRL system ac-

cepts a sentence as input and outputs all arguments 

and the semantic roles they play for every verb in 

the sentence. For instance, given the sentence “I 

have opened an American bank account in Bos-

ton.” and the predicate “opened”, the output of 

SRL is listed in Table 4, where A0 and A1 are two 

core roles, representing the agent and patient of an 

action, respectively, and other roles starting with 

“AM-”are adjunct roles, e.g., AM-LOC indicates 

the location of an action. Predicate-argument struc-

tures keep the key participants of a given verb 

while dropping other unrelated words from its us-

age context. For instance, in “My teacher said Chi-

nese is not easy to learn.”, the SRL system 

recognizes that “Chinese” is not the A1-argument 

of “said”. So “say _ Chinese”, which is irrelevant 

to the usage of said, is not extracted as a feature. 

The SRL system, however, may output 

erroneous predicate-argument structures, which 

negatively affect the performance of verb 

selection.  For instance,  for the sentence “He 

hasn’t done anything but take [make] a lot of 

money”, “lot” is incorrectly identified as the patient 

of “take”, making it hard to select “make” as the 

proper verb even though “make money” forms a 

sound collocation. To tackle this issue, we use 

local textual features, namely features related to n-

gram, chunk and chunk headword, as shown in 

Table 5.  Back-off features are generated by 

replacing the word with its POS tag to alleviate 

data sparseness. 

 

                                                           
1 http://en.wikipedia.org/wiki/Context_(language_use) 

I have made[opened] an American bank account in Boston . 

[A0] 
 

[Predicate] 
 

 
 

[A1] [AM-LOC] 

 
 

Table 4. An example of SRL output. 

1071



Table 5. An example of feature set. 

3.4 Perceptron Learning 

We choose the generalized perceptron algorithm as 

our training method because of its easy implemen-

tation and its capability of incorporating various 

features. However, there are still two concerns 

about this perceptron learning approach: its inef-

fectiveness in dealing with inseparable samples 

and its ignorance of weight normalization that po-

tentially limits its ability to generalize. In section 

4.4 we show that the training error rate drops sig-

nificantly to a very low level after several rounds 

of training, suggesting that the correct candidates 

can almost be separated from others. We also ob-

serve that our method performs well on an out-of-

domain test corpus, indicating the good generaliza-

tion ability of this method. We leave it to our fu-

ture work to replace perceptron learning with other 

models like Support Vector Machines (Vapnik, 

1995). 

In Figure 1, is  is the ith correct sentence within 

the training data. T and N represent the number of 

training iterations and training examples, respec-

tively. )( isGEN  is the function that outputs all the 

possible corrections for the input sentence is  with 

each checkpoint substituted by one of its confu-

sions, as described in Section 3.1. We observe that 

the generated candidates sometimes contain rea-

sonable outputs for the verb selection task, which 

should be removed. For instance, in “… reporters 

could not take [make] notes or tape the conversa-

tion”, both “take” and “make” are suitable verbs in 

this context. To fix this issue, we trained a trigram 

language model using SRILM (Stolcke, 2002) on 

LDC data
21

, and calculated the logarithms of the 

language model score for the original sentence and 

its artificial manipulations. We only kept manipu-

lations with a language model score that is t lower 

than that of the original sentence. We experimen-

tally set t = 5. 
 

Inputs: training examples is , i=1…N 

Initialization: 0w


 

Algorithm: 

   For r= 1.. T, i= 1..N    

   Calculate wso isGens




)(maxarg

)(
 

   If os i   

         )()( osww o 


 

Outputs: w


 

Figure 1. The perceptron algorithm, adapted from Co-

lins (2002). 

  in Figure 1 is the feature extraction function. 

)(o and )( is are vectors extracted from the out-

put and oracle, respectively. A vector field is filled 

with 1 if the corresponding feature exists, or 0 oth-

erwise; w


 is the feature weight vector, where posi-

tive elements suggest that the corresponding 

features support the hypothesis that the candidate 

is correct. 

The training process is to update w


, when the 

output differs from the oracle. For example, when 

o is “I want to look TV” and is  is “I want to watch 

TV”, w


 will be updated. 

We use the averaged Perceptron algorithm (Col-

lins, 2002) to alleviate overfitting on the training 

data. The averaged perceptron weight vector is 

defined as 

                 



TrN

riw
TN ..1,..1i

,1                      (3) 

where 
riw ,


is the weight vector immediately af-

ter the ith sentence in the  rth iteration. 
 

                                                           
2 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?cata-

logId=LDC2005T12 

Local: trigrams 

   have_opened 

   have_opened_a 

   opened_an_American 

   PRP_VBP_opened 

   VBP_opened_DT 

   opened_DT_JJ 

Local: chunk 

   have_opened 

   opened_an_American_investment_bank 

_account 

   PRP_opened 

   opened_NN 

Semantic: SRL derived features 

   A0_I_opened 

   opened_A1_account 

   opened_AM-LOC_in 

   ... 

1072



4 Experiments 

In this section, we compare our approach with the 

SMT-based approach. Furthermore, we study the 

contribution of predicate-argument-related 

features, and the performances on verbs with 

varying distance to their arguments. 

4.1 Experiment Preparation 

The training corpus for perceptron learning was 

taken from LDC2005T12. We randomly selected 

newswires containing target verbs from the New 

York Times as the training data. We then used the 

OpenNLP package
31

to extract sentences from the 

newswire text and to parse them into the corre-

sponding tokens, POS tags, and chunks. The SRL 

system is built according to Riedel and Meza-Ruiz 

(2008), using the CoNLL-2008 shared task data for 

training. We assume that the newswire data is of 

high quality and free of linguistic errors, and final-

ly we gathered 20000 sentences that contain any of 

the target verbs we were focusing on.  We experi-

mentally set the number of training rounds to T = 

50. 

We constructed two sets of testing data for in-

domain and out-of-domain test purposes, respec-

tively. To construct the in-domain test data, we 

first collected all the sentences that contain any of 

the verbs we were interested in from the previous 

unused LDC dataset; then we replaced any target 

verb in our list with a verb in its confusion set; 

next, we used the language-model-based pruning 

strategy described in 3.4 to drop possibly correct 

manipulations from the test data; and finally we 

randomly sampled 5000 sentences for testing. 

To build the out-of-domain test dataset, we 

gathered 186 samples that contained errors related 

to the verbs we were interested in from English 

blogs written by Chinese and from the CLEC cor-

pus, which were then corrected by an English na-

tive speaker. Furthermore, for every error 

involving the verbs in our target list, both the verb 

and the word that determines the error are marked 

by the English native speaker. 

4.2 Baseline 

We built up a phrasal SMT system with the word 

re-ordering feature disabled, since our task only 

concerns the substitution of the target verb. To 

                                                           
3 http://opennlp.sourceforge.net/ 

construct the training corpus, we followed the idea 

in Brockett et al. (2006), and applied a similar 

strategy described in section 3.4 to the SRL sys-

tem’s training data to generate aligned pairs. 

4.3 Evaluation Metric 

We employed the following metrics adapted from 

(Yi et al., 2008): revised precision (RP), recall of 

the correction (RC) and false alarm (FA). 

         
 sCheckpoint All of #

Proofings Correct of #
RP                        (4)      

RP reflects how many outputs are correct usag-

es. The output is regarded as a correct suggestion if 

and only if it is exactly the same as the answer. 

Paraphrasing scenarios, for example, the case that 

the output is “take notes” and the answer is “make 

notes”, are counted as errors. 

Errors Total of# 

Proofings Modified Correct of# 
RC                  (5) 

RC indicates how many erroneous sentences are 

corrected among all the errors. It measures the sys-

tem’s coverage of verb selection errors. 

     
sCheckpoint All of# 

sCheckpoint Modified Incorrect of# 
FA          (6) 

FA is related to the cases where a correct verb is 

mistakenly replaced by an inappropriate one. The-

se false suggestions are likely to disturb or even 

annoy users, and thus should be avoided as much 

as possible. 

4.4 Results and Analysis 

The training error curves of perceptron learning 

with different feature sets are shown in Figure 2. 

They drop to a low error rate and then stabilize 

after a few number of training rounds, indicating 

that most of the cases are linearly separable and 

that perceptron learning is applicable to the verb 

selection task. 

We conducted feature selection by dropping fea-

tures that occur less than N times. Here N was ex-

perimentally set to 5. We observe that, after feature 

selection, some useful features such as 

“watch_A1_TV” and “see_A1_TV” were kept, but 

some noisy features like “Jane_A0_sees” and 

“Jane_A0_watches” were removed, suggesting the 

effectiveness of this feature selection approach. 
 

1073



 

Figure 2. Training error curves of the perceptron. 

We tested the baseline and our approach on the 

in-domain and out-of-domain corpora. The results 

are shown in Table 7 and 8, respectively. 

In the in-domain test, the SMT-based approach 

has the highest false alarm rate, though its output 

with word insertions or deletions is not considered 

wrong if the substituted verb is correct. Our ap-

proach, regardless of what feature sets are used, 

outperforms the SMT-based approach in terms of 

all metrics, showing the effectiveness of percep-

tron learning for the verb selection task. Under the 

perceptron learning framework, we can see that the 

system using only SRL-related features has higher 

revised precision and recall of correction, but also 

a slightly higher false alarm rate than the system 

based on only local features. When local features 

and SRL-derived features are integrated together, 

the state-of-the-art performance is achieved with a 

5% increase in recall, and minor changes in preci-

sion and false alarm. 

In the out-of-domain test, the SMT-based ap-

proach performs much better than in the in-domain 

test, especially in terms of false alarm rate, indicat-

ing the SMT-based approach may favor short sen-

tences. However, its recall drops greatly. We ob-

serve similar performance differences between the 

systems with different feature sets under the same 

perceptron learning framework, reaffirming the 

usefulness of the SRL-based features for verb se-

lection. 

We also conducted significance test. The results 

confirm that the improvements (SRL+Local vs. 

SMT-based) are statistically significant (p-value < 

0.001) for both the open-domain and the in-domain 

experiments. 

Furthermore, we studied the performance of our 

system on verbs with varying distance to their ar-

guments on the out-of-domain test corpus. 

 

Local d<=2 2<d<=4 d>4 

RP 64.3% 60.3% 59.4% 

RC 34.6% 33.1% 28.9% 

FA 3.0% 6.3% 5.0% 

SRL d<=2 2<d<=4 d>4 

RP 65.1% 60.1% 62.1% 

RC 40.3% 34.0% 36.9% 

FA 5.0% 6.7% 6.3% 

Table 9. Performance on verbs with different distance to 

their arguments on out-of-domain test data. 

Table 9 shows that the system with only SRL-

derived features performs significantly better than 

the system with only local features on the verb 

whose usage depends on a distant argument, i.e., 

one where the number of words between the predi-

cate and the argument is larger than 4. To under-

stand the reason, consider the following sentence: 

“It's raining outside. Please wear[take] the 

black raincoat with you.” 

 SMT-based Our method 
SRL Local SRL + Local 

RP 48.4% 64.5% 62.2% 66.4% 

RC 23.5% 40.2% 32.9% 46.4% 

FA 13.3% 5.6% 4.2% 6.8% 

Table 7. In-domain test results. 

 SMT-based Our method 

SRL Local SRL + Local 

RP 50.7% 64.0% 62.6% 65.5% 

RC 13.5% 39.0% 33.3% 44.0% 

FA 6.1% 5.5% 4.0% 6.5% 

Table 8. Out-of-domain test results. 

 

1074



Intuitively, “wear” and “take” seem to fill the 

blank well, since they both form a collocation with 

“raincoat”; however, when “with [AM-MNR] you” 

is considered as part of the context, “wear” no 

longer fits it and “take” wins. In this case, the long-

distance feature devised from AM-MNR helps se-

lect the suitable verb, while the trigram features 

cannot because they cannot represent the long dis-

tance verb usage context. 

We also find some typical cases that are beyond 

the reach of the SRL-derived features. For instance, 

consider “Everyone doubts [suspects] that Tom is 

a spy.”. Both of the verbs can be followed by a 

clause. However, the SRL system regards “is”, the 

predicate of the clause, as the patient, resulting in 

features like “doubt_A1_is” and “suspect_A1_is”, 

which capture nothing about verb usage context. 

However, if we consider the whole clause “sus-

pect_Tom is a spy” as the patient, this could result 

in a very sparse feature that would be filtered. In 

the future, we will combine word-level and phrase-

level SRL systems to address this problem. 

Besides its incapability of handling verb selec-

tion errors involving clauses, the SRL-derived fea-

tures fail to work when verb selection depends on 

deep meanings that cannot be captured by current 

shallow predicate-argument structures. For exam-

ple, in “He was wandering in the park, spending 

[killing] his time watching the children playing.”, 

though “spending” and “killing” fit the syntactic 

structure and collocation agreement, and express 

the meaning “to allocate some time doing some-

thing”, the word “wandering” suggests that “kill-

ing” may be more appropriate. Current SRL 

systems cannot represent the semantic connection 

between two predicates and thus are helpless for 

this case. We argue that the performance of our 

system can be improved along with the progress of 

SRL. 

5 Conclusions and Future Work 

Verb selection is challenging because verb usage 

highly depends on the usage context, which is hard 

to capture and represent. In this paper, we propose 

to utilize the output of an SRL system to explicitly 

model verb usage context. We also propose to use 

the generalized perceptron learning framework to 

integrate SRL-derived features with other features. 

Experimental results show that our method outper-

forms the SMT-based system and achieves state-

of-the-art performance when SRL-related features 

and other local features are integrated. We also 

show that, for cases where the particular verb us-

age mainly depends on its distant arguments, a sys-

tem with only SRL-derived features performs 

much better than the system with only local fea-

tures. 

In the future, we plan to automatically construct 

confusion sets, expand our approach to more verbs 

and test our approach on a larger size of real data. 

We will try to combine the outputs of several SRL 

systems to make our system more robust. We also 

plan to further validate the effectiveness of the 

SRL-derived features under other learning methods 

like SVMs. 

Acknowledgment 

We thank the anonymous reviewers for their valu-

able comments. We also thank Changning Huang, 

Yunbo Cao, Dongdong Zhang, Henry Li and Mu 

Li for helpful discussions. 

References  

Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. 

Countability and number in Japanese to English ma-

chine translation. Proc. of the 15th conference on 

Computational Linguistics, pages 32-38. 

Chris Brockett, William B. Dolan, and Michael Gamon. 

2006. Correcting ESL errors using phrasal SMT 

techniques. Proc. of the 21st International Confer-

ence on Computational Linguistics and the 44th An-

nual Meeting on Association for Computational 

Linguistics, pages 249-256. 

Michael Collins. 2002. Discriminative training methods 

for hidden Markov models: theory and experiments 

with perceptron algorithms. Proc. of the ACL-02 

Conference on Empirical Methods in Natural Lan-

guage Processing, pages 1-8. 

Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic 

Grammar Checking for Second Language Learners – 

the Use of Prepositions. Proc. of NoDaliDa. 

Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-

dre Klementiev, William B. Dolan, Dmitrtiy Belen-

ko, and Lucy Vanderwende. 2008. Using Contextual 

Speller Techniques and Language Modeling for ESL 

Error Correction. Proc. of the International Joint 

Conference on Natural Language Processing. 

Shichun Gui and Huizhong Yang. 2002. Chinese Learn-

er English Corpus. Shanghai Foreign Languages Ed-

ucation Press, Shanghai, China. 

1075



Julia E. Heine. 1998. Definiteness predictions for Japa-

nese noun phrases. Proc. of the 36th Annual Meeting 

of the Association for Computational Linguistics and 

17th International Conference on Computational 

Linguistics, pages 519-525. 

John Lee and Stephanie Seneff. 2008. Correcting mis-

use of verb forms. Proc. of the 46th Annual Meeting 

on Association for Computational Linguistics, pages 

174-182. 

Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun and 

Changning Huang. 2000. PENS: A Machine-aided 

English Writing System for Chinese Users. Proc. of 

the 38th Annual Meeting on Association for Compu-

tational Linguistics, pages 529-536. 

Lluís Màrquez. 2009. Semantic Role Labeling Past, 

Present and Future, Tutorial of ACL-IJCNLP 2009.   

Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel 

Antohe and Roxana Girju. 2004. Models for the se-

mantic classification of noun phrases. Proc. of the 

HLT-NAACL Workshop on Computational Lexical 

Semantics, pages 60-67. 

Srini Narayanan and Sanda Harabagiu. 2004. Question 

answering based on semantic structures. Proc. of the 

20th International Conference on Computational 

Linguistics, pages 693-701. 

Franz J. Och and Hermann Ney. 2004. The Alignment 

Template Approach to Statistical Machine Transla-

tion. Journal of Computational Linguistics, 30(4), 

pages 417-449. 

Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective 

semantic role labelling with Markov Logic. Proc. of 

the Twelfth Conference on Computational Natural 

Language Learning, pages 193-197. 

Andreas Stolcke. 2002. SRILM -- An Extensible Lan-

guage Modeling Toolkit. Proc. of International Con-

ference on Spoken Language Processing, pages: 901-

904. 

Mihai Surdeanu, Lluis Màrquez, Xavier Carreras, and 

Pere R. Comas. 2007. Combination strategies for se-

mantic role labeling. Journal of Artificial Intelligence 

Research, page 105-151. 

Mihai Surdeanu, Sanda Harabagiu, John Williams, and 

Paul Aarseth. 2003. Using predicate-argument struc-

tures for information extraction. Proc. of the 41st 

Annual Meeting on Association for Computational 

Linguistics, pages 8-15. 

Joel R. Tetreault and Martin Chodorow. 2008. The ups 

and downs of preposition error detection in ESL writ-

ing. Proc. of the 22nd international Conference on 

Computational Linguistics, pages 865-872. 

Vladimir N. Vapnik. 1995. The Nature of Statistical 

Learning Theory. Springer-Verlag, New York. 

Patrick Ye and Timothy Baldwin. 2006. Verb Sense 

Disambiguation Using Selectional Preferences 

Extracted with a State-of-the-art Semantic Role 

Labeler. Proc. of the Australasian Language 

Technology Workshop, pages 141-148. 

Xing Yi, Jianfeng Gao, and William B. Dolan. 2008. A 

Web-based English Proofing System for English as a 

Second Language Users. Proc. of International Joint 

Conference on Natural Language Processing, pages 

619-624. 

1076


