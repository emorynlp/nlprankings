










































Graph-based Approaches for Organization Entity Resolution in MapReduce


Proceedings of the TextGraphs-8 Workshop, pages 70–78,
Seattle, Washington, USA, 18 October 2013. c©2013 Association for Computational Linguistics

Graph-based Approaches for Organization Entity Resolution in MapReduce

Hakan Kardes, Deepak Konidena, Siddharth Agrawal, Micah Huff and Ang Sun

inome Inc.

Bellevue, WA, USA

{hkardes,dkonidena,sagrawal,mhuff,asun}@inome.com

Abstract

Entity Resolution is the task of identifying which

records in a database refer to the same entity. A

standard machine learning pipeline for the entity res-

olution problem consists of three major components:

blocking, pairwise linkage, and clustering. The

blocking step groups records by shared properties to

determine which pairs of records should be exam-

ined by the pairwise linker as potential duplicates.

Next, the linkage step assigns a probability score to

pairs of records inside each block. If a pair scores

above a user-defined threshold, the records are pre-

sumed to represent the same entity. Finally, the clus-

tering step turns the input records into clusters of

records (or profiles), where each cluster is uniquely

associated with a single real-world entity. This paper

describes the blocking and clustering strategies used

to deploy a massive database of organization entities

to power a major commercial People Search Engine.

We demonstrate the viability of these algorithms for

large data sets on a 50-node hadoop cluster.

1 Introduction

A challenge for builders of databases whose information

is culled from multiple sources is the detection of dupli-

cates, where a single real-world entity gives rise to mul-

tiple records (see (Elmagarmid, 2007) for an overview).

Entity Resolution is the task of identifying which records

in a database refer to the same entity. Online citation

indexes need to be able to navigate through the differ-

ent capitalization and abbreviation conventions that ap-

pear in bibliographic entries. Government agencies need

to know whether a record for “Robert Smith” living on

“Northwest First Street” refers to the same person as one

for a “Bob Smith” living on “1st St. NW”. In a standard

machine learning approach to this problem all records

first go through a cleaning process that starts with the re-

moval of bogus, junk and spam records. Then all records

are normalized to an approximately common representa-

tion. Finally, all major noise types and inconsistencies

are addressed, such as empty/bogus fields, field duplica-

tion, outlier values and encoding issues. At this point, all

records are ready for the major stages of the entity resolu-

tion, namely blocking, pairwise linkage, and clustering.

Since comparing all pairs of records is quadratic in the

number of records and hence is intractable for large data

sets, the blocking step groups records by shared proper-

ties to determine which pairs of records should be exam-

ined by the pairwise linker as potential duplicates. Next,

the linkage step assigns a score to pairs of records inside

each block. If a pair scores above a user-defined thresh-

old, the records are presumed to represent the same entity.

The clustering step partitions the input records into sets

of records called profiles, where each profile corresponds

to a single entity.

In this paper, we focus on entity resolution for the or-

ganization entity domain where all we have are the orga-

nization names and their relations with individuals. Let’s

first describe the entity resolution for organization names,

and discuss its significance and the challenges in more de-

tail. Our process starts by collecting billions of personal

records from three sources of U.S. records to power a ma-

jor commercial People Search Engine. Example fields

on these records might include name, address, birthday,

phone number, (encrypted) social security number, rel-

atives, friends, job title, universities attended, and orga-

nizations worked for. Since the data sources are hetero-

geneous, each data source provides different aliases of

an organization including abbreviations, preferred names,

legal names, etc. For example, Person A might have

both “Microsoft”, “Microsoft Corp”, “Microsoft Corpo-

ration”, and “Microsoft Research” in his/her profile’s or-

ganization field. Person B might have “University of

Washington”, while Person C has “UW” as the organi-

zation listed in his/her profile. Moreover, some organi-

zations change their names, or are acquired by other in-

70



stitutions and become subdivisions. There are also many

organizations that share the same name or abbreviation.

For instance, both “University of Washington”, “Univer-

sity of Wisconsin Madison”, “University of Wyoming”

share the same abbreviation, “UW”. Additionally, some

of the data sources might be noisier than the others and

there might be different kind of typos that needs to be

addressed.

Addressing the above issues in organization fields is

crucial for data quality as graphical representations of the

data become more popular. If we show different represen-

tations of the same organization as separate institutions in

a single person’s profile, it will decrease the confidence of

a customer about our data quality. Moreover, we should

have a unique representation of organizations in order to

properly answer more complicated graph-based queries

such as “how am I connected to company X?”, or “who

are my friends that has a friend that works at organization

X, and graduated from school Y?”.

We have developed novel and highly scalable com-

ponents for our entity resolution pipeline which is cus-

tomized for organizations. The focus of this paper is the

graph-based blocking and clustering components. In the

remainder of the paper, we first describe these compo-

nents in Section 2. Then, we evaluate the performance of

our entity resolution framework using several real-world

datasets in Section 3. Finally, we conclude in Section 4.

2 Methodology

In this section, we will mainly describe the blocking and

clustering strategies as they are more graph related. We

will also briefly mention our pairwise linkage model.

The processing of large data volumes requires highly

scalable parallelized algorithms, and this is only possible

with distributed computing. To this end, we make heavy

use of the hadoop implementation of the MapReduce

computing framework, and both the blocking and cluster-

ing procedures described here are implemented as a series

of hadoop jobs written in Java. It is beyond the scope of

this paper to fully describe the MapReduce framework

(see (Lin, 2010) for an overview), but we do discuss the

ways its constraints inform our design. MapReduce di-

vides computing tasks into a map phase in which the in-

put, which is given as (key,value) pairs, is split up among

multiple machines to be worked on in parallel and a re-

duce phase in which the output of the map phase is put

back together for each key to independently process the

values for each key in parallel. Moreover, in a MapRe-

duce context, recursion becomes iteration.

2.1 Blocking

How might we subdivide a huge number of organiza-

tions based on similarity or probability scores when all

we have is their names and their relation with people? We

could start by grouping them into sets according to the

words they contain. This would go a long way towards

putting together records that represent the same organiza-

tion, but it would still be imperfect because organizations

may have nicknames, abbreviations, previous names, or

misspelled names. To enhance this grouping we could

consider a different kind of information like soundex or a

similar phonetic algorithm for indexing words to address

some of the limitations of above grouping due to typos.

We can also group together the organizations which ap-

pear in the same person’s profile. This way, we will be

able to block the different representations of the same or-

ganization to some extent. With a handful of keys like this

we can build redundancy into our system to accommodate

different types of error, omission, and natural variability.

The blocks of records they produce may overlap, but this

is desirable because it gives the clustering a chance to join

records that blocking did not put together.

The above blocks will vary widely in size. For exam-

ple, we may have a small set of records containing the

word “Netflix” which can then be passed along immedi-

ately to the linkage component. However, we may have

a set of millions of records containing the word “State”

which still needs to be cut down to subsets with manage-

able sizes, otherwise it will be again impractical to do

all pairwise computations in this block. One way to do

this is to find other common properties to further subdi-

vide this set. The set of all records containing not only

“State” but also a specific state name like “Washington”

is smaller than the set of all records containing the word

“State”, and intuitively records in this set will be more

likely to represent the same organization. Additionally

we could block together all the “State” records with the

same number of words, or combination of the initials of

each word. As with the original blocks, overlap between

these sub-blocks is desirable. We do not have to be par-

ticularly artful in our choice of sub-blocking criteria: any

property that seems like it might be individuating will

do. As long as we have an efficient way to search the

space, we can let the data dynamically choose different

sub-blocking strategies for each oversize block. To this

end, we use the ordering on block keys to define a bino-

mial tree where each node contains a list of block keys

and is the parent of nodes that have keys that come later

in the ordering appended to the list. Figure 1 shows a

tree for the oversize top-level set tTkn1 with three sub-

blocking tokens sTkn1 < sTkn2 < sTkn3. With each
node of the tree we can associate a block whose key is

the list of blocks keys in that node and whose records are

the intersection of the records in those blocks, e.g. the

tTkn1 ∩ sTkn1 ∩ sTkn2 node represents all the records
for organizations containing all these tokens. Because the

cardinality of an intersected set is less than or equal to the

cardinalities of the sets that were intersected, every block

71



tTkn1

tTkn1 ∩ sTkn3tTkn1 ∩ sTkn2

tTkn1 ∩ sTkn2 ∩ sTkn3

tTkn1 ∩ sTkn1

tTkn1 ∩ sTkn1 ∩ sTkn3tTkn1 ∩ sTkn1 ∩ sTkn2

tTkn1 ∩ sTkn1 ∩ sTkn2 ∩ sTkn3

Figure 1: The root node of this tree represents an oversized block for the name Smith and the other nodes represent possible

sub-blocks. The sub-blocking algorithm enumerates the tree breadth-first, stopping when it finds a correctly-sized sub-block.

in the tree is larger than or equal to any of its children. We

traverse the tree breadth-first and only recurse into nodes

above the maximum block size. This allows us to explore

the space of possible sub-blocks in cardinality order for a

given branch, stopping as soon as we have a small enough

sub-block.

The algorithm that creates the blocks and sub-blocks

takes as input a set of records and a maximum block size

M . All the input records are grouped into blocks defined
by the top-level properties. Those top-level blocks that

are not above the maximum size are set aside. The re-

maining oversized blocks are partitioned into sub-blocks

by sub-blocking properties that the records they con-

tain share, and those properties are appended to the key.

The process is continued recursively until all sub-blocks

have been whittled down to an acceptable size. The

pseudo code of the blocking algorithm is presented in

Figure 2. We will represent the key and value pairs

in the MapReduce framework as < key; value >.
The input organization records are represented as <
INPUT FLAG, ORG NAME >. For the first iter-
ation, this job takes the organization list as input. In

later iterations, the input is the output of the previous

blocking iteration. In the first iteration, the mapper

function extracts the top-level and sub-level tokens from

the input records. It combines the organization name

and all the sub-level tokens in a temp variable called

newV alue. Next, for each top-level token, it emits this
top-level token and the newValue in the following for-

mat: < topToken, newV alue >. For the later itera-
tions, it combines each sub level token with the current

blocking key, and emits them to the reducer. Also note

that the lexicographic ordering of the block keys allows

separate mapper processes to work on different nodes in a

level of the binomial tree without creating redundant sub-

blocks (e.g. if one mapper creates a International ∩ Busi-
ness ∩ Machines block another mapper will not create a
International ∩ Machines ∩ Business one). This is nec-
essary because individual MapReduce jobs run indepen-

dently without shared memory or other runtime commu-

nication mechanisms. In the reduce phase, all the records

will be grouped together for each block key. The reducer

function iterates over all the records in a newly-created

sub-block, counting them to determine whether or not the

block is small enough or needs to be further subdivided.

The blocks that the reducer deems oversized become in-

puts to the next iteration. Care is taken that the memory

requirements of the reducer function are constant in the

size of a fixed buffer because otherwise the reducer runs

out of memory on large blocks. Note that we create a

black list from the high frequency words in organization

names, and we don’t use these as top-level properties as

such words do not help us with individuating the records.

More formally, this process can be understood in terms

of operations on sets. In a set of N records there are
1

2
N(N − 1) unique pairs, so an enumeration over all

of them is O(N2). The process of blocking divides this
original set into k blocks, each of which contains at most
a fixed maximum of M records. The exhaustive compar-
ison of pairs from these sets is O(k), and the constant
factors are tractable if we choose a small enough M . In
the worst case, all the sub-blocks except the ones with the

very longest keys are oversize. Then the sub-blocking al-

gorithm will explore the powerset of all possible block-

ing keys and thus have exponential runtime. However, as

the blocking keys get longer, the sets they represent get

smaller and eventually fall beneath the maximum size. In

practice these two countervailing motions work to keep

this strategy tractable.

2.2 Pairwise Linkage Model

In this section, we give just a brief overview of our pair-

wise linkage system as a detailed description and evalua-

tion of that system is beyond the scope of this paper.

We take a feature-based classification approach to pre-

dict the likelihood of two organization names < o1, o2 >
referring to the same organization entity. Specifically, we

use the OpenNLP1 maximum entropy (maxent) package

as our machine learning tool. We choose to work with

maxent because the training is fast and it has a good sup-

port for classification. Regarding the features, we mainly

have two types: surface string features and context fea-

tures. Examples of surface string features are edit dis-

1http://opennlp.apache.org/

72



Blocking Iterations

map(key, value)

if(key.equals(EntityName)
String[] tokens ← value.split(“ ”)
sublevelTokenSet ← ∅

toplevelTokenSet ← ∅

for each (token ∈ tokens)
sublevelTokenSet.add(token.hashCode())

if(notExist(blackList, token))
toplevelTokenSet.add(token.hashCode())

String newValue ← value

for each (sToken ∈ sublevelTokenSet)
newValue ← newV alue.append(STR + sToken)

for each (tToken ∈ toplevelTokenSet)
emit(tToken, newV alue)

else

String[] keyTokens ← key.split(STR)
String[] valueTokens ← value.split(STR)
for each (token ∈ valueTokens)

if(token > keyTokens[keyTokens.length − 1])
emit(key.append(STR + token), value)

reduce(key, < iterable > values)
buffer ← ∅

for each (value ∈ values)
buffer.add(value)
if(buffer.length ≥ MAXBLOCKSIZE)

break

if(buffer.length ≥ MAXBLOCKSIZE)
for each (ele ∈ buffer)

emit(key, ele)
for each (value ∈ values)

emit(key, value)
elseif(buffer.length ≥ 1)

blocks.append(key, buffer)

Figure 2: Alg.1 - Blocking

tance of the two names, whether one name is an abbre-

viation of the other name, and the longest common sub-

string of the two names. Examples of context features are

whether the two names share the same url and the number

of times that the two names co-occur with each other in a

single person record.

2.3 Clustering

In this section, we present our clustering approach. Let’s,

first clarify a set of terms/conditions that will help us de-

scribe the algorithms.

Definition (Connected Component): Let G = (V, E)
be an undirected graph where V is the set of vertices and

E is the set of edges. C = (C1, C2, ..., Cn) is the set of
disjoint connected components in this graph where (C1∪
C2 ∪ ... ∪ Cn) = V and (C1 ∩ C2 ∩ ... ∩ Cn) = ∅. For
each connected component Ci ∈ C, there exists a path in
G between any two vertices vk and vl where (vk, vl) ∈
Ci. Additionally, for any distinct connected component

sClust 

Transitive 

Closure Edge List 

Node - 

ClusterID 

mapping 

anyClus

ter > 

maxSize 

no 

yes 
Extract Pairs 

Figure 3: Clustering Component

(Ci, Cj) ∈ C, there is no path between any pair vk and
vl where vk ∈ Ci, vl ∈ Cj . Moreover, the problem of
finding all connected components in a graph is finding

the C satisfying the above conditions. �

Definition (Component ID): A component id is a

unique identifier assigned to each connected component.

Definition (Max Component Size): This is the maxi-

mum allowed size for a connected component. �

Definition (Cluster Set): A cluster set is a set of

records that belong to the same real world entity. �

Definition (Max Cluster Size): This is the maximum

allowed size for a cluster. �

Definition (Match Threshold): Match threshold is a

score where pairs scoring above this score are said to rep-

resent the same entity. �

Definition (No-Match Threshold): No-Match thresh-

old is a score where pairs scoring below this score are said

to represent different entities. �

Definition (Conflict Set): Each record has a conflict

set which is the set of records that shouldn’t appear with

this record in any of the clusters. �

The naive approach to clustering for entity resolu-

tion is transitive closure by using only the pairs having

scores above the match threshold. However, in practice

we might see many examples of conflicting scores. For

example, (a,b) and (b,c) pairs might have scores above

match threshold while (a,c) pair has a score below no-

match threshold. If we just use transitive closure, we

will end up with a single cluster with these three records

(a,b,c). Another weakness of the regular transitive clo-

sure is that it creates disjoint sets. However, organiza-

tions might share name, or abbreviation. So, we need a

soft clustering approach where a record might be in dif-

ferent clusters.

On the other hand, the large volume of our data re-

quires highly scalable and efficient parallelized algo-

rithms. However, it is very hard to implement par-

73



TC-Iterate TC-Dedup 
Edge List 

Node - 

ComponentID 

mapping 

newPair 

> 0 
no 

yes 

iterationID=1 

iterationID>1 

Figure 4: Transitive Closure Component

allelized clustering approaches with high precision for

large scale graphs due to high time and space complexi-

ties (Bansal, 2003). So, we propose a two-step approach

in order to build both a parallel and an accurate clustering

framework. The high-level architecture of our cluster-

ing framework is illustrated in Figure 3. We first find the

connected components in the graph with our MapReduce

based transitive closure approach, then further, partition

each connected component in parallel with our novel soft

clustering algorithm, sClust. This way, we first combine

similar record pairs into connected components in an effi-

cient and scalable manner, and then further partition each

connected component into smaller clusters for better pre-

cision. Note that there is a dangerous phenomenon, black

hole entities, in transitive closure of the pairwise scores

(Michelson, 2009). A black hole entity begins to pull

an inordinate amount of records from an increasing num-

ber of different true entities into it as it is formed. This

is dangerous, because it will then erroneously match on

more and more records, escalating the problem. Thus, by

the end of the transitive closure, one might end up with

black hole entities with millions of records belonging to

multiple different entities. In order to avoid this problem,

we define a black hole threshold, and if we end up with

a connected component above the size of the black hole

threshold, we increment the match threshold by a delta

and further partition this black hole with one more tran-

sitive closure job. We repeat this process until the sizes

of all the connected components are below the black hole

threshold, and then apply sClust on each connected com-

ponent. Hence at the end of the entire entity resolution

process, the system has partitioned all the input records

into cluster sets called profiles, where each profile corre-

sponds to a single entity.

2.4 Transitive Closure

In order to find the connected components in a graph, we

developed the Transitive Closure (TC) module shown in

Figure 4. The input to the module is the list of all pairs

having scores above the match threshold. As an output

from the module, what we want to obtain is the mapping

from each node in the graph to its corresponding com-

ponentID. For simplicity, we use the smallest node id in

each connected component as the identifier of that com-

ponent. Thus, the module should output a mapping table

from each node in the graph to the smallest node id in

its corresponding connected component. To this end, we

designed a chain of two MapReduce jobs, namely, TC-

Transitive Closure Iterate

map(key, value)

emit(key, value)
emit(value, key)

reduce(key, < iterable > values)
minV alue ← values.next()
if(minV alue < key)

emit(key, minV alue)
for each (value ∈ values)

Counter.NewPair.increment(1)
emit(value, minV alue)

(a) Transitive Closure - Iterate

Transitive Closure Dedup

map(key, value)

emit(key.append(STR + value), null)

reduce(key, < iterable > values)
String[] keyTokens ← key.split(STR)
emit(keyTokens[0], keyTokens[1])

(b) Transitive Closure - Dedup

Figure 5: Alg.3 - Transitive Closure

Iterate, and TC-Dedup, that will run iteratively till we

find the corresponding componentIDs for all the nodes

in the graph.

TC-Iterate job generates adjacency lists AL =
(a1, a2, ..., an) for each node v, and if the node id of this
node vid is larger than the min node id amin in the adja-
cency list, it first creates a pair (vid, amin) and then a pair
for each (ai, amin) where ai ∈ AL, and ai �= amin. If
there is only one node in AL, it means we will generate
the pair that we have in previous iteration. However, if

there is more than one node in AL, it means we might
generate a pair that we didn’t have in the previous itera-

tion, and one more iteration is needed. Please note that,

if vid is smaller than amin, we don’t emit any pair.

The pseudo code of TC-Iterate is given in Figure 5-

(a). For the first iteration, this job takes the pairs having

scores above the match threshold from the initial edge list

as input. In later iterations, the input is the output of TC-

Dedup from the previous iteration. We first start with the

initial edge list to construct the first degree neighborhood

of each node. To this end, for each edge < a; b >, the
mapper emits both < a; b >, and < b; a > pairs so that
a should be in the adjacency list of b and vice versa. In
the reduce phase, all the adjacent nodes will be grouped

together for each node. Reducers don’t receive the values

in a sorted order. So, we use a secondary sort approach

to pass the values to the reducer in a sorted way with cus-

tom partitioning (see (Lin, 2010) for details). This way,

the first value becomes the minValue. If the minValue is

larger than the key, we don’t emit anything. Otherwise,

74



Bring together all edges for each partition

Phase-1

map(key, value)

if(key.equals(ConflationOutput))
if ((value.score ≤ NO MATCH THR)||

(value.score ≥ MATCH THR))
emit(value.entity1, value)

else //TCDedupOutput
temp.entity1 ← value
temp.entity2 ← null
temp.score ← null
emit(key, temp)
emit(value, temp)

reduce(key, < iterable > values)
valueList ← ∅
for each (value ∈ values)

if(value.entity2 = null)
clusID ← value.entity1

else

valueList.add(value)
for each (value ∈ valueList)

emit(clusID, value)

Phase-2

map(key, value)

emit(key, value)
reduce(key, < iterable > values)

valueList ← ∅
for each (value ∈ values)

valueList.add(value)
emit(key, valueList)

Figure 6: Alg.3 - Bring together all edges for each partition

we first emit the < key; minV alue > pair. Next, we
emit a pair for all other values as < value; minV alue >,
and increase the global NewPair counter by 1. If the

counter is 0 at the end of the job, it means that we found

all the components and there is no need for further itera-

tions.

During the TC-Iterate job, the same pair might be emit-

ted multiple times. The second job, TC-Dedup, just dedu-

plicates the output of the CCF-Iterate job. This job in-

creases the efficiency of TC-Iterate job in terms of both

speed and I/O overhead. The pseudo code for this job is

given in Figure 5-(b).

The worst case scenario for the number of necessary

iterations is d+1 where d is the diameter of the net-

work. The worst case happens when the min node in

the largest connected component is an end-point of the

largest shortest-path. The best case scenario takes d/2+1

iterations. For the best case, the min node should be at

the center of the largest shortest-path.

2.5 sClust: A Soft Agglomerative Clustering

Approach

After partitioning the records into disjoint connected

components, we further partition each connected compo-

nent into smaller clusters with sClust approach. sClust

is a soft agglomerative clustering approach, and its main

difference from any other hierarchical clustering method

is the “conflict set” term that we described above. Any of

the conflicting nodes cannot appear in a cluster with this

approach. Additionally, the maximum size of the clusters

can be controlled by an input parameter.

First as a preprocessing step, we have a two-step

MapReduce job (see Figure 6) which puts together and

sorts all the pairwise scores for each connected compo-

nent discovered by transitive closure. Next, sClust job

takes the sorted edge lists for each connected component

as input, and partitions each connected component in par-

allel. The pseudo-code for sClust job is given in Figure 7.

sClust iterates over the pairwise scores twice. During the

first iteration, it generates the node structures, and conflict

sets for each of these structures. For example, if the pair-

wise score for (a, b) pair is below the no-match threshold,
node a is added to node b’s conflict set, and vice versa. By
the end of the first iteration, all the conflict sets are gen-

erated. Now, one more pass is needed to build the final

clusters. Since the scores are sorted, we start from the

highest score to agglomeratively construct the clusters by

going over all the scores above the match threshold. Let’s

assume we have a pair (a, b) with a score above the match
threshold. There might be 4 different conditions. First,

both node a and node b are not in any of the clusters yet.
In this case, we generate a cluster with these two records

and the conflict set of this cluster becomes the union of

conflict sets of these two records. Second, node a might
already be assigned to a set of clusters C’ while node b is
not in any of the clusters. In these case, we add node b to
each cluster in C’ if it doesn’t conflict with b. If there is
no such cluster, we build a new cluster with nodes a and
b. Third is the opposite version of the second condition,
and the procedure is the same. Finally, both node a and
node b might be in some set of clusters. If they already
appear in the same cluster, no further action needed. If

they just appear in different clusters, these clusters will

be merged as long as there is no conflict between these

clusters. If there are no such unconflicting clusters, we

again build a new cluster with nodes a and b. This way,
we go over all the scores above the match threshold and

build the cluster sets. Note that if the clusters are merged,

their conflict sets are also merged. Additionally, if the

max cluster size parameter is defined, this condition is

also checked before merging any two clusters, or adding

a new node to an existing cluster.

75



Clustering

map(key, valueList)

for each (value ∈ valueList)
if(value.score ≥ MATCH THR)

nodes.insert(value.entity1)
nodes.insert(value.entity2)

else

node1Index ← find(value.entity1, nodes)
node2Index ← find(value.entity2, nodes)
nodes[node1Index].conflictSet.insert(node2Index)
nodes[node2Index].conflictSet.insert(node1Index)

for each (value ∈ valueList)
if(value.score ≥ MATCH THR)

node1Index ← find(value.entity1, nodes)
node2Index ← find(value.entity2, nodes)
node1ClusIDLength ← nodes[node1Index].clusIDs.length
node2ClusIDLength ← nodes[node2Index].clusIDs.length
if((node1ClusIDLength = 0) && (node2ClusIDLength = 0))

clusters[numClusters].nodes[0] ← node1Index
clusters[numClusters].nodes[1] ← node2Index
clusters[numClusters].confSet ←

mergeSortedLists(nodes[node1Index].confSet, nodes[node2Index].confSet)
nodes[node1Index].clusIDs.insert(numClusters)
nodes[node2Index].clusIDs.insert(numClusters)
numClusters++

elseif(node1ClusIDLength = 0)
for each (node2ClusID ∈ nodes[node2Index].clusIDs)

if(notContain(clusters[node2ClusID].confSet, node1Index))
insertToSortedList(clusters[node2ClusID].nodes, node1Index)
clusters[node2ClusID].confSet ←

mergeSortedLists(clusters[node2ClusID].confSet, nodes[node1Index].confSet)
nodes[node1Index].clusIDs.insert(node2ClusID)

elseif(node2ClusIDLength = 0)
for each (node1ClusID ∈ nodes[node1Index].clusIDs)

if(notContain(clusters[node1ClusID].confSet, node2Index))
insertToSortedList(clusters[node1ClusID].nodes, node2Index)
clusters[node1ClusID].confSet ←

mergeSortedLists(clusters[node1ClusID].confSet, nodes[node2Index].confSet)
nodes[node2Index].clusIDs.insert(node1ClusID)

elseif(notIntersect(clusters[node1ClusID].clusIDs, clusters[node2ClusID].clusIDs))
for each (node1ClusID ∈ nodes[node1Index].clusIDs)

for each (node2ClusID ∈ nodes[node2Index].clusIDs)
if( notIntersect(clusters[node1ClusID].confSet, clusters[node2ClusID].nodes) &&

notIntersect(clusters[node2ClusID].confSet, clusters[node1ClusID].nodes) )
clusters[node1ClusID].nodes ←

mergeSortedList(clusters[node1ClusID].nodes, clusters[node2ClusID].nodes)
clusters[node1ClusID].confSet ←

mergeSortedLists(clusters[node1ClusID].confSet, clusters[node2ClusID].confSet)
for each (nodeIndex ∈ clusters[node2ClusID].nodes)

nodes[nodeIndex].clusIDs.insert(node1ClusID)
clusters[node2ClusID].isRemoved ← true
clusters[node2ClusID].nodes ← null
clusters[node2ClusID].confSet ← null

Figure 7: Alg.4 - Clustering

76



(a) Block Dist. (iterations)

(b) Block Dist. (overall)

(c) Component & Cluster Size Dist.

Figure 8: Size Distributions

3 Evaluation

In this section, we present the experimental results for

our entity resolution framework. We ran the experiments

on a hadoop cluster consisting of 50 nodes, each with 8

cores. There are 10 mappers, and 6 reducers available

at each node. We also allocated 3 Gb memory for each

map/reduce task.

We used two different real-world datasets for our ex-

periments. The first one is a list of 150K organizations

along with their aliases provided by freebase2. By using

this dataset, we both trained our pairwise linkage model

and measured the precision and recall of our system. We

randomly selected 135K organizations from this list for

the training. We used the rest of the organizations to mea-

2http://www.freebase.com/

precision recall f-measure

Pairwise Classifier 97 63 76

Transitive Closure 64 98 77

sClust 95 76 84

Table 1: Performance Comparison

sure the performance of our system. Next, we generated

positive examples by exhaustively generating a pair be-

tween all the aliases. We also randomly generated equal

number of negative examples among pairs of different

organization alias sets. We trained our pairwise classi-

fier with the training set, then ran it on the test set and

measured its performance. Next, we extracted all the or-

ganization names from this set, and ran our entire entity

resolution pipeline on top of this set. Table 1 presents

the performance results. Our pairwise classifier has 97%

precision and 63% recall when we use a match threshold

of 0.65. Using same match threshold, we then performed

transitive closure. We also measured the precision and

recall numbers for transitive closure as it is the naive ap-

proach for the entity resolution problem. Since transitive

closure merges records transitively, it has very high recall

but the precision is just 64%. Finally, we performed our

sClust approach with the same match threshold. We set

the no-match threshold to 0.3. The pairwise classifier has

slightly better precision than sClust but sClust has much

better recall. Overall, sClust has a much better f-measure

than both the pairwise classifier and transitive closure.

Second, we used our production set to show the viabil-

ity of our framework. In this set, we have 68M organiza-

tion names. We ran our framework on this dataset. Block-

ing generated 14M unique blocks, and there are 842M

unique comparisons in these blocks. The distribution of

the block sizes presented in Figure 8-(a) and (b). Block-

ing finished in 42 minutes. Next, we ran our pairwise

classifier on these 842M pairs and it finished in 220 min-

utes. Finally, we ended up with 10M clusters at the end

of the clustering stage which took 3 hours. The distribu-

tion of the connected components and final clusters are

presented in Figure 8-(c).

4 Conclusion

In this paper, we presented a novel entity resolution ap-

proach for the organization entity domain. We have im-

plemented this in the MapReduce framework with low

memory requirements so that it may scale to large scale

datasets. We used two different real-world datasets in our

experiments. We first evaluated the performance of our

approach on truth data provided by freebase. Our cluster-

ing approach, sClust, significantly improved the recall of

the pairwise classifier. Next, we demonstrated the viabil-

ity of our framework on a large scale dataset on a 50-node

hadoop cluster.

77



References

A. K. Elmagarmid, P. G. Iperirotis, and V. S. Verykios. Dupli-

cate record detection: A survey. In IEEE Transactions on

Knowledge and Data Engineering, pages 1–16, 2007.
J. Lin and C. Dyer. Data-Intensive Text Processing with

MapReduce. Synthesis Lectures on Human Langugage

Technologies. Morgan & Claypool, 2010.
N. Bansal, A. Blum and S. Chawla Correlation Clustering. Ma-

chine Learning, 2003.
M. Michelson and S.A. Macskassy Record Linkage Measures

in an Entity Centric World. In Proceedings of the 4th work-

shop on Evaluation Methods for Machine Learning, Mon-

treal, Canada, 2009.
N. Adly. Efficient record linkage using a double embedding

scheme. In R. Stahlbock, S. F. Crone, and S. Lessmann, edi-

tors, DMIN, pages 274–281. CSREA Press, 2009.
A. N. Aizawa and K. Oyama. A fast linkage detection scheme

for multi-source information integration. In WIRI, pages 30–

39. IEEE Computer Society, 2005.
R. Baxter, P. Christen, and T. Churches. A comparison of fast

blocking methods for record linkage, 2003.
M. Bilenko and B. Kamath. Adaptive blocking: Learning to

scale up record linkage. In Data Mining, 2006. ICDM’, num-

ber December, 2006.
P. Christen. A survey of indexing techniques for scalable record

linkage and deduplication. IEEE Transactions on Knowl-

edge and Data Engineering, 99(PrePrints), 2011.
T. de Vries, H. Ke, S. Chawla, and P. Christen. Robust record

linkage blocking using suffix arrays. In Proceedings of the

18th ACM conference on Information and knowledge man-

agement, CIKM ’09, pages 305–314, New York, NY, USA,

2009. ACM.
T. de Vries, H. Ke, S. Chawla, and P. Christen. Robust record

linkage blocking using suffix arrays and bloom filters. ACM

Trans. Knowl. Discov. Data, 5(2):9:1–9:27, Feb. 2011.
M. A. Hernandez and S. J. Stolfo. Real-world data is dirty. data

cleansing and the merge/purge problem. Journal of Data

Mining and Knowledge Discovery, pages 1–39, 1998.
L. Jin, C. Li, and S. Mehrotra. Efficient record linkage in large

data sets. In Proceedings of the Eighth International Confer-

ence on Database Systems for Advanced Applications, DAS-

FAA ’03, pages 137–, Washington, DC, USA, 2003. IEEE

Computer Society.
A. McCallum, K. Nigam, and L. H. Ungar. Efficient clustering

of high-dimensional data sets with application to reference

matching. In Proceedings of the ACM International Confer-

ence on Knowledge Discover and Data Mining, pages 169–

178, 2000.
J. Nin, V. Muntes-Mulero, N. Martinez-Bazan, and J.-L.

Larriba-Pey. On the use of semantic blocking techniques for

data cleansing and integration. In Proceedings of the 11th

International Database Engineering and Applications Sym-

posium, IDEAS ’07, pages 190–198, Washington, DC, USA,

2007. IEEE Computer Society.
A. D. Sarma, A. Jain, and A. Machanavajjhala. CBLOCK:

An Automatic Blocking Mechanism for Large-Scale De-

duplication Tasks. Technical report, 2011.
M. Weis, F. Naumann, U. Jehle, J. Lufter, and H. Schuster.

Industry-scale duplicate detection. Proc. VLDB Endow.,

1(2):1253–1264, Aug. 2008.

S. E. Whang, D. Menestrina, G. Koutrika, M. Theobald, and

H. Garcia-Molina. Entity resolution with iterative blocking.

In Proceedings of the 35th SIGMOD international confer-

ence on Management of data, SIGMOD ’09, pages 219–232,

New York, NY, USA, 2009. ACM.

W. Winkler. Overview of record linkage and current research

directions. Technical report, U.S. Bureau of the Census,

2006.

S. Yan, D. Lee, M.-Y. Kan, and L. C. Giles. Adaptive sorted

neighborhood methods for efficient record linkage. In Pro-

ceedings of the 7th ACM/IEEE-CS joint conference on Dig-

ital libraries, JCDL ’07, pages 185–194, New York, NY,

USA, 2007. ACM.

L. Kolb, and E. Rahm. Parallel entity resolution with Dedoop.

Datenbank-Spektrum (2013): 1-10.

B. McNeill, H. Kardes, and A. Borthwick. Dynamic

Record Blocking: Efficient Linking of Massive Databases in

MapReduce. In QDB (2012).

78


