










































A Hybrid Stepwise Approach for De-identifying Person Names in Clinical Documents


Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 65–72,
Montréal, Canada, June 8, 2012. c©2012 Association for Computational Linguistics

A Hybrid Stepwise Approach for De-identifying Person Names  
in Clinical Documents  

 
Oscar Ferrández1,2, Brett R. South1,2, Shuying Shen1,2, Stéphane M. Meystre1,2 

1 Department of Biomedical Informatics, University of Utah, Salt Lake City, Utah, USA 
2 IDEAS Center SLCVA Healthcare System, Salt Lake City, Utah, USA 

oscar.ferrandez@utah.edu, 
{brett.south,shuying.shen,stephane.meystre}@hsc.utah.edu 

 
 

Abstract 

As Electronic Health Records are growing ex-
ponentially along with large quantities of un-
structured clinical information that could be 
used for research purposes, protecting patient 
privacy becomes a challenge that needs to be 
met. In this paper, we present a novel hybrid 
system designed to improve the current strate-
gies used for person names de-identification. 
To overcome this task, our system comprises 
several components designed to accomplish 
two separate goals: 1) achieve the highest re-
call (no patient data can be exposed); and 2) 
create methods to filter out false positives. As 
a result, our system reached 92.6% F2-
measure when de-identifying person names in 
Veteran’s Health Administration clinical 
notes, and considerably outperformed other 
existing “out-of-the-box” de-identification or 
named entity recognition systems.  

 

1 Introduction 

Electronic Healthcare Records are invaluable re-
sources for clinical research, however they contain 
highly sensitive Protected Health Information 
(PHI) that must remain confidential. In the United 
States, patient confidentiality is regulated by the 
Health Insurance Portability and Accountability 
Act (HIPAA). To share and use clinical documents 
for research purposes without patient consent, 
HIPAA requires prior removal of PHI. More spe-
cifically, the HIPAA “Safe Harbor”1 determines 18 

                                                
1 GPO US: 45 C.F.R. § 164 Security and Privacy. 
http://www.access.gpo.gov/nara/cfr/waisidx_08/45cfr164_08.html 
Further details about the 18 HIPAA Safe Harbor PHI identifi-
ers can be also found in (Meystre et al., 2010). 

PHI categories that have to be obscured in order to 
consider clinical data de-identified. 

An ideal de-identification system should recog-
nize PHI accurately, but also preserve relevant 
non-PHI clinical data, so that clinical records can 
later be used for various clinical research tasks. 

Of the 18 categories of PHI listed by HIPAA, 
one of the most sensitive is patient names, and all 
person names in general. Failure to de-identify 
such PHI involves a high risk of re-identification, 
and jeopardizes patient privacy. 

In this paper, we describe our effort to satisfac-
torily de-identify person names in Veteran’s Health 
Administration (VHA) clinical documents. We 
propose improvements in person names de-
identification with a pipeline of processes tailored 
to the idiosyncrasies of clinical documents. This 
effort was realized in the context of the develop-
ment of a best-of-breed clinical text de-
identification system (nicknamed “BoB”), which 
will be released as an open source software pack-
age, and it started with the implementation and 
evaluation of several existing de-identification and 
Named Entity Recognition (NER) systems recog-
nizing person names. We then devised a novel 
methodology to better tackle this task and improve 
performance. 
 

2 Background and related work  

In many aspects de-identification resembles tradi-
tional NER tasks (Grishman and Sundheim, 1996). 
NER involves detecting entities such as person 
names, locations, and organizations. Consequently, 
given the similar entities targeted by both tasks, 
NER systems can be relevant to de-identify docu-
ments. However, most named entity recognizers 
were developed for newswire articles, and not for 
clinical narratives. Clinical records are character-

65



ized by fragmented and incomplete utterances, lack 
of punctuation marks and formatting, as well as 
domain specific language. These complications, in 
addition to the fact that some entities can appear 
both as PHI and non-PHI in the same document 
(e.g., “Mr. Epley” vs. “the Epley maneuver”), 
make clinical text de-identification a challenging 
task. Therefore, although person names de-
identification is essentially NER, the unique char-
acteristics of clinical texts make it more interesting 
and challenging than recognizing names in news 
articles, which also enhance the motivation for this 
study. 

Several different approaches were proposed to 
deal with de-identification of clinical documents, 
and for named entity recognition of person names. 
These approaches are mainly focused on either 
pattern matching techniques, or statistical methods 
(Meystre et al., 2010), as exemplified below. 

Beckwith et al. (2006) developed a de-
identification system for pathology reports. This 
system implemented some patterns to detect dates, 
locations, and ID numbers, as well as a database of 
proper names and well-known markers such as 
‘Mr.’ and ‘PhD’ to find person names. 

Friedlin and McDonald (2008) described the 
Medical De-identification System (MeDS). It used 
a combination of methods including heuristics, 
pattern matching, and dictionary lookups to identi-
fy PHI. Pattern matching through regular expres-
sions was used to detect numerical identifiers, 
dates, addresses, ages, etc.; while for names, 
MeDS used lists of proper names, common usage 
words and predictive markers, as well as a text 
string nearness algorithm to deal with typograph-
ical errors.  

Neamatullah et al. (2008) proposed another rule-
based de-identification approach focused on pat-
tern matching via dictionary lookups, regular ex-
pressions and context checks heuristics denoting 
PHI. Dictionaries made up of ambiguous names 
and locations that could also be non-PHI, as well 
as dictionaries of common words were used by this 
system to disambiguate PHI terms. 

Other de-identification systems such as 
(Aberdeen et al., 2010; Gardner and Xiong, 2009) 
use machine learning algorithms to train models 
and predict new annotations. The key aspect of 
these systems is the selection of the learning algo-
rithm and features. Both (Aberdeen et al., 2010) 
and (Gardner and Xiong, 2009) use an implemen-

tation of Conditional Random Fields (CRF) and a 
set of learning features based on the morphology of 
the terms and their context. One disadvantage of 
these systems is the need for large amounts of an-
notated training examples. 

As mentioned previously, for detecting person 
names, we could also use traditional newswire-
trained NER systems. NER has long been studied 
by the research community and many different ap-
proaches have been developed (Tjong Kim Sang 
and De Meulder, 2003; Doddington et al., 2004). 
One successful and freely available named entity 
recognizer is the Stanford NER system (Finkel et 
al., 2005), which provides an implementation of 
linear chain CRF sequence models, coupled with 
well-engineered feature extractors for NER, and 
trained with newswire documents. 

 

3 Methods 

As already mentioned, we first selected and ran 
several existing de-identification and NER systems 
detecting person names in our clinical documents. 
Afterwards, we devised and present here a novel 
pipeline of processes designed to improve the PHI 
recognition task. 

3.1 Existing de-identification and NER sys-
tems 

Five available de-identification systems, as well as 
one newswire-trained named entity recognizer, 
were selected for an “out-of-the-box” evaluation. 
The aim of this evaluation was to compare the per-
formance of the various methods and resources 
when de-identifying person names in our clinical 
documents. 

We included three rule-based de-identification 
approaches:  
• HMS Scrubber (Beckwith et al., 2006); 
• MeDS (Friedlin and McDonald, 2008); and 
• MIT deid system (Neamatullah et al., 2008). 

 
And two systems based on machine learning 

classifiers: 
• The MITRE Identification Scrubber Toolkit 

(MIST) (Aberdeen et al., 2010); and  
• The Health Information DE-identification 

(HIDE) system (Gardner and Xiong, 2009). 
 

66



Regarding NER systems, we chose the Stanford 
NER system (Finkel et al., 2005), which has re-
ported successful results when detecting person 
names. These systems were described in Section 2, 
when we presented related work. 

3.2 Our best-of-breed approach 

Our names de-identification approach consists of a 
novel pipeline of processes designed to improve 
the current strategies for person names de-
identification. This system is being developed as 
an Apache UIMA2 pipeline, with two main goals:  
 
1) Obtain the highest recall (i.e., sensitivity), re-

gardless of the impact on precision; and  
2) Improve overall precision by filtering out the 

false positives produced previously.  
 
These goals correspond to the implementation of 

the main components of our system. When we 
tested existing systems (we will present results for 
these systems in Table 1), we observed that recall 
was better addressed by rule-based approaches, 
while precision was higher applying machine 
learning-based algorithms. We therefore used this 
knowledge for the design of our system: goal#1 is 
then accomplished mainly using rule-based tech-
niques, and goal#2 implementing machine learn-
ing-based approaches. 

Moreover, recall is of paramount importance in 
de-identification (patient PHI cannot be disclosed). 
And this was also a reason that motivated us to 
first focus on achieving high recall, and filtering 
out false positives afterwards as a separate proce-
dure.  

Unlike other de-identification and NER systems 
that tackle the classification problem from one per-
spective (i.e., rule-based or machine learning-
based) or from a limited combined approach (e.g., 
learning features extracted using regular expres-
sions), the design of our system allows us to take 
advantage of the strong points of both techniques 
separately. And more importantly, our classifiers 
for filtering out false-positives (goal#2) are trained 
using correct and incorrect annotations derived 
from previous modules implemented in goal#1. 
Thus, they do not predict if every token in the doc-
ument is or belongs to a PHI identifier, they in-
stead decide if an actual annotation is a false or 
                                                
2 http://uima.apache.org/ 

true positive. This design makes our classifiers 
better with less learning examples, which is a re-
striction we have to deal with, and it also allows us 
to create methods that can be only focused on max-
imizing recall regardless of the amount of false-
positives introduced (goal#1). To the best of our 
knowledge, this perspective has not been exploited 
before, and as we will show in the evaluation sec-
tion, it empirically demonstrates more robustness 
than previous approaches. 

The design of our system integrates different 
components described below. Figure 1 depicts an 
overview of our system’s architecture and work-
flow. 

 

 
 

Figure 1. System’s architecture. 
 

3.2.1 NLP preprocessing steps 

This NLP preprocessing prepares the input for the 
main components of our system. It includes sen-
tence segmentation, tokenization, part-of-speech 
tagging, chunking, and word normalization based 

67



on Lexical Variant Generation (LVG)3. The output 
of this preprocessing will be used by subsequent 
pattern matching techniques and features for ma-
chine learning algorithms. For these processes, we 
adapted several cTAKES (Savova et al., 2010) 
components. 

3.2.2 Rules and dictionary lookups 

We created a pattern matching component support-
ed by contextual keyword searches (e.g., “Dr.”, 
“Mr.”, “M.D.”, “R.N.”, “L.C.S.W.”), dictionaries 
of person names4, and a simple disambiguation 
procedure based on a list of common words and 
the capitalization of the entity. We adapted some 
of the techniques implemented in (Beckwith et al., 
2006; Friedlin and McDonald, 2008; Neamatullah 
et al., 2008) to our documents, and developed new 
patterns. For dictionary lookups, we used Lucene5 
indexing, experimenting with keyword and fuzzy 
dictionary searches. Each word token is compared 
with our indexed dictionary of names (last and first 
names from the 1990 US Census4), considering all 
matches as candidate name annotations. However, 
candidates that also match with an entry in our dic-
tionary of common words6 and do not contain an 
initial capital letter are discarded from this set of 
candidate name annotations. 

With this component, we attempt to maximize 
recall, even if precision is altered. 

3.2.3 CRF-based predictions 

To further enhance recall, we created another com-
ponent based on CRF models. We incorporated 
this component in our system considering that ma-
chine learning classifiers are more generalizable 
and can detect instances of names that are not sup-
ported by our rules or dictionaries. Therefore, alt-
hough we knew the individual results of a CRF 
classifier at this level were not enough for de-
identification, at this point our main concern is to 
obtain the highest recall. Thus, adding a machine 
learning classifier into this level we could help the 
system predicting the PHI formats and instances 
                                                
3 http://lexsrv2.nlm.nih.gov/LexSysGroup/Projects/lvg/      
current/web/index.html 
4 Frequently Occurring Names from the 1990 Census. 
http://www.census.gov/genealogy/names. 
5 http://lucene.apache.org/java/docs/index.html 
6 We used the dictionary of common words from Neamatullah 
et al. (2008). 

that could not be covered by our patterns and dic-
tionaries. 

To develop this component, we used the CRF 
classifier implementation provided by the Stanford 
NLP group7. We carried out a feature selection 
procedure using greedy forward selection. It pro-
vided us with the best learning feature set, which 
consisted of: the target word, 2-grams of letters, 
position in the document, part-of-speech tag, lem-
ma, widely-used word-shape features (e.g., initial 
capitals, all capitals, digits inside, etc.), features 
from dictionaries of names and common words, a 
2-word context window, and combinations of 
words, word-shapes and part-of-speech tags of the 
word and its local context. 

The learning features considered before and af-
ter the selection procedure are shown in Table 1. 

3.2.4 False-positive filtering 

The two previous components’ objective is maxi-
mal recall, producing numerous false positives. 
The last component of our pipeline was therefore 
designed to filter out these false positives and con-
sequently increase overall precision. We built a 
machine learning classifier for this task, based on 
LIBSVM (Chang and Lin, 2001), a library for 
Support Vector Machines (SVM), with the RBF 
(Radial Basis Function) kernel. We then trained 
this classifier with reference standard text annota-
tions, as well as the correct and incorrect annota-
tions made by the previous components. We used 
our training document set (section 4.1) for this 
purpose. 
Features for the LIBSVM machine learning model 
were: the LVG normalized form of the target anno-
tation, three words before and after, part-of-speech 
tags of the words within the annotation and the 
local context, number of tokens within the annota-
tion, position in the document, 40 orthographic 
features (denoting capitals, digits, special charac-
ters, etc.), features from dictionaries of names and 
common words, and the previous strategy used to 
make the annotation (i.e., rules, dictionary lookups 
or CRF-based predictions). 

                                                
7 http://nlp.stanford.edu/software/corenlp.shtml 

68



Feature Description Selected* 
target word The word to classify as person name Yes 
2-grams of letters Features from the 2-grams of letters from the word Yes 
3-grams of letters Features from the 3-grams of letters from the word No 
4-grams of letters Features from the 4-grams of letters from the word No 

lowercase n-grams Features from the n-grams of letters from the word in  lowercase (considering 2-, 3-, and 4-grams separately) No 

position Position of the word within a sentence Yes 
PoS Part-of-speech tag of the word Yes 
lemma Lemma of the word Yes 

word shape 

Initial capital 

Yes 

All capitals 
Mix of uppercase and lowercase letters 

Digits inside 
All digits 
Has dash 
End dash 

Alpha-numeric 
Numeric-alpha (starts with a number) 

Contains punctuation mark 

dictionaries 
Does the word match with an entry of the dictionary of names? Yes 

Does the word match with an entry of the dictionary of  
common words? Yes 

2-word window The two preceding and following words in the context Yes 
3-,4-,5-word window The three, four and five preceding and following words in the context No 

word-pairs 

Combinations of the word and the next and previous words in the  
context window, preserving direction but not position 

 (considering separate features for the different combinations  
of the context and the target word) 

No 

titles Match the word against a list of name titles (Mr, Mrs, etc.) No 
lemma_context Lemma of the words inside the contextual window No 

PoS_context Individual features from the part-of-speech tags  of the contextual window Yes 

PoS_sequence Sequence of the part-of-speech tags of the 2-word contextual window and the target word Yes 

word_shape_context Word shape features of the contextual window Yes 
word-tag Combination of the word and part-of-speech No 
Table 1. Set of learning features for the CRF-based prediction module. (* = selected in the best learning features set) 
 

4 Evaluation and discussion 

Our evaluation consists of: 1) “out-of-the-box” 
evaluation of the systems presented in Section 3.1; 
and 2) evaluation of the performance of our person 
names de-identification pipeline.   

 
 

4.1 Data 

We manually annotated all person names (includ-
ing patients, relatives, health care providers, and 
other persons) in a corpus of various types of Vet-
eran’s Health Administration (VHA) clinical notes. 
These notes were selected using a stratified ran-
dom sampling approach with documents longer 
than 500 words. Then, the 100 most frequent VHA 
note types were used as strata for sampling, and the 

69



same number of notes was randomly selected in 
each stratum. Two reviewers independently anno-
tated each document, a third reviewer adjudicated 
their disagreements, and a fourth reviewer eventu-
ally examined ambiguous and difficult adjudicated 
cases. 

The evaluation corpus presented here comprises 
a subset of 275 VHA clinical notes from the 
aforementioned corpus. For training, 225 notes 
were randomly selected (contained 748 person 
name annotations), and the remaining 50 notes 
(with 422 name annotations) were used for testing 
the systems. 

4.2 Experiments and results 

We present results in terms of precision, recall and 
F-measure (harmonic mean of recall and preci-
sion). We used a weight of 2 when calculating the 
F2-measure giving recall more (twice) importance 
than precision (Jurafsky and Martin, 2009). This 
reflects our emphasis on recall for de-
identification. To our understanding, due to legal 
and privacy issues, a good de-identification system 
should be tailored to prioritize recall, and conse-
quently patient confidentiality. It is not the scope 
of this paper to judge or modify the development 
design adopted by other de-identification systems. 

Moreover, we considered correct predictions at 
least overlapping with the entire PHI annotation in 
the reference standard (i.e., exact match with the 
reference annotation, or more than the exact 
match). We can therefore assure complete redac-
tion of PHI. 

Table 2 illustrates “out-of-the-box” evaluation 
results of the systems described in Section 3.1. For 
this evaluation, we trained MIST and HIDE with 
our 225 notes training corpus, while the Stanford 
NER was run using the trained models available 
with its distribution8. Testing was realized using 
our 50 notes testing corpus. 

Table 3 shows the performance of our names de-
identification approach. We provide results for dif-
ferent configurations of our pipeline: 

 
• Rules & Dictionaries. Results of the rules 

and dictionary lookups component de-
scribed in Section 3.2.2, in this case using a 

                                                
8 Further details about these models can be found at 
http://nlp.stanford.edu/software/CRF-NER.shtml 

keyword-search strategy for dictionary 
lookups. 

• R&D with fuzzy searches. Results from the 
rules and dictionary lookups component us-
ing Lucene’s Fuzzy Query engine for dic-
tionary searches. It implements a fuzzy 
search based on the Levenshtein (edit dis-
tance) algorithm9 (Levenshtein, 1966), 
which has to surpass a similarity threshold 
in order to produce a match. We carried out 
a greedy search on the training corpus for 
the best similarity threshold. We found 0.74 
to be the best threshold. 

• CRF-based w/FS. The CRF-based predic-
tions component results after selecting the 
best set of features (see Section 3.2.3). The 
CRF classifier was trained using our 225-
document training corpus. 

• R&D + CRF w/FS. The cumulative results 
from the rules and dictionary lookups (not 
implementing fuzzy dictionary searches) 
and the CRF-based predictions components. 

• R&D + CRF w/FS + FP-filtering. Includes 
all components together, adding the false-
positive filtering component (Section 3.2.4) 
at the end of the pipeline. The SVM model 
for this last component was created using 
our training corpus. 

 
System Prec. Rec. F2 

HMS Scrubber 0.150 0.675 0.397 
MeDS 0.149 0.768 0.419 
MIT deid 0.636 0.893 0.826 
MIST 0.865 0.319 0.356 
HIDE 0.975 0.376 0.429 
Stanford NER 0.692 0.723 0.716 

Table 2. “Out-of-the-box” evaluation of existing de-
identification and NER systems (Prec.=precision; 
Rec.=recall; F2= F2-measure). 
 

System Prec. Rec. F2 
Rules & Dictionaries 0.360 0.962 0.721 
R&D + fuzzy 0.171 0.969 0.502 
CRF-based w/FS 0.979 0.874 0.893 
R&D + CRF w/FS 0.360 0.988 0.732 
R&D + CRF w/FS + 
FP-filtering 0.774 0.974 0.926 

Table 3. Cumulative results of our pipeline of processes. 

                                                
9 http://www.merriampark.com/ld.htm 

70



4.3 Analysis 

Our novel names de-identification pipeline signifi-
cantly outperforms all other systems we evaluated 
“out-of-the-box” or trained with our VHA notes 
corpus. 

Among the five existing systems we evaluated 
(Table 1), only one achieved noteworthy recall 
around 89%. However, none of them obtained any 
remarkable F2-measure.  Most errors produced by 
the pattern matching systems (i.e., HMS Scrubber, 
MeDS, and MIT deid system) were due to false 
positive annotations of medical eponyms (e.g., 
“Achilles”, “Guyon”, etc.), as well as acronyms 
denoting medical facilities (e.g., “ER” and 
“HCS”). The false negatives consisted of ambigu-
ous person names (e.g., “Bill” and “Chase”), some 
formats not covered by the patterns (e.g., “[Last-
Name], [FirstName] [Initial]”), and a few names 
not found in the dictionaries. 

Among machine learning-based systems, the 
two de-identification applications (i.e., MIST and 
HIDE) obtained good precision, but quite low re-
call. The size of our training corpus was somewhat 
limited, and these results probably indicate a need 
for more sophisticated learning features, as well as 
feature selection procedures (rather than using the 
“out-of-the-box” feature specification that comes 
with these systems) for better performance. With 
improved learning features, we could mitigate the 
relative lack of training examples. Interestingly, 
the NER system, which was trained on newswire 
documents, performed even better than some de-
identification systems, although a need for im-
provement is still present. 

We acknowledge that the comparison with Stan-
ford NER is not completely fair due to the different 
source of documents used for training. However, 
we considered it interesting information, and alt-
hough clinical notes contain characteristics not 
present in newswire corpora, they also have simi-
larities regarding person names (e.g., titles “Mr.”, 
“Dr.”, “PhD”, part-of-speech, verb tenses). There-
fore, we think that only for names recognition, a 
newswire trained NER can provide interesting re-
sults, and this was actually what we observed. 

Table 2 points out that the combination of our 
components produces successful cumulative re-
sults. Using the training corpus to create a simple 
component made up of rules, dictionary lookups, 
and few heuristics for disambiguation allowed for 

recall values of 0.96. This demonstrates the need to 
adapt these techniques to the target documents, 
instead of employing systems “out-of-the-box”. 

Our experiments with fuzzy dictionary lookups 
did not allow for a significant increase in recall, 
but caused a decrease in precision (-19%). It sug-
gests that there was no need for considering person 
name misspellings. 

The component based on CRF predictions alone 
achieved good performance, especially in preci-
sion. It obtained the best F2-measure (0.89), clearly 
higher than the other “out-of-the-box” systems 
based on CRF models. It proves that selecting suit-
able learning features mitigates to some extent the 
scarcity of training examples.  

Our next experiment combined the rules and 
dictionaries and CRF components. It improved the 
overall recall to about 0.99, which means that 
CRF-based predictions recognized some person 
names that were missed by our pattern matching 
components, but didn’t increase the precision. We 
reached here our first goal of high recall or sensi-
tivity. 

Finally, we added the false-positive filtering 
component to our system. This component was 
able to filter out 622 (84%) false positives from a 
total of 742, improving the precision to 0.77 
(+41%); but also causing a slight decrease in recall 
(-1.4%). This application of our pipeline was suc-
cessful, reaching an F2-mesure of 0.93, and was an 
effective way of training the SVM model for false-
positives filtering. 

 

5 Conclusions 

We designed and evaluated a novel person names 
de-identification system with VHA clinical docu-
ments. We also presented an “out-of-the-box” 
evaluation of several available de-identification 
and NER systems; all of them were surpassed by 
our approach. 

With our proposal, we showed that it is possible 
to improve the recognition of person names in clin-
ical records, even when the corpus for training ma-
chine learning classifiers is limited. Furthermore, 
the workflow of our pipeline allowed us to tackle 
the de-identification task from an intuitive but 
powerful perspective, i.e. facing the achievement 
of high recall and precision as two separate goals 
implementing specific techniques and components. 

71



Packaging this two-step procedure as a boot-
strapping learning or adding the rules to define 
learning features would not allow us to use the 
qualities of the R&D and CRF components (i.e., 
obtain the highest recall by any means). Moreover, 
considering the small size of our manually anno-
tated examples, these approaches would not work 
much better than existing systems.  

As future efforts, we plan to improve the preci-
sion of the rules and dictionary lookups component 
by adding more sophisticated person names disam-
biguation procedures. Such procedures should deal 
with the peculiar formatting of clinical records as 
well as integrate enriched knowledge from bio-
medical resources. We also plan to evaluate the 
portability of our approach by using other sets of 
clinical documents, such as the 2006 i2b2 de-
identification challenge corpus (Uzuner et al., 
2007). 

 

Acknowledgments 
Funding provided by the Department of Veterans 
Affairs Health Services Research & Development 
Services Consortium for Healthcare Informatics 
Research grant (HIR 08-374). 
 

References  
John Aberdeen, Samuel Bayer, Reyyan Yeniterzi, Ben 

Wellner, Cheryl Clark, David Hanauer, Bradley Ma-
lin, and Lynette Hirschman. 2010. The MITRE Iden-
tification Scrubber Toolkit: design, training, and 
assessment. International journal of medical infor-
matics, 79 (12) (December): 849-59. 

Bruce A. Beckwith, Rajeshwarri Mahaadevan, Ulysses 
J. Balis, and Frank Kuo. 2006. Development and 
evaluation of an open source software tool for 
deidentification of pathology reports. BMC medical 
informatics and decision making, 6 (1) (January): 12. 

Chih-Chung Chang and Chih-Jen Lin. (2001). LIBSVM: 
a library for support vector machines. Computer, 1-
30. 

George Doddington, Alexis Mitchell, Mark Przybocki, 
Lance Ramshaw, Stephanie Strassel, and Ralph 
Weischedel. 2004. The Automatic Content Extraction 
(ACE) Program - Tasks, Data, and Evaluation. Pro-
ceedings of LREC 2004: 837-840. 

Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating non-local information 
into information extraction systems by Gibbs sam-

pling. Proceedings of the 43rd Annual Meeting on 
Association for Computational Linguistics: 363-370. 

F. Jeff Friedlin and Clement J. McDonald. 2008. A 
software tool for removing patient identifying infor-
mation from clinical documents. Journal of the 
American Medical Informatics Association  : JAMIA 
15 (5) (January 1): 601-10. 

James Gardner and Li Xiong. 2009. An integrated 
framework for de-identifying unstructured medical 
data. Data & Knowledge Engineering 68 (12) (De-
cember): 1441-1451. 

Ralph Grishman and Beth Sundheim. 1996. Message 
understanding conference-6: A brief history. Pro-
ceedings of the 16th conference on Computational 
linguistics - Volume 1: 466-471. Association for 
Computational Linguistics, Copenhagen, Denmark. 

Daniel Jurafsky and James H. Martin. 2009. Speech and 
Language Processing: An Introduction to Natural 
Language Processing, Speech Recognition, and 
Computational Linguistics. 2nd edition. Prentice-
Hall. Upper Saddle River, NJ, USA. 

V.I. Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet 
Physics - Doklady 10: 707–710. 

Stephane M. Meystre, F. Jeffrey Friedlin, Brett R. 
South, Shuying Shen, and Matthew H. Samore. 2010. 
Automatic de-identification of textual documents in 
the electronic health record: a review of recent re-
search. BMC medical research methodology 10 (1) 
(January): 70. 

Ishna Neamatullah, Margaret M. Douglass, Li-wei H. 
Lehman, Andrew Reisner, Mauricio Villarroel, Wil-
liam J. Long, Peter Szolovits, George B. Moody, 
Roger G. Mark, and Gari D. Clifford. 2008. Auto-
mated de-identification of free-text medical records. 
BMC medical informatics and decision making 8 (1) 
(January): 32. 

Guergana K. Savova, James J. Masanz, Philip V. Ogren, 
Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-
Schuler, and Christopher G. Chute. 2010. Mayo clin-
ical Text Analysis and Knowledge Extraction System 
(cTAKES): architecture, component evaluation and 
applications. Journal of the American Medical In-
formatics Association  : JAMIA 17 (5): 507-13. 

Erik F. Tjong Kim Sang, and Fien De Meulder. 2003. 
Introduction to the CoNLL-2003 Shared Task: Lan-
guage-Independent Named Entity Recognition. In 
Proceedings of the seventh conference on Natural 
language learning at HLT-NAACL 2003 - Volume 4: 
142-147. 

Özlem Uzuner, Yuan Luo, and Peter Szolovits. 2007. 
Evaluating the State-of-the-Art in Automatic De-
identification. Journal of the American Medical In-
formatics Association  : JAMIA 14(5):550-563. 

72


