










































INAOEUPV-CORE: Extracting Word Associations from Document Corpora to estimate Semantic Textual Similarity


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 229â€“233, Atlanta, Georgia, June 13-14, 2013. cÂ©2013 Association for Computational Linguistics

INAOE_UPV-CORE: Extracting Word Associations from  

Document Corpora to estimate Semantic Textual Similarity 
 

 

Fernando SÃ¡nchez-Vega 

Manuel Montes-y-GÃ³mez 

Luis VillaseÃ±or-Pineda 
Paolo Rosso 

Laboratorio de TecnologÃ­as del Lenguaje, 

Instituto Nacional de AstrofÃ­sica, Ã“ptica y 

ElectrÃ³nica (INAOE), Mexico. 

Natural Language Engineering Lab., ELiRF, 

Universitat PolitÃ¨cnica de ValÃ¨ncia, Spain 
prosso@dsic.upv.es 

{fer.callotl,mmontesg,villasen} 

@inaoep.mx 
 

 

Abstract 

This paper presents three methods to evaluate 

the Semantic Textual Similarity (STS). The 

first two methods do not require labeled train-

ing data; instead, they automatically extract 

semantic knowledge in the form of word asso-

ciations from a given reference corpus. Two 

kinds of word associations are considered: co-

occurrence statistics and the similarity of 

word contexts. The third method was done in 

collaboration with groups from the Universi-

ties of Paris 13, Matanzas and Alicante. It 

uses several word similarity measures as fea-

tures in order to construct an accurate predic-

tion model for the STS. 

1 Introduction 

Even with the current progress of the natural lan-

guage processing, evaluating the semantic text 

similarity is an extremely challenging task. Due to 

the existence of multiple semantic relations among 

words, the measuring of text similarity is a multi-

factorial and highly complex task (Turney, 2006). 

Despite the difficulty of this task, it remains as 

one of the most attractive research topics for the 

NLP community. This is because the evaluation of 

text similarity is commonly used as an internal 

module in many different tasks, such as, informa-

tion retrieval, question answering, document sum-

marization, etc. (Resnik, 1999). Moreover, most of 

these tasks require determining the â€œsemanticâ€ 

similarity of texts showing stylistic differences or 

using polysemicwords (Hliaoutakis et al., 2006). 

The most popular approach to evaluate the se-

mantic similarity of words and texts consists in 

using the semantic knowledge expressed in ontolo-

gies (Resnik, 1999); commonly, WorldNet is used 

for this purpose (Fellbaum, 2005). Unfortunately, 

despite the great effort that has been the creation of 

WordNet, it is still far to cover all existing words 

and senses (Curran, 2003).Therefore, the semantic 

similarity methods that use this resource tend to 

reduce their applicability to a restricted domain 

and to a specific language. 

We recognize the necessity of having and using 

manually-constructed semantic-knowledge sources 

in order to get precise assessments of the semantic 

similarity of texts, but, in turn, we also consider 

that it is possible to obtain good estimations of 

these similarities using less-expensive, and perhaps 

broader, information sources. In particular our 

proposal is to automatically extract the semantic 

knowledge from large amounts of raw data sam-

ples i.e. document corpora without labels. 

In this paper we describe two different strategies 

to compute the semantic similarity of words from a 

reference corpus. The first strategy uses word co-

occurrence statistics. It determines that two words 

are associated (in meaning) if they tend to be used 

together, in the same documents or contexts. The 

second strategy measures the similarity of words 

by taking into consideration second order word co-

occurrences. It defines two words as associated if 

they are used in similar contexts (i.e., if they co-

occur with similar words). The following section 

describes the implementation of these two strate-

gies for our participation at the STS-SEM 2013 

task, as well as their combination with the meas-

ures designed by the groups from the Universities 

of Matanzas, Alicante and Paris 13. 

229



2 Participation in STS-SEM2013 

The Semantic Textual Similarity (STS) task con-

sists of estimated the value of semantic similarity 

between two texts,ğ·1 and ğ·2 for now on. 
As we mentioned previously, our participation in 

the STS task of SEM 2013 considered two differ-

ent approaches that aimed to take advantage of the 

language knowledge latent in a given reference 

corpus. By applying simple statistics we obtained a 

semantic similarity measure between words, and 

then we used this semantic word similarity (SWS) 

to get a sentence level similarity estimation. We 

explored two alternatives for measuring the seman-

tic similarity of words, the first one, called 

ğ‘†ğ‘Šğ‘†ğ‘œğ‘ğ‘¢ğ‘Ÿğ‘Ÿ , uses the co-occurrence of words in a 
limited context

1
,and the second, ğ‘†ğ‘Šğ‘†ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ , com-

pares the contexts of the words using the vector 

model and cosine similarity to achieve this com-

parison. It is important to point out that using the 

vector space model directly, without any spatial 

transformation as those used by other approaches
2
, 

we could get greater control in the selection of the 

features used for the extraction of knowledge from 

the corpus. It is also worth mentioning that we 

applied a stemming procedure to the sentences to 

be compared as well as to all documents from the 

reference corpus. We represented the texts ğ·1 and 
ğ·2 by bags of tokens, which means that our ap-
proaches did not take into account the word order.  

Following we present our baseline method, then, 

we introduce the two proposed methods as well as 

a method done in collaboration with other groups. 

The idea of this shared-method is to enhance the 

estimation of the semantic textual similarity by 

combining different and diverse strategies for 

computing word similarities. 

2.1 STS-baseline method 

Given textsğ·1 and ğ·2, their textual similarity is 
given by: 

 
ğ‘†ğ‘‡ğ‘† âˆ’ ğµğ‘ğ‘ ğ‘’ğ‘™ğ‘–ğ‘›ğ‘’ = ğ‘€ğ¼ğ‘(ğ‘†ğ¼ğ‘€ ğ·1 ,ğ·2 , ğ‘†ğ¼ğ‘€(ğ·2 ,ğ·1)) 
 

where 

                                                           
1 In the experiments we considered a window (context) formed 

of 15 surrounding words. 
2Such as Latent Semantic Analysis (LSA) (Turney, 2005). 

ğ‘†ğ¼ğ‘€ ğ·ğ‘– ,ğ·ğ‘—  =
1

|ğ·ğ‘–|
 1(ğ‘¡ğ‘˜  âˆˆ  ğ·ğ‘— )

ğ‘¡ğ‘˜âˆˆğ·ğ‘–

 

 

This measure is based on a direct matching of to-

kens. It simply counts the number of tokens from 

one text ğ·ğ‘–  that also exist in the other text ğ·ğ‘— . Be-

cause STS is a symmetrical attribute, unlike Tex-

tual Entailment (Agirre et al., 2012), we designed 

it as a symmetric measure. We assumed that the 

relationship between both texts is at least equal to 

their smaller asymmetric similarity. 

2.2 The proposed STS methods 

These methods incorporate semantic knowledge 

extracted from a reference corpus. They aim to 

take advantage of the latent semantic knowledge 

from a large document collection. Because the 

extracted knowledge from the reference corpus is 

at word level, these methods for STS use the same 

basic â€“word matchingâ€“ strategy for comparing the 

sentences like the baseline method. Nevertheless, 

they allow a soft matching between words by in-

corporating information about their semantic simi-

larity. 

The following formula shows the proposed 

modification to the SIM function in order to incor-

porate information of the semantic word similarity 

(SWS). This modification allowed us not only to 

match words with exactly the same stem but also 

to link different but semantically related words. 
 

ğ‘†ğ¼ğ‘€ ğ·ğ‘– ,ğ·ğ‘—  =  ğ‘€ğ´ğ‘‹  ğ‘†ğ‘Šğ‘†(ğ‘¡ğ‘š , ğ‘¡ğ‘›)

ğ‘¡ğ‘›âˆˆğ·ğ‘—

 

ğ‘¡ğ‘šâˆˆğ·ğ‘–

 

 

We propose two different strategies to compute 

the semantic word similarity (SWS), ğ‘†ğ‘‡ğ‘†ğ‘‚ğ‘ğ‘ğ‘¢ğ‘Ÿ  and 
ğ‘†ğ‘‡ğ‘†ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ . The following subsections describe in 
detail these two strategies. 

2.2.1 STS based on word co-occurrence 

ğ‘†ğ‘Šğ‘†ğ‘‚ğ‘ğ‘ğ‘¢ğ‘Ÿ uses a reference corpus to get a numeri-
cal approximation of the semantic similarity be-

tween two terms ğ‘¡ğ‘–and ğ‘¡ğ‘—  (when these terms have 

not the same stem). As shown in the following 

formula, ğ‘†ğ‘Šğ‘†ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿ  takes values between 0 and 1; 
0 indicates that it does not exist any text sample in 

the corpus that contains both terms, whereas, 1 

indicates that they always occur together. 

230



 

ğ‘†ğ‘Šğ‘†ğ‘œğ‘ğ‘ğ‘¢ğ‘Ÿ  ğ‘¡ğ‘– , ğ‘¡ğ‘— =  

ğ‘¡ğ‘– = ğ‘¡ğ‘—                1               

ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ
#(ğ‘¡ğ‘– , ğ‘¡ğ‘— )

ğ‘€ğ¼ğ‘(#(ğ‘¡ğ‘–), #(ğ‘¡ğ‘— ))

  

 

where# ğ‘¡ğ‘– , ğ‘¡ğ‘—  is the number of times that ğ‘¡ğ‘–  and 

ğ‘¡ğ‘—  co-occur and # ğ‘¡ğ‘–  and # ğ‘¡ğ‘—  are the number of 

times that terms ğ‘¡ğ‘–  and ğ‘¡ğ‘—  occur in the reference 

corpus respectively. 

2.2.2 STS based on context similarity 

ğ‘†ğ‘Šğ‘†ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ is based on the idea that two terms are 
semantically closer if they tend to be used in simi-

lar contexts. This measure uses the well-known 

vector space model and cosine similarity to com-

pare the termsâ€™ contexts. In a first step, we created 

a context vector for each term, which captures all 

the terms that appear around it in the whole refer-

ence corpus. Then, we computed the semantic 

similarity of two terms by the following formula. 
 

ğ‘†ğ‘Šğ‘†ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡  ğ‘¡ğ‘– , ğ‘¡ğ‘— =  
ğ‘¡ğ‘– = ğ‘¡ğ‘—                1               

ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ ğ‘†ğ¼ğ‘€ğ¶ğ‘‚ğ‘† ğ‘‡  ğ‘– ,ğ‘‡  ğ‘—  
  

 

where the cosine similarity, SIMCOS, is calcu-

lated on the vectors ğ‘‡  ğ‘–and ğ‘‡  ğ‘— corresponding to the 

vector space model representation of terms ğ‘¡ğ‘–  and 
ğ‘¡ğ‘— , as indicated in the following equation: 
 

ğ‘†ğ¼ğ‘€ğ¶ğ‘‚ğ‘†(ğ‘‡  ğ‘– ,ğ‘‡  ğ‘— ) =
 ğ‘¡ğ‘–ğ‘˜ âˆ™ ğ‘¡ğ‘—ğ‘˜ğ‘˜  âˆˆ |ğ‘‰|

|ğ‘‡  ğ‘–| âˆ™ |ğ‘‡  ğ‘— |
 

 

It is important to point out that SIMCOS is cal-

culated on a â€œpredefinedâ€ vocabulary of interest; 

the appropriate selection of this vocabulary helps 

to get a better representation of terms, and, conse-

quently, a more accurate estimation of their seman-

tic similarities. 

2.3 STS based on a combination of measures 

In addition to our main methods we also developed 

a method that combines our SWS measures with 

measures proposed by other two research groups, 

namely: 

 

ï‚· LIPN (Laboratoire d'Informatique de Paris-
Nord, UniversitÃ© Paris 13, France). 

ï‚· UMCC_DLSI (Universidad de Matanzas Cami-
lo Cienfuegos, Cuba, in conjuction with the 

Departamento de Lenguajes y Sistemas In-

formÃ¡ticos, Universidad de Alicante, Spain). 

 

The main motivation for this collaboration was to 

investigate the relevance of using diverse strategies 

for computing word similarities and the effective-

ness of their combination for estimating the seman-

tic similarity of texts. 

The proposed method used a set of measures 

provided by each one of the groups. These meas-

ures were employed as features to obtained a pre-

diction model for the STS. Table 1 summarizes the 

used measures. For the generation and fitting of the 

model we used three approaches: linear regression, 

a Gaussian process and a multilayer neural net-

work. 

 

Description Team #  
Mean 

Rank 

Best 

Rank 

Based on IR measures LIPN 2 2.0 1 

Based on distance on WordNet LIPN 2 8.5 2 

STS-Context 
INAOE-

UPV 
1 4.0 4 

Complexity of the sentences 
INAOE-
UPV 

34 27.8 5 

STS-Occur 
INAOE-

UPV 
1 7.0 7 

Based on the alignment of 

particulars POS. 

UMCC_ 

DLSI 
12 40.9 18 

n-gram overlap LIPN 1 20.0 20 

Based on Edit distance 
UMCC_ 

DLSI 
4 42.6 27 

Syntactic dependencies overlap LIPN 1 29.0 29 

Levenshteinâ€™s distance LIPN 1 42.0 42 

Named entity overlap LIPN 1 57.0 57 

Table 1. General description of the features used by the shared me-
thod. The second column indicates the source team for each group of 

features; the third column indicates the number of used features from 

each group; the last two columns show the information gain rank of 
each group of features over the training set.  

3 Implementation considerations  

The extraction of knowledge for the computation 

of the SWS was performed over the Reuters-21578 

collection. This collection was selected because it 

is a well-known corpus and also because it in-

cludes documents covering a wide range of topics. 

Due to time and space restrictions we could not 

consider all the vocabulary from the reference cor-

pus; the vocabulary selection was conducted by 

taking the best 20,000 words according to the tran-

231



sition point method (Pinto et al., 2006). This me-

thod selects the terms associated to the main topics 

of the corpus, which presumably contain more 

information for estimating the semantic similarity 

of words. We also preserved the vocabulary from 

the evaluation samples, provided they also occur in 

the reference corpus. The size of the vocabulary 

used in the experiments and the size of the corpus 

and test set vocabularies are shown in Table 2. 

 
Experimentâ€™s  

Vocabulary 

Selected 

Vocabulary 
Ref. Corpus 

Vocabulary 
Evaluation 

Vocabulary 
 

26724 20000 31213 11491 

Table 2. Number of different stems from each of the 
considered vocabularies 

4 Evaluation and Results 

The methods proposed by our group do not require 

to be trained, i.e., they do not require tagged data, 

only a reference corpus, therefore, it was possible 

to evaluate them on the whole training set available 

this year. Table 3 shows their results on this set. 
 

Method Correlation 

STS-Baseline 0.455 

STS-Occur 0.500 

STS-Contex 0.511 

Table 3. Correlation values of the proposed methods and 

our baseline method with human judgments. 
 

Results in Table 3 show that the use of the co-

occurrence information improves the correlation 

with human judgments. It also shows that the use 

of context information further improves the results. 

One surprising finding was the competitive per-

formance of our baseline method; it is considerably 

better than the previous yearâ€™s baseline result 

(0.31). 

In order to evaluate the method done in collabo-

ration with LIPN and UMCC_DLSI, we carried 

out several experiments using the features provided 

by each group independently and in conjunction 

with the others. The experiments were performed 

over the whole training set by means of two-fold 

cross-validation. The individual and global results 

are shown in Table 4. 

As shown in Table 4, the result corresponding to 

the combination of all features clearly outper-

formed the results obtained by using each teamÂ´s 

features independently. Moreover, the best combi-

nation of features, containing selected features 

from the three teams, obtained a correlation value 

very close to last year's winner result. 

 

Featured by  Group Perdition Model Correlation 

LIPN Gaussian Process 0.587 

LIPN Lineal Regression 0.701 

LIPN Multilayer-NN 0.756 

UMCC_DLSI Gaussian Process 0.388 

UMCC_DLSI Lineal Regression 0.388 

UMCC_DLSI Multilayer-NN 0.382 

INAOE-UPV Gaussian Process 0.670 

INAOE-UPV Lineal Regression 0.674 

INAOE-UPV Multilayer-NN 0.550 

ALL Gaussian Process 0.770 

ALL Lineal Regression 0.777 

ALL Multilayer-NN 0.633 

SELECTED-SET Multilayer-NN 0.808 

LAST YEARÂ´S 
WINNER 

Simple 
log-linear regression 

0.823 

Table 4. Results obtained by the different subsets of 

features, from the different participating groups. 
 

4.1 Officials Runs 

For the official runs (refer to Table 5) we submit-

ted the results corresponding to the ğ‘†ğ‘‡ğ‘†ğ‘‚ğ‘ğ‘ğ‘¢ğ‘Ÿ  and 
ğ‘†ğ‘‡ğ‘†ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡  methods. We also submitted a result 
from the method done in collaboration with LIPN 

and UMCC_DLSI. Due to time restrictions we 

were not able to submit the results from our best 

configuration; we submitted the results for the 

linear regression model using all the features 

(second best result from Table 4).Table 5 shows 

the results in the four evaluation sub-collections; 

Headlines comes from news headlines, OnWN  

and FNWN contain pair senses definitions from 

WordNet and other resources, finally, SMT are  

translations from automatic machine translations 

and from the reference human translations. 

As shown in Table 5, the performances of the 

two proposed methods by our group were very 

close. We hypothesize that this result could be 

caused by the use of a larger vocabulary for the 

computation of co-occurrence statistics than for the 

calculation of the context similarities. We had to 

use a smaller vocabulary for the later because its 

higher computational cost. 

Finally, Table 5 also shows that the method 

done in collaboration with the other groups ob-

232



tained our best results, confirming that using more 

information about the semantic similarity of words 

allows improving the estimation of the semantic 

similarity of texts. The advantage of this approach 

over the two proposed methods was especially 

clear on the OnWN and FNWN datasets, which 

were created upon WordNet information. Some-

how this result was predictable since several meas-

ures from this â€œshare-methodâ€ use WordNet 

information to compute the semantic similarity of 

words. However, this pattern was not the same for 

the other two (WordNet unrelated) datasets. In 

these other two collections, the average perfor-

mance of our two proposed methods, without using 

any expensive and manually constructed resource, 

improved by 4% the results from the share-method. 

 

Method Headlines OnWN FNWN SMT MEAN 

STS-Occur 0.639 0.324 0.271 0.349 0.433 

STS-Contex 0.639 0.326 0.266 0.345 0.431 

Collaboration 0.646 0.629 0.409 0.304 0.508 

Table 4. Correlation values from our official runs over the 

four sub-datasets.  

5 Conclusions 

The main conclusion of this experiment is that it is 

possible to extract useful knowledge from raw 

corpora for evaluating the semantic similarity of 

texts.  Other important conclusion is that the com-

bination of methods (or word semantic similarity 

measures) helps improving the accuracy of STS.  

As future work we plan to carry out a detailed 

analysis of the used measures, with the aim of de-

termining their complementariness and a better 

way for combining them. We also plan to evaluate 

the impact of the size and vocabulary richness of 

the reference corpus on the accuracy of the pro-

posed STS methods. 

Acknowledgments 

This work was done under partial support of 

CONACyT project Grants: 134186, and Scholar-

ship 224483. This work is the result of the collabo-

ration in the framework of the WIQEI IRSES 

project (Grant No. 269180) within the FP 7 Marie 

Curie. The work of the last author was in the 

framework the DIANA-APPLICATIONS-Finding 

Hidden Knowledge in Texts: Applications 

(TIN2012-38603-C02-01) project, and the 

VLC/CAMPUS Microcluster on Multimodal Inte-

raction in Intelligent Systems. We also thank the 

teams from the Universities of Paris 13, Matanzas 

and Alicante for their willingness to collaborate 

with us in this evalaution exercise. 

References  

AngelosHliaoutakis, GiannisVarelas, EpimeneidisVout-

sakis, Euripides G. M. Petrakis, EvangelosMilios, 

2006, Information Retrieval by Semantic Similarity, 

Intern. Journal on Semantic Web and Information 

Systems: Special Issue of Multimedia Semantics 

(IJSWIS), 3(3): 55â€“73. 

Carmen Banea, Samer Hassan, Michael Mohler and 

RadaMihalcea, 2012, UNT: A Supervised Synergistic 

Approach to Semantic Text Similarity, SEM 2012: 

The First Joint Conference on Lexical and Computa-

tional Semantics, Proceedings of the Sixth Interna-

tional Workshop on Semantic Evaluation (SemEval 

2012), Montreal, Vol. 2: 635-642. 

Christiane Fellbaum,2005, WordNet and wordnets, 

Encyclopedia of Language and Linguistics, Second 

Ed., Oxford, Elsevier: 665-670. 

David Pinto, Hector JimÃ©nez H. and Paolo Rosso. Clus-

tering abstracts of scientific texts using the Transi-

tion Point technique, Proc. 7th Int. Conf. on Comput. 

Linguistics and Intelligent Text Processing, CICL-

ing-2006, Springer-Verlag, LNCS(3878): 536-546. 

EnekoAgirre, Daniel Cer, Mona Diab and Aitor Gonza-

lez-Agirre, SemEval-2012 Task 6: A Pilot on Seman-

tic Textual Similarity. SEM 2012: The First Joint 

Conference on Lexical and Computational Seman-

tics, Proceedings of the Sixth International Workshop 

on Semantic Evaluation (SemEval2012), Montreal, 

Vol. 2: 386-393. 

James Richard Curran, 2003, Doctoral Thesis: From 

Distributional to Semantic Similarity, Institute for 

Communicating and Collaborative Systems, School 

of Informatics, University of Edinburgh. 

Peter D. Turney, 2005, Measuring semantic similarity 

by latent relational analysis, IJCAI'05 Proceedings of 

the 19th international joint conference on Artificial 

intelligence, Edinburgh, Scotland: 1136-1141 

Peter D. Turney, 2006, Similarity of Semantic Relations, 

Computational Linguistics, Vol. 32, No. 3: 379-416. 

Philip Resnik, 1999, Semantic Similarity in a Taxono-

my: An Information-Based Measure and its Applica-

tion to Problems of Ambiguity in Natural Language, 

Journal of Artificial Intelligence Research, Vol. 11: 

95-130. 

233


