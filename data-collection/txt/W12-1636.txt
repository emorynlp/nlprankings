



















































Contingency and Comparison Relation Labeling and Structure Prediction in Chinese Sentences


Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 261â€“269,
Seoul, South Korea, 5-6 July 2012. cÂ©2012 Association for Computational Linguistics

Contingency and Comparison Relation Labeling and Structure 
Prediction in Chinese Sentences 

 
 

Hen-Hsen Huang Hsin-Hsi Chen 
Department of Computer Science and Department of Computer Science and 

Information Engineering, Information Engineering, 
National Taiwan University, 

Taipei, Taiwan 
National Taiwan University, 

Taipei, Taiwan 
hhhuang@nlg.csie.ntu.edu.tw hhchen@csie.ntu.edu.tw 

 
 

 

 
 

Abstract 

Unlike in English, the sentence boundaries 
in Chinese are fuzzy and not well-defined. 
As a result, Chinese sentences tend to be 
long and consist of complex discourse 
relations. In this paper, we focus on two 
important relations, Contingency and 
Comparison, which occur often inside a 
sentence. We construct a moderate-sized 
corpus for the investigation of intra-
sentential relations and propose models to 
label the relation structure. A learning 
based model is evaluated with various 
features. Experimental results show our 
model achieves accuracies of 81.63% in the 
task of relation labeling and 74.8% in the 
task of relation structure prediction.  

1 Introduction 

Discourse relation labeling has attracted much 
attention in recent years due to its potential 
applications such as opinion mining, question 
answering, etc. The release of the Penn Discourse 
Treebank (Joshi and Webber, 2004; Prasad et al., 
2008) has advanced the development of English 
discourse relation recognition (Lin et al., 2009; 
Pitler et al., 2009; Pitler and Nenkova, 2009; Wang 
et al., 2010). For Chinese, a discourse corpus is not 
publicly available yet. Thus, the research on 
Chinese discourse relation recognition is relatively 
rare. Most notably, Xue (2005) annotated discourse 

connectives in the Chinese Treebank. Our previous 
work labeled four types of relations, including 
temporal, contingency, comparison and expansion, 
between two successive sentences, and reported an 
accuracy of 88.28% and an F-score of 62.88% 
(Huang and Chen, 2011). The major issue of our 
work is the determination of discourse boundaries. 
Each Chinese sentence is always treated as one of 
the two arguments in their annotation and many 
instances of the Contingency and the Comparison 
remain uncaught. 

As suggested by the Penn Discourse Treebank 
annotation guidelines, an argument is possibly 
some clauses in a sentence, a sentence, or several 
successive sentences. In Chinese, the Contingency 
and the Comparison relations are likely to occur 
within a sentence. Thus, a lot of the Contingency 
relations and the Comparison relations are missing 
from annotation in the corpus used in our previous 
work, and the classification performance for these 
two relations, especially the Contingency relation, 
is especially poor (Huang and Chen, 2011). 

In contrast to Chinese inter-sentential discourse 
relation detection (Huang and Chen, 2011) and the 
study of English coherence evaluation (Lin et al., 
2011), this paper focuses on the Contingency 
relation and the Comparison relations that occur 
inside a sentence. In Chinese, the relations usually 
occur in the sentences which contain many clauses. 
For example, two relations occur in sample (S1). 
 

(S1) ç®¡ç†è™•é›–å˜—è©¦è¦è®“é•·æœŸä¾†ä½œç‚ºå¤§å°åŒ—å¾Œ
èŠ±åœ’çš„é™½æ˜å±±å€æ›´å›æ­¸è‡ªç„¶  (â€œAlthough the 

261



management office tried to make the 
Yangmingshan area a more natural environment as 
the long-term garden of Taipeiâ€)ï¼Œä½†éš¨è‘—é€±ä¼‘äºŒ
æ—¥ã€ç¶“æ¿Ÿç’°å¢ƒæ”¹å–„  (â€œBut due to the two-day 
weekend and the improved economic conditionsâ€)ï¼Œ
éŠå®¢å¸¶ä¾†åœè»Šã€åƒåœ¾ç­‰é–“æ¥å½±éŸ¿å»æ›´ç‚ºåš´é‡ 
(â€œThe issues of tourists parking, garbage, and other 
indirect effects become more seriousâ€)ã€‚ 

 
In (S1), the long sentence consists of three 

clauses, and such a Chinese sentence is expressed 
as multiple short sentences in English. Figure 1 
shows that a Comparison relation occurs between 
the first clause and the last two clauses, and a 
Contingency relation occurs between the second 
clause and the third clause. An explicit paired 
discourse marker é›– (although) â€¦ ä½† (but) denotes 
a Comparison relation in (S1), where the first 
clause is the first argument of this relation, and the 
second and the third clauses are the second 
argument of this relation. In addition, an implicit 
Contingency relation also occurs between the 
second and the third clauses. The second clause is 
the cause argument of this Contingency relation, 
and the third clause is its effect. It shows a nested 
relation, which makes relation labeling and relation 
structure determination challenging. 

In Chinese, an explicit discourse marker does 
not always uniquely identify the existence of a 
particular discourse relation. In sample (S2), a 
discourse marker è€Œ  â€œmoreoverâ€ appears, but 
neither Contingency nor Comparison relation 
exists between the two clauses. The discourse 
marker è€Œ has many meanings. Here, It has the 
meaning of â€œandâ€ or â€œmoreoverâ€, which indicates 
an Expansion relation. In other usages, it may have 
the meaning of â€œbutâ€ or â€œhoweverâ€, which 
indicates a Comparison relation. 
 

(S2) è€Œå¤§é™¸ç¶“æ¿Ÿé–‹æ”¾ï¼‘ï¼å¹´ä»¥ä¾†ï¼Œå…¶é€²æ­¥æ›´
ä»¤äººåˆ®ç›®ç›¸çœ‹ã€‚ (â€œMoreover, the progress of 
mainland is more impressive due to its economic 
openness for the last 10 years.â€) 

 
Note that the relation structure of a sentence 

cannot be exactly derived from the parse tree of the 
sentence. Shown in Figure 2 is the structure of 
sample (S3) based on the syntactic tree generated 
by the Stanford parser. However, it is clear that the 

correct structure of (S3) is the one shown in Figure 
3. 
 

(S3) ç›®å‰é›–ç„¶é‚„åªèƒ½åœ¨åœ–ç‰‡ä¸Šè®“å¥³æ€§éœ²éœ²è‡‰ 
(â€œAlthough women only appear in the picturesâ€)ï¼Œ
ä½† æœª ä¾† å¥³ æ€§ çš„ è²¢ ç»  (â€œThe contribution of 
womenâ€)ï¼Œå°‡æ˜¯æ•™ç§‘æ›¸å¦ä¸€å€‹è‘—å¢¨çš„é‡é» (â€œWill 
be another major focus in textbooks in the futureâ€)ã€‚ 
 

This shows that the Stanford parser does not 
capture the information that the last two clauses 
form a unit, which in turn is one of the two 
arguments of a Comparison relation. 

In this work, we investigate intra-sentential 
relation detection in Chinese. Given a Chinese 
sentence, our model will predict if Contingency or 
Comparison relations exist, and determine their 
relation structure. In Section 2, the development of 
a corpus annotated with Contingency and 
Comparison relations is presented. The methods 
and the features are proposed in Section 3. In 
Section 4, the experimental results are shown and 
discussed. Finally, Section 5 concludes this paper. 

 
Figure 1: Relation structure of sample (S1). 

 
Figure 2: Structure of sample (S3) based on the 
syntactic tree generated by the Stanford parser. 

 

 
Figure 3: Correct structure of sample (S3) 

262



2 Dataset  

The corpus is based on the Sinica Treebank 
(Huang et al., 2000). A Total of 81 articles are 
randomly selected from the Sino and Travel sets. 
All the sentences that consist of two, three, and 
four clauses are extracted for relation and structure 
labeling by native Chinese speakers. 

A web-based system is developed for 
annotation. The annotation scheme is designed as 
follows. An annotator first signs in to the 
annotation system, and a list of sentences that are 
assigned to the annotator are given. The annotator 
labels the sentences one by one in the system. A 
sentence is split into clauses along commas, and all 
of its feasible binary tree structures are shown in 
the interface. The annotator decides if a 
Contingency/Comparison relation occurs in this 
sentence. The sentence will be marked as â€œNilâ€ if 
no relation is found. If there is at least one relation 
in this sentence, the annotator then chooses the 
best tree structure of the relations, and the second 
page is shown. 

The previously chosen tree structure is 
presented again, and at this time the annotator has 
to assign a suitable relation type to each internal 
node of the tree structure. The relation type 
includes Contingency â€œå› æœâ€, Comparison â€œè½‰æŠ˜â€, 
and Nil. For example, in sample (S4), its three 
internal nodes are annotated with three relation 
types as shown in Figure 4. 

 
(S4) å³ä½¿æ²’æœ‰å‚³æ‰¿çš„ä½¿å‘½æ„Ÿ (â€œEven without 

the sense of mission of the heritageâ€)ï¼Œç‚ºäº†å°‹æ±‚
æ›´å¥½çš„æ²»ç™‚æ–¹å¼  (â€œIn order to seek better 
treatmentsâ€)ï¼Œä¹Ÿæœƒé©…ä½¿é€™äº›é†«å­¸å·¥ä½œè€…è·¨è¶Šé ˜
åŸŸå€éš” (â€œThese medical workers will be driven 
crossing domain areasâ€)ï¼Œå»å°‹æ‰¾è³‡æº (â€œTo find 
resourcesâ€)ã€‚ 

 
The number of feasible relation structures of a 

sentence may be very large depending on the 
number of clauses. For a sentence with n clauses, 
the number of its feasible structures is given as the 
recursive function f(n) as follows, and the number 
of its feasible relation structures is 3!!!ğ‘“ ğ‘› . 

 

ğ‘“ ğ‘› =
1, ğ‘› = 1

ğ‘“ ğ‘› âˆ’ ğ‘– ğ‘“(ğ‘–)
!!!

!!!
, ğ‘› > 1 

 
Figure 4: Relation structure of sample (S4). 

 
Explicit/ 
implicit 

Relations 2-
Clause 

3-
Clause 

4-
Clause 

Total % 

Explicit Both 0 5 6 11 0.89% 
Contingency 59 72 45 176 14.31% 
Comparison 41 57 22 120 9.76% 
Nil 269 249 169 687 55.85% 

Implicit Both 0 0 0 0 0.00% 
Contingency 11 8 0 19 1.54% 
Comparison 6 0 0 6 0.49% 
Nil 125 56 4 211 17.15% 

All  511 447 272 1,230 100.00% 
Table 1: Statistics of the dataset. 

 
For a two-clause sentence, there are only one 

tree structure and three possible relation tags 
(Contingency, Comparison, and Nil) for the only 
one internal node, the root. For a three-clause 
sentence, there are two candidate tree structures 
and nine combinations of the relation tags. For a 
four-clause sentence, there are five candidate tree 
structures and 27 combinations of the relation tags. 
There are theoretically 3, 18, and 135 feasible 
relation structures for the two-, three-, and four- 
clause sentences, respectively, though only 49 
types of relations structures are observed in the 
dataset. 

Each sentence is shown to three annotators, and 
the majority is taken as the ground-truth. The 
Fleiss-Kappa of the inter-annotator agreement is 
0.44 (moderate agreement). A final decider is 
involved to break ties. The statistics of our corpus 
are shown in Table 1. The explicit data are those 
sentences which have at least one discourse marker. 
The rest of the data are implicit. A total of 11 
explicit sentences which contain both Contingency 
and Comparison relations form complex sentence 
compositions. The implicit samples are relatively 
rare. 

3 Methods 

To predict the intra-sentential relations and 
structures, two learning algorithms, the modern 
implementation of the decision tree algorithm, 

263



C5.01, and the support vector machine, SVMlight2, 
are applied. The linguistic features are the crucial 
part in the learning-based approaches. Various 
features from different linguistic levels are 
evaluated in the experiments as shown below. 
Word: The bags of words in each clause. The 

Stanford Chinese word segmenter3 is applied to all 
the sentences to tokenize the Chinese words. In 
addition, the first word and the last word in each 
clause are extracted as distinguished features. 
POS: The bags of parts of speech (POS) of the 

words in each clause are also taken as features. All 
the sentences in the dataset are sent to the Stanford 
parser4 that parses a sentence from a surface form 
into a syntactic tree, labels POS for each word, and 
generates all the dependencies among the words. In 
addition, the POS tags of the first word and the last 
word in each clause are extracted as distinguished 
features. 
Length: Several length features are considered, 

including the number of clauses in the sentence 
and the number of words for each clause in the 
sentence. 
Connective: In English, some words/phrases 

called connectives are used as discourse markers. 
For example, the phrase â€œdue toâ€ is a typical 
connective that indicates a Contingency relation, 
and the word â€œhoweverâ€ is a connective that 
indicates a Comparison relation. 

Similar to the connectives in English, various 
words and word pair patterns are usually used as 
discourse markers in Chinese. A dictionary that 
contains several types of discourse markers is used. 
The statistics of the connective dictionary and 
samples are listed in Table 2. An intra-sentential 
phrase pair indicates a relation which occurs only 
inside a sentence. In other words, a relation occurs 
when the two phrases of an intra-sentential pair 
exist in the same sentence no matter whether they 
are in the same clause or not. In contrast, an inter-
sentential connective indicates a relation that can 
occur across neighboring sentences. Some 
connectives belong to both intra-sentential and 
inter-sentential types. Each connective in each 
clause is detected and marked with its 
corresponding type. For example, the phrase ç›¸å°

                                                             
1 http://www.rulequest.com/see5-unix.html 
2 http://svmlight.joachims.org/ 
3 http://nlp.stanford.edu/software/segmenter.shtml 
4 http://nlp.stanford.edu/software/lex-parser.shtml 

çš„ â€œIn contrastâ€ will be marked as a connective 
that belongs to Comparison relation. The number 
of types and scopes of the connectives in a 
sentence are used as features. 
Dependency: The dependencies among all 

words in a sentence are used as features. The 
Stanford parser generates dependency pairs from 
the sentence. A dependency pair consists of two 
arguments, i.e., the governor and the dependent, 
and their types. We are interested in those 
dependency pairs that are across two clauses. That 
is, the two arguments of a pair are from different 
clauses. In our assumption, the clauses have a 
closer connection if some dependencies occur 
between them. All such dependency pairs and their 
types are extracted and counted. 
Structure: Recent research work reported 

improved performance using syntactic information 
for English discourse relation detection. In the 
work of Pilter and Nenkova (2009), the categories 
of a tree node, its parent, its left sibling, and its 
right sibling are taken as features. In the work of 
Wang et al. (2010), the entire paragraph is parsed  

  
Relation Type  # Samples 
Temporal Single Phrase 41 ç›®å‰ â€œnowâ€ 

ä¹‹å¾Œ â€œafterâ€ 

Intra-Sent 
Phrase Pair 

80 æ¥è‘—...å† â€œThen...againâ€ 
ç•¶åˆ...æ›¾ â€œAt first...everâ€ 

Inter-Sent 
Phrase Pair 

30 ç•¶åˆ...å¾Œä¾† â€œInitially...Laterâ€ 
æœ€æ—©...ç·Šæ¥è‘— â€œAt first...Thenâ€ 

Contingency Single Phrase 62 å¦‚æ­¤ä¸€ä¾† â€œAs a resultâ€ 
å‡è¨­ â€œIfâ€ 

Intra-Sent 
Phrase Pair 

180 å¦‚æœ...å‰‡ â€œIf ... thenâ€ 
ç„¡è«–...éƒ½ â€œWhether ...â€ 

Inter-Sent 
Phrase Pair 

14 æ—¢ç„¶...çœ‹ä¾† â€œSince... It seemsâ€ 
å¹¸è€Œ...ä¸ç„¶ â€œFortunately... otherwiseâ€ 

Comparison Single Phrase 34 ç›¸å°çš„ â€œIn contrastâ€ 
æœªæ–™ â€œUnexpectedlyâ€ 

Intra-Sent 
Phrase Pair 

38 å³ä½¿...å» â€œEven ... butâ€ 
é›–ç„¶...ä» â€œAlthough...stillâ€ 

Inter-Sent 
Phrase Pair 

15 é›–èªª...å…¶å¯¦ â€œAlthough... In factâ€ 
å„˜ç®¡...ç„¶è€Œ â€œAlthough... Howeverâ€ 

Expansion Single Phrase 182 é™¤æ­¤ä¹‹å¤– â€œin additionâ€ 
è€Œä¸” â€œmoreoverâ€ 

Intra-Sent 
Phrase Pair 

106 ä¸åª...è€Œä¸” â€œNot only...but alsoâ€ 
æˆ–è€…...æˆ–è€… â€œor...orâ€ 

Inter-Sent 
Phrase Pair 

26 é¦–å…ˆ...å…¶æ¬¡ â€œFirstly...Secondlyâ€ 
æ—¢ç„¶...æ³ä¸” â€œSince...Furthermoreâ€ 

Table 2: Statistics of connectives (discourse markers). 

264



 
Figure 5: The upper three level sub-tree of (S1) and the 

punctuation sub-tree of (S1). 
 

as a syntactic tree, and three levels of tree 
expansions are extracted as structured syntactic 
features. 

To capture syntactic structure, we get the 
syntactic tree for each sentence using the Stanford 
parser, and extract the sub-tree of the upper three 
levels, which represents the fundamental 
composition of this sentence. In addition, all the 
paths from the root to each punctuation node in a 
sentence are extracted. From the paths, the depth of 
each comma node is counted, and the common 
parent node of every adjacent clause is also 
extracted. For example, the upper three level sub-
tree of the syntactic tree of (S1) is shown in Figure 
5. In addition, the sub-tree in the dotted line forms 
the structure of the punctuations in the (S1).  
Polarity: A Comparison relation implies its two 

arguments are contrasting, and some contrasts are 
presented with different polarities in the two 
arguments. For example, sample (S5) is a case of 
Comparison. 

 
(S5)  å„˜ç®¡å¤©ç„¶ç’°å¢ƒå¦‚æ­¤å„ªè¶Šï¼Œäººç‚ºçš„ä¸å¹¸é‚„

æ˜¯å«é«˜æ£‰å­æ°‘ä¸å¾—å¥½éï¼Œéåšæˆ°äº‚çš„ç—›æ¥šã€‚ 
(â€œDespite such favorable natural environment, 
man-made disasters still make the Khmer people 
unfortunate to suffer from the pain of war.â€) 

 
The first clause in (S5) is positive (â€œfavorable 

natural environmentâ€), while the last two clauses 
are negative (â€œunfortunate to suffer from the pain 
of warâ€). Besides the connectives å„˜ç®¡ â€œdespiteâ€ 
and é‚„ æ˜¯  â€œstillâ€, the opposing polarity values 
between the first and the last two clauses is also a 
strong clue to the existence of a Comparison 

relation. In addition, the same polarity of the last 
two clauses is also a hint that no Comparison 
relation occurs between them. 

To capture polarity information, we estimate the 
polarity of each clause and detect the negations 
from the clause. The polarity score is a real number 
estimated by a sentiment dictionary-based 
algorithm. For each clause, the polarity score, and 
the existence of negation are taken as features. 

4 Experiments and Discussion 

4.1   Experimental Results 

All the models in the experiments are evaluated by 
5-fold cross-validation. The metrics are accuracies 
and macro-averaged F-scores. The t-test is used for 
significance testing. 

We firstly examine our model for the task of 
two-way classification. In this task, binary 
classifiers are trained to predict the existence of 
Contingency and Comparison relations in a given 
sentence. For meaningful comparison, a majority 
classifier is used as a baseline model, which 
always predicts the majority class. In the dataset, 
72.6% of the sentences involve neither 
Contingency nor Comparison. Thus, the major 
class is â€œNilâ€, and the accuracy and the F-score of 
the baseline model is 72.6% and 42.06%, 
respectively. 

The experimental results for the two-way 
classification task are shown in Table 3. In the 
table, the symbol â€  denotes the lowest accuracy 
which has a significant improvement over the 
baseline at p=0.05 for the two models. The symbol 
â€¡ denotes the adding of a single feature yields a 
significant improvement for the model at p=0.005. 

The performance of the decision tree and the 
SVM are similar in terms of accuracy and F-score. 
Overall, the decision tree model achieves better 
accuracies. In the two-way classification task, the 
decision tree model with only the Word feature 
achieves an accuracy of 76.75%, which is 
significantly better than the baseline at p=0.05. For 
both the decision tree and the SVM, Connective is 
the most useful feature: performance is 
significantly improved with the addition of 
Connective.  

Besides the binary classification task, we extend 
our model to tackle the task of finer classification. 
In the second task, four-way classifiers are trained  

265



 
 Decision Tree SVM 
Features Accuracy F-Score Accuracy F-Score 
Word â€ 76.75%  58.94% 72.36% 56.54% 
+POS  77.15% 61.72% 72.28%  60.53% 
+Length 77.15%  61.72%  72.60% 61.09% 
+Connective â€¡81.63%  71.11% â€¡78.05% 69.17% 
+Dependency 81.14% 70.79% 77.80% 68.79% 
+Structure  81.30%  70.78%  â€ 77.48% 69.08% 
+Polarity  81.30%  70.78% 77.64% 69.09% 

Table 3: Performance of the two-way classification. 
 
  Decision Tree SVM 
Features Accuracy F-Score Accuracy F-Score 
Word â€ 76.50%  34.72% 73.58% 31.54% 
+POS  76.99% 36.77% 72.52%  34.44% 
+Length  76.99%  36.77% 72.36% 34.54% 
+Connective  79.84%  44.08% â€¡77.89%  45.26% 
+Dependency 79.92% 44.47% â€ 77.07% 44.42% 
+Structure   79.92%   44.47% 77.15% 44.69% 
+Polarity  79.92%  44.47% 77.40% 44.80% 

Table 4: Performance of the four-way classification. 
 

 Decision Tree SVM 
Features Accuracy F-Score Accuracy F-Score 
Word 73.66%   3.00% 70.00% 3.62% 
+POS  73.66% 3.00% 69.84% 4.29% 
+Length 73.66%  3.00% 70.00% 5.08% 
+Connective  74.80%  4.90% 74.39% 7.66% 
+Dependency  74.72% 4.61% 72.60% 5.60% 
+Structure  74.72%  4.61% 73.01% 5.49% 
+Polarity  74.72%  4.61% 72.76% 5.23% 

Table 5: Performance of the 49-way classification.  
 

Task Explicit Implicit 
Accuracy F-score Accuracy F-score 

2-way 77.97% 69.26% 88.98% 50.64% 
4-way 76.06% 42.54% 88.98% 31.39% 
49-way 71.33% 4.88% 89.41% 1.92% 

Table 6: Performances for explicit cases and implicit 
cases. 

 
to predict a given sentence with four classes: 
existence of Contingency relations only, existence 
of Comparison relations only, existence of Both 
relations, and Nil. The experimental results of the 
four-way classification task are shown in Table 4. 
Consistent with the results of the two-way 
classification task, the addition of Connective to 
the SVM yields a significant improvement at 
p=0.005. The performance between the decision 
tree and the SVM is still similar, but the SVM 
achieves a slightly better F-score of 45.26% in 
comparison with the best F-score of 44.47% 
achieved by the decision tree. 

We further extend our model to predict the full 
relation structure of a given sentence as shown in 
Figure 1 and Figure 4. This is a 49-way 
classification task because there are 49 types of the 
full relation structures in the dataset. Not only as 
many as 49-ways, 72.6% of instances belong to the 
Nil relation, which yields an unbalanced 
classification problem. The experimental results 
are shown in Table 5. In the most challenging case, 
the SVM achieves a better F-score of 7.66% in 
comparison with the F-score of 4.90% achieved by 
the decision tree. Connective is still the most 
helpful feature. Comparing the F-scores of the 
SVM in the three tasks with the F-scores of the 
decision tree, it shows that the SVM performs 
better for predicting finer classes. 

4.2 Explicit versus Implicit 

We compare the performances between the explicit 
instances and the implicit instances for the three 
tasks with the decision tree model trained on all 
features.  The results are shown in Table 6. 

The higher accuracies and the lower F-scores of 
the implicit cases are due to the fact that the 
classifier tends to predict the sentences as Nil when 
no connective is found, and most implicit samples 
are Nil. For example, the relation of Contingency 
in implicit sample (S6) should be inferred from the 
meaning of å¸¶çµ¦ â€œbroughtâ€. 

 
(S6) å¾—å¤©ç¨åšçš„åœ°ç†ç’°å¢ƒï¼Œçš„ç¢ºå¸¶çµ¦é€™å€‹ç™¾

å¹´æ¸¯åŸ ç„¡çª®çš„è²¡å¯Œã€‚(â€œThe unique geographical 
environment, it really brought the infinite wealth to 
this hundred-year port.â€) 

 
In addition, some informal/spoken phrases are 

useful clues for predicting the relations, but they 
are not present in our connective dictionary. For 
example, the phrase çš„ è©±  â€œifâ€ implies a 
Contingency relation in (S7). This issue can be 
addressed by using a larger connective dictionary 
that contains informal and spoken phrases. 

 
(S7) æƒ³è¦ä»¥è‡ªåŠ©æ—…è¡Œçš„æ–¹å¼é€²è¡Œçš„è©±ï¼Œé‚£éº¼

éš¨åœ˜æ—…éŠå‘¢ï¼Ÿ (â€œIf you want to backpacking, how 
about an organized tour?â€) 

   
We regard an instance as explicit if there is at 

least one connective in the sentence. However, 
many explicit instances are still not easy to label 

266



even with the connectives. As a result, predicting 
explicit samples is much more challenging than the 
task of recognizing explicit discourse relations in 
English. One reason is the ambiguous usage of 
connectives as shown in (S2). The following 
sentence depicts another issue. The word ä½†æ˜¯ 
â€œhoweverâ€ in (S8) is a connective used as a marker 
of an inter-sentential relation. That is, the entire 
sentence is one of the arguments of an inter-
sentential Comparison relation, but it does not 
contain any intra-sentential relation inside the 
sentence itself.  

 
(S8) ä½†æ˜¯ï¼Œæ“ä¸€å£æµåˆ©ä¸­æ–‡çš„å‚…å¾åº·å‰‡å…¬é–‹

æ‰¹è©•é€™ç¨®çœ‹æ³•ã€‚(â€œHowever, Fu Wu Kang, who 
speaks fluent Chinese, openly criticizes this 
opinion.â€) 

 
The fact that connectives possess multiple 

senses is one of the important reasons for their 
misclassification. This issue can be addressed by 
employing contextual information such as the 
neighboring sentences. 

4.3 Number of Clauses 
We compare the performance among the 2-clause 
instances, the 3-clause instances, and the 4-clause 
instances for the three tasks with the decision tree 
model trained on all the features. The accuracies 
(A) and F-scores (F) are reported in Table 7.  

Comparing the two-way classification and the 
four-way classification tasks, the performance of 
the longer instances decreases a little in relation 
labeling. Although sentence complexity increases 
with length, a longer sentence provides more 
information at the same time. In the 49-way 
classification, the model should predict the 
sentence structure and the relation tags from the 49 
candidate classes. The performances are greatly 
decreased because the feasible classes are 
substantially increased along with the number of 
clauses.  

4.4 Contingency versus Comparison 
The confusion matrix of the decision tree model 
trained on all features for the four-way 
classification is shown in Table 8. Each row 
represents the samples in an actual class, while 
each column of the matrix represents the samples 
in a predicted class. The precision (P), recall (R), 

  
Task 2-Clause 3-Clause 4-Clause 

A (%) F (%) A (%) F (%) A (%) F (%) 
2-way 81.80 66.39 78.52 70.32 79.41 69.32 
4-way 79.84 49.98 75.62 42.64 80.88 46.73 
49-way 80.23 29.62 70.02 9.56 69.85 2.25 

Table 7: Performances of clauses of different lengths. 
 
Actual 
Class 

Predicted Class Performance 
Cont. Comp. Both Nil P (%) R (%) F (%) 

Cont. 61 3 0 131 81.33 31.28 45.19 
Comp. 3 40 0 83 74.07 31.75 44.44 
Both 2 4 0 5 0 0 0 
Nil 9 7 0 882 80.11 98.22 88.24 

Table 8: Confusion matrix of the best model in the 4-
way classification. 

 
Feature instance Category Usages 
The first token in the third clause is the word
ä½† â€œbut; howeverâ€ 

Word 100% 

The first token in the second clause is the 
word ä½† â€œbut; howeverâ€ 

Word 99% 

The first token in the third clause is a single 
connective of Contingency 

Connective 98% 

The first token in the first clause is the word 
ç”±æ–¼ â€œbecause; due toâ€ 

Word 96% 

There is at least one word ä»¥å… â€œin order to 
avoidâ€ in the entire sentence 

Word 95% 

The first token in the second clause is the 
word è€Œ â€œmoreover; while; butâ€ 

Word 94% 

The first token in the third clause is a single 
connective of Comparison 

Connective 93% 

The second clause contains a single 
connective of Contingency 

Connective 92% 

The first token in the second clause is a 
single connective of Contingency 

Connective 91% 

The first clause contains a single connective 
of Contingency 

Connective 90% 

Table 9: Instances of the top ten useful features for the 
decision tree model 

 
and F-score (F) for each class are provided on the 
right side of the table. The class Both is too small 
to train the model, thus our model does not 
correctly predict the samples in the Both class. The 
confusion matrix shows that the confusions 
between the classes Contingency and Comparison 
are very rare. The major issue is to distinguish 
Contingency and Comparison from the largest 
class, Nil. The lower recall of the Contingency and 
Comparison relations also show that our model 
tends to predict the instances as the largest class. 

4.5 Features 
The top ten useful feature instances reported by the 
decision tree model in the 49-way classification are 
shown in Table 9. Word and Connective provide 
useful information for the classification. Moreover, 

267



seven of the ten feature instances are about the 
word or the connective category of the first token 
in each clause. This result shows that it is crucial to 
employ the information of the first token in each 
clause as distinguished features. Certain words, for 
example, ä½† â€œbut; howeverâ€, ç”±æ–¼ â€œbecause; due 
toâ€, and è€Œ â€œmoreover; while; butâ€ are especially 
useful for deciding the relations. For this reason, 
labeling these words carefully is necessary. All the 
synonyms for each of these words should be 
clustered and assigned the same category. In 
addition, a dedicated extractor should be involved 
in accurately fetching these words from the 
sentence in order to reduce tokenization errors 
introduced by the Chinese word segmenter.  

The advanced features such as Dependency, 
Structure, and Polarity are not helpful as expected. 
One possible reason is that the training data is still 
not enough to model the complex features. In such 
a case, the surface features are even more useful. 

Sample (S1) shows an interesting case of the use 
of polarity information. The first clause of (S1) is 
positive (å˜—è©¦è¦è®“é•·æœŸä¾†ä½œç‚ºå¤§å°åŒ—å¾ŒèŠ±åœ’çš„
é™½æ˜å±±å€æ›´å›æ­¸è‡ªç„¶  â€œtried to make the 
Yangmingshan area a more natural state as the 
long-term garden of Taipeiâ€), the second clause of 
(S1) is also positive (ä½†éš¨è‘—é€±ä¼‘äºŒæ—¥ã€ç¶“æ¿Ÿç’°å¢ƒ
æ”¹å–„  â€œthe two-day weekend and the improved 
economic conditions.â€), while the last clause of 
(S1) is negative (éŠå®¢å¸¶ä¾†åœè»Šã€åƒåœ¾ç­‰é–“æ¥å½±
éŸ¿å»æ›´ç‚ºåš´é‡  â€œthe issues of tourists parking, 
garbage, and other indirect effectsâ€). The polarity 
of the last clause is opposite to those of the second 
clause, but they do not form a Comparison relation. 
Instead, a Contingency relation occurs between the 
last two clauses. Likewise, the polarities of the first 
and second clauses are both positive, but a 
Comparison relation occurs after the first clause. In 
fact, we realize that this is a complex case after 
performing an in-depth analysis. Because the last 
clause plays the role of effect in the Contingency 
relation, the negative polarity of the last clause 
makes the last two clauses form a negative polarity. 
For this reason, a Comparison relation occurs 
between the first argument with positive polarity 
and the second argument (i.e., the last two clauses) 
with negative polarity without a doubt. The 
polarity diagram of sample (S1) is shown in Figure 
6. 

 

 

 
Figure 6: Polarity diagram of (S1). 

 
Overall, the interaction among structure, relation, 

and polarity is complicated. The surface polarity 
information we extract by using the sentiment 
dictionary-based algorithm does not capture such 
complexity well. A dedicated structure-sensitive 
polarity tagger will be utilized in future work. 

5 Conclusion and Future Work 

In this paper, we addressed the problem of intra-
sentential Contingency and Comparison relation 
detection in Chinese. This is a challenging task 
because Chinese sentences tend to be very long 
and therefore contain more clauses. To tackle this 
problem, we constructed a moderate-sized corpus 
and proposed a learning-based approach that 
achieves accuracies of 81.63%, 79.92%, and 
74.80% and F-scores of 71.11%, 45.26%, and 
7.66% in the two-way, the four-way, and the 49-
way classification tasks, respectively. 

From the experiments, we found that 
performance could be significantly improved by 
adding the Connective feature. The next step is to 
enlarge the connective dictionary automatically by 
a text mining approach, in particular with those 
informal connectives, in order to boost 
performance. The advanced features such as 
Dependency, Structure, and Polarity are not as 
helpful as expected due to the small size of the 
corpus. In future work, we plan to construct a large 
Chinese discourse Treebank based on the 
methodology proposed in Section 2 and release the 
corpus to the public. 

Naturally, the intra-sentential relations are 
important cues for discourse relation detection at 
the inter-sentential level. How to integrate cues 
from these two levels will be investigated. Besides, 
relation labeling and structure prediction are 
tackled at the same time with the same learning 
algorithm in this study. We will explore different 
methods to tackle the two problems separately to 
reduce the complexity.  

268



References 
Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-

ming Gao, and Kuang-Yu Chen. 2000. Sinica 
Treebank: Design Criteria, Annotation Guidelines, 
and On-line Interface. In Proceedings of 2nd Chinese 
Language Processing Workshop (Held in conjunction 
with the 38th Annual Meeting of the Association for 
Computational Linguistics, ACL-2000), pages 29-37. 

Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese 
Discourse Relation Recognition. In Proceedings of 
5th International Joint Conference on Natural 
Language Processing (IJCNLP 2011), pages 1442-
1446. 

Aravind Joshi and Bonnie L. Webber. 2004. The Penn 
Discourse Treebank. In Proceedings of the Language 
and Resources and Evaluation Conference, Lisbon. 

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. 
Recognizing Implicit Discourse Relations in the Penn 
Discourse Treebank. In Proceedings of the 2009 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2009), Singapore.  

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.  
Automatically Evaluating Text Coherence Using 
Discourse Relations. In Proceedings of the 49th 
Annual Meeting of the Association for Computational 
Linguistics (ACL 2011), pages 997-1006. 

Emily Pitler and Ani Nenkova. 2009. Using Syntax to 
Disambiguate Explicit Discourse Connectives in Text. 
In Proceedings of the ACL-IJCNLP 2009 Conference 
Short Papers, pages 13-1, Singapore. 

Emily Pitler, Annie Louis, and Ani Nenkova. 2009. 
Automatic Sense Prediction for Implicit Discourse 
Relations in Text. In Proceedings of the Joint 
Conference of the 47th Annual Meeting of the ACL 
and the 4th International Joint Conference on 
Natural Language Processing of the AFNLP (ACL-
IJCNLP 2009), Singapore. 

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
Miltsakaki, Livio Robaldo, Aravind Joshi, and 
Bonnie Webber. 2008. The Penn Discourse Treebank 
2.0. In Proceedings of the 6th International 
Conference on Language Resources and Evaluation 
(LREC). 

WenTing Wang, Jian Su, and Chew Lim Tan. 2010. 
Kernel Based Discourse Relation Recognition with 
Temporal Ordering Information. In Proceedings of 
the 48th Annual Meeting of the Association for 
Computational Linguistics (ACL 2010), Uppsala, 
Sweden, July. 

Nianwen Xue. 2005. Annotating Discourse Connectives 
in the Chinese Treebank. In Proceedings of the 
Workshop on Frontiers in Corpus Annotation II: Pie 
in the Sky, pages 84-91. 

269


