



















































Ontologically Grounded Multi-sense Representation Learning for Semantic Vector Space Models


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 683–693,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Ontologically Grounded Multi-sense Representation Learning for Semantic
Vector Space Models

Sujay Kumar Jauhar Chris Dyer Eduard Hovy
Language Technologies Institute

School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA

{sjauhar, cdyer, hovy}@cs.cmu.edu

Abstract

Words are polysemous. However, most ap-
proaches to representation learning for lexical
semantics assign a single vector to every sur-
face word type. Meanwhile, lexical ontologies
such as WordNet provide a source of com-
plementary knowledge to distributional infor-
mation, including a word sense inventory. In
this paper we propose two novel and general
approaches for generating sense-specific word
embeddings that are grounded in an ontology.
The first applies graph smoothing as a post-
processing step to tease the vectors of differ-
ent senses apart, and is applicable to any vec-
tor space model. The second adapts predictive
maximum likelihood models that learn word
embeddings with latent variables representing
senses grounded in an specified ontology. Em-
pirical results on lexical semantic tasks show
that our approaches effectively captures infor-
mation from both the ontology and distribu-
tional statistics. Moreover, in most cases our
sense-specific models outperform other mod-
els we compare against.

1 Introduction

Vector space models (VSMs) of word meaning play
a central role in computational semantics. These
represent meanings of words as contextual feature
vectors in a high-dimensional space (Deerwester et
al., 1990) or some embedding thereof (Collobert
and Weston, 2008) and are learned from unanno-
tated corpora. Word vectors in these continuous
space representations can be used for meaningful se-
mantic operations such as computing word similar-
ity (Turney, 2006), performing analogical reasoning
(Turney, 2013) and discovering lexical relationships

(Mikolov et al., 2013b). They have also proved use-
ful in downstream NLP applications such as infor-
mation retrieval (Manning et al., 2008) and question
answering (Tellex et al., 2003), among others.

However, VSMs remain flawed because they as-
sign a single vector to every word, thus ignoring
the possibility that words may have more than one
meaning. For example, the word “bank” can ei-
ther denote a financial institution or the shore of
a river. The ability to model multiple meanings is
an important component of any NLP system, given
how common polysemy is in language. The lack
of sense annotated corpora large enough to robustly
train VSMs, and the absence of fast, high quality
word sense disambiguation (WSD) systems makes
handling polysemy difficult.

Meanwhile, lexical ontologies, such as WordNet
(Miller, 1995) specifically catalog sense invento-
ries and provide typologies that link these senses
to one another. These hand-curated ontologies pro-
vide a complementary source of information to dis-
tributional statistics. Recent research tries to lever-
age this information to train better VSMs (Yu and
Dredze, 2014; Faruqui et al., 2014), but does not
tackle the problem of polysemy. Parallely, work on
polysemy for VSMs revolves primarily around tech-
niques that cluster contexts to distinguish between
different word senses (Reisinger and Mooney, 2010;
Huang et al., 2012), but does not integrate ontologies
in any way.

In this paper we present two novel approaches to
integrating ontological and distributional sources of
information. Our focus is on allowing already exist-
ing, proven techniques to be adapted to produce on-
tologically grounded word sense embeddings. Our
first technique is applicable to any sense-agnostic

683



VSM as a post-processing step that performs graph
propagation on the structure of the ontology. The
second is applicable to the wide range of current
techniques that learn word embeddings from predic-
tive models that maximize the likelihood of a corpus
(Collobert and Weston, 2008; Mnih and Teh, 2012;
Mikolov et al., 2013a). Our technique adds a latent
variable representing the word sense to each token
in the corpus, and uses EM to find parameters. Us-
ing a structured regularizer based on the ontological
graph, we learn grounded sense-specific vectors.

There are several reasons to prefer ontologies as
distant sources of supervision for learning sense-
aware VSMs over previously proposed unsuper-
vised context clustering techniques. Clustering ap-
proaches must often parametrize the number of
clusters (senses), which is neither known a priori
nor constant across words (Kilgarriff, 1997). Also
the resulting vectors remain abstract and uninter-
pretable. With ontologies, interpretable sense vec-
tors can be used in downstream applications such as
WSD, or for better human error analysis. Moreover,
clustering techniques operate on distributional simi-
larity only whereas ontologies support other kinds of
relationships between senses. Finally, the existence
of cross-lingual ontologies would permit learning
multi-lingual vectors, without compounded errors
from word alignment and context clustering.

We evaluate our methods on 3 lexical semantic
tasks across 7 datasets and show that our sense-
specific VSMs effectively integrate knowledge from
the ontology with distributional statistics. Empir-
ically, this results in consistently and significantly
better performance over baselines in most cases. In
the more marginal cases, analysis reveals that our
performance is a result of the deficient structure
of the ontology. We discuss and compare the two
different approaches from the perspectives of per-
formance, generalizability, flexibility and computa-
tional efficiency. Finally, we qualitatively analyze
the vectors and show that they indeed capture sense-
specific semantics.

2 Unified Symbolic and Distributional
Semantics

In this section, we present our two techniques for in-
ferring sense-specific vectors grounded in an ontol-

ogy. We begin with notation. LetW = {w1, ..., wn}
be a set of word types, and Ws = {sij | ∀wi ∈
W , 1 ≤ j ≤ ki} a set of senses, with ki the num-
ber of senses of wi. Moreover, let Ω = (TΩ, EΩ)
be an ontology represented by an undirected graph.
The vertices TΩ = {tij | ∀ sij ∈ Ws} correspond
to the word senses in the set Ws, while the edges
EΩ = {erij−i′j′} connect some subset of word sense
pairs (sij , si′j′) by semantic relation r1.

2.1 Retrofitting Vectors to an Ontology

Our first technique assumes that we already have
a vector space embedding of a vocabulary Û =
{ûi| ∀ wi ∈ W}. We wish to infer vectors V =
{vij | ∀ sij ∈ Ws} for word senses that are maxi-
mally consistent with both Û and Ω, by some notion
of consistency. We formalize this notion as MAP
inference in a Markov network (MN).

The MN we propose contains variables for every
vector in Û and V . These variables are connected to
one another by dependencies as follows. Variables
for vectors vij and vi′j′ are connected iff there exists
an edge erij−i′j′ ∈ EΩ connecting their respective
word senses in the ontology. Furthermore, vectors
ûi for the word types wi are each connected to all
the vectors vij of the different senses sij of wi. If wi
is not contained in the ontology, we assume it has
a single unconnected sense and set it’s only sense
vector vi1 to it’s empirical estimate ûi.

The structure of this MN is illustrated in Figure
1, where the neighborhood of the ambiguous word
“bank” is presented as a factor graph.

We set each pairwise clique potential to be of the
form exp(a‖u − v‖2) between neighboring nodes.
Here u and v are the vectors corresponding to these
nodes, and a is a weight controlling the strength of
the relation between them. We use the Euclidean
norm instead of a distance based on cosine similarity
because it is more convenient from an optimization
perspective.

Our inference problem is to find the MAP esti-
mate of the vectors V , given Û , which may be stated

1For example there might be a “synonym” edge between the
word senses “cat(1)” and “feline(1)”.

684



Figure 1: A factor graph depicting the retrofitting model
in the neighborhood of the word “bank”. Observed vari-
ables corresponding to word types are shaded in grey,
while latent variables for word senses are in white.

as follows:

C(V ) = arg min
V

∑
i−ij

α‖ûi − vij‖2

+
∑
ij−i′j′

βr‖vij − vi′j′‖2
(1)

Here α is the sense-agnostic weight and βr are
relation-specific weights for different semantic re-
lations. This objective encourages vectors of neigh-
boring nodes in the MN to pull closer together, lever-
aging the tension between sense-agnostic neighbors
(the first summation term) and ontological neighbors
(the second summation term). This allows the dif-
ferent neighborhoods of each sense-specific vector
to tease it apart from its sense-agnostic vector.

Taking the partial derivative of the objective in
equation 1 with respect to vector vij and setting to
zero gives the following solution:

vij =

αûi +
∑

i′j′∈Nij
βrvi′j′

α+
∑

i′j′∈Nij
βr

(2)

where Nij denotes the set of neighbors of ij. Thus,
the MAP sense-specific vector is an α-weighted
combination of its sense-agnostic vector and the βr-
weighted sense-specific vectors in its ontological
neighborhood.

We use coordinate descent to iteratively update
the variables V using equation 2. The optimiza-
tion problem in equation 1 is convex, and we nor-
mally converge to a numerically satisfactory station-
ary point within 10 to 15 iterations. This procedure

Algorithm 1 Outputs a sense-specific VSM, given a
sense-agnostic VSM and ontology

1: function RETROFIT(Û ,Ω)
2: V (0) ← {v(0)ij = ûi | ∀sij ∈Ws}
3: while ‖v(t)ij − v(t−1)ij ‖ ≥ � ∀i, j do
4: for tij ∈ TΩ do
5: v(t+1)ij ← update using equation 2
6: end for
7: end while
8: return V (t)
9: end function

is summarized in Algorithm 1. The generality of this
algorithm allows it to be applicable to any VSM as
a computationally attractive post-processing step.

An implementation of this technique is avail-
able at https://github.com/sjauhar/
SenseRetrofit.

2.2 Adapting Predictive Models with Latent
Variables and Structured Regularizers

Many successful techniques for semantic represen-
tation learning are formulated as models where the
desired embeddings are parameters that are learnt to
maximize the likelihood of a corpus (Collobert and
Weston, 2008; Mnih and Teh, 2012; Mikolov et al.,
2013a). In our second approach we extend an exist-
ing probability model by adding latent variables rep-
resenting the senses, and we use a structured prior
based on the topology of the ontology to ground the
sense embeddings. Formally, we assume a corpus
D = {(w1, c1), . . . , (wN , cN )} of pairs of target and
context words, and the ontology Ω, and we wish to
infer sense-specific vectors V = {vij | ∀ sij ∈Ws}.

Consider a model with parameters θ (V ∈ θ)
that factorizes the probability over the corpus as∏

(wi,ci)∈D p(wi, ci; θ). We propose to extend such
a model to learn ontologically grounded sense vec-
tors by presenting a general class of objectives of the
following form:

C(θ) = arg max
θ

∑
(wi,ci)∈D

log(
∑
sij

p(wi, ci, sij ; θ)) + log pΩ(θ)
(3)

This objective introduces latent variables sij for
senses and adds a structured regularizer pΩ(θ)

685



Chase loaned money

bank2

bank
p(s | w)

p(c | s)
Context word
(observed)

Sense
(latent)

Word
(observed)

Figure 2: The generative process associated with the
skip-gram model, modified to account for latent senses.
Here, the context of the ambiguous word “bank” is gen-
erated from the selection of a specific latent sense.

that grounds the vectors V in an ontology. This
form permits flexibility in the definition of both
p(wi, ci, sij ; θ) and pΩ(θ) allowing for a general yet
powerful framework for adapting MLE models.

In what follows we show that the popular skip-
gram model (Mikolov et al., 2013a) can be adapted
to generate ontologically grounded sense vectors.
The classic skip-gram model uses a set of parame-
ters θ = (U, V ), with U = {ui | ∀ci ∈ W} and
V = {vi | ∀wi ∈ W} being sets of vectors for
context and target words respectively. The genera-
tive story of the skip-gram model involves generat-
ing the context word ci conditioned on an observed
word wi. The conditional probability is defined to

be p(ci | wi; θ) = exp(ui · vi)∑
ci′∈W exp(ui′ · vi)

.

We modify the generative story of the skip-gram
model to account for latent sense variables by first
selecting a latent word sense sij conditional on the
observed word wi, then generating the context word
ci from the sense distinguished word sij . This pro-
cess is illustrated in Figure 2. The factorization
p(ci | wi; θ) =

∑
sij
p(ci | sij ; θ) × p(sij | wi; θ)

follows from the chain rule since senses are word-
specific. To parameterize this distribution, we de-
fine a new set of model parameters θ = (U, V,Π),
where U remains identical to the original skip-gram,
V = {vij | ∀sij ∈Ws} are a set of vectors for word
senses, and Π are the context-independent sense pro-
portions πij = p(sij | wi). We use a Dirichlet prior
over the multinomial distributions πi for every wi,
with a shared concentration parameter λ.

We define the ontological prior on vectors as
pΩ(θ) ∝ exp(−γ

∑
ij−i′j′

βr‖vij − vi′j′‖2), where γ

controls the strength of the prior. We note the sim-
ilarity to the retrofitting objective in equation 1, ex-

cept with α = 0. This leads to the following realiza-
tion of the objective in equation 3:

C(θ) = arg max
θ

∑
(wi,ci)∈D

log
(∑
sij

p(ci | sij ; θ)×

p(sij | wi; θ)
)
− γ

∑
ij−i′j′

βr‖vij − vi′j′‖2

(4)

This objective can be optimized using EM, for the
latent variables, and with lazy updates (Carpenter,
2008) every k words to account for the prior regu-
larizer. However, since we are primarily interested
in learning good vector representations, and we want
to learn efficiently from large datasets, we make the
following simplifications. First, we perform “hard”
EM, selecting the most likely sense at each position
rather than using the full posterior over senses. Also,
given that the structured regularizer pΩ(θ) is essen-
tially the retrofitting objective in equation 1, we run
retrofitting periodically every k words (with α = 0
in equation 2) instead of lazy updates.2

The following decision rule is used in the “hard”
E-step:

sij = arg max
sij

p(ci | sij ; θ(t))π(t)ij (5)

In the M-step we use Variational Bayes to update Π
with:

π
(t+1)
ij ∝

exp
(
ψ
(
c̃(wi, sij) + λπ

(0)
ij

))
exp (ψ (c̃(wi) + λ))

(6)

where c̃(·) is the online expected count and ψ(·) is
the digamma function. This approach is motivated
by Johnson (2007) who found that naive EM leads
to poor results, while Variational Bayes is consis-
tently better and promotes faster convergence of the
likelihood function. To update the parameters U and
V , we use negative sampling (Mikolov et al., 2013a)
which is an efficient approximation to the original
skip-gram objective. Negative sampling attempts to
distinguish between true word pairs in the data, rel-
ative to noise. Stochastic gradient descent on the
following equation is used to update the model pa-

2We find this gives slightly better performance.

686



rameters U and V :

L = log σ(ui · vij) +
∑
j′
j′ 6=j

log σ(−ui · vij′)

+
∑
m

Eci′∼Pn(c) [log σ(−ui′ · vij)]
(7)

Here σ(·) is the sigmoid function, Pn(c) is a noise
distribution computed over unigrams and m is the
negative sampling parameter. This is almost exactly
the same as negative sampling proposed for the orig-
inal skip-gram model. The only change is that we
additionally take a negative gradient step with re-
spect to all the senses that were not selected in the
hard E-step. We summarize the training procedure
for the adapted skip-gram model in Algorithm 2.

Algorithm 2 Outputs a sense-specific VSM, given a
corpus and an ontology

1: function SENSEEM(D,Ω)
2: θ(0) ← initialize
3: for (wi, ci) ∈ D do
4: if period > k then
5: RETROFIT(θ(t),Ω)
6: end if
7: (Hard) E-step:
8: sij ← find argmax using equation 5
9: M-step:

10: Π(t+1) ← update using equation 6
11: U (t+1), V (t+1) ← update using equation 7
12: end for
13: return θ(t)
14: end function

3 Evaluation

In this section we detail experimental results on 3
lexical semantics tasks across 8 different datasets.
We begin by detailing the training and setup for our
experiments.

3.1 Resources, Data and Training
We use WordNet (Miller, 1995) as the sense repos-
itory and ontology in all our experiments. WordNet
is a large, hand-annotated ontology of English com-
posed of 117,000 clusters of senses, or “synsets” that
are related to one another through semantic relations
such as hypernymy and hyponymy. Each synset ad-
ditionally comprises a list of sense specific lemmas

which we use to form the nodes in our graph. There
are 206,949 such sense specific lemmas, which we
connect with synonym, hypernym and hyponym3 re-
lations for a total of 488,432 edges.

To show the applicability of our techniques to dif-
ferent VSMs we experiment with two different kinds
of base vectors.

Global Context Vectors (GC) (Huang et al.,
2012): These word vectors were trained using a neu-
ral network which not only uses local context but
also defines global features at the document level to
further enhance the VSM. We distinguish three vari-
ants: the original single-sense vectors (SINGLE), a
multi-prototype variant (MULTI), – both are avail-
able as pre-trained vectors for download4 – and a
sense-based version obtained by running retrofitting
on the original vectors (RETRO).

Skip-gram Vectors (SG) (Mikolov et al.,
2013a): We use the word vector tool Word2Vec5
to train skip-gram vectors. We define 6 variants:
a single-sense version (SINGLE), two multi-sense
variants that were trained by first sense disambiguat-
ing the entire corpus using WSD tools, – one unsu-
pervised (Pedersen and Kolhatkar, 2009) (WSD) and
the other supervised (Zhong and Ng, 2010) (IMS)
– a retrofitted version obtained from the single-
sense vectors (RETRO), an EM implementation of
the skip-gram model with the structured regularizer
as described in section 2.2 (EM+RETRO), and the
same EM technique but ignoring the ontology (EM).
All models were trained on publicly available WMT-
20116 English monolingual data. This corpus of 355
million words, although adequate in size, is smaller
than typically used billion word corpora. We use this
corpus because the WSD baseline involves prepro-
cessing the corpus with sense disambiguation, which
is slow enough that running it on corpora orders of
magnitude larger was infeasible.

Retrofitted variants of vectors (RETRO) are
trained using the procedure described in algorithm
1. We set the convergence criteria to � = 0.01 with
a maximum number of iterations of 10. The weights

3We treat edges as undirected, so hypernymy and hyponymy
are collapsed and unified in our representation schema.

4http://nlp.stanford.edu/~socherr/
ACL2012_wordVectorsTextFile.zip

5https://code.google.com/p/word2vec/
6http://www.statmt.org/wmt11/

687



in the update equation 2 are set heuristically: the
sense agnostic weight α is 1.0, and relations-specific
weights βr are 1.0 for synonyms and 0.5 for hyper-
nyms and hyponyms. EM+RETRO vectors are the
exception where we use a weight of α = 0.0 instead,
as required by the derivation in section 2.2.

For skip-gram vectors (SG) we use the following
standard settings, and do not tune any of the values.
We filter all words with frequency < 5, and pre-
normalize the corpus to replace all numeric tokens
with a placeholder. We set the dimensionality of the
vectors to 80, and the window size to 10 (5 con-
text words to either side of a target). The learning
rate is set to an initial value of 0.025 and diminished
linearly throughout training. The negative sampling
parameter is set to 5. Additionally for the EM vari-
ants (section 2.2) we set the Dirichlet concentration
parameter λ to 1000. We use 5 abstract senses for
the EM vectors, and initialize the priors uniformly.
For EM+RETRO, WordNet dictates the number of
senses; also when available WordNet lemma counts
are used to initialize the priors. Finally, we set the
retrofitting period k to 50 million words.

3.2 Experimental Results
We evaluate our models on 3 kinds of lexical seman-
tic tasks: similarity scoring, synonym selection, and
similarity scoring in context.

Similarity Scoring: This task involves using a
semantic model to assign a score to pairs of words.
We use the following 4 standard datasets in this
evaluation: WS-353 (Finkelstein et al., 2002), RG-
65 (Rubenstein and Goodenough, 1965), MC-30
(Miller and Charles, 1991) and MEN-3k (Bruni et
al., 2014). Each dataset consists of pairs of words
along with an averaged similarity score obtained
from several human annotators. For example an item
in the WS-353 dataset is “book, paper→ 7.46”. We
use standard cosine similarity to assign a score to
word pairs in single-sense VSMs, and the following
average similarity score to multi-sense variants, as
proposed by Reisinger and Mooney (2010):

avgSim(wi, wi′) =
1
kikj

∑
j,j′

cos(vij , vi′j′) (8)

The output of systems is evaluated against the gold
standard using Spearman’s rank correlation coeffi-
cient.

Synonym Selection: In this task, VSMs are used
to select the semantically closest word to a tar-
get from a list of candidates. We use the follow-
ing 3 standard datasets in this evaluation: ESL-
50 (Turney, 2001), RD-300 (Jarmasz and Szpakow-
icz, 2004) and TOEFL-80 (Landauer and Dumais,
1997). These datasets consist of a list of target words
that appear with several candidate lexical items. An
example from the TOEFL dataset is “rug → sofa,
ottoman, carpet, hallway”, with “carpet” being the
most synonym-like candidate to the target. We be-
gin by scoring all pairs composed of the target and
one of the candidates. We use cosine similarity for
single-sense VSMs, and max similarity for multi-
sense models7:

maxSim(wi, wi′) = max
j,j′

cos(vij , vi′j′) (9)

These scores are then sorted in descending order,
with the top-ranking score yielding the semantically
closest candidate to the target. Systems are evalu-
ated on the basis of their accuracy at discriminating
the top-ranked candidate.

The results for similarity scoring and synonym
selection are presented in table 1. On both tasks
and on all datasets, with the partial exception of
WS-353 and MEN-3k, our vectors (RETRO &
EM+RETRO) consistently yield better results than
other VSMs. Notably, both our techniques perform
better than preprocessing a corpus with WSD infor-
mation in unsupervised or supervised fashion (SG-
WSD & SG-IMS). Simple EM without an ontolog-
ical prior to ground the vectors (SG-EM) also per-
forms poorly.

We investigated the observed drop in performance
on WS-353 and found that this dataset consists of
two parts: a set of similar word pairs (e.g. “tiger”
and “cat”) and another set of related word pairs (e.g.
“weather” and “forecast”). The synonym, hypernym
and hyponym relations we use tend to encourage
similarity to the detriment of relatedness.

We ran an auxiliary experiment to show this. SG-
EM+RETRO training also learns vectors for context
words – which can be thought of as a proxy for re-
latedness. Using this VSM we scored a word pair
by the average similarity of all the sense vectors of

7Here we are specifically looking for synonyms, so the max
makes more sense than taking an average.

688



Word Similarity (ρ) Synonym Selection (%)
WS-353 RG-65 MC-30 MEN-3k ESL-50 RD-300 TOEFL-80

GC
SINGLE 0.623 0.629 0.657 0.314 47.73 45.07 60.87
MULTI 0.535 0.510 0.309 0.359 27.27 47.89 52.17
RETRO 0.543 0.661 0.714 0.528 63.64 66.20 71.01

SG

SINGLE 0.639 0.546 0.627 0.646 52.08 55.66 66.67
EM 0.194 0.278 0.167 0.228 27.08 33.96 40.00

WSD 0.481 0.298 0.396 0.175 16.67 49.06 42.67
IMS 0.549 0.579 0.606 0.591 41.67 53.77 66.67

RETRO 0.552 0.673 0.705 0.560 56.25 65.09 73.33
EM+RETRO 0.321 0.734 0.758 0.428 62.22 66.67 68.63

Table 1: Similarity scoring and synonym selection in English across several datasets involving different VSMs. Higher
scores are better; best scores within each category are in bold. In most cases our models consistently and significantly
outperform the other VSMs.

one word to the context vector of the other word,
averaged over both words. With this scoring func-
tion the correlation ρ jumped from 0.321 to 0.493.
While still not as good as some of the other VSMs, it
should be noted that this scoring function negatively
influences the similar word pairs in the dataset.

The MEN-3k dataset is crowd-sourced and con-
tains much diversity, with word pairs evidencing
similarity as well as relatedness. However, we aren’t
sure why the performance for GC-RETRO improves
greatly over GC-SINGLE for this dataset, while that
of SG-RETRO and SG-RETRO+EM drops in rela-
tion to SG-SINGLE.

Similarity Scoring in Context: As outlined by
Reisinger and Mooney (2010), multi-sense VSMs
can be used to consider context when computing
similarity between words. We use the SCWS dataset
(Huang et al., 2012) in these experiments. This
dataset is similar to the similarity scoring datasets,
except that they additionally are presented in con-
text. For example an item involving the words
“bank” and “money”, gives the words in their re-
spective contexts, “along the east bank of the Des
Moines River” and “the basis of all money laun-
dering” with a low averaged similarity score of 2.5
(on a scale of 1.0 to 10.0). Following Reisinger and
Mooney (2010) we use the following function to as-
sign a score to word pairs in their respective con-
texts, given a multi-sense VSM:

avgSimC(wi, ci, wi′ , ci′) =∑
j,j′

p(sij |ci, wi)p(si′j′ |ci′ , wi′)cos(vij , vi′j′) (10)

Vectors SCWS (ρ)
SG-WSD 0.343
SG-IMS 0.528

SG-RETRO 0.417
GC-RETRO 0.420

SG-EM 0.613
SG-EM+RETRO 0.587

GC-MULTI 0.657

Table 2: Contextual word similarity in English. Higher
scores are better.

As with similarity scoring, the output of systems
is evaluated against gold standard using Spearman’s
rank correlation coefficient.

The results are presented in table 2. Pre-
processing a corpus with WSD information in an un-
supervised fashion (SG-WSD) yields poor results.
In comparison, the retrofitted vectors (SG-RETRO
& GC-RETRO) already perform better, even though
they do not have access to context vectors, and thus
do not take contextual information into account. Su-
pervised sense vectors (SG-IMS) are also compe-
tent, scoring better than both retrofitting techniques.
Our EM vectors (SG-EM & SG-EM+RETRO) yield
even better results and are able to capitalize on
contextual information, however they still fall short
of the pretrained GC-MULTI vectors. We were
surprised that SG-EM+RETRO actually performed
worse than SG-EM, given how poorly SG-EM per-
formed in the other evaluations. However, an anal-
ysis again revealed that this was due to the kind
of similarity encouraged by WordNet rather than
an inability of the model to learn useful vectors.
The SCWS dataset, in addition to containing related

689



Method CPU Time
RETRO ~20 secs

EM+RETRO ~4 hours
IMS ~3 days
WSD ~1 year

Table 3: Training time associated with different methods
of generating sense-specific VSMs.

words – which we showed, hurt our performance
on WS-353 – also contains word pairs with differ-
ent POS tags. WordNet synonymy, hypernymy and
hyponymy relations are exclusively defined between
lemmas of the same POS tag, which adversely af-
fects performance further.

3.3 Discussion
While both our approaches are capable of integrat-
ing ontological information into VSMs, an impor-
tant question is which one should be preferred?
From an empirical point of view, the EM+RETRO
framework yields better performance than RETRO
across most of our semantic evaluations. Addi-
tionally EM+RETRO is more powerful, allowing to
adapt more expressive models that can jointly learn
other useful parameters – such as context vectors in
the case of skip-gram. However, RETRO is far more
generalizable, allowing it to be used for any VSM,
not just predictive MLE models, and is also empiri-
cally competitive. Another consideration is compu-
tational efficiency, which is summarized in table 3.

Not only is RETRO much faster, but it scales
linearly with respect to the vocabulary size, un-
like EM+RETRO, WSD, and IMS which are de-
pendent on the input training corpus. Nevertheless,
both our techniques are empirically superior as well
as computationally more efficient than both unsu-
pervised and supervised word-sense disambiguation
paradigms.

Both our approaches are sensitive to the structure
of the ontology. Therefore, an important considera-
tion is the relations we use and the weights we as-
sociate with them. In our experiments we selected
the simplest set of relations and assigned weights
heuristically, showing that our methods can effec-
tively integrate ontological information into VSMs.
A more exhaustive selection procedure with weight
tuning on held-out data would almost certainly lead
to better performance on our evaluation suite.

3.4 Qualitative Analysis

We qualitatively attempt to address the question of
whether the vectors are truly sense specific. In ta-
ble 4 we present the three most similar words of
an ambiguous lexical item in a standard VSM (SG-
SINGLE) in comparison with the three most similar
words of different lemma senses of the same lexical
item in grounded sense VSMs (SG-RETRO & SG-
EM+RETRO).

Word or Sense Top 3 Most Similar
hanging hung dangled hangs

hanging (suspending) shoring support suspension
hanging (decoration) tapestry braid smock

climber climbers skier Loretan
climber (sportsman) lifter swinger sharpshooter

climber (vine) woodbine brier kiwi

Table 4: The top 3 most similar words for two polyse-
mous types. Single sense VSMs capture the most fre-
quent sense. Our techniques effectively separates out the
different senses of words, and are grounded in WordNet.

The sense-agnostic VSMs tend to capture only the
most frequent sense of a lexical item. On the other
hand, the disambiguated vectors capture sense speci-
ficity of even less frequent senses successfully. This
is probably due to the nature of WordNet where the
nearest neighbors of the words in question are in fact
these rare words. A careful tuning of weights will
likely optimize the trade-off between ontologically
rare neighbors and distributionally common words.

In our analyses, we noticed that lemma senses that
had many neighbors (i.e. synonyms, hypernyms and
hyponyms), tended to have more clearly sense spe-
cific vectors. This is expected, since it is these neigh-
borhoods that disambiguate and help to distinguish
the vectors from their single sense embeddings.

4 Related Work

Since Reisinger and Mooney (2010) first proposed
a simple context clustering technique to generate
multi-prototype VSMs, a number of related efforts
have worked on adaptations and improvements rely-
ing on the same clustering principle. Huang et al.
(2012) train their vectors with a neural network and
additionally take global context into account. Nee-
lakantan et al. (2014) extend the popular skip-gram
model (Mikolov et al., 2013a) in a non-parametric

690



fashion to allow for different number of senses for
words. Guo et al. (2014) exploit bilingual align-
ments to perform better context clustering during
training. Tian et al. (2014) propose a probabilistic
extension to skip-gram that treats the different pro-
totypes as latent variables. This is similar to our sec-
ond EM training framework, and turns out to be a
special case of our general model. In all these pa-
pers, however, the multiple senses remain abstract
and are not grounded in an ontology.

Conceptually, our work is also similar to Yu and
Dredze (2014) and Faruqui et al. (2014), who treat
lexicons such as the paraphrase database (PPDB)
(Ganitkevitch et al., 2013) or WordNet (Miller,
1995) as an auxiliary thesaurus to improve VSMs.
However, they do not model senses in any way. Pile-
hvar et al. (2013) do model senses from an ontology
by performing random-walks on the Wordnet graph,
however their approach does not take distributional
information from VSMs into account.

Thus, to the best of our knowledge, our
work presents the first attempt at producing sense
grounded VSMs that are symbolically tied to lexi-
cal ontologies. From a modelling point of view, it
is also the first to outline a unified, principled and
extensible framework that effectively combines the
symbolic and distributional paradigms of semantics.

Both our models leverage the graph structure
of ontologies to effectively ground the senses of
a VSM. This ties into previous research (Das and
Smith, 2011; Das and Petrov, 2011) that propa-
gates information through a factor graph to per-
form tasks such as frame-semantic parsing and POS-
tagging across languages. More generally, this ap-
proach can be viewed from the perspective of semi-
supervised learning, with an optimization over a
graph loss function defined on smoothness proper-
ties (Corduneanu and Jaakkola, 2002; Zhu et al.,
2003; Subramanya and Bilmes, 2009).

Related to the problem of polysemy is the issue of
different shades of meaning a word assumes based
on context. The space of research on this topic
can be divided into three broad categories: models
for computing contextual lexical semantics based on
composition (Mitchell and Lapata, 2008; Erk and
Padó, 2008; Thater et al., 2011), models that use
fuzzy exemplar-based contexts without composing
them (Erk and Padó, 2010; Reddy et al., 2011), and

models that propose latent variable techniques (Dinu
and Lapata, 2010; Séaghdha and Korhonen, 2011;
Van de Cruys et al., 2011). Our work, which tackles
the stronger form of lexical ambiguity in polysemy
falls into the latter two of three categories.

5 Conclusion and Future Work

We have presented two general and flexible
approaches to producing sense-specific VSMs
grounded in an ontology. The first technique is ap-
plicable to any VSM as an efficient post-processing
step while the second provides a framework to in-
tegrate ontological information with existing MLE-
based predictive models. We presented an evalua-
tion of 3 semantic tasks on 7 datasets. Our results
show that our proposed methods are effectively able
to capture the different senses in an ontology. In
most cases this results in significant improvements
over baselines. We have also discussed the trade-
offs between the two techniques from several differ-
ent perspectives. Finally, we have presented a qual-
itative analysis investigating the nature of the sense-
specific vectors, and shown that they capture the se-
mantics of different senses.

Our findings suggest several avenues for future
research. We propose to use sense-specific vectors
as features in downstream applications such a Word
Sense Disambiguation. Our current approach as-
sumes a fixed ontology, but we hope to explore a
more bi-directional relationship between ontology
and VSM in future work. In particularly we envisage
simultaneously incrementing ontologies with struc-
ture learning in addition to improving VSMs. We
also hope to extend our research to the multi-lingual
domain. We are particularly excited by the idea of
using multi-lingual WordNets to learn sense specific
semantic vectors that generalize across languages.

Acknowledgments

The authors would like to thank Manaal Faruqui,
Jesse Dodge and Noah Smith for their insight and
feedback. Thanks also go to the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following grants:
NSF grant IIS-1143703, NSF award IIS-1147810,
DARPA grant FA87501220342.

691



References
Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.

Multimodal distributional semantics. Journal of Arti-
ficial Intelligence Research (JAIR), 49:1–47.

Bob Carpenter. 2008. Lazy sparse stochastic gradient de-
scent for regularized multinomial logistic regression.
technical report, alias-i. available at http://lingpipe-
blog.com/ lingpipe-white-papers.

Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference on Machine learn-
ing, ICML ’08, pages 160–167, New York, NY, USA.
ACM.

Adrian Corduneanu and Tommi Jaakkola. 2002. On in-
formation regularization. In Proceedings of the Nine-
teenth conference on Uncertainty in Artificial Intelli-
gence, pages 151–158. Morgan Kaufmann Publishers
Inc.

Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.

Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised frame-semantic parsing for unknown pred-
icates. In Proc. of ACL.

S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society for
Information Science.

Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162–1172. Association
for Computational Linguistics.

Katrin Erk and Sebastian Padó. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’08,
pages 897–906, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Katrin Erk and Sebastian Padó. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL 2010 Conference Short Papers, pages 92–
97. Association for Computational Linguistics.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2014.
Retrofitting word vectors to semantic lexicons. arXiv
preprint arXiv:1411.4166.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The con-
cept revisited. In ACM Transactions on Information
Systems, volume 20, pages 116–131, January.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of NAACL-HLT, pages 758–
764, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embeddings
by exploiting bilingual resources. In Proceedings of
COLING, pages 497–507.

Eric H Huang, Richard Socher, Christopher D Manning,
and Andrew Y Ng. 2012. Improving word representa-
tions via global context and multiple word prototypes.
In Proceedings of the 50th ACL: Long Papers-Volume
1, pages 873–882.

Mario Jarmasz and Stan Szpakowicz. 2004. Roget’s
thesaurus and semantic similarity. Recent Advances
in Natural Language Processing III: Selected Papers
from RANLP, 2003:111.

Mark Johnson. 2007. Why doesn’t em find good hmm
pos-taggers? In EMNLP-CoNLL, pages 296–305.
Citeseer.

Adam Kilgarriff. 1997. I don’t believe in word senses.
Computers and the Humanities, 31(2):91–113.

Thomas K Landauer and Susan T. Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211–240.

Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schütze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013 Con-
ference of the NAACL: Human Language Technolo-
gies, pages 746–751, Atlanta, Georgia, June.

George Miller and Walter Charles. 1991. Contextual cor-
relates of semantic similarity. In Language and Cog-
nitive Processes, pages 1–28.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–41.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244.

Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of the 29th International Con-
ference on Machine Learning, pages 1751–1758.

692



Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP.

Ted Pedersen and Varada Kolhatkar. 2009. Wordnet::
Senserelate:: Allwords: a broad coverage word sense
tagger that maximizes semantic relatedness. In Pro-
ceedings of human language technologies: The 2009
annual conference of the north american chapter of the
association for computational linguistics, companion
volume: Demonstration session, pages 17–20. Associ-
ation for Computational Linguistics.

Mohammad Taher Pilehvar, David Jurgens, and Roberto
Navigli. 2013. Align, disambiguate and walk: A uni-
fied approach for measuring semantic similarity. In
ACL (1), pages 1341–1351.

Siva Reddy, Ioannis P Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In IJCNLP,
pages 705–713.

Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-2010), pages 109–117.

Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.

Diarmuid Ó Séaghdha and Anna Korhonen. 2011. Prob-
abilistic models of similarity in syntactic context. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ’11, pages
1047–1057, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Amarnag Subramanya and Jeff A Bilmes. 2009. En-
tropic graph regularization in non-parametric semi-
supervised classification. In NIPS, pages 1803–1811.

Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fernan-
des, and Gregory Marton. 2003. Quantitative evalu-
ation of passage retrieval algorithms for question an-
swering. In SIGIR, pages 41–47.

Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effec-
tive vector model. In IJCNLP, pages 1134–1143.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilistic
model for learning multi-prototype word embeddings.
In Proceedings of COLING, pages 151–160.

Peter D. Turney. 2001. Mining the web for syn-
onyms: Pmi-ir versus lsa on toefl. In Proceedings
of the 12th European Conference on Machine Learn-
ing, EMCL ’01, pages 491–502, London, UK, UK.
Springer-Verlag.

Peter D. Turney. 2006. Similarity of semantic relations.
Comput. Linguist., 32(3):379–416, September.

Peter D Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and para-
phrase. Transactions of the Association for Computa-
tional Linguistics, 1:353–366.

Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 1012–1022. Association for Computational Lin-
guistics.

Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings of
the 52nd Annual Meeting of the Association for Com-
putational Linguistics.

Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System
Demonstrations, pages 78–83. Association for Com-
putational Linguistics.

Xiaojin Zhu, Zoubin Ghahramani, John Lafferty, et al.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In ICML, volume 3, pages
912–919.

693


