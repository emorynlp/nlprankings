










































EAGER: Extending Automatically Gazetteers for Entity Recognition


Proceedings of the 3rd Workshop on the People’s Web Meets NLP, ACL 2012, pages 29–33,
Jeju, Republic of Korea, 8-14 July 2012. c©2012 Association for Computational Linguistics

EAGER: Extending Automatically Gazetteers for Entity Recognition

Omer Gunes, Christian Schallhart, Tim Furche
Department of Computer Science,

Oxford University, Oxford OX1 3QD
firstname.lastname@cs.ox.ac.uk

Jens Lehmann, Axel Ngonga
Institute of Computer Science,

University of Leipzig, 04103 Leipzig
lastname@informatik.uni-leipzig.de

Abstract

Key to named entity recognition, the manual
gazetteering of entity lists is a costly, error-
prone process that often yields results that
are incomplete and suffer from sampling bias.
Exploiting current sources of structured in-
formation, we propose a novel method for
extending minimal seed lists into complete
gazetteers. Like previous approaches, we
value WIKIPEDIA as a huge, well-curated, and
relatively unbiased source of entities. How-
ever, in contrast to previous work, we exploit
not only its content, but also its structure, as
exposed in DBPEDIA. We extend gazetteers
through Wikipedia categories, carefully limit-
ing the impact of noisy categorizations. The
resulting gazetteers easily outperform previ-
ous approaches on named entity recognition.

1 Introduction

Automatically learning gazetteers with minimal su-
pervision is a long standing problem in named entity
recognition.

We propose EAGER as a novel approach to ex-
tending automatically gazetteers for entity recogni-
tion, utilizing DBPEDIA (Bizer et al., 2009) rather
than WIKIPEDIA. DBPEDIA serves as a much bet-
ter foundation than WIKIPEDIA, because all the in-
formation used in previous approaches (and much
more) is already provided as a structured database
of facts and articles. The extraction is more robust
and complete than ad-hoc methods and maintained
by a large community. E.g., navigating the category
hierarchy is much easier and reliable with DBPE-
DIA.

To summarize, EAGER’s main contributions are

(1) A novel gazetteer expansion algorithm that adds
new entities from DBPEDIA. EAGER adds enti-
ties that have several categories in common with
the seed terms, addressing noisy categorizations
through a sophisticated category pruning tech-
nique.

(2) EAGER also extracts categories from DBPE-
DIA abstracts using dependency analysis. Fi-
nally, EAGER extracts plural forms and syn-
onyms from redirect information.

(3) For entity recognition, we integrate the gazetteer
with a simple, but effective machine learning
classifier, and experimentally show that the ex-
tended gazetteers improve the F1 score between
7% and 12% over our baseline approach and
outperform (Zhang and Iria, 2009) on all learned
concepts (subject, location, temporal).

2 Related Work

We divide the related work in automatic gazetteer
population into three groups: (1) Machine learning
approaches (2) Pattern driven approaches Finally,
like our own work, (3) knowledge driven approaches

Knowledge Driven. In any case, machine learn-
ing and pattern driven approaches extract their terms
from unstructured sources – despite the fact that
large, general knowledge bases became available
in the last years. One of the first knowledge-
driven methods (Magnini et al., 2002) employed
WORDNET to identify trigger words and candidate

29



gazetteer terms with its word-class and -instance re-
lations. As WORDNET covers domain specific vo-
cabularies only to a limited extent, this approach is
also limited in its general applicability.

In (Toral and Muñoz, 2006), gazetteers are built
from the noun phrases in the first sentences of
WIKIPEDIA articles by mapping these phrases to
WORDNET and adding further terms found along
the hypernymy relations. The approach presented
in (Kazama and Torisawa, 2007; Kazama and Tori-
sawa, 2008) relies solely on WIKIPEDIA, produc-
ing gazetteers without explicitly named concepts, ar-
guing that consistent but anonymous labels are still
useful.

Most closely related to our own work, the authors
of (Zhang and Iria, 2009) build an approach solely
on WIKIPEDIA which does not only exploit the arti-
cle text but also analyzes the structural elements of
WIKIPEDIA:

3 Automatically Extending Gazetteer Lists

3.1 Extraction Algorithm: Overview

Algorithm 1 shows an outline of the gazetteer ex-
pansion algorithm used in EAGER. To extend an ini-
tial seed set S EAGER proceeds, roughly, in three
steps: First, it identifies DBPEDIA articles for seed
entities and extracts implicit category and synonym
information from abstracts and redirect information
(Lines 1–11). Second, it finds additional categories
from the DBPEDIA category hierarchy (Lines 12–
20). Finally, it uses the categories from the first two
steps to extract additional entities (Lines 21–24). In
the following, we consider the three steps separately.

3.2 Implicit: Abstract and Redirects

Before EAGER can analyse abstract and redirect in-
formation for an article, we need to find the corre-
sponding DBPEDIA articles (Lines 1–3) for each
seed entry in S . There may be one or more
such entry. Here, we observe the first advantage
of DBPEDIA’s more structured information: DB-
PEDIA already contains plain text labels such as
“Barack Obama” and we can directly query (using
the SPARQL endpoint) all articles with a label equal
(or starting with) an entity in our seed set. This al-
lows for more precise article matching and avoids
complex URL encodings as necessary in previous,

Algorithm 1: GazetteerExtension(S )
1 foreach seed entity e ∈S do
2 find article a for e in DBPEDIA;
3 Articles(e)← a;
4 G ← /0; P ← /0;
5 foreach entity e, article a = Articles(e) do
6 foreach sentence s ∈ a.Abstract do
7 Ds← dependencies in s;
8 add all t : nsubj(e, t) ∈ Ds to P;
9 add all t : nsubj(e, t ′),conj(t ′, t) ∈ Ds to P;

10 foreach article a′ ∈ a.Redirects do
11 add all labels of a to G ;

12 Cats(e)← Cats(e)∪a.Cats;
13 foreach entity e, category c ∈ Cats(e) do
14 Cats(e)← Cats(e)∪CategoryNeighbors(c,k);
15 foreach category c ∈ Cats(e) for some e do
16 Support(c)← |{e′ : c ∈ Cats(e′)}|;
17 foreach connected component C in Cats do
18 Support(C )← ∑c∈C Support(c);
19 MaxCatComp← C with maximal Support;
20 add all categories in MaxCatComp to P;
21 foreach category c ∈P do
22 foreach article a with c ∈ a.Cats do
23 if |a.Cats\P| ≤ θ then
24 add all labels of a to G ;

WIKIPEDIA-based approaches such as (Kazama and
Torisawa, 2007). As (Zhang and Iria, 2009), we re-
ject redirection entries in this step as ambiguous.

With the articles identified, we can proceed to
extract category information from the abstracts and
new entities from the redirect information. In the
dependency analysis of article abstracts (Lines 6–
9), we aim to extract category (or, more generally,
hypernym) information from the abstracts of arti-
cles on the ssed list. We perform a standard de-
pendency analysis on the sentences of the abstract
and return all nouns that stand in nsubj relation to
a seed entity or (directly or indirectly) in conj (cor-
relative conjunction) relation to a noun that stands
in nsubj relation to a seed entity. This allows us
to extract, e.g., both “general” and “statesman” as
categories from a sentence such as “Julius Caesar
was a Roman general and statesman”. This analy-
sis is inspired by (Zhang and Iria, 2009), but per-
formed on the entire abstract which is clearly dis-

30



1

2

3

4

5

6

Seed List

Articles

subject

...

...

Category A

Category B
subject

Category A’broader

Category A’’
broad

er

broader Category B’

subject

...

subject

subject

subject

New Articles

6

5

3

2

1

subjec
t

sub
jec

tUnrelated Cat.

Unrelated Cat.

1

2

3

4

5

6

Gazetteer

7

8

9

10

7

8

9

...

Figure 1: EAGER Gazetteer Extension Algorithm

tinguished in WIKIPEDIA. This contrasts to (Zhang
and Iria, 2009), where this is applied only to the first
sentence (as WIKIPEDIA does not directly provide a
concept of “abstract”). All categories thus obtained
are added to P and will be used in Section 3.4 to
generate additional entities.

Finally, we are interested in redirection infor-
mation (Lines 10–11) about an article for a seed
entity as that provides such with synonyms, plural
forms, different spellings, etc. Fortunately, DB-
PEDIA provides this information directly by means
of the dbpedia-owl:wikiPageRedirects property.
The labels of all redirect articles with this property
pointing to a seed entity articles are directly added
to the Gazetteer.

3.3 Explicit: Category Graph

In addition to categories from the abstract analy-
sis, we also use the category graph of DBPEDIA.
It has been previously observed, (Zhang and Iria,
2009) and (Strube and Ponzetto, 2006), that the cat-
egory graph of poor quality. DBPEDIA improves
little on that fact. However, EAGER uses a sophis-
ticated analysis of categories related to seed entities
that allows us to prune most of the noise in the cat-
egory graph. Biased towards precision over recall,
Section 4 shows that combined with the category
extraction from abstracts it provides a significantly
extended Gazetteer without introducing substantial
noise.

The fundamental contribution of EAGER is a cat-
egory pruning based on finding a connected com-
ponent in the graph of related categories that is sup-
ported by as many different entities from the seed list
as possible. Figure 1 illustrates this further: From

the articles for the seed entities, we compute (Line
12) the direct categories (via subject edges) and as-
sociate them to their seed entities e via Cats(e). We
extent this set (Lines 13–14) with all categories in
the k-neighbourhood (here, we use k = 3), i.e., con-
nected via up to k broader edges traversed in any
direction, again maintaining via Cats(e) which cat-
egories are reached from which seed entity e. In
the resulting graph of all such categories, we iden-
tify the connected component with maximum sup-
port (Lines 15–19). The support of a component is
the sum of the support of its categories. The support
of a category c is the number of seed entities with
c ∈ Cats(e). For Figure 1, this yields the category
graph of the blue and black categories of the figure.
The blue categories form the connected component
with maximum support and are thus retained (in P),
the black categories are dropped.

3.4 Entities from Categories

Finally, in Lines 21–24, EAGER completes the
gazetteer extension by extracting the labels of all
articles of categories in P if they are sufficiently
unambiguous. An article is called sufficiently un-
ambiguous, if it is categorised only with categories
from P up to a threshold θ (here, set to 5) of non-
P categories. This avoids adding very general en-
tities that tend to have large number of categories in
WIKIPEDIA and thus DBPEDIA. The output of Al-
gorithm 3 is the extended gazetteers G .

4 Evaluation

To evaluate the impact of EAGER on entity recogni-
tion, we performed a large set of experiments (on the
archeology domain). The experiment domains and

31



Subject Location Temporal
P R F1 P R F1 P R F1

Baseline (B) 69.9 54.1 62.0 76.5 46.1 61.3 86.4 75.4 80.9
B+ Dependency 73.6 64.7 69.0 73.4 69.4 71.4 86.2 85.2 85.7
B+ Category 72.3 64.5 68.5 71.9 70.1 71.0 86.4 85.0 85.8
B+ Redirection 71.6 65.8 68.7 71.2 71.7 71.4 86.32 85.46 85.89

(Zhang and Iria, 2009) full 69.8 66.5 68.1 68.9 75.0 71.8 82.4 83.4 82.9
EAGER full 72.1 66.5 69.3 72.0 74.6 73.3 86.8 86.1 86.5

Table 1: EAGER comparison

corpora are described in Section 4.1. Finally, Sec-
tion 4.2 presents the results of the evaluation, show-
ing the contributions of the different parts of EAGER
and comparing it with (Zhang and Iria, 2009), which
we outperform for all entity types, in some cases up
to 5% in F1 score.

4.1 Evaluation Setup

In this experiment, we consider entity recognition in
the domain of archaeology.

As part of this effort, (Jeffrey et al., 2009) iden-
tified three types of entities that are most useful
for archaeological research; Subject(SUB), Tempo-
ral Terms(TEM), Location (LOC).

In this evaluation, we use the same setup as in
(Zhang and Iria, 2009): A corpus of 30 full length
UK archaeological reports archived by the Arts and
Humanities Data Service (AHDS). The length of the
documents varies from 4 to 120 pages. The corpus
is inter-annotated by three archaeologists.

4.2 Result

For the evaluation, we perform a 5-fold validation
on the above corpus. The evaluate the performance
(in terms of precision, recall and F1 score) for en-
tity recognition of the baseline system as well as
the baseline system extended with a gazetteer fea-
ture. For the latter, we consider full EAGER as de-
scribed in Section 3 as well as only the entities de-
rived from dependency analysis of abstracts, from
the category graph, and from redirection informa-
tion. Finally, we also include the performance num-
bers report in (Zhang and Iria, 2009) for comparison
(since we share their evaluation settings).

Table 1 show the results of the comparison: EA-
GER significantly improves precision and recall over
the baseline system and outperforms(Zhang and Iria,
2009) in all cases. Furthermore, the impact of all
three types of information (dependencies from ab-
stract, category, redirection) of EAGER individually
is quite notable with a slight disadvantage for cate-
gory information. However, in all cases the combi-
nation of all three types as proposed in EAGER shows
a significant further increase in performance.

5 Conclusion

At its heart, EAGER is a novel algorithm for ex-
tending sets of entities of a specific type with ad-
ditional entities of that type extracted from DBPE-
DIA. It is based on a new strategy for pruning the
category graph in DBPEDIA (and thus WIKIPEDIA),
necessary to address the inherent noise. Our evalua-
tion shows that EAGER can significantly improve the
performance of entity recognition and outperforms
existing systems in all cases. Unlike previous ap-
proaches, our approach makes use of richer content
and structural elements of DBpedia.

We believe that EAGER is a strong indicator
that DBPEDIA provides a much richer, yet easier
to use foundation for NLP tasks in general than
WIKIPEDIA.

The extensibility and domain adaptability of our
methods still need further investigation. We are cur-
rently extending the evaluation to several other do-
mains, including property descriptions in real estate
and classified adds. We are also investigating more
targeted means of detecting and addressing noise in
the category graph.

32



References
Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sören

Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009. DBpedia – A crystallization
point for the Web of Data. Web Semantics: Science,
Services and Agents on the World Wide Web, 7(3).

Stuart Jeffrey, Julian Richards, Fabio Ciravegna, Stewart
Waller, Sam Chapman, and Ziqi Zhang. 2009. The
Archaeotools project: Faceted Classification and Nat-
ural Language Processing in an Archaeological Con-
text. Phil. Trans. R. Soc. A, 367(3):2507–2519.

Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as External Knowledge for Named En-
tity Recognition. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL, pages 698–707.

Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing
Gazetteers for Named Entity Recognition by Large-
Scale Clustering of Dependency Relations. In Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 407–415.

Bernardo Magnini, Matteo Negri, Roberto Prevete, and
Hristo Tanev. 2002. A WordNet-based approach to
Named Entities recognition. In Proceedings of the
2002 workshop on Building and using semantic net-
works - Volume 11, SEMANET ’02, pages 1–7.

M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the 21st National Conference on Ar-
tificial Intelligence, pages 1419–1424.

Antonio Toral and Rafael Muñoz. 2006. A Proposal
to Automatically Build and Maintain Gazetteers for
Named Entity Recognition by using Wikipedia. In
Proceedings of the Workshop on New Text – wikis
and blogs and other Dynamic text sources, ECAL’06,
pages 56–61.

Ziqi Zhang and José Iria. 2009. A novel approach to
automatic gazetteer generation using Wikipedia. In
Proceedings of the 2009 Workshop on The People’s
Web Meets NLP: Collaboratively Constructed Seman-
tic Resources, People’s Web ’09, pages 1–9.

33


