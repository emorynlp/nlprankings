



















































Integrating Subject, Type, and Property Identification for Simple Question Answering over Knowledge Base


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 976â€“985,
Taipei, Taiwan, November 27 â€“ December 1, 2017 cÂ©2017 AFNLP

 
 
 

  

Integrating Subject, Type, and Property Identification for Simple 

Question Answering over Knowledge Base 
 

 

Wei-Chuan Hsiao, Hen-Hsen Huang and Hsin-Hsi Chen 

Department of Computer Science and Information Engineering 

National Taiwan University, Taipei, Taiwan 

weichuanhsiao@gmail.com; hhhuang@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw 

Abstract 

This paper presents an approach to 

identify subject, type and property 

from knowledge base for answering 

simple questions. We propose new fea-

tures to rank entity candidates in KB. 

Besides, we split a relation in KB into 

type and property. Each of them is 

modeled by a bi-directional LSTM. 

Experimental results show that our 

model achieves the state-of-the-art per-

formance on the SimpleQuestions da-

taset. The hard questions in the experi-

ments are also analyzed in detail.  

1 Introduction 

With the popularity of the Internet, more and more 

new information is generated every day. The in-

formation may be stored in unstructured data, such 

as Wikipedia, which is presented as an article or 

web page, or in the form of structured data. 

Knowledge base (KB), such as Freebase (Bol-

lacker et al., 2008) and DBpedia (Lehmann et al., 

2015), is very popular. The information in the KB 

is a description of the relationship between two en-

tities and is stored in the form of (subject, relation, 

object) triple. In KB, each entity is represented by 

a unique id, for example, J.K. Rowling's mid (Ma-

chine ID) in Freebase is â€œm.042xhâ€. 

With these large-scale open-domain KBs, it is 

important to access the knowledge efficiently and 

effectively to meet what users need. The most di-

rect and close to people's life is question answering 

(QA) system in natural language. People can ask 

any questions in their familiar languages, and then 

use the QA system to get answers from the web re-

sources. Current QA research is often based on KB 

to find appropriate triples for answering question, 

which is called QA over KB. 

QA systems can be classified by the form of the 

question. There are two categories of questions in 

QA system, i.e., simple questions and complex 

questions. Simple questions can be answered by 

exactly one triple in the KB. For example, the 

question â€œWho is the author of Harry Potter?â€ can 

be answered by triple â€œ(Harry Potter, author, J.K. 

Rowling)â€. Although this category is â€œsimpleâ€ 

question, retrieving a triple from the KB is not a 

trivial task due to the billions of facts in the KB. 

Complex questions contain more restrictions. 

These questions may involve two or more triples 

in the KB, or have other semantic constraints to re-

strict the answers to a smaller set. For example, 

â€œthe firstâ€ in the question â€œWhat is the name of the 

first Harry Potter novel?â€ restricts that there is only 

one answer. Previous researches showed that sim-

ple questions are the more common category in 

community QA websites (Fader et al., 2013). This 

paper focuses on factoid simple question-answer-

ing over Freebase. 

Simple question can be answered with the object 

of one KB triple. Thus, the systems only need to 

find the subject and relation of the triple which can 

describe the question properly. The issues of sim-

ple QA are the identification of the subject entity in 

a question, and the resolution of the gap between 

the natural language expression in the question and 

the relation description in the KB. After a QA sys-

tem receives usersâ€™ questions, it needs to transform 

a question into a KB query, e.g. SPARQL. The 

question can then be transformed into the KB query, 

which can access KB to get the answer.  

976



 
 
 

  

Previous simple QA model often adopts a two-

step paradigm. Entity-linking step identifies the 

subject entities in questions, and forms the candi-

date entity set and relation set. The candidate rela-

tion set is formed by all the relations which have 

connections with any entity in candidate entity set. 

Relation-finding step further identifies a proper re-

lation from the candidate relation set. Dai et al. 

(2016) propose a neural-network based two-step 

approach to simple QA over Freebase, and formu-

late the task into a probabilistic form. Given a 

question q, the first step is to find the candidate re-

lation r with high probability ğ‘ƒ(ğ‘Ÿ|ğ‘). The second 
step is to find the subject s with high ğ‘ƒ(ğ‘ |ğ‘Ÿ, ğ‘). As 
a result, the object in the KB triple which contains 

the subject s and the relation r with the highest 

ğ‘ƒ(ğ‘ , ğ‘Ÿ|ğ‘) is the answer. 
The relation in KB triple has hierarchical struc-

ture: domain-type-property. For example, in â€œpeo-

ple.person.place_of_birthâ€, â€œplace_of_birthâ€ is the 

property used to present the birth place of a person. 

The type of this relation is â€œpersonâ€, and the do-

main is â€œpeopleâ€. In Freebase, many relations have 

the same property, but are in different types. For 

example, both â€œfilm.film.genreâ€ and â€œmusic.art-

ist.genreâ€ have the property â€œgenreâ€, but they are 

under different domains and types. These two rela-

tions are regarded as the same if we only consider 

the property. The major issue is: they are different 

relations although they have the same meaning. On 

the other hand, if the whole relation is considered 

as a class, the distribution of relations is getting 

sparser. The past two-step approaches cannot dis-

tinguish the subtlety in the structured relation. 

In this paper, we will propose a novel three-step 

approach, including subject, type and property 

identification steps, to deal with the hierarchical 

structure of the relation. Our approach introduces 

new features in subject identification to distinguish 

similar entities from different aspects. Moreover, 

splitting relation into type and property predicts re-

lation more precisely. Experimental results show 

our model outperforms the existing models and 

achieves a state-of-the-art accuracy of 76.7%. 

This paper is organized as follows. Section 2 in-

troduces related works of QA system. Section 3 

presents our three-step paradigm. Section 4 shows 

the experiments on the SimpleQuestions dataset 

(Bordes et al., 2015) and compares ours with pre-

vious works. We also discuss the importance of 

each component of our system. Section 5 analyzes 

the errors in the experiments. Section 6 concludes 

the remarks. 

2 Related Works 

Previous simple QA models can be divided into 

two categories. The first one is based on semantic 

parsing, which maps a question to its logical form. 

Then, the logical form can be transformed to 

SPARQL for KB retrieval.  

Berant et al. (2013) present a semantic parser 

that does not need to be trained through the anno-

tated logical form. They construct a lexicon that 

maps natural language phrases to KB relations by 

aligning large text corpus with Freebase. Candi-

date logical forms can be obtained by this lexicon 

and the other bridging operations. 

Berant and Liang (2014) propose a semantic 

parser via paraphrasing. They use the intermediate 

question to deal with the problem of the mismatch 

between input question and its logical forms. 

Yih et al. (2015) treat a question as a query 

graph, which can be directly mapped to its logical 

form. Semantic parsing is then equivalent to find-

ing a sub-graph of the KB which can represent the 

question. 

The semantic parsing approach which often re-

quires the human annotated logical form may in-

crease the cost of obtaining training data. Although 

the use of rule-based method to generate logical 

forms can reduce the use of annotated data, it limits 

the application domain. The second approach is in-

formation extraction, which needs only question-

answer pairs for training. This method retrieves 

some candidate answers from KB for each ques-

tion, and ranks the candidates through several fea-

tures. Deep neural network models are often em-

ployed for deriving the vector representations of 

questions and the KB elements. 

The Memory Network based QA system is pro-

posed by Bordes et al. (2015). By embedding all of 

the KB elements and questions in the same vector 

space, the system can deal with the relationship be-

tween input language and the KB language. 

Glob and He (2016) propose a character-level, 

attention-based encoder-decoder QA model. Their 

977



 
 
 

  

model embeds the questions and KB elements by a 

long short-term memory (LSTM) (Hochreiter and 

Schmidhuber, 1997) encoder and a CNN-based en-

coder, respectively. And a LSTM decoder with at-

tention mechanism is used to predict the appropri-

ate answer entity and relation. 

Dai et al. (2016) formulate the QA problem into 

a probabilistic form. Given a question, the answer 

is the triple in KB containing the subject and rela-

tion with the highest conditional probability. They 

first use a focused pruning method to tag the span 

in question which is most probable to be the sub-

ject entity and gets the candidate answer triples. 

Then they use a relation network and a subject net-

work, both are a two-layer bidirectional-GRU 

model, to get the similarity scores between candi-

date triples and question. We modify their proba-

bilistic form with the splitting of the relation part ğ‘Ÿ 
into type ğ‘¡ and property ğ‘. 

Dong et al. (2015) introduce a multi-column 

CNNs (MCCNNs) to analyze the question in three 

different aspects: answer path, answer context and 

answer type. The system represents the question in 

three low-dimensional vectors, each of them then 

matches to one of the answer aspect to derive the 

scores of candidates. 

Yin et al. (2016) also use the CNN-based ap-

proach to implement the QA system. They use a 

word-level CNN with attentive max-pooling to 

model the relationship between KB relations and 

question pattern. They also add an active linker, 

which is similar to the focused pruning method in 

Dai et al. (2016), to reduce the number of candi-

dates and improve the performance significantly. 

3 Methodology 

Our method includes subject identification, type 

identification, and property identification steps, as 

shown in Figure 1. We first give an overall picture 

and then describe each of them in deep in the fol-

lowing sections. 

3.1  A Three-Step Paradigm 

The relation in a Freebase triple has a hierarchical 

structure in three levels: domain, type, and property. 

Many relations have the same property but they are 

in different types and domains. For example, both 

 

 

Figure 1: System Overview. 

the relations â€œwine.wine.colorâ€ and 

â€œroses.roses.colorâ€ are used to describe the color of 

things, but they are in type â€œwineâ€ and â€œrosesâ€, re-

spectively. We separate a relation into type and 

property parts to understand the question meaning 

more precisely. 

Given a KB ğ’¦ and a question ğ‘, the three-step 
paradigm aims at finding a KB triple containing s, 

t, and p with the highest probability ğ‘ƒ(ğ‘ , ğ‘¡, ğ‘|ğ‘) , 
where s is a subject, and t and p are type and prop-

erty of a structured relation, respectively. The pro-

cess is formulated by Equations (1) and (2). The 

answer of the question is the object o of the triple 

(s, r, o) in ğ’¦. 

ğ‘ƒ(ğ‘ , ğ‘¡, ğ‘|ğ‘) =  ğ‘ƒ(ğ‘ |ğ‘) âˆ™ ğ‘ƒ(ğ‘¡|ğ‘ , ğ‘)

âˆ™ ğ‘ƒ(ğ‘|ğ‘ , ğ‘¡, ğ‘) 
(1) 

ğ‘ âˆ—, ğ‘¡âˆ—, ğ‘âˆ— =  argmax
ğ‘ ,ğ‘¡,ğ‘âˆˆğ’¦

ğ‘ƒ(ğ‘ , ğ‘¡, ğ‘|ğ‘) (2) 

3.2 Entity Identification 

The first step is to find potential entities in the 

question, and link them to KB. We use the Freebase 

subset FB5M in the SimpleQuestions dataset as the 

KB. We retrieve all candidate subject entities in 

question from the KB and calculate their entity 

linking scores on account of eight features, as in 

steps A1-A3 of Figure 1. 

To find candidate entities for a question, we ex-

tract mentions in the question by maximum string 

matching. A mention that matches an entity name 

or an entity alias is extracted. When multiple 

matches are found in an overlapped span, the long-

est one is taken. And the mention is filtered if it is 

a stop word or a number that contains less than four 

978



 
 
 

  

digits. All the entities in ğ’¦  that have the same 
names or aliases as the mentions form the candi-

date entity set.  

After obtaining all the candidate entities, we as-

sign each of the candidates a linking score. The 

score is measured by a learning-to-rank model with 

eight features in different aspects. The eight fea-

tures are described as follows. 

Word Proportion (Yin et al., 2016):  We com-

pute the proportion as the length of candidate entity 

divided by the length of the question. The length is 

measured by the number of words. The longer the 

matched entity name is, the more likely it is a sub-

ject. 

Char Proportion: Similar to word proportion, 

the length is computed by the number of characters 

instead. 

Relative Position in Question (Yin et al., 2016): 

The relative position of a candidate entity is the po-

sition of its last token divided by the length of the 

question (in words). That models an observation: 

most entity is far from the beginning of the ques-

tion. 

 Relative Position to Be Verb/Auxiliary Verb: 

Subject entity tends to be close to and behind Be 

verb/auxiliary verb in a question. We take the sub-

traction of the Be verb/auxiliary verb position and 

the first token of candidate entity position as ğ›¼. If 
the candidate entity position is before the Be 

verb/auxiliary verb, this feature is ğ›¼ , which is a 
negative number. Otherwise, this feature is 1/ğ›¼ , 
which is a positive number. If no such verbs exist, 

this feature is set to 0 

Out-degree: The number of out-going links of 

the entity in the KB is taken as Out-degree feature. 

The more the number of links the entity has, the 

higher the feature value is and the entity in the KB 

is more informative. The direction of links from 

subject to object represents the impact of the entity. 

IDF: We take each question in SimpleQuesions 

training set as a document, and compute the in-

verse document frequency of the entity. The higher 

the value is, the more specific the entity is. 

 NER_LCS: We use the LSTM-CRF named en-

tity recognition (NER) tagger1 (Lample et al., 2016) 

to find the span from the question that is most 

 
                                                      
1 https://github.com/glample/tagger . 

 

Figure 2: Structure of Type_LSTM in entity 

identification. 

 

likely to be a subject entity, and compute the length  

of the longest common subsequence (in characters) 

between the candidate entity and the words in the 

tagged span. The tagger consists of an embedding 

layer, a bidirectional-LSTM layer, and a condi-

tional random field layer to predict the label for 

each word. The higher the NER_LCS value is, the 

more possible the candidate is a subject entity. 

Type_LSTM: Each entity has entity types in 

Freebase to describe its characteristics, e.g., entity 

â€œAlex Golfisâ€ belongs to types â€œpersonâ€, â€œactorâ€, 

and â€œdeceased_personâ€. There are total 500 types 

for entities in FB5M. For a question, we estimate 

the types of its subject. A bidirectional-LSTM with 

attention is trained to derive the probability distri-

bution of each question over the 500 types. The 

structure of the network is shown in Figure 2. The 

training input of the LSTM is the whole questions 

and the types of their ground truth entities, and the 

training objective is categorical cross entropy. The 

test input is a question, and the output is a proba-

bility distribution of the question over the 500 

types. We can get the type information of the can-

didate entity from Freebase, and sum over all di-

mensions corresponding to the types to get this fea-

ture. It means how probable the question is in some 

types. 

 With the above eight features, support vector 
machine for ranking (SVMrank) (Joachims, 2006) is 

trained to combine these features and derive a 

score ğ‘¢1(ğ‘ , ğ‘)  for each candidate entity s in the 
question q. Then each entity has the probability 

ğ‘ƒ(ğ‘ |ğ‘): 

979



 
 
 

  

ğ‘ƒ(ğ‘ |ğ‘) =  
exp(ğ‘¢1(ğ‘ , ğ‘))

âˆ‘ exp(ğ‘¢1(ğ‘ â€², ğ‘))ğ‘ â€²
 (3) 

where ğ‘ â€² is an entity in the candidate entity set. For 
the entities in the KB but not in the candidate entity 

set, their ğ‘ƒ(ğ‘ |ğ‘) = 0. 

3.3 Type Identification 

Given a question q, the type network determines 

the type of the relation most likely to be answered. 

We use ğ¸(ğ‘¡)  to represent the embedding of the 
type ğ‘¡ , and use ğ‘”1(ğ‘)  to represent the vector of 
question ğ‘. Our goal is to make the cosine similar-
ity between ğ‘”1(ğ‘) and the correct type embedding 
ğ¸(ğ‘¡âˆ—) higher than the cosine similarities between 
ğ‘”1(ğ‘) and the other types. 

  The network structure is shown in Figure 3. 

First, the question vector ğ‘”1(ğ‘) is generated by a 
bidirectional-LSTM model. We change the words 

in q to lowercase and remove the punctuation. 

Then we put the words into the embedding layer. 

After the bidirectional-LSTM layer, the two vec-

tors in left and right directions are concatenated to-

gether. The concatenated vector is put into a linear 

projection layer with sigmoid, and the question 

vector ğ‘”1(ğ‘) is generated. After that, we calculate 
the similarity between ğ‘”1(ğ‘)  and different type 
embeddings ğ¸(ğ‘¡). The embedding ğ¸(ğ‘¡) is trained 
with the bidirectional-LSTM model, and has the 

same dimension as ğ‘”1(ğ‘) . ğ¸(ğ‘¡)  and ğ‘”1(ğ‘)  are in 
the same space so that the cosine similarity 

ğ‘¢2(ğ‘¡, ğ‘) can be computed by Equation (4). 

ğ‘¢2(ğ‘¡, ğ‘) = cos(ğ‘”1(ğ‘), ğ¸(ğ‘¡)) (4) 

The probability of type ğ‘¡ given the question ğ‘ and 
the candidate entity set is defined as follows: 

ğ‘ƒ(ğ‘¡|ğ‘ , ğ‘) = {
 

exp(ğ‘¢2(ğ‘¡, ğ‘))

âˆ‘ exp(ğ‘¢2(ğ‘¡
â€², ğ‘))ğ‘¡â€²

, ğ‘–ğ‘“ ğ‘¡ â„ğ‘ğ‘  ğ‘™ğ‘–ğ‘›ğ‘˜ ğ‘¤ğ‘–ğ‘¡â„ ğ‘  

0              , ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’

 (5) 

where ğ‘¡â€² is a type in the candidate type set. To re-
duce the computation and the number of candidate 

types, only those types which have links with an 

entity in the candidate entity set is chosen into the 

candidate type set. ğ‘ƒ(ğ‘¡|ğ‘ , ğ‘)  is set to 0 for those 
types which are impossible to be the correct answer 

due to the lack of links with any candidate entity. 

 

Figure 3: Type identification network. 

 

For training the type network, the hinge loss 

with negative samples is the objective function to 

be minimized. 

â„“(ğœƒğ‘¡) = âˆ‘ max(0, ğ“‚ğ‘¡ âˆ’ ğ‘¢2(ğ‘¡
âˆ—, ğ‘) + ğ‘¢2(ğ‘¡ğ‘–, ğ‘))

ğ‘ğ‘¡
ğ‘–=1   (6) 

where ğœƒğ‘¡ is the parameters to learn, ğ‘ğ‘¡ is the num-
ber of negative samples, ğ“‚ğ‘¡  is the margin, ğ‘¡

âˆ—  is 

the correct type, and ğ‘¡ğ‘– is the type randomly sam-
pled from all types except ğ‘¡âˆ—.  

3.4 Property Identification 

Given a question q, property identification deter-

mines the property of the relation most likely to be 

answered. We use ğ¸(ğ‘)  to represent the embed-
ding of the property ğ‘, and ğ‘”2(ğ‘) to represent the 
vector of question ğ‘. Similar to the type network, 
our goal is to make the cosine similarity between 

ğ‘”2(ğ‘) and the correct property embedding ğ¸(ğ‘
âˆ—) 

higher than the cosine similarities between ğ‘”2(ğ‘) 
and the other properties. 

The structure of the property network is the 

same as that of the type network, which is shown 

in Figure 3, but all the weights and embedding ma-

trixes are not shared with the type network. After 

deriving ğ‘”2(ğ‘) and ğ¸(ğ‘), we compute the cosine 
similarity ğ‘¢3(ğ‘, ğ‘): 

ğ‘¢3(ğ‘, ğ‘) = cos(ğ‘”2(ğ‘), ğ¸(ğ‘)) (7) 

The probability of property ğ‘  given question ğ‘ , 
candidate entity s, and candidate type t is defined 

as follows: 

ğ‘ƒ(ğ‘|ğ‘ , ğ‘¡, ğ‘) = {
 

exp(ğ‘¢3(ğ‘, ğ‘))

âˆ‘ exp(ğ‘¢3(ğ‘â€², ğ‘))ğ‘â€²
, ğ‘–ğ‘“ ğ‘ â„ğ‘ğ‘  ğ‘™ğ‘–ğ‘›ğ‘˜ ğ‘¤ğ‘–ğ‘¡â„ ğ‘ , ğ‘¡ 

0              , ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’

 (8) 

where ğ‘â€² is a property in the candidate property set. 
To reduce the computation and the number of can-

didate properties, only the properties which have 

links with an entity in the candidate entity set and 

980



 
 
 

  

belongs to a type in the candidate type set are cho-

sen into the candidate property set. ğ‘ƒ(ğ‘|ğ‘ , ğ‘¡, ğ‘) is 
set to 0 for those properties which are impossible 

to be the correct answer due to the lack of links 

with any candidate entity and type. 

For training property network, the hinge loss 

with negative samples is the objective function to 

be minimized. 

â„“(ğœƒğ‘) = âˆ‘ max(0, ğ“‚ğ‘ âˆ’ ğ‘¢3(ğ‘
âˆ—, ğ‘) + ğ‘¢3(ğ‘ğ‘– , ğ‘))

ğ‘ğ‘

ğ‘–=1
  (9) 

where ğœƒğ‘ is the parameters to learn, ğ‘ğ‘ is the num-

ber of negative samples, ğ“‚ğ‘  is the margin, ğ‘
âˆ—  is 

the correct property, and ğ‘ğ‘–  is the property ran-
domly sampled from all properties except ğ‘âˆ—.  

4 Experiments 

The SimpleQuestions2 dataset (Bordes et al., 2015), 

which contains 75,910 training data, 10,845 vali-

dation data, and 21,687 test data, is adopted in the 

experiments. The evaluation is the same as in Bor-

des et al. (2015). The predicted answer is correct 

when the subject-relation pair is the same as the 

correct answer. We train our model on the training 

set. The validation set is used for early stop and pa-

rameter tuning. The test set is used for evaluation. 

4.1 Experimental Setup 

The number of negative samples used in SVMrank 

is set to 5. Other parameters for SVMrank are C = 

0.1, epsilon = 0.01, and loss function option = 2. 

The word embeddings used in each neural network 

is initialized with the pre-trained GloVe (Penning-

ton et al., 2014) with the dimension of 300. All the 

networks are optimized by mini-batch and Adam 

(Kingma et al., 2014) with the learning rate 0.001.  

The TYPE_LSTM in entity identification step 

has a drop rate of 0.2. The hidden size of LSTM is 

                                                      
2 https://research.fb.com/downloads/babi/ . 

500. Batch size is 128. The parameters of type net-

work and property network are the same. Maximal 

question length is 25. Mini-batch size is 100. The 

type and property embeddings are 500-dimen-

sional with randomly initialized. The hidden size 

of LSTM is 500. Both hinge loss margins ğ“‚ğ‘¡ and 
ğ“‚ğ‘ are 0.4. The numbers of negative samples ğ‘ğ‘¡ 

and ğ‘ğ‘ are 65. 

4.2 Overall Results 

Table 1 shows the performances of our model com-

pared with the other four methods. Bordes et al. 

(2015) use a memory network. Dai et al. (2016) 

employ a conditional focused neural-network 

based approach. Yin et al. (2016) apply attentive 

convolutional neural network with the passive or 

active linker. Golub and He (2016) use the charac-

ter-level encoder-decoder framework. In our ap-

proach, â€œprobabilityâ€ means the outcome is the tri-

ple with the highest probability computed by Equa-

tion (1). We find that the subject entity and the 

property are more important than the type. The ap-

proach â€œsumâ€ combines the scores ğ‘¢1(ğ‘ , ğ‘) , 
ğ‘¢2(ğ‘¡, ğ‘) , and ğ‘¢3(ğ‘, ğ‘)  by weighted summation, 
and selects the triple with the highest weighted sum. 

The weights are entity:type:property = 4:1:3, 

which are tuned on the validation data. 

Our â€œprobabilityâ€ approach outperforms all pre-

vious models on FB5M, including the previous 

best model by Yin et al. (2016). The â€œsumâ€ ap-

proach even significantly outperforms the â€œproba-

bilityâ€ approach on McNemarâ€™s test (p < 0.01). 

4.3 Entity Identification Results 

This section discusses the performance of the en-

tity identification step. The hit rates of the top N 

entities are shown, which means the coverage of 

N 
(Yin et al., 2016) 

Our approach 
Passive Active 

1 56.6 73.6 80.9 

5 71.1 85.0 90.2 

10 75.2 87.4 92.2 

20 81.0 88.8 93.7 

50 85.7 90.4 95.1 

100 87.9 91.6 96.0 
 

Table 2:  Hit rates of the ground-truth entity. 

Method Accuracy (%) 

(Bordes et al., 2015) 63.9 

(Dai et al., 2016) 75.7 

(Yin et al., 2016) 75.9 

(Golub and He, 2016) 70.3 

Our approach (probability) 76.1 

Our approach (sum) 76.7 
 

Table 1:  Results on the SimpleQuestions test 

data. 

981



 
 
 

  

the ground truth entity by top-N results. ğ‘ âˆˆ
 {1,10,20,50,100}. Entity linker proposed by Yin 
et al. (2016) has two versions, passive and active 

linker. The difference between them is that they use 

the longest consecutive common subsequence be-

tween question and KB entity names to get candi-

dates in the passive linker. On the other hand, the 

active linker first gets the span from the question 

that is most likely to be a subject entity by sequen-

tial labeling, and the linker uses the labeled span to 

search the candidate entity from KB. The number 

of the candidates in the active linker is less than the 

passive linker, because the active linker uses spe-

cific span in question to search candidates, and en-

tities not in this span are filtered out. 

Table 2 shows our performance in entity identi-

fication compares to Yin et al. (2016). Our entity 

linker outperforms their approach by over 7% in 

top-1 result using FB5M as background KB. We 

have six new features to score the entities in differ-

ent aspects. The features â€œChar Proportionâ€ and 

â€œRelative Position to Be Verb/Auxiliary Verbâ€ can 

analyze the entities by their surface form in ques-

tion, e.g., name and relative position. The features 

â€œOut-degreeâ€ and â€œType_LSTMâ€ can distinguish 

entities even if they have the same name. The â€œIDFâ€ 

feature can kick out common entities and keep the 

more important ones. And â€œNER_LCSâ€ feature 

has the similar effect to the active linker of Yin et 

al. (2016), but our feature can withstand the wrong 

subject entity prediction in sequential labeling, be-

cause we do not filter out any candidates, we give 

them the lower score instead. 

4.4 Importance of Entity Identification 
Features 

In this section, we discuss the importance of each 

feature in entity identification in three ways. We 

first show the performances with a single feature. 

And then, we consider the performances with one 

feature or a group of features being removed. 

Performance with Single Feature 

Table 3 shows the hit rates of the top N entities gen-

erated by a single feature. The feature â€œNER_LCSâ€ 

has the best performance because this feature con-

tains a part of information from the four surface  

 

 

                      N 

Fetures 

Hit rates @ N 

1 5 10 20 50 100 

Word Prop. 41.0 62.7 70.5 77.6 85.8 90.3 

Char Prop. 55.5 73.7 78.9 83.4 88.9 91.7 

Rel. Position in Q 16.7 36.5 47.7 60.9 77.0 86.4 

Rel. Position to 

BeV./AuxV. 
30.2 44.2 52.6 61.4 75.7 84.2 

Out-degree 15.0 39.6 52.5 67.2 82.6 89.5 

IDF 43.4 68.9 76.4 82.5 88.5 91.6 

NER_LCS 59.5 77.0 81.5 85.5 90.1 92.6 

Type_LSTM 44.1 68.8 77.0 85.3 91.6 94.4 

Table 3:  Hit rates generated by a single feature in 

entity linker. 

form features: â€œWord Proportionâ€, â€œChar Propor-

tionâ€, â€œRelative Position in Questionâ€, and â€œRela-

tive Position to Be Verb/Auxiliary Verbâ€. The NER 

tagger labels the span of possible position of the 

subject entity in a question, and thus â€œNER_LCSâ€ 

contains the position information. Moreover, the 

length of the longest common subsequence be-

tween the candidate entity and the words in the 

span gives the length proportion information. 

The feature â€œOut-degreeâ€ has the lowest perfor-

mance in N=1 hit rate, because the entities with 

many out-going links sometimes represent that 

they are general entities. For example, the entity 

â€œalbum (m.02lx2r)â€ in question â€œWhich genre of 

album is harderâ€¦â€¦faster?â€ has 323,467 out-go-

ing links in FB5M, but the subject entity 

â€œharderâ€¦â€¦faster (m.01jp8ww)â€ has only 5 links. 

Although this feature sometimes highlights the 

general entities, it can distinguish entities that have 

the same names with the help of other features. 

Table 3 also shows that the performance of â€œRel-

ative Position to Be Verb/Auxiliary Verbâ€ is almost 

twice the performance of â€œRelative Position in 

Questionâ€. It shows that the observation about po-

sition â€œsubject entity tends to be close to and be-

hind Be verb/auxiliary verb in a questionâ€ is more 

appropriate, and the Be verb/auxiliary verb plays 

an important role in identifying entities. 

Performance without One of the Features 

Table 4 shows the hit rates of the top N entities gen-

erated by removing one of the eight features from 

the entity linker. First, we can see that the perfor-

mances without one of the first four features are 

982



 
 
 

  

only reduced by about 1%, because the feature 

â€œNER_LCSâ€ contains a part of the information 

from them, and it can make up for the removal of 

them. And then, we can find that although the per-

formance of the feature â€œOut-degreeâ€ has the low-

est accuracy in Table 3, its removal affects the per-

formance by more than 2%. 

 The removal of â€œType_LSTMâ€ has the greatest 

impact. The result with this feature outperforms the 

result without it by 6%. The comparison shows the 

importance of the entity types which can help us 

get deeper meanings of an entity. And the 

â€œType_LSTMâ€ is difficult to make up by other fea-

tures. 

Performance without a Group of Features 

Table 5 shows the hit rates and overall accuracy of 

our â€œprobabilityâ€ approach generated by removing 

a group of features from the entity linker. We group 

the features with similar effect together. The first 

four features in Table 3 are about surface forms of 

entities, and thus they are put together. The â€œIndi-

vidual featuresâ€ group contains features â€œOut-de-

greeâ€ and â€œType_LSTMâ€, which can distinguish 

entities with the same name. The features â€œIDFâ€ 

and â€œNER_LCSâ€ are grouped into â€œfeatures of 

specificityâ€, which can identify the more likely 

subject entities in the question and kick out com-

mon entities. 

The removal of individual features reduces the 

performance of entity identification by more than 

20%. This result shows the importance of identify-

ing entities with the same name. Otherwise, these 

entities would have the same score. However, the 

removal of these features only reduces the overall 

accuracy of 0.7%, because type identification step 

can make up a part of the removed â€œType_LSTMâ€ 

information. For example, the entity with name 

â€œEstoniaâ€ can be a country or a book, if one â€œEsto-

niaâ€ is a â€œbookâ€, it may not have a property in type 

â€œlocationâ€, and thus we can distinguish these two 

types of â€œEstoniaâ€ a bit. 

The removal of features of specificity affects the 

overall accuracy by 2.7%. These two features fo-

cus on more specific words and make the scores of 

the general entities, such as â€œalbumâ€ or â€œmovieâ€, 

become lower. This behavior is important and can-

not be replaced by the rest of the system.  

4.5 Importance of Type Identification 

Table 6 shows the importance of our type identifi-

cation step. The models with type identification 

significantly outperform the models without type 

network on McNemarâ€™s test (p < 0.01), and in-

creases the accuracies by at least 1%, no matter the 

final score is by probability or weighted sum. This 

shows that the type identification step can effec-

tively handle the hierarchical structure of Freebase 

relations, and can better understand the semantics 

of the question. 

 

 

                      N 

Fetures 

Hit rates @ N 

1 5 10 20 50 100 

All features 80.9 90.2 92.2 93.7 95.1 96.0 

w/o Word Prop. 79.8 89.6 91.7 93.3 95.0 96.0 

w/o Char Prop. 79.4 89.3 91.4 93.2 94.9 95.9 

w/o Rel. Position  79.6 89.6 91.8 93.4 95.0 96.0 

w/o Rel. Position 

to BeV./AuxV. 
79.7 89.6 91.7 93.3 95.0 96.0 

w/o Out-degree 78.8 88.6 90.9 92.8 94.6 95.7 

w/o IDF 79.1 89.3 91.4 93.1 94.9 95.9 

w/o NER_LCS 78.4 88.8 91.1 92.9 94.8 95.8 

w/o Type_LSTM 74.9 88.0 90.7 92.7 94.4 95.6 

Table 4:  Hit rates generated by removing one 

feature from entity linker. â€œw/oâ€ means the re-

moval of the feature. 

 

 

               

                               N 

Feature 

Entity identification 

(Hit rates @ N) 

Overall 

(prob.) 

1 5 10 100 Acc. 

All features 80.9 90.2 92.2 96.0 76.2 

w/o Surface form features 78.8 89.1 91.4 96.0 75.1 

w/o Individual features 60.7 77.9 82.5 92.8 75.5 

w/o Features of specificity 75.8 87.1 89.9 95.6 73.5 

Table 5:  Hit rates generated by removing group 

of features from entity linker. 

 

 

Settings Accuracy (%) 

Probability w/ type identification 76.1 

Probability w/o type identification 75.1 

Sum w/ type identification 76.7 

Sum w/o type identification 75.2 

Table 6:  Gain by the addition of the type identifi-

cation step. 

983



 
 
 

  

5 Error Analysis 

We categorize some errors into the following types.  

ï‚· Entities with the same name: The subject 

entity of the question â€œWhat is the place of 

birth of Sam Edwards?â€  is â€œSam Edwardsâ€. 

Both entities â€œm.03kt3yâ€ (an actress) and 

â€œm.042gjtâ€ (a physicist) have the same name. 

This question does not have enough infor-

mation to distinguish the two entities. 

ï‚· Deleted entity in Freebase: Some questionsâ€™ 

subject entities are deleted in the Freebase 

dump. 

ï‚· Similar properties: Some relations are very 

similar, e.g., both â€œmusic.release.trackâ€ and 

â€œmusic.release.track_listâ€ indicate tracks in 

an album. 

ï‚· Incomplete question: Some questions in the 

dataset are not complete. For example, the 

question â€œWhat production company pro-

duced?â€ does not provide any entities.  

ï‚· Question from object-relation pair: There 

are some questions formed by object-rela-

tion pairs of the triple. E.g., the question 

â€œName a lawyer.â€ is from triple (Charlie 

Herschel, people.person.profession, lawyer). 

We cannot find the subject of the triple from 

the question. 

ï‚· Typo: Some questions contain typo. For ex-

ample, the question â€œWhat is Roger Mol-

liens gender?â€ should be â€œWhat is Roger 

Mollienâ€™s gender?â€  

ï‚· Wrong answer: Some answers are wrong. 

For example, the answer of the question 

â€œWhere was David Armstrong born?â€ is the 

triple (Undisputed Comedy Series: Lil Rel, 

film.film.language, English). Obviously, it 

is not a correct answer to the question.  

ï‚· Controversial answer: E.g., the relation of 

the question â€œLeo Bertos was born in what 

country?â€ is â€œpeople.person.nationalityâ€, 

but the more appropriate relation should be 

â€œpeople.person.place_of_birthâ€, because 

people can have the naturalized citizenship. 

6 Conclusion 

We propose a three-step approach to identify sub-

ject, type and property from knowledge base (KB) 

for answering simple questions. Our ranking 

model with additional features outperforms the 

previous models in subject entity identification. 

The experiments also show that all entity features 

are important. Besides, by splitting the structured 

relation into type and property, our model benefits 

from understanding the question meaning more 

precisely. Our model achieves the state-of-the-art 

performance on the SimpleQuestions dataset. Er-

ror analysis shows that most errors come from the 

problematic questions in the dataset. Extension to 

complex questions will be explored. 

Acknowledgments 

This research was partially supported by Ministry 

of Science and Technology, Taiwan, under grants 

MOST-104-2221-E-002-061-MY3, MOST-105-

2221-E-002-154-MY3 and MOST-106-2923-E-

002-012-MY3, and National Taiwan University 

under grant NTUCCP-106R891305. 

References  

Jonathan Berant, Andrew Chou, Roy Frostig, and 

Percy Liang. 2013. Semantic parsing on freebase 

from question-answer pairs. In Proceedings of the 

2013 Conference on Empirical Methods in Natural 

Language Processing (EMNLP 2013), pages 1533â€“

1544, Seattle, Washington, USA. 

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim 

Sturge, and Jamie Taylor. 2008. Freebase: a collab-

oratively created graph database for structuring hu-

man knowledge. In Proceedings of the 2008 ACM 

SIGMOD international conference on Management 

of data (SIGMOD 2008), pages 1247â€“1249, Van-

couver, BC, Canada. 

Jonathan Berant and Percy Liang. 2014. Semantic 

parsing via paraphrasing. In Association for Com-

putational Linguistics (ACL 2014), volume 7, page 

92â€“102, Baltimore, USA. 

Antoine Bordes, Nicolas Usunier, Sumit Chopra, and 

Jason Weston. 2015. Large-scale simple question 

answering with memory networks. arXiv preprint 

arXiv:1506.02075. 

Zihang Dai, Lei Li, and Wei Xu. 2016. CFO: Condi-

tional focused neural question answering with 

large-scale knowledge bases. In Association for 

Computational Linguistics (ACL 2016), pages 800â€“

810, Berlin, Germany. 

984



 
 
 

  

Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2015. 

Question answering over freebase with multi-col-

umn convolutional neural networks. In Proceedings 

of ACL-IJCNLP, volume 1, pages 260â€“269, Beijing, 

China. 

Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 

2013. Paraphrase-driven learning for open question 

answering. In Association for Computational Lin-

guistics (ACL 2013), pages 1608â€“1618. Sofia, Bul-

garia. 

David Golub and Xiaodong He. 2016. Character-level 

question answering with attention. In Proceedings 

of the 2016 Conference on Empirical Methods in 

Natural Language Processing (EMNLP 2016), 

pages 1598â€“1607, Austin, Texas. 

Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long 

short-term memory. Neural computation, 

9(8):1735â€“1780.  

Thorsten Joachims. 2006. Training Linear SVMs in 

Linear Time. In Proceedings of the ACM Confer-

ence on Knowledge Discovery and Data Mining 

(KDD 2006), Philadelphia, Pennsylvania, USA. 

Diederik P. Kingma and Jimmy Lei Ba. 2014. Adam: 

A method for stochastic optimization. arXiv pre-

print arXiv:1412.6980, 2014. 

Guillaume Lample, Miguel Ballesteros, Sandeep 

Subramanian, Kazuya Kawakami, and Chris Dyer. 

2016. Neural architectures for named entity recog-

nition. In Proceedings of NAACL-HLT 2016, pages 

260â€“270, San Diego, California. 

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, 

Dimitris Kontokostas, Pablo N. Mendes, Sebastian 

Hellmann, Mohamed Morsey, Patrick van Kleef, 

SÃ¶ren Auer, and Christian Bizer. 2015. Dbpedia-a 

large-scale, multilingual knowledge base extracted 

from wikipedia. Semantic Web Journal, 6(2):167â€“

195. 

Jeffrey Pennington, Richard Socher, and Christopher 

D. Manning. 2014. Glove: Global vectors for word 

representation. In Proceedings of the 2014 Confer-

ence on Empirical Methods in Natural Language 

Processing (EMNLP 2014), pages 1532â€“1543, 

Doha, Qatar. 

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and 

Jianfeng Gao. 2015. Semantic parsing via staged 

query graph generation: Question answering with 

knowledge base. In Association for Computational 

Linguistics (ACL 2015), pages 1321â€“1331, Beijing, 

China. 

Wenpeng Yin, MoYu, Bing Xiang, Bowen Zhou, and 

Hinrich SchÃ¼tze. 2016. Simple question answering 

by attentive convolutional neural network. In Pro-

ceedings of COLING, pages 1746â€“1756, Osaka, Ja-

pan. 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

985


