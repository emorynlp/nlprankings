










































Representing and resolving ambiguities in ontology-based question answering


Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 40–49,
Edinburgh, Scotland, UK, July 30, 2011. c©2011 Association for Computational Linguistics

Representing and resolving ambiguities
in ontology-based question answering

Christina Unger
Cognitive Interaction Technology – Center of Excellence (CITEC),

Universität Bielefeld, Germany
{cunger|cimiano}@cit-ec.uni-bielefeld.de

Philipp Cimiano

Abstract

Ambiguities are ubiquitous in natural lan-
guage and pose a major challenge for the au-
tomatic interpretation of natural language ex-
pressions. In this paper we focus on differ-
ent types of lexical ambiguities that play a role
in the context of ontology-based question an-
swering, and explore strategies for capturing
and resolving them. We show that by employ-
ing underspecification techniques and by us-
ing ontological reasoning in order to filter out
inconsistent interpretations as early as possi-
ble, the overall number of interpretations can
be effectively reduced by 44 %.

1 Introduction

Ambiguities are ubiquitous in natural language.
They pose a key challenge for the automatic inter-
pretation of natural language expressions and have
been recognized as a central issue in question an-
swering (e.g. in (Burger et al., 2001)). In gen-
eral, ambiguities comprise all cases in which nat-
ural language expressions (simple or complex) can
have more than one meaning. These cases roughly
fall into two classes: They either concern structural
properties of an expression, e.g. different parses due
to alternative preposition or modifier attachments
and different quantifier scopings, or they concern al-
ternative meanings of lexical items. It is these lat-
ter ambiguities, ambiguities with respect to lexical
meaning, that we are interested in. More specifi-
cally, we will look at ambiguities in the context of
ontology-based interpretation of natural language.

The meaning of a natural language expression in
the context of ontology-based interpretation is the
ontology concept that this expression verbalizes. For
example, the expression city can refer to a class
geo:city (where geo is the namespace of the corre-
sponding ontology), and the expression inhabitants
can refer to a property geo:population. The cor-
respondence between natural language expressions
and ontology concepts need not be one-to-one. On
the one hand side, different natural language expres-
sions can refer to a single ontology concept, e.g.
flows through, crosses through and traverses could
be three ways of expressing an ontological property
geo:flowsThrough. On the other hand, one natu-
ral language expression can refer to different ontol-
ogy concepts. For example, the verb has is vague
with respect to the relation it expresses – it could
map to geo:flowsThrough (in the case of rivers)
as well as geo:inState (in the case of cities). Such
mismatches between the linguistic meaning of an
expression, i.e. the user’s conceptual model, and the
conceptual model in the ontology give rise to a num-
ber of ambiguities. We will give a detailed overview
of those ambiguities in Section 3, after introducing
preliminaries in Section 2.

For a question answering system, there are mainly
two ways to resolve ambiguities: by interactive clar-
ification and by means of background knowledge
and the context with respect to which a question is
asked and answered. The former is, for example,
pursued by the question answering system FREyA
(Damljanovic et al., 2010). The latter is incorporated
in some recent work in machine learning. For exam-
ple, (Kate & Mooney, 2007) investigate the task of

40



learning a semantic parser from a corpus whith sen-
tences annotated with multiple, alternative interpre-
tations, and (Zettlemoyer & Collins, 2009) explore
an unsupervised algorithm for learning mappings
from natural language sentences to logical forms,
with context accounted for by hidden variables in a
perceptron.

In ontology-based question answering, context as
well as domain knowledge is provided by the ontol-
ogy. In this paper we explore how a given ontology
can be exploited for ambiguity resolution. We will
consider two strategies in Section 4. The first one
consists in simply enumerating all possible interpre-
tations. Since this is not efficient (and maybe not
even feasible), we will use underspecification tech-
niques for representing ambiguities in a much more
compact way and then present a strategy for resolv-
ing ambiguities by means of ontological reasoning,
so that the number of interpretations that have to be
considered in the end is relatively small and does not
comprise inconsistent and therefore undesired inter-
pretations. We will summarize with quantitative re-
sults in Section 5.

2 Preliminaries

All examples throughout the paper will be based on
Raymond Mooney’s GeoBase1 dataset and the DB-
pedia question set published in the context of the
1st Workshop on Question Answering Over Linked
Data (QALD-1)2. The former is a relatively small
and well-organized domain, while the latter is con-
siderably larger and much more heterogenous. It is
interesting to note that ontological ambiguituies turn
out to be very wide-spread even in a small and ho-
mogenuous domain like GeoBase (see Section 3 for
specific results).

For specifying entries of a grammar that a ques-
tion answering system might work with, we will use
the general and principled linguistic representations
that our question answering system Pythia3 (Unger
et al., 2010) relies on, as they are suitable for dealing
with a wide range of natural language phenomena.
Syntactic representations will be trees from Lexi-
calized Tree Adjoining Grammar (LTAG (Schabes,

1cs.utexas.edu/users/ml/nldata/geoquery.html
2http://www.sc.cit-ec.uni-bielefeld.de/qald-1
3http://www.sc.cit-ec.uni-bielefeld.de/pythia

1990)). The syntactic representation of a lexical
item is a tree constituting an extended projection of
that item, spanning all of its syntactic and semantic
arguments. Argument slots are nodes marked with a
down arrow (↓), for which trees with the same root
category can be substituted. For example, the tree
for a transitive verb like borders looks as follows:

1. S

DP1 ↓ VP

V
borders

DP2 ↓

The domain of the verb thus spans a whole sentence,
containing its two nominal arguments – one in sub-
ject position and one in object position. The corre-
sponding nodes, DP1 and DP2, are slots for which
any DP-tree can be substituted. For example, substi-
tuting the two trees in 2 for subject and object DP,
respectively, yields the tree in 3.

2. (a) DP

DET
no

NP
state

(b) DP

Hawaii

3. S

DP

DET
no

NP
state

VP

V
borders

DP

Hawaii

As semantic representations we take DUDEs
(Cimiano, 2009), representations similar to struc-
tures from Underspecified Discourse Representation
Theory (UDRT (Reyle, 1993)), extended with some
additional information that allows for flexible mean-
ing composition in parallel to the construction of
LTAG trees. The DUDE for the verb to border, for
example, would be the following (in a slightly sim-
plified version):

geo:borders (x, y)

(DP1, x), (DP2, y)

41



It provides the predicate geo:borders correspond-
ing to the intended concept in the ontology. This
correspondence is ensured by using the vocabulary
of the ontology, i.e. by using the URI4 of the con-
cept instead of a more generic predicate. The pre-
fix geo specifies the namespace, in this case the one
of the GeoBase ontology. Furthermore, the seman-
tic representation contains information about which
substitution nodes in the syntactic structure provide
the semantic arguments x and y. That is, the seman-
tic referent provided by the meaning of the tree sub-
stituted for DP1 corresponds to the first argument x
of the semantic predicate, while the semantic refer-
ent provided by the meaning of the tree substituted
for DP2 corresponds to the second argument y. The
uppermost row of the box contains the referent that
is introduced by the expression. For example, the
DUDE for Hawaii (paired with the tree in 2b) would
be the following:

h

geo:name (h,‘hawaii’)

It introduces a referent h which is related to the lit-
eral ‘hawaii’ by means of the relation geo:name.
As it does not have any arguments, the third row
is empty. The bottom-most row, empty in both
DUDEs, is for selectional restrictions of predicates;
we will see those in Section 4.

Parallel to substituting the DP-tree in 2b for the
DP1-slot in 1, the DUDE for Hawaii is combined
with the DUDE for borders, amounting to the satu-
ration of the argument (DP2, y) by unifying the vari-
ables h and y, yielding the following DUDE:

h

geo:borders (x, h)
geo:name (h,‘hawaii’)

(DP1, x)

Substituting the subject argument no state involves
quantifier representations which we will gloss over
as they do not play a role in this paper. At this point

4URI stands for Uniform Resource Identifier. URIs uniquely
identify resources on the Web. For an overview, see, e.g.,
http://www.w3.org/Addressing/.

it suffices to say that we implement the treatment of
quantifier scope in UDRT without modifications.

Once a meaning representation for a question is
built, it is translated into a SPARQL query, which
can then be evaluated with respect to a given dataset.

Not a lot hinges on the exact choice of the for-
malisms; we could as well have chosen any other
syntactic and semantic formalism that allows the in-
corporation of underspecification mechanisms. The
same holds for the use of SPARQL as formal query
language. The reason for choosing SPARQL is that
it is the standard query language for the Seman-
tic Web5; we therefore feel safe in relying on the
reader’s familiarity with SPARQL and use SPARQL
queries without further explanation.

3 Types of ambiguities

As described in the introduction above, a central task
in ontology-based interpretation is the mapping of
a natural language expression to an ontology con-
cept. And this mapping gives rise to several different
cases of ambiguities.

First, ambiguities can arise due to homonymy of a
natural language expression, i.e. an expression that
has several lexical meanings, where each of these
meanings can be mapped to one ontology concept
unambiguously. The ambiguity is inherent to the ex-
pression and is independent of any domain or ontol-
ogy. This is what in linguistic contexts is called a
lexical ambiguity. A classical example is the noun
bank, which can mean a financial institution, a kind
of seating, the edge of a river, and a range of other
disjoint, non-overlapping alternatives. An example
in the geographical domain is New York. It can mean
either New York city, in this case it would be mapped
to the ontological entity geo:new york city, or
New York state, in this case it would be mapped to
the entity geo:new york. Ambiguous names are ac-
tually the only case of such ambiguities that occur in
the GeoBase dataset.

Another kind of ambiguities is due to mismatches
between a user’s concept of the meaning of an
expression and the modelling of this meaning
in the ontology. For example, if the ontology
modelling is more fine-grained than the meaning

5For the W3C reference, see
http://www.w3.org/TR/rdf-sparql-query/.

42



of a natural language expression, then an expres-
sion with one meaning can be mapped to several
ontology concepts. These concepts could differ
extensionally as well as intensionally. An example
is the above mentioned expression starring, that
an ontology engineer could want to comprise only
leading roles or also include supporting roles. If
he decides to model this distinction and introduces
two properties, then the ontological model is
more fine-grained than the meaning of the natural
language expression, which could be seen as corre-
sponding to the union of both ontology properties.
Another example is the expression inhabitants
in question 4, which can be mapped either to
<http://dbpedia.org/property/population>
or to <http://dbpedia.org/ontology/popula-
tionUrban>. For most cities, both alternatives give
a result, but they differ slightly, as one captures only
the core urban area while the other also includes
the outskirts. For some city, even only one of them
might be specified in the dataset.

4. Which cities have more than two million inhab-
itants?

Such ambiguities occur in larger datasets like DB-
pedia with a wide range of common nouns and tran-
sitive verbs. In the QALD-1 training questions for
DBpedia, for example, at least 16 % of the questions
contain expressions that do not have a unique onto-
logical correspondent.

Another source for ambiguities is the large num-
ber of vague and context-dependent expressions in
natural language. While it is not possible to pin-
point such expressions to a fully specified lexical
meaning, a question answering system needs to map
them to one (or more) specific concept(s) in the on-
tology. Often there are several mapping possibili-
ties, sometimes depending on the linguistic context
of the expression.

An example for context-dependent expressions
in the geographical domain is the adjective big: it
refers to size (of a city or a state) either with respect
to population or with respect to area. For the ques-
tion 5a, for example, two queries could be intended
– one refering to population and one refering to area.
They are given in 5b and 5c.

5. (a) What is the biggest city?

(b) SELECT ?s WHERE {
?s a geo:city .

?s geo:population ?p . }

ORDER BY DESC ?p LIMIT 1

(c) SELECT ?s WHERE {
?s a geo:city .

?s geo:area ?a . }

ORDER BY DESC ?a LIMIT 1

Without further clarification – either by means of
a clarification dialog with the user (e.g. employed
by FREyA (Damljanovic et al., 2010)) or an ex-
plicit disambiguation as in What is the biggest city
by area? – both interpretations are possible and ade-
quate. That is, the adjective big introduces two map-
ping alternatives that both lead to a consistent inter-
pretation.

A slightly different example are vague expres-
sions. Consider the questions 6a and 7a. The
verb has refers either to the object property
flowsThrough, when relating states and rivers, or
to the object property inState, when relating states
and cities. The corresponding queries are given in
6b and 7b.

6. (a) Which state has the most rivers?
(b) SELECT COUNT(?s) AS ?n WHERE {
?s a geo:state .

?r a geo:river .

?r geo:flowsThrough ?s. }

ORDER BY DESC ?n LIMIT 1

7. (a) Which state has the most cities?
(b) SELECT COUNT(?s) AS ?n WHERE {
?s a geo:state .

?c a geo:city .

?c geo:inState ?s. }

ORDER BY DESC ?n LIMIT 1

In contrast to the example of big above, these two
interpretations, flowsThrough and inState, are
exclusive alternatives: only one of them is admis-
sible, depending on the linguistic context. This
is due to the sortal restrictions of those proper-
ties: flowsThrough only allows rivers as domain,
whereas inState only allows cities as domain.

This kind of ambiguities are very frequent, as a
lot of user questions contain semantically light ex-
pressions, e.g. the copula verb be, the verb have,

43



and prepositions like of, in and with (cf. (Cimiano
& Minock, 2009)) – expressions which are vague
and do not specify the exact relation they are de-
noting. In the 880 user questions that Mooney pro-
vides, there are 1278 occurences of the light expres-
sions is/are, has/have, with, in, and of, in addition
to 151 ocurrences of the context-dependent expres-
sions big, small, and major.

4 Capturing and resolving ambiguities

When constructing a semantic representation and
a formal query, all possible alternative meanings
have to be considered. We will look at two strate-
gies to do so: simply enumerating all interpretations
(constructing a different semantic representation and
query for every possible interpretation), and under-
specification (constructing only one underspecified
representation that subsumes all different interpreta-
tions).

4.1 Enumeration
Consider the example of a lexically ambiguous
question in 8a. It contains two ambiguous expres-
sions: New York can refer either to the city or the
state, and big can refer to size either with respect
to area or with respect to population. This leads to
four possible interpretations of the questions, given
in 8b–8e.

8. (a) How big is New York?
(b) SELECT ?a WHERE {
geo:new york city geo:area ?a . }

(c) SELECT ?p WHERE {
geo:new york city geo:population ?p.}

(d) SELECT ?a WHERE {
geo:new york geo:area ?a . }

(e) SELECT ?p WHERE {
geo:new york geo:population ?p . }

Since the question in 8a can indeed have all four in-
terpretations, all of them should be captured. The
enumeration strategy amounts to constructing all
four queries. In order to do so, we specify two
lexical entries for New York and two lexical en-
tries for the adjective big – one for each reading.
For big, these two entries are given in 9 and 10.
The syntactic tree is the same for both, while the
semantic representations differ: one refers to the

property geo:area and one refers to the property
geo:population.

9. N

ADJ
big

N↓
a
geo:area (x, a)
(N, x)

10. N

ADJ
big

N↓
p
geo:population (x, p)
(N, x)

When parsing the question How big is New York,
both entries for big are found during lexical lookup,
and analogously two entries for New York are found.
The interpretation process will use all of them and
therefore construct four queries, 8b–8e.

Vague and context-dependent expressions can be
treated similarly. The verb to have, for example,
can map either to the property flowsThrough, in
the case of rivers, or to the property inState, in
the case of cities. Now we could simply spec-
ify two lexical entries to have – one using the
meaning flowsThrough and one using the mean-
ing inState. However, contrary to lexical ambigu-
ities, these are not real alternatives in the sense that
both lead to consistent readings. The former is only
possible if the relevant argument is a river, the lat-
ter is only relevant if the relevant argument is a city.
So in order not to derive inconsistent interpretations,
we need to capture the sortal restrictions attached to
such exclusive alternatives. This will be discussed
in the next section.

4.2 Adding sortal restrictions
A straightforward way to capture ambiguities con-
sists in enumerating all possible interpretations
and thus in constructing all corresponding formal
queries. We did this by specifying a separate lex-
ical entry for every interpretation. The only diffi-
culty that arises is that we have to capture the sor-
tal restrictions that come with some natural language
expressions. In order to do so, we add sortal restric-
tions to our semantic representation format.

Sortal restrictions will be of the general form
variableˆclass. For example, the sortal restriction
that instances of the variable x must belong to the
class river in our domain would be represented as
xˆgeo:river. Such sortal restrictions are added as

44



a list to our DUDEs. For example, for the verb has
we specify two lexical entries. One maps has to
the property flowThrough, specifying the sortal re-
striction that the first argument of this property must
belong to the class river. This entry looks as fol-
lows:

S

DP1 ↓ VP

V
has

DP2 ↓
geo:flowsThrough (y, x)

(DP1, x), (DP2, y)
xˆgeo:river

The other lexical entry for has consists of the
same syntactic tree and a semantic representation
that maps has to the property inState and contains
the restriction that the first argument of this property
must belong to the class city. It looks as follows:

S

DP1 ↓ VP

V
has

DP2 ↓
geo:inState (y, x)

(DP1, x), (DP2, y)
xˆgeo:city

When a question containg the verb has, like 11a,
is parsed, both interpretations for has are found dur-
ing lexical lookup and two semantic representations
are constructed, both containing a sortal restriction.
When translating the semantic representations into a
formal query, the sortal restriction is simply added as
a condition. For 11a, the two corresponding queries
are given in 11b (mapping has to flowsThrough)
and 11c (mapping has inState). The contribution
of the sortal restriction is boxed.

11. (a) Which state has the most rivers?
(b) SELECT COUNT(?r) as ?c WHERE {
?s a geo:state .

?r a geo:river .

?r geo:flowsThrough ?s .

?r a geo:river . }

ORDER BY ?c DESC LIMIT 1

(c) SELECT COUNT(?r) as ?c WHERE {
?s a geo:state .

?r a geo:river .

?r geo:inState ?s .

?r a geo:city . }

ORDER BY ?c DESC LIMIT 1

In the first case, 11b, the sortal restriction adds a
redundant condition and will have no effect. We can
say that the sortal restriction is satisfied. In the sec-
ond case, in 11c, however, the sortal restriction adds
a condition that is inconsistent with the other condi-
tions, assuming that the classes river and city are
properly specified as disjoint. The query will there-
fore not yield any results, as no instantiiation of r
can be found that belongs to both classes. That is,
in the context of rivers only the interpretation using
flowsThrough leads to results.

Actually, the sortal restriction in 11c is al-
ready implicitly specified in the ontological relation
inState: there is no river that is related to a state
with this property. However, this is not necessar-
ily the case and there are indeed queries where the
sortal restriction has to be included explicitly. One
example is the interpretation of the adjective major
in noun phrases like major city and major state. Al-
though with respect to the geographical domain ma-
jor always expresses the property of having a pop-
ulation greater than a certain threshold, this thresh-
old differs for cities and states: major with respect
to cities is interpreted as having a population greater
than, say, 150 000, while major with respect to states
is interpreted as having a population greater than,
say, 10 000 000. Treating major as ambiguous be-
tween those two readings without specifying a sortal
restriction would lead to two readings for the noun
phrase major city, sketched in 12. Both would yield
non-empty results and there is no way to tell which
one is the correct one.

12. (a) SELECT ?c WHERE {
?c a geo:city .

?c geo:population ?p .

FILTER ( ?p > 150000 ) }

(b) SELECT ?c WHERE {
?c a geo:city .

?c geo:population ?p .

FILTER ( ?p > 10000000 ) }

Specifying sortal restrictions, on the other hand,
would add the boxed material in 13, thereby caus-
ing the wrong reading in 13b to return no results.

13. (a) SELECT ?c WHERE {
?c a geo:city .

?c geo:population ?p .

45



FILTER ( ?p > 150000 ) .

?c a geo:city . }

(b) SELECT ?c WHERE {
?c a geo:city .

?c geo:population ?p .

FILTER ( ?p > 10000000 ) .

?c a geo:state . }

The enumeration strategy thus relies on a conflict
that results in queries which return no result. Un-
wanted interpretations are thereby filtered out auto-
matically. But two problems arise here. The first
one is that we have no way to distinguish between
queries that return no result due to an inconsistency
introduced by a sortal restriction, and queries that
return no result, because there is none, as in the case
of Which states border Hawaii?. The second prob-
lem concerns the number of readings that are con-
structed. In view of the large number of ambiguities,
even in the restricted geographical domain we used,
user questions easily lead to 20 or 30 different pos-
sible interpretations. In cases in which several natu-
ral language terms can be mapped to many different
ontological concepts, this number rises. Enumerat-
ing all alternative interpretations is therefore not ef-
ficient. A more practical alternative is to construct
one underspecified representation instead and then
infer a specific interpretation in a given context. We
will explore this strategy in the next section.

4.3 Underspecification

In the following, we will explore a strategy for rep-
resenting and resolving ambiguities that uses under-
specification and ontological reasoning in order to
keep the number of constructed interpretations to a
minimum. For a general overview of underspecifica-
tion formalisms and their applicability to linguistic
phenomena see (Bunt, 2007).

In order not to construct a different query for
every interpretation, we do not any longer specify
separate lexical entries for each mapping but rather
combine them by using an underspecified semantic
representation. In the case of has, for example, we
do not specify two lexical entries – one with a se-
mantic representation using flowsThrough and one
entry with a representation using inState – but in-
stead specify only one lexical entry with a represen-
tation using a metavariable, and additionally specify

which properties this metavariable stands for under
which conditions.

So first we extend DUDEs such that they now can
contain metavariables, and instead of a list of sor-
tal restrictions contain a list of metavariable speci-
fications, i.e. possible instantiations of a metavari-
able given that certain sortal restrictions are satis-
fied, where sortal restrictions can concern any of the
property’s arguments. Metavariable specifications
take the following general form:

P → p1 (x = class1, . . . , y = class2)
| p2 (x = class3, . . . , y = class4)
| . . .
| pn (x = classi, . . . , y = classj)

This expresses that some metavariable P stands for
a property p1 if the types of the arguments x, . . . , y
are equal to or a subset of class1,. . . ,class2, and
stands for some other property if the types of the
arguments correspond to some other classes. For
example, as interpretation of has, we would chose
a metavariable P with a specification stating that P
stands for the property flowsThrough if the first ar-
gument belongs to class river, and stands for the
property inState if the first argument belongs to
the class city. Thus, the lexical entry for has would
contain the following underspecified semantic repre-
sentation.

14. Lexical meaning of ‘has’:

P (y, x)
(DP1, x), (DP2, y)
P → geo:flowsThrough (y = geo:river)

| geo:inState (y = geo:city)

Now this underspecified semantic representation has
to be specified in order to lead to a SPARQL query
that can be evaluated w.r.t. the knowledge base.
That means, in the course of interpretation we need
to determine which class an instantiation of y be-
longs to and accordingly substitute P by the prop-
erty flowsThrough or inState. In the following
section, we sketch a way of exploiting the ontology
to this end.

46



4.4 Reducing alternatives with ontological
reasoning

In order to filter out interpretations that are inconsis-
tent as early as possible and thereby reduce the num-
ber of interpretations during the course of a deriva-
tion, we check whether the type information of a
variable that is unified is consistent with the sor-
tal restrictions connected to the metavariables. This
check is performed at every relevant step in a deriva-
tion, so that inconsistent readings are not allowed to
percolate and multiply. Let us demonstrate this strat-
egy by means of the example Which state has the
biggest city?.

In order to build the noun phrase the biggest
city, the meaning representation of the superlative
biggest, given in 15, is combined with that of the
noun city, which simply contributes the predication
geo:city (y), by means of unification.

15.

z

Q (y, z)
(N, y)
Q → geo:area(y = geo:city t geo:state)
| geo:population(y = geo:city t geo:state)

The exact details of combining meaning represen-
tations do not matter here. What we want to fo-
cus on is the metavariableQ that biggest introduces.
When combining 15 with the meaning of city, we
can check whether the type information connected
to the unified referent y is compatible with the do-
main restrictions of Q′s interpretations. One way
to do this is by integrating an OWL reasoner and
checking the satisfiability of

geo:city u (geo:city t geo:state)

(for both interpretations of Q, as the restrictions on
y are the same). Since this is indeed satisfiable,
both interpretations are possible, thus cannot be dis-
carded, and the resulting meaning representation of
the biggest city is the following:

y z

geo:city(y)
Q (y, z)
max(z)

Q → geo:area(y = geo:city t geo:state)
| geo:population(y = geo:city t geo:state)

This is desired, as the ambiguity of biggest is a lexi-
cal ambiguity that could only be resolved by the user
specifying which reading s/he intended.

In a next step, the above representation is com-
bined with the semantic representation of the verb
has, given in 14. Now the type information of the
unified variable y has to be checked for compati-
bility with instantiations of an additional metavari-
able, P . The OWL reasoner would therefore have to
check the satisfiability of the following two expres-
sions:

16. (a) geo:city u geo:river
(b) geo:city u geo:city

While 16b succeeds trivially, 16a fails, assuming
that the two classes geo:river and geo:city are
specified as disjoint in the ontology. Therefore
the instantiation of P as geo:flowsThrough is
not consistent and can be discarded, leading to the
following combined meaning representation, where
P is replaced by its only remaining instantiation
geo:inState:

y z

geo:city(y)
geo:inState (y, x)
Q (y, z)
(DP1, x)
Q → geo:area(y = geo:city t geo:state)

| geo:population(y = geo:city t geo:state)

Finally, this meaning representation is com-
bined with the meaning representation of which
state, which simply contributes the predication
geo:state (x). As the unified variable x does not
occur in any metavariable specification, nothing fur-
ther needs to be checked. The final meaning repre-
sentation thus leaves one metavariable with two pos-
sible instantiations and will lead to the following two
corresponding SPARQL queries:

47



17. (a) SELECT ?x WHERE {
?x a geo:city .

?y a geo:state.

?x geo:population ?z .

?x geo:inState ?y . }

ORDER BY DESC(?z) LIMIT 1

(b) SELECT ?x WHERE {
?x a geo:city .

?y a geo:state.

?x geo:area ?z .

?x geo:inState ?y . }

ORDER BY DESC(?z) LIMIT 1

Note that if the ambiguity of the metavariable
P were not resolved, we would have ended up
with four SPARQL queries, where two of them use
the relation geo:flowsThrough and therefore yield
empty results. So in this case, we reduced the num-
ber of constructed queries by half by discarding in-
consistent readings. We therefore solved the prob-
lems mentioned at the end of 4.2: The number of
constructed queries is reduced, and since we discard
inconsistent readings, null answers can only be due
to the lack of data in the knowledge base but not can-
not anymore be due to inconsistencies in the gener-
ated queries.

5 Implementation and results

In order to see that the possibility of reducing the
number of interpretations during a derivation does
not only exist in a small number of cases, we ap-
plied Pythia to Mooney’s 880 user questions, imple-
menting the underspecification strategy in 4.3 and
the reduction strategy in 4.4. Since Pythia does not
yet integrate a reasoner, it approximates satisfiabil-
ity checks by means of SPARQL queries. When-
ever meaning representations are combined, it ag-
gregates type information for the unified variable,
together with selectional information connected to
the occuring metavariables, and uses both to con-
struct a SPARQL query. This query is then evalu-
ated against the underlying knowledge base. If the
query returns results, the interpetations are taken to
be compatible, if it does not return results, the in-
terpretations are taken to be incompatible and the
according instantiation possibility of the metavari-
able is discarded. Note that those SPARQL queries
are only an approximation for the OWL expressions

used in 4.4. Furthermore, the results they return are
only an approximation of satisfiability, as the reason
for not returning results does not necessarily need to
be unsatisfiability of the construction but could also
be due the absence of data in the knowledge base.
In order to overcome these shortcomings, we plan to
integrate a full-fledged OWL reasoner in the future.

Out of the 880 user questions, 624 can be parsed
by Pythia (for an evaluation on this dataset and rea-
sons for failing with the remaining 256 questions,
see (Unger & Cimiano, 2011)). Implementing the
enumeration strategy, i.e. not using disambiguation
mechanisms, there was a total of 3180 constructed
queries. With a mechanism for removing scope am-
biguities by means of simulating a linear scope pref-
erence, a total of 2936 queries was built. Addi-
tionally using the underspecification and resolution
strategies described in the previous section, by ex-
ploiting the ontology with respect to which natural
language expressions are interpreted in order to dis-
card inconsistent interpretations as early as possible
in the course of a derivation, the number of total
queries was further reduced to 2100. This amounts
to a reduction of the overall number of queries by
44 %. The average and maximum number of queries
per question are summarized in the following table.

Avg. # queries Max. # queries
Enumeration 5.1 96
Linear scope 4.7 (-8%) 46 (-52%)
Reasoning 3.4 (-44%) 24 (-75%)

6 Conclusion

We investigated ambiguities arising from mis-
matches between a natural language expressions’
lexical meaning and its conceptual modelling in an
ontology. Employing ontological reasoning for dis-
ambiguation allowed us to significantly reduce the
number of constructed interpretations: the average
number of constructed queries per question can be
reduced by 44 %, the maximum number of queries
per question can be reduced even by 75 %.

48



References
Bunt, H.: Semantic Underspecification: Which Tech-

nique For What Purpose? In: Computing Meaning,
vol. 83, pp. 55–85. Springer Netherlands (2007)

Cimiano, P.: Flexible semantic composition with
DUDES. In: Proceedings of the 8th International Con-
ference on Computational Semantics (IWCS). Tilburg
(2009)

Unger, C., Hieber, F., Cimiano, P.: Generating LTAG
grammars from a lexicon-ontology interface. In: S.
Bangalore, R. Frank, and M. Romero (eds.): 10th In-
ternational Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+10), Yale University
(2010)

Unger, C., Cimiano, P.: Pythia: Compositional mean-
ing construction for ontology-based question answer-
ing on the Semantic Web. In: Proceedings of the 16th
International Conference on Applications of Natural
Language to Information Systems (NLDB) (2011)

Schabes, Y.: Mathematical and Computational Aspects
of Lexicalized Grammars. Ph. D. thesis, University of
Pennsylvania (1990)

Reyle, U.: Dealing with ambiguities by underspecifica-
tion: Construction, representation and deduction. Jour-
nal of Semantics 10, 123–179 (1993)

Kamp, H., Reyle, U.: From Discourse to Logic. Kluwer,
Dordrecht (1993)

Cimiano, P., Minock, M.: Natural Language Interfaces:
What’s the Problem? – A Data-driven Quantitative
Analysis. In: Proceedings of the International Confer-
ence on Applications of Natural Language to Informa-
tion Systems (NLDB), pp. 192–206 (2009)

Damljanovic, D., Agatonovic, M., Cunningham, H.:
Natural Language Interfaces to Ontologies: Combin-
ing Syntactic Analysis and Ontology-based Lookup
through the User Interaction. In: Proceedings of the
7th Extended Semantic Web Conference, Springer
Verlag (2010)

Zettlemoyer, L., Collins, M.: Learning Context-
dependent Mappings from Sentences to Logical Form.
In: Proceedings of the Joint Conference of the As-
sociation for Computational Linguistics and Interna-
tional Joint Conference on Natural Language Process-
ing (ACL-IJCNLP), pp. 976–984 (2009)

Burger, J., Cardie, C., Chaudhri, V., Gaizauskas,
R., Israel, D., Jacquemin, C., Lin, C.-Y., Maio-
rano, S., Miller, G., Moldovan, D., Ogden,
B., Prager, J., Riloff, E., Singhal, A., Shrihari,
R., Strzalkowski, T., Voorhees, E., Weischedel,
R.: Issues, tasks, and program structures to
roadmap research in question & answering (Q & A).
http://www-nlpir.nist.gov/projects/duc/
papers/qa.Roadmap-paper v2.doc (2001)

Kate, R., Mooney, R.: Learning Language Semantics
from Ambiguous Supervision. In: Proceedings of the
22nd Conference on Artificial Intelligence (AAAI-07),
pp. 895–900 (2007)

49


