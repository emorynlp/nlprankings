Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038–1046,

Beijing, August 2010

1038

Modeling Socio-Cultural Phenomena in Discourse 

Tomek Strzalkowski1,2, George Aaron Broadwell1, Jennifer Stromer-Galley1, 

 

Samira Shaikh1, Sarah Taylor3 and Nick Webb1 

1ILS Institute, University at Albany, SUNY 

2IPI, Polish Academy of Sciences 
3Lockheed Martin Corporation 

tomek@albany.edu 

Abstract 

In this paper, we describe a novel ap-
proach 
to  computational  modeling 
and  understanding  of  social  and  cul-
tural  phenomena  in  multi-party  dia-
logues.  We  developed  a  two-tier  ap-
proach  in  which  we  first  detect  and 
classify  certain  social  language  uses, 
including topic control, disagreement, 
and  involvement,  that  serve  as  first 
order models from which presence the 
higher level social constructs such as 
leadership, may be inferred.  

face-to-face 

interactions, 

1.  Introduction 
We  investigate  the  language  dynamics  in 
small  group  interactions  across  various  set-
tings.  Our  focus  in  this  paper  is  on  English 
online chat conversations; however, the mod-
els we are developing are more universal and 
applicable  to  other  conversational  situations: 
informal 
formal 
meetings,  moderated  discussions,  as  well  as 
interactions  conducted  in  languages  other 
than English, e.g., Urdu and Mandarin.  
Multi-party  online  conversations  are  particu-
larly  interesting  because  they  become  a  per-
vasive form of communication within virtual 
communities, ubiquitous across all age groups. 
In  particular,  a  great  amount  of  communica-
tion online occurs in virtual chat-rooms, typi-
cally  conducted  using  a  highly  informal  text 
dialect.  At  the  same  time,  the  reduced-cue 
environment of online interaction necessitates 
more  explicit  linguistic  devices  to  convey 
social and cultural nuances than is typical in 
face-to-face or even voice conversations.  
Our  objective  is  to  develop  computational 
models of how certain social phenomena such 
as leadership, power, and conflict are signaled 
and  reflected  in  language  through  the  choice 
of  lexical,  syntactic,  semantic  and  conversa-
tional forms by discourse participants. In this 

paper we report the results of an initial phase 
of  our  work  during  which  we  constructed  a 
prototype  system  called  DSARMD-1  (De-
tecting  Social  Actions  and  Roles  in  Multi-
party  Dialogue).  Given  a  representative  seg-
ment  of  multiparty  task-oriented  dialogue, 
DSARMD-1  automatically  classifies  all  dis-
course  participants  by  the  degree  to  which 
they  deploy  selected  social  language  uses, 
such  as  topic  control,  task  control,  involve-
ment,  and  disagreement.  These  are 
the 
mid-level  social  phenomena,  which  are  de-
ployed  by  discourse  participants  in  order  to 
achieve  or  assert  higher-level  social  con-
structs, including leadership. In this work we 
adopted  a  two-tier  empirical  approach  where 
social  language  uses  are  modeled  through 
observable  linguistic  features  that  can  be 
automatically  extracted  from  dialogue.  The 
high-level  social  constructs  are  then  inferred 
from  a  combination  of  language  uses  attrib-
uted to each discourse participant; for exam-
ple, a high degree of influence and a high de-
gree of involvement by the same person may 
indicate  a  leadership  role.  In  this  paper  we 
limit our discussion to the first tier only: how 
to  effectively  model  and  classify  social  lan-
guage uses in multi-party dialogue.  
2.  Related Research 
Issues  related  to  linguistic  manifestation  of 
social  phenomena  have  not  been  systemati-
cally  researched  before  in  computational  lin-
guistics;  indeed,  most  of  the  effort  thus  far 
was  directed  towards  the  communicative  di-
mension of discourse. While the Speech Acts 
theory  (Austin,  1962;  Searle,  1969)  provides 
a  generalized  framework  for  multiple  levels 
of discourse analysis (locution, illocution and 
perlocution), most current approaches to dia-
logue  focus  on 
information  content  and 
structural  components  (Blaylock,  2002;  Car-
berry & Lambert, 1999; Stolcke, et al., 2000) 
in dialogue; few take into account the effects 
that  speech  acts  may  have  upon  the  social 

1039

roles  of  discourse  participants.  Also  relevant 
is  research  on  modeling  sequences  of  dia-
logue acts – to predict the next one (Samuel et 
al. 1998; Ji & Bilmes, 2006 inter alia) – or to 
map  them  onto  subsequences  or  “dialogue 
games”  (Carlson  1983;  Levin  et  al.,  1998), 
which  are  attempts  to  formalize  participants’ 
roles in conversation (e.g., Linell, 1990; Poe-
sio & Mikheev, 1998; Field et al., 2008). 
There is a body of literature in anthropology, 
linguistics, sociology, and communication on 
the relationship between language and power, 
as well as other social phenomena, e.g., con-
flict, 
leadership;  however,  existing  ap-
proaches  typically  look  at  language  use  in 
situations  where  the  social  relationships  are 
known,  rather  than  using  language  predic-
tively.  For  example,  conversational  analysis 
(Sacks  et  al.,  1974)  is  concerned  with  the 
structure  of  interaction:  turn-taking,  when 
interruptions occur, how repairs are signaled, 
but  not  what  they  reveal  about  the  speakers. 
Research in anthropology and communication 
has concentrated on how certain social norms 
and  behaviors  may  be  reflected  in  language 
(e.g., Scollon and Scollon, 2001; Agar, 1994) 
with few systematic studies attempting to ex-
plore  the  reverse,  i.e.,  what  the  linguistic 
phenomena  tell  us  about  social  norms  and 
behaviors.  
3.  Data & Annotation 
Our  initial  focus  has  been  on  on-line  chat 
dialogues. While chat data is plentiful on-line, 
its adaptation for research purposes presents a 
number of challenges that include users’ pri-
vacy  issues  on  the  one  hand,  and  their  com-
plete  anonymity  on  the  other.  Furthermore, 
most  data  that  may  be  obtained  from  public 
chat-rooms is of limited value for the type of 
modeling tasks we are interested in due to its 
high-level of noise, lack of focus, and rapidly 
shifting,  chaotic  nature,  which  makes  any 
longitudinal  studies  virtually  impossible.  To 
derive complex models of conversational be-
havior, we need the interaction to be reasona-
bly focused on a task and/or social objectives 
within a group. 
Few data collections exist covering multiparty 
dialogue,  and  even  fewer  with  on-line  chat. 
Moreover, the few collections that exist were 
built  primarily  for  the  purpose  of  training 
dialogue  act  tagging  and  similar  linguistic 
phenomena;  few  if  any  of  these  corpora  are 

and 

labeled 

include 

resources 

suitable  for  deriving  pragmatic  models  of 
conversation,  including  socio-linguistic  phe-
nomena.  Existing 
a 
multi-person  meeting  corpus  ICSI-MRDA 
and the AMI Meeting Corpus (Carletta, 2007), 
which  contains  100  hours  of  meetings  cap-
tured  using  synchronized  recording  devices. 
Still,  all  of  these  resources  look  at  spoken 
language  rather  than  on-line  chat.  There  is  a 
parallel  interest  in  the  online  chat  environ-
ment, although the development of useful re-
sources  has  progressed  less.  Some  corpora 
exist  such  as  the  NPS  Internet  chat  corpus 
(Forsyth  and  Martell,  2007),  which  has  been 
hand-anonymized 
with 
part-of-speech  tags  and  dialogue  act  labels. 
The StrikeCom corpus (Twitchell et al., 2007) 
consists  of  32  multi-person  chat  dialogues 
between players of a strategic game, where in 
50% of the dialogues one participant has been 
asked to behave ‘deceptively’. 
It is thus more typical that those interested in 
the  study  of  Internet  chat  compile  their  own 
corpus on an as needed basis, e.g., Wu et al. 
(2002), Khan et al. (2002), Kim et al. (2007).  
Driven by the need to obtain a suitable dataset 
we designed a series of experiments in which 
recruited  subjects  were  invited  to  participate 
in  a  series  of  on-line  chat  sessions  in  a  spe-
cially  designed  secure  chat-room.  The  ex-
periments  were  carefully  designed  around 
topics, tasks, and games for the participants to 
engage in so that appropriate types of behav-
ior, e.g., disagreement, power play, persuasion, 
etc.  may  emerge  spontaneously.  These  ex-
periments and the resulting corpus have been 
described  elsewhere  (Shaikh  et  al.,  2010b), 
and  we  refer  the  reader  to  this  source.  Ulti-
mately  a  corpus  of  50  hours  of  English  chat 
dialogue was collected comprising more than 
20,000  turns  and  120,000  words.  In  addition 
we  also  assembled  a  corpus  of  20  hours  of 
Urdu chat.  
A subset of English language dataset has been 
annotated at four levels: communication links, 
dialogue  acts,  local  topics  and  meso-topics 
(which are essentially the most persistent lo-
cal topics). Although full details of these an-
notations are impossible to explain within the 
scope of this article, we briefly describe them 
below.  Annotated  datasets  were  used  to  de-
velop and train automatic modules that detect 
and  classify  social  uses  of  language  in  dis-
course. It is important to note that the annota-

1040

tion has been developed to support the objec-
tives  of  our  project  and  does  not  necessarily 
conform  to  other  similar  annotation  systems 
used in the past.  
•  Communicative links.  In  a  multi-party  dia-
logue an utterance may be directed towards 
a  specific  participant,  a  subgroup  of  par-
ticipants or to everyone.  

•  Dialogue  Acts.  We  developed  a  hierarchy 
of 15 dialogue acts for annotating the func-
tional aspect of the utterance in discussion.  
The tagset we adopted is based on DAMSL 
(Allen & Core, 1997) and SWBD (Jurafsky 
et  al.,  1997),  but  compressed  to  15  tags 
tuned  significantly  towards  dialogue  prag-
matics  and  away  from  more  surface  char-
acteristics  of  utterances  (Shaikh  et  al., 
2010a).  

•  Topic  reference  polarity.  Some 

•  Local  topics.  Local  topics  are  defined  as 
nouns or noun phrases introduced into dis-
course  that  are  subsequently  mentioned 
again via repetition, synonym, or pronoun.  
topics, 
which we call meso-topics, persist through 
a number of turns in conversation. A selec-
tion  of  meso-topics  is  closely  associated 
with  the  task  in  which  the  discourse  par-
ticipants  are  engaged.  Meso-topics  can  be 
distinguished from the local topics because 
the  speakers  often  make  polarized  state-
ments about them.  

(LU) 

selected  Language  Uses 

4.  Socio-linguistic Phenomena 
We are interested in modeling the social phe-
nomena of Leadership and Power in discourse. 
These high-level phenomena (or Social Roles, 
SR)  will  be  detected  and  attributed  to  dis-
course participants based on their deployment 
of 
in 
multi-party  dialogue.  Language  Uses  are 
mid-level  socio-linguistic  devices  that  link 
linguistic  components  deployed  in  discourse 
(from  lexical  to  pragmatic)  to  social  con-
structs obtaining for and between the partici-
pants. The language uses that we are currently 
studying  are  Agenda  Control,  Disagreement, 
and Involvement (Broadwell et al., 2010). 
Our research so far is focused on the analysis 
of  English-language  synchronous  chat,  and 
we are looking for correlations between vari-
ous metrics that can be used to detect LU in 
multiparty  dialogue.  We  expect  that  some  of 
these  correlations  may  be  culturally  specific 
or  language-specific,  as  we  move  into  the 

analysis  of  Urdu  and  Mandarin  discourse  in 
the next phase of this project. 

4.1 Agenda Control in Dialogue 
Agenda  Control  is  defined  as  efforts  by  a 
member or members of the group to advance 
the  group’s  task  or  goal.  This  is  a  complex 
LU that we will model along two dimensions: 
(1) Topic Control and (2) Task Control. Topic 
Control  refers  to  attempts  by  any  discourse 
participants  to  impose  the  topic  of  conversa-
tion.  Task  Control,  on  the  other  hand,  is  an 
effort  by  some  members  of  the  group  to  de-
fine  the  group’s  project  or  goal  and/or  steer 
the  group  towards  that  goal.  We  believe  that 
both  behaviors  can  be  detected  using  scalar 
measures  per  participant  based  on  certain 
linguistic features of their utterances. 
For  example,  one  hypothesis  is  that  topic 
control is indicated by the rate of local topic 
introductions  (LTI)  per  participant  (Givon, 
1983).  Local  topics  may  be  defined  quite 
simply  as  noun  phrases  introduced  into  dis-
course,  which  are  subsequently  mentioned 
again  via  repetition,  synonym,  pronoun,  or 
other  form  of  co-reference.  Thus,  one  meas-
ure  of  topic  control  is  the  number  of  local 
topics  introduced  by  each  participant  as  per-
centage of all local topics in a discourse.  
Using  an  LTI  index  we  can  construct  asser-
tions  about  topic  control  in  a  discourse.  For 
example,  suppose  the  following  information 
is  discovered  about  the  speaker  LE  in  a 
multi-party  discussion  dialogue-11 where  90 
local topics are identified: 
1. LE  introduces  23/90  (25.6%)  of  local  top-

ics in this dialogue. 

2. The  mean  rate  of  local  topic  introductions 
is  this  dialogue  is  14.29%,  and  standard 
deviation is 8.01. 

3. LE is in the top quintile of participants for 

introducing new local topics 

We  can  now  claim  the  following,  with  a  de-
gree of confidence (to be determined): 

TopicControlLTI (LE, 5, dialogue-1) 

We read this as follows: speaker LE exerts the 
highest degree of topic control in dialogue-1. 
Of course, LTI is just one source of evidence 
and  we  developed  other  metrics  to  comple-
ment it. We mention three of them here: 
                                                 
1 Dialogue-1  refers  to  an  actual  dataset  of  90-minute  chat 
among  7  participants,  covering  approximately  700  turns.  The 
task is to select a candidate for a job given a set of resumes. 

1041

to 

•  SMT Index. This is a measure of topic con-
trol  suggested  in  (Givon,  1983)  and  it  is 
based  on  subsequent  mentions  of  already 
introduced  local  topics.  Speakers  who  in-
troduce  topics  that  are  discussed  at  length 
by the group tend to control the topic of the 
discussion. The subsequent mentions of lo-
cal topics  (SMT)  index  calculates  the  per-
centage  of  second  and  subsequent  refer-
ences  to  the  local  topics,  by  repetition, 
synonym,  or  pronoun,  relative 
the 
speakers who introduced them.  

•  Cite Score. This index measures the extent 
to  which  other  participants  discuss  topics 
introduced  by  that  speaker.  The  difference 
between SMT and CiteScore is that the lat-
ter reflect to what degree a speaker’s efforts 
to control the topic are assented to by other 
participants in a conversation. 

•  TL  Index  (TL).  This  index  stipulates  that 
more influential speakers take longer turns 
than those who are less influential. The TL 
index  is  defined  as  the  average  number  of 
words  per  turn  for  each  speaker.  Turn 
length  also  reflects the  extent  to which 
other  participants  are  willing  to  ‘yield  the 
floor’ in conversation. 

Like  LTI,  all  the  above  indices  are  mapped 
into a degree of topic control, based on quin-
tiles in normal distribution (Table 1). 
 

SMT 

LTI 
5 
4 
4 
3 
2 
2 
1 

AVG 
LE 
5.00 
JR 
3.75 
KI 
2.75 
KN 
4.00 
KA 
2.50 
CS 
2.00 
JY 
1.25 
Table 1: Topic Control distribution in dialogue-1. Each 

TL 
5 
3 
1 
4 
4 
2 
2 

CS 
5 
4 
3 
4 
2 
2 
1 

5 
4 
3 
5 
2 
2 
1 

row represents a speaker in the group (LE, JR, etc.). 
Columns show indices used, with degrees per speaker 
on 5-point scale based on quintiles in normal distribu-

tion, and the average value. 

Ideally,  all  the  above  indices  (and  others  yet 
to  be  defined)  should  predict  the  same  out-
come, i.e., for each dialogue participant they 
should assign the same degree of topic control, 
relative to other speakers. This is not always 
the case, and where the indices divert in their 
predictions,  our  level  of  confidence  in  the 
generated claims decreases. We are currently 

 

 

working  on  how  these  different  metrics  cor-
relate  to  each  other  and  how  they  should  be 
weighted  to  maximize  accuracy  of  making 
Topic  Control  claims.  Nonetheless,  we  can 
already output a Topic Control map (shown in 
Table 1) that captures a sense of internal so-
cial dynamics within the group.  
The  other  aspect  of  Agenda  Control  phe-
nomenon is Task Control. It is defined as an 
effort  to  determine  the  group's  goal  and/or 
steer  the  group  towards  that  goal.  Unlike 
Topic Control, which is imposed by influenc-
ing the subject of conversation, Task Control 
is  gained  by  directing  other  participants  to 
perform  certain  tasks  or  accept  certain  opin-
ions.  Consequently,  Task  Control  is  detected 
by  observing  the  usage  of  certain  dialogue 
acts, 
Action-Directive, 
Agree-Accept,  Disagree-Reject,  and  related 
categories.  Here  again,  we  define  several  in-
dices  that  allow  us  to  compute  a  degree  of 
Task Control in dialogue for each participant: 
•  Directive  Index  (DI).  The  participant  who 
directs  others  is  attempting  to  control  the 
course  of  the  task  that  the  group  is  per-
forming. We count the number of directives, 
i.e., 
as  Ac-
tion-Directive, made by each participant as 
a percentage of all directives in discourse. 

utterances 

including 

classified 

•  Directed Topic Shift Index (DTSI). When a 
participant  who  controls  the  task  offers  a 
directive on the task, then the topic of con-
versation shifts. In order to detect this con-
dition, we calculate the ratio of coincidence 
of  directive  dialogue  acts  by  each  partici-
pant with topic shifts following them.  

•  Process Management index (PMI). Another 
measure  of  Task  Control  is  the  proportion 
of turns each participant has that explicitly 
address  the  problem  solving  process.  This 
includes  utterances  that  involve  coordinat-
ing  the  activities  of  the  participants,  plan-
ning  the  order  of  activities,  etc.  These  fall 
into  the  category  of  Task  (or  Process) 
Management in most DA tagging systems.  
Index 
(PMSI). This index measures the degree of 
success  by  each  speaker  at  controlling  the 
task. A credit is given to the speaker whose 
suggested  curse  of  action  is  supported  by 
other  speakers  for  each  response  that  sup-
ports the suggestion. Conversely, a credit is 
taken away for each response that rejects or 

•  Process  Management 

Success 

1042

qualifies the suggestion. PMSI is computed 
as  distribution  of  task  management  credits 
among  the  participants  over  all  dialogue 
utterances  classified  as  Task/Process  Man-
agement. 2 

As  an  example,  let’s  consider  the  following 
information computed for the PMI index over 
dialogue-1:  
1. Dialogue-1  contains  246  utterances  classi-
fied  as  Task/Process  Management  rather 
than doing the task. 

2. Speaker  KI  makes  65  of  these  utterances 

for a PMI of 26.4%. 

3. Mean  PMI  for  participants  is  14.3%;  80th 
percentile is >21.2%. PMI for KI is in the 
top quintile for all participants. 

Based  on  this  evidence  we  may  claim  (with 
yet to be determined confidence) that: 
TaskControlPMI(KI, 5, dialogue-1) 

This  may  be  read  as  follows: speaker KI ex-
erts  the  highest  degree  of  Task  Control  in 
dialogue-1.  We  note  that  Task  Control  and 
Topic  Control  do  not  coincide  in  this  dis-
course, at least based on the PMI index. Other 
index  values  for  Task  Control  may  be  com-
puted and tabulated in a way similar to LTI in 
Table  1.  We  omit  these  here  due  to  space 
limitations. 

4.2 Disagreement in Dialogue 
Disagreement  is  another  language  use  that 
correlates  with  speaker’s  power  and  leader-
ship.  There  are  two  ways  in  which  disagree-
ment is realized: expressive disagreement and 
topical  disagreement  (Stromer-Galley,  2007; 
Price, 2002). Both can be detected using sca-
lar measures applied to subsets of participants, 
typically any two participants. In addition, we 
can also measure for each participant the rate 
with which he or she generates disagreement 
(with any and all other speakers). Expressive 
Disagreement  is  normally  understood  at  the 
level  of  dialogue  acts,  i.e.,  when  discourse 
participants  make  explicit  utterances  of  dis-
agreement,  disapproval,  or  rejection  in  re-
sponse to a prior speaker’s utterance. Here is 
an example (KI and KA are two speakers in a 
multiparty  dialogue  in  which  participants 
                                                 
2 The exact structure of the credit function is still being deter-
mined experimentally. For example, more credit may be given 
to first supporting response and less for subsequent responses; 
more credit may be given for unprompted suggestions than for 
those that were responding to questions from others. 

discuss candidates for a youth counselor job): 
KA: CARLA... women are always better with 

kids 

KI: That’s not true! 
KI: Men can be good with kids too 

While such exchanges are vivid examples of 
expressive disagreement, we are interested in 
more  sustained  phenomenon  where 
two 
speakers repeatedly disagree, thus revealing a 
social  relationship  between  them.  Therefore, 
one measure of Expressive Disagreement that 
we consider is the number of Disagree-Reject 
dialogue acts between any two speakers as a 
percentage  of  all  utterances  exchanged  be-
tween  these  two  speakers.  This  becomes  a 
basis for the Disagree-Reject Index (DRX). In 
dialogue-1 we have: 
1. Speakers KI and KA have 47 turns between 
them. Among these there are 8 turns classi-
fied  as  Disagree-Reject,  for  the  DRX  of 
15.7%. 

2. The mean DRX for speakers who make any 
Disagree-Reject  utterances  is  9.5%.  The 
pair of speakers KI-KA is in the top quin-
tile (>13.6%). 

Based  on  this  evidence  we  can  conclude  the 
following: 
  ExpDisagreementDRX (KI,KA, 5, dialogue-1) 
which  may  be  read  as  follows:  speakers  KI 
and  KA  have  the  highest  level  of  expressive 
disagreement  in  dialogue-1.  This  measure  is 
complemented by a Cumulative Disagreement 
Index  (CDX),  which  is  computed  for  each 
speaker as a percentage of all Disagree-Reject 
utterances  in  the  discourse  that  are  made  by 
this speaker. Unlike DRX, which is computed 
for pairs of speakers, the CDX values are as-
signed to each group participant and indicate 
the  degree  of  disagreement  that  each  person 
generates. 
While  Expressive  Disagreement  is  based  on 
the  use  of  more  overt  linguistic  devices, 
Topical Disagreement  is  defined  as  a  differ-
ence  in  referential  valence  in  utterances 
(statements,  opinions,  questions,  etc.)  made 
on a topic. Referential valence of an utterance 
is determined by the type of  statement made 
about  the  topic  in  question,  which  can  be 
positive  (+),  negative  (−),  or  neutral  (0).  A 
positive statement is one in favor of (express 
advocacy) or in support of (supporting infor-
mation) the topic being discussed. A negative 
statement is one that is against or negative on 

1043

the topic being discussed. A neutral statement 
is one that does not indicate the speaker’s po-
sition on the topic. Here is an example of op-
posing  polarity  statements  about  the  same 
topic in discourse: 

Sp-1: I like that he mentions “Volunteerism 

and Leadership” 

Sp-2: but if they’re looking for someone who 

is experienced then I’d cross him off 

Detecting topical disagreement in discourse is 
more  complicated  because  its  strength  may 
vary  from  one  topic  in  a  conversation  to  the 
next. A reasonable approach is thus to meas-
ure  the  degree  of  disagreement  between  two 
speakers on one topic first, and then extrapo-
late  over  the  entire  discourse.  Accordingly, 
our measure of topical disagreement is valua-
tion differential between any two speakers as 
expressed  in  their  utterances  about  a  topic. 
Here,  the  topic  (or  an  “issue”)  is  understood 
more narrowly than the local topic defined in 
the previous section (as used in Topic Control, 
for  example),  and  may  be  assumed  to  cover 
only the most persistent local topics, i.e., top-
ics  with  the  largest  number  of  references  in 
dialogue, or what we call the meso-topics. For 
example,  in  a  discussion  of  job  applicants, 
each of the applicants becomes a meso-topic, 
and there may be additional meso-topics pre-
sent, such as qualifications required, etc.  
The  resulting  Topical  Disagreement  Metric 
(TDM) captures the degree to which any two 
speakers  advocate  the  opposite  sides  of  a 
meso-topic. TDM is computed as an average 
of  P-valuation  differential  for  one  speaker 
(advocating 
and 
(−P)-valuation  differential 
the  other 
speaker (advocating against the meso-topic).  
Using  TDM  we  can  construct  claims  related 
to  disagreement  in  a  given  multiparty  dia-
logue  of  sufficient  duration  (exactly  what 
constitutes  a  sufficient  duration  is  still  being 
researched). Below is an example based on a 
90-minute  chat  dialogue-1  about  several  job 
candidates for a youth counselor. The discus-
sion involved 7 participants, including KI and 
KA.  Topical  disagreement  is  measured  on  5 
points  scale  (corresponding  to  quintiles  in 
normal distribution): 
TpDisAgreeTDM(KI,KA,“Carla”,4,dialogue-1) 
This may be read as follows: speakers KI and 
KA  topically  disagree  to  degree  4  on  topic 
[job  candidate]  “Carla”  in  dialogue-1.  In  or-

a  meso-topic) 

for 

for 

der to calculate this we compute the value of 
TDM index between these two speakers. We 
find that KA makes 30% of all positive utter-
ances made by anyone about Carla (40), while 
KI  makes  45%  of  all  negative  utterances 
against Carla. This places these two speakers 
in the top quintiles in the “for Carla” polarity 
distribution  and  “against  Carla”  distribution, 
respectively.  Taking  into  account  any  oppos-
ing  polarity  statements  made  by  KA  against 
Carla  and  any  statements  made  by  KI  for 
Carla,  we  calculate  the  level  of  topical  dis-
agreement between KA and KI to be 4 on the 
1-5 scale. 
TDM allows us to compute topical disagree-
ment between any two speakers in a discourse, 
which  may  also  be 
in  a 
2-dimensional  table  revealing  another  inter-
esting aspect of internal group dynamics.  

represented 

4.3 Involvement in Dialogue 
The third type of social language use that we 
discuss  in  this  paper  is  Involvement.  In-
volvement  is  defined  as  a  degree  of  engage-
ment  or  participation  in  the  discussion  of  a 
group.  It  is  an  important  element  of  leader-
ship,  although  its  importance  is  expected  to 
differ  between  cultures;  in  Western  cultures, 
high involvement and influence (topic control) 
often correlates with group leadership. 
In order to measure Involvement we designed 
several  indices  based  on  turn  characteristics 
for  each  speaker.  Four  of  the  indices  are 
briefly explained below:  
•  The NP index (NPI) is a measure of gross 
informational  content  contributed  by  each 
speaker  in  discourse.  NPI  counts  the  ratio 
of  third-person  nouns  and  pronouns  used 
by  a  speaker  to  the  total  number  of  nouns 
and pronouns in the discourse.  

•  The Turn index (TI) is a measure of inter-
actional  frequency;  it  counts  the  ratio  of 
turns per participant to the total number of 
turns in the discourse.  

•  The  Topic  Chain  Index  (TCI)  counts  the 
degree to which participants discuss of the 
most persistent topics. In order to calculate 
TCI values, we define a topic chains for all 
local  topics.  We  compute  frequency  of 
mentions  of  these  longest  topics  for  each 
participant.  

•  The  Allotopicality  Index  (ATP)  counts  the 
number  of  mentions  of  local  topics  that 
were  introduced  by  other  participants.  An 

1044

ATP  value  is  the  proportion  of  a  speaker's 
allotopical  mentions, 
excluding 
“self-citations”,  to  all  allotopical  mentions 
in a discourse.  

i.e., 

As an example, we may consider the follow-
ing situation in dialogue-1: 
1. Dialogue-1  contains  796 

third  person 
nouns and pronouns, excluding mentions of 
participants’ names. 

2. Speaker  JR  uses  180  nouns  and  pronouns 

for an NPI of 22.6%.  

3. The  median  NPI  is  14.3%;  JR  are  in  the 

upper quintile of participants (> 19.9%). 

From  the  above  evidence  we  can  draw  the 
following claim: 

InvolvementNPI(JR, 5, dialogue-1) 

a  prototype 

This  may  be  read  as:  speaker JR is the most 
involved participant in dialogue-1. 
As with other language uses, multiple indices 
for  Involvement  can  be  combined  into  a 
2-dimensional  map  capturing  the  group  in-
ternal dynamics.  
5.  Implementation & Evaluation 
automated 
We  developed 
DSARMD  system  that  comprises  a  series  of 
modules  that  create  automated  annotation  of 
the  source  dialogue  for  all  the  language  ele-
ments  discussed  above,  including  communi-
cative links, dialogue acts, local/meso topics, 
and  polarity.  Automatically  annotated  dia-
logue  is  then  used  to  generate  language  use 
degree  claims.  In  order  to  evaluate  accuracy 
of the automated process we conducted a pre-
liminary evaluation comparing the LU claims 
generated  from  automatically  annotated  data 
to the claims generated from manually coded 
dialogues.  Below  we  briefly  describe  the 
methodology and metrics used. 
Each  language  use  is  asserted  per  a  partici-
pant  in  a  discourse  (or  per  each  pair  of  par-
ticipants, e.g., for Disagreement) on a 5-point 
“strength”  scale.  This  can  be  represented  as 
an ordered sequence LUX(d1, d2, … dn), where 
LU  is  the  language  use  being  asserted,  X  is 
the index used, di is the degree of LU attrib-
uted to speaker i. This assignment is therefore 
a  5-way  classification  of  all  discourse  par-
ticipants  and  its  correctness  is  measured  by 
dividing  the  number  of  correct  assignments 
by the total number of elements to be classi-
fied,  which  gives  the  micro-averaged  preci-
sion.  The  accuracy  metric  is  computed  with 

several variants as follows: 
1. Strict  mapping:  each  complete  match  is 
counted as 1; all mismatches are counted as 
0. For example, the outputs LUX (5,4,3,2,1) 
and  LUX  (4,5,3,1,1)  produce  two  exact 
matches (for the third and the last speaker) 
for a precision of 0.4. 

2. Weighted mapping: since each degree value 
di in LUX(d1, d2, … dn) represents a quintile 
in normal distribution, we consider the po-
sition  of  the  value  within  the  quintile.  If 
two  mismatched  values  are  less  than  ½ 
quintile  apart  we  assign  a  partial  credit 
(currently 0.5). 

3. Highest – Rest: we measure accuracy with 
which the highest LU degree (but not nec-
essarily the same degree) is assigned to the 
right  speaker  vs.  any  other  score.  This  re-
sults in binary classification of scores. The 
sequences in (1) produce 0.6 match score. 
4. High – Low: An alternative binary classifi-
cation where scores 5 and 4 are considered 
High,  while  the  remaining  scores  are  con-
sidered  Low.  Under  this  metric,  the  se-
quences in (1) match with 100% precision. 
The  process  of  automatic  assignment  of  lan-
guage  uses  derived  from  automatically  proc-
essed  dialogues  was  evaluated  against  the 
control  set  of  assignments  based  on  hu-
man-annotated data. In order to obtain a reli-
able  “ground  truth”,  each  test  dialogue  was 
annotated  by  at  least  three  human  coders 
(linguistics  and  communication  graduate  stu-
dents,  trained).  Since  human  annotation  was 
done at the linguistic component level, a strict 
inter-annotator  agreement  was  not  required; 
instead,  we  were  interested  whether  in  each 
case  a  comparable  statistical  distribution  of 
the  corresponding  LU  index  was  obtained. 
Annotations that produced index distributions 
dissimilar from the majority were eliminated. 
Automated  dialogue  processing  involved  the 
following modules: 
•  Local topics detection  identifies  first  men-
tions  by  tracking  occurrences  of  noun 
phrases.  Subsequent  mentions  are  identi-
fied using fairly simple pronoun resolution 
(based  mostly  on  lexical  features),  with 
Wordnet used to identify synonyms, etc. 

•  Meso-topics are identified as longest-chain 
local  topics.  Their  polarity  is  assessed  at 
the  utterance  level  by  noting  presence  of 
positive or negative cue words and phrases. 
•  Dialogue acts are tagged based on presence 

1045

of certain cue phrases derived from a train-
ing corpus (Webb et al., 2008).  

•  Communicative links  are  mapped  by  com-
puting  inter-utterance  similarity  based  on 
n-gram overlap. 

Preliminary  evaluation  results  are  shown  in 
Tables  3-5  with  average  performance  over  3 
chat  sessions  (approx  4.5  hours)  involving 
three  groups  of  speakers  and  different  tasks 
(job  candidates,  political  issues).  Topic  Con-
trol  and  Involvement  tables  show  average 
accuracy per index. For example, the LTI in-
dex,  computed  over  automatically  extracted 
local  topics,  produces  Topic  Control  assign-
ments  with  the  average  precision  of  80% 
when compared to assignments derived from 
human-annotated  data  using  the  strict  accu-
racy  metric.  However,  automated  prediction 
of Involvement based on NPI index is far less 
reliable,  although  we  can  still  pick  the  most 
involved speaker with 67% accuracy. We omit 
the indices based on turn length (TL) and turn 
count  (TI)  because  their  values  are  trivially 
computed.  At  this  time  we  do  not  combine 
indices  into  a  single  LU  prediction.  Addi-
tional  experiments  are  needed  to  determine 
how much each of these indices contributes to 
LU prediction. 
Topic  
Control 
Strict 

SMT 

0.40 

0.80 

0.40 

LTI 

CS 

Weighted 

Highest‐Rest 

0.90 

0.90 

0.53 

0.67 

0.53 

0.67 

0.84 

1.00 

High‐Low 
Table  3:  Topic  Control  LU  assignment  performance 
averages of selected indices over a subset of data cov-
ering  three  dialogues  with  combined  duration  of  4.5 
hours with total of 19 participants (7, 5, 7 per session). 

0.90 

Involvement 

Strict 

Weighted 

Highest‐Rest 

NPI 

0.31 

0.46 

0.67 

TCI 

0.42 

0.49 

0.77 

ATP 

0.39 

0.42 

0.68 

0.58 

High‐Low 
Table  4:  Involvement  LU  assignment  performance  av-
erages for selected indices over the same subset of data 
as in Table 3. 

0.48 

0.74 

Topical  Disagreement  performance  is  shown 
in Table 5. We calculated precision and recall 
of assigning a correct degree of disagreement 

to each pair of speakers who are members of 
a  group.  Precision  and  recall  averages  are 
then computed over all meso-topics identified 
in  the  test  dataset,  which  consists  of  three 
separate  90-minute  dialogues  involving  7,  5 
and  7  speakers,  respectively.  Our  calculation 
includes  the  cases  where  different  sets  of 
meso-topics  were  identified  by  the  system 
and by the human coder. A strict mapping of 
levels  of  disagreement  between  speakers  is 
hard to compute accurately; however, finding 
the  speakers  who  disagree  the  most,  or  the 
least, is significantly more robust. 

Topical 
Disagreement 
Strict 

Weighted 

Highest‐Rest 

High‐Low 

Prec. 

Recall 

0.33 

0.54 

0.89 

0.77 

0.32 

0.54 

0.85 

0.73 

Table  5:  Topical  Disagreement  LU  assignment  per-
formance  averages  over  13  meso-topics  discussed  in 
three  dialogues  with  combined  duration  of  4.5  hours 
with total of 19 participants (7, 5, and 7 per session). 
6.  Conclusion 
In  this  paper  we  presented  a  preliminary 
design  for  modeling  certain  types  of  social 
phenomena  in  multi-party  on-line  dialogues. 
Initial,  limited-scale  evaluation  indicates  that 
the  model  can  be  effectively  automated. 
Much  work  lies  ahead,  including  large  scale 
evaluation, 
and 
resilience 
level  error. 
Current  performance  of  the  system  is  based 
on  only  preliminary  versions  of  linguistic 
modules 
polarity 
assignments,  etc.)  which  perform  at  only 
70-80%  accuracy,  so 
to  be 
improved  as  well.  Research  on  Urdu  and 
Chinese dialogues is just starting. 

to  NL  component 

these  need 

extraction, 

stability 

testing 

(topic 

index 

Acknowledgements 
This research was funded by the Office of the 
Director  of  National  Intelligence  (ODNI), 
Intelligence  Advanced  Research  Projects 
Activity  (IARPA),  through  the  U.S.  Army 
Research Lab. All statements of fact, opinion 
or  conclusions  contained  herein  are  those  of 
the  authors  and  should  not  be  construed  as 
representing  the  official  views  or  policies  of 
IARPA, the ODNI or the U.S. Government. 

1046

References 

Agar,  Michael.  1994.  Language  Shock,  Under-
standing  the  Culture  of  Conversation.  Quill, 
William Morrow, New York. 

Allen, J. M. Core. 1997. Draft of DAMSL: Dialog 
Act  Markup  in  Several  Layers.  www.cs.  roch-
ester.edu/research/cisd/resources/damsl/  

Anderson,  A.,  et  al.  1991.  The  HCRC  Map  Task 
Corpus. Language and Speech 34(4), 351--366. 
Austin, J. L. 1962. How to do Things with Words. 

Clarendon Press, Oxford. 

Bird, Steven, et al. 2009. Natural Language Proc-
essing  with  Python:  Analyzing  Text  with  the 
Natural Language Toolkit. O'Reilly Media.  

Blaylock,  Nate.  2002.  Managing  Communicative 
Intentions  in  Dialogue  Using  a  Collaborative 
Problem-Solving Model. Technical Report 774, 
University of Rochester, CS Dept. 

Broadwell, G. A et al. (2010). Social Phenomena 

and Language Use. ILS Technical report. 

Carberry,  Sandra  and  Lynn  Lambert.  1999.  A 
Process Model for Recognizing Communicative 
Acts  and  Modeling  Negotiation  Dialogue. 
Computational Linguistics, 25(1), pp. 1-53. 

Carletta,  J.  (2007).  Unleashing  the  killer  corpus: 
experiences  in  creating  the  multi-everything 
AMI Meeting Corpus. Language Resources and 
Evaluation Journal 41(2): 181-190 

Carlson,  Lauri.  1983.  Dialogue  Games:  An  Ap-

proach to Discourse Analysis. D. Reidel. 

Eric N. Forsyth and Craig H. Martell. 2007. Lexi-
cal and Discourse Analysis of Online Chat Dia-
log.  First  IEEE  International  Conference  on 
Semantic Computing (ICSC 2007), pp. 19-26. 

Field, D., et al. 2008. Automatic Induction of Dia-
logue Structure from the Companions Dialogue 
Corpus, 4th Int. Workshop on Human-Computer 
Conversation, Bellagio. 

Givon, Talmy. 1983. Topic continuity in discourse: 
A  quantitative  cross-language  study.  Amster-
dam: John Benjamins.  

Ivanovic, Edward. 2005. Dialogue Act Tagging for 
Instant  Messaging  Chat  Sessions.  In  Proceed-
ings  of  the  ACL  Student  Research  Workshop. 
79–84. Ann Arbor, Michigan. 

Ji, Gang Jeff Bilmes. 2006. Backoff Model Train-
ing using Partially Observed Data: Application 
to Dialog Act Tagging. HLT-NAACL 

Jurafsky,  Dan,  Elizabeth  Shriberg,  and  Debra  Bi-
asca. 1997. Switchboard SWBD-DAMSL Shal-
low-Discourse-Function  Annotation  Coders 
Manual. 
http://stripe.colorado.edu/~jurafsky/ 
manual.august1.html 

Jurafsky,  D.,  et  al.  1997.  Automatic  detection  of 
discourse  structure  for  speech  recognition  and 
understanding.  IEEE  Workshop  on  Speech 
Recognition and Understanding, Santa Barbara. 
Khan,  Faisal  M.,  et  al.  2002.  Mining  Chat-room 
Conversations for Social and Semantic Interac-

tions.  Computer  Science  and  Engineering,  Le-
high University. 

Kim,  Jihie.,  et  al.  2007.  An  Intelligent  Discus-
sion-Bot  for  Guiding  Student  Interactions  in 
Threaded  Discussions.  AAAI  Spring  Sympo-
sium  on  Interaction  Challenges  for  Intelligent 
Assistants 

Levin,  L.,  et  al.  (1998).  A  discourse  coding 
scheme  for  conversational  Spanish.  Interna-
tional  Conference  on  Speech  and  Language 
Processing. 

Levin,  L.,  et  al.  (2003).  Domain  specific  speech 
acts  for  spoken  language  translation.  4th  SIG-
dial Workshop on Discourse and Dialogue. 

Linell, Per. 1990. The power of dialogue dynamics. 
In  Ivana  Markov´a  and  Klaus  Foppa,  editors, 
The Dynamics of Dialogue. Harvester, 147–177. 
Poesio, Massimo and Andrei Mikheev. 1998. The 
predictive power of game structure in dialogue 
act  recognition.  International  Conference  on 
Speech and Language Processing (ICSLP-98). 

Price,  V.,  Capella,  J.  N.,  &  Nir,  L.  (2002).  Does 
disagreement  contribute  to  more  deliberative 
opinion? Political Communication, 19, 95-112. 
Sacks, H. and Schegloff, E., Jefferson, G. 1974. A 
simplest  systematic  for  the  organization  of 
turn-taking  for  conversation.  In:  Language 
50(4), 696-735.  

Samuel, K. et al. 1998. Dialogue Act Tagging with 
Transformation-Based  Learning.  36th  Annual 
Meeting of the ACL. 

Scollon,  Ron  and  Suzanne  W.  Scollon.  2001. 
Intercultural  Communication,  A  Discourse  Ap-
proach. Blackwell Publishing, Second Edition. 
Searle,  J.  R.  1969.  Speech Acts.  Cambridge  Uni-

versity Press, London-New York. 

Shaikh,  S.  et  al.  2010.  DSARMD  Annotation 

Guidelines, V. 2.5. ILS Technical Report.  

Shaikh  S.  et  al.  2010.  MPC:  A  Multi-Party  Chat 
Corpus  for  Modeling  Social  Phenomena  in 
Discourse, Proc. LREC-2010, Malta. 

Stolcke, Andreas et al. 2000. Dialogue Act Mod-
eling for Automatic Tagging and Recognition of 
Conversational Speech. Computational Linguis-
tics, 26(3). 

Stromer-Galley, J. 2007. Measuring deliberation’s 
content:  A  coding  scheme.  Journal  of  Public 
Deliberation, 3(1).  

Tianhao Wu, et al. 2002. Posting Act Tagging Us-
ing  Transformation-Based  Learning.  Founda-
tions  of  Data  Mining  and  Discovery,  IEEE  In-
ternational Conference on Data Mining 

Twitchell,  Douglas  P.,  Jay  F.  Nunamaker  Jr.,  and 
Judee  K.  Burgoon.  2004.  Using  Speech  Act 
Profiling  for  Deception  Detection.  Intelligence 
and Security Informatics, LNCS, Vol. 3073 

Webb, N., T. Liu, M. Hepple and Y. Wilks. 2008. 
Cross-Domain  Dialogue  Act  Tagging.  6th  In-
ternational Conference on Language Resources 
and Evaluation (LREC-2008), Marrakech. 

