



















































A Probabilistic Rich Type Theory for Semantic Interpretation


Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 72–79,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

A Probabilistic Rich Type Theory for Semantic Interpretation

Robin Cooper1, Simon Dobnik1, Shalom Lappin2, and Staffan Larsson1
1University of Gothenburg, 2King’s College London

{cooper,sl}@ling.gu.se, simon.dobnik@gu.se, shalom.lappin@kcl.ac.uk

Abstract
We propose a probabilistic type theory in which a

situation s is judged to be of a type T with probabil-

ity p. In addition to basic and functional types it in-

cludes, inter alia, record types and a notion of typ-

ing based on them. The type system is intensional

in that types of situations are not reduced to sets

of situations. We specify the fragment of a com-

positional semantics in which truth conditions are

replaced by probability conditions. The type sys-

tem is the interface between classifying situations

in perception and computing the semantic interpre-

tations of phrases in natural language.

1 Introduction

Classical semantic theories (Montague, 1974), as
well as dynamic (Kamp and Reyle, 1993) and un-
derspecified (Fox and Lappin, 2010) frameworks
use categorical type systems. A type T identifies
a set of possible denotations for expressions in T ,
and the system specifies combinatorial operations
for deriving the denotation of an expression from
the values of its constituents.

These theories cannot represent the gradience
of semantic properties that is pervasive in speak-
ers’ judgements concerning truth, predication, and
meaning relations. In general, predicates do not
have determinate extensions (or intensions), and
so, in many cases, speakers do not make categor-
ical judgements about the interpretation of an ex-
pression. Attributing gradience effects to perfor-
mance mechanisms offers no help, unless one can
show precisely how these mechanisms produce the
observed effects.

Moreover, there is a fair amount of evidence in-
dicating that language acquisition in general cru-
cially relies on probabilistic learning (Clark and
Lappin, 2011). It is not clear how a reasonable
account of semantic learning could be constructed
on the basis of the categorical type systems that ei-
ther classical or revised semantic theories assume.

Such systems do not appear to be efficiently learn-
able from the primary linguistic data (with weak
learning biases), nor is there much psychological
data to suggest that they provide biologically de-
termined constraints on semantic learning.

A semantic theory that assigns probability
rather than truth conditions to sentences is in a
better position to deal with both of these issues.
Gradience is intrinsic to the theory by virtue of
the fact that speakers assign values to declarative
sentences in the continuum of real numbers [0,1],
rather than Boolean values in {0,1}. In addition,
a probabilistic account of semantic learning is fa-
cilitated if the target of learning is a probabilistic
representation of meaning. Both semantic repre-
sentation and learning are instances of reasoning
under uncertainty.

Probability theorists working in AI often de-
scribe probability judgements as involving distri-
butions over worlds. In fact, they tend to limit
such judgements to a restricted set of outcomes
or events, each of which corresponds to a par-
tial world which is, effectively, a type of situa-
tion (Halpern, 2003). A classic example of the re-
duction of worlds to situation types in probability
theory is the estimation of the likelihood of heads
vs tails in a series of coin tosses. Here the world
is held constant except along the dimension of a
binary choice between a particular set of possi-
ble outcomes. A slightly more complex case is
the probability distribution for possible results of
throwing a single die, which allows for six pos-
sibilities corresponding to each of its numbered
faces. This restricted range of outcomes consti-
tutes the sample space.

We are making explicit the assumption, com-
mon to most probability theories used in AI, with
clearly defined sample spaces, that probability
is distributed over situation types (Barwise and
Perry, 1983), rather than over sets of entire worlds.
An Austinian proposition is a judgement that a

72



situation is of a particular type, and we treat it
as probabilistic. In fact, it expresses a subjec-
tive probability in that it encodes the belief of an
agent concerning the likelihood that a situation is
of that type. The core of an Austinian proposi-
tion is a type judgement of the form s : T , which
states that a situation s is of type T . On our ac-
count this judgement is expressed probabilistically
as p(s : T ) = r, where r ∈ [0,1].1

On the probabilistic type system that we pro-
pose situation types are intensional objects over
which probability distributions are specified. This
allows us to reason about the likelihood of alter-
native states of affairs without invoking possible
worlds.

Complete worlds are not tractably repre-
sentable. Assume that worlds are maximal con-
sistent sets of propositions (Carnap, 1947). If
the logic of propositions is higher-order, then the
problem of determining membership in such a set
is not complete. If the logic is classically first-
order, then the membership problem is complete,
but undecidable.

Alternatively, we could limit ourselves to
propositional logic, and try to generate a maxi-
mally consistent set of propositions from a single
finite proposition P in Conjunctive Normal Form
(CNF, a conjunction of disjunctions), by simply
adding conjuncts to P . But it is not clear what
(finite) set of rules or procedures we could use to
decide which propositions to add in order to gen-
erate a full description of a world in a systematic
way. Nor is it obvious at what point the conjunc-
tion will constitute a complete description of the
world.

Moreover, all the propositions that P entails
must be added to it, and all the propositions with
which P is inconsistent must be excluded, in or-
der to obtain the maximal consistent set of propo-
sitions that describe a world. But then testing the
satisfiability of P is an instance of the ksat prob-
lem, which, in the general case, is NP-complete.2

1Beltagy et al. (2013) propose an approach on which clas-
sical logic-based representations are combined with distribu-
tional lexical semantics and a probabilistic Markov logic, in
order to select among the set of possible inferences from a
sentence. Our concern here is more foundational. We seek to
replace classical semantic representations with a rich proba-
bilistic type theory as the basis of both lexical and composi-
tional interpretation.

2The ksat problem is to determine whether a formula in
propositional logic has a satisfying set of truth-value assign-
ments. For the complexity results of different types of ksat
problem see Papadimitriou (1995).

By contrast situation types can be as large or as
small as we need them to be. They are not max-
imal in the way that worlds are, and so the issue
of completeness of specification does not arise.
Therefore, they can, in principle, be tractably rep-
resented.

2 Rich Type Theory and Probability

Central to standard formulations of rich type the-
ories (for example, (Martin-Löf, 1984)) is the no-
tion of a judgement a : T , that object a is of type
T . We represent the probability of this judgement
as p(a : T ). Our system (based on Cooper (2012))
includes the following types.

Basic Types are not constructed out of other ob-
jects introduced in the theory. If T is a basic type,
p(a : T ) for any object a is provided by a probabil-
ity model, an assignment of probabilities to judge-
ments involving basic types.

PTypes are constructed from a predicate and
an appropriate sequence of arguments. An exam-
ple is the predicate ‘man’ with arity 〈Ind ,Time〉
where the types Ind and Time are the basic type
of individuals and of time points respectively.
Thus man(john,18:10) is the type of situation (or
eventuality) where John is a man at time 18:10.
A probability model provides probabilities p(e :
r(a1, . . . , an)) for ptypes r(a1, . . . , an). We take
both common nouns and verbs to provide the com-
ponents out of which PTypes are constructed.

Meets and Joins give, for T1 and T2, the meet,
T1 ∧ T2 and the join T1 ∨ T2, respectively. a :
T1 ∧ T2 just in case a : T1 and a : T2. a : T1 ∨
T2 just in case either a : T1 or a : T2 (possibly
both).3 The probabilities for meet and joint types
are defined by the classical (Kolmogorov, 1950)
equations p(a : T1 ∧ T2) = p(a : T1)p(a : T2 | a : T1)
(equivalently, p(a : T1 ∧ T2) = p(a : T1, a : T2)), and
p(a : T1 ∨ T2) = p(a : T1) + p(a : T2) − p(a : T1 ∧ T2),
respectively.

Subtypes A type T1 is a subtype of type T2,
T1 v T2, just in case a : T1 implies a : T2 no mat-
ter what we assign to the basic types. If T1 v T2
then a : T1∧T2 iff a : T1 and a : T1∨T2 iff a : T2.
Similarly, if T2 v T1 then a : T1 ∧ T2 iff a : T2
and a : T1 ∨ T2 iff a : T1.

If T2 v T1, then p(a : T1 ∧ T2) = p(a : T2),
and p(a : T1 ∨ T2) = p(a : T1). If T1 v T2,

3This use of intersection and union types is not standard in
rich type theories, where product and disjoint union are pre-
ferred following the Curry-Howard correspondence for con-
junction and disjunction.

73



then p(a : T1) ≤ p(a : T2). These definitions
also entail that p(a : T1 ∧ T2) ≤ p(a : T1), and
p(a : T1) ≤ p(a : T1 ∨ T2).

We generalize probabilistic meet and join types
to probabilities for unbounded conjunctive and
disjunctive type judgements, again using the clas-
sical equations.

Let
∧
p

(a0 : T0, . . . , an : Tn) be the conjunctive

probability of judgements a0 : T0, . . . , an : Tn.
Then

∧
p

(a0 : T0, . . . , an : Tn) =
∧
p

(a0 : T0, . . . , an−1 :

Tn−1)p(an : Tn | a0 : T0, . . . , an−1 : Tn−1). If n = 0,∧
p

(a0 : T0, . . . , an : Tn) = 1.

We interpret universal quantification as an un-
bounded conjunctive probability, which is true if
it is vacuously satisfied (n = 0) (Paris, 2010).

Let
∨p

(a0 : T0, a1 : T1, . . .) be the disjunctive

probability of judgements a0 : T0, a1 : T1, . . ..
It is computed by

∨p
(a0 : T0, . . . , an : Tn) =∨p

(a0 : T0, . . . , an−1 : Tn−1) + p(an : Tn) −
∧
p

(a0 :

T0, . . . , an−1 : Tn−1)p(an : Tn | a0 : T0, . . . , an−1 :
Tn−1). If n = 0,

∨p
(a0 : T0, . . . , an : Tn) = 0.

We take existential quantification to be an un-
bounded disjunctive probability, which is false if it
lacks a single non-nil probability instance (n = 0).

Conditional Conjunctive Probabilities are
computed by

∧
p

(a0 : T0, . . . , an : Tn | a : T ) =∧
p

(a0 : T0, . . . , an−1 : Tn−1 | a : T )p(an : Tn |

a0 : T0, . . . , an−1 : Tn−1, a : T )). If n = 0,
∧
p

(a0 :

T0, . . . , an : Tn | a : T ) = 1.
Function Types give, for any types T1 and T2,

the type (T1 → T2). This is the type of total func-
tions with domain the set of all objects of type
T1 and range included in objects of type T2. The
probability that a function f is of type (T1 → T2)
is the probability that everything in its domain is of
type T1 and that everything in its range is of type
T2, and furthermore that everything not in its do-
main which has some probability of being of type
T1 is not in fact of type T1. p(f : (T1 → T2)) =∧
a∈dom(f)p

(a : T1, f(a) : T2)(1−
∨

a6∈dom(f)

p
(a : T1))

Suppose that T1 is the type of event where there
is a flash of lightning and T2 is the type of event
where there is a clap of thunder. Suppose that f
maps lightning events to thunder events, and that

it has as its domain all events which have been
judged to have probability greater than 0 of being
lightning events. Let us consider that all the puta-
tive lightning events were clear examples of light-
ning (i.e. judged with probability 1 to be of type
T1) and are furthermore associated by f with clear
events of thunder (i.e. judged with probability 1 to
be of type T2). Suppose there were four such pairs
of events. Then the probability of f being of type
(T1 → T2) is (1× 1)4, that is, 1.

Suppose, alternatively, that for one of the four
events f associates the lightning event with a silent
event, that is, one whose probability of being of
T2 is 0. Then the probability of f being of type
(T1 → T2) is (1 × 1)3 × (1 × 0) = 0. One clear
counterexample is sufficient to show that the func-
tion is definitely not of the type.

In cases where the probabilities of the an-
tecedent and the consequent type judgements are
higher than 0, the probability of the entire judge-
ment on the existence of a functional type f will
decline in proportion to the size of dom(f). As-
sume, for example that there are k elements a ∈
dom(f), where for each such a p(a : T1) =
p(f(a) : T2) ≥ .5. Every ai that is added to
dom(f) will reduce the value of p(f : (T1 →
T2)), even if it yields higher values for p(a : T1)
and p(f(a) : T2). This is due to the fact that we
are treating the probability of p(f : (T1 → T2))
as the likelihood of there being a function that is
satisfied by all objects in its domain. The larger
the domain, the less probable that all elements in
it fulfill the functional relation.

We are, then, interpreting a functional type
judgement of this kind as a universally quantified
assertion over the pairing of objects in dom(f)
and range(f). The probability of such an asser-
tion is given by the conjunction of assertions cor-
responding to the co-occurrence of each element a
in f ’s domain as an instance of T1 with f(a) as an
instance of T2. This probability is the product of
the probabilities of these individual assertions.

This seems reasonable, but it only deals with
functions whose domain is all objects which have
been judged to have some probability, however
low, of being of type T1. Intuitively, functions
which leave out some of the objects with lower
likelihood of being of type T1 should also have a
probability of being of type (T1 → T2). This fac-
tor in the probability is represented by the second
element of the product in the formula.

74



Negation ¬T , of type T , is the function type
(T → ⊥), where ⊥ is a necessarily empty type
and p(⊥) = 0. It follows from our rules for func-
tion types that p(f : ¬T ) = 1 if dom(f) = ∅, that
is T is empty, and 0 otherwise.

We also assign probabilities to judgements con-
cerning the (non-)emptiness of a type, p(T ). we
pass over the details of how we compute the prob-
abilities of such judgements, but we note that our
account of negation entails that p(T ∨ ¬T ) = 1,
and (ii) p(¬¬T ) = p(T ). Therefore, we sustain
classical Boolean negation and disjunction, in con-
trast to Martin-Löf’s (1984) intuitionistic type the-
ory.

Dependent Types are functions from objects to
types. Given appropriate arguments as functions
they will return a type. Therefore, the account of
probabilities associated with functions above ap-
plies to dependent types.

Record Types A record in a type system asso-
ciated with a set of labels is a set of ordered pairs
(fields) whose first member is a label and whose
second member is an object of some type (possibly
a record). Records are required to be functional on
labels (each label in a record can only occur once
in the record’s left projection).

A dependent record type is a set of fields (or-
dered pairs) consisting of a label ` followed by T
as above. The set of record types is defined by:

1. [], that is the empty set or Rec, is a record type. r : Rec
just in case r is a record.

2. If T1 is a record type, ` is a label not occurring in T1,
and T2 is a type, then T1 ∪ {〈`, T2〉} is a record type.
r : T1 ∪ {〈`, T2〉} just in case r : T1, r.` is defined (`
occurs as a label in r) and r.` : T2.

3. If T is a record type, ` is a label not occuring in
T , T is a dependent type requiring n arguments, and
〈π1, . . . , πn〉 is an n-place sequence of paths in T ,4
then T ∪ {〈`, 〈T , 〈π1, . . . , πn〉〉〉} is a record type.
r : T ∪ {〈`, 〈T , 〈π1, . . . , πn〉〉〉} just in case r : T ,
r.` is defined and r.` : T (r.π1, . . . , r.πn).

The probability that an object r is of a record
type T is given by the following clauses:

1. p(r : Rec) = 1 if r is a record, 0 otherwise

2. p(r : T1 ∪ {〈`, T2〉}) =
∧
p

(r : T1, r.` : T2)

3. If T : (T1 → (. . . → (Tn → Type) . . .)), then
p(r : T ∪ {〈`, 〈T , 〈π1, . . . , πn〉〉〉}) =

∧
p

(r : T, r.` :

T (r.π1, . . . , r.πn) | r.π1 : T1, . . . , r.πn : Tn)
4In the full version of TTR we also allow absolute paths

which point to particular records, but we will not include
them here.

3 Compositional Semantics

Montague (1974) determines the denotation of a
complex expression by applying a function to an
intensional argument (as in [[ NP ]]([[ ∧VP ]])). We
employ a variant of this general strategy by ap-
plying a probabilistic evaluation function [[ · ]]p to
a categorical (non-probabilistic) semantic value.
For semantic categories that are interpreted as
functions, [[ · ]]p yields functions from categorical
values to probabilities. For sentences it produces
probability values.

The probabilistic evaluation function [[ · ]]p pro-
duces a probabilistic interpretation based on a
classical compositional semantics. For sentences
it will return the probability that the sentence is
true. For categories that are interpreted as func-
tions it will return functions from (categorical) in-
terpretations to probabilities. We are not propos-
ing strict compositionality in terms of probabili-
ties. Probabilities are like truth-values (or rather,
truth-values are the limit cases of probabilities).

We would not expect to be able to compute the
probability associated with a complex constituent
on the basis of the probabilities associated with its
immediate constituents, any more than we would
expect to be able to compute a categorical inter-
pretation entirely in terms of truth-functions and
extensions. However, the simultaneous computa-
tion of categorical and probabilistic interpretations
provides us with a compositional semantic system
that is closely related to the simultaneous com-
putation of intensions and extensions in classical
Montague semantics.

The following definition of [[ · ]]p for a fragment
of English is specified on the basis of our proba-
bilistic type system and a non-probabilistic inter-
pretation function [[ · ]], which we do not give in
this version of the paper. (It’s definition is given
by removing the probability p from the definition
below.)

[[ [S S1 and S2] ]]p = p(
[

e1:[[ S1 ]]
e2:[[ S2 ]]

]
)

[[ [S S1 or S2] ]]p = p(
[
e:[[ S1 ]]∨[[ S2 ]]

]
)

[[ [S Neg S] ]]p = [[ Neg ]]p([[ S ]])
[[ [S NP VP] ]]p = [[ NP ]]p([[ VP ]])
[[ [NP Det N] ]]p = [[ Det ]]p([[ N ]])
[[ [NP Nprop] ]]p = [[ Nprop ]]p
[[ [VP Vt NP] ]]p = [[ Vt ]]p([[ NP ]])
[[ [VP Vi] ]]p = [[ Vi ]]p

[[ [Neg “it’s not true that”] ]]p = λT :RecType(p(
[
e:¬T ]))

[[ [Det “some”] ]]p = λQ:Ppty(λP :Ppty(p(
[
e:some(Q, P )

]
)))

[[ [Det “every”] ]]p = λQ:Ppty(λP :Ppty(p(
[
e:every(Q, P )

]
)))

[[ [Det “most”] ]]p = λQ:Ppty(λP :Ppty(p(
[
e:most(Q, P )

]
)))

75



[[ [N “boy”] ]]p = λr:
[
x:Ind

]
(p(
[
e:boy(r.x)

]
))

[[ [N “girl”] ]]p = λr:
[
x:Ind

]
(p(
[
e:girl(r.x)

]
))

[[ [Adj “green”] ]]p =

λP :Ppty(λr:
[
x:Ind

]
(p((
[
e:green(r.x,P )

]
)))))

[[ [Adj “imaginary”] ]]p =

λP :Ppty(λr:
[
x:Ind

]
(p((
[
e:imaginary(r.x,P )

]
)))))5

[[ [Nprop “Kim”] ]]p = λP :Ppty(p(P (
[
x=kim

]
)))

[[ [Nprop “Sandy”] ]]p = λP :Ppty(p(P (
[
x=sandy

]
)))

[[ [Vt “knows”] ]]p =

λP:Quant(λr1:
[
x:Ind

]
(p(P(λr2:(

[
e:know(r1.x,r2.x)

]
)))))

[[ [Vt “sees”] ]]p =

λP:Quant(λr1:
[
x:Ind

]
(p(P(λr2:(

[
e:see(r1.x,r2.x)

]
)))))

[[ [Vi “smiles”] ]]p = λr:
[
x:Ind

]
(p(
[
e:smile(r.x)

]
))

[[ [Vi “laughs”] ]]p = λr:
[
x:Ind

]
(p(
[
e:laugh(r.x)

]
))

A probability distribution d for this fragment,
based on a set of situations S, is such that:
pd(a : Ind) = 1 if a is kim or sandy6

pd(s : T ) ∈ [0, 1] if s ∈ S and T is a ptype
pd(s : T ) = 0 if s 6∈ S and T is a ptype7
pd(a : [

τP ]) = pd(P (
[
x=a
]
))

pd(some(P,Q)) = pd([τP ] ∧ [τQ])
pd(every(P,Q)) = pd([τP ]→ [τQ])
pd(most(P,Q)) = min(1, pd([

τP ]∧[τQ]
θmost pd([

τP ])
)

The probability that an event e is of the type in
which the relation some holds of the properties P
andQ is the probability that e is of the conjunctive
type P ∧Q. The probability that e is of the every
type for P and Q is the likelihood that it instanti-
ates the functional type P → Q. As we have de-
fined the probabilities associated with functional
types in terms of universal quantification (an un-
bounded conjunction of the pairings between the
elements of the domain P of the function and its
range Q), this definition sustains the desired read-
ing of every. The likelihood that e is of the type
most for P and Q is the likelihood that e is of
type P ∧Q, factored by the product of the contex-
tually determined parameter θmost and the likeli-
hood that e is of type P , where this fraction is less
than 1, and 1 otherwise.

Consider a simple example.
[[ [S [NP [Nprop Kim]] [VP [Vi smiles]]] ]]p =

λP :Ppty(p(P (
[
x=kim

]
)))(λr:

[
x:Ind

]
(
[
e:smile(r.x)

]
)) =

p(λr:
[
x:Ind

]
(
[
e:smile(r.x)

]
)(
[
x=kim

]
)) =

p(
[
e:smile(kim)

]
)

5Notice that we characterize adjectival modifiers as rela-
tions between records of individuals and properties. We can
then invoke subtyping to capture the distinction between in-
tersective and non-intersective modifier relations.

6This seems an intuitive assumption, though not a neces-
sary one.

7Again this seems an intuitive, though not a necessary as-
sumption.

Suppose that pd(s1:smile(kim)) = .7,
pd(s2:smile(kim)) = .3, pd(s3:smile(kim)) =
.4, and there are no other situations si such
that pd(si:smile(kim)) > 0. Furthermore, let
us assume that these probabilities are indepen-
dent of each other, that is, pd(s3:smile(kim)) =
pd(s3:smile(kim) | s1:smile(kim), s2:smile(kim))
and so on. Then
pd(smile(kim))=∨p
d(s1 : smile(kim), s2 : smile(kim), s3 : smile(kim)) =∨p
d(s1 : smile(kim), s2 : smile(kim)) + .4 − .4

∨p
d(s1 :

smile(kim), s2 : smile(kim)) =

(.7 + .3− .7× .3) + .4− .4(.7 + .3− .7× .3) =
.874

This means that pd(
[
e:smile(kim)

]
) = .874.

Hence [[ [S [NP [Nprop Kim]] [VP [Vi smiles]]] ]]pd = .874
(where [[ α ]]pd is the result of computing [[ α ]]p
with respect to the probability distribution d).

Just as for categorical semantics, we can con-
struct type theoretic objects corresponding to
probabilistic judgements. We call these proba-
bilistic Austinian propositions. These are records
of type sit : Sitsit-type : Type

prob : [0,1]


where [0,1] is used to represent the type of real
numbers between 0 and 1. They assert that the
probability that a situation s is of type Type is the
value of prob.

The definition of [[ · ]]p specifies a compositional
procedure for generating an Austinian proposition
(record) of this type from the meanings of the syn-
tactic constituents of a sentence.

4 An Outline of Semantic Learning

We outline a schematic theory of semantic learn-
ing on which agents acquire classifiers that form
the basis for our probabilistic type system. For
simplicity and ease of presentation we take these
to be Naive Bayes classifiers, which an agent ac-
quires from observation. In future developments
of this theory we will seek to extend the approach
to Bayesian networks (Pearl, 1990).

We assume that agents keep records of observed
situations and their types, modelled as probabilis-
tic Austinian propositions. For example, an obser-
vation of a man running might yield the following
Austinian proposition for some a:Ind, s1:man(a),
s2:run(a):

76




sit =

 ref = acman = s1
crun = s2


sit-type =

 ref : Indcman : man(ref)
crun : run(ref)


prob = 0.7


An agent, A, makes judgements based on a

finite string of probabilistic Austinian proposi-
tions, J, corresponding to prior judgements held
in memory. For a type, T , JT represents that set of
Austinian propositions j such that j.sit-type v T .
If T is a type and J a finite string of probabilis-
tic Austinian propositions, then || T ||J represents
the sum of all probabilities associated with T in J
(
∑
j∈JT j.prob). P(J) is the sum of all probabilities

in J (
∑
j∈J j.prob).

We use priorJ(T ) to represent the prior proba-
bility that anything is of type T given J, that is
||T ||J
P(J) if P(J) > 0, and 0 otherwise.
pA,J(s : T ) denotes the probability that agent A

assigns with respect to prior judgements J to s be-
ing of type T . Similarly, pA,J(s : T1 | s : T2) is
the probability that agent A assigns with respect
to prior judgements J to s being of type T1, given
that A judges s to be of type T2.

When an agent A encounters a new situation
s and considers whether it is of type T , he/she
uses probabilistic reasoning to determine the value
of pA,J(s : T ). A uses conditional probabilities
to calculate this value, where A computes these
conditional probabilities with the equation pA,J(s :
T1 | s : T2) = ||T1∧T2||J||T2||J , if || T2 ||J 6= 0. Otherwise,
pA,J(s : T1 | s : T2) = 0.

This is our type theoretic variant of the stan-
dard Bayesian formula for conditional probabili-
ties: p(A | B) = |A&B||B| . But instead of counting
categorical instances, we sum the probabilities of
judgements. This is because our “training data” is
not limited to categorical observations. Instead it
consists of probabilistic observational judgements
that situations are of particular types.8

Assume that we have the following types:
Tman =

[
ref : Ind
cman : man(ref)

]
and

Trun =
[

ref : Ind
crun : run(ref)

]
8As a reviewer observes, by using an observer’s previous

judgements for the probability of an event being of a partic-
ular type, as the prior for the rule that computes the proba-
bility of a new event being of that type, we have, in effect,
compressed information that properly belongs in a Bayesian
network into our specification of a naive Bayesian classifier.
This is a simplification that we adopt here for ease of expo-
sition. In future work, we will characterise classifier learning
through full Bayesian networks.

Assume also that JTman∧Trun has three members,
corresponding to judgements by A that a man was
running in three observed situations s1, s3, and
s4, and that these Austinian propositions have the
probabilities 0.6, 0.6. and 0.5 respectively.

Take JTman to have five members correspond-
ing to judgements by A that there was a man in
s1, . . . , s5, and that the Austinian propositions as-
signing Tman to s1, . . . , s5 all have probability 0.7.
Given these assumptions, the conditional probabil-
ity that A will assign on the basis of J to someone
runs, given that he is a man is pA,J(r : Trun | r :
Tman) =

||Tman∧Trun||J
||Tman||J =

0.6+0.6+0.5
0.7+0.7+0.7+0.7+0.7

= .486

We use conditional probabilities to construct a
Naive Bayes classifier. A classifies a new situa-
tion s based on the prior judgements J, and what-
ever evidence A can acquire about s. This evi-
dence has the form pA,J(s : Te1 ), . . ., pA,J(s : Ten ),
where Te1 , . . . , Ten are the evidence types. The
Naive Bayes classifier assumes that the evidence is
independent, in that the probability of each piece
of evidence is independent of every other piece of
evidence.

We first formulate Bayes’ rule of conditional
probability. This rule defines the conditional prob-
ability of a conclusion r : Tc, given evidence r :
Te1 , r : Te2 , . . . , r : Ten , in terms of conditional prob-
abilities of the form p(si : Tei | si : Tc), 1 ≤ i ≤ n,
and priors for conclusion and evidence:
pA,J(r : Tc | r : Te1 , . . . , r : Ten) =

priorJ(Tc)
||Te1∧Tc||J
||Tc||J

...
||Ten∧Tc||J
||Tc||J

priorJ(Te1 )...priorJ(Ten )

The conditional probabilities are computed
from observations as indicated above. The rule of
conditional probability allows the combination of
several pieces of evidence, without requiring pre-
vious observation of a situation involving all the
evidence types.

We formulate a Naive Bayes classifier as a func-
tion from evidence types Te1 , Te2 , . . . , Ten (i.e. from
a record of type Te1 ∧ Te2 ∧ . . . ∧ Ten) to conclusion
types Tc1 , Tc2 , . . . , Tcm . The conclusion is a disjunc-
tion of one or more T ∈ {Tc1 , Tc2 , . . . , Tcm}, where
m ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier. This function
is specified as follows.
κ : (Te1 ∧ . . .∧Ten)→ (Tc1 ∨ . . .∨Tcm) such that κ(r) =
(
∨

argmax
T∈〈Tc1 ,...,Tcm 〉

pA,J(r : T | r : Te1 , . . . , r : Ten)

The classifier returns the type T which max-
imises the conditional probability of r : T given

77



the evidence provided by r. The argmax operator
here takes a sequence of arguments and a func-
tion and yields a sequence containing the argu-
ments which maximise the function (if there are
more than one).

The classifier will output a disjunction in case
both possibilities have the same probability. The∨

operator takes a sequence and returns the dis-
junction of all elements of the sequence.

In addition to computing the conclusion which
receives the highest probability given the evi-
dence, we also want the posterior probability of
the judgement above, i.e. the probability of the
judgement in light of the evidence. We obtain the
non-normalised probabilities (pnnA,J) of the different
possible conclusions by factoring in the probabili-
ties of the evidence:
pnnA,J(r : κ(r)) =∑
T∈∨−1 κ(r) pA,J(r : T | r : Te1 , . . . , r : Ten)pA,J(r :

Te1) . . . pA,J(r : Ten)

where
∨−1 is the inverse of ∨, i.e. a function that

takes a disjunction and returns the set of disjuncts.
We then take the probability of r : κ(r) and

normalise over the sum of the probabilities of
all the possible conclusions. This gives us the
normalised probability of the judgement resulting
from classification p(r : κ(r)) = p

nn
A,J(r:κ(r))∑

1≤i≤m pnnA,J(r:Tci )
.

However, since the probabilities of the evidence
are identical for all possible conclusions, we can
ignore them and instead compute the normalised
probability with the following equation (where m
ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier, as above).

pA,J(r : κ(r)) =

∑
T∈∨−1 κ(r) pA,J(r:T |r:Te1 ,...,r:Ten )∑
1≤i≤m pA,J(r:Tci |r:Te1 ,...,r:Ten )

The result of classification can be represented as
an Austinian proposition sit = ssit-type = κ(s)

prob = pA,J(s : κ(s))


which A adds to J as a result of observing and
classifying s, and is thus made available for sub-
sequent probabilistic reasoning.

5 Conclusions and Future Work

We have presented a probabilistic version of a rich
type theory with records, relying heavily on classi-
cal equations for types formed with meet, join, and

negation. This has permitted us to sustain classi-
cal equivalences and Boolean negation for com-
plex types within an intensional type theory. We
have replaced the truth of a type judgement with
the probability of it being the case, and we have
applied this approach to judgements that a situa-
tion if of type T .

Our probabilistic formulation of a rich type the-
ory with records provides the basis for a compo-
sitional semantics in which functions apply to cat-
egorical semantic objects in order to return either
functions from categorical interpretations to prob-
abilistic judgements, or, for sentences, to proba-
bilistic Austinian propositions. One of the inter-
esting ways in which this framework differs from
classical model theoretic semantics is that the ba-
sic types and type judgements at the foundation of
the type system correspond to perceptual judge-
ments concerning objects and events in the world,
rather than to entities in a model and set theoretic
constructions defined on them.

We have offered a schematic view of semantic
learning. On this account observations of situa-
tions in the world support the acquisition of naive
Bayesian classifiers from which the basic proba-
bilistic types of our type theoretical semantics are
extracted. Our type theory is, then, the interface
between observation-based learning of classifiers
for objects and the situations in which they figure
on one hand, and the computation of complex se-
mantic values for the expressions of a natural lan-
guage from these simple probabilistic types and
type judgements on the other. Therefore our gen-
eral model of interpretation achieves a highly in-
tegrated bottom-up treatment of linguistic mean-
ing and perceptually-based cognition that situates
meaning in learning how to make observational
judgements concerning the likelihood of situations
obtaining in the world.

The types of our semantic theory are inten-
sional. They constitute ways of classifying situ-
ations, and they cannot be reduced to set of situa-
tions. The theory achieves fine-grained intension-
ality through a rich and articulated type system,
where the foundation of this system is anchored in
perceptual observation.

The meanings of expressions are acquired on
the basis of speakers’ experience in the applica-
tion of classifiers to objects and events that they
encounter. Meanings are dynamic and updated in
light of subsequent experience.

78



Probability is distributed over alternative situ-
ation types. Possible worlds, construed as maxi-
mal consistent sets of propositions (ultrafilters in a
proof theoretic lattice of propositions) play no role
in this framework.

Bayesian reasoning from observation provides
the incremental basis for learning and refining
predicative types. These types feed the combina-
torial semantic procedures for interpreting the sen-
tences of a natural language.

In future work we will explore implementations
of our learning theory in order to study the viabil-
ity of our probabilistic type theory as an interface
between perceptual judgement and compositional
semantics. We hope to show that, in addition to
its cognitive and theoretical interest, our proposed
framework will yield results in robotic language
learning, and dialogue modelling.

Acknowledgments

We are grateful to two anonymous reviewers for
very helpful comments on an earlier draft of this
paper. We also thank Alex Clark, Jekaterina
Denissova, Raquel Fernández, Jonathan Ginzburg,
Noah Goodman, Dan Lassiter, Michiel van Lam-
balgen, Poppy Mankowitz, Aarne Ranta, and Pe-
ter Sutton for useful discussion of ideas presented
in this paper. Shalom Lappin’s participation in
the research reported here was funded by grant
ES/J022969/1 from the Economic and Social Re-
search Council of the UK, and a grant from the
Wenner-Gren Foundations. We also gratefully ac-
knowledge the support of Vetenskapsrådet, project
2009-1569, Semantic analysis of interaction and
coordination in dialogue (SAICD); the Depart-
ment of Philosophy, Linguistics, and Theory of
Science; and the Centre for Language Technology
at the University of Gothenburg.

References
Jon Barwise and John Perry. 1983. Situations and

Attitudes. Bradford Books. MIT Press, Cambridge,
Mass.

I. Beltagy, C. Chau, G. Boleda, D. Garrette, K. Erk,
and R. Mooney. 2013. Montague meets markov:
Deep semantics with probabilistic logical form. In
Second Joint Conference on Lexical and Computa-
tional Semantics, Vol. 1, pages 11–21. Association
of Computational Linguistics, Atlanta, GA.

R. Carnap. 1947. Meaning and Necessity. University
of Chicago Press, Chicago.

A. Clark and S. Lappin. 2011. Linguistic Nativism
and the Poverty of the Stimulus. Wiley-Blackwell,
Chichester, West Sussex, and Malden, MA.

Robin Cooper. 2012. Type theory and semantics in
flux. In Ruth Kempson, Nicholas Asher, and Tim
Fernando, editors, Handbook of the Philosophy of
Science, volume 14: Philosophy of Linguistics. El-
sevier BV, 271–323. General editors: Dov M. Gab-
bay, Paul Thagard and John Woods.

C. Fox and S. Lappin. 2010. Expressiveness and
complexity in underspecified semantics. Linguistic
Analysis, Festschrift for Joachim Lambek, 36:385–
417.

J. Halpern. 2003. Reasoning About Uncertainty. MIT
Press, Cambridge MA.

H. Kamp and U. Reyle. 1993. From Discourse to
Logic: Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.

A.N. Kolmogorov. 1950. Foundations of Probability.
Chelsea Publishing, New York.

Per Martin-Löf. 1984. Intuitionistic Type Theory. Bib-
liopolis, Naples.

Richard Montague. 1974. Formal Philosophy: Se-
lected Papers of Richard Montague. Yale University
Press, New Haven. ed. and with an introduction by
Richmond H. Thomason.

C. Papadimitriou. 1995. Computational Complexity.
Addison-Wesley Publishing Co., Readin, MA.

J. Paris. 2010. Pure inductive logic. Winter School in
Logic, Guangzhou, China.

J. Pearl. 1990. Bayesian decision methods. In
G. Shafer and J. Pearl, editors, Readings in Uncer-
tain Reasoning, pages 345–352. Morgan Kaufmann.

79


