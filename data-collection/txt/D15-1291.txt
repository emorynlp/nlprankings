



















































Syntactic Dependencies and Distributed Word Representations for Analogy Detection and Mining


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2441–2450,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Syntactic Dependencies and Distributed Word Representations for
Chinese Analogy Detection and Mining

Likun Qiu1,2, Yue Zhang2, Yanan Lu3
1School of Chinese Language and Literature, Ludong University, China

2Singapore University of Technology and Design, Singapore
3Computer School, Wuhan University, China

qiulikun@pku.edu.cn, yue zhang@sutd.edu.sg, luyanan@whu.edu.cn

Abstract

Distributed word representations capture
relational similarities by means of vec-
tor arithmetics, giving high accuracies on
analogy detection. We empirically inves-
tigate the use of syntactic dependencies
on improving Chinese analogy detection
based on distributed word representation-
s, showing that a dependency-based em-
beddings does not perform better than an
ngram-based embeddings, but dependen-
cy structures can be used to improve anal-
ogy detection by filtering candidates. In
addition, we show that distributed repre-
sentations of dependency structure can be
used for measuring relational similarities,
thereby help analogy mining.

1 Introduction

Relational similarity measures the correspondence
between word-word relations (Medin et al., 1990).
It is relevant to many tasks in NLP (Turney, 2006),
such as word sense disambiguation, information
extraction, question answering, information re-
trieval, semantic role identification and metaphor
detection. Typical tasks on relational similarity in-
clude analogy detection, which measures the de-
gree of relational similarities, and analogy mining,
which extracts analogous word pairs from unstruc-
tured text.

Recently, distributed word representations (i.e.
embeddings) (Mikolov et al., 2013a; Mikolov et
al., 2013b; Levy and Goldberg, 2014b) have been
used for unsupervised analogy detection. Mikolov
et al. use attributional similarities between words
in a relation to compute relational similarities, and
show that the method outperforms the best sys-

tem in the SemEval 2012 shared task on analo-
gy detection. Levy and Goldberg (2014b) fur-
ther improve Mikolov’s relational similarity mea-
sure method using novel arithmetic combination-
s of attributional similarities. For simplicity, we
call the method of Mikolov et al. embedding-
based analogy detection, without stressing the dif-
ference between distributed and distributional (i.e.
counting-based) word representations.

Most work on embedding-based analogy detec-
tion uses relational similarities as a measure of the
quality of embeddings. However, relatively little
has been done in the opposite direction, exploring
how to leverage embeddings for improving rela-
tional similarity algorithms. We empirically study
the use of word embeddings for Chinese analogy
detection and mining, leveraging syntactic depen-
dencies, which has been shown to be closely asso-
ciated with semantic relations (Levin, 1993; Chi-
u et al., 2007). Compared with many other lan-
guages, this association is particularly strong for
Chinese, which is fully configurational and lack-
s morphology. To our knowledge, relatively little
work has been reported on Chinese relational sim-
ilarities, compared to other tasks in Chinese NLP,
including syntactic parsing, information extraction
and machine translation.

We work on three specific problems. First, we
study the effect of dependency-based word em-
beddings for analogy detection. There are two
variations of Mikolov et al’s skip-gram embed-
ding model, one training the distributed word rep-
resentation of a word using its context words in
local ngram window (Mikolov et al., 2013a), and
the other training the distributed representation of
a word using words in a syntactic dependency
context (Levy and Goldberg, 2014b; Bansal et al.,
2014). The latter has attracted much recent atten-

2441



tion due to its potential in capturing more syntac-
tic regularities. It has been shown to outperfor-
m the former in a variety of NLP tasks, and can
potentially also improve relation similarity. Our
experiments on both English and Chinese show
that the dependency-context embeddings consis-
tently under-perform ngram-context embeddings.
We give some theoretical justifications to the find-
ings.

Second, we propose to use syntactic depen-
dencies as a context for improving embedding-
based analogy detection, pruning the search space
and filtering noise using syntactic dependencies.
While highly useful for measuring relational sim-
ilarities, attributional similarities between words
are not the only source of information for analo-
gy detection. Traditional methods, such as Tur-
ney and Littman (2005), Turney (2006), Chiu et
al. (2007) and Ó Séaghdha and Copestake (2009),
also leverage context between word pairs in a
corpus for better accuracies, which the current
embedding-based methods ignore. Results show
that our proposed method achieves significant im-
provements for this task.

Third, we show that a novel distributed repre-
sentation of syntactic dependencies between word
pairs can be used to mine analogous dependencies
from a large Chinese corpus. Inspired by the fact
that distributed word representations can be used
to measure word similarities, we use our distribut-
ed dependency representations to measure relation
similarities. We propose a bootstrapping algorith-
m for analogy mining using dependency embed-
dings, and experiments on a large Chinese corpus
show that the method can achieve a precision of
95.2% at a recall of 56.8%.

Our automatically-parsed corpus, trained em-
beddings and evaluation datasets are released
publicly at http://people.sutd.edu.sg/
˜yue_zhang/publication.html. To our
knowledge, we are the first to present results on
Chinese analogy detection and to release large-
scale Chinese word embeddings.

2 Background

2.1 Relational Similarity Tasks

There are three main tasks for relational similarity.
This first is relation classification, which has been
used in Task 2 of SemEval 2012 (Jurgens et al.,
2012). In this task, all four words in two word
pairs are given, and one needs to judge whether

Figure 1: Dependency tree of the sentence
“1991c (in 1991) § (,) cnê (Obama) oÚ
(President). (graduate)u (from)MÃ (Har-
vard){Æ� (Law School)”.

they belong to a same relation type. In order to
address this task, various supervised methods have
been used (Bollegala et al., 2008; Herdaǧdelen and
Baroni, 2009; Turney, 2013).

The second task is analogy detection (Mikolov
et al., 2013b), which takes three words in two
word pairs, and searches for a most suitable word
from the vocabulary to recover the hidden word.
This task has been addressed using word embed-
dings (Mikolov et al., 2013b; Levy and Goldberg,
2014b).

The third task is analogy mining (Chiu et al.,
2007), which takes one word pair belonging to a
certain semantic relation as a seed, and searches
for all the word pairs that share the same relation
with the seed. Compared with relation classifi-
cation and analogy detection, analogy mining can
be practically more useful because it requires less
given information, and provides a large quantity of
analogous word pairs automatically.

2.2 Skip-gram Word Embeddings

As a by-product of neural language models (Ben-
gio et al., 2003; Mnih and Hinton, 2007), word
embeddings are distributed vector representations
of words, trained using local contexts. They cap-
ture linguistic regularities in languages (Mikolov
et al., 2013b) and have been used in various tasks
(Collobert and Weston, 2008; Turian et al., 2010;
Socher et al., 2011).

In this paper, we apply the Skip-gram method
of Mikolov et al. (2013a) for training embed-
dings, which works by maximizing the probabil-
ity of a word given a context of multiple words.
Mikolov et al. (2013b) use an ngram window as
the context, and observe that the resulting embed-
dings are highly useful for unsupervised analogy
detection.

2442



2.3 Embedding-based Analogy Detection
Formally, the task of analogy detection is to find
a word b* given a pair of words a:b and a word
a* such that a*:b* is analogous to a:b. Mikolov
et al. (2013b) show that the task can be solved by
finding a word that maximizes:

score = sim(b∗, b− a+ a∗) (1)
where sim is a similarity measure, typically the
cosine function. Levy and Goldberg (2014b)
show that the Equation 1 is equivalent to:

score = cos(b∗, b)−cos(b∗, a)+cos(b∗, a∗) (2)
As a result, the goal of analogy detection is to find
a word b* which is similar to b and a* but differ-
ent from a. Levy and Goldberg (2014b) further
propose to substitute the addictive functions in E-
quation 2 with multiplicative functions:

score = cos(b∗, b)cos(b∗, a∗)/(cos(b∗, a) + ε)
(3)

Here ε = 0.001 is used to prevent division by zero.
Their experiments show that the use of Equation 3
can improve the state-of-the-art. Following Levy
and Goldberg (2014b), we refer to Equation 1 and
2 as 3COSADD and Equation 3 as 3COSMUL,
respectively.

2.4 Chinese Relational Similarity
There are various types of relational similarities.
Syntactically, inflections can be treated as a type
of word-word relation (Mikolov et al., 2013b).
For example, the comparative pairs “good:better”
and “rough:rougher” are analogous, and the past
tense inflections “see:saw” and “return:returned”
are analogous. However, such inflectional rela-
tions do not apply to Chinese, which is fully con-
figurational and lacks morphology. Consequent-
ly, our main focus is semantic similarities, which
include antonymy (e.g. (9 (hot):e (cold)) VS
(¯ (fast):ú (slow))), meronymy (e.g. ( (car):Ó
f (wheel)) VS (= (bear):Ý (paw))), gender (e.g.
(I< (man):å< (woman)) VS (I� (king):å
� (queen))) and function relations (e.g. (Ñ
(clothing):B (wear)) VS (lf (hat): (wear))),
etc.

Chiu et al. (2007) show that English semantic
relations are also reflected by syntactic dependen-
cies. Their finding coincides with Levin (1993),
who study English verbs. We find that this obser-
vation is even more prevalent for Chinese. In our

automatically-parsed Chinese corpus of 3.4 bil-
lion words (Section 5.1), 86.4% word pairs from
the analogy test dataset (Section 5.2) have corre-
sponding dependencies, each of which appearing
at least ten times.

The frequent correlation between semantic re-
lations and syntactic dependencies can be due
to the lack of morphology and function words
in Chinese. In fact, Chinese syntactic ambigui-
ties often need to be resolved by leveraging se-
mantic information (Xiong et al., 2005; Zhang
et al., 2014). Although not all occurrences of
semantically-related word pairs must also form a
syntactic dependency in a corpus, we show that
syntactic dependencies can effectively improve
analogy detection.

3 Dependency-context Word
Embeddings for Analogy Detection

A first use of syntactic dependencies for
embedding-based analogy detection is to use
them directly for embeddings. Recently, a depen-
dency context has been used for the skip-gram
method, for capturing more syntactic regularities.
Taking the sentence in Figure 1 for example, a
bi-gram context for the word “. (graduate)”
can be “cnê (Obama), oÚ (President), u
(from), MÃ (Harvard)”, while a dependency
context of the same word can be “1991c/ADV,
oÚ/SBV, u/CMP, {Æ�/POB u”1, where
“ADV, SBV, CMP, POB” indicate adverbial
modifier, subject, complement and prepositional
object, respectively.

It has been shown that a dependency context
leads to embeddings that better help parsing
(Bansal et al., 2014) and measuring word sim-
ilarity (Levy and Goldberg, 2014a), compared
with ngram contexts. However, little previous
work has systematically compared dependency
contexts with ngram contexts in analogy detec-
tion. We empirically study this problem (c.f Sec-
tion 6.3), finding that dependency context lead-
s to significantly worse analogy detection results
for both Chinese and English using state-of-the-art
embedding-based methods (Levy and Goldberg,
2014b). We give analysis in Section 6.4.

1The last token is a grand-child of “. (graduate)”, via
the preposition “u (at)” (Levy and Goldberg, 2014a).

2443



4 Search Space Pruning Using Syntactic
Dependencies

We study an alternative way of making use of syn-
tactic dependencies, by using them to prune the
vocabulary-sized search space of analogy detec-
tion. Given two word pairs a:b and a*:b*, where
b* is hidden and a is the head word, we search
for dependencies, taking a* as the head word. The
dependent words in the search candidates need to
share the POS tag of b. If there are several type-
s of dependencies between a and b, only the one
with highest frequency is used. We rank all result-
ing dependencies using the 3COSMUL objective,
and take the word b* in the highest-scored depen-
dencies as the answer.

For example, given the word pair (i.9
¶ (Sarajevo):Åç (Bosnia and Herzegovina)),
whose most frequency dependency is <i.9
¶ (Sarajevo), Åç (Bosnia and Herzegovina),
ATT>, and the unknown pair (Ôí (London):b*),
we acquire a list of dependencies, including <Ô
í (London), {I (USA), ATT>, <Ôí (Lon-
don), ni (Paris), COO>, <Ôí (London), \
< (Canada), ATT> and<Ôí (London),=I
(England), ATT>. Some of these dependencies,
such as <Ôí (London), ni (Paris), COO>,
are parsed as the coordinate relation (COO), and
thus pruned because the target syntactic relation is
ATT. From the resulting list, the 3COSMUL ob-
jective successfully ranks the triple <Ôí (Lon-
don),=I (England), ATT> as the top candidate.
In contrast, Levy and Goldberg’s method takes “H
 (South Africa)” as the answer, which does not
form an attributive-head phrase with “Ôí (Lon-
don)”.

5 Analogy Mining Using Dependency
Embeddings

Formally, analogy mining is the task of mining
analogous dependencies<x1, y1, r>,<x2, y2, r>
...<xn, yn, r> that share the same relation r with
a given dependency <a, b, r>. We mine analo-
gous dependencies by considering relational sim-
ilarity and attributional similarity simultaneously
using the skip-gram model for embeddings.

5.1 Dependency Embedding

Inspired by the fact that word similarities can be
measured by using distributed word representa-
tions, we hypothesize that relation similarities can

Input : dependency embedding DT, word
embedding DW, seed dependency s,
threshold α and β.

Output: set of ranked dependencies WP.
1 Function Mine (DT,DW,s,WP,α,β):
2 begin
3 DTSet =∅;
4 MScore =0;
5 SimDT =GetSimDT (DT,s);
6 for each Triple ∈ SimDT do
7 MWS =GetMWord (s);
8 HWS =GetHWord (s);
9 MWD =GetMWord (Triple);

10 HWD =GetHWord (Triple);
11 ScoreX =Sim (MWS,MWD,DW);
12 ScoreY =Sim (HWS,HWD,DW);
13 ScoreXY =ScoreX × ScoreY;
14 MScore =Max (ScoreXY,MScore);
15 TopK (ScoreXY,Triple,DTSet,α)
16 end
17 MScore =MScore × β ;
18 for each Triple, ScoreXY ∈ DTSet do
19 if ScoreXY > MScore and Triple /∈

WP then
20 AddToSet (Triple,WP);
21 s =Triple;
22 Mine (DT,DW,s,WP,α,β);
23 end
24 end
25 end
26 WP =∅;
27 Mine (DT,DW,s,WP,α,β);

Algorithm 1: Bootstrapping for analogy
mining.

be measured by distributed relation representa-
tions. Based on the observation in Section 2.4,
semantically analogous word pairs typically have
syntactic dependencies. We use the skip-gram al-
gorithm to train distributed representations of syn-
tactic dependencies, and use them for mining anal-
ogous word pairs.

With respect to the skip-gram model, words are
the most common target for embeddings (Levy
and Goldberg, 2014b; Levy and Goldberg, 2014a;
Mikolov et al., 2013a), although continuous vec-
tor representations can be trained for other struc-
tures. For example, Mikolov et al. (2013a) take
idiomatic phrases as embedding targets. Depen-
dencies, which consist of a modifier word, a head
word and a syntactic relation between them, can
also be represented by continuous embeddings us-
ing the same algorithm.

To induce dependency embeddings, we take
the union of the dependency context of both the
dependent and the head of a dependency as the
context. For instance, in the example sentence,
the context of the dependency <oÚ (Presiden-

2444



t), . (graduate), SBV> consists of four to-
kens: “1991c/ADV”, “cnê/ATT”, “u/CMP”
and “{Æ�/POB u”. The same skip-gram algo-
rithm is used to train embeddings for dependency
structures.

5.2 Analogy Mining by Bootstrapping

A bootstrapping algorithm is used to mine anal-
ogous word pairs based on dependency-context
word embeddings and dependency embeddings.
Algorithm 1 shows pseudocode of the recursive
bootstrapping algorithm.

The recursive function Mine (Algorithm 1)
contains three steps with six parameters, includ-
ing the dependency embeddings DT, word embed-
dings DW, a seed dependency s, and two thresh-
olds α and β. Step 1 (lines 3 to 5) is an initial-
ization process, where the dependency embedding
is used to return up to 100 most similar dependen-
cies for the given seed s. These dependencies are
stored in SimDT, and the candidate analogous de-
pendency set DTSet is initialized to an empty set.

In Step 2 (lines 6 to 16), an analogous score S-
coreXY is computed for each dependency Triple
in SimDT by multiplying the similarity scores be-
tween the two dependents and the two heads in
Triple and s, respectively. Triple is stored into the
set DTSet if ScoreXY is ranked top α. The top 1
score in DTSet is referred to as MScore. In Step 3
(lines 17 to 24), if the score of a dependency Triple
in DTSet is larger than β×MScore, it is used as a
new seed for mining more analogous dependen-
cies, by calling the function Mine recursively.

We take the seed dependency <� (play), g
 (piano), VOB> as an example to illustrate the
work-flow of the Mine function. In Step 1, a set of
similar dependencies (e.g., <� (play),3¦ (gui-
tar), VOB>, <� (play), (lyra), VOB>), is cal-
culated using the dependency embeddings DT and
stored in SimDT. Each dependency in SimDT is s-
cored in Step 2, and the top α scores are put into
the set DTSet. Finally, a dependency is used as
seed to mine new analogous dependencies if its s-
core is larger than a threshold (β×MScore). For
instance, the dependency <� (play),  (lyra),
VOB> is used to mine the new dependency <�
(play), �8 (zheng), VOB>, which is then used
to mine other dependencies such as <N (blow),
�¨j (cucurbit flute), VOB> and <N (blow),
id (sax), VOB>.

6 Experiments

6.1 Word Embeddings

We train three sets of word embeddings: NG5 (n-
gram context with 5 words to the left of the target
word and 5 words to the right), NG2 (2 words to
the left and right) and DEP (dependency context),
and one set of dependency embeddings DT (de-
pendency context), using the Skip-Gram model.
WORD2VEC2 is used to train NG5 and NG2, and
WORD2VECF3 is used to train DEP and DT. The
negative-sampling parameter is set to 15 in all the
training processes.

All embeddings are trained on a free Chinese
news archive4 that contains about 170 million-
s sentences and 3.4 billions words. We segment
and parse these sentences using the MVT imple-
mentation of ZPar 0.75 (Zhang and Clark, 2011),
which is trained on a large-scale annotated cor-
pus and achieves state-of-the-art analyzing accu-
racy on contemporary Chinese (Qiu et al., 2014)6.
Targets and contexts for word and dependency em-
beddings were filtered with a minimum frequency
of 100 and 10, respectively, and all the four types
of embeddings are trained with 200 dimensions.

6.2 Datasets and Evaluation Metrics

Three datasets are used for evaluating Chinese
embeddings. First, we construct a set of se-
mantic analogy questions. This set contains five
types of semantic analogy questions, including
capital-country (136 word pairs, and 18354 anal-
ogy questions), provincial capital-province (28,
756), city-province (637, 386262), family mem-
ber (male-female) (18, 306) and currency-country
(62, 3782). We collect the five types of word pairs
and then produce analogy questions automatical-
ly by concatenating two word pairs. The resulting
analogy dataset contains 400K analogy question-
s. We refer to this dataset as the Chinese Analogy
Question Set (CAQS).

2http://code.google.com/p/word2vec/
3https://bitbucket.org/yoavgo/

word2vecf
4This dataset contains news articles in 2014 from various

news websites, and can be downloaded from http://pan.
baidu.com/s/1o6wRjp4

5http://people.sutd.edu.sg/\%7Eyue_
zhang/doc/doc/multiview.html

6The system achieves 96.1% , 92.6% and 83.28% F1-
score for words segmentation, joint POS-tagging and depen-
dency parsing, respectively, on 1493 manually annotated sen-
tences.

2445



Data Metrics NG5 NG2 DEP
Cilin P@1 43.3% 45.9% 43.6%

P@5 31.1% 33.3% 32.6%
P@10 25.5% 27.5% 27.5%
P@20 20.5% 22.2% 22.7%
P@50 15.0% 16.2% 17.0%
P@100 11.5% 12.2% 12.8%

CWS Kendall’s τ 38.6% 44.1% 42.4%
Spearman’s ρ 54.5% 62.2% 60.7%

Table 1: Results on Cilin and CWS.

Because embeddings are central for analogy de-
tection, yet there is little large-scale evaluation
results on Chinese embeddings in the literature,
we perform embedding evaluation on two dataset-
s. The first one is the Chinese WordSim (CWS),
translated from the English WordSim-353 Set and
re-scored by native Chinese speakers (Jin and Wu,
2012). This dataset consists of 297 word pairs.

The second one is the Chinese thesaurus
Tongyicicilin (Cilin) (Che et al., 2010), which
groups 74,000 Chinese words into five-layer hi-
erarchies and has been used for evaluating the
accuracy of word similarity by traditional sparse
vector space models (Qiu et al., 2011; Jin et al.,
2012). The third level of Cilin, which contain-
s 1428 classes, is used to evaluate whether two
words are semantically similar.

For comparison between Chinese and English,
we also use an English analogy question dataset,
the Google dataset7 (Mikolov et al., 2013a), to e-
valuate the English word embeddings of Levy and
Goldberg (2014a)8 on analogy detection.

On both the CAQS and the Google dataset-
s, the 3COSMUL method (Levy and Goldberg,
2014b) is used to to answer analogy questions
based on given embeddings. The results on the
CWS dataset are evaluated using the two standard
metrics for the task, namely Spearman’s ρ and K-
endall’s τ rank correlation coefficients. The re-
sults on Cilin are evaluated using Precision@K:
the percentage of words from the top-K candidates
that belong to the Cilin category of the target word.
If one of the top-K candidates belongs to the same
third-level category in Cilin as the target word, the
candidate word is taken as correct.

6.3 Dependency-based and Word-based
Word Similarity and Analogy Detection

Word Similarity
7http://code.google.com/p/word2vec/

source/browse/trunk/questions-words.txt
8http://levyomer.wordpress.com/2014/

04/25/dependency-based-word-embeddings/

Relation NG5 NG2 DEP
MUL capital-country 68.8% 52.7% 9.9%

capital-province 84.0% 87.7% 50.0%
city-province 80.9% 80.3% 22.6%
family 39.7% 45.1% 41.5%
currency 10.4% 9.9% 2.5%
All 80.0% 78.8% 22.0%

IMP capital-country 87.9% 88.0% 87.6%
capital-province 84.9% 86.8% 84.9%
city-province 91.8% 92.0% 90.5%
family 45.3% 48.0% 47.1%
currency 7.9% 7.0% 25.9%
All 90.9% 91.1% 89.8%

Table 2: Results on CAQS. MUL and IMP indi-
cate 3COSMUL and our improved method, re-
spectively.

Relation NG5 NG2 DEP
capital-country 94.6% 84.5% 38.5%
capital-world 71.5% 64.7% 14.2%
city-in-state 53.2% 42.5% 13.1%
family 82.0% 81.2% 81.0%
currency 10.5% 10.7% 6.0%
All 63.7% 60.7% 38.8%

Table 3: English results on the Google set.

Table 1 shows the results of the three Chinese
embedding on Cilin and CWS, where NG2 per-
forms much better than NG5 on both datasets.
This demonstrates that one does not need to use
large window sizes in training word-based embed-
dings for capturing word similarities. The result is
similar to the finding of Shi et al. (2010), which
indicates that a window size of 2 is better than a
window size of 4 for capturing word similarity by
using distributional word representations.

DEP performs slightly worse than NG2 on
CWS and Cilin in P@1 and P@5. However, it
achieves better results on Cilin in P@10 to P@100
when more candidate similar words are evaluated.
In contrast, NG5 and NG2 mix more semantically
related words. This finding is consistent with that
of Levy and Goldberg (2014a).

Analogy Detection

Table 2 shows the results of the three Chinese
embeddings on CAQS. Unlike on Cilin and CWS,
NG5 outperforms DEP, and is also slightly better
than NG2. Similar tendency is shown in Table 3
for the three English embeddings evaluated on the
Google dataset. These results show that dependen-
cy embeddings are relatively weak for answering
analogy questions. On the other hand, the perfor-
mance also varies across different relation types.

2446



Target NG5 NG2 DEP
B (wear) á¦ (shorts), ;� (slim-fit), �

B (wear), 	@ (coat), *f
(skirt)

�B (wear), �X (wear), á¦
(shorts), (wear),;� (slim-fit)

�B (wear), �X (wear), 
(wear), UB (change cloths), 	
B (wear outside)

' 
(Guan
Yu; P)

ë�(Zhao Yun; P), 4� (Liu
Bei; P),Ã� (Zhuge Liang; P),
Ü (Zhang Fei; P), ùö (Cao
Cao; P)

ë� (Zhao Yun; P), 4� (Liu
Bei; P), Ü (Zhang Fei; P), ù
ö (Cao Cao; P),gû� (Xiahou
Yuan; P)

ë� (Zhao Yun; P), ¸& (Han
Xin; P),ùö (Cao Cao; P),4�
(Liu Bei; P),C?Û (Asura; P)

x ²
(Zhengzhou;
C)

[B (Shijiazhuang; C), â�
(Luoyang; C),ÜS (Xian; C),N
� (Xuchang; C),�� (Taiyuan)

[B (Shijiazhuang; C), ��
(Taiyuan; C),LH (Ji-nan; C),Ü
 (Hefei; C),ÜS (Xi-an; C)

Ü (Hefei; C),LH (Jinan; C),
ÉÇ (Wuhan; C), [B (Shiji-
azhuang; C),Hw (Nanning; C)

Table 4: Comparison between NG2, NG5 and DEP Embeddings. (P: personal name, C: city name)

6.4 Analysis

To analyze the difference between the three Chi-
nese embeddings methods qualitatively, we man-
ually inspect the words “B (wear)”, “' (Guan
Yu, a person name in the novel ‘nIüÂ
(Romance of the three kingdoms)’)”, and “x²
(Zhengzhou, a city)”. Their most similar words
are shown in Table 4.
Word Similarity

For the word “B (wear)”, both NG5 and NG2
yield similar words such as “�B (wear)”, “�
X (wear)”, “ (wear)” and related words such as
“á¦ (shorts)”, “;� (slim-fit)”, “	@ (coat)”,
“*f (skirt)”, although NG5 gives more related
words. In contrast, DEP gives only words that are
similar both syntactically and semantically. This
observation holds for other verbs and nouns, and
can be explained by the context extraction method-
s. For instance, the word “B (wear)” usually takes
one of the words “á¦ (shorts)”, “	@ (coat)”,
“*f (skirt)” as its object, and thus shares sim-
ilar contexts with them in NG5 and NG2. The
context extraction method in DEP, on the oth-
er hand, yields different context across syntactic
roles, such as verbs (e.g. “B (wear)”) and their
objects (e.g. “á¦ (shorts)” and “	@ (coat)”).

Observations on the person name “' (Guan
Yu)” and location “x² (Zhengzhou)” are simi-
lar. For “' (Guan Yu)”, NG5 and NG2 can
yield more person names in the same novel, while
DEP yields person names from other novels (i.e.
“¸& (Hanxin)” and “C?Û (Asura)”). For “x
² (Zhengzhou)”, the provincial capital of “àH
(Henan)”, NG5 and NG2 give more cities in the
same province “àH (Henan)”, while DEP yields
capitals of other provinces.
Analogy Detection

As mentioned in Section 2.3, both 3COSADD
and 3COSMUL seek a word b∗ that is similar to
b and a∗ but dissimilar to a. Ideally, the two word

pairs b:b∗ and a:a∗ should be semantically similar
while the two word pairs a:b and a∗:b∗ should be
semantically related. Therefore, 3COSADD and
3COSMUL require the embeddings to give high-
er cosine scores for both semantically similar and
related words.

Our analysis above shows that word-context
embeddings tend to mix semantically related and
similar words, but dependency-context embed-
dings only capture semantic similarity. This partly
explains the reason that dependency-context word
embeddings are weak for analogy detection.

It has also been shown in Section 6.3 that the
performances of analogy detection vary across dif-
ferent types of relations, which indicates that there
are more sophisticated underlying factors. One in-
tuitive explanation is that different semantic rela-
tions correspond to different syntactic dependency
structures. For example, the male-female family
member relation is expected to stand less frequent-
ly in a syntactic dependency relation, compared
with geographic relations such as city-country,
which stand frequently in attributional syntactic
relations (e.g. “London, England”). As a result,
where the coupling between syntactic and seman-
tic relations is weak, our analysis in Section 6.3
and other work based on syntactic relations can
find limitations.

6.5 Syntactic Dependencies for Improved
Analogy Detection

The results on CAQS using the method in Section
4 are shown in the IMP rows of Table 2. The
method achieves significant improvements (from
80.0% to 90.9% using NG5) compared with Levy
and Goldberg’s method. In addition, DEP al-
so performs significantly better than with MUL,
with an increase from 22.0% to 89.8%. The main
reason for this improvement is that the filtering
process using syntactic dependencies successfully
prunes noisy words.

2447



Seed Count Prec
¯ (eat),°J (apple), VOB 572 84.70%
� (play),g (piano), VOB 142 40.49%
B (wear),Ñ (clothing), VOB 452 67.37%
� (write),�` (novel), VOB 441 53.40%
¥I (China),�® (Beijing), ATT 2224 95.23%
�� (Hubei),ÉÇ (Wuhan), ATT 3201 96.34%

Table 5: Main results of Analogy Mining.

Error analysis shows that the main errors by the
improved method are quite different from those
by the baseline. For instance, the main errors of
Levy and Goldberg’s method for the city-province
relation are caused by giving another province as
the answer, while the improved method gives the
name of the country as answer. This is because ir-
relevant provinces do not co-occur frequently with
the city in syntactic dependencies, and hence can
be filtered by our method. On the other hand, both
the country name and province name co-occur fre-
quently with the city name in syntactic dependen-
cies, and our method cannot make a choice be-
tween them.

6.6 Dependency Structure Embeddings for
Analogy Mining

Shown in Table 5, we use six seeds to mine anal-
ogous dependencies. The first seed is used for de-
velopment and the others for test. The first three
seeds, the fourth seed and the last two seeds be-
long to the Use:Thing, Produce:Thing and Sub-
Location:Location relations, respectively. α and
β are set to 20, and 0.6, respectively. Each set of
mined dependencies together with the seed depen-
dency and relation type is shown to two human
evaluators, who are required to give a Yes/No an-
swer to each dependency in the set. We take the
average scores of the two evaluators (the average
inter-annotator agreement is 0.95) as the final pre-
cision scores.

As shown in the table, the precisions using dif-
ferent seeds are quite different, ranging from 40%
to 96%. One possible reason is that different rela-
tions have different numbers of analogous depen-
dencies, ranging from dozens to thousands, and
thus the fixed thresholds tuned on a development
seed does not apply as effectively to all test cas-
es. For instance, “� (play)” and its analogous ac-
tions, “N (blow)” and “. (play)”, are all human
actions on musical instruments, while the action-
s “¯ (eat)” and “� (write)” can apply to many
patients. For the seed <� (play), g (piano),
VOB>, irrelevant results such as << (use), }

f (scissors), VOB> and << (use), >Ù (flash-
light), VOB>, have the verb “< (use)”, which
is also a human action, yet cannot be considered
as usage of the patients “}f (scissors)” and “>
Ù (flashlight)”. Because of the stricter selectional
preference of “� (play)”, its precision of analogy
mining is lower.

We tentatively measure the recall of the algo-
rithm by taking the first three types of word pairs
in CAQS as the gold set, which contains 801 word
pairs. All the three types of word pairs belong to
the relation Sub-Location:Location. The recall is
computed as the percentage of the gold word pairs
covered by the mined dependencies. When us-
ing the two seeds <ÉÇ (Wuhan),�� (Hubei),
ATT> and <�® (Beijing),¥I (China), ATT>
for analogy mining, the recalls are 50.2% and
11.3%, respectively. Their union recall is 56.8%.
When the precision of each seed is similar, we can
achieve better recall without precision loss by us-
ing more seeds.

7 Related Work

Turney (2006) introduces a latent relational anal-
ysis (LRA) model to measure relational similari-
ty, and apply a novel co-occurrence-based method
for analogy filtering. The model can be used
for both analogy detection and relation classifi-
cation, yet cannot scale up well to large dataset-
s due to the complexity of Singular Value De-
composition. Recently, distributed word repre-
sentations using the skip-gram model (Mikolov
et al., 2013a) has been shown to give competi-
tive results on analogy detection. Levy and Gold-
berg (2014a) extends the skip-gram method with
dependency-context embeddings. We study the ef-
fect of Levy and Goldberg’s embeddings on analo-
gy detection, and further extend their embeddings
to dependency-context dependency structure em-
beddings for analogy mining.

Chiu et al. (2007) presents a similarity graph
tranversal (SGT) method to mine analogous re-
lations from raw English text automatically, us-
ing syntactic dependencies to find candidate rela-
tions. The method is unsupervised, and can scale
up well to large data sets. However, Chiu et al.
(2007) mainly focuses on relations between sub-
jects and objects because of its word-pair extrac-
tion method. Ó Séaghdha and Copestake (2009)
is a supervised method, which combines lexical
similarity and relational similarity to classify se-

2448



mantic relations. These methods are based on dis-
tributional word representation models and fit for
classifying noun-noun word pairs. In contrast, our
methods are based on distributed word representa-
tion models, and can mine noun-noun word pairs
as well as verb-noun word pairs. In addition, our
analogy mining method is unsupervised, while the
methods of both Turney (2006) and Ó Séaghdha
and Copestake (2009) are supervised.

8 Conclusion

We studied several Chinese relational similarity
tasks to train embeddings under the context of dis-
tributed word representations using the skip-gram
model and syntactic dependencies. For Chinese
analogy detection, we compared word-context and
dependency-context embeddings, finding that the
former results in much better accuracies. Observ-
ing that common relations in Chinese are frequent-
ly represented by syntactic dependencies, we im-
proved Chinese analogy detection using a depen-
dency context. Further, we empirically studied
Chinese analogy mining by proposing a bootstrap-
ping algorithm using a novel distributed represen-
tation of syntactic dependencies.

Acknowledgments

We thank the anonymous reviewers for their con-
structive comments, and gratefully acknowledge
the support of the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301,
the National Natural Science Foundation of Chi-
na (No. 61572245, 61170144, 61103089), Ma-
jor National Social Science Fund of China (No.
12&ZD227), Scientific Research Foundation of
Shandong Province Outstanding Young Scientist
Award (No. BS2013DX020) and Humanities and
Social Science Projects of Ludong University (No.
WY2013003).

References
Mohit Bansal, Kevin Gimpel, and Karen Livescu.

2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of ACL, Balti-
more, Maryland, June.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.

Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2008. Www sits the sat: Measuring re-

lational similarity on the web. In ECAI, pages 333–
337.

Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Lt-
p: A chinese language technology platform. In Pro-
ceedings of COLING, pages 13–16.

Andy Chiu, Pascal Poupart, and Chrysanne DiMar-
co. 2007. Generating lexical analogies using de-
pendency relations. In Proceedings of EMNLP-
CoNLL 2007, pages 561–570, Prague, Czech Re-
public, June.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160–167. ACM.

Amaç Herdaǧdelen and Marco Baroni. 2009. Bag-
pack: A general framework to represent semantic
relations. In Proceedings of the Workshop on Ge-
ometrical Models of Natural Language Semantics,
pages 33–40.

Peng Jin and Yunfang Wu. 2012. Semeval-2012 task 4:
evaluating chinese word similarity. In Proceedings
of the Sixth International Workshop on Semantic E-
valuation, pages 374–377.

Peng Jin, John Carroll, Yunfang Wu, and Diana Mc-
Carthy. 2012. Distributional similarity for chinese:
Exploiting characters and radicals. Mathematical
Problems in Engineering, 2012.

David A Jurgens, Peter D Turney, Saif M Mohammad,
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 356–364. Association for
Computational Linguistics.

Beth Levin. 1993. English verb classes and alterna-
tions: a preliminary investigation.

Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of ACL,
pages 302–308, Baltimore, Maryland, June.

Omer Levy and Yoav Goldberg. 2014b. Linguistic reg-
ularities in sparse and explicit word representations.
In Proceedings of CONLL, pages 171–180, Ann Ar-
bor, Michigan, June.

Douglas L Medin, Robert L Goldstone, and Dedre
Gentner. 1990. Similarity involving attributes and
relations: Judgments of similarity and difference are
not inverses. Psychological Science, 1(1):64–69.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751. Citeseer.

2449



Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML, pages 641–648. ACM.

Diarmuid Ó Séaghdha and Ann Copestake. 2009. Us-
ing lexical and relational similarity to classify se-
mantic relations. In Proceedings of EACL 2009,
pages 621–629, Athens, Greece, March.

Likun Qiu, Yunfang Wu, and Yanqiu Shao. 2011.
Combining contextual and structural information for
supersense tagging of chinese unknown words. In
Computational Linguistics and Intelligent Text Pro-
cessing, pages 15–28. Springer.

Likun Qiu, Yue Zhang, Peng Jin, and Houfeng Wang.
2014. Multi-view chinese treebanking. In Proceed-
ings of COLING, pages 257–268, Dublin, Ireland,
August.

Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class
mining: Distributional vs. pattern-based approach-
es. In Proceedings of COLING, pages 993–1001,
Beijing, China, August.

Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of ICML, pages 129–136.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of A-
CL, pages 384–394.

Peter D Turney and Michael L Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251–278.

Peter D Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.

Peter D Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. arXiv preprint arXiv:1310.5042.

Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the penn chi-
nese treebank with semantic knowledge. In Natural
Language Processing–IJCNLP 2005, pages 70–81.
Springer.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.

Meishan Zhang, Yue Zhang, Wanxiang Che, and T-
ing Liu. 2014. A semantics oriented grammar for
chinese treebanking. In Computational Linguistic-
s and Intelligent Text Processing, pages 366–378.
Springer.

2450


