



















































TANL-1: Coreference Resolution by Parse Analysis and Similarity Clustering


Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 108–111,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

TANL-1: Coreference Resolution by Parse Analysis and 

Similarity Clustering 

 

Giuseppe Attardi 

Dipartimento di Informatica 

Università di Pisa 

Largo B. Pontecorvo, 3 

attardi@di.unipi.it 

Stefano Dei Rossi 

Dipartimento di Informatica 

Università di Pisa 

Largo B. Pontecorvo, 3 

deirossi@di.unipi.it 

Maria Simi 

Dipartimento di Informatica 

Università di Pisa 

Largo B. Pontecorvo, 3 

simi@di.unipi.it 

 

  

 

Abstract 

Our submission to the Semeval 2010 task 

on coreference resolution in multiple lan-

guages is based on parse analysis and si-

milarity clustering. The system uses a bi-

nary classifier, based on Maximum En-

tropy, to decide whether or not there is a 

relationship between each pair of men-

tions extracted from a textual document. 

Mention detection is based on the analy-

sis of the dependency parse tree. 

1 Overview 

Coreference resolution can be described as the 

problem of clustering noun phrases (NP), also 

called mentions, into sets referring to the same 

discourse entity.  

The “Coreference Resolution in Multiple Lan-

guages task” at SemEval-2010 is meant to assess 

different machine learning techniques in a multi-

lingual context, and by means of different 

evaluation metrics. Two different scenarios are 

considered: a gold standard scenario (only avail-

able for Catalan and Spanish), where correct 

mention boundaries are provided to the partici-

pants, and a regular scenario, where mention 

boundaries are to be inferred from other linguis-

tic annotations provided in the input data. In par-

ticular the linguistic annotations provided for 

each token in a sentence are: position in sentence 

(ID), word (TOKEN), lemma and predicted 

lemma (LEMMA and PLEMMA), morpho-

syntactic information, both gold and/or predicted 

(POS and PPOS, FEAT and PFEAT), depend-

ency parsing annotations (HEAD and PHEAD, 

DEPREL and PDEPREL), named entities (NE 

and PNE), and semantic roles (PRED, PPRED, 

and corresponding roles in the following col-

umns). In the gold scenario, mention boundaries 

annotations (in column COREF) can also be used 

as input. 

Our approach to the task was to split corefer-

ence resolution into two sub-problems: mention 

identification and creation of entities. Mention 

recognition was based on the analysis of parse 

trees produced from input data, which were pro-

duced by manual annotation or state-of-the-art 

dependency parsers. Once the mentions are iden-

tified, coreference resolution involves partition-

ing them into subsets corresponding to the same 

entity. This problem is cast into the binary classi-

fication problem of deciding whether two given 

mentions are coreferent. A Maximum Entropy 

classifier is trained to predict how likely two 

mentions refer to the same entity. This is fol-

lowed by a greedy procedure whose purpose is to 

cluster mentions into entities.  

According to Ng (2005), most learning based 

coreference systems can be defined by four ele-

ments: the learning algorithm used to train the 

coreference classifier, the method of creating 

training instances for the learner, the feature set 

used to represent a training or test instance, and 

the clustering algorithm used to coordinate the 

coreference classification decisions. In the fol-

lowing we will detail our approach by making 

explicit the strategies used in each of above men-

tioned components. 

The data model used by our system is based 

on the concepts of entity and mention. The col-

lection of mentions referring to the same object 

in a document forms an entity. A mention is an 

instance referring to an object: it is represented 

by the start and end positions in a sentence, a 

type and a sequence number. For convenience it 

also contains a frequency count and a reference 

to the containing sentence. 

108



2 Mention detection 

The first stage of the coreference resolution 

process tries to identify the occurrence of men-

tions in documents. 

In the training phase mentions are obtained 

from the NE (or PNE) column of the corpus and 

are partitioned into entities using the information 

provided in the COREF column. 

In the regular setting, we used an algorithm for 

predicting boundaries that relies on the parse tree 

of the sentence produced from the gold annota-

tions in columns HEAD and DEP, if available, or 

else from columns PHEAD and PDEP, the out-

put of a dependency parser provided as input da-

ta.  

This analysis relied on minimal language 

knowledge, in order to determine possible heads 

of sub-trees counting as mentions, i.e. noun 

phrases or adverbial phrases referring to quanti-

ties, times and locations. POS tags and morpho-

logical features, when available, were mostly 

taken into account in determining mention heads. 

The leaves of the sub-trees of each detected head 

were collected as possible mentions. 

The mentions identified by the NE column 

were then added to this set, discarding duplicates 

or partial overlaps. Partial overlaps in principle 

should not occur, but were present occasionally 

in the data. When this occurred, we applied a 

strategy to split them into a pair of mentions.  

The same mention detection strategy was used 

also in the gold task, where we could have just 

returned the boundaries present in the data, scor-

ing 100% in accuracy. This explains the small 

loss in accuracy we achieved in mention identifi-

cation in the gold setting. 

Relying on parse trees turned out to be quite 

effective, especially for languages where gold 

parses where available. For some other languag-

es, the strategy was less effective. This was due 

to different annotation policies across different 

languages, and, in part, to inconsistencies in the 

data. For example in the Italian data set, named 

entities may include prepositions, which are typ-

ically the head of the noun phrase, while our 

strategy of looking for noun heads leaves the 

preposition out of the mention boundaries. 

Moreover this strategy obviously fails when 

mentions span across sentences as was the case, 

again, for Italian. 

3 Determining coreference 

For determining which mentions belong to the 

same entity, we applied a machine learning tech-

nique. We trained a Maximum Entropy classifier 

written in Python (Le, 2004) to determine 

whether two mentions refer to the same entity.  

We did do not make any effort to optimize the 

number of training instances for the pair-wise 

learner: a positive instance is created for each 

anaphoric NP, paired with each of its antecedents 

with the same number, and a negative instance is 

created by pairing each NP with each of its pre-

ceding non-coreferent noun phrases.  

The classifier is trained using the following 

features, extracted for each pair of mentions. 

Lexical features 

 Same: whether two mentions are equal; 

 Prefix: whether one mention is a prefix of 
the other;  

 Suffix: whether one mention is a suffix of 
the other; 

 Acronym: whether one mention is the 
acronym of the other. 

 Edit distance: quantized editing distance 
between two mentions. 

 Distance features 

 Sentence distance: quantized distance be-
tween the sentences containing the two 

mentions; 

 Token distance: quantized distance be-
tween the start tokens of the two mentions; 

 Mention distance: quantized number of 
other mentions between two mentions. 

Syntax features 

 Head: whether the heads of two mentions 
have the same POS; 

 Head POS: pairs of POS of the two men-
tions heads; 

Count features 

 Count: pairs of quantized numbers, each 
counting how many times a mention oc-

curs. 

Type features 

 Type: whether two mentions have the 
same associated NE (Named Entity) type. 

Pronoun features 

109



When the most recent mention is a pronominal 

anaphora, the following features are extracted: 

 Gender: pair of attributes {female, male or 
undetermined}; 

 Number: pair of attributes {singular, plur-
al, undetermined}; 

 Pronoun type: this feature is language de-
pendent and represents the type of prono-

minal mention, i.e. whether the pronoun is 

reflexive, possessive, relative, … 

In the submitted run we used the GIS (Genera-

lized Iterative Scaling) algorithm for parameter 

estimation, with 600 iterations, which appeared 

to provide better results than using L-BFGS (a 

limited-memory algorithm for unconstrained op-

timization). Training times ranged from one 

minute for German to 8 minutes for Italian, 

hence the slower speed of GIS was not an issue. 

3.1 Entity creation 

The mentions detected in the first phase were 

clustered, according to the output of the classifi-

er, using a greedy clustering algorithm.  

Each mention is compared to all previous 

mentions, which are collected in a global men-

tions table. If the pair-wise classifier assigns a 

probability greater than a given threshold to the 

fact that a new mention belongs to a previously 

identified entity, it is assigned to that entity. In 

case more than one entity has a probability great-

er than the threshold, the mention is assigned to 

the one with highest probability. This strategy 

has been described as best-first clustering by Ng 

(2005). 

In principle the process is not optimal since, 

once a mention is assigned to an entity, it cannot 

be later assigned to another entity to which it 

more likely refers. Luo et al. (2004) propose an 

approach based on the Bell tree to address this 

problem. Despite this potential limitation, our 

system performed quite well. 

4 Data preparation 

We used the data as supplied by the task organ-

izers for all languages except Italian. A modified 

version of the Hunpos tagger (Halácsy, Kornai & 

Oravecz, 2007; Attardi et al., 2009) was used to 

add to the Italian training and development cor-

pora more accurate POS tags than those supplied, 

as well as missing information about morphol-

ogy. The POS tagger we used, in fact is capable 

of tagging sentences with detailed POS tags, 

which include morphological information; this 

was added to column PFEATS in the data. Just 

for this reason our submission for Italian is to be 

considered an open task submission. 

The Italian training corpus appears to contain 

several errors related to mention boundaries. In 

particular there are cases of entities starting in a 

sentence and ending in the following one. This 

appears to be due to sentence splitting (for in-

stance at semicolons) performed after named ent-

ities had been tagged. As explained in section 2, 

our system was not prepared to deal with these 

situations. 

Other errors in the annotations of entities oc-

curred in the Italian test data, in particular incor-

rect balancing of openings and closings named 

entities, which caused problems to our submis-

sion. We could only complete the run after the 

deadline, so we could only report unofficial re-

sults for Italian. 

5 Results 

We submitted results to the gold and regular 

challenges for the following languages: Catalan, 

English, German and Spanish. 

Table 1 summarizes the performance of our 

system, according to the different accuracy 

scores for the gold task, Table 2 for the regular 

task. We have outlined in bold the cases where 

we achieved the best scores among the partici-

pating systems. 

 

 Mention CEAF MUC B
3 BLANC 

Catalan 98.4 64.9 26.5 76.2 54.4 

German 100 77.7 25.9 85.9 57.4 

English 89.8 67.6 24.0 73.4 52.1 

Spanish 98.4 65.8 25.7 76.8 54.1 

Table 1. Gold task, Accuracy scores. 

 

 Mention CEAF MUC B
3 BLANC 

Catalan 82.7 57.1 22.9 64.6 51.0 

German 59.2 49.5 15.4 50.7 44.7 

English 73.9 57.3 24.6 61.3 49.3 

Spanish 83.1 59.3 21.7 66.0 51.4 

Table 2. Regular task. Accuracy scores. 

6 Error analysis 

We performed some preliminary error analysis. 

The goal was to identify systematic errors and 

possible corrections for improving the perfor-

mance of our system. 

We limited our analysis to the mention boun-

daries detection for the regular tasks. A similar 

110



analysis for coreference detection, would require 

the availability of gold test data.  

7 Mention detection errors 

As described above, the strategy used for the ex-

traction of mentions boundaries is based on de-

pendency parse trees and named entities. This 

proved to be a good strategy in some languages 

such as Catalan (F1 score: 82.7) and Spanish (F1 

score: 83.1) in which the dependency data avail-

able in the corpora were very accurate and con-

sistent with the annotation of named entities. In-

stead, there have been unexpected problems in 

other languages like English or German, where 

the dependencies information were annotated 

using a different approach. 

For German, while we achieved the best B3 

accuracy on coreference analysis in the gold set-

tings, we had a quite low accuracy in mention 

detection (F1: 59.2), which was responsible of a 

significant drop in coreference accuracy for the 

regular task. This degradation in performance 

was mainly due to punctuations, which in Ger-

man are linked to the sub-tree containing the 

noun phrase rather than to the root of the sen-

tence or tokens outside the noun phrase, as it 

happens in Catalan and Spanish. This misled our 

mention detection algorithm to create many men-

tions with wrong boundaries, just because punc-

tuation marks were included. 

In the English corpus different conventions 

were apparently used for dependency parsing and 

named entity annotations (Table 3), which pro-

duced discrepancies between the boundaries of 

the named entities present in the data and those 

predicted by our algorithm. This in turn affected 

negatively the coreference detection algorithm 

that uses both types of information. 

 
ID TOKEN HEAD DEPREL NE COREF 

1 Defense 2 NAME (org) (25 

2 Secretary 4 NMOD _ _ 

3 William 4 NAME (person _ 

4 Cohen 5 SBJ person) 25) 

Table 3. Example of different conventions for NE and 

COREF in the English corpus. 

 

Error analysis also has shown that further im-

provements could be obtained, for all languages, 

by using more accurate language specific extrac-

tion rules. For example, we missed to consider a 

number of specific POS tags as possible identifi-

ers for the head of noun phrases. By some simple 

tuning of the algorithm we obtained some im-

provements. 

8 Conclusions 

We reported our experiments on coreference res-

olution in multiple languages. We applied an ap-

proach based on analyzing the parse trees in or-

der to detect mention boundaries and a Maxi-

mum Entropy classifier to cluster mentions into 

entities. 

Despite a very simplistic approach, the results 

were satisfactory and further improvements are 

possible by tuning the parameters of the algo-

rithms. 

References 

G. Attardi et al., 2009. Tanl (Text Analytics and Natu-

ral Language Processing). SemaWiki project: 

http://medialab.di.unipi.it/wiki/SemaWiki. 

P. Halácsy, A. Kornai, and C. Oravecz, 2007. Hun-

Pos: an open source trigram tagger. Proceedings of 

the ACL 2007, Prague. 

Z. Le, Maximum Entropy Modeling Toolkit for Pytho 

and C++, Reference Manual. 

X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla & S. 

Roukos. 2004. A Mention-Synchronous Corefer-

ence Resolution Algorithm Based on the Bell Tree. 

Proceedings of the ACL 2004, Barcelona. 

V. Ng, Machine Learning for Coreference Resolution: 

From Local Classification to Global Ranking, Pro-

ceedings of the 43rd Annual Meeting of the Associ-

ation for Computational Linguistics (ACL), Ann 

Arbor, MI, June 2005, pp. 157-164. 

M. Recasens, L. Màrquez,  E. Sapena, M. A. Martí,  

M. Taulé, V. Hoste, M. Poesio and Y. Versley, 

SemEval-2010 Task 1: Coreference resolution in 

multiple languages, in Proceedings of the 5th In-

ternational Workshop on Semantic Evaluations 

(SemEval-2010), Uppsala, Sweden, 2010. 

 

 

111


