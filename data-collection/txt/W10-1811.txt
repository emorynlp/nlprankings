










































Retrieving Correct Semantic Boundaries in Dependency Structure


Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 91–99,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

Retrieving Correct Semantic Boundaries in Dependency Structure

Jinho D. Choi
Department of Computer Science
University of Colorado at Boulder
choijd@colorado.edu

Martha Palmer
Department of Linguistics

University of Colorado at Boulder
martha.palmer@colorado.edu

Abstract

This paper describes the retrieval of cor-
rect semantic boundaries for predicate-
argument structures annotated by depen-
dency structure. Unlike phrase structure,
in which arguments are annotated at the
phrase level, dependency structure does
not have phrases so the argument labels are
associated with head words instead: the
subtree of each head word is assumed to
include the same set of words as the an-
notated phrase does in phrase structure.
However, at least in English, retrieving
such subtrees does not always guarantee
retrieval of the correct phrase boundaries.
In this paper, we present heuristics that
retrieve correct phrase boundaries for se-
mantic arguments, called semantic bound-
aries, from dependency trees. By apply-
ing heuristics, we achieved an F1-score
of 99.54% for correct representation of
semantic boundaries. Furthermore, error
analysis showed that some of the errors
could also be considered correct, depend-
ing on the interpretation of the annotation.

1 Introduction

Dependency structure has recently gained wide in-
terest because it is simple yet provides useful in-
formation for many NLP tasks such as sentiment
analysis (Kessler and Nicolov, 2009) or machine
translation (Gildea, 2004). Although dependency
structure is a kind of syntactic structure, it is quite
different from phrase structure: phrase structure
gives phrase information by grouping constituents
whereas dependency structure gives dependency
relations between pairs of words. Many depen-
dency relations (e.g., subject, object) have high
correlations with semantic roles (e.g., agent, pa-
tient), which makes dependency structure suit-

able for representing semantic information such as
predicate-argument structure.

In 2009, the Conference on Computational Nat-
ural Language Learning (CoNLL) opened a shared
task: the participants were supposed to take de-
pendency trees as input and produce semantic role
labels as output (Hajič et al., 2009). The depen-
dency trees were automatically converted from the
Penn Treebank (Marcus et al., 1993), which con-
sists of phrase structure trees, using some heuris-
tics (cf. Section 3). The semantic roles were ex-
tracted from the Propbank (Palmer et al., 2005).
Since Propbank arguments were originally anno-
tated at the phrase level using the Penn Treebank
and the phrase information got lost during the con-
version to the dependency trees, arguments are an-
notated on head words instead of phrases in depen-
dency trees; the subtree of each head word is as-
sumed to include the same set of words as the an-
notated phrase does in phrase structure. Figure 1
shows a dependency tree that has been converted
from the corresponding phrase structure tree.

S

NP1

DT

The

NNS

results

VP

VBP

appear

PP1

IN

in

NP

NP

NN

today

POS

’s

NN

news

The results appear in today 's newsroot
NMOD SBJ LOC NMOD

NMOD
ROOT PMOD

Figure 1: Phrase vs. dependency structure

91



In the phrase structure tree, arguments of the verb
predicate appear are annotated on the phrases:
NP1 as ARG0 and PP1 as ARGM-LOC. In the de-
pendency tree, the arguments are annotated on the
head words instead: results as the ARG0 and in as
the ARGM-LOC. In this example, both PP1 and the
subtree of in consist of the same set of words {in,
today, ’s, news} (as is the case for NP1 and the
subtree of results); therefore, the phrase bound-
aries for the semantic arguments, called semantic
boundaries, are retrieved correctly from the depen-
dency tree.

Retrieving the subtrees of head words usually
gives correct semantic boundaries; however, there
are cases where the strategy does not work. For
example, if the verb predicate is a gerund or a past-
participle, it is possible that the predicate becomes
a syntactic child of the head word annotated as a
semantic argument of the predicate. In Figure 2,
the head word plant is annotated as ARG1 of the
verb predicate owned, where owned is a child of
plant in the dependency tree. Thus, retrieving the
subtree of plant would include the predicate it-
self, which is not the correct semantic boundary
for the argument (the correct boundary would be
only {The, plant}).

The plant owned by Mark
NMOD NMOD LGS PMOD

Figure 2: Past-participle example

For such cases, we need some alternative for re-
trieving the correct semantic boundaries. This is
an important issue that has not yet been thoroughly
addressed. In this paper, we first show how to con-
vert the Penn Treebank style phrase structure to
dependency structure. We then describe how to
annotate the Propbank arguments, already anno-
tated in the phrase structure, on head words in the
dependency structure. Finally, we present heuris-
tics that correctly retrieve semantic boundaries in
most cases. For our experiments, we used the en-
tire Penn Treebank (Wall Street Journal). Our ex-
periments show that it is possible to achieve an F1-
score of 99.54% for correct representation of the
semantic boundaries.

2 Related work

Ekeklint and Nivre (2007) tried to retrieve seman-
tic boundaries by adding extra arcs to dependency
trees, so the structure is no longer a tree but a

graph. They experimented with the same cor-
pus, the Penn Treebank, but used a different de-
pendency conversion tool, Penn2Malt.1 Our work
is distinguished from theirs because we keep the
tree structure but use heuristics to find the bound-
aries. Johansson (2008) also tried to find seman-
tic boundaries for evaluation of his semantic role
labeling system using dependency structure. He
used heuristics that apply to general cases whereas
we add more detailed heuristics for specific cases.

3 Converting phrase structure to
dependency structure

We used the same tool as the one used for the
CoNLL’09 shared task to automatically convert
the phrase structure trees in the Penn Treebank
to the dependency trees (Johansson and Nugues,
2007). The script gives several options for the con-
version; we mostly used the default values except
for the following options:2

• splitSlash=false: do not split slashes. This
option is taken so the dependency trees pre-
serve the same number of word-tokens as the
original phrase structure trees.

• noSecEdges=true: ignore secondary edges
if present. This option is taken so all sib-
lings of verb predicates in phrase structure
become children of the verbs in dependency
structure regardless of empty categories. Fig-
ure 3 shows the converted dependency tree,
which is produced when the secondary edge
(*ICH*) is not ignored, and Figure 4 shows
the one produced by ignoring the secondary
edge. This option is useful because NP∗ and
PP-2∗ are annotated as separate arguments of
the verb predicate paid in Propbank (NP∗ as
ARG1 and PP-2∗ as ARGM-MNR).

S

NP-1

He

VP

VBD

was

VP

VBN

paid

NP

*-1

NP*

NP

.. salary

PP

*ICH*-2

PP-2∗

with ..

1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
2http://nlp.cs.lth.se/software/treebank converter/

92



paidHe wasroot
SBJ NMOD

ROOT NMOD

$342Ka salary with
VC NMOD

NMOD

$280Ka bonus

OBJ

NMOD

NMOD

Figure 3: When the secondary edge is not ignored

paidHe wasroot
SBJ NMOD

ROOT NMOD

$342Ka salary with
VC NMOD

NMOD

$280Ka bonus

OBJ NMOD
ADV

Figure 4: When the secondary edge is ignored

Total 49,208 dependency trees were converted
from the Penn Treebank. Although it was pos-
sible to apply different values for other options,
we found them not helpful in finding correct se-
mantic boundaries of Propbank arguments. Note
that some of non-projective dependencies are re-
moved by ignoring the secondary edges. However,
it did not make all dependency trees projective;
our methods can be applied for either projective
or non-projective dependency trees.

4 Adding semantic roles to dependency
structure

4.1 Finding the head words

For each argument in the Propbank annotated on
a phrase, we extracted the set of words belonging
to the phrase. Let this set be Sp. In Figure 1, PP1
is the ARGM-LOC of appear so Sp is {in, today,
’s, news}. Next, we found a set of head words,
say Sd, whose subtrees cover all words in Sp (e.g.,
Sd = {in} in Figure 1). It would be ideal if there
existed one head word whose subtree covers all
words in Sp, but this is not always the case. It is
possible that Sd needs more than one head word to
cover all the words in Sp.

Figure 5 shows an algorithm that finds a set of
head words Sd whose subtrees cover all words in
Sp. For each word w in Sp, the algorithm checks
if w’s subtree gives the maximum coverage (if w’s
subtree contains more words than any other sub-
tree); if it does, the algorithm adds w to Sd, re-
moves all words in w’s subtree from Sp, then re-
peats the search. The search ends when all words
in Sp are covered by some subtree of a head word
in Sd. Notice that the algorithm searches for the
minimum number of head words by matching the
maximum coverages.

Input: Sp = a set of words for each argument
in the Propbank

Output: Sd = a set of head words whose
subtrees cover all words in Sp

Algorithm:getHeadWords(Sp)1
Sd = {}2
while Sp 6= ∅ do3

max = None4
foreach w ∈ Sp do5

if |subtree(w)| > |subtree(max)|6
then

max = w7
end8
Sd.add(max)9
Sp.removeAll(subtree(max))10

end11
return Sd12

Figure 5: Finding the min-set of head words

The algorithm guarantees to find the min-set Sd
whose subtrees cover all words in Sp. This gives
100% recall for Sd compared to Sp; however, the
precision is not guaranteed to be as perfect. Sec-
tion 5 illustrates heuristics that remove the over-
generated words so we could improve the preci-
sion as well.

4.2 Ignoring empty categories

As described in Figures 3 and 4, dependency trees
do not include any empty categories (e.g., null
elements, traces, PRO’s): the empty categories
are dropped during the conversion to the depen-
dency trees. In the Penn Treebank, 11.5% of the
Propbank arguments are annotated on empty cat-
egories. Although this is a fair amount, we de-
cided to ignore them for now since dependency
structure is not naturally designed to handle empty
categories. Nonetheless, we are in the process of
finding ways of automatically adding empty cate-
gories to dependency trees so we can deal with the
remaining of 11.5% Propbank arguments.

4.3 Handling disjoint arguments

Some Propbank arguments are disjoint in the
phrase structure so that they cannot be represented
as single head words in dependency trees. For ex-
ample in Figure 6, both NP-1∗ and S∗ are ARG1 of
the verb predicate continued but there is no head
word for the dependency tree that can represent
both phrases. The algorithm in Figure 5 naturally

93



handles this kind of disjoint arguments. Although
words in Sp are not entirely consecutive ({Yields,
on, mutual, funds, to, slide}), it iteratively finds
both head words correctly: Yields and to.

S

NP-1∗

NP

Yields

PP

IN

on

NP

mutual funds

VP

VBD

continued

S∗

NP

*-1

VP

TO

to

VP

slide

Yields on mutual toroot
NMOD OPRDNMOD

PMOD

ROOT
SBJ

funds continued slide

IM

Figure 6: Disjoint argument example

5 Retrieving fine-grained semantic
boundaries

There are a total of 292,073 Propbank arguments
in the Penn Treebank, and only 88% of them map
to correct semantic boundaries from the depen-
dency trees by taking the subtrees of head words.
The errors are typically caused by including more
words than required: the recall is still 100% for the
error cases whereas the precision is not. Among
several error cases, the most critical one is caused
by verb predicates whose semantic arguments are
the parents of themselves in the dependency trees
(cf. Figure 2). In this section, we present heuris-
tics to handle such cases so we can achieve preci-
sion nearly as good as the recall.

5.1 Modals
In the current dependency structure, modals (e.g.,
will, can, do) become the heads of the main verbs.
In Figure 7, will is the head of the verb predicate
remain in the dependency tree; however, it is also
an argument (ARGM-MOD) of the verb in Prop-
bank. This can be resolved by retrieving only the
head word, but not the subtree. Thus, only will is
retrieved as the ARGM-MOD of remain.

Modals can be followed by conjuncts that are
also modals. In this case, the entire coordination
is retrieved as ARGM-MOD (e.g., {may, or, may,
not} in Figure 8).

They will remain on the list
SBJ

root
VC PRD NMOD

PRDROOT

Figure 7: Modal example 1

He may or read the bookroot
SBJ COORD ADV NMOD

OBJROOT

may not
CONJ

COORD

Figure 8: Modal example 2

5.2 Negations

Negations (e.g., not, no longer) are annotated as
ARGM-NEG in Propbank. In most cases, nega-
tions do not have any child in dependency trees,
so retrieving only the negations themselves gives
the correct semantic boundaries for ARGM-NEG,
but there are exceptions. One is where a negation
comes after a conjunction; in which case, the nega-
tion becomes the parent of the main verb. In Fig-
ure 9, not is the parent of the verb predicate copy
although it is the ARGM-NEG of the verb.

You may come but notroot
SBJ COORD

ROOT

to read
PRP

copy
VC IM CONJ COORD

Figure 9: Negation example 1

The other case is where a negation is modified by
some adverb; in which case, the adverb should
also be retrieved as well as the negation. In Fig-
ure 10, both no and longer should be retrieved as
the ARGM-NEG of the verb predicate oppose.

They no longer the legislationroot

SBJ

NMOD
OBJ

oppose
AMOD

TMP

ROOT

Figure 10: Negation example 2

5.3 Overlapping arguments

Propbank does not allow overlapping arguments.
For each predicate, if a word is included in one
argument, it cannot be included in any other argu-
ment of the predicate. In Figure 11, burdens and
in the region are annotated as ARG1 and ARGM-
LOC of the verb predicate share, respectively. The
arguments were originally annotated as two sepa-
rate phrases in the phrase structure tree; however,

94



in became the child of burdens during the conver-
sion, so the subtree of burdens includes the subtree
of in, which causes overlapping arguments.

S

NP

U.S.

VP

VBZ

encourages

S

NP

Japan

VP

TO

to

VP

VB

share

NP

NP

burdens

PP

in ..

U.S. encourages Japan inroot share
LOC

OPRD

to burdens the region

NMOD
PMOD

OBJIMOBJSBJ
ROOT

Figure 11: Overlapping argument example 1

When this happens, we reconstruct the depen-
dency tree so in becomes the child of share instead
of burdens (Figure 12). By doing so, taking the
subtrees of burdens and in no longer causes over-
lapping arguments.3

U.S. encourages Japan inroot share

OPRD

to burdens the region

NMOD
PMOD

OBJIMOBJSBJ
ROOT LOC

Figure 12: Overlapping argument example 2

5.4 Verb predicates whose semantic
arguments are their syntactic heads

There are several cases where semantic arguments
of verb predicates become the syntactic heads of
the verbs. The modals and negations in the previ-
ous sections are special cases where the seman-
tic boundaries can be retrieved correctly with-
out compromising recall. The following sec-
tions describe other cases, such as relative clauses
(Section 5.4.2), gerunds and past-participles (Sec-
tion 5.4.3), that may cause a slight decrease in re-
call by finding more fine-grained semantic bound-
aries. In these cases, the subtree of the verb predi-
cates are excluded from the semantic arguments.

3This can be considered as a Treebank/Propbank dis-
agreement, which is further discussed in Sectino 6.2.

5.4.1 Verb chains
Three kinds of verb chains exist in the current
dependency structure: auxiliary verbs (including
modals and be-verbs), infinitive markers, and con-
junctions. As discussed in Section 5.1, verb chains
become the parents of their main verbs in depen-
dency trees. This indicates that when the subtree
of the main verb is to be excluded from semantic
arguments, the verb chain needs to be excluded as
well. This usually happens when the main verbs
are used within relative clauses. In addition, more
heuristics are needed for retrieving correct seman-
tic boundaries for relative clauses, which are fur-
ther discussed in Section 5.4.2.

The following figures show examples of each
kind of verb chain. It is possible that multiple verb
chains are joined with one main verb. In this case,
we find the top-most verb chain and exclude its
entire subtree from the semantic argument. In Fig-
ure 13, part is annotated as ARG1 of the verb pred-
icate gone, chained with the auxiliary verb be, and
again chained with the modal may. Since may is
the top-most verb chain, we exclude its subtree so
only a part is retrieved as the ARG1 of gone.

a part that be

NMOD

may gone

PRDVCDEPNMOD

Figure 13: Auxiliary verb example

Figure 14 shows the case of infinitive markers.
those is annotated as ARG0 of the verb predicate
leave, which is first chained with the infinitive
marker to then chained with the verb required. By
excluding the subtree of required, only those is re-
trieved as the ARG0 of leave.

rules are toughroot those

ROOT

on required

SBJ

to

AMOD

leave
PRD PMOD APPO OPRD IM

Figure 14: Infinitive marker example

Figure 15 shows the case of conjunctions. people
is annotated as ARG0 of the verb predicate exceed,
which is first chained with or then chained with
meet. By excluding the subtree of meet, only peo-
ple is retrieved as the ARG0 of exceed.

When a verb predicate is followed by an ob-
ject complement (OPRD), the subtree of the object
complement is not excluded from the semantic ar-
gument. In Figure 16, distribution is annotated as

95



people who meet exceed

NMOD

or the

DEP NMOD
OBJ

expectation

CONJCOORD

Figure 15: Conjunction example

ARG1 of the verb predicate expected. By excluding
the subtree of expected, the object complement to
occur would be excluded as well; however, Prop-
bank annotation requires keeping the object com-
plement as the part of the argument. Thus, a dis-
tribution to occur is retrieved as the ARG1 of ex-
pected.

a distribution expected to occur
NMOD IMOPRDAPPO

Figure 16: Object complement example

5.4.2 Relative clauses
When a verb predicate is within a relative clause,
Propbank annotates both the relativizer (if present)
and its antecedent as part of the argument. For ex-
ample in Figure 15, people is annotated as ARG0
of both meet and exceed. By excluding the subtree
of meet, the relativizer who is also excluded from
the semantic argument, which is different from the
original Propbank annotation. In this case, we
keep the relativizer as part of the ARG0; thus, peo-
ple who is retrieved as the ARG0 (similarly, a part
that is retrieved as the ARG0 of gone in Figure 13).

It is possible that a relativizer is headed by a
preposition. In Figure 17, climate is annotated as
ARGM-LOC of the verb predicate made and the
relativizer which is headed by the preposition in.
In this case, both the relativizer and the preposi-
tion are included in the semantic argument. Thus,
the climate in which becomes the ARGM-LOC of
made.

the climate in decisionsthe was
PMOD

madewhich
NMOD NMOD

LOC
DEP

VC

Figure 17: Relativizer example

5.4.3 Gerunds and past-participles
In English, when gerunds and past-participles are
used without the presence of be-verbs, they often
function as noun modifiers. Propbank still treats
them as verb predicates; however, these verbs be-
come children of the nouns they modify in the de-

pendency structure, so the heuristics discussed in
Section 5.4 and 5.4.1 need to be applied to find the
correct semantic boundaries. Furthermore, since
these are special kinds of verbs, they require even
more rigorous pruning.

When a head word, annotated to be a seman-
tic argument of a verb predicate, comes after the
verb, every word prior to the verb predicate needs
to be excluded from the semantic argument. In
Figure 18, group is annotated as ARG0 of the
verb predicate publishing, so all words prior to the
predicate (the Dutch) need to be excluded. Thus,
only group is retrieved as the ARG0 of publishing.

the Dutch publishing group
NMOD

NMOD
NMOD

Figure 18: Gerund example

When the head word comes before the verb pred-
icate, the subtree of the head word, excluding the
subtree of the verb predicate, is retrieved as the se-
mantic argument. In Figure 19, correspondence is
annotated as ARG1 of the verb predicate mailed,
so the subtree of correspondence, excluding the
subtree of mailed, is retrieved to be the argument.
Thus, correspondence about incomplete 8300s be-
comes the ARG1 of mailed.

correspondence mailed about
NMOD
NMOD

incomplete 8300s
NMOD

PMOD

Figure 19: Past-participle example 1

When the subtree of the verb predicate is imme-
diately followed by comma-like punctuation (e.g.,
comma, colon, semi-colon, etc.) and the head
word comes before the predicate, every word after
the punctuation is excluded from the semantic ar-
gument. In Figure 20, fellow is annotated as ARG1
of the verb predicate named, so both the subtree
of the verb (named John) and every word after the
comma (, who stayed for years) are excluded from
the semantic argument. Thus, only a fellow is re-
trieved as the ARG1 of named.

5.5 Punctuation

For evaluation, we built a model that excludes
punctuation from semantic boundaries for two rea-
sons. First, it is often not clear how punctuation

96



a named John who stayedfellow , for years
NMOD APPO OPRD

P
DEP TMP PMOD

NMOD

Figure 20: Past-participle example 2

needs to be annotated in either Treebank or Prop-
bank; because of that, annotation for punctuation
is not entirely consistent, which makes it hard to
evaluate. Second, although punctuation gives use-
ful information for obtaining semantic boundaries,
it is not crucial for semantic roles. In fact, some
of the state-of-art semantic role labeling systems,
such as ASSERT (Pradhan et al., 2004), give an
option for omitting punctuation from the output.
For these reasons, our final model ignores punctu-
ation for semantic boundaries.

6 Evaluations

6.1 Model comparisons

The following list describes six models used for
the experiments. Model I is the baseline approach
that retrieves all words in the subtrees of head
words as semantic boundaries. Model II to VI use
the heuristics discussed in the previous sections.
Each model inherits all the heuristics from the pre-
vious model and adds new heuristics; therefore,
each model is expected to perform better than the
previous model.

• I - all words in the subtrees (baseline)

• II - modals + negations (Sections 5.1, 5.2)

• III - overlapping arguments (Section 5.3)

• IV - verb chains + relative clauses (Sec-
tions 5.4.1, 5.4.2)

• V - gerunds + past-participles (Section 5.4.3)

• VI - excluding punctuations (Section 5.5)

The following list shows measurements used for
the evaluations. gold(arg) is the gold-standard
set of words for the argument arg. sys(arg) is
the set of words for arg produced by our system.
c(arg1, arg2) returns 1 if arg1 is equal to arg2;
otherwise, returns 0. T is the total number of ar-
guments in the Propbank.

Accuracy =
1
T
·
∑
∀arg

c(gold(arg), sys(arg))

Precision =
1
T
·
∑
∀arg

|gold(arg) ∩ sys(arg)|
|sys(arg)|

Recall =
1
T
·
∑
∀arg

|gold(arg) ∩ sys(arg)|
|gold(arg)|

F1 =
2 · Precision ·Recall
Precision + Recall

Table 1 shows the results from the models us-
ing the measurements. As expected, each model
shows improvement over the previous one in
terms of accuracy and F1-score. The F1-score
of Model VI shows improvement that is statisti-
cally significant compared to Model I using t-test
(t = 149.00, p < 0.0001). The result from the
final model is encouraging because it enables us
to take full advantage of dependency structure for
semantic role labeling. Without finding the correct
semantic boundaries, even if a semantic role label-
ing system did an excellent job finding the right
head words, we would not be able to find the ac-
tual chunks for the arguments. By using our ap-
proach, finding the correct semantic boundaries is
no longer an issue for using dependency structure
for automatic semantic role labeling.

Model Accuracy Precision Recall F1
I 88.00 92.51 100 96.11
II 91.84 95.77 100 97.84
III 92.17 97.08 100 98.52
IV 95.89 98.51 99.95 99.23
V 97.00 98.94 99.95 99.44
VI 98.20 99.14 99.95 99.54

Table 1: Model comparisons (in percentage)

6.2 Error analysis

Although each model consistently shows improve-
ment on the precision, the recall is reduced a bit for
some models. Specifically, the recalls for Mod-
els II and III are not 100% but rather 99.9994%
and 99.996%, respectively. We manually checked
all errors for Models II and III and found that they
are caused by inconsistent annotations in the gold-
standard. For Model II, Propbank annotation for
ARGM-MOD was not done consistently with con-

97



junctions. For example in Figure 8, instead of an-
notating may or may not as the ARGM-MOD, some
annotations include only may and may not but not
the conjunction or. Since our system consistently
included the conjunctions, they appeared to be dif-
ferent from the gold-standard, but are not errors.

For Model III, Treebank annotation was not
done consistently for adverbs modifying nega-
tions. For example in Figure 10, longer is some-
times (but rarely) annotated as an adjective where
it is supposed to be an adverb. Furthermore,
longer sometimes becomes a child of the verb
predicate oppose (instead of being the child of no).
Such annotations made our system exclude longer
as a part of ARGM-NEG, but it would have found
them correctly if the trees were annotated consis-
tently.

There are a few cases that caused errors in Mod-
els IV and V. The most critical one is caused by PP
(prepositional phrase) attachment. In Figure 21,
enthusiasm is annotated as ARG1 of the verb pred-
icate showed, so our system retrieved the subtree
of enthusiasm, excluding the subtree of showed,
as the semantic boundary for the ARG1 (e.g., the
enthusiasm). However, Propbank originally an-
notated both the enthusiasm and for stocks as the
ARG1 in the phrase structure tree (so the preposi-
tional phrase got lost in our system).

the investors showed forenthusiasm stocks
NMOD

NMOD
SBJ ADV PMOD

Figure 21: PP-attachment example 1

This happens when there is a disagreement be-
tween Treebank and Propbank annotations: the
Treebank annotation attached the PP (for stocks)
to the verb (showed) whereas the Propbank anno-
tation attached the PP to the noun (enthusiasm).
This is a potential error in the Treebank. In this
case, we can trust the Propbank annotation and re-
construct the tree so the Treebank and Propbank
annotations agree with each other. After the re-
construction, the dependency tree would look like
one in Figure 22.

the investors showed forenthusiasm stocks
NMOD

NMOD
SBJ PMOD

ADV

Figure 22: PP-attachment example 2

7 Conclusion and future work

We have discussed how to convert phrase struc-
ture trees to dependency trees, how to find the
minimum-set of head words for Propbank argu-
ments in dependency structure, and heuristics for
retrieving fine-grained semantic boundaries. By
using our approach, we correctly retrieved the se-
mantic boundaries of 98.2% of the Propbank ar-
guments (F1-score of 99.54%). Furthermore, the
heuristics can be used to fix some of the incon-
sistencies in both Treebank and Propbank annota-
tions. Moreover, they suggest ways of reconstruct-
ing dependency structure so that it can fit better
with semantic roles.

Retrieving correct semantic boundaries is im-
portant for tasks like machine translation where
not only the head words but also all other words
matter to complete the task (Choi et al., 2009).
In the future, we are going to apply our approach
to other corpora and see how well the heuristics
work. In addition, we will try to find ways of auto-
matically adding empty categories to dependency
structure so we can deal with the full set of Prop-
bank arguments.

Acknowledgments

Special thanks are due to Professor Joakim Nivre
of Uppsala University and Claire Bonial of the
University of Colorado at Boulder for very helpful
insights. We gratefully acknowledge the support
of the National Science Foundation Grants CISE-
CRI-0551615, Towards a Comprehensive Lin-
guistic Annotation and CISE-CRI 0709167, Col-
laborative: A Multi-Representational and Multi-
Layered Treebank for Hindi/Urdu, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-C-
0022, subcontract from BBN, Inc. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.

References
Jinho D. Choi, Martha Palmer, and Nianwen Xue.

2009. Using parallel propbanks to enhance word-
alignments. In Proceedings of ACL-IJCNLP work-
shop on Linguistic Annotation (LAW‘09), pages
121–124.

98



Susanne Ekeklint and Joakim Nivre. 2007. A
dependency-based conversion of propbank. In
Proceedings of NODALIDA workshop on Building
Frame Semantics Resources for Scandinavian and
Baltic Languages (FRAME’07), pages 19–25.

Daniel Gildea. 2004. Dependencies vs. constituents
for tree-based alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’04), pages 214–221.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL’09), pages 1–18.

Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA’07).

Richard Johansson. 2008. Dependency-based Seman-
tic Analysis of Natural-language Text. Ph.D. thesis,
Lund University.

Jason S. Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking of
linguistic configurations. In Proceedings of the 3rd
International AAAI Conference on Weblogs and So-
cial Media (ICWSM’09).

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313–330.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.

Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In Proceedings of the Human Language Technology
Conference/North American chapter of the Associ-
ation for Computational Linguistics annual meeting
(HLT/NAACL’04).

99


