303

Coling 2010: Poster Volume, pages 303–311,

Beijing, August 2010

Building Systematic Reviews Using Automatic Text Classification 

Techniques  

 

 

 

 

Oana Frunza, Diana Inkpen, and Stan Matwin 

School of Information Technology and Engineering 

University of Ottawa  

{ofrunza,diana,stan}@site.uottawa.ca 

 

Abstract 

The  amount  of  information  in  medical 
publications  continues  to  increase  at  a 
tremendous  rate.  Systematic  reviews  help 
to  process  this  growing  body  of  informa-
tion.  They  are  fundamental  tools  for  evi-
dence-based  medicine.  In  this  paper,  we 
show that automatic text classification can 
be  useful  in  building  systematic  reviews 
for medical topics to speed up the review-
ing  process.  We  propose  a  per-question 
classification  method  that  uses  an  ensem-
ble of classifiers that exploit the particular 
protocol  of  a  systematic  review.  We  also 
show  that  when  integrating  the  classifier 
in  the  human  workflow  of  building  a  re-
view  the  per-question  method  is  superior 
to  the  global  method.  We  test  several 
evaluation measures on a real dataset. 

1 

Introduction 

Systematic  reviews  are  the  result  of  a  tedious 
process  which involves  human  reviewers to  ma-
nually  screen  references  of  papers  to  determine 
their relevance to the review. This process often 
entails  reading  thousands  or  even  tens  of  thou-
sands  of  abstracts  from  prospective  articles.  As 
the body of available articles continues to grow, 
this process is becoming increasingly difficult.  
Common  systematic  review  practices  stipu-
 
late that two reviewers are used at the screening 
phases of a systematic review to review each ab-
stract  of  the  documents  retrieved  after  a  simple 
query-based  search.  After  a  final  decision  is 
made for each abstract (the two reviewers decide 
if  the  abstract  is  relevant  or  not  to  the  topic  of 

review), in the next phase further analysis (more 
strict  screening  steps)  on  the  entire  article  is 
done.  A  systematic  review  has  to  be  complete, 
articles  that  are  published on  a certain  topic  and 
are  clinically  relevant  need  to  be  part  of  the  re-
view.  This  requires  near-perfect  recall  since  the 
accidental exclusion of a potentially relevant ab-
stract can have a significantly negative impact on 
the validity of the overall systematic review (Co-
hen et al., 2006). Our goal in this paper is to pro-
pose  an  automatic  system  that  can  help  human 
judges in the process of triaging articles by look-
ing  only  at  abstracts  and  not  the  entire  docu-
ments. This decision step is known as the initial 
screening  phase  in  the  protocol  of  building  sys-
tematic  reviews,  only  the  abstracts  are  used  as 
source of information.  

One reviewer will still read the entire collec-
tion of abstracts while the other will benefit from 
the help of the system; this reviewer will have to 
label  only  the  articles  that  will  be  used  to  train 
the  classifier  (ideally  a  small  proportion  for 
workload  reduction),  the  rest  of  the  articles  will 
be labeled by the classifier.  
 
In  the  systematic  review  preparation,  if  at 
least  one  reviewer  agrees  to  include  an  abstract, 
the abstract will have the labeled included and it 
will pass to the next screening phase; otherwise, 
it  will  be  discarded.  Therefore,  the  benefit  of 
doubt  plays  an  important  role  in  the  decision 
process. When we replace one reviewer with the 
automatic classifier, because we keep one human 
judge in the process, the confidence and reliabil-
ity  of the  review  is  still  higher  while  the  overall 
workload  is  reduced.  The  reduction  is  from  the 
time  required  for  two  passes  through  the  collec-
tion  (for  the  two  humans)  to  only  one  pass  and 
the smaller part labeled by the reviewer which is 
assisted  by  the  classifier.    Figure  1  presents  on 
overview of our proposed workflow.   

304

ing systematic reviews. In that paper the authors 
experimented with a variety of text classification 
techniques  using  the  data derived from  the  ACP 
Journal  Club  as  their  corpus.  They  found  that 
support vector machine (SVM) was the best clas-
sifier according to a variety of measures. Further 
work for systematic reviews was done by Cohen 
et al. (2006). Their work is mostly focused on the 
elimination  of  non  relevant  documents.  As  their 
main  goal  is  to  save  work  for  the  reviewers  in-
volved  in  systematic  review  preparation,  they 
define  a  measure,  called  work  saved  over  sam-
pling  (WSS)  that  captures  the  amount  of  work 
that  the  reviewers  will  save  with  respect  to  a 
baseline  of  just  sampling  for  a  given  value  of 
recall.  The  idea  is  that  a  classifier  returns,  with 
high recall, a set of abstracts, and only those ab-
stracts  need  to  be  read  to  weed  out  the  non-
relevant  ones.  The  savings  are  measured  with 
respect  to  the  number  of  abstracts  that  would 
have  to  be  read  if  a  random  baseline  classifier 
was  used.  Such  baseline  corresponds  to  uni-
formly  sampling  a  given  percentage  of  abstracts 
(equal to the desired recall) from the entire set. In 
Cohen et al. (2006), the WSS measure is applied 
to  report  the  reduction  in  reviewer's  work  when 
retrieving  95%  of  the  relevant  documents;  the 
precision was very low.  
  We focus on developing a classifier for sys-
tematic review preparation, relying on character-
istics  of  the  data  that  were  not  included  in  the 
Cohen  et  al.’s  (2006),  because  the  questions 
asked  in  the  preparation  of  the  reviews  are  not 
available,  Therefore  we  cannot  perform  a  direct 
comparison  of  results  here.  Also,  the  data  sets 
that  they  used  in  their  experiments  are  signifi-
cantly smaller than the one that we used. 

3  The Data Set 

A  set  of  47,274  abstracts  with  titles  were  col-
lected  from  MEDLINE2 as  part  of  a  systematic 
review  done  by  the  McMaster  University’s  Evi-
dence-Based  Practice  Center  using  TrialStat 
Corporation’s  Systematic  Review  System 3 ,  a 
web-based  software  platform  used  to  conduct 
systematic reviews.  

The initial set of abstracts was collected using 
a set of Boolean search queries that were run for 

                                                 
2 http://medline.cos.com 
3 http://www.trialstat.com/ 

 

 

Figure 1. Embedding automatic text classification in 

the process of building a systematic review. 

 

The task that needs to be solved in order to help 
the systematic review process is a text classifica-
tion task intended to classify an abstract as rele-
vant or not relevant to the topic of review. 
 
The  hypothesis  that  guides  our  research  is 
that it is possible to save time for the human re-
viewers  and  obtain  good  performance  levels, 
similar  to  the  ones  obtained  by  humans.  In  this 
current  study  we  show  that  we  can  achieve  this 
by  building  a  classification  model  that  is  based 
on the natural human workflow used for building 
systematic reviews. We show, on a real data set, 
that  a  human-machine  system  obtains  the  best 
results when an ensemble of classifiers is used as 
the classification model.  

2  Related Work  

The  traditional  way  to  collect  and  triage  the  ab-
stracts from a systematic review consists in using 
simple query search techniques based on MeSH1 
or  keywords  terms.  The  queries  are  usual  Boo-
lean-based and are optimized either for precision 
or  for  recall.  The  studies  done  by  Haynes  et  al. 
(1994) show that it is difficult to obtain high per-
formance for both measures.  

  The research done by Aphinyanaphongs and 
Aliferis (2005) is probably the first application of 
automatic  text  classification  to the  task  of creat-

                                                 
1 http://www.nlm.nih.gov/mesh/ 

305

the  specific  topic  of  the  systematic  review:  “the 
dissemination strategy of health care services 
for elderly people of age 65 and over”.  

In the protocol applied, two reviewers work in 
parallel. They read the entire collection of 47,274 
abstracts  and  answer  a  set of  questions  to  deter-
mine if an abstract is relevant or not to the topic 
of  review.  Examples  of  questions  present  in  the 
protocol:  Is  this  article  about  a  dissemination 
strategy  or  a  behavioral  intervention?;  Is  this  a 
primary  study?;  Is  this  a  review?;  etc.  An  ab-
stract is not considered to pass to the next screen-
ing  phase,  when  the entire  article  is  available,  if 
the  two  reviewers  respond  negative  to  the  same 
question for a certain abstract. All other cases of 
possible  responses  suggest  that  the  abstract  will 
be part of the next screening phase. In this paper 
we focus on the initial screening phase, the only 
source of information is the abstract and the title 
of  the  article,  with  the  main  goal  to  achieve  an 
acceptable  level  of  recall  not  to  mistakenly  ex-
clude relevant abstracts.  

  From  the  entire  collection  of  labeled  ab-
stracts  only  7,173  are  relevant.  Usually  in  the 
process of building systematic reviews the num-
ber  of  non-relevant  documents  is  much  higher 
than the number of relevant ones. The initial re-
trieval query is purposefully very broad, so as not 
to miss any relevant papers.  

4  Methods 

The  machine  learning  techniques  that  could  be 
used in the process of automating the creation of 
systematic  reviews  need  to  take  into  account 
some  issues  that  can  arise  when  dealing  with 
such  tasks.  Imbalanced  data  sets  are  usually 
what  we  deal  with  when  building  reviews,  the 
proportion  of  relevant  articles  that  end  up  being 
present in the review is significantly lower com-
pared  with  the  original  data  set.  The  benefit  of 
doubt  will  affect  the  quality  of  the  data  used  to 
train  the  classifier,  since  a  certain  amount  of 
noise is introduced: abstracts that are in fact non-
relevant  can  be  labeled  as  being  relevant  in  the 
first screening process. The relatively high num-
ber of abstracts involved in the process will make 
the  classification  algorithms  deal  with  a  high 
number  of  features  and  the  representation  tech-
nique should try to capture aspects pertaining of 
the medical domain.    
 

4.1  Representation Techniques 

In our current research, we use three representa-
tion  techniques:  bag-of-words  (BOW),  concepts 
from  the  Unified  Medical  Language  System 
(UMLS), and a combination of both.  

The  bag-of-words  representation  is  com-
monly  used  for  text  classification  and  we  have 
chosen  to  use  binary  feature  values.  Binary  fea-
ture values were shown to out-perform weighted 
values for text classification tasks in the medical 
domain as shown by Cohen et al. (2006) and bi-
nary values tend to be more stable in results than 
frequency  values  for  a  task  similar  to  ours,  as 
shown by Ma (2007). 

We  considered  feature  words  delimitated  by 
space  and  simple  punctuation  marks  that  ap-
peared  at  least  three  times  in  the  training  data, 
were  not  part  of  a  stop  words  list4,  and  had  a 
length greater than three characters. 30,000 word 
features were extracted. No stemming was used. 

UMLS  concepts  which  are  part  of  the  U.S. 
National  Library  of  Medicine 5  (NLM)  knowl-
edge repository are identified and extracted form 
the  collection  of  abstracts  using  the  MetaMap6 
system. This conceptual representation helped us 
overcome  some  of  the  shortcomings  of  BOW 
representation, and allowed us to use multi-word 
features,  medical  knowledge,  and  higher-level 
meanings of words in context. As Cohen (2008) 
shows,  multi-word  and  medical  concept  repre-
sentations are suitable to use.  

4.2  Classification Algorithms  

As  a  classification  algorithm  we  have  chosen  to 
use  the  complement  naive  Bayes  (CNB)  (Frank 
and  Bouckaert,  2006)  classifier  from  the  Weka7 
tool.  The  reason  for  this  choice  is  that  the  CNB 
classifier  implements  state-of-the-art  modifica-
tions  of  the  standard  multinomial  naïve  Bayes 
(MNB)  classifier  for  a  classification  task  with 
highly skewed class distribution (Drummond and 
Holte,  2003).  As  the  systematic  reviews  data 
usually  contain  a  large  majority  of  not  relevant 
abstracts,  resulting  in  a  skewness  reaching  even 
below 1%, it is important to use appropriate clas-
sifiers.    Other  classifiers,  such  as  decision  tress, 

                                                 
4 http://www.site.uottawa.ca/~diana/csi5180/StopWords 
5 http://www.nlm.nih.gov/pubs/factsheets/umls.html 
6 http://mmtx.nlm.nih.gov/ 
7 www.cs.waikato.ac.nz/machine learning/weka/ 

306

support vector machine, instance-based learning, 
and boosting, were used but the results obtained 
with CNB were always better. 

4.3  Global Text Classification Method 

The  first  method  that  we  propose  in  order  to 
solve  the  text  classification  task  that  is  intended 
to help a systematic review process is a straight-
forward machine learning approach. We trained a 
classifier,  CNB,  on  a  collection  of  abstracts  and 
then  evaluated  the  classifier’s  performance  on  a 
separate test data set. The power of this classifi-
cation  technique  stands  in  the  ability  to  use  a 
suitable  classification algorithm  and  a  good  rep-
resentation for the text classification task; Cohen 
et  al.  (2006)  also  used  this  approach.  We  ran-
domly  split  the  data  set  described  in  Section  3, 
into a training set and a test set. The two possible 
classes  are  Included  (relevant)  or  Excluded 
(non relevant). We decided to work with a train-
ing  set  smaller  than  the  test  set  because  ideally 
good  results  need  to  be  obtained  without  using 
too  much  training  data.  We  have  to  take  into 
consideration  that  training  a  classifier  for  a  par-
ticular topic, human effort is required for annota-
tion.  
 
 Table  1  presents  a  summary  of  the  data 
along  with  the  class  distribution  in  the  training 
and test data sets. We randomly sampled the data 
to  build  the  training  and  test  data  sets,  and  the 
original  distribution  of  1:5.6  between  the  two 
classes holds in both sets.  
 

Data 
set 

No. of 

Class distribution 

abstracts 

Included : Excluded (ratio) 

Training  20,000 

3,056 : 16,944 (1:5.6) 

Testing 

27,274 

4,117 : 23,157 (1:5.6) 

Table 1. Training and test data sets. 

Feature Selection 

 
4.3.1 
 
Using  the  global  method,  we  performed  experi-
ments  with  several  feature  selection  algorithms. 
We used only the BOW representation. 
Chi2 is a  measure  that evaluates  the  worth of an 
attribute  by  computing  the  value  of  the  chi-
squared  statistic  with  respect  to  the  class.  We 
selected  the  top  k1  CHI2 features  that  are  exclu-
sively  included  (appeared  only  in  the  training 
abstracts that are classified as Included) and the 
top k2 CHI2 features that are exclusively excluded 

(appeared  only  in  the  training  abstracts  that  are 
classified as  Excluded) and  used them  as a rep-
resentation for our data set. We varied the k1 pa-
rameter from 10 to 150 and k2 from 5 to 150 We 
used a  minimum  of  20  features and a  maximum 
of 300. 

InfoGain  evaluates the worth of an attribute 
by  measuring  the  information  gain  with  respect 
to the class. We run experiments when we varied 
the  number  of  selected  features  from  50  to  500. 
We used a number of 50, 100, 150, 250, 300 and 
500 top features.  

Bi-Normal Separation (BNS) is a feature se-
lection  technique  that  measures  the  separation 
between the threshold occurrences of a feature in 
one of the two classes. The latter measure is de-
scribed  in  detail  in  Forman  (2002).  We  used  a 
ratio of features that varies from 10 to 150 for the 
most  representative  features  for  the  Included 
class  and  from  5  to  150  for  the  Excluded  class. 
For some experiments the number of features for 
the  Included  class  is  higher  than  the  number  of 
features for the Excluded class. We have chosen 
to  do  so  because  we  wanted  to  re-balance  the 
imbalance of classes in the training data set. Af-
ter  selecting  the  number  of  Included  and  Ex-
cluded features, we used the combination to rep-
resent our entire collection of abstracts.  

We used the implementation from the Weka 
package for the Chi2 and InfoGain and the BNS 
implementation done by Ma (2007).  

4.4  Per-Question Classification Method 

The  second  method  that  we  propose  for  solving 
the  task  takes  into  account  the  specifics  of  the 
systematic  review  process.  It  takes  advantage  of 
the set of questions the reviewers use in the proc-
ess  of  deciding  if  an  abstract  is  relevant  or  not. 
These questions are created in the design step of 
the  systematic  review  and  almost  all  systematic 
reviews have them. By using these questions we 
better emulate how the human judges work when 
building systematic reviews.  
  We  have  chosen  to  use  only  the  questions 
that have inclusion/exclusion criteria, there were 
also  some  opened  answer  questions  involved  in 
the  review,  because  they  are  the  ones  that  are 
important  for  reviewers  to  make  a  decision.  To 
collect  training  data  for  each  question,  we  used 
the same training and test data set as in the pre-
vious  method  (but  note  that  not  all  the  abstracts 

307

have  answers  for  all  the  questions;  therefore  the 
training set sizes differ for each question). Table 
2 presents the questions and data sets used. 

When we created a training data set for each 
question we removed the abstracts for which we 
had a disagreement between the human experts – 
two  different  answers  for  a  specific  question, 
they  represent  noise  in  the  training  data.    For 
each of the questions from Table 2, we trained a 
CNB classifier on the corresponding data set.  

 

Question 

(Training : Included class : Excluded class) 

Q1 - Is this article about a dissemination strat-
egy or a behavioural intervention? (14,057:1,145: 
12,912) 
Q2 - Is the population in this article made of indi-
viduals 65-year old or older or does it comprise 
individuals who serve the elderly population needs 
(i.e. health care providers, policy makers, organi-
zations, community)? (15,005:7,360:7,645) 
Q3 - Is this a primary study? (8,825:6,895:1,930) 

Q4 - Is this a review? (6,429:5,640:789)  

Table 2. Data sets for the per-question classification 

method. 

 
We  used  the  same  representation  for  the  per-
question classifiers as we did for the global clas-
sifier: BOW, UMLS (the concepts that appeared 
only  in  the  new  question-oriented  training  data 
sets),  and  the  combination  BOW+UMLS.  We 
used  each  trained  model  to  obtain  a  prediction 
for each instance from the test set; therefore each 
test instance was assigned four prediction values 
of 0 or 1. To assign a final class for each test in-
stance, from the prediction of all four classifiers, 
the class of a test instance is decided according to 
one of the following four schemes:  
1. If any one vote is Excluded, the final class 
 
of  a  test  instance  is  Excluded.  This  is  a  1-vote 
scheme. 
       2.  If  any  two  votes  are  Excluded,  the  final 
class of a test instance is Excluded. This is a 2-
vote scheme. 
3.  If any three votes are Excluded, the final 
 
class of a test instance is Excluded. This is a 3-
vote scheme.  
4.  If  all  four  votes  are  Excluded,  the  final 
 
class of a test instance is Excluded. This is a 4-
vote scheme.  

 When  we  combined  of  the  classifiers,  we 

gave each classifier an equal importance. 

5  Evaluation Measures and Results 

When  performing  the  evaluation  for  the  task  of 
classifying an abstract into one of the two classes 
Included  (relevant)  or  Excluded  (non  rele-
vant),  two  objectives  are  of  great  importance: 
Objective 1 - ensure the completeness of the sys-
tematic review (maximize the number of relevant 
documents  included);  Objective  2  -  reduce  the 
reviewers'  workload  (maximize  the  number  of 
irrelevant documents excluded).  
  We  observe  that  objective  1  is  more  impor-
tant than objective 2 and this is why we decided 
to  report  recall  and  precision  for  the  Included 
class.  We  also  report  F-measure,  since  we  are 
dealing with imbalanced data sets.  

   Besides  the  standard  evaluation  measures, 
we report WSS8 measure as well in order to give 
a clearer view of the results we obtain.  
  As  baseline  for  our  methods  we  consider: 
two  extreme  baselines  and  a  random-baseline 
classifier  that  takes  into  account  the  distribution 
of  the  two  classes  in  the  training  data  set.  The 
baselines  results  are:  Include_All  –  a  baseline 
that  classifies  everything  in  the  majority  class: 
Recall  =  100%,  Precision  =  15%,  F-measure  = 
26.2%; WSS = 0% Exclude_All – a baseline that 
classifies  everything  as  Excluded:  Recall =  0%, 
Precision  =  100%,  F-measure  =  64.2%;  WSS  = 
0% Random baseline: Recall = 8.9%, Precision = 
15.4%, F-measure = 67.8%; WSS = 0.23%. 

5.1  Results for the Global Method 

In this subsection, we present the results obtained 
using our global method with the three represen-
tation techniques and CNB as classification algo-
rithm.  To  get  a  clear  image  of  the  results  we 
show  the  confusion  matrix  in  Table  3  for  the 
reader  to  better  understand  the  workload  reduc-
tion when using classifiers to help the process of 
building systematic reviews.  

BOW  features  were  identified  following  the 
guidelines presented in Section 3.4 and a number 
of  23,906  features  were  selected.  UMLS  con-
cepts were identified using the MetaMap system. 

                                                 
8 WSS = (TE + FE)/(TE + FE + TI + FI) – 1+ TI/(TI + FE) 
where T stands for true; F – false I – Included class; E- Ex-
cluded class. 

308

 

 

True Inc.  
False Inc. 
True Exc. 
False Exc. 

Recall 

 

BOW  UMLS  BOW+UMLS 
2,692 
5,022 
18,135 
 1,425 
65.3% 
34.9% 
45.5% 
37.1% 

2,715 
5,086 
18,071 
1,402 
65.9% 
34.8% 
45.5% 
37.3% 
Table 3. Results for the global method. 

2,793 
8,922 
14,235 
1,324 
67.8% 
23.8% 
35.2% 
24.9% 

Precision 
F-measure 

WSS 

From the whole training abstracts collection, 
a number of 459 UMLS features were identified. 
Analyzing  the  results  from  Table  5,  in  terms  of 
recall,  the  UMLS  representation  obtained  the 
best  recall  results,  67.8%  for  the  global  method 
but much lower precision, 23.8% than BOW rep-
resentation,  34.9%.  The  hybrid  representation, 
BOW+  UMLS  features  had  similar  results  with 
the  BOW  alone.  Recall  increased  a  bit  for  the 
hybrid  representation  compared  to  BOW  alone, 
0.6%  but  its  value  is  still  not  acceptable.  We 
conclude that the levels of recall, our main objec-
tive for this task, were not acceptable for a classi-
fier to be used as replacement of a human judge 
in the workflow of building a systematic review. 
The levels of precision that we obtained with the 
global  method  are  acceptable  but  they  cannot 
substitute the low level of recall. Since our major 
focus is recall, we investigated more and we fur-
ther improved our precision scores with the per-
question classification method. 

 

5.1.1  Results for Feature Selection 
 
Table  4  presents  the  results  obtained  with  our 
feature  selection  techniques.  We  decided  to  re-
port  only  representative  results  using  CNB  as  a 
classifier  and  a  specific  representation  setting. 
The number of features used in the experiment is 
presented in the round brackets. The first number 
represents the number of features extracted from 
the  Included  class  data  set  while  the  second 
from the Excluded class data set.  
 
Similar  experiments  were  performed  when 
using  Naïve  Bayes  as  classifier.  The  results  ob-
tained  were  opposite  to  ones  obtained  for  CNB, 
all  abstracts  were  classified  as  Excluded.  We 
believe  that  this  is  the  case  because  the  CNB 
classifier tries to compensate for the class imbal-
ance and gives more credit to the minority class,  

 

 

True Inc. 
False Inc. 
True Exc. 
False Exc. 

Recall 

Precision 
F-measure 

WSS 

Chi2 

(150:150) 

3,819 
19,233 
3,924 
298 

92.8% 
16.6% 
28% 
8.2% 

InfoGain 

(300) 
3,875 
19,638 
3,518 
242 

94.1% 
16.5% 
28% 
7.9% 

BNS 
(10:8) 
2,690 
13,905 
9,253 
1,427 
65.3% 
16.2% 
25% 
4.5% 

Table 4. Representative results obtained for various 

feature selection techniques. 

 

 
while the Naïve Bayes classifier will let the ma-
jority class overwhelm the classifier. 
 
Besides  the  results  presented  in  Table  4,  we 
also tried to boost the representative features for 
the  Included  class  hoping  to  re-balance  the  im-
balance  present  in  the  training  data  set.  To  per-
form  these  experiments  we  selected  the  top  k 
CHI2 word features and then added to this set of 
features  the  top  k1  CHI2  representative  features 
only for the Included class. The parameter k var-
ied from 50 to 100 and the parameter k1 from 30 
to 70. We performed experiments when using the 
original imbalanced training data set and using a 
balanced  data  set  as  well,  with  both  CNB  and 
Naïve  Bayes  classifier.  The  results  obtained  for 
these experiments were similar to the ones when 
we  used  the  previous  feature  selection  tech-
niques. There was no significant difference in the 
results compared to the ones in Table 5. 

5.2  Results for the Per-Question Method 

The results for our second method using the four 
voting schemes are presented in Table 5.  

Compared with the global method the results 
obtained  by  the  per-question  method,  especially 
the ones for 2 votes are the best so far in terms of 
the  balance  between  the  two  objectives.  A  large 
number  of  abstracts  that  should  be  excluded  are 
classified as Excluded whereas wrongly exclud-
ing  very  few  abstracts  that  should have  been in-
cluded (a lot fewer than in the case of the global 
classification method).  

The 2-votes scheme performs better than the 
1-vote  schemes  because  of  potential  classifica-
tion errors. When the classifiers for two different 
questions  (that  look  at  two  different  aspects  of 
the  systematic  review  topic)  are  confident  that 
the abstract is not relevant, the chance of correct 

309

prediction is higher; a balance between excluding 
an article and keeping it as relevant is achieved. 
When  using  the  classifiers  for  3  or  4  questions 
the performance goes down in terms of precision; 
a higher number of abstracts get classified as In-
cluded - some abstracts do not address all target 
question of the review topic.  
 

1-Vote  
True Inc. 
False Inc. 
True Exc. 
False Exc. 

Recall 

Precision 
F-measure 

WSS 
2-Vote 

True Inc. 
False Inc. 
True Exc. 
False Exc. 

Recall 

Precision 
F-measure 

WSS 
3-Vote  
True Inc. 
False Inc. 
True Exc. 
False Exc. 

Recall 

Precision 
F-measure 

WSS 
4-Vote  
True Inc. 
False Inc. 
True Exc. 
False Exc. 

Recall 

Precision 
F-measure 

WSS 

BOW 
1,262 
745 

22,412 
2,855 
30.6% 
62.8% 
41.2% 
23.2% 
BOW 
3,181 
9,976 
13,181 

936 

77.2% 
24.1% 
36.8% 
29.0% 
BOW 
3,898 
18,915 
4,242 
219 

94.6% 
17.0% 
28.9% 
11.0% 
BOW 
4,085 
21,946 
1,211 

32 

99.2% 
15.6% 
27.1% 
3.7% 

BOW+UMLS 

1,264 
741 

22,416 
2,853 
30.7% 
63.0% 
41.2% 
23.3% 

UMLS  BOW+UMLS 
1,222 
2,266 
20,891 
2,895 
29.6% 
35.0% 
32.1% 
16.8% 
UMLS 
2,603 
9,505 
13,652 
1,514 
63.2% 
21.5% 
32.0% 
18.8% 
UMLS 
3,480 
16,472 
6,685 
637 

3,890 
18,881 
4,276 
227 

79.7% 
23.4% 
36.2% 
28.4% 

3,283 
10,720 
12,437 

834 

BOW+UMLS 

94.4% 
17.0% 
28.9% 
11.0% 

84.5% 
17.4% 
28.9% 
11.3% 
UMLS  BOW+UMLS 
3,947 
20,869 
2,288 
170 

4,086 
21,964 
1,193 

31 

95.8% 
15.9% 
27.2% 
4.8% 

99.2% 
15.6% 
27.0% 
3.7% 

Table 5. Results for the per-question method for the 

Included class. 

 

For  the  per-question  technique  the  recall  value 
peaked  at  99.2%  with  the  4-vote  method  BOW 
and  BOW+UMLS  representation  technique.  In 
the same  time  the lowest  values  of  precision  for 
the  per-question  technique,  15.6%  is  obtained 
with  the  same  experimental  setting.  It  is  impor-
tant to aim for a high recall but not to dismiss the 
precision values. The difference of even less than 

2% in precision values can cause the reviewers to 
read  additional  thousands  of  documents,  as  ob-
served  in  the  confusion  matrices  for  2-vote,  3-
vote and 4-vote methods in Table 5.  

From the confusion matrix in Table 5 for the 
2-vote method and the 3- and 4-vote method we 
observe  the  high  difference  in  the  number  of 
documents  a  reviewer  will  have  to  read  (the 
falsely  included  documents).  The  difference  in 
precision  from  24.1%  for  the  2-vote  method  to 
15.6% for the 4-vote method makes the reviewer 
go through 11,988 additional abstracts.  

The best value for the WSS  measure for the 
per-question  method  is  achieved  by  the  2-vote 
scheme. The result is lower than the one obtained 
by the global method but the recall level is higher 
therefore, we still keep as a potential winner the 
2-vote scheme.  

5.3  Results for Human-Machine Workflow 

In  Figure  1,  we  envisioned  the  way  we  can  use 
the automatic classifier in the workflow of build-
ing a systematic review. In order to determine the 
performance  of  the  human-machine  workflow 
that  we  propose  we  computed  the  recall  values 
when the human reviewer’s labels are combined 
with  the  labels  obtained  from  the  classifier.  The 
same labeling technique is applied as for the hu-
man-human workflow: if at least one decision for 
an  abstract  is  to  include  it  in  the  systematic  re-
view, then the final label is Included.  
  We  also  calculated  the  evaluation  measures 
for  the  two  reviewers.  The  evaluation  measures 
for  the  human  judge  that  is  kept  in  the  human-
machine  workflow,  Reviewer  1  in  Figure  1,  are 
64.29% for recall and 15.20% for precision. The 
evaluation measures for the reviewer that is to be 
replaced  in  the  human-machine  classification, 
Reviewer 2 in Figure 1 are 59.66% for recall and 
15.09%  for  precision.  The  recall  value  for  the 
two  human  judges  combined  is  85.26%  and  the 
precision  value  is  100%. As  we  can observe  the 
recall value for the second reviewer, the one that 
is  replaced  in  the  human-classifier  workflow  is 
low.  In  Table  6  we  present  precision  and  recall 
results for the symbiotic model for both our me-
thods. In these results we can clearly see that the 
2-vote  technique  is  superior  to  the  other  voting 
techniques and to the global method. For almost 
the same level of precision the level or recall it is 
much higher. These observations support the fact 

310

that the extra effort spent in identifying the most 
suitable methodology pays off.  
 
The  fact  that  we  keep  a  human  in  the  loop 
makes  our  method acceptable as a  workflow  for 
building a systematic review.  
 

Method  BOW 

UMLS 

Global 
1-Vote 
2-Vote 
3-Vote 
4-Vote 

   17.9/87.7% 
17.1/75.3% 
17.1/91.6% 
15.8/97.9% 
15.3/99.6% 

17.0/88.6% 
16.5/74.8% 
16.4/86.6% 
15.8/94.2% 
15.4/98.3% 

BOW+ 
UMLS 
17.9/87.7% 
17.1/75.4% 
17.1/92.7% 
15.8/97.8% 
15.3/99.6% 

Table 6. Precision/recall results for the human-

classifier workflow for the Included class. 

6  Discussion 

The global method achieves good results in terms 
of  precision  while  the  best  recall  is  obtained  by 
the per-question method.   
The  best  results  for  the  task  were  obtained 
 
using  the  per-question  method  with  the  2-vote 
scheme  with  or  without  UMLS  features.  The  3-
vote  scheme  with  UMLS  representation  is  close 
to  the  2-vote  scheme  but  looking  at  F-measure 
and WSS results the 2-vote scheme is better. The 
clear  distinction  between  the  methods  comes 
when  we  combined  the  classifiers  with  the  hu-
man judge in the workflow of building reviews. 

The  per-question  technique  is  more  robust 
and it offers the possibility to choose the desired 
type of performance. If the reviewers are willing 
to read almost the entire collection of documents, 
knowing that the recall is high, then a 3 or 4-vote 
scheme can be the set-up (though the 3 or 4-vote 
method  is  not  likely  to  achieve  100%  recall  be-
cause  it  is  very  rare  that  an  abstract  contain  an-
swers to three or four of the questions associated 
with the systematic review). If the reviewers will 
like  to  read  a  small  collection  being  confident 
that almost all the abstracts are relevant, then a 1-
vote scheme can be the set-up required. The per-
question  method  confirms  the  fact  that  an  en-
semble of classifiers is better than one classifier; 
(Dietterich, 1997).  

When we combine the human and the system 
results  we  obtain  a  major  improved  in  terms  of 
recall.  We  base  our  discussion  for  the  human-
machine results for the experiment that obtained 
the  best  results,  the  2-vote  scheme  with  a 
BOW+UMLS  representation  technique.  When 
combining  the  human  and  classifier  decisions, 

the  precision  level  decreased  a  bit  compared  to 
the  one  that  the  machine  obtained.  We  believe 
that this is the case because some of the abstracts 
that the classifier excluded were included by the 
first  human  reviewer  and,  with  this  decision 
process in place, the level of precision dropped. 

Our  goal  of  improving  the  recall  level  from 
the  first  level  of  screening  is  achieved,  since 
when both the classifier and the human judge are 
integrated in the workflow, the recall level jumps 
from 79.7% to 92.7%. 

We  believe  that  the  low  level  of  precision 
that  is  obtained  for  the  human  reviewer,  for  the 
human-classifier workflow, and for the classifier, 
is due to the fact that we are running experiments 
for  the  first  screening  phase  when  we  use  only 
the abstracts as source of information and not the 
entire articles.  

We believe that further investigations are re-
quired to fully replace a human reviewer with an 
automatic classifier but the results obtained with 
the per-question method encourage us to believe 
that  this  is  a  suitable  solution  for  reaching  our 
final goal.  

7  Conclusions and Future Work  

In  this  paper,  we  looked  at  two  methods  by 
which we envision the way automatic text classi-
fication  techniques  could  help  the  workflow  of 
building systematic reviews.  

The first  method  is  a  straight-forward  appli-
cation  of  the  representations  and  learning  algo-
rithms that capture the specifics of the data: med-
ical domain, huge number of features, misclassi-
fication, and imbalanced classes.  

We  showed  that  the  specifics  of  the  human 
protocol  in  which  systematic  reviews  are  built 
have a positive effect when deployed in an auto-
matic  way.  We  believe  that  the  tedious  process 
that  is  currently  used  for  building  systematic  re-
views can be lightened by the use of a classifier 
in  combination  with  only  one  human  judge.  By 
having a human judge in the loop, we ensure that 
the  workflow is  reliable and  that  the  system  can 
be easily integrated in the workflow.  

 In  future  work  we  would  like  to  look  into 
ways  of  improving  the  results  by  the  way  we 
chose  the  training  data  set  and  by  integrating 
more domain specific knowledge. We would also 
like to investigate  ways  by  witch  we  can  update 
systematic reviews.  

311

References  

Aphinyanaphongs Y. and Aliferis C. Text Categoriza-
tion Models for Retrieval of High Quality Articles. 
Journal  of  the  American  Medical  Informatics  As-
sociation 2005; 12:207-216. 

Cohen  A.M.  Optimizing  Feature  Representation  for 
Automated Systematic Review  Work Prioritization. 
Proceedings  of  the  AMIA  Annual  Symposium 
2008; 6:121-126. 

Cohen A.M., Hersh W.R., Peterson K., Yen P.Y. Re-
ducing  Workload  in  Systematic  Review  Prepara-
tion  Using  Automated  Citation  Classification. 
Journal  of  the  American  Medical  Informatics  As-
sociation 2006; 13:206-219. 

Dietterich,  T.  Machine-Learning  Research:  Four 
Current  Directions.  Artificial  Intelligence  Maga-
zine. 18(4): 97-136 (1997) 

Drummond C. and Holte R.C. C4.5, Class Imbalance, 
and  Cost  Sensitivity:  Why  Under-Sampling  beats 
Over-Sampling.  Proceedings  of  the  Twentieth  In-
ternational  Conference  on  Machine  Learning: 
Workshop on Learning from Imbalanced Data Sets 
(II), 2003. 

Forman G. Choose Your Words Carefully: An Empiri-
cal  Study  of  Feature  Selection  Metrics  for  Text 
Classification. In the Joint Proceedings of the 13th 
European  Conference  on  Machine  Learning  and 
the  6th  European  Conference  on  Principles  and 
Practice  of  Knowledge  Discovery  in  Databases 
(ECML/PKDD), 2002. 

Frank  E.  and  Bouckaert  R.R.  Naive  Bayes  for  Text 
Classification  with  Unbalanced  Classes.  In  the 
Proceedings  of  the  10th  European  Conference  on 
Principles and Practice of Knowledge Discovery in 
Databases, Berlin, Germany, 2006, pp. 503-510. 

Haynes R.B., Wilczynski N., McKibbon K.A., Walker 
C.J., Sinclair J.C. Developing optimal search strat-
egies  for  detecting  clinically  sound  studies  in 
MEDLINE.  Journal  of  the  American  Medical  In-
formatics Association 1994; 1:447-58. 

Kohavi R. and Provost F. Glossary of Terms. Editorial 
for  the  Special  Issue  on  Applications  of  Machine 
Learning  and  the  Knowledge  Discovery  Process 
1998; 30:271-274. 

Ma  Y.  2007.  Text  classification  on  imbalanced  data: 
Application  to  Systematic  Reviews  Automation.  
M.Sc. Thesis. University of Ottawa. 

