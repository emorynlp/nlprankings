Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 19–27,

Beijing, August 2010

19

Robust Measurement and Comparison of Context Similarity for Finding

Translation Pairs

Daniel Andrade†, Tetsuya Nasukawa‡, Jun’ichi Tsujii†
†Department of Computer Science, University of Tokyo

{daniel.andrade, tsujii}@is.s.u-tokyo.ac.jp

‡IBM Research - Tokyo

nasukawa@jp.ibm.com

Abstract

In cross-language information retrieval it
is often important to align words that are
similar in meaning in two corpora writ-
ten in different languages. Previous re-
search shows that using context similar-
ity to align words is helpful when no
dictionary entry is available. We sug-
gest a new method which selects a sub-
set of words (pivot words) associated with
a query and then matches these words
across languages. To detect word associa-
tions, we demonstrate that a new Bayesian
method for estimating Point-wise Mutual
Information provides improved accuracy.
In the second step, matching is done in
a novel way that calculates the chance of
an accidental overlap of pivot words us-
ing the hypergeometric distribution. We
implemented a wide variety of previously
suggested methods. Testing in two con-
ditions, a small comparable corpora pair
and a large but unrelated corpora pair,
both written in disparate languages, we
show that our approach consistently out-
performs the other systems.

1 Introduction
Translating domain-speciﬁc, technical terms from
one language to another can be challenging be-
cause they are often not listed in a general dictio-
nary. The problem is exempliﬁed in cross-lingual
information retrieval (Chiao and Zweigenbaum,
2002) restricted to a certain domain. In this case,
the user might enter only a few technical terms.
However, jargons that appear frequently in the

data set but not in general dictionaries, impair the
usefulness of such systems. Therefore, various
means to extract translation pairs automatically
have been proposed. They use different clues,
mainly

• Spelling distance or transliterations, which
are useful to identify loan words (Koehn and
Knight, 2002).

• Context similarity, helpful since two words
with identical meaning are often used in sim-
ilar contexts across languages (Rapp, 1999).

The ﬁrst type of information is quite speciﬁc; it
can only be helpful in a few cases, and can thereby
engender high-precision systems with low recall,
as described for example in (Koehn and Knight,
2002). The latter is more general.
It holds for
most words including loan words. Usually the
context of a word is deﬁned by the words which
occur around it (bag-of-words model).

Let us brieﬂy recall the main idea for using
context similarity to ﬁnd translation pairs. First,
the degree of association between the query word
and all content words is measured with respect to
the corpus at hand. The same is done for every
possible translation candidate in the target cor-
pus. This way, we can create a feature vector
for the query and all its possible translation can-
didates. We can assume that, for some content
words, we have valid translations in a general dic-
tionary, which enables us to compare the vectors
across languages. We will designate these content
words as pivot words. The query and its trans-
lation candidates are then compared using their
feature vectors, where each dimension in the fea-
ture vector contains the degree of association to

20

one pivot word. We deﬁne the degree of associa-
tion, as a measurement for ﬁnding words that co-
occur, or which do not co-occur, more often than
we would expect by pure chance.1

We argue that common ways for comparing
similarity vectors across different corpora perform
worse because they assume that degree of associa-
tions are very similar across languages and can be
compared without much preprocessing. We there-
fore suggest a new robust method including two
steps. Given a query word, in the ﬁrst step we
determine the set of pivots that are all positively
associated with statistical signiﬁcance. In the sec-
ond step, we compare this set of pivots with the set
of pivots extracted for a possible translation can-
didate. For extracting positively associated piv-
ots, we suggest using a new Bayesian method for
estimating the critical Pointwise Mutual Informa-
tion (PMI) value.
In the second step, we use a
novel measure to compare the sets of extracted
pivot words which is based on an estimation of
the probability that pivot words overlap by pure
chance. Our approach engenders statistically sig-
niﬁcant improved accuracy for aligning transla-
tion pairs, when compared to a variety of previ-
ously suggested methods. We conﬁrmed our ﬁnd-
ings using two very different pairs of comparable
corpora for Japanese and English.

In the next section, we review previous related
In Section 3 we explain our method in
work.
detail, and argue that it overcomes subtle weak-
nesses of several previous efforts. In Section 4, we
show with a series of cross-lingual experiments
that our method, in some settings, can lead to con-
siderable improvement in accuracy. Subsequently
in Section 4.2, we analyze our method in contrast
to the baseline by giving two examples. We sum-
marize our ﬁndings in Section 5.

2 Related Work
Extracting context similarity for nouns and then
matching them across languages to ﬁnd trans-
lation pairs was pioneered in (Rapp, 1999) and
(Fung, 1998). The work in (Chiao and Zweigen-
baum, 2002), which can be regarded as a varia-
1For example ”car” and ”tire” are expected to have a high
(positive) degree of association, and ”car” and ”apple” is ex-
pected to have a high (negative) degree of association.

tion of (Fung, 1998), uses tf.idf, but suggests to
normalize the term frequency by the maximum
number of co-occurrences of two words in the cor-
pus. All this work is closely related to our work
because they solely consider context similarity,
whereas context is deﬁned using a word window.
The work in (Rapp, 1999; Fung, 1998; Chiao and
Zweigenbaum, 2002) will form the baselines for
our experiments in Section 4.2 This baseline is
also similar to the baseline in (Gaussier et al.,
2004), which showed that it can be difﬁcult to beat
such a feature vector approach.

In principle our method is not restricted to how
context is deﬁned; we could also use, for exam-
ple, modiﬁers and head words, as in (Garera et
al., 2009). Although, we found in a preliminary
experiment that using a dependency parser to dif-
ferentiate between modiﬁers and head words like
in (Garera et al., 2009), instead of a bag-of-words
model, in our setting, actually decreased accuracy
due to the narrow dependency window. How-
ever, our method could be combined with a back-
translation step, which is expected to improve
translation quality as in (Haghighi et al., 2008),
which performs indirectly a back-translation by
matching all nouns mutually exclusive across cor-
pora. Notably,
there also exist promising ap-
proaches which use both types of information,
spelling distance, and context similarity in a joint
framework, see (Haghighi et al., 2008), or (D´ejean
et al., 2002) which include knowledge of a the-
saurus. In our work here, we concentrate on the
use of degrees of association as an effective means
to extract word translations.

In this application, to measure association ro-
bustly, often the Log-Likelihood Ratio (LLR)
measurement is suggested (Rapp, 1999; Morin et
al., 2007; Chiao and Zweigenbaum, 2002). The
occurrence of a word in a document is modeled
as a binary random variable. The LLR measure-
ment measures stochastic dependency between

2Notable differences are that we neglected word order, in
contrast to (Rapp, 1999), as it is little useful to compare it
between Japanese and English. Furthermore in contrast to
(Fung, 1998) we use only one translation in the dictionary,
which we select by comparing the relative frequencies. We
also made a second run of the experiments where we man-
ually selected the correct translations for the ﬁrst half of the
most frequent pivots – Results did not change signiﬁcantly.

21

two such random variables (Dunning, 1993), and
is known to be equal to Mutual Information that is
linearly scaled by the size of the corpus (Moore,
2004). This means it is a measure for how much
the occurrence of word A makes the occurrence
of word B more likely, which we term positive
association, and how much the absence of word
A makes the occurrence of word B more likely,
which we term negative association. However, our
experiments show that only positive association is
beneﬁcial for aligning words cross-lingually.
In
fact, LLR can still be used for extracting posi-
tive associations by ﬁltering in a pre-processing
step words with possibly negative associations
(Moore, 2005). Nevertheless a problem which
cannot be easily remedied is that conﬁdence es-
timates using LLR are unreliable for small sample
sizes (Moore, 2004). We suggest a more princi-
pled approach that measures from the start only
how much the occurrence of word A makes the
occurrence of word B more likely, which is des-
ignated as Robust PMI.

Another point that is common to (Rapp, 1999;
Morin et al., 2007; Chiao and Zweigenbaum,
2002; Garera et al., 2009; Gaussier et al., 2004)
is that word association is compared in a ﬁne-
grained way, i.e. they compare the degree of asso-
ciation3 with every pivot word, even when it is low
or exceptionally high. They suggest as a compar-
ison measurement Jaccard similarity, Cosine sim-
ilarity, and the L1 (Manhattan) distance.

3 Our Approach

We presume that rather than similarity between
degree (strength of) of associations, the existence
of common word associations is a more reliable
measure for word similarity because the degrees
of association are difﬁcult to compare for the fol-
lowing reasons:

the same corpus, we will in general measure
different degrees of association.

• Differences in sub-domains / sub-topics
Corpora sharing the same topic can still dif-
fer in sub-topics.

• Differences in style or language
Differences in word usage. 4

Other information that
is used in vector ap-
proaches such as that in (Rapp, 1999) is nega-
tive association, although negative association is
less informative than positive. Therefore, if it is
used at all, it should be assigned a much smaller
weight.

Our approach caters to these points, by ﬁrst de-
ciding whether a pivot word is positively associ-
ated (with statistical signiﬁcance) or whether it
is not, and then uses solely this information for
ﬁnding translation pairs in comparable corpora. It
is divisible into two steps. In the ﬁrst, we use a
Bayesian estimated Pointwise Mutual Information
(PMI) measurement to ﬁnd the pivots that are pos-
itively associated with a certain word with high
conﬁdence. In the second step, we compare two
words using their associated pivots as features.
The similarity of feature sets is calculated using
pointwise entropy. The words for which feature
sets have high similarity are assumed to be related
in meaning.

3.1 Extracting positively associated words –

Feature Sets

To measure the degree of positive association be-
tween two words x and y, we suggest the use
of information about how much the occurrence
of word x makes the occurrence of word y more
likely. We express this using Pointwise Mutual
Information (PMI), which is deﬁned as follows:

• Small differences in the degree of associa-
tion are not statistically signiﬁcant
Taking, for example, two sample sets from

3To clarify terminology, where possible, we will try to
distinguish between association and degree of association.
For example word “car” has the association “tire”, whereas
the degree of association with “tire” is a continuous number,
like 5.6.

P M I(x, y) = log

p(x, y)
p(x) · p(y)

= log

p(x|y)
p(x)

.

Therein, p(x) is the probability that word x oc-
curs in a document; p(y) is deﬁned analogously.
Furthermore, p(x, y) is the probability that both
4For example, “stop” is not the only word to describe the

fact that a car halted.

22

words occur in the same document. A positive as-
In related
sociation is given if p(x|y) > p(x).
works that use the PMI (Morin et al., 2007), these
probabilities are simply estimated using relative
frequencies, as

P M I(x, y) = log

f (x,y)

n
f (x)
f (y)

,

n

n

where f (x), f (y) is the document frequency
of word x and word y, and f (x, y) is the co-
occurrence frequency; n is the number of docu-
ments. However, using relative frequencies to es-
timate these probabilities can, for low-frequency
words, produce unreliable estimates for PMI
(Manning and Sch¨utze, 2002). It is therefore nec-
essary to determine the uncertainty of PMI esti-
mates. The idea of deﬁning conﬁdence intervals
over PMI values is not new (Johnson, 2001); how-
ever, the problem is that exact calculation is very
computationally expensive if the number of docu-
ments is large, in which case one can approximate
the binomial approximation for example with a
Gaussian, which is, however only justiﬁed if n
is large and p, the probability of an occurrence,
is not close to zero (Wilcox, 2009). We suggest
to deﬁne a beta distribution over each probabil-
ity of the binary events that word x occurs, i.e.
[x], and analogously [x|y]. It was shown in (Ross,
2003) that a Bayesian estimate for Bernoulli trials
using the beta distribution delivers good credibil-
ity intervals5, importantly, when sample sizes are
small, or when occurrence probabilities are close
to 0. Therefore, we assume that

p(x|y) ∼ beta(α′x|y, β′x|y), p(x) ∼ beta(α′x, β′x)
where the parameters for the two beta distribu-
tions are set to

α′x|y = f (x, y) + αx|y ,
β′x|y = f (y) − f (x, y) + βx|y , and
α′x = f (x) + αx, β′x = n − f (x) + βx .

Prior information related to p(x) and the con-
ditional probability p(x|y) can be incorporated
5In the Bayesian notation we refer here to credibility in-
tervals instead of conﬁdence intervals.

These can,

by setting the hyper-parameters of the beta-
distribtutions.6
for example, be
learned from another unrelated corpora pair and
then weighted appropriately by setting α + β. For
our experiments, we use no information beyond
the given corpora pair; the conditional priors are
therefore set equal to the prior for p(x). Even if
we do not know which word x is, we have a notion
about p(x) because Zipf’s law indicates to us that
we should expect it to be small. A crude estima-
tion is therefore the mean word occurrence proba-
bility in our corpus as

γ =

1

|all words| ∑x∈{all words}

f (x)

n

.

We give this estimate a total weight of one obser-
vation. That is, we set

α = γ , β = 1 − γ .

From a practical perspective, this can be inter-
preted as a smoothing when sample sizes are
small, which is often the case for p(x|y). Because
we assume that p(x|y) and p(x) are random vari-
ables, PMI is consequently also a random variable
that is distributed according to a beta distribution
ratio.7 For our experiments, we apply a general
sampling strategy. We sample p(x|y) and p(x) in-
dependently and then calculate the ratio of times
P M I > 0 to determine P (P M I > 0).8 We will
refer to this method as Robust PMI (RPMI).

Finally we can calculate, for any word x, the set
of pivot words which have most likely a positive
association with word x. We require that this set
be statistically signiﬁcant: the probability of one
or more words being not a positive association is
smaller than a certain p-value.9

6The hyper-parameters α and β, can be intuitively inter-
preted in terms of document frequency. For example αx is
the number of times we belief the word x occurs, and βx the
number of times we belief that x does not occur in a corpus.
Analogously αx|y and βx|y can be interpreted with respect
to the subset of the corpus where the word y occurs, instead
of the whole corpus. Note however, that α and β do not nec-
essarily have to be integers.

7The resulting distribution for the general case of a beta
distribution ratio was derived in (Pham-Gia, 2000). Unfortu-
nately, it involves the calculation of a Gauss hyper-geometric
function that is computationally expensive for large n.

8For experiments, we used 100, 000 samples for each es-

timate of P (P M I > 0).

9We set, for all of our experiments, the p-value to 0.01.

23

As an alternative for determining the probabil-
ity of a positive association using P (P M I > 0),
we calculate LLR and assume that approximately
LLR ∼ χ2 with one degree of freedom (Dunning,
1993). Furthermore, to ensure that only positive
association counts, we set the probability to zero
if p(x, y) < p(x) · p(y), where the probabilities
are estimated using relative frequencies (Moore,
2005). We refer to this as LLR(P); lacking this
correction, it is LLR.

3.2 Comparing Word Feature Sets Across

Corpora

So far, we have explained a robust means to ex-
tract the pivot words that have a positive associa-
tion with the query. The next task is to ﬁnd a sen-
sible way to use these pivots to compare the query
with candidates from the target corpus. A simple
means to match a candidate with a query is to see
how many pivots they have in common, i.e. using
the matching coefﬁcient (Manning and Sch¨utze,
2002) to score candidates. This similarity mea-
sure produces a reasonable result, as we will show
in the experiment section; however, in our error
analysis, we found out that this gives a bias to
candidates with higher frequencies, which is ex-
plainable as follows. Assuming that a word A has
a ﬁxed number of pivots that are positively associ-
ated, then depending on the sample size—the doc-
ument frequency in the corpus—not all of these
are statistically signiﬁcant. Therefore, not all true
positive associations are included in the feature
set to avoid possible noise. If the document fre-
quency increases, then we can extract more sta-
tistically signiﬁcant positive associations and the
cardinality of the feature set increases. This con-
sequently increases the likelihood of having more
pivots that overlap with pivots from the query’s
feature set. For example, imagine two candidate
words A and B, for which feature sets of both in-
clude the feature set of the query, i.e. a complete
match, however A’s feature set is much larger than
B’s feature set. In this case, the information con-
veyed by having a complete match with the query
word‘s feature set is lower in the case of A’s fea-
ture set than in case of B’s feature set. Therefore,
we suggest its use as a basis of our similarity mea-
sure, the degree of pointwise entropy of having an

estimate of m matches, as

Information(m, q, c) = − log(P (matches = m)).
Therein, P (matches = m) is the likelihood that a
candidate word with c pivots has m matches with
the query word, which has q pivots. Letting w be
the total number of pivot words, we can then cal-
culate that the probability that the candidate with
c pivots was selected by chance

P (matches = m) = ( q

m) ·(w−q
c−m)
(w
c)

.

Note that this probability equals a hypergeometric
distribution.10 The smaller P (matches = m) is,
the less likely it is that we obtain m matches by
pure chance. In other words, if P (matches = m)
is very small, m matches are more than we would
expect to occur by pure chance.11

Alternatively, in our experiments, we also con-
sider standard similarity measurements (Manning
and Sch¨utze, 2002) such as the Tanimoto coefﬁ-
cient, which also lowers the score of candidates
that have larger feature sets.

4 Experiments
In our experiments, we speciﬁcally examine trans-
lating nouns, mostly technical terms, which occur
in complaints about cars collected by the Japanese
Ministry of Land, Infrastructure, Transport and
Tourism (MLIT)12, and in complaints about cars
collected by the USA National Highway Trafﬁc
Safety Administration (NHTSA)13. We create for
each data collection a corpus for which a doc-
ument corresponds to one car customer report-
ing a certain problem in free text. The com-
plaints are, in general, only a few sentences long.

10` q
m´ is the number of possible combinations of pivots
fore,` q
m´ ·`w−q
c−m´ is the number of possible different feature
pivots with the query. Furthermore,`w
c´ is the total number

sets that the candidate can have such that it shares m common

which the candidate has in common with the query. There-

of possible feature sets the candidate can have.

11The discussion is simpliﬁed here.

It can also be that
P (matches = m) is very small, if there are less occur-
rences of m that we would expect to occur by pure chance.
However, this case can be easily identiﬁed by looking at the
gradient of P (matches = m).

12http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
13http://www-odi.nhtsa.dot.gov/downloads/index.cfm

24

To verify whether our results can be generalized
over other pairs of comparable corpora, we ad-
ditionally made experiments using two corpora
extracted from articles of Mainichi Shinbun, a
Japanese newspaper, in 1995 and English articles
from Reuters in 1997. There are two notable dif-
ferences between those two pairs of corpora: the
content is much less comparable, Mainichi re-
ports more national news than world news, and
secondly, Mainichi and Reuters corpora are much
larger than MLIT/NHTSA.14

For both corpora pairs, we extracted a
gold-standard semi-automatically by looking at
Japanese nouns and their translations with docu-
ment frequency of at least 50 for MLIT/NHTSA,
and 100 for Mainichi/Reuters. As a dictionary we
used the Japanese-English dictionary JMDic15.
In general, we preferred domain-speciﬁc terms
over very general terms, i.e.
for example for
MLIT/NHTSA the noun 噴射 “injection” was
preferred over 取り付け “installation”. We ex-
tracted 100 noun pairs for MLIT/NHTSA and
Mainichi/Reuters, each.
Each Japanese noun
which is listed in the gold-standard forms a query
which is input into our system. The resulting
ranking of the translation candidates is automat-
ically evaluated using the gold-standard. There-
fore, synonyms that are not listed in the gold stan-
dard are not recognized, engendering a conserva-
tive estimation of the translation accuracy. Be-
cause all methods return a ranked list of trans-
lation candidates, the accuracy is measured us-
ing the rank of the translation listed in the gold-
standard.16 The Japanese corpora are prepro-
cessed with MeCab (Kudo et al., 2004); the En-
glish corpora with Stepp Tagger (Tsuruoka et al.,
2005) and Lemmatizer (Okazaki et al., 2008). As
a dictionary we use the Japanese-English dictio-
nary JMDic17. In line with related work (Gaussier
et al., 2004), we remove a word pair (Japanese
noun s, English noun t) from the dictionary, if s
occurs in the gold-standard. Afterwards we deﬁne

14MLIT/MLIT

documents.
Mainichi/Reuters corpora 75,935 and 148,043 documents,
respectively.

20,000

each

has

15http://www.csse.monash.edu.au/ jwb/edict doc.html
16In cases for which there are several translations listed for

one word, the rank of the ﬁrst is used.

17http://www.csse.monash.edu.au/ jwb/edict doc.html

the pivot words by consulting the remaining dic-
tionary.

4.1 Crosslingual Experiment
We compare our approach used for extract-
ing cross-lingual translation pairs against several
baselines. We compare to LLR + Manhattan
(Rapp, 1999) and our variation LLR(P) + Man-
hattan. Additionally, we compare TFIDF(MSO)
+ Cosine, which is the TFIDF measure, whereas
the Term Frequency is normalized using the max-
imal word frequency and the cosine similarity
for comparison suggested in (Fung, 1998). Fur-
thermore, we implemented two variations of this,
TFIDF(MPO) + Cosine and TFIDF(MPO) + Jac-
card coefﬁcient, which were suggested in (Chiao
and Zweigenbaum, 2002). In fact, TFIDF(MPO)
is the TFIDF measure, whereas the Term Fre-
quency is normalized using the maximal word pair
frequency. The results are displayed in Figure 1.
Our approach clearly outperforms all baselines;
notably it has Top 1 accuracy of 0.14 and Top 20
accuracy of 0.55, which is much better than that
for the best baseline, which is 0.11 and 0.44, re-
spectively.

1:

Crosslingual

Figure
Experiment
MLIT/NHTSA – Percentile Ranking of RPMI
+ Entropy Against Various Previous Suggested
Methods.

We next leave the proposed framework con-
stant, but change the mode of estimating posi-
tive associations and the way to match feature
sets. As alternatives for estimating the proba-
bility that there is a positive association, we test
LLR(P) and LLR. As alternatives for comparing
feature sets, we investigate the matching coef-
ﬁcient (match), cosine similarity (cosine), Tan-
imoto coefﬁcient (tani), and overlap coefﬁcient

!#)"

!#("

!#’"

!#&"

!#%"

!#$"

!"

!"

*+,-"."/012345"

66*7+8.,90:9;90"

<=->=7,+?8.@3AB0C"

<=->=7,D?8.@3AB0C"

66*.,90:9;90"

<=->=7,+?8.E9FF92>"

("

$!"

$("

%!"

%("

25

(over) (Manning and Sch¨utze, 2002). The re-
sult of every combination is displayed concisely
in Table 1 using the median rank18. The cases
in which the median ranks are close to RPMI +
Entropy are magniﬁed in Table 2. We can see
there that RPMI + Entropy, and LLR(P) + En-
tropy perform nearly equally. All other combina-
tions perform worse, especially in Top 1 accuracy.
Finally, LLR(P) presents a clear edge over LLR,
which suggests that indeed only positive associa-
tions seem to matter in a cross-lingual setting.

RPMI
LLR(P)
LLR

Entropy Match
13.0
16.0
23.5

17.0
15.0
22.0

Cosine
24.0
22.5
27.5

Tani Over
36.0
37.5
25.5
34.0
50.5
50.0

Table 1: Crosslingual experiment MLIT/NHTSA
– Evaluation matrix showing the median ranks of
several combinations of association and similarity
measures.

RPMI + Entropy
RPMI + Matching
LLR(P) + Entropy
LLR(P) + Matching

Top 1
0.14
0.08
0.14
0.08

Top 10
0.46
0.41
0.46
0.44

Top 20
0.55
0.57
0.55
0.55

Table 2: Accuracies for crosslingual experiment
MLIT/NHTSA.

Finally we conduct an another experiment using
the corpora pair Mainichi/Reuters which is quite
different from MLIT/NHTSA. When comparing
to the best baselines in Table 3 we see that our
approach again performs best. Furthermore, the
experiments displayed in Table 4 suggest that Ro-
bust PMI and pointwise entropy are better choices
for positive association measurement and similar-
ity measurement, respectively. We can see that

RPMI + Entropy
LLR(P) + Manhattan
TFIDF(MPO) + Cos

Top 1
0.15
0.10
0.05

Top 10
0.38
0.26
0.12

Top 20
0.46
0.33
0.18

Table 3: Accuracies for crosslingual experiment
Mainichi/Reuters – Comparison to best baselines.

18A median rank of i, means that 50% of the correct trans-

lations have a rank higher than i.

RPMI + Entropy
RPMI + Matching
LLR(P) + Entropy
LLR(P) + Matching

Top 1
0.15
0.08
0.13
0.08

Top 10
0.38
0.30
0.36
0.29

Top 20
0.46
0.35
0.47
0.37

Table 4: Accuracies for crosslingual experiment
Mainichi/Reuters – Comparison to alternatives.

the overall best baseline turns out to be LLR(P) +
Manhattan. Comparing the rank from each word
from the gold-standard pairwise, we see that our
approach, RPMI + Entropy, is signiﬁcantly better
than this baseline in MLIT/NHTSA as well as in
Mainichi/Reuters.19

4.2 Analysis
In this section, we provide two representative ex-
amples extracted from the previous experiments
which sheds light into a weakness of the stan-
dard feature vector approach which was used as a
baseline before. The two example queries and the
corresponding responses of LLR(P) + Manhattan
and our approach are listed in Table 5. Further-
more in Table 6 we list the pivot words with the
highest degree of association (here LLR values)
for the query and its correct translation. We can
see that a query and its translation shares some
pivots which are associated with statistical signif-
icance20. However it also illustrates that the ac-
tual LLR value is less insightful and can hardly be
compared across these two corpora.

Let us analyze the two examples in more de-
tail. In Table 6, we see that the ﬁrst query ギア
“gear”21 is highly associated with 入れる “shift”.
However, on the English side we see that gear is
most highly associated with the pivot word gear.
Note that here the word gear is also a pivot word
corresponding to the Japanese pivot word 歯車
“gear (wheel)”.22 Since in English the word gear
(shift) and gear (wheel) is polysemous, the surface
forms are the same leading to a high LLR value of

19Using pairwise test with p-value 0.05.
20Note that for example, an LLR value bigger than 11.0
means the chances that there is no association is smaller than
0.001 using that LLR ∼ χ2.

21For a Japanese word, we write the English translation

which is appropriate in our context, immediately after it.

22In other words, we have the entry (歯車, gear) in our
dictionary but not the entry (ギア, gear). The ﬁrst pair is
used as a pivot, the latter word pair is what we try to ﬁnd.

26

gear. Finally, the second example query ペダル
“pedal” shows that words which, not necessarily
always, but very often co-occur, can cause rela-
tively high LLR values. The Japanese verb 踏む
“to press” is associated with ペダル with a high
LLR value – 4 times higher than 戻る “return”
– which is not reﬂected on the English side. In
summary, we can see that in both cases the degree
of associations are rather different, and cannot be
compared without preprocessing. However, it is
also apparent that in both examples a simple L1
normalization of the degree of associations does
not lead to more similarity, since the relative dif-
ferences remain.

ギア “gear”
Method Top 3 candidates
jolt, lever, design
baseline
reverse, gear, lever
ﬁltering
ペダル “pedal”
Method Top 3 candidates
baseline mj, toyota, action
ﬁltering

pedal, situation, occasion

Rank
284
2

Rank
176
1

Table 5: List of translation suggestions using
LLR(P) + Manhattan (baseline) and our method
(ﬁltering). The third column shows the rank of
the correct translation.

ギア

Pivots
入る “shift”
入れる “shift”
抜ける “come out”
ペダル

Pivots
踏む “press”
戻る “return”
足 “foot”

LLR(P)
154
144
116

LLR(P)
628
175
127

gear

pedal

LLR(P)
7064
1270
314

LLR(P)
1150
573
235

Pivots
gear
shift
reverse

Pivots
ﬂoor
stop
press

Table 6: Shows the three pivot words which have
the highest degree of association with the query
(left side) and the correct translation (right side).

5 Conclusions
We introduced a new method to compare con-
text similarity across comparable corpora using a
Bayesian estimate for PMI (Robust PMI) to ex-
tract positive associations and a similarity mea-
surement based on the hypergeometric distribu-
tion (measuring pointwise entropy). Our experi-

ments show that, for ﬁnding cross-lingual trans-
lations, the assumption that words with similar
meaning share positive associations with the same
words is more appropriate than the assumption
that the degree of association is similar. Our ap-
proach increases Top 1 and Top 20 accuracy of
up to 50% and 39% respectively, when compared
to several previous methods. We also analyzed
the two components of our method separately. In
general, Robust PMI yields slightly better per-
formance than the popular LLR, and, in contrast
to LLR, allows to extract positive associations as
well as to include prior information in a principled
way. Pointwise entropy for comparing feature sets
cross-lingually improved the translation accuracy
clearly when compared with standard similarity
measurements.

Acknowledgment
We thank Dr. Naoaki Okazaki and the anony-
mous reviewers for their helpful comments. Fur-
thermore we thank Daisuke Takuma, IBM Re-
search - Tokyo, for mentioning previous work
on statistical corrections for PMI. This work was
partially supported by Grant-in-Aid for Specially
Promoted Research (MEXT, Japan). The ﬁrst au-
thor is supported by the MEXT Scholarship and
by an IBM PhD Scholarship Award.

References
Chiao, Y.C. and P. Zweigenbaum. 2002. Looking
for candidate translational equivalents in special-
ized, comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguis-
tics, pages 1–5. International Committee on Com-
putational Linguistics.

D´ejean, H., ´E. Gaussier, and F. Sadat. 2002. An ap-
proach based on multilingual thesauri and model
combination for bilingual lexicon extraction.
In
Proceedings of
the International Conference on
Computational Linguistics, pages 1–7. International
Committee on Computational Linguistics.

Dunning, T. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61–74.

Fung, P.

1998. A statistical view on bilingual
lexicon extraction:
from parallel corpora to non-
parallel corpora. Lecture Notes in Computer Sci-
ence, 1529:1–17.

27

Okazaki, N., Y. Tsuruoka, S. Ananiadou, and J. Tsu-
jii. 2008. A discriminative candidate generator for
string transformations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 447–456. Association for Com-
putational Linguistics.

Pham-Gia, T. 2000. Distributions of the ratios of in-
dependent beta variables and applications. Com-
munications in Statistics. Theory and Methods,
29(12):2693–2715.

Rapp, R.

1999. Automatic identiﬁcation of word
translations from unrelated English and German
corpora.
In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
pages 519–526. Association for Computational Lin-
guistics.

Ross, T.D. 2003. Accurate conﬁdence intervals for
binomial proportion and Poisson rate estimation.
Computers in Biology and Medicine, 33(6):509–
531.

Tsuruoka, Y., Y. Tateishi, J. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. De-
veloping a robust part-of-speech tagger for biomed-
ical text.
Lecture Notes in Computer Science,
3746:382–392.

Wilcox, R.R. 2009. Basic Statistics: Understanding
Conventional Methods and Modern Insights. Ox-
ford University Press.

Garera, N., C. Callison-Burch, and D. Yarowsky.
2009. Improving translation lexicon induction from
monolingual corpora via dependency contexts and
part-of-speech equivalences. In Proceedings of the
Conference on Computational Natural Language
Learning, pages 129–137. Association for Compu-
tational Linguistics.

Gaussier, E., J.M. Renders, I. Matveeva, C. Goutte,
and H. Dejean. 2004. A geometric view on bilin-
gual lexicon extraction from comparable corpora.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 526–
533. Association for Computational Linguistics.

Haghighi, A., P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 771–779. Association for Computa-
tional Linguistics.

Johnson, M. 2001. Trading recall for precision with
conﬁdence-sets. Technical report, Brown Univer-
sity.

Koehn, P. and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of ACL Workshop on Unsupervised Lexical Acquisi-
tion, volume 34, pages 9–16. Association for Com-
putational Linguistics.

Kudo, T., K. Yamamoto, and Y. Matsumoto. 2004.
Applying conditional random ﬁelds to Japanese
morphological analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 230–237. Association for Com-
putational Linguistics.

Manning, C.D. and H. Sch¨utze. 2002. Foundations
of Statistical Natural Language Processing. MIT
Press.

Moore, R.C. 2004. On log-likelihood-ratios and the
signiﬁcance of rare events.
In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 333–340. Association for
Computational Linguistics.

Moore, R.C. 2005. A discriminative framework for
bilingual word alignment.
In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81–88. Association for Computational
Linguistics.

Morin, E., B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining-using brain,
not brawn comparable corpora. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics, volume 45, pages 664–671. As-
sociation for Computational Linguistics.

