



















































HIT-CIR: An Unsupervised WSD System Based on Domain Most Frequent Sense Estimation


Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 407–410,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics

HIT-CIR: An Unsupervised WSD System Based on Domain Most
Frequent Sense Estimation

Yuhang Guo, Wanxiang Che, Wei He, Ting Liu, Sheng Li
Harbin Institute of Technolgy
Harbin, Heilongjiang, PRC
yhguo@ir.hit.edu.cn

Abstract
This paper presents an unsupervised sys-
tem for all-word domain specific word
sense disambiguation task. This system
tags target word with the most frequent
sense which is estimated using a thesaurus
and the word distribution information in
the domain. The thesaurus is automati-
cally constructed from bilingual parallel
corpus using paraphrase technique. The
recall of this system is 43.5% on SemEval-
2 task 17 English data set.

1 Introduction

Tagging polysemous word with its most frequent
sense (MFS) is a popular back-off heuristic in
word sense disambiguation (WSD) systems when
the training data is inadequate. In past evalua-
tions, MFS from WordNet performed even bet-
ter than most of the unsupervised systems (Snyder
and Palmer, 2004; Navigli et al., 2007).

MFS is usually obtained from a large scale
sense tagged corpus, such as SemCor (Miller et al.,
1994). However, some polysemous words have
different MFS in different domains. For example,
in the Koeling et al. (2005) corpus, target word
coach means “manager” mostly in the SPORTS
domain but means “bus” mostly in the FINANCE
domain. So when the MFS is applied to specific
domains, it needs to be re-estimated.

McCarthy et al. (2007) proposed an unsuper-
vised predominant word sense acquisition method
which obtains domain specific MFS without sense
tagged corpus. In their method, a thesaurus, in
which words are connected with their distribu-
tional similarity, is constructed from the domain
raw text. Word senses are ranked by their preva-
lence score which is calculated using the thesaurus
and the sense inventory.

In this paper, we propose another way to con-
struct the thesaurus. We use statistical machine

Figure 1: The architecture of HIT-CIR

translation (SMT) techniques to extract paraphrase
pairs from bilingual parallel text. In this way, we
avoid calculating similarities between every pair
of words and could find semantic similar words or
compounds which have dissimilar distributions.

Our system is comprised of two parts: the word
sense ranking part and the word sense tagging part.
Senses are ranked according to their prevalence
score in the target domain, and the predominant
sense is used to tag the occurrences of the target
word in the test data. The architecture of this sys-
tem is shown in Figure 1.

The word sense ranking part includes following
steps.

1. Tag the POS of the background text, count
the word frequency in each POS, and get the
polysemous word list of the POS.

2. Using SMT techniques to extract phrase table

407



Figure 2: Word sense ranking for the noun backbone

from the bilingual corpus. Extract the para-
phrases (called as neighbor words) with the
phrase table for each word in the polysemous
word list.

3. Calculate the prevalence score of each sense
of the target words, rank the senses with the
score and obtain the predominant sense.

We applied our system on the English data set
of SemEval-2 specific domain WSD task. This
task is an all word WSD task in the environ-
mental domain. We employed the domain back-
ground raw text provided by the task organizer as
well as the English WordNet 3.0 (Fellbaum, 1998)
and the English-Spanish parallel corpus from Eu-
roparl (Koehn, 2005).

This paper is organized as follows. Section 2
introduces how to rank word senses. Section 3
presents how to obtain the most related words of
the target words. We describe the system settings
in Section 4 and offer some discussions in Sec-
tion 5.

2 Word Sense Ranking

In our method, word senses are ranked according
to their prevalence score in the specific domain.
According to the assumption of McCarthy et al.
(2007), the prevalence score is affected by the fol-
lowing two factors: (1) The relatedness score be-
tween a given sense of the target word and the
target word’s neighbor word. (2) The similarity
between the target word and its neighbor word.
In addition, we add another factor, (3) the impor-
tance of the neighbor word in the specific domain.

In this paper, “neighbor words” means the words
which are most semantically similar to the target
word.

Figure 2 illustrates the word sense ranking pro-
cess of noun backbone. The contribution of a
neighbor word to a given word sense is measured
by the similarity between them and weighted by
the importance of the neighbor word in the tar-
get domain and the relatedness between the neigh-
bor word and the target word. Sum up the con-
tributions of each neighbor words, and we get the
prevalence score of the word sense.

Formally, the prevalence score of sense si of a
target word w is assigned as follows:

ps(w, si) =
∑

nj∈Nw
rs(w, nj) × ns(si, nj) × dw(nj)

(1)
where

ns(si, nj) =
sss(si, nj)∑

si′∈senses(w) sss(si′ , nj)
, (2)

sss(si, nj) = maxsx∈senses(nj)sss
′(si, sx). (3)

rs(w, nj) is the relatedness score between w and
a neighbor word nj . Nw = {n1, n2, . . . , nk}
is the top k relatedness score neighbor word set.
ns(si, nj) is the normalized form of the sense sim-
ilarity score between sense si and the neighbor
word nj (i.e. sss(si, nj)). We define this score
with the maximum WordNet similarity score be-
tween si and the senses of nj (i.e. sss′(si, nj)).
In our system, lesk algorithm is used to measure
the sense similarity score between word senses.

408



Figure 3: Finding the neighbor words of noun backbone

The similarity of this algorithm is the count of
the number of overlap words in the gloss or the
definition of the senses (Banerjee and Pedersen,
2002). The domain importance weight dw(nj) is
assigned with the count of nj in the domain back-
ground corpus. For the neighbor word that does
not occur in the domain background text, we use
the add-one strategy. We will describe how to ob-
tain nj and rs in Section 3.

3 Thesaurus Construction

The neighbor words of the target word as well as
the relatedness score are obtained by extracting
paraphrases from bilingual parallel texts. When
a word is translated from source language to tar-
get language and then translated back to the source
language, the final translation may have the same
meaning to the original word but with different ex-
pressions (e.g. different word or compound). The
translation in the same language could be viewed
as a paraphrase term or, at least, related term of the
original word.

For example, in Figure 3, English noun back-
bone can be translated to columna, columna verte-
bral, pilar and convicciones etc. in Spanish, and
these words also have other relevant translations
in English, such as vertebral column, column, pil-
lar and convictions etc., which are semantically re-
lated to the target word backbone.

We use a statistical machine translation sys-
tem to calculate the translation probability from
English to another language (called as pivot lan-
guage) as well as the translation probability from
that language to English. By multiplying these
two probabilities, we get a paraphrase probabil-
ity. This method was defined in (Bannard and
Callison-Burch, 2005).

In our system, we choose the top k paraphrases

as the neighbor words of the target word, which
have the highest paraphrase probability. Note that
there are two directions of the paraphrase, from
target word to its neighbor word and from the
neighbor word to the target word. We choose
the paraphrase score of the former direction as
the relatedness score (rs). Because the higher
of the score in this direction, the target word is
more likely paraphrased to that neighbor word,
and hence the prevalence of the relevant target
word sense will be higher than other senses. For-
mally, the relatedness score is given by

rs(w, nj) =
∑
f

p(f |w)p(nj |f), (4)

where f is the pivot language word.
We use the English-Spanish parallel text from

Europarl (Koehn, 2005). We choose Spanish as
the pivot language because in the both directions
the BLEU score of the translation between English
and Spanish is relatively higher than other English
and other languages (Koehn, 2005).

4 Data set and System Settings

The organizers of the SemEval-2 specific domain
WSD task provide no training data but raw back-
ground data in the environmental domain. The En-
glish background data is obtained from the offi-
cial web site of World Wide Fund (WWF), Euro-
pean Centre for Nature Conservation (ECNC), Eu-
ropean Commission and the United Nations Eco-
nomic Commission for Europe (UNECE). The
size of the raw text is around 15.5MB after sim-
ple text cleaning. The test data is from WWF and
ECNC, and contains 1398 occurrence of 436 tar-
get words.

For the implementation, we used bpos (Shen et
al., 2007) for the POS tagging. The maximum

409



number of the neighbor word of each target word k
was set to 50. We employed Giza++1 and Moses2

to get the phrase table from the bilingual paral-
lel corpus. The WordNet::Similarity package3 was
applied for the implement of the lesk word sense
similarity algorithm.

For the target word that is not in the polysemous
word list, we use the MFS from WordNet as the
back-off method.

5 Discussion and Future Work

The recall of our system is 43.5%, which is lower
than that of the MFS baseline, 50.5% (Agirre et
al., 2010). The baseline uses the most frequent
sense from the SemCor corpus (i.e. the MFS of
WordNet). This means that for some target words,
the MFS from SemCor is better than the domain
MFS we estimated in the environmental domain.
In the future, we will analysis errors in detail to
find the effects of the domain on the MFS.

For the domain specific task, it is better to use
parallel text in the domain of the test data in our
method. However, we didn’t find any available
parallel text in the environmental domain yet. In
the future, we will try some parallel corpus acqui-
sition techniques to obtain relevant corpus for en-
vironmental domain for our method.

Acknowledgments

This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the “863” National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).

References
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-

baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluations (SemEval-2010), Association for Com-
putational Linguistics.

Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ’02: Proceedings
1http://www.fjoch.com/GIZA++.html
2http://www.statmt.org/moses/
3http://wn-similarity.sourceforge.net/

of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136–145, London, UK. Springer-Verlag.

Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
’05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 597–
604, Morristown, NJ, USA. Association for Compu-
tational Linguistics.

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit, Phuket, Thailand.

Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 419–426, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.

Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553–590, December.

G. A. Miller, C. Leacock, R. Tengi, and R. Bunker.
1994. A semantic concordance. In Proc. ARPA
Human Language Technology Workshop ’93, pages
303–308, Princeton, NJ, March. distributed as Hu-
man Language Technology by San Mateo, CA: Mor-
gan Kaufmann Publishers.

Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 30–35, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.

Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760–767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.

Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41–43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.

410


