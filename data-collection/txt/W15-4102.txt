



















































Multi-system machine translation using online APIs for English-Latvian


Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 6‚Äì10,
Beijing, China, July 31, 2015. c¬©2015 Association for Computational Linguistics

Multi-system machine translation using online APIs 

for English-Latvian 

 

 

Matƒ´ss Rikters 

University of Latvia 

19 Raina Blvd., 

Riga, Latvia 

matiss@lielakeda.lv 

 

  

 

Abstract 

This paper describes a hybrid machine 

translation (HMT) system that employs 

several online MT system application pro-

gram interfaces (APIs) forming a Multi-

System Machine Translation (MSMT) ap-

proach. The goal is to improve the auto-

mated translation of English ‚Äì Latvian 

texts over each of the individual MT APIs. 

The selection of the best hypothesis trans-

lation is done by calculating the perplexity 

for each hypothesis. Experiment results 

show a slight improvement of BLEU 

score and WER (word error rate). 

1 Introduction 

MSMT is a subset of HMT where multiple MT 

systems are combined in a single system to com-

plement each other‚Äôs weaknesses in order to boost 

the accuracy level of the translations. Other types 

of HMT include modifying statistical MT (SMT) 

systems with rule-based MT (RBMT) generated 

output and generating rules for RBMT systems 

with the help of SMT [19]. 

MSMT involves usage of multiple MT sys-

tems in parallel and combining their output with 

the aim to produce better result as for each of the 

individual systems. It is a relatively new branch of 

MT and interest from researchers has emerged 

more widely during the last 10 years. And even 

now such systems mostly live as experiments in 

lab environments instead of real, live, functional 

MT systems. Since no single system can be per-

fect and different systems have different ad-

vantages over others, a good combination must 

lead towards better overall translations. 

There are several recent experiments that use 

MSMT. Ahsan and Kolachina [1] describe a way 

of combining SMT and RBMT systems in multi-

ple setups where each one had input from the 

SMT system added in a different phase of the 

RBMT system.  

Barrault [3] describes a MT system combina-

tion method where he combines confusion net-

works of the best hypotheses from several MT 

systems into one lattice and uses a language model 

for decoding the lattice to generate the best hy-

pothesis. 

Mellebeek et al. [12] introduce a hybrid MT 

system that utilised online MT engines for 

MSMT. They introduce a system that at first at-

tempts to split sentences into smaller parts for eas-

ier translation by the means of syntactic analysis, 

then translate each part with each individual MT 

system while also providing some context, and fi-

nally create the output from the best scored trans-

lations of each part (they use three heuristics for 

selecting the best translation). 

Most of the research is done English ‚Äì Hindi, 

Arabic ‚Äì English and English ‚Äì Spanish language 

pairs in their experiments. Where it concerns Eng-

lish - Latvian machine translation, no such exper-

iments have been conducted.  

This paper presents a first attempt in using an 

MSMT approach for the under-resourced English-

Latvian language pair. Furthermore the first re-

sults of this hybrid system are analysed and com-

pared with human evaluation. The experiments 

described use multiple combinations of outputs 

from two MT systems and one experiment uses 

three different MT systems. 

6



2 System description 

The main system consists of three major constitu-

ents ‚Äì tokenization of the source text, the acquisi-

tion of a translation via online APIs and the selec-

tion of the best translation from the candidate hy-

potheses. A visualized workflow of the system is 

presented in Figure 1. 

Currently the system uses three translation 

APIs (Google Translate1 , Bing Translator2  and 

LetsMT3), but it is designed to be flexible and 

adding more translation APIs has been made sim-

ple. Also, it is initially set to translate from Eng-

lish into Latvian, but the source and target lan-

guages can also be changed to any language pair 

supported by the APIs. 

 

Sentence tokenization

Translation with APIs

Google Translate Bing Translator LetsMT

Selection of the best 
translation

Output

 
Figure 1: General workflow of the translation process 

2.1 API description 

Currently there are three online translation APIs 

included in the project ‚Äì Google Translate, Bing 

Translator and LetsMT. These specific APIs were 

chosen for their public availability and descriptive 

documents as well as the wide range of languages 

that they offer. One of the main criteria when 

searching for translation APIs was the option to 

translate from English to Latvian. 

                                                 
1 Google Translate API - 

https://cloud.google.com/translate/ 
2 Bing Translator Control - http://www.bing.com/dev/en-
us/translator 

2.2 Selection of the final translation 

The selection of the best translation is done by cal-

culating the perplexity of each hypothesis transla-

tion using KenLM [8]. First, a language model 

(LM) must be created using a preferably large set 

of training sentences. Then for each machine-

translated sentence a perplexity score represents 

the probability of the specific sequence of words 

appearing in the training corpus used to create the 

LM. Sentence perplexity has been proven to cor-

relate with human judgments close to the BLEU 

score and is a good evaluation method for MT 

without reference translations [7]. It has been also 

used in other previous attempts of MSMT to score 

output from different MT engines as mentioned 

by Callison-Burch et al. [4] and Akiba et al. [2].  

KenLM calculates probabilities based on the 

observed entry with longest matching history ùë§ùëì
ùëõ: 

ùëù(ùë§ùëõ |ùë§1
ùëõ‚àí1) = ùëù(ùë§ùëõ |ùë§ùëì

ùëõ‚àí1) ‚àè ùëè(ùë§ùëñ
ùëõ‚àí1)

ùëì‚àí1

ùëñ=1

 

where the probability ùëù(ùë§ùëõ |ùë§ùëì
ùëõ‚àí1) and backoff 

penalties ùëè(ùë§ùëñ
ùëõ‚àí1) are given by an already-esti-

mated language model. Perplexity is then calcu-

lated using this probability: 

 
where given an unknown probability distribution 

p and a proposed probability model q, it is evalu-

ated by determining how well it predicts a sepa-

rate test sample x1, x2... xN drawn from p. 

3 System usage 

The source code with working examples and sam-

ple data has been made open source and is availa-

ble on GitHub4. To run the basic setup a Linux 

system is required with PHP and cURL installed. 

Before running, the user needs to edit the 

MSHT.php file and add his Google Translate, 

Bing Translator and LetsMT credentials as well as 

specify source and target languages (the defaults 

are set for English ‚Äì Latvian). 

The data required for an experiment is a source 

language text as a plain text file and a language 

model. The LM can be generated via KenLM us-

ing a large monolingual training corpus. The LM 

should be converted to binary format for more ef-

ficient usage. 

3 LetsMT! Open Translation API - 
https://www.letsmt.eu/Integration.aspx 
4 Multi-System-Hybrid-Translator - 
https://github.com/M4t1ss/Multi-System-Hybrid-Translator 

7



4 Experiments 

 The first experiments were conducted on the Eng-

lish ‚Äì Latvian part of the JRC Acquis corpus ver-

sion 2.2 [18] from which both the language model 

and the test data were retrieved. The test data con-

tained 1581 randomly selected sentences. The lan-

guage model was created using KenLM with or-

der 5. 

 Translations were obtained from each API in-

dividually, combining each two APIs and lastly 

combining all three APIs. Thereby forming 7 dif-

ferent variants of translations. Google Translate 

and Bing Translator APIs were used with the de-

fault configuration and the LetsMT API used the 

configuration of TB2013 EN-LV v035. 

 Evaluation on each of the seven outputs was 

done with three scoring methods ‚Äì BLEU [13], 

TER (translation edit rate) [16] and WER [9]. The 

resulting translations were inspected with a mod-

ified iBLEU tool [11] that allowed to determine 

which system from the hybrid setups was chosen 

to get the specific translation for each sentence. 

The results of the first translation experiment 

are summarized in Table 2. Surprisingly all hybrid 

systems that include the LetsMT API produce 

lower results than the baseline LetsMT system. 

However the combination of Google Translate 

and Bing Translator shows improvements in 

BLEU score and WER compared to each of the 

baseline systems. 

The table also shows the percentage of transla-

tions from each API for the hybrid systems. Alt-

hough according to scores the LetsMT system was 

by far better than the other two, it seems that the 

language model was reluctant to favor its transla-

tions. 

 Since the systems themselves are more of a 

general domain and the first test was conducted on 

a legal domain corpus, a second experiment was 

conducted on a smaller data set containing 512 

sentences of a general domain [15]. In this exper-

iment only the BLEU score was calculated as it is 

shown in Table 1. 

 

System BLEU 

Google Translate 24.73 

Bing Translator 22.07 

LetsMT 32.01 

Hybrid Google + Bing 23.75 

Hybrid Google + LetsMT 28.94 

Hybrid LetsMT + Bing 27.44 

Hybrid Google + Bing + LetsMT 26.74 
Table 1: Second experiment results 

System BLEU TER WER 
Translations selected  

Google Bing LetsMT Equal 

Google Translate 16.92 47.68 58.55 100 % - - - 

Bing Translator 17.16 49.66 58.40 - 100 % - - 

LetsMT 28.27 36.19 42.89 - - 100 % - 

Hybrid Google + Bing 17.28 48.30 58.15 50.09 % 45.03 % - 4.88 % 

Hybrid Google + LetsMT 22.89 41.38 50.31 46.17 % - 48.39 % 5.44 % 

Hybrid LetsMT + Bing 22.83 42.92 50.62 - 45.35 % 49.84 % 4.81 % 

Hybrid Google + Bing + LetsMT 21.08 44.12 52.99 28.93 % 34.31 % 33.98 % 2.78 % 
Table 2: First experiment results

  

System User 1 User 2 User 3 User 4 User 5 
AVG 

user 
Hybrid BLEU 

Bing 21,88% 53,13% 28,13% 25,00% 31,25% 31,88% 28,93% 16.92 

Google 28,13% 25,00% 25,00% 28,13% 46,88% 30,63% 34,31% 17.16 

LetsMT 50,00% 21,88% 46,88% 46,88% 21,88% 37,50% 33,98% 28.27 

Table 3: Native speaker evaluation results

 

  

                                                 
5 https://www.letsmt.eu/TranslateText.aspx?id=smt-
e3080087-866f-498b-977d-63ea391ba61e 

8



 

5 Human evaluation 

A random 2% (32 sentences) of the translations 

from the first experiment were given to five native 

Latvian speakers with an instruction to choose the 

best translation (just like the hybrid system 

should). The results are shown in Table 3. Com-

paring the evaluation results to the BLEU scores 

and the selections made by the hybrid MT a ten-

dency towards the LetsMT translation can be ob-

served among the user ratings and BLEU score 

that is not visible from the selection of the hybrid 

method. 

6 Conclusion 

This short paper described a machine translation 

system combination approach using public online 

MT system APIs. The main focus was to gather 

and utilize only the publically available APIs that 

support translation for the under-resourced Eng-

lish-Latvian language pair. 

One of the test cases showed an improvement 

in BLEU score and WER over the best baseline.  

In all hybrid systems that included the LetsMT 

API a decline in overall translation quality was 

observed. This can be explained by scale of the 

engines - the Bing and Google systems are more 

general, designed for many language pairs, 

whereas the MT system in LetsMT was specifi-

cally optimized for English ‚Äì Latvian translations. 

This problem could potentially be resolved by cre-

ating a language model using a larger training cor-

pus and a higher order for more precision. 

7 Future work 

The described system currently is only at the be-

ginning of its lifecycle and further improvements 

are planned ahead. There are several methods that 

could improve the current system combination ap-

proach. One way is the application of other possi-

ble methods for selection of the best hypothesis.  

For instance ‚Äì the QuEst framework [17] can 

be used to extract various linguistic features for 

each sentence in the training corpora. Afterwards 

using the features along with a quality rating for 

each sentence a machine learning algorithm can 

train a model for predicting translation quality. 

                                                 
6 HTK Speech Recognition Toolkit - 
http://htk.eng.cam.ac.uk/ 

The resulting model can then evaluate each candi-

date translation in a multi-system setup instead of 

perplexity. 

Another path for hypothesis selection is the 

creation of a confusion network as described by 

Rosti, et al. [14]. This can be done with tools from 

either the Hidden Markov Toolkit6 or the NIST 

Scoring Toolkit7. 

It would also be worth looking into any other 

forms of evaluating translations that do not re-

quire reference translations or MT quality estima-

tion. For instance an evaluation using n-gram co-

occurrence statistics as mentioned by Doddington 

[6] and Lin et al. [10] or quality estimation using 

tree kernels introduced by Cohn et al. [5]. 

Acknowledgements 

This research work was supported by the research 

project ‚ÄúOptimization methods of large scale sta-

tistical models for innovative machine translation 

technologies‚Äù, project financed by The State Edu-

cation Development Agency (Latvia) and Euro-

pean Regional Development Fund, contract No. 

2013/0038/2DP/2.1.1.1.0/13/APIA/ VI-AA/029. 

The author would also like to thank Inguna 

Skadi≈Üa for advices and contributions, and the 

anonymous reviewers for their comments and 

suggestions. 

Reference 

[1] Ahsan, A., and P. Kolachina. "Coupling Statisti-
cal Machine Translation with Rule-based Trans-

fer and Generation, AMTA-The Ninth Confer-

ence of the Association for Machine Translation 

in the Americas." Denver, Colorado (2010). 

[2] Akiba, Yasuhiro, Taro Watanabe, and Eiichiro 
Sumita. "Using language and translation models 

to select the best among outputs from multiple 

MT systems." Proceedings of the 19th interna-

tional conference on Computational linguistics-

Volume 1. Association for Computational Lin-

guistics, 2002. 

[3] Barrault, Lo√Øc. "MANY: Open source machine 
translation system combination." The Prague 

Bulletin of Mathematical Linguistics 93 (2010): 

147-155. 

[4] Callison-Burch, Chris, and Raymond S. 
Flournoy. "A program for automatically selecting 

the best output from multiple machine translation 

7 NIST Scoring Toolkit Version 0.1  - 
http://www1.icsi.berkeley.edu/Speech/docs/sctk-

1.2/sctk.htm 

9



engines." Proceedings of the Machine Translation 

Summit VIII. 2001. 

[5] Cohn, Trevor, and Lucia Specia. "Modelling An-
notator Bias with Multi-task Gaussian Processes: 

An Application to Machine Translation Quality 

Estimation." ACL (1). 2013. 

[6] Doddington, George. "Automatic evaluation of 
machine translation quality using n-gram co-oc-

currence statistics." Proceedings of the second in-

ternational conference on Human Language 

Technology Research. Morgan Kaufmann Pub-

lishers Inc., 2002. 

[7] Gamon, Michael, Anthony Aue, and Martine 
Smets. "Sentence-level MT evaluation without 

reference translations: Beyond language model-

ing." Proceedings of EAMT. 2005. 

[8] Heafield, Kenneth. "KenLM: Faster and smaller 
language model queries." Proceedings of the 

Sixth Workshop on Statistical Machine Transla-

tion. Association for Computational Linguistics, 

2011. 

[9] Klakow, Dietrich, and Jochen Peters. "Testing the 
correlation of word error rate and perplexity." 

Speech Communication 38.1 (2002): 19-28. 

[10] Lin, Chin-Yew, and Eduard Hovy. "Automatic 
evaluation of summaries using n-gram co-occur-

rence statistics." Proceedings of the 2003 Confer-

ence of the North American Chapter of the Asso-

ciation for Computational Linguistics on Human 

Language Technology-Volume 1. Association for 

Computational Linguistics, 2003. 

[11] Madnani, Nitin. "iBLEU: Interactively debug-
ging and scoring statistical machine translation 

systems." Semantic Computing (ICSC), 2011 

Fifth IEEE International Conference on. IEEE, 

2011. 

[12] Mellebeek, Bart, et al. "Multi-engine machine 
translation by recursive sentence decomposition." 

(2006). 

[13] Papineni, Kishore, et al. "BLEU: a method for au-
tomatic evaluation of machine translation." Pro-

ceedings of the 40th annual meeting on associa-

tion for computational linguistics. Association for 

Computational Linguistics, 2002. 

[14] Rosti, Antti-Veikko I., et al. "Combining Outputs 
from Multiple Machine Translation Systems." 

HLT-NAACL. 2007. 

[15] Skadi≈Üa, Inguna, et al. "A Collection of Compa-
rable Corpora for Under-resourced Languages." 

Proceedings of the Fourth International Confer-

ence Baltic HLT 2010. 2010. 

[16] Snover, Matthew, et al. "A study of translation 
edit rate with targeted human annotation." Pro-

ceedings of association for machine translation in 

the Americas. 2006. 

[17] Specia, Lucia, et al. "QuEst-A translation quality 
estimation framework." ACL (Conference Sys-

tem Demonstrations). 2013. 

[18] Steinberger, Ralf, et al. "The JRC-Acquis: A mul-
tilingual aligned parallel corpus with 20+ lan-

guages." arXiv preprint cs/0609058 (2006). 

[19] Thurmair, Gregor. "Comparingdifferentarchitec-
turesof hybridMachineTranslationsystems." 

(2009). 

10


