



















































Data-driven sentence generation with non-isomorphic trees


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Data-driven sentence generation with non-isomorphic trees

Miguel Ballesteros1 Bernd Bohnet2 Simon Mille1 Leo Wanner1,3
1Natural Language Processing Group, Pompeu Fabra University, Barcelona, Spain

2Google Inc.
3Catalan Institute for Research and Advanced Studies (ICREA)

1,3{name.lastname}@upf.edu 2bohnetbd@google.com

Abstract

Abstract structures from which the generation
naturally starts often do not contain any func-
tional nodes, while surface-syntactic struc-
tures or a chain of tokens in a linearized tree
contain all of them. Therefore, data-driven
linguistic generation needs to be able to cope
with the projection between non-isomorphic
structures that differ in their topology and
number of nodes. So far, such a projection
has been a challenge in data-driven genera-
tion and was largely avoided. We present
a fully stochastic generator that is able to
cope with projection between non-isomorphic
structures. The generator, which starts from
PropBank-like structures, consists of a cas-
cade of SVM-classifier based submodules that
map in a series of transitions the input struc-
tures onto sentences. The generator has been
evaluated for English on the Penn-Treebank
and for Spanish on the multi-layered Ancora-
UPF corpus.

1 Introduction

Applications such as machine translation that inher-
ently draw upon sentence generation increasingly
deal with deep meaning representations; see, e.g.,
(Aue et al., 2004; Jones et al., 2012; Andreas et al.,
2013). Deep representations tend to differ in their
topology and number of nodes from the correspond-
ing surface structures since they do not contain, e.g.,
any functional nodes, while syntactic structures or
chains of tokens in linearized trees do. This means
that sentence generation needs to be able to cope

with the projection between non-isomorphic struc-
tures. However, most of the recent work in data-
driven sentence generation still avoids this chal-
lenge. Some systems focus on syntactic generation
(Bangalore and Rambow, 2000; Langkilde-Geary,
2002; Filippova and Strube, 2008) or linearization
and inflection (Filippova and Strube, 2007; He et
al., 2009; Wan et al., 2009; Guo et al., 2011a), and
avoid thus the need to cope with this projection all
together; some use a rule-based module to handle
the projection between non-isomorphic structures
(Knight and Hatzivassiloglou, 1995; Langkilde and
Knight, 1998; Bohnet et al., 2011); and some adapt
the meaning structures to be isomorphic with syn-
tactic structures (Bohnet et al., 2010). However, it is
obvious that a “syntacticization” of meaning struc-
tures can be only a temporary workaround and that a
rule-based module raises the usual questions of cov-
erage, maintenance and portability.

In this paper, we present a fully stochastic gener-
ator that is able to cope with the projection between
non-isomorphic structures.1 Such a generator can
be used as a stand-alone application and also, e.g.,
in text simplification (Klebanov et al., 2004) or deep
machine translation (Jones et al., 2012) (where the
transfer is done at a deep level). In abstractive sum-
marization, it facilitates the generation of the sum-
maries, and in extractive summarization a better sen-
tence fusion.2

1The data-driven sentence generator is available
for public downloading at https://github.com/
talnsoftware/deepgenerator/wiki.

2For all of these applications, the deep representation can
be obtained by a deep parser, such as, e.g., (Ballesteros et al.,
2014a).

387



The generator, which starts from elementary
predicate-argument lexico-structural structures as
used in sentence planning by Stent et al. (2004),
consists of a cascade of Support Vector Machines
(SVM)-classifier based submodules that map the in-
put structures onto sentences in a series of transi-
tions. Following the idea presented in (Ballesteros et
al., 2014b), a separate SVM-classifier is defined for
the mapping of each linguistic category. The genera-
tor has been tested on Spanish with the multi-layered
Ancora-UPF corpus (Mille et al., 2013) and on En-
glish with an extended version of the dependency
Penn TreeBank (Johansson and Nugues, 2007).

The remainder of the paper is structured as
follows. In the next section, we briefly out-
line the fundamentals of sentence generation as
we view it in our work, focusing in partic-
ular on the most challenging part of it: the
transition between the non-isomorphic predicate-
argument lexico-structural structures and surface-
syntactic structures. Section 3 outlines the setup of
our system. Section 4 discusses the experiments we
carried out and the results we obtained. In Section
5, we briefly summarize related work, before in Sec-
tion 6 some conclusions are drawn and future work
is outlined.

2 The Fundamentals

Sentence generation realized in this paper is part
of the sentence synthesis pipeline argued for by
Mel’čuk (1988). It consists of a sequence of two
mappings:

1. Predicate-argument lexico-structural structure
→ Syntactic structure

2. Syntactic Structure→ Linearized structure
Following the terminology in (Mel’čuk, 1988),

we refer to the predicate-argument lexico-structural
structures as “deep-syntactic structures” (DSyntSs)
and to the syntactic structures as “surface-syntactic
structures” (SSyntSs).

While SSyntSs and linearized structures are iso-
morphic, the difference in the linguistic abstraction
of the DSyntSs and SSyntSs leads to divergences
that impede the isomorphy between the two and
make the first mapping a challenge for statistical
generation. Therefore, we focus in this section on

the presentation of the DSyntSs and SSyntSs and the
mapping between them.

2.1 DSyntSs and SSyntSs

2.1.1 Input DSyntSs

DSyntSs are very similar to the PropBank
(Babko-Malaya, 2005) structures and the structures
as used for the deep track of the First Surface Re-
alization Shared Task (SRST, (Belz et al., 2011))
annotations. DSyntSs are connected trees that con-
tain only meaning-bearing lexical items and both
predicate-argument (indicated by Roman numbers:
I, II, III, IV, . . . ) and lexico-structural, or deep-
syntactic, (ATTR(ibutive), APPEND(itive) and CO-
ORD(inative)) relations. In other words, they do
not contain any punctuation and functional nodes,
i.e., governed elements, auxiliaries and determin-
ers. Governed elements such governed prepositions
and subordinating conjunctions are dropped because
they are imposed by sub-categorization restrictions
of the predicative head and void of own meaning—
as, for instance, to in give TO your friend or that
in I know that you will come.3 Auxiliaries do not
appear as nodes in DSyntSs. Rather, the informa-
tion they encode is captured in terms of tense, as-
pect and voice attributes of the corresponding full
verbal nodes. Equally, determiners are substituted
by attribute–value pairs of givenness they encode,
assigned to their governors. See Figure 1 (a) for a
sample DSyntS.4

2.1.2 SSyntSs

SSyntSs are connected dependency trees in which
the nodes are labeled by open or closed class lexical
items and the edges by grammatical function rela-
tions of the type ‘subject’, ‘oblique object’, ‘adver-
bial’, ‘modifier’, etc. A SSyntS is thus a typical de-
pendency tree as used in data-driven syntactic pars-
ing (Hajič et al., 2009) and generation (Belz et al.,
2011). See Figure 1 (b) for illustration of a SSyntS.

3In contrast, on in the bottle is on the table is not dropped
because it is semantic.

4“That” is considered a kind of determiner (to be derived
from the Information Structure). This is the reason to omit it in
the deep structure.

388



(a) (b)

Figure 1: A DSyntS (a) and its corresponding SSyntS (b) for the sentence Almost 1.2 million jobs have been created
by the state in that time

2.2 Projection of DSyntS to SSyntS
In order to project a DSyntS onto its correspond-
ing SSyntS in the course of generation (where both
DSyntSs and their corresponding SSyntSs are stored
in the 14-column CoNLL’09 format), the following
types of actions need to be performed:5

1. Project each node in the DSyntS onto its
SSynS-correspondence. This correspondence can be
a single node, as, e.g., job → [NN] (where NN is a
noun), or a subtree (hypernode, known as syntagm
in linguistics), as, e.g., time→ [DT NN] (where DT
is a determiner and NN a noun) or create→ [VAUX
VAUX VB IN] (where VAUX is an auxiliary, VB a full
verb and IN a preposition). In formal terms, we as-
sume any SSyntS-correspondence to be a hypernode
with a cardinality ≥ 1.
2. Generate the correct lemma for the nodes in
SSyntS that do not have a 1:1 correspondence with
an origin DSyntS node (as DT and VAUX above).6

3. Establish the dependencies within the individual
SSyntS-hypernodes.
4. Establish the dependencies between the SSyntS-

5For Spanish, we apply after the DSyntS–SSyntS transition
in a postprocessing stage rules for the generation of relative pro-
nouns that are implied by the the SSyntS. Since we cannot count
on the annotation of coreference in the training data, we do not
treat other types of referring expressions.

6The lemmas of nodes with 1:1 correspondence are the same
in both structures.

hypernodes (more precisely, between the nodes of
different SSyntS-hypernodes) to obtain a connected
SSyntS-tree.

2.3 Treebanks used in the experiments

2.3.1 Spanish Treebank
For the validation of the performance of our gen-

erator on Spanish, we use the AnCora-UPF tree-
bank, which contains only about 100,000 tokens, but
which has been manually annotated and validated on
the SSyntS- and DSyntS-layers, such that its quality
is rather high. The deep annotation does not con-
tain any functional prepositions since they have been
removed for all predicates of the corpus, and the
DSyntS-relations have been edited following anno-
tation guidelines. AnCora-UPF SSyntSs are anno-
tated with fine-grained dependencies organized in a
hierarchical scheme (Mille et al., 2012), in a similar
fashion as the dependencies of the Stanford Scheme
(de Marneffe et al., 2006).7 Thus, it is possible to
use the full set of labels or to reduce it according to
our needs. We performed preliminary experiments
in order to assess which tag granularity is better
suited for generation and came up with the 31-label
tagset.

7The main difference with the Stanford scheme is that in
AnCora-UPF no distinction is explicitly made between argu-
mental and non-argumental dependencies.

389



2.3.2 English Treebank
For the validation of the generator on English,

we use the dependency Penn TreeBank (about
1,000,000 tokens), which we extend by a DSynt
layer defined by the same deep dependency rela-
tions, features and node correspondences as the
Spanish DSynt layer. The Penn TreeBank DSynt
layer is obtained by a rule-based graph transducer.
The transducer removes definite and indefinite deter-
miners, auxiliaries, THAT complementizers, TO in-
finitive markers, and a finite list of functional prepo-
sitions. The functional prepositions have been man-
ually compiled from the description and examples of
the roles in the PropBank and NomBank annotations
of the 150 most frequent predicates of the corpus. A
dictionary has been built, which contains for each of
the 150 predicates the argument slots (roles) and the
prepositions associated to it, such that given a predi-
cate and a preposition, we know to which role it cor-
responds. Consider, for illustration, Figure 2, which
indicates that for the nominal predicate plan 01, a
dependent introduced by the preposition to corre-
sponds to the second argument of plan 01, while a
dependent introduced by for is its third argument.

Figure 2: A sample (partial) mapping dictionary entry

For each possible surface dependency relation be-
tween a governor and a dependent, a default map-
ping is provided, which is applied if

(i) The syntactic structure fulfills the conditions of
the default mapping (e.g., ‘subject’ is by de-
fault mapped onto ‘I’ unless it is the subject of
a passive verb, in which case it is mapped to the
second argument ‘II’), and

(ii) The pair governor–dependent is not found in
the dictionary; that is, if the dependent of the
SSyntS dependency relation is a preposition
found in the governor’s entry in the dictio-
nary, the information provided in the dictio-
nary is used instead of the default mapping.8

8In the PropBank annotation, a distinction is made between

For instance, in the sentence Sony announced
its plans to hire Mr. Guber, to is a dependent
of plan with the SSyntS dependency NMOD.
NMOD is by default mapped onto the deep re-
lation ATTR, but since in the dictionary entry
of plan it is stated that a dependent introduced
by to is mapped to ‘II’ (cf. Figure 2), II is the
relation that appears in the DSyntS-annotation.

The features definiteness, voice, tense, aspect in
the FEATS column of the CoNLL format capture
the information conveyed by determiners and aux-
iliaries. The conversion procedure maps surface de-
pendency relations as found in the Penn TreeBank
onto the restricted set of deep dependency relations
as described in Section 2.1.1.

The nodes in the original (surface-oriented) and
deep annotations are connected through their IDs.
In the FEATS column of the output CoNLL file,
id0 indicates the deep identifier of a word, while
id1 indicates the ID of the surface node it corre-
sponds to. There are less nodes in DSyntSs than
in SSyntSs since SSyntSs contain all the words of
a sentence. Hence, a DSynt-node can correspond to
several SSyntS nodes. Multiple correspondences are
indicated by the presence of the id2 (id3, id4, etc)
feature in the FEATS column.

3 Deep Generation

3.1 Baselines
Since no available data-driven generator uses as in-
put DSyntSs, we developed as baselines two rule-
based graph transducer generators which produce
for English respectively Spanish the best possible
SSyntSs, using only the information contained in the
starting DSyntS.

The two baseline generators are structured simi-
larly: both contain around 50 graph transducer rules,
separated into two clusters. The first cluster maps
DSyntS-nodes onto SSyntS-nodes, while the second
one handles the introduction of SSyntS dependency
relations between the generated SSyntS-nodes. For
instance, in English, one rule maps DSyntS-nodes
that have a one-to-one correspondence in the SSyntS

external and internal arguments, such that for some predicates
the arguments are numbered starting from ‘0’, and for other
starting from ‘1’. This has been normalized in order to make
all arguments start from ‘1’ for all predicates.

390



if N1 is a Vfin and ((R1,2 == I and N1 is in active
voice and N2 is not by)
or (R1,2 == II and N1 is in passive voice))

if ∃ one-to-one correspondence between NDi and NSi
then introduce SBJ between NS1 and NS2

else
if NS2 is top node of the SSyntS hypernode and
((NS1 is top node of the SSynt hypernode and is
AUX)
or (NS1 is the bottom node of the SSynt

hypernode and is Vfin)
or (NS1 is not top node or bottom node of

the SSynt-hypernode and is AUX))
then introduce SBJ between NS1 and NS2

endif
endif

Figure 3: Sample graph transducer rule

(simple nouns, verbs, adverbs, adjectives, etc.), 22
rules map DSyntS-nodes that have a one-to-many
correspondence in the SSyntS (N → DET+NN, N
→DET+NN+governed PREP, V→AUX+VV, V→
that COMPL+AUX+VV+governed PREP, etc.), and
25 rules generate the dependency relations.9 The
transduction rules apply in two phases, see Figure 3.
During the first phase, all nodes and intra-hypernode
dependencies are created in the output structure.
During the second phase, all inter-hypernode depen-
dencies are established. Since there are one-to-many
DSyntS-SSyntS correspondences, the rules of the
second phase have to ensure that the correct output
nodes are targeted, i.e., that jobs in Figure 1(b) is
made a dependent of have, and not of been or cre-
ated, which all correspond to create in the input.
Consider, for illustration of the complexity of the
rule-based generator, the transduction rule in Figure
3. The rule creates the SSynt dependency relation
SBJ (subject) in a target SSyntS (with a governor
node ND1 and a dependent node ND2 linked by a
deep dependency relation R1,2 in the input DSyntS
and two nodes NS1 and NS2 which correspond to
ND1 and ND2 respectively in the target SSyntS).

The evaluation shows that all straightforward
mappings are performed correctly; English auxil-
iaries, that complementizers, infinitive markers and

9‘N’ stands for “noun”, ‘NN’ for “common noun”, ‘DET’
for “determiner”, ‘PREP’ for “preposition”, ‘V’ for “verb”,
‘AUX’ for “auxiliary verb”, ‘VV’ for “main verb”, and
‘COMPL’ for “complementizer”.

determiners are introduced, and so are Spanish aux-
iliaries, reflexive pronouns, and determiners. That
is, the rules produce well-formed SSyntSs of all
possible combinations of auxiliaries, conjunctions
and/or prepositions for verbs, determiners and/or
prepositions for nouns, adjectives and adverbs.

When there are several possible mappings, the
baseline takes decisions by default. For example,
when a governed preposition must be introduced, we
always introduce the most common one (of in En-
glish, de ‘of’ in Spanish).

3.2 Data-Driven Generator
The data-driven generator is defined as a tree trans-
ducer framework that consists of a cascade of 6 data-
driven small tasks; cf. Figure 4. The first four tasks
capture the actions 1.–4. from Section 2.2; the 5th
linearizes the obtained SSyntS. Figure 4 provides a
sample input and output of each submodule. The
system outputs a 14 column CoNLL’09 linearized
format without morphological inflections or punctu-
ation marks.

In the next sections, we discuss how these ac-
tions are realized and how they are embedded into
the overall generation process.

The intra- and inter-hypernode dependency deter-
mination works as an informed dependency parser
that uses the DSyntS as input. The search space is
thus completely pruned. Note also that for each step,
the space of classes for the SVMs is based on lin-
guistic facts extracted from the training corpus (for
instance, for the preposition generation SVM, the
classes are the possible prepositions; for the auxil-
iary generation SVM, the possible auxiliaries, etc.).

3.2.1 Hypernode Identification
Given a node nd from the DSyntS, the system

must find the shape of the surface hypernode that
corresponds to nd in the SSyntS. The hypernode
identification SVMs use the following features:

PoS of nd, PoS of nd’s head, verbal voice (active,
passive) and aspect (perfective, progressive) of the
current node, lemma of nd, and nd’s dependencies.

In order to simplify the task, we define the shape
of a surface hypernode as a list of surface PoS tags.
This unordered list contains the PoS of each of the
lemmas contained within the hypernode and a tag
that encodes the original deep node; for instance:

391



Figure 4: Workflow of the Data-Driven Generator.

[ NN(deep), DT]

For each deep, i.e., DSyntS, PoS tag (which can
be one of the following four: N (noun), V (verb),
Adv (adverb), A (adjective)), a separate multi-class
classifier is defined.10 For instance, in the case of
N, the N-classifier will use the above features to
assign to the a DSynt-node with PoS N the most
appropriate (most likely) hypernode—in this case,
[NN(deep), DT].

3.2.2 Lemma Generation

Once the hypernodes of the SSyntS under con-
struction have been produced, the functional nodes
that have been newly introduced in the hypernodes
must be assigned a lemma label. The lemma gener-
ation SVMs use the following features of the deep
nodes nd in the hypernodes to select the most likely
lemma:

verbal finiteness (finite, infinitive, gerund, partici-
ple) and aspect (perfective, progressive), degree of
definiteness of nouns, PoS of nd, lemma of nd, PoS
of the head of nd

Again, for each surface PoS tag, a separate clas-
sifier is defined. Thus, the DT-classifier would pick
for the hypernode [NN(deep), DT] the most likely
lemma for the DT-node (optimally, a determiner).

10As will be seen in the discussion of the results, the strategy
proposed by Ballesteros et al. (2014b) to define a separate clas-
sifier for each linguistic category here and in the other stages
largely pays off because it reduces the classification search
space enormously and thus leads to a higher accuracy.

3.2.3 Intra-hypernode Dependency Generation
Given a hypernode and its lemmas provided by

the two previous stages, the dependencies (i.e., the
dependency attachments and dependency labels) be-
tween the elements of the created SSyntS hypern-
odes must be determined (and thus also the gover-
nors of the hypernodes). For this task, the intra-
hypernode dependency generation SVMs use the
following features:

lemmas included in the hypernode, PoS-tags of the
lemmas in the hypernode, voice of the head h of the
hypernode, deep dependency relation to h.

For each kind of hypernode, dynamically a sepa-
rate classifier is generated.11 In the case of the hy-
pernode [NN(deep), DT], the corresponding classi-
fier will create a link between the determiner and the
noun, with the noun as head and the determiner as
dependent because it is the best link that it can find;
cf. Figure 5 for illustration. We ensure that the out-
put of the classifiers is a tree by controlling that ev-
ery node (except the root) has one and only one gov-
ernor. The DSynt input is a tree; in the case of hy-
pernodes of cardinality one, the governor/dependent
relation is maintained; in the case of hypernodes of
higher cardinality, only one node receives an incom-
ing arc and only one can govern another hypernode.

3.2.4 Inter-hypernode Dependency Generation
Once the individual hypernodes have been con-

verted into connected dependency subtrees, the hy-
11This implies that the number of classifiers varies depending

on the training set. For instance, during the intra-hypernode
dependency creation for Spanish, 108 SVMs are generated.

392



[ NN(deep), DT]

det

Figure 5: Internal dependency within a hypernode

pernodes must be connected between each other,
such that we obtain a complete SSyntS. The inter-
hypernode dependency generation SVMs use the
following features of a hypernode ss to determine
for each hypernode its governor. For each hyper-
node with a distinct internal dependency pattern, a
separate classifier is dynamically derived (for our
treebanks, we obtained 114 different SVM classi-
fiers because they also take into account hypernodes
with just one token).:

the internal dependencies of ss, the head of ss, the
lemmas of ss, the PoS of the dependent of the head
of ss in DSyntS

For instance, the classifier for the hypernode
[JJ(deep)] is most likely to identify as its governor
NN in the hypernode [NN(deep), DT]; cf. Figure 6.

The task faced by the inter-hypernode depen-
dency classifiers is the same as that of a dependency
parser, only that its search space is very small (which
is favorably reflected in the accuracy figures).

[ NN(deep), DT] [ JJ(deep)]

modif

Figure 6: Surface dependencies between two hypernodes.

3.3 Linearization

Once we obtained a SSyntS, the linearizer must find
the correct order of the words. There is already a
body of work available on statistical linearization.
Therefore, these tasks were not in the focus of our
work. Rather, we adopt the most successful tech-
nique of the first SRST (Belz et al., 2011), a bottom-
up tree linearizer that orders bottom-up each head
and its children (Bohnet et al., 2011; Guo et al.,
2011a). This has the advantage that the linear or-
der obtained previously can provide context features
for ordering sub-trees higher up in the dependency
tree. Each head and its children are ordered with a
beam search.

The beam is initialized with entries of single
words that are expanded in the next step by the re-
maining words of the sub-tree, which results in a
number of new entries for the next iteration. After
the expansion step, the new beam entries are sorted
and pruned. We keep the 30 best entries and con-
tinue with the expansion and pruning steps until no
further nodes of the subtree are left. We take an
SVM to obtain the scores for sorting the beam en-
tries, using the same feature templates as in Guo et
al. (2011b) and Bohnet et al. (2011).

4 Experiments and Results

In our experiments, the Spanish treebank has been
divided into: (i) a development set of 219 sentences,
with 3,437 tokens in the DSyntS treebank and 4,799
tokens in the SSyntS treebank (with an average of
21.91 words by sentence in SSynt); (ii) a training
set of 3,036 sentences, with 57,665 tokens in the
DSyntS treebank and 84,668 tokens in the SSyntS
treebank (with an average of 27.89 words by sen-
tence in SSynt); and a (iii) a held-out test for eval-
uation of 258 sentences, with 5,878 tokens in the
DSyntS treebank and 8,731 tokens in the SSyntS
treebank (with an average of 33.84 words by sen-
tence in SSynt).

For the English treebank, we used a classical
split of (i) a training set of 39,279 sentences, with
724,828 tokens in the DSynt treebank and 958,167
tokens in the SSynt treebank (with an average of
24.39 words by sentence in SSynt); and (ii) a test
set of 2,399 sentences, with 43,245 tokens in the
DSynt treebank and 57,676 tokens in the SSynt tree-
bank (with an average of 24.04 words by sentence in
SSynt).

In what follows, we show the system performance
on both treebanks. The Spanish treebank was used
for development and testing, while the English tree-
bank was only used for testing.

4.1 Results

In this section, we present the performance of,
first of all, the individual tasks of the data-driven
DSyntS–SSyntS projection, since these have been
the challenging tasks that we addressed. Table 1
shows similar results for all tasks on the develop-
ment and test sets with gold-standard input, that is,

393



the results of the classifiers as a stand-alone mod-
ule, assuming that the previous module provides a
perfect output.

Spanish Dev.set # %
Hypernode identification 3327/3437 96.80

Lemma generation 724/767 94.39
Intra-hypernode dep. generation 756/756 100.00
Inter-hypernode dep. generation 2628/2931 89.66

Spanish Test set # %
Hyper-node identification 5640/5878 95.95

Lemma generation 1556/1640 94.88
Intra-hypernode dep. generation 1622/1622 100.00
Inter-hypernode dep. generation 4572/5029 90.91

Table 1: Results of the evaluation of the SVMs for the
non-isomorphic transition for the Spanish DSyntS devel-
opment and test sets

English Test set # %
Hyper-node identification 42103/43245 97.36

Lemma generation 6726/7199 93.43
Intra-hypernode dep. generation 6754/7179 94.08
Inter-hypernode dep. generation 35922/40699 88.26

Table 2: Results of the evaluation of the SVMs for the
non-isomorphic transition for the English DSyntS test set

To have the entire generation pipeline in place,
we carried out several linearization experiments,
starting from: (i) the SSyntS gold standard, (ii)
SSyntSs generated by the rule-based baselines, and
(iii) SSyntSs generated by the data-driven deep gen-
erator; cf. surface gen., baseline deep gen, and deep
gen. respectively in Tables 3 and 4).12

Development Set BLEU NIST Exact
surface gen. 0.754 11.29 24.20 %
baseline deep gen. 0.547 9.98 10.96 %
deep gen. 0.582 10.78 12.33 %
Test Set BLEU NIST Exact
surface gen. 0.762 12.08 15.89 %
baseline deep gen. 0.515 10.60 2.33 %
deep gen. 0.542 11.24 3.49 %

Table 3: Overview of the results on the Spanish develop-
ment and test sets excluding punctuation marks after the
linearization

12Following (Langkilde-Geary, 2002; Belz et al., 2011) and
other works on statistical text generation, we access the quality
of the linearization module via BLEU score, NIST and exactly
matched sentences.

Test Set BLEU NIST Exact
surface gen. 0.91 15.26 56.02 %
baseline deep gen. 0.69 13.71 12.38 %
deep gen. 0.77 14.42 21.05 %

Table 4: Overview of the results on the English test set
excluding punctuation marks after the linearization

4.2 Discussion and Error Analysis

In general, Tables 1–4 show that the quality of the
presented deep data-driven generator is rather good
both during the individual stages of the DSyntS–
SSyntS transition and as part of the DSyntS–
linearized sentence pipeline.

Two main problems impede an even better perfor-
mance figures than those reflected in Tables 1 and 2.
First, the introduction of prepositions causes most
errors in hypernode detection and lemma genera-
tion: when a preposition should be introduced or not
and which preposition should be introduced depends
exclusively on the subcategorization frame of the
governor of the DSyntS node. A corpus of a limited
size does not capture the subcategorization frames
of ALL predicates. This is especially true for our
Spanish treebank, which is particularly small. Sec-
ond, the inter-hypernode dependency suffers from
the fact that the SSyntS tagset is quite fine-grained,
at least in the case of Spanish, which makes the task
of the classifiers harder (e.g., there are nine different
types of verbal objects). In spite of these problems,
each set of classifiers achieves results above 88% on
the test sets.

The results of deep generation in Tables 3 and
4 can be explained by the fact of error propaga-
tion: while (only) about 1 out of 10 hypernodes and
about 1 out of 10 lemmas are not correct and very
little information is lost in the stage of the intra-
hypernode dependencies determination, already al-
most 1.75 out of 10 inter-hypernode dependencies,
and finally 1 out 10 linear orderings are incorrect for
English and more than 2 out 10 for Spanish.

As already mentioned above, the size of the train-
ing corpus strongly affects the results. Thus, for
English, for which the size of the training dataset
has been 10 times bigger than for Spanish, the data-
driven generator provides, without any tuning, more
than 0.2 BLEU points more that for Spanish. A big-
ger corpus also covers more linguistic phenomena

394



(lexical features, subcategorization frames, syntac-
tic sentential constructions, etc.)—which can be also
exploited for rule-based generation.

The linearizer also suffers from a small size of the
training set. Thus, while the small Spanish training
corpus leads to 0.754 BLEU and 0.762 BLEU for the
development and test sets respectively, for English,
we achieve 0.91 BLEU, which is a very competi-
tive outcome compared to other English linearizers
(Song et al., 2014).

We also found that the data-driven generator tends
to output slightly shorter sentences, when compared
to the rule-based baseline. It is always difficult to
find the best evaluation metric for plain text sen-
tences (Smith et al., 2014). In our experiments,
we used BLEU, NIST and the exact match metric.
BLEU is the average of n-gram precisions and in-
cludes a brevity penalty, which reduces the score
if the length of the output sentence is shorter than
the gold. In other words, BLEU favors longer sen-
tences. We believe that this is one of the reasons
why the machine-learning based generator shows a
bigger difference for the English test set and the
Spanish development set than the rule-based base-
line. Firstly, there are extremely long sentences in
the Spanish test set (31 words per sentence, in the
average; the longest being 165 words). Secondly,
the English sentences and the Spanish development
sentences are much shorter than the Spanish test sen-
tences, such that the ML approach has the potential
to perform better.

5 Related work

There is an increasing amount of work on statistical
sentence generation, although hardly any addresses
the problem of deep generation from semantic struc-
tures that are not isomorphic with syntactic- struc-
tures as a purely data-driven problem (as we do). To
the best of our knowledge, the only exception is our
earlier work in (Ballesteros et al., 2014b), where we
discuss the principles of classifiers for data-driven
generators. As already mentioned in Section 1, most
of the state-of-the-art work focuses on syntactic gen-
eration; see, among others (Bangalore and Rambow,
2000; Langkilde-Geary, 2002; Filippova and Strube,
2008), or only on linearization and inflection (Filip-
pova and Strube, 2007; He et al., 2009; Wan et al.,

2009; Guo et al., 2011a). A number of proposals
are hybrid in that they combine statistical machine
learning-based generation with rule-based genera-
tion. Thus, some combine machine learning with
pre-generated elements, as, e.g., (Marciniak and
Strube, 2004; Wong and Mooney, 2007; Mairesse et
al., 2010), or with handcrafted rules, as, e.g., (Ring-
ger et al., 2004; Belz, 2005). Others derive auto-
matically grammars for rule-based generation mod-
ules from annotated data, which can be used for
surface generation, as, e.g., (Knight and Hatzivas-
siloglou, 1995; Langkilde and Knight, 1998; Oh and
Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et
al., 2011; Rajkumar et al., 2011) or for generation
from ontology triples, as, e.g., (Gyawali and Gar-
dent, 2013).

6 Conclusions

We presented a statistical deep sentence generator
that successfully handles the non-isomorphism be-
tween meaning representations and syntactic struc-
tures in terms of a principled machine learning ap-
proach. This generator has been successfully tested
on an English and a Spanish corpus, as a stand-alone
DSyntS–SSyntS generator and as a part of the gen-
eration pipeline. We are currently about to apply it
to other languages—including Chinese, French and
German. Furthermore, resources are compiled to
use it for generation of spoken discourse in Arabic,
Polish and Turkish.

We believe that our generator can be used not only
in generation per se, but also, e.g., in machine trans-
lation (MT), since MT could profit from using mean-
ing representations such as DSyntSs, which abstract
away from the surface syntactic idiosyncrasies of
each language, but are still linguistically motivated,
as transfer representations.

Acknowledgments

Our work on deep stochastic sentence generation
is partially supported by the European Commis-
sion under the contract numbers FP7-ICT-610411
(project MULTISENSOR) and H2020-RIA-645012
(project KRISTINA).

395



References
J. Andreas, A. Vlachos, and S. Clark. 2013. Seman-

tic Parsing as Machine Translation. In Proceedings of
ACL ’13.

A. Aue, A. Menezes, R. Moore, C. Quirk, and E. Ringger.
2004. Statistical Machine Translation Using Labeled
Semantic Dependency Graphs. In Proceedings of TMI
’04.

Olga Babko-Malaya, 2005. Propbank Annotation Guide-
lines.

Miguel Ballesteros, Bernd Bohnet, Simon Mille, and Leo
Wanner. 2014a. Deep-syntactic parsing. In Proceed-
ings of COLING’14, Dublin, Ireland.

Miguel Ballesteros, Simon Mille, and Leo Wanner.
2014b. Classifiers for Data-Driven Deep Sentence
Generation. In Proceedings of the International Con-
ference of Natural Language Generation (INLG).

Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th conference on Computa-
tional linguistics-Volume 1, pages 42–48. Association
for Computational Linguistics.

Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proceedings of the Generation Chal-
lenges Session at the 13th European Workshop on Nat-
ural Language Generation, pages 217–226.

Anja Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proceedings of the
10th European Workshop on Natural Language Gen-
eration, pages 15–23.

Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of COLING ’10, pages ”98–106”.

Bernd Bohnet, Simon Mille, Benoı̂t Favre, and Leo Wan-
ner. 2011. StuMaBa: From deep representation
to surface. In Proceedings of ENLG 2011, Surface-
Generation Shared Task, Nancy, France.

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC), volume 6,
pages 449–454.

Katja Filippova and Michael Strube. 2007. Generating
constituent order in german clauses. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics, volume 45, page 320.

Katja Filippova and Michael Strube. 2008. Sentence fu-
sion via dependency graph compression. In Proceed-
ings of the Conference on Empirical Methods in Nat-

ural Language Processing, EMNLP ’08, pages 177–
185, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Yuqing Guo, Deirdre Hogan, and Josef van Genabith.
2011a. Dcu at generation challenges 2011 surface
realisation track. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 227–229,
Nancy, France, September. Association for Computa-
tional Linguistics.

Yuqing Guo, Haifeng Wang, and Josef Van Genabith.
2011b. Dependency-based n-gram models for general
purpose sentence realisation. Natural Language Engi-
neering, 17(04):455–483.

B. Gyawali and C. Gardent. 2013. LOR-KBGEN,
A Hybrid Approach To Generating from the KBGen
Knowledge-Base. In Proceedings of the KBGen Chal-
lenge http://www.kbgen.org/papers/.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.

Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu. 2009.
Dependency based chinese sentence realization. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2-Volume 2, pages 809–816. As-
sociation for Computational Linguistics.

Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of the 16th Nordic Conference of Com-
putational Linguistics (NODALIDA), pages 105–112,
Tartu, Estonia, May 25-26.

B. Jones, J. Andreas, D. Bauer, K.M. Hermann, and
K. Knight. 2012. Semantics-Based Machine Transla-
tion with Hyperedge Replacement Grammars. In Pro-
ceedings of COLING ’12.

Beata Beigman Klebanov, Kevin Knight, and Daniel
Marcu. 2004. Text simplification for information-
seeking applications. In On the Move to Meaningful
Internet Systems, Lecture Notes in Computer Science,
pages 735–747. Springer Verlag.

Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd annual meeting on Association for Compu-
tational Linguistics, pages 252–260. Association for
Computational Linguistics.

396



I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proceed-
ings of the COLING/ACL, pages 704–710.

Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proceedings of the 12th Interna-
tional Natural Language Generation Workshop, pages
17–24. Citeseer.

François Mairesse, Milica Gašić, Filip Jurčı́ček, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation us-
ing graphical models and active learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1552–1561. Associ-
ation for Computational Linguistics.

Tomasz Marciniak and Michael Strube. 2004.
Classification-based generation using tag. In Natural
Language Generation, pages 100–109. Springer.

Igor Mel’čuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press, Albany.

Simon Mille, Alicia Burga, Gabriela Ferraro, and Leo
Wanner. 2012. How does the granularity of an an-
notation scheme influence dependency parsing perfor-
mance? In Proceedings of COLING 2012, pages 839–
852, Mumbai, India.

Simon Mille, Alicia Burga, and Leo Wanner. 2013.
Ancora-upf: A multi-level annotation of spanish. In
Proceedings of the Second International Conference
on Dependency Linguistics, Prague, Czech Republic.

Alice H Oh and Alexander I Rudnicky. 2002. Stochastic
natural language generation for spoken dialog systems.
Computer Speech & Language, 16(3):387–407.

Rajakrishnan Rajkumar, Dominic Espinosa, and Michael
White. 2011. The osu system for surface realization
at generation challenges 2011. In Proceedings of the
13th European workshop on natural language gener-
ation, pages 236–238. Association for Computational
Linguistics.

Eric Ringger, Michael Gamon, Robert C Moore, David
Rojas, Martine Smets, and Simon Corston-Oliver.
2004. Linguistically informed statistical models of
constituent structure for ordering in sentence realiza-
tion. In Proceedings of the 20th international confer-
ence on Computational Linguistics, page 673. Associ-
ation for Computational Linguistics.

Aaron Smith, Christian Hardmeier, and Jörg Tiedemann.
2014. Bleu is not the colour: How optimising bleu
reduces translation quality.

Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.
2014. Joint morphological generation and syntactic
linearization. In Proceedings of the Twenty-Eighth
AAAI Conference on Artificial Intelligence, July 27 -
31, 2014, Québec City, Québec, Canada., pages 1522–
1528.

A. Stent, R. Prasad, and M. Walker. 2004. Trainable sen-
tence planning for complex information presentation
in spoken dialog systems. In Proceedings of the ACL
’04, pages 79–86.

Stephen Wan, Mark Dras, Robert Dale, and Cécile Paris.
2009. Improving grammaticality in statistical sentence
generation: Introducing a dependency spanning tree
algorithm with an argument satisfaction model. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 852–860. Association for Computational
Linguistics.

Yuk Wah Wong and Raymond J Mooney. 2007. Genera-
tion by inverting a semantic parser that uses statistical
machine translation. In HLT-NAACL, pages 172–179.

Huayan Zhong and Amanda Stent. 2005. Building sur-
face realizers automatically from corpora. Proceed-
ings of UCNLG, 5:49–54.

397


