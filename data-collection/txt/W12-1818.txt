










































One Year of Contender: What Have We Learned about Assessing and Tuning Industrial Spoken Dialog Systems?


NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 45–48,
Montréal, Canada, June 7, 2012. c©2012 Association for Computational Linguistics

One Year of Contender: What Have We Learned about Assessing and

Tuning Industrial Spoken Dialog Systems?

David Suendermann
SpeechCycle, New York, USA
david@suendermann.com

Roberto Pieraccini
ICSI, Berkeley, USA

roberto@icsi.berkeley.edu

Abstract

A lot. Since inception of Contender, a ma-
chine learning method tailored for computer-
assisted decision making in industrial spo-
ken dialog systems, it was rolled out in over
200 instances throughout our applications pro-
cessing nearly 40 million calls. The net ef-
fect of this data-driven method is a signifi-
cantly increased system performance gaining
about 100,000 additional automated calls ev-
ery month.

1 From the unwieldiness of data to the
Contender process

Academic institutions involved in the research on

spoken dialog systems often lack access to data for

training, tuning, and testing their systems. This is

simply because the majority of systems only live in

laboratory environments and hardly get deployed to

the live user1. The lack of data can result in sys-

tems not sufficiently tested, models trained on non-

representative or artificial data, and systems of lim-

ited domains (usually restaurant or flight informa-

tion).

On the other hand, in industrial settings, spoken

dialog systems are often deployed to take over tasks

of call center agents associated with potentially very

large amounts of traffic. Here, we are speaking of

applications which may process more than one mil-

lion calls per week. Having applications log every

1One of the few exceptions to this rule is the Let’s Go bus in-

formation system maintained at the Carnegie Mellon University

in Pittsburgh (Raux et al., 2005).

action they take during the course of a call can pro-

vide developers with valuable data to tune and test

the systems they maintain. As opposed to the aca-

demic world, often, there appears to be too much

data to capture, permanently store, mine, and re-

trieve. Harddisks on application servers run full,

log processing scripts demand too much comput-

ing capacity, database queues get stuck, queries slow

down, and so on and so forth. Even if these billions

and billions of log entries are eventually available

for random access from a highly indexed database

cluster, it is not clear what one should search for

in an attempt to improve a dialog system’s perfor-

mance.

About a year and a half ago, we proposed a

method we called Contender playing the role of a

live experiment in a deployed spoken dialog sys-

tem (Suendermann et al., 2010a). Conceptually, a

Contender is an activity in a call flow which has an

input transition and multiple output transitions (al-

ternatives). When a call hits a Contender’s input

transition, a randomization is carried out to deter-

mine which alternative the call will continue with

(see Figure 1). The Contender itself does not do any-

thing else but performing the random decision dur-

ing runtime. The different call flow activities and

processes the individual alternatives get routed to

make calls depend on the Contenders’ decisions.

Say, one wants to find out which of ten possible

time-out settings in an activity is optimal. This could

be achieved by duplicating the activity in question

ten times and setting each copy’s time-out to a dif-

ferent value. Now, a Contender is placed whose ten

alternatives get connected to the ten competing ac-

45



randomizer

Alternative 1 Alternative 2 Alternative 3

randomization 
weights

Figure 1: Contender with three alternatives.

tivities. Finally, the outbound transitions of the com-

peting activities have to be bundled to make the rest

of the application be independent of the Contender.

A Contender can be used for all sorts of exper-

iments in dialog systems. For instance, if system

designers are unsure about which of a number of

prompts has more expressive power, they can imple-

ment all of them in the application and have the Con-

tender decide at runtime which one to play. Or if it is

unclear which actions to perform in which order, dif-

ferent strategies can be compared using a Contender.

The same applies to certain parameter settings, error

handling approaches, confirmation strategies, and so

on. Every design aspect with one or more alterna-

tives can be implemented by means of a Contender.

Once an application featuring Contenders starts

taking live production traffic, an analysis has to be

carried out, to determine which alternative results

in the highest average performance. In doing so,

it is crucial to implement some measure of statisti-

cal significance as, otherwise, conclusions may be

misleading. If no statistical significance measure

was in place, processing two calls in a two-way

Contender, one routed to Alternative 1 and ending

up automated and one routed to Alternative 2 end-

ing up non-automated, could lead to the conclu-

sion that Alternative 1’s automation rate is 100%

and Alternative 2’s is 0. To avoid such potentially

erroneous conclusions, we are using two-sample t-

tests for Contenders with two alternatives and pair-

wise two-sample t-tests with probability normaliza-

tion for more alternatives as measures of statistical

significance. A more exact but computationally very

expensive method was explained in (Suendermann

et al., 2010a), but for the sake of performing statis-

tical analysis with acceptable delays given the vast

amount of data, we primarily use the former in pro-

duction deployments.

If an alternative is found to statistically signifi-

cantly outperform the other alternatives, it is deemed

the winner, and it would be advisable routing most

(if not all) calls to that alternative. While this hard

reset maximizes performance induced by this Con-

tender going forward, it sometimes takes quite a

while before the required statistical significance is

actually reached. Hence, in the time span before

this hard reset, the Contender may perform subop-

timally. Furthermore, even though statistical mea-

sures could indicate which alternative the likely win-

ner is, this fact is potentially subject to change over

time depending upon alterations in the caller popu-

lation, the distribution of call reasons, or the appli-

cation itself. For this reason, it is recommendable to

keep exploring seemingly underperforming alterna-

tives by routing a very small portion of calls to them.

The statistical model we discussed in (Suender-

mann et al., 2010a) presents a solution to the above

listed issues. The model associates each alternative

of a Contender with a weight controlling which per-

centage of traffic is routed down this alternative on

average. As derived in (Suendermann et al., 2010a),

the weight for an alternative is generated based on

the probability that this alternative is the actual win-

ner of the Contender given the available historic

data. The weights are subject to regular updates

computed by a statistical analysis engine that con-

tinuously analyzes the behavior of all Contenders in

production deployment. In order to do so, the en-

gine accesses the entirety of available application

logs associating performance metrics, such as au-

tomation rate (the fraction of processed calls that

satisfied the call reason) or average handling time

(average call duration), with Contenders and their

alternatives. This is relatively straightforward since

the application can log call category (to tell whether

a call was automated or not), call duration, the Con-

tenders visited and the results of the randomization

at each of the Contender. In Figure 2, a high-level

diagram of the Contender process is shown.

46



w1 wn

application logs

Application

statistical analysis

Analysis Engine

w2

randomizer

Alternative 1 Alternative 2 Alternative n

w1,
w2,
...,
wn

Figure 2: Contender process.

Since statistical analysis of Contenders involves

data points of hundreds of thousands of calls, per-

formance measurement needs to be based on autom-

ically derivable, i.e. objective, metrics. Popular ob-

jective metrics are automation rate, average handling

time, “speech errors”, retry rate, number of hang-ups

or opt-outs (Suendermann et al., 2010c). There are

also techniques correlating objective metrics to sub-

jective ones in an attempt to predict user or caller ex-

perience, i.e., to evaluate interaction quality as per-

ceived by the caller (Walker et al., 1997; Evanini et

al., 2008; Möller et al., 2008). Despite the impor-

tance of making interactions as smooth and pleasant

as possible, stakeholders of industrial systems often

insist on using metrics directly tied to the savings

generated by the deployed spoken dialog system. As

we introduced in (Suendermann et al., 2010b), sav-

ings mainly depend on automation rate (A) and av-

erage handling time (T ) and can be expressed by the

reward

R = TAA− T

where TA is a trade-off factor that depends on aver-

age agent salary and hosting and telecommunication

fees.

2 A snapshot of our last year’s experiences

Shortly after setting the mathematical foundations of

the Contender process and establishing the involved

software and hardware pieces, the first Contenders

were implemented in production applications. Un-

der the close look of operations, quality assurance,

engineering, speech science, as well as technical ac-

count management departments, the process under-

went a number of refinement cycles. In the mean-

time, more and more Contenders were implemented

into a variety of applications and released into pro-

duction traffic. Until to date, 233 Contenders were

released into production systems processing an total

call volume of 39 million calls. Table 1 shows some

statistics of a number of example Contenders per ap-

plication. These statistics are drawn from spoken di-

alog systems for technical troubleshooting of cable

services as discussed e.g. in (Acomb et al., 2007).

Such applications assist callers fixing problems with

their cable TV or Internet (such as no, slow, or inter-

mittent connection, e-mail issues). In addition to the

application and a short description of the Contender,

the table shows three quantities:

• the number of calls processed by the Contender

since its establishment (# calls),

• the reward difference between the highest- and

lowest-performing alternative of a Contender

∆R (a high value indicates that the best-
performing alternative is substantially better

than the worst-performing one, that is, the Con-

tender is very effective), and

• an estimate of the number of automated calls

gained or saved per month by running the Con-

tender ∆At [mo−1] (this value indicates the
net effect of having all calls route through

the best-performing alternative vs. the worst-

performing one, that is, the upper bound of how

many calls were gained or saved). This metric

47



Table 1: Statistics of example Contenders.

application Contender # calls ∆At [mo−1] ∆R

TV problem capture 13,477,810 40,362 0.05

TV cable box reboot order 4,322,428 28,975 0.11

TV outage prediction 2,758,963 08,198 0.04

TV on demand 485,300 08,123 0.17

TV input source troubleshooting 1,162,445 03,487 0.05

TV account lookup 9,627 03,201 0.02

Internet troubleshooting paths I 275,248 05,568 0.02

Internet troubleshooting paths II 1,389,489 03,530 0.01

Internet computer monitor instruction 1,500,010 03,271 0.01

TV/Internet opt in 6,865,929 31,764 0.05

is calculated by multiplying the observed dif-

ference in automation rate ∆A with the number
of monthly calls hitting the Contender (t).

3 Conclusion

We have seen that the use of Contenders (a method

to assess and tune arbitrary components of indus-

trial spoken dialog systems) can be very benefi-

cial in multiple respects. Applications can self-

correct as soon as reliable data becomes available

without additional manual analysis and intervention.

Moreover, performance can increase substantially

in applications implementing Contenders. Looking

at only the 10 best-performing Contenders out of

233 running in our applications to-date, the number

of automated calls increased by about 100,000 per

month.

However, multiple Contenders that are active in

the same call flow cannot always be regarded inde-

pendent of each other. A routing decision made in

Contender 1 earlier in the call can potentially have

an impact on which decision is optimal in Contender

2 further down the call. In this respect, reward gains

of Contenders installed in the same application are

not necessarily additive. Not only can optimal deci-

sions in a Contender depend on other Contenders but

also on other runtime parameters such as time of the

day, day of the week, geographic origin of the caller

population, or the equipment used by the caller. Our

current research focuses on evaluating these depen-

dencies and accordingly optimize the way decisions

are made in Contenders.

References

K. Acomb, J. Bloom, K. Dayanidhi, P. Hunter, P. Krogh,
E. Levin, and R. Pieraccini. 2007. Technical Support
Dialog Systems: Issues, Problems, and Solutions. In
Proc. of the HLT-NAACL, Rochester, USA.

K. Evanini, P. Hunter, J. Liscombe, D. Suendermann,
K. Dayanidhi, and R. Pieraccini:. 2008. Caller Expe-
rience: A Method for Evaluating Dialog Systems and
Its Automatic Prediction. In Proc. of the SLT, Goa,
India.

S. Möller, K. Engelbrecht, and R. Schleicher. 2008. Pre-
dicting the Quality and Usability of Spoken Dialogue
Services. Speech Communication, 50(8-9).

A. Raux, B. Langner, D. Bohus, A. Black, and M. Eske-
nazi. 2005. Let’s Go Public! Taking a Spoken Dialog
System to the Real World. In Proc. of the Interspeech,
Lisbon, Portugal.

D. Suendermann, J. Liscombe, and R. Pieraccini. 2010a.
Contender. In Proc. of the SLT, Berkeley, USA.

D. Suendermann, J. Liscombe, and R. Pieraccini. 2010b.
Minimally Invasive Surgery for Spoken Dialog Sys-
tems. In Proc. of the Interspeech, Makuhari, Japan.

D. Suendermann, J. Liscombe, R. Pieraccini, and
K. Evanini. 2010c. ‘How am I Doing?’ A New
Framework to Effectively Measure the Performance
of Automated Customer Care Contact Centers. In
A. Neustein, editor, Advances in Speech Recogni-
tion: Mobile Environments, Call Centers and Clinics.
Springer, New York, USA.

M. Walker, D. Litman, C. Kamm, and A. Abella. 1997.
PARADISE: A Framework for Evaluating Spoken Di-
alogue Agents. In Proc. of the ACL, Madrid, Spain.

48


