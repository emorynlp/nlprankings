320

Coling 2010: Poster Volume, pages 320–328,

Beijing, August 2010

Monolingual Distributional Proﬁles for Word Substitution in Machine

Translation

Rashmi Gangadharaiah
rgangadh@cs.cmu.edu

Ralf D. Brown

ralf@cs.cmu.edu

Jaime Carbonell

jgc@cs.cmu.edu

Language Technologies Institute,

Carnegie Mellon University

Abstract

Out-of-vocabulary (OOV) words present a
signiﬁcant challenge for Machine Trans-
lation. For low-resource languages, lim-
ited training data increases the frequency
of OOV words and this degrades the qual-
ity of the translations. Past approaches
have suggested using stems or synonyms
for OOV words. Unlike the previous
methods, we show how to handle not just
the OOV words but rare words as well
in an Example-based Machine Transla-
tion (EBMT) paradigm. Presence of OOV
words and rare words in the input sentence
prevents the system from ﬁnding longer
phrasal matches and produces low qual-
ity translations due to less reliable lan-
guage model estimates. The proposed
method requires only a monolingual cor-
pus of the source language to ﬁnd can-
didate replacements. A new framework
is introduced to score and rank the re-
placements by efﬁciently combining fea-
tures extracted for the candidate replace-
ments. A lattice representation scheme al-
lows the decoder to select from a beam
of possible replacement candidates. The
new framework gives statistically signif-
icant
improvements in English-Chinese
and English-Haitian translation systems.

1

Introduction

An EBMT system makes use of a parallel corpus
to translate new sentences. Each input sentence
is matched against the source side of a training

corpus. When matches are found, the correspond-
ing translations in the target language are obtained
through sub-sentential alignment. In our EBMT
system, the ﬁnal translation is obtained by com-
bining the partial target translations using a sta-
tistical target Language Model. EBMT systems,
like other data-driven approaches, require large
amounts of data to function well (Brown, 2000).
Having more training data is beneﬁcial re-
sulting in log-linear improvement in translation
quality for corpus-based methods (EBMT, SMT).
Koehn (2002) shows translation scores for a num-
ber of language pairs with different training sizes
translated using the Pharaoh SMT toolkit (Koehn
et al., 2003). However, obtaining sizable paral-
lel corpora for many languages is time-consuming
and expensive. For rare languages, ﬁnding bilin-
gual speakers becomes especially difﬁcult.

One of the main reasons for low quality transla-
tions is the presence of large number of OOV and
rare words (low frequency words in the training
corpus). Variation in domain and errors in spelling
increase the number of OOV words. Many of the
present translation systems either ignore these un-
known words or leave them untranslated in the ﬁ-
nal target translation. When data is limited, the
number of OOV words increases, leading to the
poor performance of the translation models and
the language models due to the absence of longer
sequences of source word matches and less reli-
able language model estimates.

Approaches in the past have suggested using
stems or synonyms for OOV words as replace-
ments (Yang and Kirchhoff, 2006). Similarity
measures have been used to ﬁnd words that are
closely related (Marton et al., 2009). For morpho-

321

logically rich languages, the OOV word is mor-
phologically analyzed and the stem is used as its
replacement (Popovi´c and Ney, 2004).

This paper presents a simpler method inspired
by the Context-based MT approach (Carbonell et
al., 2006) to improve translation quality. The
method requires a large source language mono-
lingual corpus and does not require any other
language dependent resources to obtain replace-
ments. Approaches suggested in the past only
concentrated on ﬁnding replacements for the OOV
words and not the rare words. This paper pro-
poses a uniﬁed method to ﬁnd possible replace-
ments for OOV words as well as rare words based
on the context in which these words appear.
In
the case of rare words, the translated sentence is
traced back to ﬁnd the origin of the translations
and the target translations of the replacements are
replaced with the translations of the rare words. In
the case of OOV words, the target translations are
replaced by the OOV word itself. The main idea
for adopting this approach is the belief that the
EBMT system will be able to ﬁnd longer phrasal
matches and that the language model will be able
to give better probability estimates while decod-
ing if it is not forced to fragment text at OOV and
rare-word boundaries. This method is highly ben-
eﬁcial for low-resource languages that do not have
morphological analysers or Part-of-Speech (POS)
taggers and in cases where the similarity measures
proposed in the past do not ﬁnd closely related
words for certain OOV words.

The rest of the paper is organized as follows.
The next section (Section 2) discusses related
work in handling OOV words. Section 3 describes
the method adopted in this paper. Section 4 de-
scribes the experimental setup. Section 5 reports
the results obtained with the new framework for
English-Chinese and English-Haitian translation
systems. Section 6 concludes and suggests pos-
sible future work.

2 Related Work
Orthographic and morpho-syntactic techniques
for preprocessing training and test data have been
Popovi´c
shown to reduce OOV word rates.
and Ney (2004) demonstrated this on rich mor-
phological languages in an SMT system. They

introduced different types of transformations to
the verbs to reduce the number of unseen word
forms. Habash (2008) addresses spelling, name-
transliteration OOVs and morphological OOVs in
an Arabic-English Machine Translation system.
Phrases with the OOV replacements in the phrase
table of a phrase-based SMT system were “recy-
cled” to create new phrases in which the replace-
ments were replaced by the OOV words.

Yang and Kirchhoff (2006) proposed a back-
off model for phrase-based SMT that translated
word forms in the source language by hierarchi-
cal morphological phrase level abstractions.
If
an unknown word was found, the word was ﬁrst
stemmed and the phrase table entries for words
sharing the same stem were modiﬁed by replacing
the words with their stems. If a phrase entry or a
single word phrase was found, the corresponding
translation was used, otherwise the model backed
off to the next level and applied compound split-
ting to the unknown word. The phrase table in-
cluded phrasal entries based on full word forms as
well as stemmed and split counterparts.

Vilar et al. (2007) performed the translation
process treating both the source and target sen-
tences as a string of letters. Hence, there are
no unknown words when carrying out the actual
translation of a test corpus. The word-based sys-
tem did most of the translation work and the letter-
based system translated the OOV words.

The method proposed in this work to han-
dle OOV and rare words is very similar to the
method adopted by Carbonell et al. (2006) to gen-
erate word and phrasal synonyms in their Context-
based MT system. Context-based MT does not
require parallel text but requires a large monolin-
gual target language corpus and a fullform bilin-
gual dictionary. The main principle is to ﬁnd those
n-gram candidate translations from a large target
corpus that contain as many potential word and
phrase translations of the source text from the dic-
tionary and fewer spurious content words. The
overlap decoder combines the target n-gram trans-
lation candidates by ﬁnding maximal left and right
overlaps with the translation candidates of the pre-
vious and following n-grams. When the overlap
decoder does not ﬁnd coherent sequences of over-
lapping target n-grams, more candidate transla-

322

tions are obtained by substituting words or phrases
in the target n-grams by their synonyms.

Barzilay and McKeown (2001) and Callison-
Burch et al. (2006) extracted paraphrases from
monolingual parallel corpus where multiple trans-
lations were present for the same source. The syn-
onym generation in Carbonell et al. (2006) differs
from the above in that it does not require paral-
lel resources containing multiple translations for
the same source language.
In Carbonell et al.
(2006), a list of paired left and right contexts that
contain the desired word or phrase are extracted
from the monolingual corpus. The same corpus
is used to ﬁnd other words and phrases that ﬁt the
paired contexts in the list. The idea is based on the
distributional hypothesis which states that words
with similar meanings tend to appear in similar
contexts (Harris, 1954). Hence, their approach
performed synonym generation on the target lan-
guage to ﬁnd translation candidates that would
provide maximal overlap during decoding.

Marton et al. (2009) proposed an approach sim-
ilar to Carbonell et al. (2006) to obtain replace-
ments for OOV words, where monolingual dis-
tributional proﬁles for OOV words were con-
structed. Hence, the approach was applied on the
source language side as opposed to Carbonell et
al. (2006) which worked on the target language.
Only similarity scores and no other features were
used to rank the paraphrases (or replacements)
that occured in similar contexts. The high rank-
ing paraphrases were used to augment the phrase
table of phrase-based SMT.

All of the previously suggested methods only
handle OOV words (except Carbonell et al. (2006)
which handles low frequency target phrases) and
no attempt is made to handle rare words. Many of
the methods explained above directly modify the
training corpus (or phrase table in phrase-based
SMT) increasing the size of the corpus. Our
method clusters words and phrases based on their
context as described by Carbonell et al. (2006) but
uses the clustered words as replacements for not
just the OOV words but also for the rare words
on the source language side. Our method does
not make use of any morphological analysers,
POS taggers or manually created dictionaries
as they may not be available for many rare or

low-resource languages. The translation of the
replacements in the ﬁnal decoded target sentence
is replaced by the translation of the original word
(or the source word itself in the OOV case),
hence, we do not speciﬁcally look for synonyms.
The only condition for a word to be a candidate
replacement is that its left and right context need
to match with that of the OOV/rare-word. Hence,
the clustered words could have different semantic
relations. For example,

(cluster1):“laugh, giggle, chuckle, cry, weep”
where “laugh, giggle, chuckle” are synonyms and
“cry, weep” are antonyms of “laugh”.

Clusters can also contain hypernyms (or hy-
ponyms), meronyms (or holonyms), troponyms
and coordinate terms along with synonyms and
antonyms. For example,

(cluster2):“country,
region, place, area, dis-
trict, state, zone, United States, Canada, Korea,
Malaysia”.
where “country” is a hypernym of “United
States/Canada/Korea/Malaysia”.
“district” is a
meronym of “state”.
“United States, Canada,
Korea, Malaysia” are coordinate terms sharing
“country” as their hypernym.

The contributions made by the paper are three-
fold: ﬁrst, replacements are found for not just the
OOV words but for the rare words as well. Sec-
ond, the framework used allows scoring replace-
ments based on multiple features to permit op-
timization. Third, instead of directly modifying
the training corpus by replacing the candidate re-
placements by the OOV words, a new representa-
tion scheme is used for the test sentences to efﬁ-
ciently handle a beam of possible replacements.

3 Proposed Method

Like Marton et al. (2009), only a large monolin-
gual corpus is required to extract candidate re-
placements. To retrieve more replacements, the
monolingual corpus is pre-processed by ﬁrst gen-
eralizing numbers, months and years by NUM-
BER, MONTH and YEAR tags, respectively.

323

3.1 OOV and Rare words
Words in the test sentence (new source sentence
to be translated) that do not appear in the training
corpus are called OOV words. Words in the test
sentence that appear less than K times in the train-
ing corpus are considered as rare words (in this
paper K = 3). The method presented in the fol-
lowing sections holds for both OOV as well as rare
words. In the case of rare words, the ﬁnal transla-
tion is postprocessed (Section 3.7) to include the
translation of the rare word.

The procedure adopted will be explained with
a real example T (the rest of the sentence is
removed for the sake of clarity) encountered in
the test data with “hawks” as the OOV word,

T :a mobile base , hitting three hawks with
one arrow over the past few years ...

3.2 Context
As the goal is to obtain longer target phrasal trans-
lations for the test sentence before decoding,
only words that ﬁt the left and right context of the
OOV/rare-word in the test sentence are extracted.
Unlike Marton et al. (2009) where a context list
for each OOV is generated from the contexts
of their replacements, this paper uses only the
left and right context of the OOV/rare-word.
The default window size for the context is ﬁve
words (two words to the left and two words to the
right of the OOV/rare-word).
If the windowed
words contain only function words, the window
is incremented until at least one content word is
present in the resulting context. This enables one
to ﬁnd sensible replacements that ﬁt the context
well. The contexts for T are:

Left-context (L): hitting three
Right-context (R): with one arrow

The above contexts are further processed to
generalize the numbers by a N U M BER tag
to produce more candidate replacements. The
resulting contexts are now:

Left-context (L): hitting N U M BER
Right-context (R): with N U M BER arrow

As a single L − R context
smaller number of replacements are extracted.

is used, a far

3.3 Finding Candidate replacements
The monolingual corpus (M L) of the source lan-
guage is used to ﬁnd words and phrases (Xk) that
ﬁt LXkR i.e., with L as its left context and/or R as
its right context. The maximum length for Xk is
set to 3 currently. The replacements are further ﬁl-
tered to obtain only those replacements that con-
tain at least one content word. As illustrated ear-
lier, the resulting replacement candidates are not
necessarily synonyms.

3.4 Features
A local context of two to three words to the left
of an OOV/rare-word (wordi) and two to three
words to the right of wordi contain sufﬁcient
clues for the word, wordi. Hence, local contextual
features are used to score each of the replacement
candidates (Xi,k) of wordi. Each Xi,k extracted
in the previous step is converted to a feature vector
containing 11 contextual features. Certainly more
features can be extracted with additional knowl-
edge sources. The framework allows adding more
features, but for the present results, only these 11
features were used.

As our aim is to assist the translation system in
ﬁnding longer target phrasal matches, the features
are constructed from the occurrence statistics of
Xi,k from the bilingual training corpus (BL). If a
candidate replacement does not occur in the BL,
then it is removed from the list of possible replace-
ment candidates.

Frequency counts for the features of a partic-
ular replacement, Xi,k, extracted in the context
of Li,−2Li,−1 (two preceding words of wordi)
and Ri,+1Ri,+2 (two following words of wordi)
(the remaining words in the left and right context
of wordi are not used for feature extraction) are
obtained as follows:

f1: frequency of Xi,kRi,+1
f2: frequency of Li,−1Xi,k
f3: frequency of Li,−1Xi,kRi,+1
f4: frequency of Li,−2Li,−1Xi,k
f5: frequency of Xi,kRi,+1Ri,+2
f6: frequency of Li,−2Li,−1Xi,kRi,+1

324

f7: frequency of Li,−1Xi,kRi,+1Ri,+2
f8: frequency of Li,−2Li,−1Xi,kRi,+1Ri,+2
f9: frequency of Xi,k in ML
f10: frequency of Xi,k in BL
f11: number of feature values (f1, ..f10) > 0

f11 is a vote feature which counts the num-
ber of features (f1 ...
f10) that have a value
greater than zero. The features are normalized
to fall within [0, 1]. The sentences in ML, BL
and test data are padded with two begin markers
and two end markers for obtaining counts for
OOV/rare-words that appear at the beginning or
end of a test sentence.

3.5 Representation
Before we go on to explaining the lattice repre-
sentation, we would like to make a small clariﬁca-
tion in the terminalogy used. In the MT commu-
nity, a lattice usually refers to the list of possible
partially-overlapping target translations for each
possible source n−gram phrase in the input sen-
tence. Since we are using the term lattice to also
refer to the possible paths through the input sen-
tence, we will call the lattice used by the decoder,
the “decoding lattice”. The lattice obtained from
the input sentence representing possible replace-
ment candidates will be called the “input lattice”.
An input lattice (Figure 1) is constructed with
a beam of replacements for the OOV and rare
words. Each replacement candidate is given a
score (Eqn 1) indicating the conﬁdence that a suit-
able replacement is found. The numbers in Fig-
ure 1 indicate the start and end indices (based
on character counts) of the words in the test sen-
tence. In T , two replacements were found for the
word “hawks”: “homers” and “birds”. However,
“homers” was not found in the BL and hence, it
was removed from the replacement list.

The input lattice also includes the OOV word
with a low score (Eqn 2). This allows the EBMT
system to also include the OOV/rare-word dur-
ing decoding.
In the Translation Model of the
EBMT system, this test lattice is matched against
the source sentences in the bilingual training cor-
pus. The matching process would now also look
for phrases with “birds” and not just “hawks”.
When a match is found, the corresponding trans-

Figure 1: Lattice of the input sentence T contain-
ing replacements for OOV words.

Figure 2: Sample English candidate replacements
obtained.

lation in the target language is obtained through
sub-sentential alignment (Section 3.7). The scores
on the input lattice are later used by the decoder
(Section 3.7).
Each replacement Xi,k for the
OOV/rare-word (wordi) is scored with a logistic
function (Bishop, 2006) to convert the dot product
of the features and weights (~λ ˙ ~fi,k) to a score be-
tween 0 and 1 (Eqn 1 and Eqn 2).

pλ(Xi,k|wordi) =

exp(~λ˙ ~fi,k)

1+Pj=1...S exp(~λ˙ ~fi,j)

(1)

hawks with one arrow .....

T :     a mobile base , hitting three 
 
input lattice:
0 
0 
6 
1 
10 
7 
11 
11 
18 
12 
17 
13 
22 
18 
18 
22 
23  26 
27 
29 
30  34 
       
       

( “ a ” )
( “ mobile ” )
( “ base ” )
( “,” )
( “ hitting ” )
( “ three ” )
( “ hawks ”   0.0026)
( “ birds ”     0.9974)
( “ with ” )
( “ one ” )
( “ arrow ” )

OOV/Rare word

Candidate Replacements

Spelling errors
krygyzstan

yusukuni

kilomaters

somoa

ear

buyers

plummet

optimal

kyrgyzstan,...

yasukuni,..

kilometers, miles, km, ...

Coordinate terms
india, turkey, germany, russia, japan,...

body, arms, hands, feet, mind, car, ...

dealer, inspector, the experts, smuggler,.

Synonyms
drop, dropped, fell, ....

Synonyms and Antonyms
worse, better, minimal,....

325

pλ(wordi) =

1

1 +Pj=1...S exp(~λ ˙ ~fi,j)

(2)

where, ~fi,j is the feature vector for the jth replace-
ment candidate of wordi, S is the number of re-
placements, ~λ is the weight vector indicating the
importance of the corresponding features.

3.6 Tuning feature weights
We would like to select those feature weights (~λ)
which would lead to the least expected loss in
translation quality (Eqn 3). −log(BLEU ) (Pap-
ineni et al., 2002) is used to calculate the expected
loss over a development set. As this objective
function has many local minima and is piecewise
constant, the surface is smoothed using the L2-
norm regularization. Powell’s algorithm (Powell,
1964) with grid-based line optimization is used to
ﬁnd the best weights. 7 different random guesses
are used to initialize the algorithm.

min

λ

Eλ[L(ttune)] + τ ∗ ||λ||2

(3)

The algorithm assumes that partial derivates of
the function are not available. Approximations of
the weights (λ1, ..λN ) are generated successively
along each of the N standard base vectors. The
procedure is iterated with a stopping criteria based
on the amount of change in the weights and the
change in the loss. A cross-validation set (in ad-
dition to the regularization term) is used to pre-
vent overﬁtting at the end of each iteration of the
Powell’s algorithm. This process is repeated with
different values of τ, as in Deterministic Anneal-
ing (Rose, 1998). τ is initialized with a high value
and is halved after each process.

3.7 System Description
The EBMT system ﬁnds phrasal matches for the
test (or input) sentence from the source side of
the bilingual corpus.
The corresponding tar-
get phrasal translations are obtained through sub-
sentential alignment. When an input lattice is
given instead of an input sentence, the system per-
forms the same matching process for all possible
phrases obtained from the input lattice. Hence,
the system also ﬁnds matches for source phrases
that contain the replacements for the OOV/rare-
word. Only the top C ranking replacement candi-

Figure 3: Lattice containing possible phrasal tar-
get translations for the test sentence T .

dates for every OOV/rare word are used in build-
ing the input lattice. The optimal value of C was
empirically found to be 2. On examining the ob-
tained input lattices, the proposed method found
replacements for at the most 3 OOV/rare words in
each test sentence (Section 4). Hence, the number
of possible paths through the input lattice is not
substantially large.

The target translations of all the source phrases
are placed on a common decoding lattice. An
example of a decoding lattice for example T is
given in Figure 3. The system is now able to ﬁnd
longer matches (“ three birds with one arrow ”
and “ three birds ”) which was not possible earlier
with the OOV word, “hawks”. The local order-
ing information between the translations of “three
birds” and “with one arrow” is well captured due
to the retrieval of the longer source phrasal match,
“three birds with one arrow”. Our ultimate goal
is to obtain translations for such longer n−gram
source phrases boosting the conﬁdence of both the
translation model and the language model.

target

fragments)

translations (or

The decoder used in this paper (Brown, 2003)
works on this decoding lattice of possible
phrasal
for
source phrases present in the input lattice to gen-
erate the target translation. Similar to Pharaoh
(Koehn et al., 2003),
the decoder uses multi-
level beam search with a priority queue formed
based on the number of source words translated.
Bonuses are given for paths that have overlapping
fragments. The total score (T S) for a path (Eqn
4) through the translation lattice is the arithmetic
average of the scores for each target word in the

Decoding Lattice

     birds
雕 

      a    mobile  base   ,  hitting   three   hawks    with       one       arrow  ....
                     

一

移动

,基地

击球

三

    

   流动 基地

一

同

箭

hawks

 三 雕

 

 一 箭 三雕

 一 箭

 “ three birds ”

 “ three birds with one arrow ”

326

path. The EBMT engine assigns each candidate
phrasal translation a quality score computed as
a log-linear combination of alignment score and
translation probability. The alignment score indi-
cates the engine’s conﬁdence that the right target
translation has been chosen for a source phrase.
The translation probability is the proportion of
times each distinct alternative translation was en-
countered out of all the translations. If the path
includes a candidate replacement, the log of the
score, pλ(wi), given for a candidate replacement
is incorporated into T S as an additional term with
a weight wt5.

T S =

1
t

tXi=1

[wt1 log(bi) + wt2 log(peni)

+wt3 log(qi) + wt4 log(P (wi|wi−2, wi−1))
+1I(wi=replacement)wt5 log(pλ(wi)) ]

(4)

where, t is the number of target words in the path,
wtj indicates the importance of each score, bi is
the bonus factor given for long phrasal matches,
peni is the penalty factor for source and target
phrasal-length mismatches, qi is the quality score
and P (wi|wi−2, wi−1) is the LM score. The pa-
rameters of the EBMT system (wtj) are tuned on
a development set.

The target translation is postprocessed to in-
clude the translation of the OOV/rare-word with
the help of the best path information from the
decoder.
In the case of OOV words, since the
translation is not available, the OOV word is put
back into the ﬁnal output translation in place of
the translation of its replacement.
In the output
translation of the test example T , the translation
of “birds” is replaced by the word, “hawks”. For
rare words, knowing that the translation of the rare
word may not be correct (due to poor alignment
statistics), the target translation of the replacement
is replaced by the translation of the rare word
obtained from the dictionary.
If the rare word
has multiple translations, the translation with the
highest score is chosen.

4 Experimental Setup
As we are interested in improving the per-
formance of low-resource EBMT, the English-
Haitian (Eng-Hai) newswire data (Haitian Cre-

ole, CMU, 2010) containing 15,136 sentence-
pairs was used. To test the performance in other
languages, we simulated sparsity by choosing less
training data for English-Chinese (Eng-Chi). For
the Eng-Chi experiments, we extracted 30k train-
ing sentence pairs from the FBIS (NIST, 2003)
corpus. The data was segmented using the Stan-
ford segmenter (Tseng et al., 2005). Although
we are only interested in small data sets, we also
performed experiments with a larger data set of
200k. 5-gram Language Models were built from
the target half of the training data with Kneser-
Ney smoothing. For the monolingual English cor-
pus, 9 million sentences were collected from the
Hansard Corpus (LDC, 1997) and FBIS data.

EBMT system without OOV/rare-word han-
dling is chosen as the Baseline system. The pa-
rameters of the EBMT system are tuned with 200
sentence pairs for both Eng-Chi and Eng-Hai. The
tuned EBMT parameters are used for the Base-
line system and the system with OOV/rare-word
handling. The feature weights for the proposed
method are then tuned on a seperate development
set of 200 sentence-pairs with source sentences
containing at least 1 OOV/rare-word. The cross-
validation set for this purpose is made up of 100
sentence-pairs.
In the OOV case, 500 sentence
pairs containing at least 1 OOV word are used for
testing. For the rare word handling experiments,
500 sentence pairs containing at least 1 rare word
are used for testing.

To assess the translation quality, 4-gram word-
based BLEU is used for Eng-Hai and 3-gram
word-based BLEU is used for Eng-Chi. Since
BLEU scores have a few limitations, the NIST and
TER metrics are also used. The test data used for
comparing the system handling OOV words and
the Baseline (without OOV word handling) is dif-
ferent from the test data used for comparing the
system handling rare words and the Baseline sys-
tem (without rare word handling). In the former
case, the test data handles only OOV words and
in the latter, the test data only handles rare words.
Hence, the test data for both the cases do not com-
pletely overlap. As we are interested in determin-
ing whether handling rare words in test sentences
is useful, we keep both the test data sets seper-
ate and assess the improvements obtained by only

327

OOV/Rare

OOV

Rare

system
Baseline

TER BLEU NIST
77.89
4.8525
4.9664
Handling OOV 76.95
5.3803
74.23
74.02
5.4406

18.61
19.32
22.84
23.12

Handling Rare

Baseline

Table 1: Comparison of translation scores of the
Baseline system and system handling OOV and
Rare words for Eng-Hai.

handling OOV words and by only handling rare
words over their corresponding Baselines. As fu-
ture work, it would be interesting to create one test
data set to handle both OOV and rare words to see
the overall gain.

The test set is further split into 5 ﬁles and the
Wilcoxon (Wilcoxon, 1945) Signed-Rank test is
used to ﬁnd the statistical signiﬁcance.

5 Results
Sample replacements found are given in Figure 2.
For both Eng-Chi and Eng-Hai experiments, only
the top C ranking replacement candidates were
used. The value of C was tuned on the develop-
ment set and the optimal value was found to be
2. Translation quality scores obtained on the test
data with 30k and 200k Eng-Chi training data sets
are given in Table 2. Table 1 shows the results
obtained on Eng-Hai. Statistically signiﬁcant im-
provements (p < 0.0001) were seen by handling
OOV words as well as rare words over their cor-
responding baselines.

As the goal of the approach was to obtain longer
target phrasal matches, we counted the number of
n-grams for each value of n present on the de-
coding lattice in the 30k Eng-Chi case. The sub-
plots: A and B in Figure 4, shows the frequency
of n-grams for higher values of n (for n > 5)
when handling OOV and rare words. The plots
clearly show the increase in number of longer tar-
get phrases when compared to the phrases ob-
tained by the baseline systems.

Since the BLEU and NIST scores were com-
puted only up to 3-grams, we further found the
number of n-gram matches (for n > 3) in the
ﬁnal translation of the test data with respect to
the reference translations (subplots: C and D).
As expected, a larger number of longer n−gram
matches were found. For the OOV case, matches

Figure 4: A, B: number of n-grams found for in-
creasing values of n on the decoding lattice. C, D:
number of target n-gram matches for increasing
values of n with respect to the reference transla-
tions.

OOV/Rare Training
data size

system

TER BLEU NIST

OOV

Rare

30k
30k
200k
200k
30k
30k
200k
200k

Baseline

Baseline

82.03
Handling OOV 80.97
79.41
Handling OOV 77.66
82.09
80.02
78.04
77.35

Handling Rare

Handling Rare

Baseline

Baseline

14.12
14.78
19.90
20.50
15.36
16.03
20.96
21.17

4.1186
4.1798
4.6822
4.7654
4.3626
4.4314
4.9647
5.0122

Table 2: Comparison of translation scores of the
Baseline system and system handling OOV and
Rare words for Eng-Chi.

up to 9-grams were found where the baseline only
found matches up to 8-grams.

6 Conclusion and Future Work

A simple approach to improve translation quality
by handling both OOV and rare words was pro-
posed. The framework allowed scoring and rank-
ing each replacement candidate efﬁciently.

The method was tested on two language pairs
and statistically signiﬁcant improvements were
seen in both cases. The results showed that rare
words also need to be handled to see improve-
ments in translation quality.

In this paper, the proposed method was only ap-
plied on words, as future work we would like to
extend it to OOV and rare-phrases as well.

e
c
i
t
t

a

l
 

i

g
n
d
o
c
e
d

 

e
h

t
 

n
o

 
s
m
a
r
g
−
n
#

12000
10000
8000
6000
4000
2000
0

 

15000

10000

5000

0

 

A

Baseline
Handling OOV words

6

7

8

9 10 11 12 13 14 15
n−gram

B

Baseline
Handling Rare words

6 7 8 9 10 11 12 13 14 15

n−gram

 

 

s
m
a
r
g
−
n

 

d
e

t

l

a
s
n
a
r
t
 
y
l
t
c
e
r
r
o
c
#

150

100

50

0

 

40

30

20

10

0

 

C
Baseline
Handling OOV words

 

4

5

6

9 10 11

7
8
n−gram
D
Baseline
Handling Rare words

 

4

5

6

7
8
n−gram

9 10 11

328

Y. Marton, C. Callison-Burch and P. Resnik. 2009.
Improved Statistical Machine Translation Using
Monolingually-derived Paraphrases. In Proceed-
ing of The Empirical Methods in Natural Language
Processing, pp. 381-390.

NIST.

2003.

Machine translation evaluation.

http://nist.gov/speech/tests/mt/

K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of The Associa-
tion for Computational Linguistics. pp. 311-318.

M. Popovi´c and H. Ney. 2004. Towards the use of
Word Stems and Sufﬁxes for Statistical Machine
Translation. In Proceedings of The International
Conference on Language Resources and Evalua-
tion.

M. J. D. Powell. 1964. An efﬁcient method for ﬁnd-
ing the minimum of a function of several variables
without calculating derivatives Computer Journal.
Volume 7, pp. 152-162.

K. Rose. 1998. Deterministic annealing for clustering,
compression, classiﬁcation, regression, and related
optimization problems. In Proceedings of The In-
stitute of Electrical and Electronics Engineers, pp.
2210-2239.

H. Tseng, P. Chang, G. Andrew, D. Jurafsky and C.
Manning.
2005. A Conditional Random Field
Word Segmenter. Fourth SIGHAN Workshop on
Chinese Language Processing.

D. Vilar, J. Peter, and H. Ney. 2007. Can we translate
letters? In Proceedings of Association Computa-
tional Linguistics Workshop on SMT, pp. 33-39.

M. Yang and K. Kirchhoff.

Phrase-based
back-off models for machine translation of highly
inﬂected languages. In Proceedings of European
Chapter of the ACL, 41-48.

2006.

F. Wilcoxon.

1945.

ranking methods. Biometrics, 1,
80-83.
http://faculty.vassar.edu/lowry/wilcoxon.html

Individual comparisons by
tool:

References
R. Barzilay and K. McKeown 2001. Extracting para-
phrases from a parallel corpus. In Proceedings
of the 39th Annual Meeting of the Association for
Computaional Linguistics, pp. 50-57.

C. M. Bishop 2006. Pattern Recognition and Machine

Learning, Springer.

R. D. Brown, R. Hutchinson, P. N. Bennett, J. G. Car-
bonell, P. Jansen. 2003. Reducing Boundary Fric-
tion Using Translation-Fragment Overlap. In Pro-
ceedings of The Ninth Machine Translation Summit,
pp. 24-31.

R. D. Brown. 2000. Automated Generalization of
Translation Examples. In Proceedings of The Inter-
national Conference on Computational Linguistics,
pp. 125-131.

C. Callison-Burch, P. Koehn and M. Osborne. 2006.
Improved Statistical Machine Translation Using
Paraphrases. In Proceedings of The North Ameri-
can Chapter of the Association for Computational
Linguistics, pp. 17-24.

J. Carbonell, S. Klien, D. Miller, M. Steinbaum, T.
Grassiany and J. Frey. 2006. Context-Based Ma-
chine Translation Using Paraphrases. In Proceed-
ings of The Association for Machine Translation in
the Americas, pp. 8-12.

N. Habash.

2008.

Four Techniques for On-
line Handling of Out-of-Vocabulary Words in
Arabic-English Statistical Machine Translation. In
Proceedings of Association for Computational
Linguistics-08: HLT, pp. 57-60.

Public

release
data

guage
http://www.speech.cs.cmu.edu/haitian/

Creole
by Carnegie Mellon,

Haitian

of

lan-
2010.

Z. Harris.

1954. Distributional structure. Word,

10(23): 146-162.

P. Koehn. 2004. Pharaoh: a Beam Search Decoder for
Phrase-Based Statistical Machine Translation Mod-
els. The Association for Machine Translation.

P. Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical Phrase-Based Translation. In Proceedings of
HLT:The North American Chapter of the Associa-
tion for Computational Linguistics.

P. Koehn

2002 Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished,
http://www.isi.edu/koehn/publications/europarl/

Linguistic Data Consortium. 1997 Hansard Corpus of
Parallel English and French. Linguistic Data Con-
sortium, December. http://www.ldc.upenn.edu/

