Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 851–859,

Beijing, August 2010

851

Simple and Efﬁcient Algorithm

for Approximate Dictionary Matching

Naoaki Okazaki
University of Tokyo

okazaki@is.s.u-tokyo.ac.jp

Jun’ichi Tsujii

University of Tokyo

University of Manchester

National Centre for Text Mining

tsujii@is.s.u-tokyo.ac.jp

Abstract

is a nontrivial task to ﬁnd the entry from these sur-
face expressions appearing in text.

This paper presents a simple and efﬁ-
cient algorithm for approximate dictio-
nary matching designed for similarity
measures such as cosine, Dice, Jaccard,
and overlap coefﬁcients. We propose this
algorithm, called CPMerge, for the τ-
overlap join of inverted lists. First we
show that this task is solvable exactly by
a τ-overlap join. Given inverted lists re-
trieved for a query, the algorithm collects
fewer candidate strings and prunes un-
likely candidates to efﬁciently ﬁnd strings
that satisfy the constraint of the τ-overlap
join. We conducted experiments of ap-
proximate dictionary matching on three
large-scale datasets that include person
names, biomedical names, and general
English words.
The algorithm exhib-
ited scalable performance on the datasets.
For example, it retrieved strings in 1.1
ms from the string collection of Google
Web1T unigrams (with cosine similarity
and threshold 0.7).

This paper addresses approximate dictionary
matching, which consists of ﬁnding all strings in
a string collection V such that they have similar-
ity that is no smaller than a threshold α with a
query string x. This task has a broad range of ap-
plications, including spelling correction, ﬂexible
dictionary look-up, record linkage, and duplicate
detection (Henzinger, 2006; Manku et al., 2007).
Formally, the task obtains a subset Yx,α ⊆ V ,
(1)

Yx,α = {y ∈ V (cid:12)(cid:12) sim(x, y) ≥ α},

where sim(x, y) presents the similarity between x
and y. A na¨ıve solution to this task is to com-
pute similarity values |V | times, i.e., between x
and every string y ∈ V . However, this solution
is impractical when the number of strings |V | is
huge (e.g., more than one million).
In this paper, we present a simple and efﬁ-
cient algorithm for approximate dictionary match-
ing designed for similarity measures such as co-
sine, Dice, Jaccard, and overlap coefﬁcients. Our
main contributions are twofold.

1

Introduction

Languages are sufﬁciently ﬂexible to be able to
express the same meaning through different dic-
tion. At the same time, inconsistency of surface
expressions has persisted as a serious problem in
natural language processing. For example, in the
biomedical domain, cardiovascular disorder can
be described using various expressions: cardio-
vascular diseases, cardiovascular system disor-
der, and disorder of the cardiovascular system. It

1. We show that the problem of approximate
dictionary matching is solved exactly by a
τ-overlap join (Sarawagi and Kirpal, 2004)
of inverted lists. Then we present CPMerge,
which is a simple and efﬁcient algorithm for
the τ-overlap join. In addition, the algorithm
is easily implemented.

2. We demonstrate the efﬁciency of the al-
gorithm on three large-scale datasets with
person names, biomedical concept names,

852

and general English words. We com-
pare the algorithm with state-of-the-art al-
gorithms, including Locality Sensitive Hash-
ing (Ravichandran et al., 2005; Andoni and
Indyk, 2008) and DivideSkip (Li et al.,
2008). The proposed algorithm retrieves
strings the most rapidly, e.g., in 1.1 ms from
Google Web1T unigrams (with cosine simi-
larity and threshold 0.7).

2 Proposed Method
2.1 Necessary and sufﬁcient conditions
In this paper, we assume that the features of a
string are represented arbitrarily by a set. Al-
though it is important to design a string represen-
tation for an accurate similarity measure, we do
not address this problem: our emphasis is not on
designing a better representation for string simi-
larity but on establishing an efﬁcient algorithm.

The most popular representation is given by n-
grams: all substrings of size n in a string. We
use trigrams throughout this paper as an example
of string representation. For example, the string
“methyl sulphone” is expressed by 17 elements
of letter trigrams1, {‘$$m’, ‘$me’, ‘met’,
‘eth’, ‘thy’, ‘hyl’, ‘yl ’, ‘l s’, ‘ su’,
‘sul’, ‘ulp’, ‘lph’, ‘pho’, ‘hon’,
‘one’, ‘ne$’, ‘e$$’}. We insert two $s be-
fore and after the string to denote the start or end
of the string. In general, a string x consisting of
|X| letters yields (|x| + n − 1) elements of n-
grams. We call |x| and |X| the length and size,
respectively, of the string x.
Let X and Y denote the feature sets of the
strings x and y, respectively. The cosine similarity
between the two strings x and y is,

cosine(X, Y ) = |X ∩ Y |

.

(2)

p|X||Y |

By integrating this deﬁnition with Equation 1, we
obtain the necessary and sufﬁcient condition for

1In practice, we attach ordinal numbers to n-grams to rep-
resent multiple occurrences of n-grams in a string (Chaud-
huri et al., 2006). For example, the string “prepress”, which
contains two occurrences of the trigram ‘pre’, yields
the set {‘$$p’#1, ‘$pr’#1, ‘pre’#1, ‘rep’#1,
‘epr’#1, ‘pre’#2, ‘res’#1, ‘ess’#1, ‘ss$’#1,
‘s$$’#1}.

α

Table 1: Conditions for each similarity measure
Measure min|Y | max|Y |
τ (= min|X ∩ Y |)
2−α
Dice
α |X|
2 α(|X| + |Y |)
α(|X|+|Y |)
Jaccard
|X|/α
|X|/α2
Cosine
—
Overlap

2−α|X|
α|X|
α2|X|
—

αp|X||Y |

α min{|X|,|Y |}

1+α

1

approximate dictionary matching,

(3)
This inequality states that two strings x and y must

lαp|X||Y |m ≤ |X ∩ Y | ≤ min{|X|,|Y |}.
have at least τ = lαp|X||Y |m features in com-

mon. When ignoring |X ∩Y | in the inequality, we
have an inequality about |X| and |Y |,
(cid:6)α2|X|(cid:7) ≤ |Y | ≤(cid:22)|X|
α2(cid:23)

This inequality presents the search range for re-
trieving similar strings;
that is, we can ignore
strings whose feature size is out of this range.
Other derivations are also applicable to similar-
ity measures, including Dice, Jaccard, and overlap
coefﬁcients. Table 1 summarizes the conditions
for these similarity measures.

(4)

We explain one usage of these conditions. Let
query string x = “methyl sulphone” and thresh-
old for approximate dictionary matching α = 0.7
with cosine similarity. Representing the strings
with letter trigrams, we have the size of x, |X| =
17. The inequality 4 gives the search range of |Y |
of the retrieved strings, 9 ≤ |Y | ≤ 34. Presum-
ing that we are searching for strings of |Y | = 16,
we obtain the necessary and sufﬁcient condition
for the approximate dictionary matching from the
inequality 3, τ = 12 ≤ |X ∩ Y |. Thus, we need
to search for strings that have at least 12 letter tri-
grams that overlap with X. When considering a
string y = “methyl sulfone”, which is a spelling
variant of y (ph → f), we conﬁrm that the string
is a solution for approximate dictionary matching
because |X∩Y | = 13 (≥ τ). Here, the actual sim-
ilarity is cosine(X, Y ) = 13/√17 × 16 = 0.788
(≥ α).
2.2 Data structure and algorithm
Algorithm 1 presents the pseudocode of the ap-
proximate dictionary matching based on Table 1.

853

Input: V : collection of strings
Input: x: query string
Input: α: threshold for the similarity
Output: Y: list of strings similar to the query
X ← string to feature(x);
Y ←[];
for l ← min y(|X|, α) to max y(|X|, α) do

1
2
3
4
5
6
7
8
Algorithm 1: Approximate dictionary
matching.

τ ← min overlap(|X|, l, α);
R ← overlapjoin(X, τ, V , l);
foreach r ∈ R do append r to Y;

end
return Y;

1
2
3
4
5
6
7
8
9
10
11

Input: X: array of features of the query string
Input: τ: minimum number of overlaps
Input: V : collection of strings
Input: l: size of target strings
Output: R: list of strings similar to the query
M ← {};
R ← [];
foreach q ∈ X do

foreach i ∈ get(V , l, q) do

M [i] ← M [i] + 1;
if τ ≤ M [i] then
append i to R;
end

end

end
return R;
Algorithm 2: AllScan algorithm.

Given a query string x, a collection of strings V ,
and a similarity threshold α, the algorithm com-
putes the size range (line 3) given by Table 1.
For each size l in the range, the algorithm com-
putes the minimum number of overlaps τ (line 4).
The function overlapjoin (line 5) ﬁnds sim-
ilar strings by solving the following problem (τ-
overlap join): given a list of features of the query
string X and the minimum number of overlaps τ,
enumerate strings of size l in the collection V such
that they have at least τ feature overlaps with X.
To solve this problem efﬁciently, we build an
inverted index that stores a mapping from the fea-
tures to their originating strings. Then, we can
perform the τ-overlap join by ﬁnding strings that
appear at least τ times in the inverted lists re-
trieved for the query features X.

Algorithm 2 portrays a na¨ıve solution for the
τ-overlap join (AllScan algorithm). In this algo-
rithm, function get(V , l, q) returns the inverted
list of strings (of size l) for the feature q.
In
short, this algorithm scans strings in the inverted
lists retrieved for the query features X, counts the
frequency of occurrences of every string in the
inverted lists, and returns the strings whose fre-
quency of occurrences is no smaller than τ.

This algorithm is inefﬁcient in that it scans
all strings in the inverted lists. The number of
scanned strings is large, especially when some
query features appear frequently in the strings,
e.g., ‘s$$’ (words ending with ‘s’) and ‘pre’
(words with substring ‘pre’). To make matters
worse, such features are too common for charac-
terizing string similarity. The AllScan algorithm

is able to maintain numerous candidate strings in
M, but most candidates are not likely to qualiﬁed
because they have few overlaps with X.

To reduce the number of the candidate strings,
we refer to signature-based algorithms (Arasu et
al., 2006; Chaudhuri et al., 2006):
Property 1 Let there be a set (of size h) X and a
set (of any size) Y . Consider any subset Z ⊆ X of
size (h− τ + 1). If |X ∩ Y | ≥ τ, then Z ∩ Y 6= φ.
We explain one usage of this property. Let query
string x = “methyl sulphone” and its trigram set
X be features (therefore, |X| = h = 17). Pre-
suming that we seek strings whose trigrams are
size 16 and have 12 overlaps with X, then string y
must have at least one overlap with any subset of
size 6 (= 17 − 12 + 1) of X. We call the subset
signatures. The property leads to an algorithmic
design by which we obtain a small set of candi-
date strings from the inverted lists for signatures,
(|X| − τ + 1) features in X, and verify whether
each candidate string satisﬁes the τ overlap with
the remaining (τ − 1) n-grams.
Algorithm 3 presents the pseudocode employ-
ing this idea. In line 1, we arrange the features in
X in ascending order of the number of strings in
their inverted lists. We denote the k-th element in
the ordered features as Xk (k ∈ {0, ...,|X| − 1}),
where the index number begins with 0. Based on
this notation, X0 and X|X|−1 are the most uncom-
mon and the most common features in X, respec-
tively.

In lines 2–7, we use (|X| − τ + 1) features

854

Input: X: array of features of the query string
Input: τ: minimum number of overlaps
Input: V : collection of strings
Input: l: size of target strings
Output: R: list of strings similar to the query
sort elements in X by order of |get(V , l, Xk)|;
M ← {};
for k ← 0 to (|X| − τ ) do
M [s] ← M [s] + 1;

foreach s ∈ get(V , l, Xk) do
end

end
R ← [];
for k ← (|X| − τ + 1) to (|X| − 1) do

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
end
21
return R;
22
Algorithm 3: CPMerge algorithm.

end
if τ ≤ M [s] then
append s to R;
remove s from M;

else if M [s] + (|X| − k − 1) < τ then
end

if bsearch(get(V , l, Xk), s) then

M [s] ← M [s] + 1;

foreach s ∈ M do

remove s from M;

end

X0, ..., X|X|−τ to generate a compact set of can-
didate strings. The algorithm stores the occur-
rence count of each string s in M [s]. In lines 9–
21, we increment the occurrence counts if each
of X|X|−τ +1, ..., X|X|−1 inverted lists contain the
candidate strings. For each string s in the candi-
dates (line 10), we perform a binary search on the
inverted list (line 11), and increment the overlap
count if the string s exists (line 12). If the overlap
counter of the string reaches τ (line 14), then we
append the string s to the result list R and remove
s from the candidate list (lines 15–16). We prune
a candidate string (lines 17–18) if the candidate is
found to be unreachable for τ overlaps even if it
appears in all of the unexamined inverted lists.

3 Experiments

We report the experimental results of approximate
dictionary matching on large-scale datasets with
person names, biomedical names, and general En-
glish words. We implemented various systems of
approximate dictionary matching.

• Proposed: CPMerge algorithm.

• Naive: Na¨ıve algorithm that computes the
cosine similarity |V | times for every query.
• AllScan: AllScan algorithm.
• Signature: CPMerge algorithm without
pruning; this is equivalent to Algorithm 3
without lines 17–18.
• DivideSkip: our implementation of the algo-
rithm (Li et al., 2008)2.
• Locality Sensitive Hashing (LSH) (Andoni
and Indyk, 2008): This baseline system fol-
lows the design of previous work (Ravichan-
dran et al., 2005). This system approxi-
mately solves Equation 1 by ﬁnding dictio-
nary entries whose LSH values are within
the (bit-wise) hamming distance of θ from
the LSH value of a query string. To adapt
the method to approximate dictionary match-
ing, we used a 64-bit LSH function com-
puted with letter trigrams. By design, this
method does not ﬁnd an exact solution to
Equation 1; in other words, the method can
miss dictionary entries that are actually sim-
ilar to the query strings. This system has
three parameters, θ, q (number of bit permu-
tations), and B (search width), to control the
tradeoff between retrieval speed and recall3.
Generally speaking, increasing these param-
eters improves the recall, but slows down the
speed. We determined θ = 24 and q = 24
experimentally4, and measured the perfor-
mance when B ∈ {16, 32, 64}.

The systems, excluding LSH, share the same
implementation of Algorithm 1 so that we can
speciﬁcally examine the differences of the algo-
rithms for τ-overlap join. The C++ source code of
the system used for this experiment is available5.
We ran all experiments on an application server
running Debian GNU/Linux 4.0 with Intel Xeon
5140 CPU (2.33 GHz) and 8 GB main memory.

3We

2We tuned parameter values µ ∈ {0.01, 0.02, 0.04, 0.1,
0.2, 0.4, 1, 2, 4, 10, 20, 40, 100} for each dataset. We se-
lected the parameter with the fastest response.
the original pa-
per (Ravichandran et al., 2005) here. Refer to the original
paper for deﬁnitions of the parameters θ, q, and B.

followed the notation of

4q was set to 24 so that the arrays of shufﬂed hash values
are stored in memory. We chose θ = 24 from {8, 16, 24} be-
cause it showed a good balance between accuracy and speed.

5

http://www.chokkan.org/software/simstring/

855

3.1 Datasets
We used three large datasets with person names
(IMDB actors), general English words (Google
Web1T), and biomedical names (UMLS).

• IMDB actors: This dataset comprises actor
names extracted from the IMDB database6.
We used all actor names (1,098,022 strings;
18 MB) from the ﬁle actors.list.gz.
The average number of letter trigrams in the
strings is 17.2. The total number of trigrams
is 42,180. The system generated index ﬁles
of 83 MB in 56.6 s.
• Google Web1T unigrams: This dataset con-
sists of English word unigrams included in
the Google Web1T corpus (LDC2006T13).
We used all word unigrams (13,588,391
strings; 121 MB) in the corpus after remov-
ing the frequency information. The aver-
age number of letter trigrams in the strings
is 10.3. The total number of trigrams is
301,459. The system generated index ﬁles
of 601 MB in 551.7 s.
• UMLS: This dataset consists of English
names and descriptions of biomedical con-
cepts included in the Uniﬁed Medical Lan-
guage System (UMLS). We extracted all
English concept names (5,216,323 strings;
212 MB) from MRCONSO.RRF.aa.gz and
MRCONSO.RRF.ab.gz in UMLS Release
2009AA. The average number of letter tri-
grams in the strings is 43.6. The total number
of trigrams is 171,596. The system generated
index ﬁles of 1.1 GB in 1216.8 s.

For each dataset, we prepared 1,000 query
strings by sampling strings randomly from the
dataset. To simulate the situation where query
strings are not only identical but also similar to
dictionary entries, we introduced random noise
to the strings.
In this experiment, one-third of
the query strings are unchanged from the original
(sampled) strings, one-third of the query strings
have one letter changed, and one-third of the
query strings have two letters changed. When
changing a letter, we randomly chose a letter po-
sition from a uniform distribution, and replaced

6

ftp://ftp.fu-berlin.de/misc/movies/database/

the letter at the position with an ASCII letter ran-
domly chosen from a uniform distribution.

3.2 Results
To examine the scalability of each system, we
controlled the number of strings to be indexed
from 10%–100%, and issued 1,000 queries. Fig-
ure 1 portrays the average response time for re-
trieving strings whose cosine similarity values are
no smaller than 0.7. Although LSH (B=16) seems
to be the fastest in the graph, this system missed
many true positives7; the recall scores of approx-
imate dictionary matching were 15.4% (IMDB),
13.7% (Web1T), and 1.5% (UMLS). Increasing
the parameter B improves the recall at the expense
of the response time. LSH (B=64)8. It not only
ran slower than the proposed method, but also
suffered from low recall scores, 25.8% (IMDB),
18.7% (Web1T), and 7.1% (UMLS). LSH was
useful only when we required a quick response
much more than recall.

The other systems were guaranteed to ﬁnd
the exact solution (100% recall). The proposed
algorithm was the fastest of all exact systems
on all datasets:
the response times per query
(100% index size) were 1.07 ms (IMDB), 1.10 ms
(Web1T), and 20.37 ms (UMLS). The response
times of the Na¨ıve algorithm were too slow, 32.8 s
(IMDB), 236.5 s (Web1T), and 416.3 s (UMLS).
The proposed algorithm achieved substantial
improvements over the AllScan algorithm:
the
proposed method was 65.3 times (IMDB), 227.5
times (Web1T), and 13.7 times (UMLS) faster
than the Na¨ıve algorithm. We observed that the
Signature algorithm, which is Algorithm 3 with-
out lines 17–18, did not perform well: The Sig-
nature algorithm was 1.8 times slower (IMDB),
2.1 times faster (Web1T), and 135.0 times slower
(UMLS) than the AllScan algorithm. These re-
sults indicate that it is imperative to minimize the
number of candidates to reduce the number of
binary-search operations. The proposed algorithm
was 11.1–13.4 times faster than DivideSkip.

Figure 2 presents the average response time

7Solving Equation 1, all systems are expected to retrieve

the exact set of strings retrieved by the Na¨ıve algorithm.

8The response time of LSH (B=64) on the IMDB dataset

was 29.72 ms (100% index size).

856

Figure 1: Average response time for processing a query (cosine similarity; α = 0.7).

Figure 2: Average response time for processing a query.

of the proposed algorithm for different similarity
measures and threshold values. When the similar-
ity threshold is lowered, the algorithm runs slower
because the number of retrieved strings |Y| in-
creases exponentially. The Dice coefﬁcient and
cosine similarity produced similar curves.

Table 2 summarizes the run-time statistics of
the proposed method for each dataset (with co-
sine similarity and threshold 0.7). Using the
IMDB dataset, the proposed method searched for
strings whose size was between 8.74 and 34.06;
it retrieved 4.63 strings per query string. The
proposed algorithm scanned 279.7 strings in 4.6
inverted lists to obtain 232.5 candidate strings.
The algorithm performed a binary search on 4.3
inverted lists containing 7,561.8 strings in all.
In contrast, the AllScan algorithm had to scan
16,155.1 strings in 17.7 inverted lists and con-
sidered 9,788.7 candidate strings, and found only
4.63 similar strings.

This table clearly demonstrates three key con-
tributions of the proposed algorithm for efﬁcient

approximate dictionary matching. First, the pro-
posed algorithm scanned far fewer strings than did
the AllScan algorithm. For example, to obtain
candidate strings in the IMDB dataset, the pro-
posed algorithm scanned 279.7 strings, whereas
the AllScan algorithm scanned 16,155.1 strings.
Therefore, the algorithm examined only 1.1%–
3.5% of the strings in the entire inverted lists in
the three datasets. Second, the proposed algo-
rithm considered far fewer candidates than did
the AllScan algorithm: the number of candidate
strings considered by the algorithm was 1.2%–
6.6% of those considered by the AllScan algo-
rithm. Finally, the proposed algorithm read fewer
inverted lists than did the AllScan algorithm. The
proposed algorithm actually read 8.9 (IMDB), 6.0
(Web1T), and 31.7 (UMLS) inverted lists during
the experiments9. These values indicate that the
proposed algorithm can solve τ-overlap join prob-
lems by checking only 50.3% (IMDB), 53.6%
(Web1T), and 51.9% of the total inverted lists re-

9These values are 4.6 + 4.3, 3.1 + 2.9, and 14.3 + 17.4.

0

5

10

15

20

25

0

20

40

60

80

100

]
s
m

[
 
y
r
e
u
q

 

 
r
e
p
e
s
n
o
p
s
e
r
 

e
g
a
r
e
v
A

Number of indexed strings (%)

Proposed

AllScan

Signature

DivideSkip

LSH (B=16)

LSH (B=32)

0

10

20

30

40

50

0

20

40

60

80

100

]
s
m

[
 
y
r
e
u
q

 

 
r
e
p
e
s
n
o
p
s
e
r
 

e
g
a
r
e
v
A

Number of indexed strings (%)

Proposed

AllScan

Signature

DivideSkip

LSH (B=16)

LSH (B=32)

LSH (B=64)

0

10

20

30

40

50

60

0

20

40

60

80

100

]
s
m

[
 
y
r
e
u
q

 

 
r
e
p
e
s
n
o
p
s
e
r
 

e
g
a
r
e
v
A

Number of indexed strings (%)

(a) IMDB actors

(b) Google Web1T unigrams

(c) UMLS

Proposed

AllScan

Signature

DivideSkip

LSH (B=16)

LSH (B=32)

LSH (B=64)

(a) IMDB actors

(b) Google Web1T unigram

(c) UMLS

0

5

10

15

20

25

30

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

]
s
m

[
 
y
r
e
u
q

 

 
r
e
p
e
s
n
o
p
s
e
r
 
e
g
a
r
e
v
A

Similarity threshold

Dice

Jaccard

Cosine

Overlap

0

10

20

30

40

50

60

70

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

]
s
m

[
 
y
r
e
u
q

 

 
r
e
p
e
s
n
o
p
s
e
r
 
e
g
a
r
e
v
A

Similarity threshold

Dice

Jaccard

Cosine

Overlap

0

50

100

150

200

250

300

350

400

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

]
s
m

[
 
y
r
e
u
q

 

 
r
e
p
e
s
n
o
p
s
e
r
 
e
g
a
r
e
v
A

Similarity threshold

Dice

Jaccard

Cosine

Overlap

857

Averaged item
min|y|
max|y|
τ
|Y|
Total
# inverted lists
# strings
# unique strings
Candidate stage
# inverted lists
# strings
# candidates
Validation stage
# inverted lists
# strings

Table 2: Run-time statistics of the proposed algorithm for each dataset

IMDB
8.74
34.06
14.13
4.63

Web1T
5.35
20.46
9.09
3.22

UMLS Description
21.87 minimum size of trigrams of target strings
88.48 maximum size of trigrams of target strings
47.77 minimum number of overlaps required/sufﬁcient per query
111.79

17.7
16 155.1
9 788.7

11.2
52 557.6
44 834.6

61.1
49 561.4
17 457.5

4.6
279.7
232.5

3.1
552.7
523.7

14.3
1 756.3
1 149.7

4.3
7 561.8

2.9
19 843.6

17.4
20 443.7

number of retrieved strings per query
— averaged for each query and target size:
number of inverted lists retrieved for a query
number of strings in the inverted list
number of unique strings in the inverted list
— averaged for each query and target size:
number of inverted lists scanned for generating candidates
number of strings scanned for generating candidates
number of candidates generated for a query
— averaged for each query and target size:
number of inverted lists examined by binary search for a query
number of strings targeted by binary search

trieved for queries.

4 Related Work
Numerous studies have addressed approximate
dictionary matching. The most popular conﬁgu-
ration uses n-grams as a string representation and
the edit distance as a similarity measure. Gra-
vano et al. (1998; 2001) presented various ﬁlter-
ing strategies, e.g., count ﬁltering, position ﬁl-
tering, and length ﬁltering, to reduce the num-
ber of candidates. Kim et al. (2005) proposed
two-level n-gram inverted indices (n-Gram/2L) to
eliminate the redundancy of position information
in n-gram indices. Li et al. (2007) explored the
use of variable-length grams (VGRAMs) for im-
proving the query performance. Lee et al. (2007)
extended n-grams to include wild cards and de-
veloped algorithms based on a replacement semi-
lattice. Xiao et al. (2008) proposed the Ed-Join
algorithm, which utilizes mismatching n-grams.

Several studies addressed different paradigms
for approximate dictionary matching. Bocek et
al. (2007) presented the Fast Similarity Search
(FastSS), an enhancement of the neighborhood
generation algorithms, in which multiple variants
of each string record are stored in a database.
Wang et al. (2009) further improved the technique
of neighborhood generation by introducing parti-
tioning and preﬁx pruning. Huynh et al. (2006)
developed a solution to the k-mismatch problem
in compressed sufﬁx arrays. Liu et al. (2008)
stored string records in a trie, and proposed a
framework called TITAN. These studies are spe-

cialized for the edit distance measure.

A few studies addressed approximate dictio-
nary matching for similarity measures such as
cosine and Jaccard similarities. Chaudhuri et
al. (2006) proposed the SSJoin operator for sim-
ilarity joins with several measures including the
edit distance and Jaccard similarity. This algo-
rithm ﬁrst generates signatures for strings, ﬁnds
all pairs of strings whose signatures overlap,
and ﬁnally outputs the subset of these candi-
date pairs that satisfy the similarity predicate.
Arasu et al. (2006) addressed signature schemes,
i.e., methodologies for obtaining signatures from
strings. They also presented an implementation of
the SSJoin operator in SQL. Although we did not
implement this algorithm in SQL, it is equivalent
to the Signature algorithm in Section 3.

Sarawagi and Kirpal (2004) proposed the Mer-
geOpt algorithm for the τ-overlap join to approx-
imate string matching with overlap, Jaccard, and
cosine measures. This algorithm splits inverted
lists for a given query A into two groups, S and
L, maintains a heap to collect candidate strings on
S, and performs a binary search on L to verify the
condition of the τ-overlap join for each candidate
string. Their subsequent work includes an efﬁ-
cient algorithm for the top-k search of the overlap
join (Chandel et al., 2006).

Li et al. (2008) extended this algorithm to the
SkipMerge and DivideSkip algorithms. The Skip-
Merge algorithm uses a heap to compute the τ-
overlap join on entire inverted lists A, but has
an additional mechanism to increment the fron-

858

tier pointers of inverted lists efﬁciently based on
the strings popped most recently from the heap.
Consequently, SkipMerge can reduce the number
of strings that are pushed to the heap. Similarly
to the MergeOpt algorithm, DivideSkip splits in-
verted lists A into two groups S and L, but it ap-
plies SkipMerge to S. In Section 3, we reported
the performance of DivideSkip.

Charikar (2002) presented the Locality Sen-
sitive Hash (LSH) function (Andoni and Indyk,
2008), which preserves the property of cosine
similarity. The essence of this function is to map
strings into N-bit hash values where the bitwise
hamming distance between the hash values of two
strings approximately corresponds to the angle of
the two strings. Ravichandran et al. (2005) ap-
plied LSH to the task of noun clustering. Adapting
this algorithm to approximate dictionary match-
ing, we discussed its performance in Section 3.

Several researchers have presented reﬁned sim-
ilarity measures for strings (Winkler, 1999; Cohen
et al., 2003; Bergsma and Kondrak, 2007; Davis et
al., 2007). Although these studies are sometimes
regarded as a research topic of approximate dic-
tionary matching, they assume that two strings for
the target of similarity computation are given; in
other words, it is out of their scope to ﬁnd strings
in a large collection that are similar to a given
string. Thus, it is a reasonable approach for an ap-
proximate dictionary matching to quickly collect
candidate strings with a loose similarity threshold,
and for a reﬁned similarity measure to scrutinize
each candidate string for the target application.

5 Conclusions
We present a simple and efﬁcient algorithm for
approximate dictionary matching with the co-
sine, Dice, Jaccard, and overlap measures. We
conducted experiments of approximate dictio-
nary matching on large-scale datasets with person
names, biomedical names, and general English
words. Even though the algorithm is very sim-
ple, our experimental results showed that the pro-
posed algorithm executed very quickly. We also
conﬁrmed that the proposed method drastically re-
duced the number of candidate strings considered
during approximate dictionary matching. We be-
lieve that this study will advance practical NLP

applications for which the execution time of ap-
proximate dictionary matching is critical.

An advantage of the proposed algorithm over
existing algorithms (e.g., MergeSkip) is that it
does not need to read all the inverted lists retrieved
by query n-grams. We observed that the proposed
algorithm solved τ-overlap joins by checking ap-
proximately half of the inverted lists (with cosine
similarity and threshold α = 0.7). This charac-
teristic is well suited to processing compressed
inverted lists because the algorithm needs to de-
compress only half of the inverted lists. It is nat-
ural to extend this study to compressing and de-
compressing inverted lists for reducing disk space
and for improving query performance (Behm et
al., 2009).

Acknowledgments

This work was partially supported by Grants-
in-Aid for Scientiﬁc Research on Priority Areas
(MEXT, Japan) and for Solution-Oriented Re-
search for Science and Technology (JST, Japan).

References
Andoni, Alexandr and Piotr Indyk.

2008. Near-
optimal hashing algorithms for approximate nearest
neighbor in high dimensions. Communications of
the ACM, 51(1):117–122.

Arasu, Arvind, Venkatesh Ganti, and Raghav Kaushik.
2006. Efﬁcient exact set-similarity joins. In VLDB
’06: Proceedings of the 32nd International Confer-
ence on Very Large Data Bases, pages 918–929.

Behm, Alexander, Shengyue Ji, Chen Li, and Jiaheng
Lu. 2009. Space-constrained gram-based indexing
for efﬁcient approximate string search.
In ICDE
’09: Proceedings of the 2009 IEEE International
Conference on Data Engineering, pages 604–615.

Bergsma, Shane and Grzegorz Kondrak.

2007.
Alignment-based discriminative string similarity. In
ACL ’07: Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 656–663.

Bocek, Thomas, Ela Hunt, and Burkhard Stiller. 2007.
Fast similarity search in large dictionaries. Tech-
nical Report iﬁ-2007.02, Department of Informatics
(IFI), University of Zurich.

859

Chandel, Amit, P. C. Nagesh, and Sunita Sarawagi.
2006. Efﬁcient batch top-k search for dictionary-
based entity recognition.
In ICDE ’06: Proceed-
ings of the 22nd International Conference on Data
Engineering.

Charikar, Moses S. 2002. Similarity estimation tech-
niques from rounding algorithms.
In STOC ’02:
Proceedings of the thiry-fourth annual ACM sym-
posium on Theory of computing, pages 380–388.

Chaudhuri, Surajit, Venkatesh Ganti, and Raghav
Kaushik. 2006. A primitive operator for similar-
ity joins in data cleaning. In ICDE ’06: Proceed-
ings of the 22nd International Conference on Data
Engineering.

Cohen, William W., Pradeep Ravikumar,

and
Stephen E. Fienberg.
2003. A comparison of
string distance metrics for name-matching tasks.
In Proceedings of the IJCAI-2003 Workshop on
Information Integration on the Web (IIWeb-03),
pages 73–78.

Davis, Jason V., Brian Kulis, Prateek Jain, Suvrit Sra,
and Inderjit S. Dhillon. 2007. Information-theoretic
metric learning. In ICML ’07: Proceedings of the
24th International Conference on Machine Learn-
ing, pages 209–216.

Li, Chen, Bin Wang, and Xiaochun Yang.

2007.
Vgram:
improving performance of approximate
queries on string collections using variable-length
grams. In VLDB ’07: Proceedings of the 33rd In-
ternational Conference on Very Large Data Bases,
pages 303–314.

Li, Chen, Jiaheng Lu, and Yiming Lu. 2008. Efﬁ-
cient merging and ﬁltering algorithms for approx-
imate string searches.
In ICDE ’08: Proceedings
of the 2008 IEEE 24th International Conference on
Data Engineering, pages 257–266.

Liu, Xuhui, Guoliang Li, Jianhua Feng, and Lizhu
Zhou. 2008. Effective indices for efﬁcient approxi-
mate string search and similarity join. In WAIM ’08:
Proceedings of the 2008 The Ninth International
Conference on Web-Age Information Management,
pages 127–134.

Manku, Gurmeet Singh, Arvind Jain, and Anish
Das Sarma. 2007. Detecting near-duplicates for
web crawling.
In WWW ’07: Proceedings of the
16th International Conference on World Wide Web,
pages 141–150.

Navarro, Gonzalo and Ricardo Baeza-Yates. 1998. A
practical q-gram index for text retrieval allowing er-
rors. CLEI Electronic Journal, 1(2).

Gravano, Luis, Panagiotis G. Ipeirotis, H. V. Jagadish,
Nick Koudas, S. Muthukrishnan, and Divesh Srivas-
tava. 2001. Approximate string joins in a database
(almost) for free. In VLDB ’01: Proceedings of the
27th International Conference on Very Large Data
Bases, pages 491–500.

Ravichandran, Deepak, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: us-
ing locality sensitive hash function for high speed
noun clustering.
In ACL ’05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 622–629.

Henzinger, Monika.

2006. Finding near-duplicate
web pages: a large-scale evaluation of algorithms.
In SIGIR ’06: Proceedings of the 29th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 284–
291.

Huynh, Trinh N. D., Wing-Kai Hon, Tak-Wah Lam,
and Wing-Kin Sung. 2006. Approximate string
matching using compressed sufﬁx arrays. Theoreti-
cal Computer Science, 352(1-3):240–249.

Kim, Min-Soo, Kyu-Young Whang, Jae-Gil Lee, and
Min-Jae Lee. 2005. n-Gram/2L: a space and time
efﬁcient two-level n-gram inverted index structure.
In VLDB ’05: Proceedings of the 31st International
Conference on Very Large Data Bases, pages 325–
336.

Lee, Hongrae, Raymond T. Ng, and Kyuseok Shim.
2007. Extending q-grams to estimate selectivity of
string matching with low edit distance.
In VLDB
’07: Proceedings of the 33rd International Confer-
ence on Very Large Data Bases, pages 195–206.

Sarawagi, Sunita and Alok Kirpal. 2004. Efﬁcient
set joins on similarity predicates. In SIGMOD ’04:
Proceedings of the 2004 ACM SIGMOD interna-
tional conference on Management of data, pages
743–754.

Wang, Wei, Chuan Xiao, Xuemin Lin, and Chengqi
Zhang. 2009. Efﬁcient approximate entity extrac-
tion with edit distance constraints.
In SIGMOD
’09: Proceedings of the 35th SIGMOD Interna-
tional Conference on Management of Data, pages
759–770.

Winkler, William E. 1999. The state of record link-
age and current research problems. Technical Re-
port R99/04, Statistics of Income Division, Internal
Revenue Service Publication.

Xiao, Chuan, Wei Wang, and Xuemin Lin. 2008. Ed-
Join: an efﬁcient algorithm for similarity joins with
edit distance constraints.
In VLDB ’08: Proceed-
ings of the 34th International Conference on Very
Large Data Bases, pages 933–944.

