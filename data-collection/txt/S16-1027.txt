



















































UNIMELB at SemEval-2016 Tasks 4A and 4B: An Ensemble of Neural Networks and a Word2Vec Based Model for Sentiment Classification


Proceedings of SemEval-2016, pages 183â€“189,
San Diego, California, June 16-17, 2016. cÂ©2016 Association for Computational Linguistics

UNIMELB at SemEval-2016 Tasks 4A and 4B: An Ensemble of
Neural Networks and a Word2Vec Based Model for Sentiment

Classification

XingYi Xu HuiZhi Liang Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne VIC 3010, Australia

stevenxxiu@gmail.com oklianghuizi@gmail.com tb@ldwin.net

Abstract

This paper describes our sentiment clas-
sification system for microblog-sized doc-
uments, and documents where a topic is
present. The system consists of a soft-
voting ensemble of a word2vec language
model adapted to classification, a convolu-
tional neural network (CNN), and a long-
short term memory network (LSTM). Our
main contribution consists of a way to in-
troduce topic information into this model,
by concatenating a topic embedding, con-
sisting of the averaged word embedding for
that topic, to each word embedding vector
in our neural networks. When we apply
our models to SemEval 2016 Task 4 sub-
tasks A and B, we demonstrate that the
ensemble performed better than any sin-
gle classifier, and our method of includ-
ing topic information achieves a substan-
tial performance gain. According to re-
sults on the official test sets, our model
ranked 3rd for ğ¹PNin the message-only
subtask A (among 34 teams) and 1st for
accuracy on the topic-dependent subtask
B (among 19 teams).

1 Introduction

The rapid growth of user-generated content,
much of which is sentiment-laden, has fueled an
interest in sentiment analysis (Pang and Lee,
2008; Liu, 2010). One popular form of senti-
ment analysis involves classifying a document
into discrete classes, depending on whether it
expresses positive or negative sentiment (or nei-
ther). The classification can also be dependent

upon a particular topic. In this work, we de-
scribe the method we used for the sentiment
classification of tweets, with or without a topic.

Our approach to the document classification
task consists of an ensemble of 3 classifiers via
soft-voting, 2 of which are neural network mod-
els. One is the convolutional neural network
(CNN) architecture of Kim (2014), and another
is a Long Short Term Memory (LSTM)-based
network (Hochreiter and Schmidhuber, 1997).
Both were first tuned on a distant-labelled data
set. The third classifier adapted word2vec to
output classification probabilities using Bayesâ€™
formula, a slightly modified version of Taddy
(2015). Despite the word2vec classifier being in-
tended as a baseline, and having a small weight
in the ensemble, it proved crucial for the ensem-
ble to work well. To adapt our models to the
case where a topic is present, in the neural net-
work models, we concatenated the embedding
vectors for each word with a topic embedding,
which consisted of the element-wise average of
all word vectors in a particular topic.

We applied our approach to SemEval 2016
Task 4, including the message-only subtask
(Task A) and the topic-dependent subtask (Task
B) (Nakov et al., to appear). Our model ranked
third for ğ¹PNin the message-only subtask A
(among 34 teams) and first for accuracy1 on the
topic-dependent subtask B (among 19 teams).

1There were some issues surrounding the evalua-
tion metrics. We only got 7th for ğœŒPNand 2nd for
ğ¹PNofficially, but when we retrained our model using
ğœŒPNas the subtask intended, we place first across all met-
rics.

183



The source code for our approach is available
at https://github.com/stevenxxiu/senti.

2 Models

We now describe the classifiers we used in de-
tail, our ensemble method, and our motivations
behind choosing these classifiers.

2.1 Convolutional neural network

We used the dynamic architecture of Kim (2014)
for our convolutional neural network. This con-
sists of a single 1-d convolution layer with a non-
linearity, a max-pooling layer, a dropout layer,
and a softmax classification layer.
This model was chosen since it was a good

performer empirically. However, due to max-
pooling, this model is essentially a bag-of-
phrases model, which ignores important order-
ing information if the tweet contains a long ar-
gument, or 2 sentences. We now give a review
of the layers used.

2.1.1 Word embedding layer

The input to the model is a document, treated
as a sequence of words. Each word can possi-
bly be represented by a vector of occurrences, a
vector of counts, or a vector of features. A vec-
tor of learnt, instead of hand-crafted features,
is also called a word embedding. This tends to
have dimensionality ğ‘‘ â‰ª |ğ‘‰ |, the vocabulary
size. Hence the vectors are dense, which allows
us to learn more functions of features with lim-
ited data.
Given an embedding of dimension ğ‘‘, ğ‘Šemb âˆˆ

Rğ‘‘Ã—|ğ‘‰ |, we mapped map each document ğ‘  to a
matrix ğ‘Šemb,ğ‘  âˆˆ Rğ‘‘Ã—|ğ‘ |, with each word corre-
sponding to a row vector in the order they ap-
pear in. ğ‘Šemb can be trained.

2.1.2 1-d convolution layer

A 1-d convolution layer aims to extract pat-
terns useful for classification, by sliding a fixed-
length filter along the input. The convolution
operation for an input matrix ğ‘† âˆˆ Rğ‘‘Ã—|ğ‘ | and
a single filter ğ¹ âˆˆ Rğ‘‘Ã—ğ‘š of width ğ‘š creates a
feature ğ‘¦conv âˆˆ R|ğ‘ |+ğ‘šâˆ’1 by:

ğ‘¦conv,ğ‘– =
âˆ‘ï¸

ğ‘˜,ğ‘—

(ğ‘†[:,ğ‘–:ğ‘–+ğ‘šâˆ’1] âŠ™ ğ¹ )ğ‘˜,ğ‘— + ğ‘conv

where âŠ™ is element-wise multiplication, and
ğ‘conv is a bias. There are typically ğ‘› > 1 filters,
which by stacking the feature vectors, results in
ğ‘Œconv âˆˆ Rğ‘›Ã—(|ğ‘ |+ğ‘šâˆ’1). Each filter has its own
separate bias.
We used a common modification to the fil-

ter sliding, by padding the document embedding
matrix with ğ‘šâˆ’1 zeroes on its top and bottom.
This is done so that every word in the document
is covered by ğ‘š filters.

2.1.3 Max pooling layer

There may be very few phrases targeted by a
feature map in the document. For this reason,
we only need to know if the desired feature is
present in the document, which can be obtained
by taking the maximum. Formally, we obtain a
vector ğ‘‘ âˆˆ Rğ‘›, such that:

ğ‘¦pool,ğ‘– = max
ğ‘—

ğ‘Œconv,ğ‘–,ğ‘—

2.1.4 Softmax layer

To convert our features into classification
probabilities, we first use a dense layer, defined
by:

ğ‘¦dense = ğ‘Šdense Â· ğ‘¦pool + ğ‘dense
with a softmax activation function:

softmax(ğ‘¥)ğ‘– =
ğ‘’ğ‘¥ğ‘–âˆ‘ï¸€
ğ‘– ğ‘’

ğ‘¥ğ‘–

such that the output dimension is the same as
the number of classes. Note that output values
are non-negative and sum to 1, which form a
discrete probability distribution.

2.1.5 Regularization

To regularize our CNN model, dropout (Sri-
vastava et al., 2014) is used after the max pool-
ing layer. Intuitively, dropout assumes that we
can still obtain a reasonable classification even
when some of the features are dropped. To do
this, each dimension is randomly set to 0 using
a Bernoulli distribution ğµ(ğ‘). In order to have
the training and testing to be of the same order,
the test outputs can be scaled by ğ‘.
The softmax layer for the CNN model also

uses a form of empirical Bayes regularization,
where each row of ğ‘Šsoft is restricted using an â„“2

184



norm, by re-normalizing the vector if the norm
threshold is exceeded.

2.2 Long short term memory network

We used an LSTM for our recurrent architec-
ture, which consisted of an embedding layer,
LSTM layer, and a softmax classification layer.

A recurrent neural network is a neural net-
work designed for sequential problems. Even
simple RNNs are Turing complete, and they can
theoretically obtain information from the entire
sequence instead of only an unordered bag of
phrases. But finding good architectures which
can capture this and training them can be diffi-
cult. Indeed, there were many instances where
our LSTM failed to capture important ordering
information. We now give a brief review of the
LSTM.

Given an input sequence ğ‘¥ = [ğ‘¥1, . . . , ğ‘¥ğ‘‡ ], a
recurrent network defines an internal state func-
tion ğ‘“ğ‘ and an output function ğ‘“ğ‘¦ to iterate over
ğ‘¥, so that at time step ğ‘¡:

ğ‘ğ‘¡ = ğ‘“ğ‘(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡âˆ’1, ğ‘ğ‘¡âˆ’1)

ğ‘¦ğ‘¡ = ğ‘“ğ‘¦(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡âˆ’1, ğ‘ğ‘¡)

where ğ‘0 and ğ‘¦0 are initial bias states. The sim-
plest RNN, where:

ğ‘“ğ‘¦(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡âˆ’1, ğ‘ğ‘¡) = tanh(ğ‘Š Â·
[ï¸‚
ğ‘¥ğ‘¡
ğ‘¦ğ‘¡âˆ’1

]ï¸‚
)

suffers from the gradient vanishing and ex-
ploding problem (Hochreiter and Schmidhuber,
1997). In particular, products of saturated tanh
activations can vanish the gradient, and prod-
ucts of ğ‘Š can vanish or explode the gradient.

The LSTM is a way to remedy this (Hochre-
iter and Schmidhuber, 1997). It sets:

ğ‘–ğ‘¡ = ğœğ‘–(ğ‘Šğ‘¥ğ‘–ğ‘¥ğ‘¡ +ğ‘Šâ„ğ‘–â„ğ‘¡âˆ’1 + ğ‘¤ğ‘ğ‘– âŠ™ ğ‘ğ‘¡âˆ’1 + ğ‘ğ‘–)
ğ‘“ğ‘¡ = ğœğ‘“ (ğ‘Šğ‘¥ğ‘“ğ‘¥ğ‘¡ +ğ‘Šâ„ğ‘“â„ğ‘¡âˆ’1 + ğ‘¤ğ‘ğ‘“ âŠ™ ğ‘ğ‘¡âˆ’1 + ğ‘ğ‘“ )
ğ‘ğ‘¡ = ğ‘“ğ‘¡ âŠ™ ğ‘ğ‘¡âˆ’1 + ğ‘–ğ‘¡ âŠ™ ğœğ‘(ğ‘Šğ‘¥ğ‘ğ‘¥ğ‘¡ +ğ‘Šâ„ğ‘â„ğ‘¡âˆ’1 + ğ‘ğ‘)
ğ‘¦ğ‘¡ = ğœğ‘œ(ğ‘Šğ‘¥ğ‘œğ‘¥ğ‘¡ +ğ‘Šâ„ğ‘œâ„ğ‘¡âˆ’1 + ğ‘¤ğ‘ğ‘œ âŠ™ ğ‘ğ‘¡ + ğ‘ğ‘œ)âŠ™ ğœâ„(ğ‘ğ‘¡)

Here, ğ‘Šâˆ™ is made up of weights; ğ‘–ğ‘¡, ğ‘“ğ‘¡, ğ‘ğ‘¡, ğ‘¦ğ‘¡
are the input gates, forget gates, cell states
and output states. Cell states have an identity

activation function. The gradient will not van-
ish if input ğ‘¥ğ‘– needs to be carried to ğ‘¥ğ‘— , since
this can only happen when the forget gates are
near 1. However, gradient explosion can still
be present. We use the common approach of
cutting off gradients above a threshold for the
gradients inside ğœğ‘–, ğœğ‘“ , ğœğ‘, ğœğ‘œ.

To use the LSTM, we first used a document
embedding matrix in the same manner as our
convolutional neural network architecture. This
was fed into to an LSTM layer. The output for
the final timestep of the LSTM layer was then
fed into a final softmax layer with the appropri-
ate output size for classification.
We also experimented with a similar and com-

monly used network, the Gated Recurrent Net-
work (GRU), but it was not used due to lower
results compared to the LSTM.

2.3 word2vec Bayes

Our word2vec Bayes model is our baseline
model, and described in Taddy (2015), with the
inclusion of a class prior. Taddy (2015) uses
Bayes formula to compute the probabilities of a
document belonging to a sentiment class. Given
a document ğ‘‘, its words {ğ‘¤}ğ‘–, label ğ‘¦, Bayes
formula is:

ğ‘(ğ‘¦|ğ‘‘) = ğ‘(ğ‘‘|ğ‘¦)ğ‘(ğ‘¦)
ğ‘(ğ‘‘)

For classification problems, we can ignore ğ‘(ğ‘‘)
since ğ‘‘ is fixed. ğ‘(ğ‘‘|ğ‘¦) is estimated by first train-
ing word2vec on a subset of the corpus with
label ğ‘¦, then using the skipgram objective com-
posite likelihood as an approximation:

log ğ‘(ğ‘‘|ğ‘¦) â‰ˆ
âˆ‘ï¸

ğ‘ âˆˆğ‘‘

|ğ‘ |âˆ‘ï¸

ğ‘—=1

|ğ‘ |âˆ‘ï¸

ğ‘˜=1

11â‰¤|ğ‘˜âˆ’ğ‘—|â‰¤ğ‘ log ğ‘(ğ‘¤ğ‘˜|ğ‘¤ğ‘— , ğ‘¦)

We estimated ğ‘(ğ‘¦) via class frequencies, i.e. the
MLE for the categorical distribution, compared
to Taddy (2015), who used the discrete uniform
prior.
This model was chosen since it provides a rea-

sonable baseline, and also appears to be inde-
pendent enough from our neural networks to
provide a performance gain in the ensemble.
The word2vec based model benefits from being

185



a semi-supervised method, but it also loses or-
dering information outside a wordâ€™s context win-
dow, and the prediction of neighboring words
also ignores distance to that word. The limited
amount of data for each class is also an issue.

2.4 Ensemble

If the errors made by each classifier are indepen-
dent enough, then combining them in an ensem-
ble can reduce the overall error rate. We used
soft voting as a method to combine the outputs
of the above classifiers. We define soft voting as:

ğ‘¦vote =
âˆ‘ï¸

ğ‘–

ğ‘¤ğ‘–ğ‘¦ğ‘–, s.t.
âˆ‘ï¸

ğ‘–

ğ‘¤ğ‘– = 1,âˆ€ğ‘– : ğ‘¤ğ‘– â‰¥ 0

where ğ‘¦ğ‘– is the output of classifier ğ‘–.

3 Topic dependent models

To adapt our neural networks to topic-
dependent sentiment classification, in our neural
network models, we augmented each embedding
vector by concatenating it with a topic embed-
ding. The motivation behind this approach is
to allow the model to interpret each word to be
within the context of some topic.

Our topic embeddings were obtained by the
element-wise average of word embeddings for
each word in that topic. We found that empiri-
cally, this is a simple yet effective way of achiev-
ing a document embedding. When used directly
as a feature vector in logistic regression for senti-
ment analysis, we have found this to outperform
methods described in Le and Mikolov (2014).

Word embeddings of dimension ğ‘‘ = 300 pre-
trained over Google News were used directly,
without any further tuning. Words in the tweet
which were not present in this pretrained em-
bedding were ignored.

4 Experiments and evaluation

We evaluated our models on SemEval 2016 Task
4 subtask A, the message-only subtask, and sub-
task B, the topic-dependent subtask.

4.1 Data

Task A consisted of 3 sentiment classes â€” posi-
tive, neutral and negative â€” whilst Task B

consisted of 2 sentiment classes â€” positive and
negative. We only managed to download 90%
of the entire set of tweets for the 2016 SemEval
data, due to tweets becoming â€œnot availableâ€. In
addition to the Twitter data for 2016, for Task
A we also used training data from SemEval 2016
Task 10. The data is summarized in Table 1.

To pretrain our network using distant learn-
ing (described below), we took a random sam-
ple of 10M English tweets from a 5.3TB Twitter
dataset crawled from 18 June to 4 Dec, 2014
using the Twitter Trending API. We then pro-
cessed tweets with a regular expression: tweets
which contained emoticons like :) were con-
sidered positive, while those which contained
emoticons like :( were considered negative;
tweets which contained both positive and nega-
tive emoticons, or others emoticons such as :â€” ,
were ignored. We extracted 1M tweets each for
the positive and negative classes.

4.2 Evaluation

Evaluation consisted of accuracy, macro-
averaged ğ¹1 across the positive and nega-
tive classes, which we denote ğ¹PN, and macro-
averaged recall across the positive and nega-
tive classes, which we denote ğœŒPN.

4.3 Preprocessing

All methods use the same preprocessing. We
normalized the tweets by first replacing URLs
with url and author methods such as @Ladi-
ibubblezz with author . Casing was preserved,
as the pretrained word2vec vectors included
casing. The tweets were tokenized using
twokenize, with itâ€™s being split into it and â€™s.

4.4 Training and hyperparameters

4.4.1 Neural networks

For both our models, we initialized ğ‘Šemb to
word embeddings pretrained using word2vecâ€™s
skip-gram model (Mikolov et al., 2013) on the
Google News corpus, where ğ‘‘ = 300. Unknown
words were drawn from ğ‘ˆ [âˆ’0.25, 0.25] to match
the variance of the pretrained vectors. For the
CNN model, we also stripped words so all doc-
uments had a length â‰¤ 56.

186



Datset A total A used B total B used

Twitter 2016-train 6000 5465 4346 3941
(+11340)

Twitter 2016-dev 2000 1829 1325 1210
Twitter 2016-test 2000 1807 1417 1270

Table 1: Semeval-2016 data. The negative : neutral : positive split was 16 : 42 : 42 for all of
2016 Task A used. The negative : positive split was 19 : 81 for all of 2016 Task B used.

For our CNN, we used used 3 1-d convolu-
tions, with filter sizes of 3, 4, 5, each with 100
filters. Our dropout rate was 0.5, and our â„“2
norm restriction was 3. For our LSTM, we used
a cell dimension size of 300, and our activations
were chosen empirically to be ğœğ‘– = ğœğ‘“ = ğœğ‘œ =
sigmoid, ğœğ‘ = tanh, our gradient cutoff was 100.

To train our neural networks, we used cross
entropy loss and minibatch gradient descent
with a batch size of 64. For our CNN, we
used the adadelta (Zeiler, 2012) gradient de-
scent method with the default settings. For our
LSTM, we used the rmsprop gradient descent
method with a learning rate of 0.01.

Due to limited training data, we can use dis-
tant learning (Severyn and Moschitti, 2015), by
initializing the weights of our neural networks
by first training them on a silver standard data
set (generated using Twitter emoticons which
we describe below), then tuning them further
on the gold standard data set (Severyn and Mos-
chitti, 2015). However, we did not use this for
our topic-dependent models, as there was no
performance gain.

We split the distant data into 104 tweets per
epoch, and took the best epoch on the valida-
tion set as the initial weights, using ğ¹PNas our
scoring metric. We repeatedly iterated over the
SemEval data with 103 tweets per epoch for 102

epochs, and again took the best epoch on the
validation set as the final weights.

4.4.2 word2vec Bayes

Gensim (RÌŒehÌŠurÌŒek and Sojka, 2010) was used
to train the word embeddings and obtain ğ‘(ğ‘‘|ğ‘¦).
We used the skipgram objective, with a embed-
ding dimension of 100, window size of 10, hi-
erarchical softmax with 5 samples, 20 training
iterations, no frequent word cutoff, and a sam-

Dataset ğ¹PN

Twitter 2013 0.6877
Twitter 2014 0.7067
Twitter 2015 0.6504
Twitter 2016 0.6173
Twitter Sarcasm 2014 0.44911
SMS 2013 0.59310
LiveJournal 2014 0.6839

Table 2: Official test scores and ranks for Task
A.

Dataset Val metric ğœŒPN ğ¹PN Acc

Twitter 2016 ğ¹PN 0.7587 0.7882 0.8701
Twitter 2016 ğœŒPN 0.8071 0.8061 0.8671

Table 3: Test scores and ranks for Task B. The
official run incorrectly used ğ¹PNas the validation
metric.

ple coefficient of 0.

4.4.3 Soft voting

To find ğ‘¤ğ‘–, we first relaxed the sum condi-
tion of ğ‘¤ğ‘– by setting ğ‘¤1 = 1 and noting that
maxğ‘˜ ğ‘¦ğ‘£ğ‘œğ‘¡ğ‘’,ğ‘–,ğ‘˜ is invariant under scaling. We
then used the L-BFGS-B algorithm, with ini-
tial weights of 1, combined with basin-hopping
for 1000 iterations. We optimized for accuracy,
since this most-consistently improved results.

5 Results

The official evaluation results are shown in Ta-
ble 2 and Table 3. The results for Task A suggest
that our models are overfitting. Our best posi-
tion was achieved on the Twitter 2016 dataset,
and indeed, this is what our parameters were
chosen on.

Our own evaluation of our different classifiers,
using Twitter 2016-test, is shown in Table 4 and

187



Model Accuracy ğ¹PN

soft voting all 0.5772 0.6000
lstm 0.5379 0.5869
soft voting cnn + lstm 0.5606 0.5848
cnn 0.5612 0.5841
word2vec Bayes 0.5130 0.4983

Table 4: Results for Task A, sorted by ğ¹PN.

Model Accuracy ğœŒPN

soft voting all 0.8008 0.7849
soft voting cnn + lstm 0.7976 0.7846
cnn topic 0.8047 0.7762
lstm topic 0.6756 0.7494

cnn 0.8354 0.7253
word2vec Bayes 0.7654 0.7138
lstm 0.8118 0.6916

Table 5: Results for Task B, sorted by ğœŒPN.
The dashed line separates topic models from
message-only models. The lstm topic model has
poorer accuracy due to being optimized on ğœŒPN.

Table 5. Taking into account all evaluation met-
rics, we can see that in both tasks, our CNN
outperforms our LSTM, in Task A slightly and
in Task B substantially. The word2vec Bayes
model is worse than both, moreso in Task B.

Soft voting outperforms all classifiers, showing
that there is some independence amongst the er-
rors made. In Task A, there appears to be more
correlation between the CNN and LSTM classi-
fiers, as excluding the word2vec Bayes model re-
duces the performance. In Task B, the word2vec
Bayes model appears to perform too poorly to
provide a marked benefit.

From Table 5 we can see that the inclusion of
topic information provides a substantial boost
to both of our neural networks. This shows that
our method of incorporating topic information is
a useful way of modifying neural networks, and
provides a strong baseline for alternative ways
of doing this.

6 Conclusions

We described our ensemble approach to senti-
ment analysis both the task of topic-dependent
document classification and document classifi-

cation by itself. We gave a detailed descrip-
tion of how to modify our classifiers to be topic-
dependent. The results show that ensembles can
work for neural nets, and that our way of includ-
ing topics achieves performance gains, and forms
a good basis for future research in this area.

References

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and JuÌˆrgen Schmidhuber. 1997. Long short-term
memory. Neural computation, 9(8):1735â€“1780.

[Kim2014] Yoon Kim. 2014. Convolutional neural
networks for sentence classification. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1746â€“1751, Doha, Qatar.

[Le and Mikolov2014] Quoc V. Le and Tomas
Mikolov. 2014. Distributed representations of
sentences and documents. In arXiv preprint
arXiv:1405.4053.

[Liu2010] Bing Liu. 2010. Sentiment analysis and
subjectivity. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing. Chapman & Hall/CRC, Boca Raton,
USA, 2nd edition.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S. Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
and their compositionality. In Advances in neural
information processing systems, pages 3111â€“3119.

[Nakov et al.to appear] Preslav Nakov, Alan Ritter,
Sara Rosenthal, Veselin Stoyanov, and Fabrizio
Sebastiani. to appear. SemEval-2016 Task 4: Sen-
timent analysis in Twitter. In Proceedings of the
10th International Workshop on Semantic Evalu-
ation (SemEval 2016), San Diego, USA.

[Pang and Lee2008] Bo Pang and Lillian Lee. 2008.
Opinion mining and sentiment analysis. Founda-
tions and trends in information retrieval, 2(1-2):1â€“
135.

[RÌŒehÌŠurÌŒek and Sojka2010] Radim RÌŒehÌŠurÌŒek and Petr
Sojka. 2010. Software framework for topic
modelling with large corpora. In Proceed-
ings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45â€“50,
Valletta, Malta. ELRA. http://is.muni.cz/
publication/884893/en.

[Severyn and Moschitti2015] Aliaksei Severyn and
Alessandro Moschitti. 2015. UNITN: Training
deep convolutional neural network for Twitter sen-
timent classification. In Proceedings of the 9th

188



International Workshop on Semantic Evaluation
(SemEval), pages 464â€“469, Denver, USA.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey
Hinton, Alex Krizhevsky, Ilya Sutskever, and Rus-
lan Salakhutdinov. 2014. Dropout: A sim-
ple way to prevent neural networks from overfit-
ting. The Journal of Machine Learning Research,
15(1):1929â€“1958.

[Taddy2015] Matt Taddy. 2015. Document classifi-
cation by inversion of distributed language repre-
sentations. In arXiv:1504.07295 [cs, stat].

[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
An adaptive learning rate method. arXiv preprint
arXiv:1212.5701.

189


