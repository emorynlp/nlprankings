










































Open-domain Commonsense Reasoning Using Discourse Relations from a Corpus of Weblog Stories


Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 43–51,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Open-domain Commonsense Reasoning Using Discourse Relations from a

Corpus of Weblog Stories

Matt Gerber

Department of Computer Science
Michigan State University
gerberm2@msu.edu

Andrew S. Gordon and Kenji Sagae
Institute for Creative Technologies
University of Southern California

{gordon,sagae}@ict.usc.edu

Abstract

We present a method of extracting open-

domain commonsense knowledge by apply-

ing discourse parsing to a large corpus of per-

sonal stories written by Internet authors. We

demonstrate the use of a linear-time, joint syn-

tax/discourse dependency parser for this pur-

pose, and we show how the extracted dis-

course relations can be used to generate open-

domain textual inferences. Our evaluations

of the discourse parser and inference models

show some success, but also identify a num-

ber of interesting directions for future work.

1 Introduction

The acquisition of open-domain knowledge in sup-

port of commonsense reasoning has long been a

bottleneck within artificial intelligence. Such rea-

soning supports fundamental tasks such as textual

entailment (Giampiccolo et al., 2008), automated

question answering (Clark et al., 2008), and narra-

tive comprehension (Graesser et al., 1994). These

tasks, when conducted in open domains, require vast

amounts of commonsense knowledge pertaining to

states, events, and their causal and temporal relation-

ships. Manually created resources such as FrameNet

(Baker et al., 1998), WordNet (Fellbaum, 1998), and

Cyc (Lenat, 1995) encode many aspects of com-

monsense knowledge; however, coverage of causal

and temporal relationships remains low for many do-

mains.

Gordon and Swanson (2008) argued that the

commonsense tasks of prediction, explanation, and

imagination (collectively called envisionment) can

be supported by knowledge mined from a large cor-

pus of personal stories written by Internet weblog

authors.1 Gordon and Swanson (2008) identified

three primary obstacles to such an approach. First,

stories must be distinguished from other weblog

content (e.g., lists, recipes, and reviews). Second,

stories must be analyzed in order to extract the im-

plicit commonsense knowledge that they contain.

Third, inference mechanisms must be developed that

use the extracted knowledge to perform the core en-

visionment tasks listed above.

In the current paper, we present an approach to

open-domain commonsense inference that addresses

each of the three obstacles identified by Gordon and

Swanson (2008). We built on the work of Gordon

and Swanson (2009), who describe a classification-

based approach to the task of story identification.

The authors’ system produced a corpus of approx-

imately one million personal stories, which we used

as a starting point. We applied efficient discourse

parsing techniques to this corpus as a means of ex-

tracting causal and temporal relationships. Further-

more, we developed methods that use the extracted

knowledge to generate textual inferences for de-

scriptions of states and events. This work resulted

in an end-to-end prototype system capable of gen-

erating open-domain, commonsense inferences us-

ing a repository of knowledge extracted from un-

structured weblog text. We focused on identifying

1We follow Gordon and Swanson (2009) in defining a story

to be a “textual discourse that describes a specific series of

causally related events in the past, spanning a period of time

of minutes, hours, or days, where the author or a close associate

is among the participants.”

43



strengths and weaknesses of the system in an effort

to guide future work.

We structure our presentation as follows: in Sec-

tion 2, we present previous research that has inves-

tigated the use of large web corpora for natural lan-

guage processing (NLP) tasks. In Section 3, we de-

scribe an efficient method of automatically parsing

weblog stories for discourse structure. In Section 4,

we present a set of inference mechanisms that use

the extracted discourse relations to generate open-

domain textual inferences. We conclude, in Section

5, with insights into story-based envisionment that

we hope will guide future work in this area.

2 Related work

Researchers have made many attempts to use the

massive amount of linguistic content created by

users of the World Wide Web. Progress and chal-

lenges in this area have spawned multiple workshops

(e.g., those described by Gurevych and Zesch (2009)

and Evert et al. (2008)) that specifically target the

use of content that is collaboratively created by In-

ternet users. Of particular relevance to the present

work is the weblog corpus developed by Burton et

al. (2009), which was used for the data challenge

portion of the International Conference on Weblogs

and Social Media (ICWSM). The ICWSM weblog

corpus (referred to here as Spinn3r) is freely avail-

able and comprises tens of millions of weblog en-

tries posted between August 1st, 2008 and October

1st, 2008.

Gordon et al. (2009) describe an approach to

knowledge extraction over the Spinn3r corpus using

techniques described by Schubert and Tong (2003).

In this approach, logical propositions (known as fac-

toids) are constructed via approximate interpreta-

tion of syntactic analyses. As an example, the sys-

tem identified a factoid glossed as “doors to a room

may be opened”. Gordon et al. (2009) found that

the extracted factoids cover roughly half of the fac-

toids present in the corresponding Wikipedia2 arti-

cles. We used a subset of the Spinn3r corpus in

our work, but focused on discourse analyses of en-

tire texts instead of syntactic analyses of single sen-

tences. Our goal was to extract general causal and

temporal propositions instead of the fine-grained

2http://en.wikipedia.org

properties expressed by many factoids extracted by

Gordon et al. (2009).

Clark and Harrison (2009) pursued large-scale

extraction of knowledge from text using a syntax-

based approach that was also inspired by the work

of Schubert and Tong (2003). The authors showed

how the extracted knowledge tuples can be used

to improve syntactic parsing and textual entailment

recognition. Bar-Haim et al. (2009) present an ef-

ficient method of performing inference with such

knowledge.

Our work is also related to the work of Persing

and Ng (2009), in which the authors developed a

semi-supervised method of identifying the causes of

events described in aviation safety reports. Simi-

larly, our system extracts causal (as well as tem-

poral) knowledge; however, it does this in an open

domain and does not place limitations on the types

of causes to be identified. This greatly increases

the complexity of the inference task, and our results

exhibit a corresponding degradation; however, our

evaluations provide important insights into the task.

3 Discourse parsing a corpus of stories

Gordon and Swanson (2009) developed a super-

vised classification-based approach for identifying

personal stories within the Spinn3r corpus. Their

method achieved 75% precision on the binary task

of predicting story versus non-story on a held-out

subset of the Spinn3r corpus. The extracted “story

corpus” comprises 960,098 personal stories written

by weblog users. Due to its large size and broad

domain coverage, the story corpus offers unique op-

portunities to NLP researchers. For example, Swan-

son and Gordon (2008) showed how the corpus can

be used to support open-domain collaborative story

writing.3

As described by Gordon and Swanson (2008),

story identification is just the first step towards com-

monsense reasoning using personal stories. We ad-

dressed the second step - knowledge extraction -

by parsing the corpus using a Rhetorical Structure

Theory (Carlson and Marcu, 2001) parser based on

the one described by Sagae (2009). The parser

performs joint syntactic and discourse dependency

3The system (called SayAnything) is available at

http://sayanything.ict.usc.edu

44



parsing using a stack-based, shift-reduce algorithm

with runtime that is linear in the input length. This

lightweight approach is very efficient; however, it

may not be quite as accurate as more complex, chart-

based approaches (e.g., the approach of Charniak

and Johnson (2005) for syntactic parsing).

We trained the discourse parser over the causal

and temporal relations contained in the RST corpus.

Examples of these relations are shown below:

(1) [cause Packages often get buried in the load]
[result and are delivered late.]

(2) [before Three months after she arrived in L.A.]
[after she spent $120 she didn’t have.]

The RST corpus defines many fine-grained rela-

tions that capture causal and temporal properties.

For example, the corpus differentiates between re-

sult and reason for causation and temporal-after and

temporal-before for temporal order. In order to in-

crease the amount of available training data, we col-

lapsed all causal and temporal relations into two

general relations causes and precedes. This step re-

quired normalization of asymmetric relations such

as temporal-before and temporal-after.

To evaluate the discourse parser described above,

we manually annotated 100 randomly selected we-

blog stories from the story corpus produced by Gor-

don and Swanson (2009). For increased efficiency,

we limited our annotation to the generalized causes

and precedes relations described above. We at-

tempted to keep our definitions of these relations

in line with those used by RST. Following previous

discourse annotation efforts, we annotated relations

over clause-level discourse units, permitting rela-

tions between adjacent sentences. In total, we an-

notated 770 instances of causes and 1,009 instances

of precedes.

We experimented with two versions of the RST

parser, one trained on the fine-grained RST rela-

tions and the other trained on the collapsed relations.

At testing time, we automatically mapped the fine-

grained relations to their corresponding causes or

precedes relation. We computed the following ac-

curacy statistics:

Discourse segmentation accuracy For each pre-

dicted discourse unit, we located the reference

discourse unit with the highest overlap. Accu-

racy for the predicted discourse unit is equal to

the percentage word overlap between the refer-

ence and predicted discourse units.

Argument identification accuracy For each dis-

course unit of a predicted discourse relation,

we located the reference discourse unit with the

highest overlap. Accuracy is equal to the per-

centage of times that a reference discourse rela-

tion (of any type) holds between the reference

discourse units that overlap most with the pre-

dicted discourse units.

Argument classification accuracy For the subset

of instances in which a reference discourse re-

lation holds between the units that overlap most

with the predicted discourse units, accuracy is

equal to the percentage of times that the pre-

dicted discourse relation matches the reference

discourse relation.

Complete accuracy For each predicted discourse

relation, accuracy is equal to the percentage

word overlap with a reference discourse rela-

tion of the same type.

Table 1 shows the accuracy results for the fine-

grained and collapsed versions of the RST discourse

parser. As shown in Table 1, the collapsed version

of the discourse parser exhibits higher overall ac-

curacy. Both parsers predicted the causes relation

much more often than the precedes relation, so the

overall scores are biased toward the scores for the

causes relation. For comparison, Sagae (2009) eval-

uated a similar RST parser over the test section of

the RST corpus, obtaining precision of 42.9% and

recall of 46.2% (F1 = 44.5%).
In addition to the automatic evaluation described

above, we also manually assessed the output of the

discourse parsers. One of the authors judged the

correctness of each extracted discourse relation, and

we found that the fine-grained and collapsed ver-

sions of the parser performed equally well with a

precision near 33%; however, throughout our exper-

iments, we observed more desirable discourse seg-

mentation when working with the collapsed version

of the discourse parser. This fact, combined with the

results of the automatic evaluation presented above,

45



Fine-grained RST parser Collapsed RST parser

Accuracy metric causes precedes overall causes precedes overall

Segmentation 36.08 44.20 36.67 44.36 30.13 43.10

Argument identification 25.00 33.33 25.86 26.15 23.08 25.87

Argument classification 66.15 50.00 64.00 79.41 83.33 79.23

Complete 22.20 28.88 22.68 31.26 21.21 30.37

Table 1: RST parser evaluation. All values are percentages.

led us to use the collapsed version of the parser in

all subsequent experiments.

Having developed and evaluated the discourse

parser, we conducted a full discourse parse of the

story corpus, which comprises more than 25 million

sentences split into nearly 1 million weblog entries.

The discourse parser extracted 2.2 million instances

of the causes relation and 220,000 instances of the

precedes relation. As a final step, we indexed the

extracted discourse relations with the Lucene infor-

mation retrieval engine.4 Each discourse unit (two

per discourse relation) is treated as a single docu-

ment, allowing us to query the extracted relations

using information retrieval techniques implemented

in the Lucene toolkit.

4 Generating textual inferences

As mentioned previously, Gordon and Swan-

son (2008) cite three obstacles to performing com-

monsense reasoning using weblog stories. Gordon

and Swanson (2009) addressed the first (story col-

lection). We addressed the second (story analysis)

by developing a discourse parser capable of extract-

ing causal and temporal relations from weblog text

(Section 3). In this section, we present a prelimi-

nary solution to the third problem - reasoning with

the extracted knowledge.

4.1 Inference method

In general, we require an inference method that takes

as input the following things:

1. A description of the state or event of interest.

This is a free-text description of any length.

2. The type of inference to perform, either causal

or temporal.

4Available at http://lucene.apache.org

3. The inference direction, either forward or back-

ward. Forward causal inference produces the

effects of the given state or event. Backward

causal inference produces causes of the given

state or event. Similarly, forward and back-

ward temporal inferences produce subsequent

and preceding states and events, respectively.

As a simple baseline approach, we implemented the

following procedure. First, given a textual input de-

scription d, we query the extracted discourse units

using Lucene’s modified version of the vector space

model over TF-IDF term weights. This produces a

ranked list Rd of discourse units matching the input

description d. We then filterRd, removing discourse

units that are not linked to other discourse units by

the given relation and in the given direction. Each el-

ement of the filtered Rd is thus linked to a discourse

unit that could potentially satisfy the inference re-

quest.

To demonstrate, we perform forward causal infer-

ence using the following input description d:

(3) John traveled the world.

Below, we list the three top-ranked discourse units

that matched d (left-hand side) and their associated

consequents (right-hand side):

1. traveling the world→ to murder

2. traveling from around the world to be there →
even though this crowd was international

3. traveled across the world→ to experience it

In a naı̈ve way, one might simply choose the top-

ranked clause in Rd and select its associated clause

as the answer to the inference request; however, in

the example above, this would incorrectly generate

“to murder” as the effect of John’s traveling (this is

46



more appropriately viewed as the purpose of trav-

eling). The other effect clauses also appear to be

incorrect. This should not come as much of a sur-

prise because the ranking was generated soley from

the match score between the input description and

the causes in Rd, which are quite relevant.

One potential problem with the naı̈ve selection

method is that it ignores information contained in

the ranked list R′d of clauses that are associated with

the clauses in Rd. In our experiments, we often

observed redundancies in R′d that captured general

properties of the desired inference. Intuitively, con-

tent that is shared across elements ofR′d could repre-

sent the core meaning of the desired inference result.

In what follows, we describe various re-rankings

of R′d using this shared content. For each model

described, the final inference prediction is the top-

ranked element of R′d.

Centroid similarity To approximate the shared

content of discourse units in R′d, we treat each

discourse unit as a vector of TF scores. We then

compute the average vector and re-rank all dis-

course units in R′d based on their cosine simi-

larity with the average vector. This favors infer-

ence results that “agree” with many alternative

hypotheses.

Description score scaling In this approach, we in-

corporate the score from Rd into the centroid

similarity score, multiplying the two and giving

equal weight to each. This captures the intu-

ition that the top-ranked element of R′d should

represent the general content of the list but

should also be linked to an element of Rd that

bears high similarity to the given state or event

description d.

Log-length scaling When working with the cen-

troid similarity score, we often observed top-

ranked elements of R′d that were only a few

words in length. This was typically the case

when components from sparse TF vectors in

R
′

d matched well with components from the

centroid vector. Ideally, we would like more

lengthy (but not too long) descriptions. To

achieve this, we multiplied the centroid simi-

larity score by the logarithm of the word length

of the discourse unit in R′d.

Description score/log-length scaling In this ap-

proach, we combine the description score scal-

ing and log-length scaling, multiplying the cen-

troid similarity by both and giving equal weight

to all three factors.

4.2 Evaluating the generated textual inferences

To evaluate the inference re-ranking models de-

scribed above, we automatically generated for-

ward/backward causal and temporal inferences for

five documents (265 sentences) drawn randomly

from the story corpus. For simplicity, we gener-

ated an inference for each sentence in each docu-

ment. Each inference re-ranking model is able to

generate four textual inferences (forward/backward

causal/temporal) for each sentence. In our experi-

ments, we only kept the highest-scoring of the four

inferences generated by a model. One of the authors

then manually evaluated the final predictions for cor-

rectness. This was a subjective process, but it was

guided by the following requirements:

1. The generated inference must increase the lo-

cal coherence of the document. As described

by Graesser et al. (1994), readers are typically

required to make inferences about the text that

lead to a coherent understanding thereof. We

required the generated inferences to aid in this

task.

2. The generated inferences must be globally

valid. To demonstrate global validity, consider

the following actual output:

(4) I didn’t even need a jacket (until I got

there).

In Example 4, the system-generated forward

temporal inference is shown in parentheses.

The inference makes sense given its local con-

text; however, it is clear from the surround-

ing discourse (not shown) that a jacket was not

needed at any point in time (it happened to be

a warm day). As a result, this prediction was

tagged as incorrect.

Table 2 presents the results of the evaluation. As

shown in the table, the top-performing models are

those that combine centroid similarity with one or

both of the other re-ranking heuristics.

47



Re-ranking model Inference accuracy (%)

None 10.19

Centroid similarity 12.83

Description score scaling 17.36

Log-length scaling 12.83

Description score/log-length scaling 16.60

Table 2: Inference generation evaluation results.

0

0.05

0.1

0.15

0.2

0.25

0.3

0.
05 0.

1
0.

15 0.
2

0.
25 0.

3
0.

35 0.
4

0.
45 0.

5
0.

55 0.
6

0.
65 0.

7
0.

75 0.
8

0.
85 0.

9
0.

95 1

Confidence-ordered percentage of all 

inferences

In
fe

re
n

c
e
 a

c
c
u

ra
c
y

None

Centroid similarity

Description score scaling

Log-length scaling

Combined scaling

Figure 1: Inference rate versus accuracy. Values along the x-axis indicate that the top-scoring x% of all inferences

were evaluated. Values along the y-axis indicate the prediction accuracy.

The analysis above demonstrates the relative per-

formance of the models when making inferences for

all sentences; however it is probably the case that

many generated inferences should be rejected due to

their low score. Because the output scores of a single

model can be meaningfully compared across predic-

tions, it is possible to impose a threshold on the in-

ference generation process such that any prediction

scoring at or below the threshold is withheld. We

varied the prediction threshold from zero to a value

sufficiently large that it excluded all predictions for

a model. Doing so demonstrates the trade-off be-

tween making a large number of textual inferences

and making accurate textual inferences. Figure 1

shows the effects of this variable on the re-ranking

models. As shown in Figure 1, the highest infer-

ence accuracy is reached by the re-ranker that com-

bines description score and log-length scaling with

the centroid similarity measure. This accuracy is at-

tained by keeping the top 25% most confident infer-

ences.

5 Conclusions

We have presented an approach to commonsense

reasoning that relies on (1) the availability of a large

corpus of personal weblog stories and (2) the abil-

ity to analyze and perform inference with these sto-

ries. Our current results, although preliminary, sug-

gest novel and important areas of future exploration.

We group our observations according to the last two

problems identified by Gordon and Swanson (2008):

story analysis and envisioning with the analysis re-

sults.

5.1 Story analysis

As in other NLP tasks, we observed significant per-

formance degradation when moving from the train-

ing genre (newswire) to the testing genre (Internet

48



weblog stories). Because our discourse parser relies

heavily on lexical and syntactic features for classi-

fication, and because the distribution of the feature

values varies widely between the two genres, the

performance degradation is to be expected. Recent

techniques in parser adaptation for the Brown corpus

(McClosky et al., 2006) might be usefully applied to

the weblog genre as well.

Our supervised classification-based approach to

discourse parsing could also be improved with ad-

ditional training data. Causal and temporal relations

are instantiated a combined 2,840 times in the RST

corpus, with a large majority of these being causal.

In contrast, the Penn Discourse TreeBank (Prasad et

al., 2008) contains 7,448 training instances of causal

relations and 2,763 training instances of temporal

relations. This represents a significant increase in

the amount of training data over the RST corpus. It

would be informative to compare our current results

with those obtained using a discourse parser trained

on the Penn Discourse TreeBank.

One might also extract causal and temporal rela-

tions using traditional semantic role analysis based

on FrameNet (Baker et al., 1998) or PropBank

(Kingsbury and Palmer, 2003). The former defines a

number of frames related to causation and temporal

order, and roles within the latter could be mapped to

standard thematic roles (e.g., cause) via SemLink.5

5.2 Envisioning with the analysis results

We believe commonsense reasoning based on we-

blog stories can also be improved through more so-

phisticated uses of the extracted discourse relations.

As a first step, it would be beneficial to explore alter-

nate input descriptions. As presented in Section 4.2,

we make textual inferences at the sentence level for

simplicity; however, it might be more reasonable to

make inferences at the clause level, since clauses are

the basis for RST and Penn Discourse TreeBank an-

notation. This could result in the generation of sig-

nificantly more inferences due to multi-clause sen-

tences; thus, more intelligent inference filtering will

be required.

Our models use prediction scores for the tasks

of rejecting inferences and selecting between mul-

tiple candidate inferences (i.e., forward/backward

5Available at http://verbs.colorado.edu/semlink

causal/temporal). Instead of relying on prediction

scores for these tasks, it might be advantageous to

first identify whether or not envisionment should be

performed for a clause, and, if it should, what type

and direction of envisionment would be best. For

example, consider the following sentence:

(5) [clause1 John went to the store] [clause2
because he was hungry].

It would be better - from a local coherence perspec-

tive - to infer the cause of the second clause instead

of the cause of the first. This is due to the fact that a

cause for the first clause is explicitly stated, whereas

a cause for the second clause is not. Inferences made

about the first clause (e.g., that John went to the store

because his dog was hungry), are likely to be unin-

formative or in conflict with explicitly stated infor-

mation.

Example 5 raises the important issue of context,

which we believe needs to be investigated further.

Here, context refers to the discourse that surrounds

the clause or sentence for which the system is at-

tempting to generate a textual inference. The con-

text places a number of constraints on allowable in-

ferences. For example, in addition to content-based

constraints demonstrated in Example 5, the context

limits pronoun usage, entity references, and tense.

Violations of these constraints will reduce local co-

herence.

Finally, the story corpus, with its vast size, is

likely to contain a significant amount of redundancy

for common events and states. Our centroid-based

re-ranking heuristics are inspired by this redun-

dancy, and we expect that aggregation techniques

such as clustering might be of some use when ap-

plied to the corpus as a whole. Having identified

coherent clusters of causes, it might be easier to find

a consequence for a previously unseen cause.

In summary, we have presented preliminary re-

search into the task of using a large, collaboratively

constructed corpus as a commonsense knowledge

repository. Rather than relying on hand-coded on-

tologies and event schemas, our approach relies on

the implicit knowledge contained in written natu-

ral language. We have demonstrated the feasibility

of obtaining the discourse structure of such a cor-

pus via linear-time parsing models. Furthermore,

49



we have introduced inference procedures that are ca-

pable of generating open-domain textual inferences

from the extracted knowledge. Our evaluation re-

sults suggest many opportunities for future work in

this area.

Acknowledgments

The authors would like to thank the anonymous

reviewers for their helpful comments and sugges-

tions. The project or effort described here has

been sponsored by the U.S. Army Research, Devel-

opment, and Engineering Command (RDECOM).

Statements and opinions expressed do not necessar-

ily reflect the position or the policy of the United

States Government, and no official endorsement

should be inferred.

References

Collin Baker, Charles Fillmore, and John Lowe. 1998.

The Berkeley FrameNet project. In Christian Boitet

and PeteWhitelock, editors, Proceedings of the Thirty-

Sixth Annual Meeting of the Association for Computa-

tional Linguistics and Seventeenth International Con-

ference on Computational Linguistics, pages 86–90,

San Francisco, California. MorganKaufmann Publish-

ers.

Roy Bar-Haim, Jonathan Berant, and Ido Dagan. 2009.

A compact forest for scalable inference over entail-

ment and paraphrase rules. In Proceedings of the 2009

Conference on Empirical Methods in Natural Lan-

guage Processing, pages 1056–1065, Singapore, Au-

gust. Association for Computational Linguistics.

K. Burton, A. Java, and I. Soboroff. 2009. The icwsm

2009 spinn3r dataset. In Proceedings of the Third An-

nual Conference on Weblogs and Social Media.

Lynn Carlson and Daniel Marcu. 2001. Discourse tag-

ging manual. Technical Report ISI-TR-545, ISI, July.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-

fine n-best parsing and maxent discriminative rerank-

ing. In Proceedings of the 43rd Annual Meeting on

Association for Computational Linguistics.

Peter Clark and Phil Harrison. 2009. Large-scale extrac-

tion and use of knowledge from text. In K-CAP ’09:

Proceedings of the fifth international conference on

Knowledge capture, pages 153–160, New York, NY,

USA. ACM.

Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, Phil

Harrison, William R. Murray, and John Thompson.

2008. Augmenting WordNet for Deep Understanding

of Text. In Johan Bos and Rodolfo Delmonte, editors,

Semantics in Text Processing. STEP 2008 Conference

Proceedings, volume 1 of Research in Computational

Semantics, pages 45–57. College Publications.

Stefan Evert, Adam Kilgarriff, and Serge Sharoff, edi-

tors. 2008. 4th Web as Corpus Workshop Can we beat

Google?

Christiane Fellbaum. 1998. WordNet: An Electronic

Lexical Database (Language, Speech, and Communi-

cation). The MIT Press, May.

Danilo Giampiccolo, Hoa Trang Dang, Bernardo

Magnini, Ido Dagan, and Bill Dolan. 2008. The

fourth fascal recognizing textual entailment challenge.

In Proceedings of the First Text Analysis Conference.

Andrew Gordon and Reid Swanson. 2008. Envision-

ing with weblogs. In International Conference on New

Media Technology.

Andrew Gordon and Reid Swanson. 2009. Identifying

personal stories in millions of weblog entries. In Third

International Conference on Weblogs and Social Me-

dia.

Jonathan Gordon, Benjamin Van Durme, and Lenhart

Schubert. 2009. Weblogs as a source for extracting

general world knowledge. In K-CAP ’09: Proceed-

ings of the fifth international conference on Knowledge

capture, pages 185–186, New York, NY, USA. ACM.

A. C. Graesser, M. Singer, and T. Trabasso. 1994. Con-

structing inferences during narrative text comprehen-

sion. Psychological Review, 101:371–395.

Iryna Gurevych and Torsten Zesch, editors. 2009. The

Peoples Web Meets NLP: Collaboratively Constructed

Semantic Resources.

Paul Kingsbury and Martha Palmer. 2003. Propbank: the

next level of treebank. In Proceedings of Treebanks

and Lexical Theories.

Douglas B. Lenat. 1995. Cyc: a large-scale investment

in knowledge infrastructure. Communications of the

ACM, 38(11):33–38.

David McClosky, Eugene Charniak, and Mark Johnson.

2006. Reranking and self-training for parser adapta-

tion. In ACL-44: Proceedings of the 21st Interna-

tional Conference on Computational Linguistics and

the 44th annual meeting of the Association for Compu-

tational Linguistics, pages 337–344, Morristown, NJ,

USA. Association for Computational Linguistics.

Isaac Persing and Vincent Ng. 2009. Semi-supervised

cause identification from aviation safety reports. In

Proceedings of the Joint Conference of the 47th An-

nual Meeting of the ACL and the 4th International

Joint Conference on Natural Language Processing of

the AFNLP, pages 843–851, Suntec, Singapore, Au-

gust. Association for Computational Linguistics.

Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-

sakaki, Geraud Campion, Aravind Joshi, and Bonnie

50



Webber. 2008. Penn discourse treebank version 2.0.

Linguistic Data Consortium, February.

Kenji Sagae. 2009. Analysis of discourse structure with

syntactic dependencies and data-driven shift-reduce

parsing. In Proceedings of the 11th International Con-

ference on Parsing Technologies (IWPT’09), pages

81–84, Paris, France, October. Association for Com-

putational Linguistics.

Lenhart Schubert and Matthew Tong. 2003. Extract-

ing and evaluating general world knowledge from the

brown corpus. In Proceedings of the HLT-NAACL

2003 workshop on Text meaning, pages 7–13, Morris-

town, NJ, USA. Association for Computational Lin-

guistics.

Reid Swanson and AndrewGordon. 2008. Say anything:

A massively collaborative open domain story writing

companion. In First International Conference on In-

teractive Digital Storytelling.

51


