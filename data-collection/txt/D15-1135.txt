



















































Automatically Solving Number Word Problems by Semantic Parsing and Reasoning


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1132‚Äì1142,
Lisbon, Portugal, 17-21 September 2015. c¬©2015 Association for Computational Linguistics.

Automatically Solving Number Word Problems 

by Semantic Parsing and Reasoning 

 

Shuming Shi1, Yuehui Wang2*, Chin-Yew Lin1, Xiaojiang Liu1 and Yong Rui1 
1 Microsoft Research 

{shumings, cyl, xiaojl, yongrui}@microsoft.com 

2 University of Science and Technology of China 

wyh9346@mail.ustc.edu.cn 

 

  

 

Abstract 

This paper presents a semantic parsing 

and reasoning approach to automatically 

solving math word problems. A new 

meaning representation language is de-

signed to bridge natural language text and 

math expressions. A CFG parser is imple-

mented based on 9,600 semi-automati-

cally created grammar rules. We conduct 

experiments on a test set of over 1,500 

number word problems (i.e., verbally ex-

pressed number problems) and yield 

95.4% precision and 60.2% recall. 

1 Introduction 

Computers, since their creation, have exceeded 

human beings in (speed and accuracy of) mathe-

matical calculation. However, it is still a big chal-

lenge nowadays to design algorithms to automat-

ically solve even primary-school-level math word 

problems (i.e., math problems described in natural 

language). 

Efforts to automatically solve math word prob-

lems date back to the 1960s (Bobrow, 1964a, b). 

Previous work on this topic falls into two catego-

ries: symbolic approaches and statistical learning 

methods. In symbolic approaches (Bobrow, 

1964a, b; Charniak, 1968; Bakman, 2007; Liguda 

& Pfeiffer, 2012), math problem sentences are 

transformed to certain structures by pattern 

matching or verb categorization. Equations are 

then derived from the structures. Statistical learn-

ing methods are employed in two recent papers 

(Kushman et al., 2014; Hosseini et al., 2014). 

 

Most (if not all) previous symbolic approaches 

suffer from two major shortcomings. First, natural 

language (NL) sentences are processed by simply 

applying pattern matching and/or transformation 

rules in an ad-hoc manner (refer to the related 

work section for more details). Second, surpris-

ingly, they seldom report evaluation results about 

the effectiveness of the methods (except for some 

examples for demonstration purposes). For the 

small percentage of work with evaluation results 

available, it is unclear whether the patterns and 

rules are specially designed for specific sentences 

in a test set. 

 

Figure 1: Number word problem examples 

In this paper, we present a computer system 

called SigmaDolphin which automatically solves 

math word problems by semantic parsing and rea-

soning. We design a meaning representation lan-

guage called DOL (abbreviation of dolphin lan-

guage) as the structured semantic representation 

of NL text. A semantic parser is implemented to 

transform math problem text into DOL trees. A 

reasoning module is included to derive math ex-

pressions from DOL trees and to calculate final 

answers. Our approach falls into the symbolic cat-

egory, but makes improvements over previous 

symbolic methods in the following ways, 

______________________________________ 

* Work done while this author was an intern at Microsoft 
Research 

1). One number is 16 more than another. If the 
smaller number is subtracted from 2/3 of the larger, 
the result is 1/4 of the sum of the two numbers. Find 
the numbers. 

2). Nine plus the sum of an even integer and its 
square is 3 raised to the power of 4. What is the num-
ber? 

3). The tens digit of a two-digit number is 3 more 
than the units digit. If the number is 8 more than 6 
times the sum of the digits, find the number. 

4). If the first and third of three consecutive even in-
tegers are added, the result is 12 less than three times 
the second integer. Find the integers. 

1132



1) We introduce a systematic way of parsing 

NL text, based on context-free grammar (CFG). 

2) Evaluation is enhanced in terms of both data 

set construction and evaluation mechanisms. We 

split the problem set into a development set 

(called dev set) and a test set. Only the dev set is 

accessible during our algorithm design (especially 

in designing CFG rules and in implementing the 

parsing algorithm), which avoids over-tuning to-

wards the test set. Three metrics (precision, recall, 

and F1) are employed to measure system perfor-

mance from multiple perspectives, in contrast to 

all previous work (including the statistical ones) 

which only measures accuracy. 

We target, in experiments, a subtype of word 

problems: number word problems (i.e., verbally 

expressed number problems, as shown in Figure 

1). We hope to extend our techniques to handle 

general math word problems in the future. 

We build a test set of over 1,500 problems and 

make a quantitative comparison with state-of-the-

art statistical methods. Evaluation results show 

that our approach significantly outperforms base-

line methods on our test set. Our system yields an 

extremely high precision of 95.4% and a reasona-

ble recall of 60.2%, which shows promising appli-

cation of our system in precision-critical situa-

tions. 

2 Related Work 

2.1 Math word problem solving 

Most previous work on automatic word problem 

solving is symbolic. STUDENT (Bobrow, 1964a, 

b) handles algebraic problems by first transform-

ing NL sentences into kernel sentences using a 

small set of transformation patterns. The kernel 

sentences are then transformed to math expres-

sions by recursive use of pattern matching. 

CARPS (Charniak, 1968, 1969) uses a similar ap-

proach to solve English rate problems. The major 

difference is the introduction of a tree structure as 

the internal representation of the information 

gathered for one object. Liguda & Pfeiffer (2012) 

propose modeling math word problems with aug-

mented semantic networks. Addition/subtraction 

problems are studied most in early research (Bri-

ars & Larkin, 1984; Fletcher, 1985; Dellarosa, 

1986; Bakman, 2007; Ma et al., 2010). Please re-

fer to Mukherjee & Garain (2008) for a review of 

symbolic approaches before 2008. 

                                                 
1 http://www.wolframalpha.com  

No empirical evaluation results are reported in 

most of the above work. Almost all of these ap-

proaches parse NL text by simply applying pattern 

matching rules in an ad-hoc manner. For example, 

as mentioned in Bobrow (1964b), due to the pat-

tern ‚Äú($, AND $)‚Äù, the system would incorrectly 

divide ‚ÄúTom has 2 apples, 3 bananas, and 4 

pears.‚Äù into two ‚Äúsentences‚Äù: ‚ÄúTom has 2 apples, 

3 bananas.‚Äù and ‚Äú4 pears.‚Äù 

WolframAlpha1 shows some examples2 of au-

tomatically solving elementary math word prob-

lems, with technique details unknown to the gen-

eral public. Other examples on the web site 

demonstrate a large coverage of short phrase que-

ries on math and other domains. By randomly se-

lecting problems from our dataset and manually 

testing on their web site, we find that it fails to 

handle most problems in our problem collection. 

Statistical learning methods have been pro-

posed recently in two papers: Hosseini et al. 

(2014) solve single step or multi-step homoge-

nous addition and subtraction problems by learn-

ing verb categories from the training data. Kush-

man et al. (2014) can solve a wide range of word 

problems, given that the equation systems and so-

lutions are attached to problems in the training set. 

The method of the latter paper (referred to as 

KAZB henceforth) is used as one of our baselines. 

2.2 Semantic parsing 

There has been much work on analyzing the se-

mantic structure of NL strings. In semantic role 

labeling and frame-semantic parsing (Gildea & 

Jurafsky, 2002; Carreras & Marquez, 2004; 

Marquez et al., 2008; Baker et al., 2007; Das et 

al., 2014), predicate-argument structures are dis-

covered from text as their shallow semantic repre-

sentation. In math problem solving, we need a 

deeper and richer semantic representation from 

which to facilitate the deriving of math expres-

sions. 

Another type of semantic parsing work (Zelle 

& Mooney, 1996; Zettlemoyer & Collins, 2005; 

Zettlemoyer & Collins, 2007; Wong & Mooney, 

2007; Cai & Yates, 2013; Berant et al., 2013; 

Kwiatkowski et al., 2013; Berant & Liang, 2014) 

maps NL text into logical forms by supervised or 

semi-supervised learning. Some of them are based 

on or related to combinatory categorial grammar 

(CCG) (Steedman, 2000). Abstract Meaning Rep-

resentation (AMR) (Banarescu et al., 2013) keeps 

richer semantic information than CCG and logical 

2 https://www.wolframalpha.com/examples/Elementary-

Math.html (bottom-right part) 

1133



forms. In Section 3.1.4, we discuss the differences 

between DOL, AMR, and CCG, and explain why 

we choose DOL as the meaning representation 

language for math problem solving. 

3 Approach 

Consider the first problem in Figure 1 (written be-

low for convenience), 
One number is 16 more than another. If the smaller 

number is subtracted from 2/3 of the larger, the result 

is 1/4 of the sum of the two numbers. Find the numbers. 

To automatically solve this problem, the com-

puter system needs to figure out, somehow, that 1) 

two numbers x, y are demanded, and 2) they sat-

isfy the equations below, 

 x = 16 + y 

(2/3)x ‚Äì y = (x + y) / 4 

(1) 

(2) 

To achieve this, reasoning must be performed 

based on common sense knowledge and the infor-

mation provided by the source problem. Given the 

difficulty of performing reasoning directly on un-

structured and ambiguous natural language text, it 

is reasonable to transform the source text into a 

structured, less ambiguous representation. 

Our approach contains three modules: 

1) A meaning representation language called 
DOL newly designed by us as the semantic 

representation of natural language text. 

2) A semantic parser which transforms natu-
ral language sentences of a math problem 

into DOL representation. 

3) A reasoning module to derive math expres-
sions from DOL representation. 

 

Figure 2: DOL example 

3.1 DOL: Meaning representation language 

Every meaningful piece of NL text is represented 

in DOL as a semantic tree of various node types. 

Figure 2 shows the DOL representation of the sec-

ond problem of Figure 1. It contains two semantic 

trees, corresponding to the two sentences. 

3.1.1 Node types 

Node types of a DOL tree include constants, clas-

ses, and functions. Each interim node of a tree is 

always a function; and each leaf node can be a 

constant, a class, or a zero-argument function. 

Constants in DOL refer to specific objects in 

the world. A constant can be a number (e.g., 3.57), 

a lexical string (like ‚ÄúNew York‚Äù), or an entity. 

Classes: An entity class refers to a category of 

entities sharing common semantic properties. For 

example, all cities are represented by the class lo-

cation.city; and math.number is a class for all 

numbers. It is clear that, 

3.14159 ‚àà math.number 
city.new_york ‚àà location.city 

A class C1 is a sub-class (denoted by ‚äÜ) of an-
other class C2 if and only if every instance of C1 

are in C2. The following holds according to com-

mon sense knowledge, 

math.number ‚äÜ math.expression 
person.pianist ‚äÜ person.performer 

Template classes are classes with one or more 

parameters, just like template classes in C++. The 

most important template class in DOL is 

t.list<c,m,n> 

where c is a class; m and n are integers. Each in-

stance of this class is a list containing at least m 

and at most n elements of type c. For example, 

each instance of t.list<math.number,2,+‚àû> is a 
list containing at least 2 numbers. 

Functions are used in DOL as the major way 

to form larger language units from smaller ones. 

A function is comprised of a name, a list of core 

arguments, and a return type. DOL enables func-

tion overloading (again borrowing ideas from pro-

gramming languages). That is, one function name 

can have multiple core-argument specifications. 

Below are two specifications for fn.math.sum 

(which appears in the example of Figure 2). 

nf.math.sum!1: 

$1: math.expression;  $2: math.expression 

return type: math.expression 

return value: The sum of its arguments 

nf.math.sum!2: 

$1: t.list<math.expression,2,+‚àû> 
return type: math.expression 

return value: The sum of the elements in $1 

English: Nine plus the sum of an even integer and 

its square is 3 raised to the power of 4. What is the 

number? 

DOL trees: 

vf.be.equ 

nf.math.sum 

nf.list-v1 

math.integer 

nf.math.power 

4 3 9 

1 mf.number.even 

nf.math.2nd_power 

nf.it-v1 

nf.math.sum 

vf.be.equ 

nf.what nf.list-v1 

math.number 1 

1134



Here ‚Äú$1: math.expression‚Äù means the first ar-

gument has type math.expression. 

DOL supports three kinds of functions: noun 

functions, verb functions, and modifier functions. 

Noun functions map entities to their properties 

or to other entities having specific relations with 

the argument(s). For example, nf.math.sum maps 

math expressions to their sum. Noun functions are 

used to represent noun phrases in natural language 

text. More noun functions are shown in Table 1. 

Among all noun functions, nf.list has a special 

important position due to its high frequency in 

DOL trees. The function is specified below, 

nf.list 

$1: class;  $2: math.number 

return type: t.list<$1> 

return value: An entity list with cardinality $2 

and element type $1 

For example nf.list(math.number,5) returns a 

list containing 5 elements of type math.number. It 

is the semantic representation of ‚Äúfive numbers‚Äù. 

Pronoun functions are special zero-argument 

noun functions. Examples are nf.it (representing 

an already-mentioned entity or event) and nf.what 

(denoting an unknown entity or entity list). 

Verb functions act as sentences or sub-sen-

tences in DOL. As an example, vf.be.equ (in Fig-

ure 2) is a verb function that has two arguments of 

the quantity type. 

vf.be.equ 

$1: quantity.generic;  $2: quantity.generic 

return type: t.vf 

Meaning: Two quantities $1 and $2 have the 

same value 

In addition to core arguments ($1, $2, etc.), 

many functions can take additional extended ar-

guments as their modifiers. Our last function type 

called modifier functions often take the role of ex-

tended arguments, to modify noun functions, verb 

functions, or other modifier functions. Modifier 

functions are used in DOL as the semantic repre-

sentation of adjectives, adverb phrases (including 

conjunctive adverb phrases), and prepositional 

phrases in natural languages. In the example of 

Figure 2, the function mf.number.even modifies 

the noun function nf.list as its extended argument. 

3.1.2 Entity variables 

Variables are assigned to DOL sub-trees for indi-

cating the co-reference of sub-trees to entities and 

for facilitating the construction of logical forms 

and math expressions from DOL. In Figure 2, the 

same variable v1 (meaning a variable with ID 1) 

is assigned to two sub-trees in the first sentence 

and one sub-tree in the second sentence. Thus the 

three sub-trees refer to the same entity. 

Function Remarks 

nf.math.numerator 
  $1: math.fraction 
  ret: math.number 

Get the numerator of fraction 

$1 

nf.math.gcd 
  $1: t.list<math.integer,2,+‚àû> 
  ret: math.integer 

Get the greatest common di-

visor of the elements of $1 

nf.e.height 
  $1: e.concrete 
  ret: quantity.length 

Get the height of $1 which is 

a concrete entity 

vf.believe 
  $1: e.agent; $2: t.vf.std 
  ret: t.vf 

Agent $1 believes that $2 is 

true as a predicate 

mf.number.even 
  ret: t.mf.adj 

Indicating the property of be-

ing an even number 

Table 1: Example DOL functions 

3.1.3 Key features of DOL 

DOL has some nice characteristics that are critical 

to building a high-precision math problem solving 

system. That is why we invent DOL as our mean-

ing representation language instead of employing 

an existing one. 

First, DOL is a strongly typed language. Every 

function has clearly defined argument types and a 

return type. A valid DOL tree must satisfy the 

type-compatibility property: 

Type-compatibility: The type of each child of a 

function node should match the corresponding ar-

gument type of the function. 

For example, in Figure 2, the return type of 

nf.math.power is math.expression, which matches 

the second argument of vf.be.equ. However, the 

following two trees (yielded from the correspond-

ing pieces of text) are invalid because they do not 

satisfy type-compatibility. 

sum of 100  [unreasonable text] 

nf.math.sum!2(100)  [invalid DOL tree] 

sum of 3 and Jordan  [unreasonable text] 

nf.math.sum!2({3, ‚ÄúJordan‚Äù})  [invalid tree] 

Second, we maintain in DOL an open-domain 

type system. The type system contains over 1000 

manually verified classes and more automatically 

generated ones (refer to Section 3.2.1 for more de-

tails). Such a comprehensive type system makes it 

possible to define various kinds of functions and 

to perform type-compatibility checking. In con-

trast, most previous semantic languages have at 

most 100+ types at the grammar level. In addition, 

by introducing template classes, we avoid main-

taining a lot of potentially duplicate types and re-

duce the type system management efforts. To the 

best of our knowledge, template classes are not 

1135



available in other semantic representation lan-

guages. 

Third, DOL has built-in data structures like 

t.list and nf.list which greatly facilitate both func-

tion declaration and text representation (espe-

cially math text representation). For example, the 

two variants of nf.math.sum (refer to Section 3.1.1 

for their specifications) are enough to represent 

the following English phrases: 

3 plus 5  

ÔÉ† nf.math.sum!1(3, 5) 

sum of 3, 5, 7, and 9 

ÔÉ† nf.math.sum!2(nf.list(3, 5, 7, 9)) 

sum of ten thousand numbers 

ÔÉ† nf.math.sum!2(nf.list(math.number,10000)) 

Without t.list or nf.list, we would have to define 

a lot of overloaded functions for nf.math.sum to 

deal with different numbers of addends. 

3.1.4 Comparing with other languages 

Among all meaning representation languages, 

AMR (Banarescu et al., 2013) is most similar to 

DOL. Their major differences are: First, they use 

very different mechanisms to represent noun 

phrases. In AMR, a sentence (e.g., ‚Äúthe boy de-

stroyed the room‚Äù) and a noun phrase (e.g., ‚Äúthe 

boy‚Äôs destruction of the room‚Äù) can have the same 

representation. While in DOL, a sentence is al-

ways represented by a verb function; and a noun 

phrase is always a noun function or a constant. 

Second, DOL has a larger type system and is 

stricter in type compatibility checking. Third, 

DOL has template classes and built-in data struc-

tures like t.list and nf.list to facilitate the represen-

tation of math concepts. 

CCG (Steedman, 2000) provides a transparent 

interface between syntax and semantics. In CCG, 

semantic information is defined on words (e.g., 

‚ÄúŒªx.odd(x)‚Äù for ‚Äúodd‚Äù and ‚ÄúŒªx.number(x)‚Äù for 

‚Äúnumber‚Äù). In contrast, DOL explicitly connects 

NL text patterns to semantic elements. For exam-

ple, as shown in Table 2 (Section 3.2.1), one CFG 

grammar rule connects pattern ‚Äú{$1} raised to the 

power of {$2}‚Äù to function nf.math.power. 

Logical forms are another way of meaning rep-

resentation. We choose not to transform NL text 

directly to logical forms for two reasons: On one 

hand, state-of-the-art methods for mapping NL 

text into logical forms typically target short, one-

sentence queries in restricted domains. However, 

many math word problems are long and contain 

multiple sentences. On the other hand, variable-id 

assignment is a big issue in direct logical form 

construction for many math problems. Let‚Äôs use 

the following problem (i.e., the first problem of 

Figure 1) to illustrate, 

One number is 16 more than another. If the smaller 

number is subtracted from 2/3 of the larger, the result 

is 1/4 of the sum of the two numbers. Find the numbers. 

For this problem, it is difficult to determine 

whether ‚Äúthe smaller number‚Äù refers to ‚Äúone num-

ber‚Äù or ‚Äúanother‚Äù in directly constructing logical 

forms. It is therefore a challenge to construct a 

correct logical form for such kinds of problems. 

Our solution to the above challenge is assigning 

a new variable ID (which is different from the IDs 

of ‚Äúone number‚Äù and ‚Äúanother‚Äù) and to delay the 

final variable-ID assignment to the reasoning 

stage. To enable this mechanism, the meaning 

representation language should support a lazy var-

iable ID assignment and keep as much infor-

mation (e.g., determiners, plurals, modifiers) from 

the noun phrases as possible. DOL is a language 

that always keeps the structure information of 

phrases, whether or not it has been assigned a var-

iable ID. 

In summary, compared with other languages, 

DOL has some unique features which make it 

more suitable for our math problem solving sce-

nario. 

3.2 Semantic Parsing 

Our parsing algorithm is based on context-free 

grammar (CFG) (Chomsky, 1956; Backus, 1959; 

Jurafsky & Martin, 2000), a commonly used 

mathematical system for modeling constituent 

structure in natural languages. 

3.2.1 CFG for connecting DOL and NL 

The core part of a CFG is the set of grammar 

rules. Example English grammar rules for build-

ing syntactic parsers include ‚ÄúS ‚Üí NP VP‚Äù, ‚ÄúNP ‚Üí 

CD | DT NN | NP PP‚Äù, etc. Table 2 shows some 

example CFG rules in our system for mapping 

DOL nodes to natural language word sequences. 

The left side of each rule is a DOL element (a 

function, class, or constant); and the right side is a 

sequence of words and arguments. The grammar 

rules are consumed by our parser for building 

DOL trees from NL text. 

So far there are 9,600 grammar rules in our sys-

tem. For every DOL node type, the lexicon and 

grammar rules are constructed together in a semi-

automatic way. Math-related classes, functions, 

and constants and their grammar rules are manu-

ally built by referring to text books and online tu-

1136



torials. About 35 classes and 200 functions are ob-

tained in this way. Additional instances of each 

element type are constructed in the ways below. 

Classes: Additional classes and grammar rules 

are obtained from two data sources: Freebase 3 

types, and automatically extracted lexical seman-

tic data. By treating Freebase types as DOL clas-

ses and the mapping from types to lexical names 

as grammar rules, we get the first version of gram-

mar for classes. To improve coverage, we run a 

term peer similarity and hypernym extraction al-

gorithm (Hearst, 1992; Shi et al., 2010; Zhang et 

al., 2011) on a web snapshot of 3 billion pages, 

and get a peer-similarity graph and a collection of 

is-a pairs. An is-a pair example is (Megan Fox, 

actress), where ‚ÄúMegan Fox‚Äù and ‚Äúactress‚Äù are in-

stance and type names respectively. In our peer 

similarity graph, ‚ÄúMegan Fox‚Äù and ‚ÄúBritney 

Spears‚Äù have a high similarity score. The peer 

similarity graph is used to clean the is-a data col-

lection (with the idea that peer terms often share 

some common type names). Given the cleaned is-

a data, we sort the type names by weight and man-

ually create classes for top-1000 type names. For 

example, create a class person.actress and add a 

grammar rule ‚Äúperson.actress ‚Üí actress‚Äù. For the 
other 2000 type names in the top 3000, we create 

classes and rules automatically, in the form of 

‚Äúclass.TN ‚Üí TN‚Äù, where TN is a type name. For 
example, create rule ‚Äúclass.succulent ‚Üí  succu-
lent‚Äù for name ‚Äúsucculent‚Äù. 

vf.be.equ($1,$2) ‚Üí {$1} be equal to {$2} 
                             |  {$1} equal {$2} 
                             |  {$1} be {$2} 
vf.give($1,$2,$3) ‚Üí {$1} give {$2} to {$3} 
                              |  {$1} give {$3} {$2} 
nf.math.sum!1($1,$2) ‚Üí {$1} plus {$2} 
                                      | {$2} added to {$1} 
nf.math.sum!2($1) ‚Üí sum of {$1} 
                                 | addition of {$1} 
nf.math.power($1,$2) 

‚Üí {$1} raised to the {power|exponent} of {$2} 
nf.list($1,$2) ‚Üí {$2} {$1} 
mf.number.even ‚Üí even 
mf.condition.if($1) ‚Üí if {$1} 
mf.approximately ‚Üí approximately 
                               | roughly 
education.university ‚Üí university 
math.number ‚Üí number 
math.integer ‚Üí integer 

Table 2: Example grammar for connecting DOL 

and NL 

                                                 
3 Freebase: http://www.freebase.com/ 

Functions: Additional noun functions are auto-

matically created from Freebase properties and at-

tribute extraction results (Pasca et al., 2006; 

Durme et al., 2008), using a similar procedure 

with creating classes from Freebase types and is-

a extraction results. We have over 50 manually 

defined math-related verb functions. Our future 

plan is automatically generating verb functions 

from databases like PropBank (Kingsbury & 

Palmer, 2002), FrameNet (Fillmore et al., 2003), 

and VerbNet4 (Schuler, 2005). Additional modi-

fier functions are automatically created from an 

English adjective and adverb list, in the form of 

‚Äúmf.adj.TN ‚Üí  TN‚Äù and ‚Äúmf.adv.TN ‚Üí  TN‚Äù 
where TN is the name of an adjective or adverb. 

 

Figure 3: The DOL semantic parse tree for ‚ÄúNine 

plus an integer is equal to 314‚Äù 

 

Figure 4: A syntactic parse tree 

3.2.2 Parsing 

Parsing for CFG is a well-studied topic with lots 

of algorithms invented (Kasami, 1965; Earley, 

1970). The core idea behind almost all the algo-

rithms is exploiting dynamic programming to 

achieve efficient search through the space of pos-

sible parse trees. For syntactic parsing, a well-

known serious problem is ambiguity: the appear-

ance of many syntactically correct but semanti-

cally unreasonable parse trees. Modern syntactic 

parsers reply on statistical information to reduce 

4 VerbNet: http://verbs.colorado.edu/~mpalmer/pro-

jects/verbnet.html  

vf.be.equ 

314 nf.math.sum 

9 list 

1 math.integer 

integer an is equal to 314 plus Nine 

S 

VP NP 

Nine 

CD 

NP PP 

NP 

plus 

IN 

an 

DT 

integer 

NN 

ADJP 

is 

VB 

equal 

JJ PP 

to 

IN 

314 

CD 

NP 

1137



ambiguity. They are often based on probabilistic 

CFGs (PCFGs) or probabilistic lexicalized CFGs 

trained on hand-labeled TreeBanks. 

With the new set of DOL-NL grammar rules 

(examples in Table 2) and the type-compatibility 

property (Section 3.1.3), ambiguity can hopefully 

be greatly reduced, because semantically unrea-

sonable parsing often results in invalid DOL trees. 

We implement a top-down parser for our new 

CFG of Section 3.2.1, following the Earley algo-

rithm (Earley, 1970). No probabilistic information 

is attached in the grammar rules because no Tree-

banks are available for learning statistical proba-

bilities for the new CFG. Figure 3 shows the parse 

tree returned by our parser when processing a sim-

ple sentence. The DOL tree can be obtained by re-

moving the dotted lines (corresponding to the 

non-argument part in the right side of the grammar 

rules). A traditional syntactic parse tree is shown 

in Figure 4 for reference. 

During parsing, a score is calculated for each 

DOL node. The score of a tree T is the weighted 

average of the scores of its sub-trees, 

 
ùë∫(ùëª) =

‚àë ùë≥(ùëªùíä) ‚àô ùë∫(ùëªùíä)
ùíå
ùíä=ùüè

‚àë ùë≥(ùëªùíä)
ùíå
ùíä=ùüè

‚àô ùíë(ùëª) (3) 

where ùëáùëñ is a sub-tree, and ùêø(ùëáùëñ) is the number of 
words to which the sub-tree corresponds in the 

original text. If the type-compatibility property for 

T is satisfied, ùëù(ùëá)=1; otherwise ùëù(ùëá)=0. 
All leaf nodes are assigned a score of 1.0, ex-

cept for pure lexical string nodes (which are used 

as named entity names). The score of a lexical 

string node is set to 1/(1+ùúán), where n is the num-
ber of words in the node, and ùúá (=0.2 in experi-
ments) is a parameter whose value does not have 

much impact on parsing results. Such a score 

function encourages interpreting a word sequence 

with our grammar than treating it as an entity 

name. 

Among all candidate DOL trees yielded during 

parsing, we return the one with the highest score 

as the final parsing result. A null tree is returned 

if the highest score is zero. 

3.3 Reasoning 

The reasoning module is responsible for deriving 

math expressions from DOL trees and calculating 

problem answers by solving equation systems. 

Math expressions have different definitions in dif-

ferent contexts. In some definitions, equations and 

inequations are excluded from math expressions. 

In this paper, equations and inequations (like 

‚Äúa=b‚Äù and ‚Äúax+b>0‚Äù) are called s-expressions be-

cause they represent mathematical sentences, 

while other math expressions (like ‚Äúx+5‚Äù) are 

named n-expressions since they are essentially 

noun phrases. Our definition of ‚Äúmath expres-

sions‚Äù therefore includes both n-expressions and 

s-expressions. 

Different types of nodes may generate different 

types of math expressions. In most cases, s-ex-

pressions are derived from verb function nodes 

and modifier function nodes, while n-expressions 

are generated from constants and noun function 

nodes. For example, the s-expression ‚Äú9+x=314‚Äù 

can be derived from the DOL tree of Figure 3, if 

variable x represents the integer. In the same Fig-

ure, The n-expression ‚Äú9+x‚Äù is derived from the 

left sub-tree. 

The pseudo-codes of our math expression deri-

vation algorithm are shown in Figure 5. The algo-

rithm generates the math expression for a DOL 

tree T by first calling the expression derivation 

procedure of sub-trees, and then applying the se-

mantic interpretation of T. All the s-expressions 

derived so far are stored in an expression list 

named XL. 

Algorithm MathExpDerivation 

    Input: DOL tree T 

Output: Math expression X(T) 

Global data structure: Expression list XL 

1: For each child Ci of T 

2:     X(Ci) = MathExpDerivation(Ci) 

3:     If X(Ci) is an s-expression 

4:         Add X(Ci) to XL 

5: X(T) ‚Üê Applying the semantic interpretation 
of T 

6: Return X(T) 

Figure 5: Math expression derivation algorithm 

vf.be.equ($1,$2) ‚Üí X($1) = X($2) (1) 
nf.math.sum!1($1,$2) ‚Üí X($1) + X($2) (2) 
nf.math.sum!2($1) ‚Üí ‚àë ùêó(ùêû)ùêû‚àà$ùüè  (3) 
nf.math.gcd($1) ‚Üí gcd({X(e) | ùêû ‚àà $ùüè}) (4) 
nf.list($1,$2) ‚Üí V = (v1, v2‚Ä¶, vn), n=X($2) (5) 
mf.number.even ‚Üí X($‚Üë) % 2 = 0 (6) 

Table 3: Example semantic interpretations 

The semantic interpretation of DOL nodes 

plays a critical role in the algorithm. Table 3 

shows some example interpretations of some rep-

resentative DOL functions. In the table, $1, $2 etc. 

are function arguments, and $‚Üë  for a modifier 
node denotes the node which the modifier modi-

fies. So far the semantic interpretations are built 

manually. Please note that it is not necessary to 

make semantic interpretations for every DOL 

1138



node in solving number word problems. For ex-

ample, most class nodes and many adverb nodes 

can have null interpretations at the moment. 

4 Experiments 

4.1 Experimental setup 

Datasets: Our problem collection5 contains 1,878 

math number word problems, collected from two 

web sites: algebra.com6 (a web site for users to 

post math problems and get help from tutors) and 

answers.yahoo.com7. Problems on both sites are 

organized into categories. For algebra.com, prob-

lems are randomly sampled from the number 

word problems category; for answers.yahoo.com, 

we first randomly sample an initial set of prob-

lems from the math category and then ask human 

annotators to manually choose number word 

problems from them. Math equations 8  and an-

swers to the problems are manually added by hu-

man annotators. 

We randomly split the dataset into a dev set (for 

algorithm design and debugging) and a test set. 

More subsets are extracted to meet the require-

ments of the baseline methods (see below). Table 

4 shows the statistics of the datasets. 

Baseline methods: We compare our approach 

with two baselines: KAZB (Kushman et al., 2014) 

and BasicSim. 

KAZB is a learning-based statistical method 

which solves a problem by mapping it to one of 

the equation templates determined by the anno-

tated equations in the training data. We run the 

ALLEQ version of their algorithm since it per-

forms much better than the other two (i.e., 5EQ 

and 5EQ+ANS). Their codes support only linear 

equations and require that there are at least two 

problems for each equation template (otherwise 

an exception will be thrown). By choosing prob-

lems from the collection that meet these require-

ments, we build a sub-dataset called LinearT2. In 

the dataset of KAZB, each equation template cor-

responds to at least 6 problems. So we form an-

other sub-dataset called LinearT6 by removing 

from the test set the problems for which the asso-

ciated equation template appears less than 6 times. 

BasicSim is a simple statistical method which 

works by computing the similarities between a 

testing problem and those in the training set, and 

then applying the equations of the most similar 

problem. This method has similar performance 

                                                 
5 Available from http://research.microsoft.com/en-us/pro-

jects/dolphin/  
6 http://www.algebra.com  

with KAZB on their dataset, but does not have the 

two limitations mentioned above. Therefore we 

adopt it as the second baseline. 

For both baselines, experiments are conducted 

using 5-fold cross-validation with the dev set al-

ways included in the training data. In other words, 

we always use the dev set and 4/5 of the test set as 

training data for each fold. 

Evaluation metrics: Evaluation is performed in 

the setting that a system can choose NOT to an-

swer all problems in the test set. In other words, 

one has the flexibility of generating answers only 

when she knows how to solve it or she is confident 

about her answer. In this setting, the following 

three metrics are adopted in reporting evaluation 

results (assuming, in a test set of size n, a system 

generates answers for m problems, where k of 

them are correct): 

Precision: k/m 

Recall (or coverage): k/n 

F1: 2PR/(P+R) = 2k/(m+n) 

Dataset #problems #sentences 
(average) 

#words 
(average) 

All 
dev 374 1.79 20.3 
test 1,504 1.75 22.5 

Linear 
dev 247 1.78 19.6 
test 986 1.72 19.0 

LinearT2 
dev 172 1.85 18.8 
test 669 1.71 17.4 

LinearT6 
dev 71 1.96 16.8 
test 348 1.80 16.1 

Table 4: Dataset statistics (Linear: problems with 

linear equations; T2: problems corresponding to 

template size ‚â• 2) 

4.2 Experimental results 

The Overall evaluation results are summarized in 

Table 5, where ‚ÄúDolphin‚Äù represents our ap-

proach. The results show that our approach signif-

icantly outperforms (with p<<0.01 according to 

two-tailed t-test) the two baselines on every test 

set, in terms of precision, recall, and F-measure. 

Our approach achieves a particularly high preci-

sion of 95%. That means once an answer is pro-

vided by our approach, it has a very high proba-

bility of being correct. 

Please note that our grammar rules and parsing 

algorithm are NOT tuned for the evaluation data. 

Only the dev set is referred to in system building. 

7 https://answers.yahoo.com/  
8 Math equations are used in the baseline approaches as part 

of training data. 

1139



Since the baselines generate results for all prob-

lems, the precision, recall, and F1 are all the same 

for each dataset. 

Dataset Method 
Precision 

(%) 
Recall 

(%) 
F1 
(%) 

LinearT6 
KAZB 49.1 49.1 49.1 
BasicSim 59.7 59.7 59.7 
Dolphin 98.1 72.9 83.6 

LinearT2 
KAZB 37.5 37.5 37.5 
BasicSim 46.3 46.3 46.3 
Dolphin 97.3 68.0 80.0 

Linear 
BasicSim 32.3 32.3 32.3 
Dolphin 95.7 63.6 76.4 

Test set 
all 

BasicSim 29.0 29.0 29.0 
Dolphin 95.4 60.2 73.8 

Table 5: Evaluation results 

The reason for such a high precision is that, by 

transforming NL text to DOL trees, the system 

‚Äúunderstands‚Äù the problem (or has structured and 

accurate information about quantity relations). 

Therefore it is more likely to generate correct re-

sults than statistical methods who simply ‚Äúguess‚Äù 

according to features. By examining the problems 

in the dev set that we cannot generate answers, we 

find that most of them are due to empty parsing 

results. 

On the other hand, statistical approaches have 

the advantage of generating answers without un-

derstanding the semantic meaning of problems (as 

long as there are similar problems in the training 

data). So they are able to handle (with probably 

low precision) problems that are complex in terms 

of language and logic. 

Please pay attention that our experimental re-

sults reported here are on number word problems. 

General math word problems are much harder to 

our approach because the entity types, properties, 

relations, and actions contained in general word 

problems are much larger in quantity and more 

complex in quality. We are working on extending 

our approach to general math word problems. 

Now our DOL language and CFG grammar al-

ready have a good coverage on common entity 

types, but the coverage on properties, relations, 

and actions is quite limited. As a result, our parser 

fails to parse many sentences in general math 

word problems because they contain properties, 

relations or actions that are unknown to our sys-

tem. We also observe that sometimes we are able 

to parse a problem successfully, but cannot derive 

math expressions in the reasoning stage. This is 

often because some relations or actions in the 

problem are not modeled appropriately. As future 

work, we plan to extend our DOL lexicon and 

grammar to improve the coverage of properties, 

relations, and actions. We also plan to study the 

mechanism of modeling relations and actions. 

5 Conclusion 

We proposed a semantic parsing and reasoning 

approach to automatically solve math number 

word problems. We have designed a new meaning 

representation language DOL to bridge NL text 

and math expressions. A CFG parser is imple-

mented to parse NL text to DOL trees. A reason-

ing module is implemented to derive math expres-

sions from DOL trees, by applying the semantic 

interpretation of DOL nodes. We achieve a high 

precision and a reasonable recall on our test set of 

over 1,500 problems. We hope to extend our tech-

niques to handling general math word problems 

and to other domains (like physics and chemistry) 

in the future. 

Acknowledgments 

We would like to thank the annotators for their ef-

forts in assigning math equations and answers to 

the problems in our dataset. Thanks to the anony-

mous reviewers for their helpful comments and 

suggestions. 

Reference 

J.W. Backus. 1959. The syntax and semantics of the 

proposed international algebraic language of the 

Zurich ACM-GAMM conference. Proceedings of 

the International Conference on Information Pro-

cessing, 1959. 

Y. Bakman. 2007. Robust understanding of word prob-

lems with extraneous information. http://arxiv.org/ 

abs/math/0701393. Accessed Feb. 2nd, 2015. 

C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-

2007 Task 19: Frame semantic structure extraction. 

In Proceedings of SemEval. 

L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. 

Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. 

Palmer, and N. Schneider. 2013. Abstract meaning 

representation for sembanking. In Proc. of the Lin-

guistic Annotation Workshop and Interoperability 

with Discourse. 

J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Se-

mantic parsing on Freebase from question-answer 

pairs. In Empirical Methods in Natural Language 

Processing (EMNLP). 

J. Berant and P. Liang. 2014. Semantic Parsing via Par-

aphrasing. In ACL'2014. 

1140



D.G. Bobrow. 1964a. Natural language input for a 

computer problem solving system. Report MAC-

TR-1, Project MAC, MIT, Cambridge, June 

D.G. Bobrow. 1964b. Natural language input for a 

computer problem solving system. Ph.D. Thesis, 

Department of Mathematics, MIT, Cambridge 

D.L. Briars, J.H. Larkin. 1984. An integrated model of 

skill in solving elementary word problems. Cogni-

tion and Instruction, 1984, 1 (3) 245-296. 

Q. Cai and A. Yates. 2013. Large-scale semantic pars-

ing via schema matching and lexicon extension. In 

Association for Computational Linguistics (ACL). 

X. Carreras. and L. Marquez. 2004. Introduction to the 

CoNLL-2004 shared task: Semantic role labeling. In 

Proceedings of CoNLL. 

E. Charniak. 1968. CARPS: a program which solves 

calculus word problems. Report MAC-TR-51, Pro-

ject MAC, MIT, Cambridge, July 

E. Charniak. 1969. Computer solution of calculus word 

problems. In Proceedings of international joint con-

ference on artificial intelligence. Washington, DC, 

pp 303‚Äì316 

N. Chomsky. 1956. Three models for the description of 

language. Information Theory, IRE Transactions on, 

2(3), 113-124. 

S. Clark, and J. Curran. 2007. Wide-coverage efficient 

statistical parsing with CCG and log-linear models. 

Computational Linguistics, 33(4):493-552. 

D. Das, D. Chen, A.F.T. Martins, N. Schneider and 

N.A. Smith. 2014. Frame-Semantic Parsing. Com-

putational Linguistics 40:1, pages 9-56 

D. Dellarosa. 1986. A computer simulation of chil-

dren‚Äôs arithmetic word problem solving. Behavior 

Research Methods, Instruments, & Computers, 

18:147‚Äì154 

V. Durme, T. Qian, and L. Schubert. 2008. Class-

driven attribute extraction. In Proceedings of the 

22nd International Conference on Computational 

Linguistics-Volume 1, pp. 921-928. Association for 

Computational Linguistics, 2008. 

J. Earley. 1970. An efficient context-free parsing algo-

rithm. Communications of the ACM, 13(2), 94-102. 

C.J. Fillmore, C.R. Johnson, and M.R. Petruck. 2003. 

Background to FrameNet. International Journal of 

Lexicography, 16(3). 

C.R. Fletcher. 1985. Understanding and solving arith-

metic word problems: a computer simulation. Be-

havior Research Methods, Instruments, & Comput-

ers, 17:565‚Äì571 

D. Gildea, and D. Jurafsky. 2002. Automatic labeling 

of semantic roles. Computational Linguistics, 28(3). 

M. Hearst. 1992. Automatic Acquisition of Hyponyms 

from Large Text Corpora. In Fourteenth Interna-

tional Conference on Computational Linguistics, 

Nantes, France. 

M.J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kush-

man. 2014. Learning to Solve Arithmetic Word 

Problems with Verb Categorization. In 

EMNLP‚Äô2014. 

D. Jurafsky, and J.H. Martin. 2000. Speech & language 

processing. Pearson Education India. 

T. Kasami. 1965. An efficient recognition and syntax-

analysis algorithm for context-free languages 

(Technical report). AFCRL. 65-758. 

P. Kingsbury, and M. Palmer. 2002. From TreeBank to 

PropBank. In Proceedings of LREC. 

N. Kushman, Y. Artzi, L. Zettlemoyer, and R. Barzi-

lay. 2014. Learning to automatically solve algebra 

word problems. In Proc. of the Annual Meeting of 

the Association for Computational Linguistics 

(ACL). 

T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. 

2013. Scaling semantic parsers with on-the-fly on-

tology matching. In Empirical Methods in Natural 

Language Processing (EMNLP). 

I. Lev, B. MacCartney, C. Manning, and R. Levy. 

2004. Solving logic puzzles: From robust pro-

cessing to precise semantics. In Proceedings of the 

Workshop on Text Meaning and Interpretation. As-

sociation for Computational Linguistics. 

C. Liguda, T. Pfeiffer. 2012. Modeling Math Word 

Problems with Augmented Semantic Networks. 

NLDB‚Äô2012, pp. 247-252. 

Y. Ma, Y. Zhou, G. Cui, R. Yun, R. Huang. 2010. 

Frame-based calculus of solving arithmetic multi-

step addition and subtraction word problems. In In-

ternational Workshop on Education Technology and 

Computer Science, vol. 2, pp. 476‚Äì479. 

L. Marquez, X. Carreras, K.C. Litkowski, and S. Ste-

venson. 2008. Semantic role labeling: an introduc-

tion to the special issue. Computational Linguistics, 

34(2). 

A. Mukherjee and U. Garain. 2008. A review of meth-

ods for automatic understanding of natural language 

mathematical problems. Artificial Intelligence Re-

view, 29(2). 

M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 

2006. Organizing and searching the world wide web 

of facts-step one: the one-million fact extraction 

challenge. In AAAI (Vol. 6, pp. 1400-1405). 

K.K. Schuler. 2005. VerbNet: A broad-coverage, com-

prehensive verb lexicon. Dissertation. http://reposi-

tory.upenn.edu/dissertations/AAI3179808 

1141



S. Shi, H. Zhang, X. Yuan, and J.-R. Wen. 2010. Cor-

pus-based semantic class mining: distributional vs. 

pattern-based approaches. In Proceedings of the 

23rd International Conference on Computational 

Linguistics, pages 993‚Äì1001. Association for Com-

putational Linguistics. 

M. Steedman. 2000. The Syntactic Process. The MIT 

Press. 

Y. W. Wong and R. J. Mooney. 2007. Learning syn-

chronous grammars for semantic parsing with 

lambda calculus. In Association for Computational 

Linguistics (ACL), pages 960‚Äì967. 

M. Zelle and R.J. Mooney. 1996. Learning to parse da-

tabase queries using inductive logic proramming. In 

Association for the Advancement of Artificial Intel-

ligence (AAAI), pages 1050‚Äì1055. 

L.S. Zettlemoyer and M. Collins. 2005. Learning to 

map sentences to logical form: Structured classifi-

cation with probabilistic categorial grammars. In 

Uncertainty in Artificial Intelligence (UAI), pages 

658‚Äì666. 

L.S. Zettlemoyer and M. Collins. 2007. Online Learn-

ing of Relaxed CCG Grammars for Parsing to Log-

ical Form. In Proceedings of the Joint Conference 

on Empirical Methods in Natural Language Pro-

cessing and Computational Natural Language 

Learning (EMNLP-CoNLL). 

F. Zhang, S. Shi, J. Liu, S. Sun, and C.-Y. Lin. 2011. 

Nonlinear evidence fusion and propagation for hyp-

onymy relation mining. In ACL, volume 11, pages 

1159‚Äì1168. 

 

1142


