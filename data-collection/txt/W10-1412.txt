










































Easy-First Dependency Parsing of Modern Hebrew


Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103–107,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Easy First Dependency Parsing of Modern Hebrew

Yoav Goldberg∗ and Michael Elhadad
Ben Gurion University of the Negev

Department of Computer Science
POB 653 Be’er Sheva, 84105, Israel

{yoavg|elhadad}@cs.bgu.ac.il

Abstract

We investigate the performance of an easy-
first, non-directional dependency parser on the
Hebrew Dependency treebank. We show that
with a basic feature set the greedy parser’s ac-
curacy is on a par with that of a first-order
globally optimized MST parser. The addition
of morphological-agreement feature improves
the parsing accuracy, making it on-par with a
second-order globally optimized MST parser.
The improvement due to the morphological
agreement information is persistent both when
gold-standard and automatically-induced mor-
phological information is used.

1 Introduction

Data-driven Dependency Parsing algorithms are
broadly categorized into two approaches (Kübler et
al., 2009). Transition based parsers traverse the
sentence from left to right1 using greedy, local in-
ference. Graph based parsers use global inference
and seek a tree structure maximizing some scoring
function defined over trees. This scoring function
is usually decomposed over tree edges, or pairs of
such edges. In recent work (Goldberg and Elhadad,
2010), we proposed another dependency parsing ap-
proach: Easy First, Non-Directional dependency

∗Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University

1Strictly speaking, the traversal order is from start to end.
This distinction is important when discussing Hebrew parsing,
as the Hebrew language is written from right-to-left. We keep
the left-to-right terminology throughout this paper, as this is the
common terminology. However, “left” and “right” should be
interpreted as “start” and “end” respectively. Similarly, “a token
to the left” should be interpreted as “the previous token”.

parsing. Like transition based methods, the easy-
first method adopts a local, greedy policy. How-
ever, it abandons the strict left-to-right processing
order, replacing it with an alternative order, which
attempts to make easier attachments decisions prior
to harder ones. The model was applied to English
dependency parsing. It was shown to be more accu-
rate than MALTPARSER, a state-of-the-art transition
based parser (Nivre et al., 2006), and near the perfor-
mance of the first-order MSTPARSER, a graph based
parser which decomposes its score over tree edges
(McDonald et al., 2005), while being more efficient.

The easy-first parser works by making easier de-
cisions before harder ones. Each decision can be
conditioned by structures created by previous deci-
sions, allowing harder decisions to be based on rel-
atively rich syntactic structure. This is in contrast to
the globally optimized parsers, which cannot utilize
such rich syntactic structures. It was hypothesized
in (Goldberg and Elhadad, 2010) that this rich con-
ditioning can be especially beneficial in situations
where informative structural information is avail-
able, such as in morphologically rich languages.

In this paper, we investigate the non-directional
easy-first parser performance on Modern Hebrew, a
semitic language with rich morphology, relatively
free constituent order, and a small treebank com-
pared to English. We are interested in two main
questions: (a) how well does the non-directional
parser perform on Hebrew data? and (b) can the
parser make effective use of morphological features,
such as agreement?

In (Goldberg and Elhadad, 2009), we describe
a newly created Hebrew dependency treebank, and

103



report results on parsing this corpus with both
MALTPARSER and first- and second- order vari-
ants of MSTPARSER. We find that the second-
order MSTPARSER outperforms the first order MST-
PARSER, which in turn outperforms the transition
based MALTPARSER. In addition, adding mor-
phological information to the default configurations
of these parsers does not improve parsing accu-
racy. Interestingly, when using automatically in-
duced (rather than gold-standard) morphological in-
formation, the transition based MALTPARSER’s ac-
curacy improves with the addition of the morpho-
logical information, while the scores of both glob-
ally optimized parsers drop with the addition of the
morphological information.

Our experiments in this paper show that the ac-
curacy of the non-directional parser on the same
dataset outperforms the first-order MSTPARSER.
With the addition of morphological agreement fea-
tures, the parser accuracy improves even further, and
is on-par with the performance of the second-order
MSTPARSER. The improvement due to the morpho-
logical information persists also when automatically
induced morphological information is used.

2 Modern Hebrew

Some aspects that make Hebrew challenging from a
language-processing perspective are:

Affixation Common prepositions, conjunctions
and articles are prefixed to the following word, and
pronominal elements often appear as suffixes. The
segmentation of prefixes and suffixes is often am-
biguous and must be determined in a specific context
only. In term of dependency parsing, this means that
the dependency relations occur not between space-
delimited tokens, but instead between sub-token el-
ements which we’ll refer to as segments. Further-
more, mistakes in the underlying token segmenta-
tions are sure to be reflected in the parsing accuracy.

Relatively free constituent order The ordering of
constituents inside a phrase is relatively free. This
is most notably apparent in the verbal phrases and
sentential levels. In particular, while most sentences
follow an SVO order, OVS and VSO configurations
are also possible. Verbal arguments can appear be-
fore or after the verb, and in many ordering. For

example, the message “went from Israel to Thai-
land” can be expressed as “went to Thailand from
Israel”, “to Thailand went from Israel”, “from Israel
went to Thailand”, “from Israel to Thailand went”
and “to Thailand from Israel went”. This results in
long and flat VP and S structures and a fair amount
of sparsity, which suggests that a dependency repre-
sentations might be more suitable to Hebrew than a
constituency one.

NP Structure and Construct State While con-
stituents order may vary, NP internal structure is
rigid. A special morphological marker (Construct
State) is used to mark noun compounds as well as
similar phenomena. This marker, while ambiguous,
is essential for analyzing NP internal structure.

Case Marking definite direct objects are marked.
The case marker in this case is the function word z`
appearing before the direct object.2

Rich templatic morphology Hebrew has a very
productive morphological structure, which is based
on a root+template system. The productive mor-
phology results in many distinct word forms and
a high out-of-vocabulary rate which makes it hard
to reliably estimate lexical parameters from anno-
tated corpora. The root+template system (combined
with the unvocalized writing system) makes it hard
to guess the morphological analyses of an unknown
word based on its prefix and suffix, as usually done
in other languages.

Unvocalized writing system Most vowels are not
marked in everyday Hebrew text, which results in a
very high level of lexical and morphological ambi-
guity. Some tokens can admit as many as 15 distinct
readings, and the average number of possible mor-
phological analyses per token in Hebrew text is 2.7,
compared to 1.4 in English (Adler, 2007).

Agreement Hebrew grammar forces morpholog-
ical agreement between Adjectives and Nouns
(which should agree in Gender and Number and def-
initeness), and between Subjects and Verbs (which
should agree in Gender and Number).

2The orthographic form z` is ambiguous. It can also stand
for the noun “shovel” and the pronoun “you”(2nd,fem,sing).

104



3 Easy First Non Directional Parsing

Easy-First Non Directional parsing is a greedy
search procedure. It works with a list of partial
structures, pi, . . . , pk, which is initialized with the
n words of the sentence. Each structure is a head
token which is not yet assigned a parent, but may
have dependants attached to it. At each stage of the
parsing algorithm, two neighbouring partial struc-
tures, (pi, pi+1) are chosen, and one of them be-
comes the parent of the other. The new dependant is
then removed from the list of partial structures. Pars-
ing proceeds until only one partial structure, corre-
sponding to the root of the sentence, is left. The
choice of which neighbouring structures to attach is
based on a scoring function. This scoring function
is learned from data, and attempts to attach more
confident attachments before less confident ones.
The scoring function makes use of features. These
features can be extracted from any pre-built struc-
tures. In practice, the features are defined on pre-
built structures which are around the proposed at-
tachment point. For complete details about training,
features and implementation, refer to (Goldberg and
Elhadad, 2010).

4 Experiments

We follow the setup of (Goldberg and Elhadad,
2009).

Data We use the Hebrew dependency treebank de-
scribed in (Goldberg and Elhadad, 2009). We use
Sections 2-12 (sentences 484-5724) as our training
set, and report results on parsing the development
set, Section 1 (sentences 0-483). As in (Goldberg
and Elhadad, 2009), we do not evaluate on the test
set in this work.

The data in the treebank is segmented and POS-
tagged. Both the parsing models were trained on
the gold-standard segmented and tagged data. When
evaluating the parsing models, we perform two sets
of evaluations. The first one is an oracle experi-
ment, assuming gold segmentation and tagging is
available. The second one is a real-world experi-
ment, in which we segment and POS-tag the test-
set sentences using the morphological disambigua-
tor described in (Adler, 2007; Goldberg et al., 2008)
prior to parsing.

Parsers and parsing models We use our freely
available implementation3 of the non-directional
parser.

Evaluation Measure We evaluate the resulting
parses in terms of unlabeled accuracy – the percent
of correctly identified (child,parent) pairs4. To be
precise, we calculate:

number of correctly identified pairs

number of pairs in gold parse

For the oracle case in which the gold-standard to-
ken segmentation is available for the parser, this is
the same as the traditional unlabeled-accuracy eval-
uation metric. However, in the real-word setting in
which the token segmentation is done automatically,
the yields of the gold-standard and the automatic
parse may differ, and one needs to decide how to
handle the cases in which one or more elements in
the identified (child,parent) pair are not present in
the gold-standard parse. Our evaluation metric pe-
nalizes these cases by regarding them as mistakes.

5 Results
Base Feature Set On the first set of experiments,
we used the English feature set which was used in
(Goldberg and Elhadad, 2010). Our only modifica-
tion to the feature set for Hebrew was not to lex-
icalize prepositions (we found it to work somewhat
better due to the smaller treebank size, and Hebrew’s
rather productive preposition system).

Results of parsing the development set are sum-
marized in Table 1. For comparison, we list the per-
formance of the MALT and MST parsers on the same
data, as reported in (Goldberg and Elhadad, 2009).

The case marker z`, as well as the morpholog-
ically marked construct nouns, are covered by all
feature models. z` is a distinct lexical element in a
predictable position, and all four parsers utilize such
function word information. Construct nouns are dif-
ferentiated from non-construct nouns already at the
POS tagset level.

All models suffer from the absence of gold
POS/morphological information. The easy-first
non-directional parser with the basic feature set

3http://www.cs.bgu.ac.il/∼yoavg/software/nondirparser/
4All the results are macro averaged.

105



(NONDIR) outperforms the transition based MALT-
PARSER in all cases. It also outperforms the first or-
der MST1 model when gold POS/morphology infor-
mation is available, and has nearly identical perfor-
mance to MST1 when automatically induced POS/-
morphology information is used.

Additional Morphology Features Error inspec-
tion reveals that most errors are semantic in nature,
and involve coordination, PP-attachment or main-
verb hierarchy. However, some small class of er-
rors reflected morphological disagreement between
nouns and adjectives. These errors were either in-
side a simple NP, or, in some cases, could affect rel-
ative clause attachments. We were thus motivated to
add specific features encoding morphological agree-
ment to try and avoid this class of errors.

Our features are targeted specifically at capturing
noun-adjective morphological agreement.5 When
attempting to score the attachment of two neigh-
bouring structures in the list, pi and pi+1, we in-
spect the pairs (pi, pi+1), (pi, pi+2), (pi−1, pi+1),
(pi−2, pi), (pi+1, pi+2). For each such pair, in case
it is made of a noun and an adjective, we add two
features: a binary feature indicating presence or ab-
sence of gender agreement, and another binary fea-
ture for number agreement.

The last row in Table 1 (NONDIR+MORPH)
presents the parser accuracy with the addition of
these agreement features. Agreement contributes
to the accuracy of the parser, making it as accu-
rate as the second-order MST2. Interestingly, the
non-directional model benefits from the agreement
features also when automatically induced POS/mor-
phology information is used (going from 75.5% to
76.2%). This is in contrast to the MST parsers,
where the morphological features hurt the parser
when non-gold morphology is used (75.6 to 73.9
for MST1 and 76.4 to 74.6 for MST2). This can
be attributed to either the agreement specific na-
ture of the morphological features added to the non-
directional parser, or to the easy-first order of the
non-directional parser, and to the fact the morpho-
logical features are defined only over structurally
close heads at each stage of the parsing process.

5This is in contrast to the morphological features used in
out-of-the-box MST and MALT parsers, which are much more
general.

Gold Morph/POS Auto Morph/POS
MALT 80.3 72.9
MALT+MORPH 80.7 73.4
MST1 83.6 75.6
MST1+MORPH 83.6 73.9
MST2 84.3 76.4
MST2+MORPH 84.4 74.6
NONDIR 83.8 75.5
NONDIR+MORPH 84.2 76.2

Table 1: Unlabeled dependency accuracy of the various
parsing models.

6 Discussion
We have verified that easy-first, non-directional de-
pendency parsing methodology of (Goldberg and El-
hadad, 2010) is successful for parsing Hebrew, a
semitic language with rich morphology and a small
treebank. We further verified that the model can
make effective use of morphological agreement fea-
tures, both when gold-standard and automatically in-
duced morphological information is provided. With
the addition of the morphological agreement fea-
tures, the non-directional model is as effective as a
second-order globally optimized MST model, while
being much more efficient, and easier to extend with
additional structural features.

While we get adequate parsing results for Hebrew
when gold-standard POS/morphology/segmentation
information is used, the parsing performance in
the realistic setting, in which gold POS/morpholo-
gy/segmentation information is not available, is still
low. We strongly believe that parsing and morpho-
logical disambiguation should be done jointly, or
at least interact with each other. This is the main
future direction for dependency parsing of Modern
Hebrew.

References

Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.

Yoav Goldberg and Michael Elhadad. 2009. Hebrew De-
pendency Parsing: Initial Results. In Proc. of IWPT.

Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proc. of NAACL.

Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.

106



EM Can find pretty good HMM POS-Taggers (when
given a good start). In Proc. of ACL.

Sandra Kübler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.

Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc of ACL.

Joakim Nivre, Johan Hall, and Jens Nillson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proc. of LREC.

107


