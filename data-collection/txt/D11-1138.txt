



















































Training dependency parsers by jointly optimizing multiple objectives


Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1489–1499,
Edinburgh, Scotland, UK, July 27–31, 2011. c©2011 Association for Computational Linguistics

Training dependency parsers by jointly optimizing multiple objectives

Keith Hall Ryan McDonald Jason Katz-Brown Michael Ringgaard
Google Research

{kbhall|ryanmcd|jasonkb|ringgaard}@google.com

Abstract

We present an online learning algorithm for
training parsers which allows for the inclusion
of multiple objective functions. The primary
example is the extension of a standard su-
pervised parsing objective function with addi-
tional loss-functions, either based on intrinsic
parsing quality or task-specific extrinsic mea-
sures of quality. Our empirical results show
how this approach performs for two depen-
dency parsing algorithms (graph-based and
transition-based parsing) and how it achieves
increased performance on multiple target tasks
including reordering for machine translation
and parser adaptation.

1 Introduction

The accuracy and speed of state-of-the-art depen-
dency parsers has motivated a resumed interest in
utilizing the output of parsing as an input to many
downstream natural language processing tasks. This
includes work on question answering (Wang et al.,
2007), sentiment analysis (Nakagawa et al., 2010),
MT reordering (Xu et al., 2009), and many other
tasks. In most cases, the accuracy of parsers de-
grades when run on out-of-domain data (Gildea,
2001; McClosky et al., 2006; Blitzer et al., 2006;
Petrov et al., 2010). But these accuracies are mea-
sured with respect to gold-standard out-of-domain
parse trees. There are few tasks that actually depend
on the complete parse tree. Furthermore, when eval-
uated on a downstream task, often the optimal parse
output has a model score lower than the best parse
as predicted by the parsing model. While this means

that we are not properly modeling the downstream
task in the parsers, it also means that there is some
information from small task or domain-specific data
sets which could help direct our search for optimal
parameters during parser training. The goal being
not necessarily to obtain better parse performance,
but to exploit the structure induced from human la-
beled treebank data while targeting specific extrinsic
metrics of quality, which can include task specific
metrics or external weak constraints on the parse
structure.

One obvious approach to this problem is to em-
ploy parser reranking (Collins, 2000). In such a
setting, an auxiliary reranker is added in a pipeline
following the parser. The standard setting involves
training the base parser and applying it to a devel-
opment set (this is often done in a cross-validated
jack-knife training framework). The reranker can
then be trained to optimize for the downstream or
extrinsic objective. While this will bias the reranker
towards the target task, it is limited by the oracle
performance of the original base parser.

In this paper, we propose a training algorithm for
statistical dependency parsers (Kübler et al., 2009)
in which a single model is jointly optimized for a
regular supervised training objective over the tree-
bank data as well as a task-specific objective – or
more generally an extrinsic objective – on an ad-
ditional data set. The case where there are both
gold-standard trees and a task-specific objective for
the entire training set is a specific instance of the
larger problem that we address here. Specifically,
the algorithm takes the form of an online learner
where a training instance is selected and the param-

1489



eters are optimized based on the objective function
associated with the instance (either intrinsic or ex-
trinsic), thus jointly optimizing multiple objectives.
An update schedule trades-off the relative impor-
tance of each objective function. We call our algo-
rithm augmented-loss training as it optimizes mul-
tiple losses to augment the traditional supervised
parser loss.

There have been a number of efforts to exploit
weak or external signals of quality to train better pre-
diction models. This includes work on generalized
expectation (Mann and McCallum, 2010), posterior
regularization (Ganchev et al., 2010) and constraint
driven learning (Chang et al., 2007; Chang et al.,
2010). The work of Chang et al. (2007) on constraint
driven learning is perhaps the closest to our frame-
work and we draw connections to it in Section 5.
In these studies the typical goal is to use the weak
signal to improve the structured prediction models
on the intrinsic evaluation metrics. For our setting
this would mean using weak application specific sig-
nals to improve dependency parsing. Though we
explore such ideas in our experiments, in particular
for semi-supervised domain adaptation, we are pri-
marily interested in the case where the weak signal
is precisely what we wish to optimize, but also de-
sire the benefit from using both data with annotated
parse structures and data specific to the task at hand
to guide parser training.

In Section 2 we outline the augmented-loss algo-
rithm and provide a convergence analysis. In Sec-
tion 3 and 4 we present a set of experiments defin-
ing diffent augmented losses covering a task-specific
extrinsic loss (MT reordering), a domain adapta-
tion loss, and an alternate intrinsic parser loss. In
all cases we show the augmented-loss framework
can lead to significant gains in performance. In
Section 5 we tie our augmented-loss algorithm to
other frameworks for encoding auxiliary informa-
tion and/or joint objective optimization.

2 Methodology

We present the augmented-loss algorithm in the con-
text of the structured perceptron. The structured
perceptron (Algorithm 1) is an on-line learning al-
gorithm which takes as input: 1) a set of training
examples di = (xi, yi) consisting of an input sen-

Algorithm 1 Structured Perceptron
{Input data sets: D = {d1 = (x1, y1) . . . dN = (xN , yN )}}
{Input 0/1 loss: L(Fθ(x), y) = [Fθ(x) 6= y ? 1 : 0]}
{Let: Fθ(x) = arg maxy∈Y θ · Φ(y)}
{Initialize model parameters: θ = ~0}
repeat

for i = 1 . . . N do
{Compute structured loss}
ŷi = Fθ(xi)
if L(ŷi, yi) > 0 then
{Update model Parameters}
θ = θ + Φ(yi)− Φ(ŷi)

end if
end for

until converged
{Return model θ}

tence xi and an output yi; and 2) a loss-function,
L(ŷ, y), that measures the cost of predicting out-
put ŷ relative to the gold standard y and is usu-
ally the 0/1 loss (Collins, 2002). For dependency
parser training, this set-up consists of input sen-
tences x and the corresponding gold dependency
tree y ∈ Yx, where Yx is the space of possible
parse trees for sentence x. In the perceptron setting,
Fθ(x) = arg maxy∈Yx θ ·Φ(y) where Φ is mapping
from a parse tree y for sentence x to a high dimen-
sional feature space. Learning proceeds by predict-
ing a structured output given the current model, and
if that structure is incorrect, updating the model: re-
warding features that fire in the gold-standard Φ(yi),
and discounting features that fire in the predicted
output, Φ(ŷi).

The structured perceptron, as given in Algo-
rithm 1, only updates when there is a positive loss,
meaning that there was a prediction mistake. For
the moment we will abstract away from details such
as the precise definition of F (x) and Φ(y). We
will show in the next section that our augmented-
loss method is general and can be applied to any de-
pendency parsing framework that can be trained by
the perceptron algorithm, such as transition-based
parsers (Nivre, 2008; Zhang and Clark, 2008) and
graph-based parsers (McDonald et al., 2005).

2.1 Augmented-Loss Training

The augmented-loss training algorithm that we pro-
pose is based on the structured perceptron; however,
the augmented-loss training framework is a general

1490



mechanism to incorporate multiple loss functions in
online learner training. Algorithm 2 is the pseudo-
code for the augmented-loss structured perceptron
algorithm. The algorithm is an extension to Algo-
rithm 1 where there are 1) multiple loss functions
being evaluated L1, . . . , LM ; 2) there are multiple
datasets associated with each of these loss functions
D1, . . . ,DM ; and 3) there is a schedule for pro-
cessing examples from each of these datasets, where
Sched(j, i) is true if the jth loss function should be
updated on the ith iteration of training. Note that
for data point dji = (x, y), which is the i

th training
instance of the jth data set, that y does not neces-
sarily have to be a dependency tree. It can either
be a task-specific output of interest, a partial tree, or
even null, in the case where learning will be guided
strictly by the loss Lj . The training algorithm is ef-
fectively the same as the perceptron, the primary dif-
ference is that if Lj is an extrinsic loss, we cannot
compute the standard updates since we do not nec-
essarily know the correct parse (the line indicated by
†). Section 2.2 shows one method for updating the
parser parameters for extrinsic losses.

In the experiments in this paper, we only consider
the case where there are two loss functions: a super-
vised dependency parsing labeled-attachment loss;
and an additional loss, examples of which are pre-
sented in Section 3.

2.2 Inline Ranker Training

In order to make Algorithm 2 more concrete, we
need a way of defining the loss and resulting pa-
rameter updates for the case when Lj is not a stan-
dard supervised parsing loss († from Algorithm 2).
Assume that we have a cost function C(xi, ŷ, yi)
which, given a training example (xi, yi) will give a
score for a parse ŷ ∈ Yxi relative to some output
yi. While we can compute the score for any parse,
we are unable to determine the features associated
with the optimal parse, as yi need not be a parse
tree. For example, consider a machine translation re-
ordering system which uses the parse ŷ to reorder the
words of xi, the optimal reordering being yi. Then
C(xi, ŷ, yi) is a reordering cost which is large if the
predicted parse induces a poor reordering of xi.

We propose a general purpose loss function which
is based on parser k-best lists. The inline reranker
uses the currently trained parser model θ to parse

Algorithm 2 Augmented-Loss Perceptron
{Input data sets}:
D1 = {d11 = (x11, y11) . . . d1N1 = (x1N1 , y1N1)},
. . .
DM = {dM1 = (xM1 , yM1 ) . . . dMNM = (xMNM , yMNM )}
{Input loss functions: L1 . . . LM}
{Initialize indexes: c1 . . . cM = ~0}
{Initialize model parameters: θ = ~0}
i = 0
repeat

for j = 1 . . .M do
{Check whether to update Lj on iteration i}
if Sched(j, i) then
{Compute index of instance – reset if cj ≡ N j}
cj = [(cj ≡ N j) ? 0 : cj + 1]
{Compute structured loss for instance}
if Lj is intrinsic loss then
ŷ = Fθ(x

j
cj )

if Lj(ŷ, yjcj ) > 0 then
θ = θ + Φ(yjcj )− Φ(ŷ) {yjcj is a tree}

end if
else if Lj is an extrinsic loss then
{See Section 2.2}†

end if
end if

end for
i = i+ 1

until converged
{Return model θ}

the external input, producing a k-best set of parses:
Fk-bestθ (xi) = {ŷ1, . . . , ŷk}. We can compute the
cost function C(xi, ŷ, yi) for all ŷ ∈ Fk-bestθ (xi). If
the 1-best parse, ŷ1, has the lowest cost, then there is
no lower cost parse in this k-best list. Otherwise, the
lowest-cost parse in Fk-bestθ (xi) is taken to be the
correct output structure yi, and the 1-best parse is
taken to be an incorrect prediction. We can achieve
this by substituting the following into Algorithm 2
at line †.

Algorithm 3 Reranker Loss
{ŷ1, . . . , ŷk} = Fk-bestθ (xi)
τ = minτ C(x

j
cj , ŷτ , y

j
cj ) {τ is min const index}

Lj(ŷ1, y
j
cj ) = C(x

j
cj , ŷ1, y

j
cj )− C(xjcj , ŷτ , yjcj )

if Lj(ŷ1, yjcj ) > 0 then
θ = θ + Φ(ŷτ )− Φ(ŷ1)

end if

Again the algorithm only updates when there is
an error – when the 1-best output has a higher cost
than any other output in the k-best list – resulting

1491



in positive Lj . The intuition behind this method is
that in the presence of only a cost function and a
k-best list, the parameters will be updated towards
the parse structure that has the lowest cost, which
over time will move the parameters of the model to
a place with low extrinsic loss.

We exploit this formulation of the general-
purpose augmented-loss function as it allows one to
include any extrinsic cost function which is depen-
dent of parses. The scoring function used does not
need to be factored, requiring no internal knowledge
of the function itself. Furthermore, we can apply this
to any parsing algorithm which can generate k-best
lists. For each parse, we must retain the features
associated with the parse (e.g., for transition-based
parsing, the features associated with the transition
sequence resulting in the parse).

There are two significant differences from the in-
line reranker loss function and standard reranker
training. First, we are performing this decision per
example as each data item is processed (this is done
in the inner loop of the Algorithm 2). Second, the
feedback function for selecting a parse is based on
an external objective function. The second point is
actually true for many minimum-error-rate training
scenarios, but in those settings the model is updated
as a post-processing stage (after the base-model is
trained).

2.3 Convergence of Inline Ranker Training
A training setD is loss-separable with margin γ > 0
if there exists a vector u with ‖u‖ = 1 such that
for all y′, y′′ ∈ Yx and (x, y) ∈ D, if L(y′, y) <
L(y′′, y), then u·Φ(y′)−u·Φ(y′′) ≥ γ. Furthermore,
let R ≥ ||Φ(y)− Φ(y′)||, for all y, y′.
Assumption 1. Assume training set D is loss-
separable with margin γ.
Theorem 1. Given Assumption 1. Letm be the num-
ber of mistakes made when training the perceptron
(Algorithm 2) with inline ranker loss (Algorithm 3)
on D, where a mistake occurs for (x, y) ∈ D with
parameter vector θ when ∃ŷj ∈ F k-bestθ (x) where
ŷj 6= ŷ1 and L(ŷj , y) < L(ŷ1, y). If training is run
indefinitely, then m ≤ R2

γ2
.

Proof. Identical to the standard perceptron proof,
e.g., Collins (2002), by inserting in loss-separability
for normal separability.

Like the original perceptron theorem, this implies
that the algorithm will converge. However, unlike
the original theorem, it does not imply that it will
converge to a parameter vector θ such that for all
(x, y) ∈ D, if ŷ = arg maxŷ θ ·Φ(ŷ) then L(ŷ, y) =
0. Even if we assume for every x there exists an out-
put with zero loss, Theorem 1 still makes no guar-
antees. Consider a training set with one instance
(x, y). Now, set k = 2 for the k-best output list and
let ŷ1, ŷ2, and ŷ3 be the top-3 scoring outputs and
let L(ŷ1, y) = 1, L(ŷ2, y) = 2 and L(ŷ3, y) = 0.
In this case, no updates will ever be made and ŷ1
will remain unchanged even though it doesn’t have
minimal loss. Consider the following assumption:

Assumption 2. For any parameter vector θ that ex-
ists during training, either 1) for all (x, y) ∈ D,
L(ŷ1, y) = 0 (or some optimal minimum loss),
or 2) there exists at least one (x, y) ∈ D where
∃ŷj ∈ F k-bestθ (x) such that L(ŷj , y) < L(ŷ1, y).

Assumption 2 states that for any θ that exists
during training, but before convergence, there is at
least one example in the training data where k is
large enough to include one output with a lower loss
when ŷ1 does not have the optimal minimal loss. If
k = ∞, then this is the standard perceptron as it
guarantees the optimal loss output to be in the k-best
list. But we are assuming something much weaker
here, i.e., not that the k-best list will include the min-
imal loss output, only a single output with a lower
loss than the current best guess. However, it is strong
enough to show the following:

Theorem 2. Given Assumption 1 and Assumption 2.
Training the perceptron (Algorithm 2) with inline
ranker loss (Algorithm 3) on D 1) converges in fi-
nite time, and 2) produces parameters θ such that
for all (x, y) ∈ D, if ŷ = arg maxŷ θ · Φ(ŷ) then
L(ŷ, y) = 0 (or equivalent minimal loss).

Proof. It must be the case for all (x, y) ∈ D that
L(ŷ1, y) = 0 (and ŷ1 is the argmax) after a finite
amount of time. Otherwise, by Assumption 2, there
exists some x, such that when it is next processed,
there would exist an output in the k-best list that
had a lower loss, which will result in an additional
mistake. Theorem 1 guarantees that this can not
continue indefinitely as the number of mistakes is
bounded.

1492



Thus, the perceptron algorithm will converge to
optimal minimal loss under the assumption that k
is large enough so that the model can keep improv-
ing. Note that this does not mean k must be large
enough to include a zero or minimum loss output,
just large enough to include a better output than
the current best hypothesis. Theorem 2, when cou-
pled with Theorem 1, implies that augmented-loss
learning will make at most R2/γ2 mistakes at train-
ing, but does not guarantee the rate at which these
mistakes will be made, only that convergence is fi-
nite, providing that the scheduling time (defined by
Sched()) between seeing the same instance is always
finite, which is always true in our experiments.

This analysis does not assume anything about the
loss L. Every instance (x, y) can use a different loss.
It is only required that the loss for a specific input-
output pair is fixed throughout training. Thus, the
above analysis covers the case where some training
instances use an extrinsic loss and others an intrin-
sic parsing loss. This also suggests more efficient
training methods when extracting the k-best list is
prohibitive. One can parse with k = 2, 4, 8, 16, . . .
until an k is reached that includes a lower loss parse.
It may be the case that for most instances a small
k is required, but the algorithm is doing more work
unnecessarily if k is large.

3 Experimental Set-up

3.1 Dependency Parsers
The augmented-loss framework we present is gen-
eral in the sense that it can be combined with any
loss function and any parser, provided the parser can
be parameterized as a linear classifier, trained with
the perceptron and is capable of producing a k-best
list of trees. For our experiments we focus on two
dependency parsers.

• Transition-based: An implementation of the
transition-based dependency parsing frame-
work (Nivre, 2008) using an arc-eager transi-
tion strategy and are trained using the percep-
tron algorithm as in Zhang and Clark (2008)
with a beam size of 8. Beams with varying
sizes can be used to produce k-best lists. The
features used by all models are: the part-of-
speech tags of the first four words on the buffer
and of the top two words on the stack; the word

identities of the first two words on the buffer
and of the top word on the stack; the word iden-
tity of the syntactic head of the top word on the
stack (if available); dependency arc label iden-
tities for the top word on the stack, the left and
rightmost modifier of the top word on the stack,
and the left most modifier of the first word in
the buffer (if available). All feature conjunc-
tions are included.

• Graph-based: An implementation of graph-
based parsing algorithms with an arc-factored
parameterization (McDonald et al., 2005). We
use the non-projective k-best MST algorithm to
generate k-best lists (Hall, 2007), where k = 8
for the experiments in this paper. The graph-
based parser features used in the experiments
in this paper are defined over a word, wi at po-
sition i; the head of this word wρ(i) where ρ(i)
provides the index of the head word; and part-
of-speech tags of these words ti. We use the
following set of features similar to McDonald
et al. (2005):

isolated features: wi, ti, wρ(i), tρ(i)
word-tag pairs: (wi, ti); (wρ(i), tρ(i))
word-head pairs: (wi, wρ(i)), (ti, tρ(i))
word-head-tag triples: (tρ(i), wi, ti)

(wρ(i), wi, ti)
(wρ(i), tρ(i), ti)
(wρ(i), tρ(i), wi)

tag-neighbourhood: (tρ(i), tρ(i)+1, ti−1, ti)
(tρ(i), tρ(i)+1, ti+1, ti)
(tρ(i), tρ(i)−1, ti−1, ti)
(tρ(i), tρ(i)−1, ti+1, ti)

between features: ∀j i < j < ρ(i) || ρ(i) < j < i
(tρ(i), tj , ti)

arc-direction/length : (i− ρ(i) > 0, |i− ρ(i)|)

3.2 Data and Tasks
In the next section, we present a set of scoring func-
tions that can be used in the inline reranker loss
framework, resulting in a new augmented-loss for
each one. Augmented-loss learning is then applied
to target a downstream task using the loss functions
to measure gains. We show empirical results for two
extrinsic loss-functions (optimizing for the down-
stream task): machine translation and domain adap-
tation; and for one intrinsic loss-function: an arc-
length parsing score. For some experiments we also

1493



measure the standard intrinsic parser metrics unla-
beled attachment score (UAS) and labeled attach-
ment score (LAS) (Buchholz and Marsi, 2006).

In terms of treebank data, the primary training
corpus is the Penn Wall Street Journal Treebank
(PTB) (Marcus et al., 1993). We also make use
of the Brown corpus, and the Question Treebank
(QTB) (Judge et al., 2006). For PTB and Brown
we use standard training/development/testing splits
of the data. For the QTB we split the data into
three sections: 2000 training, 1000 development,
and 1000 test. All treebanks are converted to de-
pendency format using the Stanford converter v1.6
(de Marneffe et al., 2006).

4 Experiments

4.1 Machine Translation Reordering Score

As alluded to in Section 2.2, we use a reordering-
based loss function to improve word order in a ma-
chine translation system. In particular, we use a sys-
tem of source-side reordering rules which, given a
parse of the source sentence, will reorder the sen-
tence into a target-side order (Collins et al., 2005).
In our experiments we work with a set of English-
Japanese reordering rules1 and gold reorderings
based on human generated correct reordering of an
aligned target sentences. We use a reordering score
based on the reordering penalty from the METEOR
scoring metric. Though we could have used a fur-
ther downstream measure like BLEU, METEOR has
also been shown to directly correlate with translation
quality (Banerjee and Lavie, 2005) and is simpler to
measure.

reorder-score = 1− # chunks− 1
# unigrams matched− 1

reorder-cost = 1− reorder-score

All reordering augmented-loss experiments are
run with the same treebank data as the baseline
(the training portions of PTB, Brown, and QTB).
The extrinsic reordering training data consists of
10930 examples of English sentences and their cor-
rect Japanese word-order. We evaluate our results on
an evaluation set of 6338 examples of similarly cre-
ated reordering data. The reordering cost, evaluation

1Our rules are similar to those from Xu et al. (2009).

Exact Reorder
trans–PTB + Brown + QTB 35.29 76.49
trans–0.5×aug.-loss 38.71 78.19
trans–1.0×aug.-loss 39.02 78.39
trans–2.0×aug.-loss 39.58 78.67
graph–PTB + Brown + QTB 25.71 69.84
graph–0.5× aug.-loss 28.99 72.23
graph–1.0×aug.-loss 29.99 72.88
graph–2.0×aug.-loss 30.03 73.15

Table 1: Reordering scores for parser-based reordering
(English-to-Japanese). Exact is the number of correctly
reordered sentences. All models use the same treebank-
data (PTB, QTB, and the Brown corpus). Results for
three augmented-loss schedules are shown: 0.5 where for
every two treebank updates we make one augmented-loss
update, 1 is a 1-to-1 mix, and 2 is where we make twice
as many augmented-loss updates as treebank updates.

criteria and data used in our experiments are based
on the work of Talbot et al. (2011).

Table 1 shows the results of using the reordering
cost as an augmented-loss to the standard treebank
objective function. Results are presented as mea-
sured by the reordering score as well as a coarse
exact-match score (the number of sentences which
would have correct word-order given the parse and
the fixed reordering rules). We see continued im-
provements as we adjust the schedule to process the
extrinsic loss more frequently, the best result being
when we make two augmented-loss updates for ev-
ery one treebank-based loss update.

4.2 Semi-supervised domain adaptation

Another application of the augmented-loss frame-
work is to improve parser domain portability in the
presence of partially labeled data. Consider, for ex-
ample, the case of questions. Petrov et al. (2010)
observed that dependency parsers tend to do quite
poorly when parsing questions due to their lim-
ited exposure to them in the news corpora from
the PennTreebank. Table 2 shows the accuracy
of two parsers (LAS, UAS and the F1 of the root
dependency attachment) on the QuestionBank test
data. The first is a parser trained on the standard
training sections of the PennTreebank (PTB) and
the second is a parser trained on the training por-
tion of the QuestionBank (QTB). Results for both

1494



LAS UAS Root-F1
trans–PTB 67.97 73.52 47.60
trans–QTB 84.59 89.59 91.06
trans–aug.-loss 76.27 86.42 83.41
graph–PTB 65.27 72.72 43.10
graph–QTB 82.73 87.44 91.58
graph–aug.-loss 72.82 80.68 86.26

Table 2: Domain adaptation results. Table shows (for
both transition and graph-based parsers) the labeled ac-
curacy score (LAS), unlabeled accuracy score (UAS)
and Root-F1 for parsers trained on the PTB and QTB
and tested on the QTB. The augmented-loss parsers are
trained on the PTB but with a partial tree loss on QTB
that considers only root dependencies.

transition-based parsers and graph-based parsers are
given. Clearly there is significant drop in accu-
racy for a parser trained on the PTB. For example,
the transition-based PTB parser achieves a LAS of
67.97% relative to 84.59% for the parser trained on
the QTB.

We consider the situation where it is possible to
ask annotators a single question about the target do-
main that is relatively easy to answer. The question
should be posed so that the resulting answer pro-
duces a partially labeled dependency tree. Root-F1
scores from Table 2 suggest that one simple ques-
tion is “what is the main verb of this sentence?” for
sentences that are questions. In most cases this task
is straight-forward and will result in a single depen-
dency, that from the root to the main verb of the sen-
tence. We feel this is a realistic partial labeled train-
ing setting where it would be possible to quickly col-
lect a significant amount of data.

To test whether such weak information can signif-
icantly improve the parsing of questions, we trained
an augmented-loss parser using the training set of
the QTB stripped of all dependencies except the de-
pendency from the root to the main verb of the sen-
tence. In other words, for each sentence, the parser
may only observe a single dependency at training
from the QTB – the dependency to the main verb.
Our augmented-loss function in this case is a simple
binary function: 0 if a parse has the correct root de-
pendency and 1 if it does not. Thus, the algorithm
will select the first parse in the k-best list that has the

correct root as the proxy to a gold standard parse.2

The last row in each section of Table 2 shows the
results for this augmented-loss system when weight-
ing both losses equally during training. By simply
having the main verb annotated in each sentence –
the sentences from the training portion of the QTB
– the parser can eliminate half of the errors of the
original parser. This is reflected by both the Root-
F1 as well as LAS/UAS. It is important to point out
that these improvements are not limited to simply
better root predictions. Due to the fact that parsing
algorithms make many parsing decisions jointly at
test time, all such decisions influence each other and
improvements are seen across the board. For exam-
ple, the transition-based PTB parser has an F1 score
of 41.22% for verb subjects (nsubj), whereas the
augmented-loss parser has an F1 of 73.52%. Clearly
improving just a single (and simple to annotate) de-
pendency leads to general parser improvements.

4.3 Average Arc Length Score
The augmented-loss framework can be used to in-
corporate multiple treebank-based loss functions as
well. Labeled attachment score is used as our base
model loss function. In this set of experiments we
consider adding an additional loss function which
weights the lengths of correct and incorrect arcs, the
average (labeled) arc-length score:

ALS =
∑

i δ(ρ̂i, ρi)(i− ρi)∑
i(i− ρi)

For each word of the sentence we compute the dis-
tance between the word’s position i and the posi-
tion of the words head ρi. The arc-length score is
the summed length of all those with correct head as-
signments (δ(ρ̂i, ρi) is 1 if the predicted head and
the correct head match, 0 otherwise). The score is
normalized by the summed arc lengths for the sen-
tence. The labeled version of this score requires that
the labels of the arc are also correct. Optimizing
for dependency arc length is particularly important
as parsers tend to do worse on longer dependencies
(McDonald and Nivre, 2007) and these dependen-
cies are typically the most meaningful for down-
stream tasks, e.g., main verb dependencies for tasks

2For the graph-based parser one can also find the higest scor-
ing tree with correct root by setting the score of all competing
arcs to −∞.

1495



LAS UAS ALS
trans–PTB 88.64 91.64 82.96
trans–unlabeled aug.-loss 88.74 91.91 83.65
trans–labeled aug.-loss 88.84 91.91 83.46
graph–PTB 85.75 88.70 73.88
graph–unlabeled aug.-loss 85.80 88.81 74.26
graph–labeled aug.-loss 85.85 88.93 74.40

Table 3: Results for both parsers on the development set
of the PTB. When training with ALS (labeled and unla-
beled), we see an improvement in UAS, LAS, and ALS.
Furthermore, if we use a labeled-ALS as the metric for
augmented-loss training, we also see a considerable in-
crease in LAS.

like information extraction (Yates and Etzioni, 2009)
and textual entailment (Berant et al., 2010).

In Table 3 we show results for parsing with the
ALS augmented-loss objective. For each parser, we
consider two different ALS objective functions; one
based on unlabeled-ALS and the other on labeled-
ALS. The arc-length score penalizes incorrect long-
distance dependencies more than local dependen-
cies; long-distance dependencies are often more de-
structive in preserving sentence meaning and can be
more difficult to predict correctly due to the larger
context on which they depend. Combining this with
the standard attachment scores biases training to fo-
cus on the difficult head dependencies.

For both experiments we see that by adding the
ALS augmented-loss we achieve an improvement in
LAS and UAS in addition to ALS. The augmented-
loss not only helps us improve on the longer depen-
dencies (as reflected in the increased ALS), but also
in the main parser objective function of LAS and
UAS. Using the labeled loss function provides better
reinforcement as can be seen in the improvements
over the unlabeled loss-function. As with all experi-
ments in this paper, the graph-based parser baselines
are much lower than the transition-based parser due
to the use of arc-factored features. In these experi-
ments we used an inline-ranker loss with 8 parses.
We experimented with larger sizes (16 and 64) and
found very similar improvements: for example, the
transition parser’s LAS for the labeled loss is 88.68
and 88.84, respectively).

We note that ALS can be decomposed locally and
could be used as the primary objective function for

parsing. A parse with perfect scores under ALS
and LAS will match the gold-standard training tree.
However, if we were to order incorrect parses of a
sentence, ALS and LAS will suggest different order-
ings. Our results show that by optimizing for losses
based on a combination of these metrics we train a
more robust parsing model.

5 Related Work

A recent study by Katz-Brown et al. (2011) also in-
vestigates the task of training parsers to improve MT
reordering. In that work, a parser is used to first
parse a set of manually reordered sentences to pro-
duce k-best lists. The parse with the best reordering
score is then fixed and added back to the training set
and a new parser is trained on resulting data. The
method is called targeted self-training as it is simi-
lar in vein to self-training (McClosky et al., 2006),
with the exception that the new parse data is targeted
to produce accurate word reorderings. Our method
differs as it does not statically fix a new parse, but
dynamically updates the parameters and parse selec-
tion by incorporating the additional loss in the inner
loop of online learning. This allows us to give guar-
antees of convergence. Furthermore, we also evalu-
ate the method on alternate extrinsic loss functions.

Liang et al. (2006) presented a perceptron-based
algorithm for learning the phrase-translation param-
eters in a statistical machine translation system.
Similar to the inline-ranker loss function presented
here, they use a k-best lists of hypotheses in order to
identify parameters which can improve a global ob-
jective function: BLEU score. In their work, they
are interested in learning a parameterization over
translation phrases (including the underlying word-
alignment) which optimizes the BLEU score. Their
goal is considerably different; they want to incor-
porate additional features into their model and de-
fine an objective function which allows them to do
so; whereas, we are interested in allowing for mul-
tiple objective functions in order to adapt the parser
model parameters to downstream tasks or alternative
intrinsic (parsing) objectives.

The work that is most similar to ours is that
of Chang et al. (2007), who introduced the Con-
straint Driven Learning algorithm (CODL). Their al-
gorithm specifically optimizes a loss function with

1496



the addition of constraints based on unlabeled data
(what we call extrinsic datasets). For each unla-
beled example, they use the current model along
with their set of constraints to select a set of k au-
tomatically labeled examples which best meet the
constraints. These induced examples are then added
to their training set and, after processing each unla-
beled dataset, they perform full model optimization
with the concatenation of training data and newly
generated training items. The augmented-loss al-
gorithm can be viewed as an online version of this
algorithm which performs model updates based on
the augmented-loss functions directly (rather than
adding a set of examples to the training set). Un-
like the CODL approach, we do not perform com-
plete optimization on each iteration over the unla-
beled dataset; rather, we incorporate the updates in
our online learning algorithm. As mentioned earlier,
CODL is one example of learning algorithms that
use weak supervision, others include Mann and Mc-
Callum (2010) and Ganchev et al. (2010). Again,
these works are typically interested in using the ex-
trinsic metric – or, in general, extrinsic information
– to optimize the intrinsic metric in the absence of
any labeled intrinsic data. Our goal is to optimize
both simultaneously.

The idea of jointly training parsers to optimize
multiple objectives is related to joint learning and in-
ference for tasks like information extraction (Finkel
and Manning, 2009) and machine translation (Bur-
kett et al., 2010). In such works, a large search space
that covers both the space of parse structures and
the space of task-specific structures is defined and
parameterized so that standard learning and infer-
ence algorithms can be applied. What sets our work
apart is that there is still just a single parameter set
that is being optimized – the parser parameters. Our
method only uses feedback from task specific objec-
tives in order to update the parser parameters, guid-
ing it towards better downstream performance. This
is advantageous for two reasons. First, it decouples
the tasks, making inference and learning more effi-
cient. Second, it does not force arbitrary paraemter
factorizations in order to define a joint search space
that can be searched efficiently.

Finally, augmented-loss training can be viewed
as multi-task learning (Caruana, 1997) as the model
optimizes multiple objectives over multiple data sets

with a shared underlying parameter space.

6 Discussion

The empirical results show that incorporating an
augmented-loss using the inline-ranker loss frame-
work achieves better performance under metrics as-
sociated with the external loss function. For the in-
trinsic loss, we see that the augmented-loss frame-
work can also result in an improvement in parsing
performance; however, in the case of ALS, this is
due to the fact that the loss function is very closely
related to the standard evaluation metrics of UAS
and LAS.

Although our analysis suggests that this algorithm
is guaranteed to converge only for the separable
case, it makes a further assumption that if there is
a better parse under the augmented-loss, then there
must be a lower cost parse in the k-best list. The em-
pirical evaluation presented here is based on a very
conservative approximation by choosing lists with
at most 8 parses. However, in our experiments, we
found that increasing the size of the lists did not sig-
nificantly increase our accuracy under the external
metrics. If we do have at least one improvement
in our k-best lists, the analysis suggests that this is
enough to move in the correct direction for updating
the model. The assumption that there will always
be an improvement in the k-best list if there is some
better parse breaks down as training continues. We
suspect that an increasing k, as suggested in Sec-
tion 2.3, will allow for continued improvements.

Dependency parsing, as presented in this pa-
per, is performed over (k-best) part-of-speech tags
and is therefore dependent on the quality of the
tagger. The experiments presented in this paper
made use of a tagger trained on the source treebank
data which severely limits the variation in parses.
The augmented-loss perceptron algorithm presented
here can be applied to any online learning prob-
lem, including part-of-speech tagger training. To
build a dependency parser which is better adapted
to a downstream task, one would want to perform
augmented-loss training on the tagger as well.

7 Conclusion

We introduced the augmented-loss training algo-
rithm and show that the algorithm can incorporate

1497



additional loss functions to adapt the model towards
extrinsic evaluation metrics. Analytical results are
presented that show that the algorithm can opti-
mize multiple objective functions simultaneously.
We present an empirical analysis for training depen-
dency parsers for multiple parsing algorithms and
multiple loss functions.

The augmented-loss framework supports both in-
trinsic and extrinsic losses, allowing for both com-
binations of objectives as well as multiple sources
of data for which the results of a parser can be eval-
uated. This flexibility makes it possible to tune a
model for a downstream task. The only requirement
is a metric which can be defined over parses of the
downstream data. Our dependency parsing results
show that we are not limited to increasing parser
performance via more data or external domain adap-
tation techniques, but that we can incorporate the
downstream task into parser training.

Acknowledgements: We would like to thank Kuz-
man Ganchev for feedback on an earlier draft of this
paper as well as Slav Petrov for frequent discussions
on this topic.

References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-

matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion.

J. Berant, I. Dagan, and J. Goldberger. 2010. Global
learning of focused entailment graphs. In Proc. of
ACL.

J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of EMNLP.

S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.

D. Burkett, J. Blitzer, and D. Klein. 2010. Joint parsing
and alignment with weakly synchronized grammars.
In Proc. of NAACL.

R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41–75.

M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
Proc. of ACL.

M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In Proc. of ICML.

M. Collins, P. Koehn, and I. Kučerová. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL.

Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.

M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.

M.C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC, Genoa,
Italy.

J.R. Finkel and C.D. Manning. 2009. Joint parsing and
named entity recognition. In Proc. of NAACL.

K. Ganchev, J. Graça, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.

D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.

K. Hall. 2007. k-best spanning tree parsing. In Proc. of
ACL, June.

J. Judge, A. Cahill, and J. Van Genabith. 2006. Question-
bank: Creating a corpus of parse-annotated questions.
In Proc. of ACL, pages 497–504.

J. Katz-Brown, S. Petrov, R. McDonald, D. Talbot,
F. Och, H. Ichikawa, M. Seno, and H. Kazawa. 2011.
Training a parser for machine translation reordering.
In Proc. of EMNLP.

S. Kübler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.

P. Liang, A. Bouchard-Ct, D. Klein, and B. Taskar. 2006.
An end-to-end discriminative approach to machine
translation. In Proc. of COLING/ACL.

G.S. Mann and A. McCallum. 2010. Generalized Ex-
pectation Criteria for Semi-Supervised Learning with
Weakly Labeled Data. The Journal of Machine Learn-
ing Research, 11:955–984.

M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19:313–330.

D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL.

R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CoNLL.

1498



R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.

T. Nakagawa, K. Inui, and S. Kurohashi. 2010. De-
pendency tree-based sentiment classification using crfs
with hidden variables. In Proc. of NAACL.

J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513–553.

S. Petrov, P.C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In Proc. of EMNLP, pages 705–713.

D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalu-
ation framework for machine translation reordering.
In Proc. of the Sixth Workshop on Statistical Machine
Translation.

M. Wang, N.A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proc. of EMNLP-CoNLL.

P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Us-
ing a dependency parser to improve SMT for Subject-
Object-Verb languages. In Proc. of NAACL.

A. Yates and O. Etzioni. 2009. Unsupervised meth-
ods for determining object and relation synonyms on
the web. Journal of Artificial Intelligence Research,
34(1):255–296.

Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP, pages 562–571.

1499


