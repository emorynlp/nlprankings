










































Acoustic transformations to improve the intelligibility of dysarthric speech


Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 11–21,
Edinburgh, Scotland, UK, July 30, 2011. c©2011 Association for Computational Linguistics

Acoustic transformations to improve the intelligibility of dysarthric speech

Frank Rudzicz
University of Toronto, Department of Computer Science

6 King’s College Road
Toronto, Ontario, Canada

frank@cs.toronto.edu

Abstract

This paper describes modifications to acous-
tic speech signals produced by speakers with
dysarthria in order to make those utter-
ances more intelligible to typical listeners.
These modifications include the correction of
tempo, the adjustment of formant frequencies
in sonorants, the removal of aberrant voic-
ing, the deletion of phoneme insertion errors,
and the replacement of erroneously dropped
phonemes. Through simple evaluations of in-
telligibility with naı̈ve listeners, we show that
the correction of phoneme errors results in the
greatest increase in intelligibility and is there-
fore a desirable mechanism for the eventual
creation of augmentative application software
for individuals with dysarthria.

1 Introduction

Dysarthria is a set of neuromotor disorders that im-
pair the physical production of speech. These im-
pairments reduce the normal control of the primary
vocal articulators but do not affect the regular com-
prehension or production of meaningful, syntacti-
cally correct language. For example, damage to the
recurrent laryngeal nerve reduces control of vocal
fold vibration (i.e., phonation), which can result in
aberrant voicing. Inadequate control of soft palate
movement caused by disruption of the vagus cra-
nial nerve may lead to a disproportionate amount of
air being released through the nose during speech
(i.e., hypernasality). The lack of articulatory control
also leads to various involuntary non-speech sounds
including velopharyngeal or glottal noise (Rosen

and Yampolsky, 2000). More commonly, a lack
of tongue and lip dexterity often produces heavily
slurred speech and a more diffuse and less differen-
tiable vowel target space (Kent and Rosen, 2004).

The neurological damage that causes dysarthria
usually affects other physical activity as well which
can have a drastically adverse affect on mobility
and computer interaction. For instance, severely
dysarthric speakers are 150 to 300 times slower than
typical users in keyboard interaction (Hosom et al.,
2003; Hux et al., 2000). However, since dysarthric
speech is often only 10 to 17 times slower than that
of typical speakers (Patel, 1998), speech is a viable
input modality for computer-assisted interaction.

Consider a dysarthric individual who must travel
into a city by public transportation. This might in-
volve purchasing tickets, asking for directions, or in-
dicating intentions to fellow passengers, all within
a noisy and crowded environment. A personal
portable communication device in this scenario (ei-
ther hand-held or attached to a wheelchair) would
transform relatively unintelligible speech spoken
into a microphone to make it more intelligible before
being played over a set of speakers. Such a system
could facilitate interaction and overcome difficult or
failed attempts at communication in daily life.

We propose a system that avoids drawbacks of
other voice-output communication aids that output
only synthetic speech. Before software for such a
device is designed, our goal is to establish and evalu-
ate a set of modifications to dysarthric speech to pro-
duce a more intelligible equivalent. Understanding
the utility of each of these techniques will be crucial
to effectively designing the proposed system.

11



2 Background and related work

Hawley et al. (2007) described an experiment in
which 8 dysarthric individuals (with either cere-
bral palsy or multiple sclerosis) controlled non-
critical devices in their home (e.g., TV) with auto-
matic speech recognition. Command vocabularies
consisted of very simple phrases (e.g., “TV chan-
nel up”, “Radio volume down”) and feedback was
provided to the user either by visual displays or
by auditory cues. This speech-based environmen-
tal control was compared with a ‘scanning’ inter-
face in which a button is physically pressed to it-
eratively cycle through a list of alternative com-
mands, words, or phrases. While the speech inter-
face made more errors (between 90.8% and 100%
accuracy after training) than the scanning inter-
face (100% accuracy), the former was significantly
faster (7.7s vs 16.9s, on average). Participants com-
mented that speech was significantly less tiring than
the scanning interface, and just as subjectively ap-
pealing (Hawley et al., 2007). Similar results were
obtained in other comparisons of speech and scan-
ning interfaces (Havstam, Buchholz, and Hartelius,
2003), and command-and-control systems (Green et
al., 2003). Speech is a desirable method of expres-
sion for individuals with dysarthria. There are many
augmentative communication devices that employ
synthetic text-to-speech in which messages can be
written on a specialized keyboard or played back
from a repository of pre-recorded phrases (Messina
and Messina, 2007). This basic system architec-
ture can be modified to allow for the replacement
of textual input with spoken input. However, such
a scenario would involve some degree of automatic
speech recognition, which is still susceptible to fault
despite recent advances (Rudzicz, 2011). Moreover,
the type of synthetic speech output produced by such
systems often lacks a sufficient degree of individual
affectation or natural expression that one might ex-
pect in typical human speech (Kain et al., 2007). The
use of prosody to convey personal information such
as one’s emotional state is generally not supported
by such systems but is nevertheless a key part of a
general communicative ability.

Transforming one’s speech in a way that pre-
serves the natural prosody will similarly also pre-
serve extra-linguistic information such as emotions,

and is therefore a pertinent response to the limita-
tions of current technology. Kain et al. (2007) pro-
posed the voice transformation system shown in fig-
ure 1 which produced output speech by concatenat-
ing together original unvoiced segments with syn-
thesized voiced segments that consisted of a super-
position of the original high-bandwidth signal with
synthesized low-bandwidth formants. These synthe-
sized formants were produced by modifications to
input energy, pitch generation, and formant mod-
ifications. Modifications to energy and formants
were performed by Gaussian mixture mapping, as
described below, in which learned relationships be-
tween dysarthric and target acoustics were used to
produce output closer to the target space. This pro-
cess was intended to be automated, but Kain et al.
(2007) performed extensive hand-tuning and manu-
ally identified formants in the input. This will obvi-
ously be impossible in a real-time system, but these
processes can to some extent be automated. For ex-
ample, voicing boundaries can be identified by the
weighted combination of various acoustic features
(e.g., energy, zero-crossing rate) (Kida and Kawa-
hara, 2005; Hess, 2008), and formants can be iden-
tified by the Burg algorithm (Press et al., 1992) or
through simple linear predictive analysis with con-
tinuity constraints on the identified resonances be-
tween adjacent frames (O’Shaughnessy, 2008).

Spectral modifications traditionally involve fil-
tering or amplification methods such as spectral
subtraction or harmonic filtering (O’Shaughnessy,
2000), but these are not useful for dealing with more
serious mispronounciations (e.g., /t/ for /n/). Ho-
som et al. (2003) showed that Gaussian mixture
mapping can be used to transform audio from one
set of spectral acoustic features to another. During
analysis, context-independent frames of speech are
analyzed for bark-scaled energy and their 24th order
cepstral coefficients.

For synthesis, a cepstral analysis approximates
the original spectrum, and a high-order linear pre-
dictive filter is applied to each frame, and excited
by impulses or white noise (for voiced and unvoiced
segments). Hosom et al. (2003) showed that given
99% human accuracy in recognizing normal speech
data, this method of reconstruction gave 93% accu-
racy on the same data. They then trained a transfor-
mative model between dysarthric and regular speech

12



Audio recordings

Voicing
detector

2-band
filter bank

Energy
analysis

Formant
analysis

Energy
modification

F0
generation

Formant
modification

Formant
synthesis+

Overlapp-add

Input speech

voiced

unvoiced

highpass
lowpass

energy

energy'

CV boundaries

formants

formants'F0''

voiced'

Figure 1: Voice transformation system proposed by Kain
et al. (2007).

using aligned, phoneme-annotated, and orthograph-
ically identical sentences spoken by dysarthric and
regular speakers, and a Gaussian Mixture Model
(GMM) to model the probability distribution of the
dysarthric source spectral features x as the sum of
D normal distributions with mean vector µ, diago-
nal covariance matrix Σ, and prior probability α:

p(x) =

D∑
d=1

αdN (x;µd,Σd) . (1)

The GMM parameters were trained in an unsuper-
vised mode using the expectation-maximization al-
gorithm and 1, 2, 4, 8, and 16 mixture components,
with D = 4 apparently being optimal. A probabilis-
tic least-squares regression mapped the source fea-
tures x onto the target (regular speaker) features y,
producing the model Wd(x) + bd for each class, and
a simple spectral distortion is performed to produce
regularized versions of dysarthic speech ŷ:

ŷ(x) =
D∑
d=1

hd(x) (Wd(x) + bd) (2)

for posterior probabilities hd(x). This model is in-
teresting in that it explicitly maps the acoustic differ-
ences for different features between disordered and
regular speech1. Reconstructing the dysarthric spec-
trum in this way to sound more ‘typical’ while leav-
ing pitch (F0), timing, and energy characteristics in-
tact resulted in a 59.4% relative error rate reduction
(68% to 87% accuracy) among a group of 18 naive
human listeners each of whom annotated a total of
206 dysarthric test words (Hosom et al., 2003).

3 The TORGOMorph transformations

TORGOMorph encapsulates of a number of trans-
formations of the acoustics uttered by speakers with
dysarthria. Each modification is implemented in re-
action to a particular effect of dysarthria on intelligi-
bility as determined by observations on the TORGO
database of dysarthric speech (Rudzicz, Namasi-
vayam, and Wolff, 2011). Currently, these modifica-
tions are uniformly preceded by noise reduction us-
ing spectral subtraction and either phonological or
phonemic annotations. This latter step is currently
necessary, since certain modifications require either
knowledge of the manner of articulation or the iden-
tities of the vowel segments, as explained below.
The purpose of this exercise is to determine which
modifications result in the most significant improve-
ments to intelligibility, so the correct annotation se-
quence is vital to avoid the introduction of an ad-
ditional dimension of error. Therefore, the annota-
tions used below are extracted directly from the pro-
fessional markup in the TORGO database. In prac-
tice, however, phonemic annotations determined au-
tomatically by speech recognition would be imper-
fect, which is why investigations of this type often
forgo that automation altogether (e.g., see Kain et
al. (2007)). Possible alternatives to full ASR are dis-
cussed in section 5.

In some cases, the dysarthric speech must be com-
pared or supplemented with another vocal source.
Here, we synthesize segments of speech using a
text-to-speech application developed by Black and
Lenzo (2004). This system is based on the Uni-
versity of Edinburgh’s Festival tool and synthesizes
phonemes using a standard method based on lin-

1This model can also be used to measure the difference be-
tween any two types of speech.

13



ear predictive coding with a pronunciation lexicon
and part-of-speech tagger that assists in the selection
of intonation parameters (Taylor, Black, and Caley,
1998). This system is invoked by providing the ex-
pected text uttered by the dysarthic speaker. In or-
der to properly combine this purely synthetic sig-
nal and the original waveforms we require identical
sampling rates, so we resample the former by a ra-
tional factor using a polyphase filter with low-pass
filtering to avoid aliasing (Hayes, 1999). Since the
discrete phoneme sequences themselves can differ,
we find an ideal alignment between the two by the
Levenshtein algorithm (Levenshtein, 1966), which
provides the total number of insertion, deletion, and
substitution errors.

The following sections detail the components of
TORGOMorph, which is outlined in figure 2. These
components allow for a cascade of one transfor-
mation followed by another, although we can also
perform these steps independently to isolate their
effects. In all cases, the spectrogram is derived
with the fast Fourier transform given 2048 bins on
the range of 0–5 kHz. Voicing boundaries are ex-
tracted in a unidimensional vector aligned with the
spectrogram using the method of Kida and Kawa-
hara (2005) which uses GMMs trained with zero-
crossing rate, amplitude, and the spectrum as in-
put parameters. A pitch (F0) contour is also ex-
tracted from the source by the method proposed by
Kawahara et al. (2005), which uses a Viterbi-like po-
tential decoding of F0 traces described by cepstral
and temporal features. That work showed an error
rate of less than 0.14% in estimating F0 contours as
compared with simultaneously-recorded electroglot-
tograph data. These contours are not in general mod-
ified by the methods proposed below, since Kain et
al. (2007) showed that using original F0 results in
the highest intelligibility among alternative systems.
Over a few segments, however, these contours can
sometimes be decimated in time during the modi-
fication proposed in section 3.3 and in some cases
removed entirely (along with all other acoustics) in
the modification proposed in section 3.2.

3.1 High-pass filter on unvoiced consonants

The first acoustic modification is based on the ob-
servation that unvoiced consonants are improperly
voiced in up to 18.7% of plosives (e.g. /d/ for /t/)

Input acoustics 

Transformed acoustics 

Spectral subtraction 

High-pass filtering  
of voiceless consonants 

(section 3.1) 

Splicing: correcting 
pronunciation errors 

(section 3.2) 

Morphing in time 
(section 3.3) 

Morphing in frequency 
(section 3.4) 

Figure 2: Outline of the TORGOMorph system. The
black path indicates the cascade to be used in practice.
Solid arrows indicate paths taken during evaluation.

and up to 8.5% of fricatives (e.g. /v/ for /f/) in
dysarthric speech in the TORGO database. Voiced
consonants are typically differentiated from their un-
voiced counterparts by the presence of the voice bar,
which is a concentration of energy below 150 Hz
indicative of vocal fold vibration that often persists
throughout the consonant or during the closure be-
fore a plosive (Stevens, 1998). Empirical analysis
of TORGO data suggests that for at least two male
dysarthric speakers this voice bar extends consider-
ably higher, up to 250 Hz.

In order to correct these mispronunciations, the
voice bar is filtered out of all acoustic sub-sequences
annotated as unvoiced consonants. For this task we
use a high-pass Butterworth filter, which is “maxi-
mally flat” in the passband2 and monotonic in mag-
nitude in the frequency domain (Butterworth, 1930).
Here, this filter is computed on a normalized fre-
quency range respecting the Nyquist frequency, so
that if a waveform’s sampling rate is 16 kHz, the
normalized cutoff frequency for this component is
f∗Norm = 250/(1.6× 104/2) = 3.125× 10−2. The
Butterworth filter is an all-pole transfer function be-
tween signals, and we use the 10th-order low-pass

2The passband is the frequency range in which the compo-
nent magnitudes in the original signal should not be changed.

14



Butterworth filter whose magnitude response is

|B(z; 10)|2 = |H(z; 10)|2 = 1
1 +

(
jz/jz∗Norm

)2×10
(3)

where z is the complex frequency in polar coordi-
nates and z∗Norm is the cutoff frequency in that do-
main (Hayes, 1999). This allows the transfer func-
tion

B(z; 10) = H(z; 10) = 1
1 + z10 +

∑10
i=1 ciz

10−i

(4)
whose poles occur at known symmetric intervals
around the unit complex-domain circle (Butter-
worth, 1930). These poles are then transformed
by the Matlab function zp2ss, which produces the
state-space coefficients αi and βi that describe the
output signal resulting from applying the low-pass
Butterworth filter to the discrete signal x[n]. These
coefficients are further converted by

~a = z∗Norm~α
−1

~b = −z∗Norm
(
~α−1~β

) (5)
giving the high-pass Butterworth filter with the same
cutoff frequency of z∗Norm. This continuous system
is converted to the discrete equivalent through the
impulse-invariant discretization method and is im-
plemented by the difference equation

y[n] =
10∑
k=1

aky[n− k] +
10∑
k=0

bkx[n− k]. (6)

As previously mentioned, this equation is applied to
each acoustic sub-sequence annotated as unvoiced
consonants, thereby smoothly removing the energy
below 250 Hz.

3.2 Splicing: correcting dropped and inserted
phoneme errors

The Levenshtein algorithm finds a best possible
alignment of the phoneme sequence in actually ut-
tered speech and the expected phoneme sequence,
given the known word sequence. Isolating phoneme
insertions and deletions are therefore a simple matter
of iteratively adjusting the source speech according
to that alignment. There are two cases where action
is required:

insertion error In this case a phoneme is present
where it ought not be. In the TORGO database,
these insertion errors tend to be repetitions of
phonemes occurring in the first syllable of a
word, according to the International Speech
Lexicon Dictionary (Hasegawa-Johnson and
Fleck, 2007). When an insertion error is iden-
tified the entire associated segment of the sig-
nal is simply removed. In the case that the as-
sociated segment is not surrounded by silence,
adjacent phonemes can be merged together
with time-domain pitch-synchronous overlap-
add (Moulines and Charpentier, 1990).

deletion error The vast majority of accidentally
deleted phonemes in the TORGO database are
fricatives, affricates, and plosives. Often, these
involve not properly pluralizing nouns (e.g.,
book instead of books). Given their high pre-
ponderance of error, these phonemes are the
only ones we insert into the dysarthric source
speech. Specifically, when the deletion of a
phoneme is recognized with the Levenshtein
algorithm, we simply extract the associated
segment from the aligned synthesized speech
and insert it into the appropriate spot in the
dysarthric speech. For all unvoiced fricatives,
affricates, and plosives no further action is re-
quired. When these phonemes are voiced, how-
ever, we first extract and remove the F0 curve
from the synthetic speech, linearly interpolate
the F0 curve from adjacent phonemes in the
source dysarthric speech, and resynthesize with
the synthetic spectrum and interpolated F0. If
interpolation is not possible (e.g., the synthetic
voiced phoneme is to be inserted beside an un-
voiced phoneme), we simply generate a flat F0
equal to the nearest natural F0 curve.

3.3 Morphing in time

Figure 3 exemplifies that vowels uttered by
dysarthric speakers are significantly slower than
those uttered by typical speakers. In fact, sonorants
can be twice as long in dysarthric speech, on aver-
age (Rudzicz, Namasivayam, and Wolff, 2011). In
this modification, phoneme sequences identified as
sonorant are simply contracted in time in order to be
equal in extent to the greater of half their original

15



(a) (b)

Figure 3: Repetitions of /iy p ah/ over 1.5s by (a) a male
speaker with athetoid CP, and (b) a female control in the
TORGO database. Dysarthric speech is notably slower
and more strained than regular speech.

length or the equivalent synthetic phoneme’s length.
In all cases this involved shortening the dysarthric
source sonorant.

Since we wish to contract the length of a signal
segment here without affecting its pitch or frequency
characteristics, we use a phase vocoder based on
digital short-time Fourier analysis (Portnoff, 1976).
Here, Hamming-windowed segments of the source
phoneme are analyzed with a z-transform giving
both frequency and phase estimates for up to 2048
frequency bands. During pitch-preserving time-
scaled warping, we specify the magnitude spectrum
directly from the input magnitude spectrum with
phase values chosen to ensure continuity (Sethares,
2007). Specifically, for the frequency band at fre-
quency F and frames j and k > j in the modified
spectrogram, the phase θ is predicted by

θ
(F )
k = θ

(F )
j + 2πF (j − k). (7)

In our case the discrete warping of the spectrogram
involves simple decimation by a constant factor. The
spectrogram is then converted into a time-domain
signal modified in tempo but not in pitch relative
to the original phoneme segment. This conversion
is accomplished simply through the inverse Fourier
transform.

3.4 Morphing in frequency

Formant trajectories inform the listener as to the
identities of vowels, but the vowel space of
dysarthric speakers tends to be constrained (Kain
et al., 2007). In order to improve a listener’s abil-
ity to differentiate between the vowels, this modifi-
cation component identifies formant trajectories in
the acoustics and modifies these according to the
known vowel identity of a segment. Here, formants
are identified with a 14th-order linear-predictive

coder with continuity constraints on the identi-
fied resonances between adjacent frames (Snell and
Milinazzo, 1993; O’Shaughnessy, 2008). Band-
widths are determined by the negative natural log-
arithm of the pole magnitude, as implemented in
the STRAIGHT analysis system (Banno et al., 2007;
Kawahara, 2006).

For each identified vowel in the dysarthric
speech3, formant candidates are identified at each
frame in time up to 5 kHz. Only those time frames
having at least 3 such candidates within 250 Hz of
expected values are considered. The expected values
of formants are derived from analyses performed by
Allen et al. (1987). Given these subsets of candidate
time frames in the vowel, the one having the highest
spectral energy within the middle 50% of the length
of the vowel is established as the anchor position,
and the three formant candidates within the expected
ranges are established as the anchor frequencies for
formants F1 to F3. If more than one formant can-
didate falls within expected ranges, the one with the
lowest bandwidth becomes the anchor frequency.

Given identified anchor points and target
sonorant-specific frequencies and bandwidths,
there are several methods to modify the spectrum.
The most common may be to learn a statistical
conversion function based on Gaussian mixture
mapping, as described earlier, typically preceded by
alignment of sequences using dynamic time warping
(Stylianou, 2008). Here, we use the STRAIGHT
morphing implemented by Kawahara and Matsui
(2003), among others. The transformation of a
frame of speech xA for speaker A is performed with
a multivariate frequency-transformation function
TAβ given known targets β using

TAβ(xA) =

∫ xA
0

exp

(
log

(
δTAβ(λ)

δλ

))
δλ

=

∫ xA
0

exp

(
(1− r) log

(
δTAA(λ)

δλ

)
+ r log

(
δTAβ(λ)

δλ

))
δλ

=

∫ xA
0

(
δTAβ(λ)

δλ

)r
δλ,

(8)

3Accidentally inserted vowels are also included here, unless
previously removed by the splicing technique in section 3.2.

16



time (ms)

fr
eq

u
en

cy
 (

H
z)

1300 1400 1500 1600 1700 1800 1900 2000
0

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

(a)

time (ms)

fr
eq

u
en

cy
 (

H
z)

1300 1400 1500 1600 1700 1800 1900 2000
0

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

(b)

Figure 4: Spectrograms for (a) the dysarthric original and
(b) the frequency-modified renditions of the word fear.
Circles represent indicative formant locations.

where λ is the frame-based time dimension and
where 0 ≤ r ≤ 1 is an interpolative rate at which
to perform morphing (i.e., r = 1 implies complete
conversion of the parameters of speakerA to param-
eter set β and r = 0 implies no conversion.) (Kawa-
hara et al., 2009). An example of the results of this
morphing technique is shown in figure 4 in which
the three identified formants are shifted to their ex-
pected frequencies.

This method tracks formants and warps the fre-
quency space automatically, whereas Kain et al.
(2007) perform these functions manually. A future
implementation may use Kalman filters to reduce the
noise inherent in trajectory tracking. Such an ap-
proach has shown significant improvements in for-
mant tracking, especially for F1 (Yan et al., 2007).

4 Intelligibility experiments with
TORGOMorph

The intelligibility of both purely synthetic and mod-
ified speech signals can be measured objectively by
simply having a set of participants transcribe what
they hear from a selection of word, phrase, or sen-
tence prompts (Spiegel et al., 1990), although no sin-
gle standard has emerged as pre-eminent (Schroeter,
2008). Hustad (2006) suggested that orthographic
transcriptions provide a more accurate predictor of
intelligibility among dysarthric speakers than the
more subjective estimates used in clinical settings,
e.g., Enderby (1983). That study had 80 listeners
who transcribed audio (which is an atypically large
group for this task) and showed that intelligibility
increased from 61.9% given only acoustic stimuli to
66.75% given audiovisual stimuli on the transcrip-

tion task in normal speech. In the current work, we
modify only the acoustics of dysarthric speech; how-
ever future work might consider how to prompt lis-
teners in a more multimodal context.

In order to gauge the intelligibility of our mod-
ifications, we designed a simple experiment in
which human listeners attempt to identify words in
sentence-level utterances under a number of acoustic
scenarios. Sentences are either uttered by a speaker
with dysarthria, modified from their original source
acoustics, or manufactured by a text-to-speech syn-
thesizer. Each participant is seated at a personal
computer with a simple graphical user interface with
a button which plays or replays the audio (up to 5
times), a text box in which to write responses, and
a second button to submit those responses. Audio is
played over a pair of headphones. The participants
are told to only transcribe the words with which they
are reasonably confident and to ignore those that
they cannot discern. They are also informed that
the sentences are grammatically correct but not nec-
essarily semantically coherent, and that there is no
profanity. Each participant listens to 20 sentences
selected at random with the constraints that at least
two utterances are taken from each category of au-
dio, described below, and that at least five utter-
ances are also provided to another listener, in order
to evaluate inter-annotator agreement. Participants
are self-selected to have no extensive prior experi-
ence in speaking with individuals with dysarthria,
in order to reflect the general population. Although
dysarthric utterances are likely to be contextualized
within meaningful conversations in real-world situ-
ations, such pragmatic aspects of discourse are not
considered here in order to concentrate on acoustic
effects alone. No cues as to the topic or semantic
context of the sentences are given, as there is no
evidence that such aids to comprehension affect in-
telligibility (Hustad and Beukelman, 2002). In this
study we use sentence-level utterances uttered by
male speakers from the TORGO database.

Baseline performance is measured on the original
dysarthric speech. Two other systems are used for
reference:

Synthetic Word sequences are produced by the
Cepstral commercial text-to-speech system us-
ing the U.S. English voice ‘David’. This sys-

17



tem is based on Festival in almost every respect,
including its use of linguistic pre-processing
(e.g., part-of-speech tagging) and rule-based
generation (Taylor, Black, and Caley, 1998).
This approach has the advantage that every as-
pect of the synthesized speech (e.g., the word
sequence) can be controlled although here, as
in practice, synthesized speech will not mimic
the user’s own acoustic patterns, and will of-
ten sound more ‘mechanical’ due to artificial
prosody (Black and Lenzo, 2007).

GMM This system uses the Gaussian mixture map-
ping type of modification suggested by Toda,
Black, and Tokuda (2005) and Kain et al.
(2007). Here, we use the FestVox implementa-
tion of this algorithm, which includes pitch ex-
traction, some phonological knowledge (Toth
and Black, 2005), and a method for resynthe-
sis. Parameters for this model are trained by the
FestVox system using a standard expectation-
maximization approach with 24th-order cep-
stral coefficients and 4 Gaussian components.
The training set consists of all vowels uttered
by a male speaker in the TORGO database
and their synthetic realizations produced by the
method above.

Performance is evaluated on the three other acous-
tic transformations, namely those described in sec-
tions 3.2, 3.3, and 3.4 above. Tables 1 and 2 respec-
tively show the percentage of words and phonemes
correctly identified by each listener relative to the
expected word sequence under each acoustic con-
dition. In each case, annotator transcriptions were
aligned with the ‘true’ or expected sequences us-
ing the Levenshtein algorithm described in section
3. Plural forms of singular words, for example,
are considered incorrect in word alignment although
one obvious spelling mistake (i.e., ‘skilfully’) is cor-
rected before evaluation. Words are split into com-
ponent phonemes according to the CMU dictionary,
with words having multiple pronunciations given the
first decomposition therein.

In these experiments there is not enough data from
which to make definitive claims of statistical signifi-
cance, but it is clear that the purely synthetic speech
has a far greater intelligibility than other approaches,
more than doubling the average accuracy of the

Orig. GMM Synth. Splice Time Freq.
L01 22.1 15.6 82.0 40.2 34.7 35.2
L02 27.8 12.2 75.5 44.9 39.4 33.8
L03 38.3 14.8 76.3 37.5 12.9 21.4
L04 24.7 10.8 72.1 32.6 22.2 18.4
Avg. 28.2 13.6 76.5 38.8 27.3 27.2

Table 1: Percentage of words correctly identified by each
listener (L0*) relative to the expected sequence. Sections
3.2, 3.3, and 3.4 discuss the ‘Splice’, ‘Time’, and ‘Freq.’
techniques, respectively.

Orig. GMM Synth. Splice Time Freq.
L01 52.0 43.1 98.2 64.7 47.8 55.1
L02 57.8 38.2 92.9 68.9 50.6 53.3
L03 50.1 41.4 96.8 57.1 30.7 46.7
L04 51.6 33.8 88.7 51.9 43.2 45.0
Avg. 52.9 39.1 94.2 60.7 43.1 50.0

Table 2: Percentage of phonemes correctly identified by
each listener relative to the expected sequence. Sections
3.2, 3.3, and 3.4 discuss the ‘Splice’, ‘Time’, and ‘Freq.’
techniques, respectively.

TORGOMorph modifications. The GMM transfor-
mation method proposed by Kain et al. (2007) gave
poor performance, although our experiments are dis-
tinguished from theirs in that our formant traces are
detected automatically, rather than by hand. The rel-
ative success of the synthetic approach is not an ar-
gument against the type of modifications proposed
here and by Kain et al. (2007), since our aim is to
avoid the use of impersonal and invariant utterances.
Indeed, future study in this area should incorporate
subjective measures of ‘naturalness’. Further uses
of acoustic modifications not attainable by text-to-
speech synthesis are discussed in section 5.

In all cases, the splicing technique of removing
accidentally inserted phonemes and inserting miss-
ing ones gives the highest intelligibility relative to
all acoustic transformation methods. Although more
study is required, this result emphasizes the impor-
tance of lexically correct phoneme sequences. In the
word-recognition experiment, there are an average
of 5.2 substitution errors per sentence in the unmod-
ified dysarthric speech against 2.75 in the synthetic
speech. There are also 2.6 substitution errors on av-
erage per sentence for the speech modified in fre-
quency, but 3.1 deletion errors, on average, against
0.24 in synthetic speech. No correlation is found be-

18



tween the ‘loudness’ of the speech (determined by
the overall energy in the sonorants) and intelligibil-
ity results, although this might change with the ac-
quisition of more data. Neel (2009), for instance,
found that loud or amplified speech from individu-
als with Parkinson’s disease was more intelligible to
human listeners than quieter speech.

Our results are comparable in many respects to the
experiments of Kain et al. (2007), although they only
looked at simple consonant-vowel-consonant stim-
uli. Their results showed an average of 92% correct
synthetic vowel recognition (compared with 94.2%
phoneme recognition in table 2) and 48% correct
dysarthric vowel recognition (compared with 52.9%
in table 2). Our results, however, show that modi-
fied timing and modified frequencies do not actually
benefit intelligibility in either the word or phoneme
cases. This disparity may in part be due to the fact
that our stimuli are much more complex (quicker
sentences do not necessarily improve intelligibility).

5 Discussion

This work represents an inaugural step towards
speech modification systems for human-human and
human-computer interaction. Tolba and Torgoman
(2009) claimed that significant improvements in au-
tomatic recognition of dysarthric speech are attain-
able by modifying formants F1 and F2 to be more
similar to expected values. In that study, formants
were identified using standard linear predictive cod-
ing techniques, although no information was pro-
vided as to how these formants were modified nor
how their targets were determined. However, they
claimed that modified dysarthric speech resulted
in ‘recognition rates’ (by which they presumably
meant word-accuracy) of 71.4% in the HTK speech
recognition system, as compared with 28% on the
unmodified dysarthric speech from 7 individuals.
The results in section 4 show that human listen-
ers are more likely to correctly identify utterances
in which phoneme insertion and deletion errors are
corrected than those in which formant frequencies
are adjusted. Therefore, one might hypothesize
that such pre-processing might provide even greater
gains than those reported by Tolba and Torgoman
(2009). Ongoing work ought to confirm or deny this
hypothesis.

A prototypical client-based application based on
our research for unrestricted speech transformation
of novel sentences is currently in development. Such
work will involve improving factors such as accu-
racy and accessibility for individuals whose neuro-
motor disabilities limit the use of modern speech
recognition, and for whom alternative interaction
modalities are insufficient. This application is being
developed under the assumption that it will be used
in a mobile device embeddable within a wheelchair.
If word-prediction is to be incorporated, the pre-
dicted continuations of uttered sentence fragments
can be synthesized without requiring acoustic input.

In practice, the modifications presented here will
have to be based on automatically-generated anno-
tations of the source audio. This is especially im-
portant to the ‘splicing’ module in which word-
identification is crucial. There are a number of tech-
niques that can be exercised in this area. Czyzewski,
Kaczmarek, and Kostek (2003) apply both a vari-
ety of neural networks and rough sets to the task
of classifying segments of speech according to the
presence of stop-gaps, vowel prolongations, and in-
correct syllable repetitions. In each case, input in-
cludes source waveforms and detected formant fre-
quencies. They found that stop-gaps and vowel pro-
longations could be detected with up to 97.2% ac-
curacy and that vowel repetitions could be detected
with up to 90% accuracy using the rough set method.
Accuracy was similar although slightly lower us-
ing traditional neural networks (Czyzewski, Kacz-
marek, and Kostek, 2003). These results appear
generally invariant even under frequency modifica-
tions to the source speech. Arbisi-Kelm (2010), for
example, suggest that disfluent repetitions can be
identified reliably through the use of pitch, dura-
tion, and pause detection (with precision up to 93%
(Nakatani, 1993)). If more traditional models of
speech recognition are to be deployed to identify
vowels, the probabilities that they generate across
hypothesized words might be used to weight the
manner in which acoustic transformations are made.

The use of one’s own voice to communicate is a
desirable goal, and continuations of this research are
therefore focused on the practical aspects of this re-
search towards usable and portable systems.

19



References
Allen, Jonathan, M. Sharon Hunnicutt, Dennis H. Klatt,

Robert C. Armstrong, and David B. Pisoni. 1987.
From text to speech: the MITalk system. Cambridge
University Press, New York, NY, USA.

Arbisi-Kelm, Timothy. 2010. Intonation structure and
disfluency detection in stuttering. Laboratory Phonol-
ogy 10, 4:405–432.

Banno, Hideki, Hiroaki Hata, Masanori Morise, Toru
Takahashi, Toshio Irino, and Hideki Kawahara. 2007.
Implementation of realtime STRAIGHT speech ma-
nipulation system: Report on its first implementation.
Acoustical Science and Technology, 28(3):140–146.

Black, Alan W. and Kevin A. Lenzo. 2004. Multilingual
text-to-speech synthesis. In 2004 IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP 2004).

Black, Alan W. and Kevin A. Lenzo. 2007. Building
synthetic voices. http://www.festvox.org/
festvox/bsv.ps.gz.

Butterworth, Stephen. 1930. On the theory of filter am-
plifiers. Experimental Wireless and the Wireless Engi-
neer, 7:536–541.

Czyzewski, Andrzej, Andrzej Kaczmarek, and Bozena
Kostek. 2003. Intelligent processing of stuttered
speech. Journal of Intelligent Information Systems,
21(2):143–171.

Enderby, Pamela M. 1983. Frenchay Dysarthria Assess-
ment. College Hill Press.

Green, Phil, James Carmichael, Athanassios Hatzis, Pam
Enderby, Mark Hawley, and Mark Parker. 2003. Au-
tomatic speech recognition with sparse training data
for dysarthric speakers. In Proceedings of Eurospeech
2003, pages 1189–1192, Geneva.

Hasegawa-Johnson, Mark and Margaret Fleck. 2007. In-
ternational Speech Lexicon Project. http://www.
isle.illinois.edu/dict/.

Havstam, Christina, Margret Buchholz, and Lena
Hartelius. 2003. Speech recognition and dysarthria: a
single subject study of two individuals with profound
impairment of speech and motor control. Logopedics
Phoniatrics Vocology, 28:81–90(10), August.

Hawley, Mark S., Pam Enderby, Phil Green, Stuart
Cunningham, Simon Brownsell, James Carmichael,
Mark Parker, Athanassios Hatzis, Peter O’Neill,
and Rebecca Palmer. 2007. A speech-controlled
environmental control system for people with se-
vere dysarthria. Medical Engineering & Physics,
29(5):586–593, June.

Hayes, Monson H. 1999. Digital Signal Processing.
Schaum’s Outlines. McGraw Hill.

Hess, Wolfgang J. 2008. Pitch and voicing determination
of speech with an extension toward music signal. In

Jacob Benesty, M. Mohan Sondhi, and Yiteng Huang,
editors, Speech Processing. Springer.

Hosom, John-Paul, Alexander B. Kain, Taniya Mishra,
Jan P. H. van Santen, Melanie Fried-Oken, and Jan-
ice Staehely. 2003. Intelligibility of modifications to
dysarthric speech. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP ’03), volume 1, pages 924–927,
April.

Hustad, Katherine C. 2006. Estimating the intelligibil-
ity of speakers with dysarthria. Folia Phoniatrica et
Logopaedica, 58(3):217–228.

Hustad, Katherine C. and David R. Beukelman. 2002.
Listener comprehension of severely dysarthric speech:
Effects of linguistic cues and stimulus cohesion. Jour-
nal of Speech, Language, and Hearing Research,
45:545–558, June.

Hux, Karen, Joan Rankin-Erickson, Nancy Manasse, and
Elizabeth Lauritzen. 2000. Accuracy of three speech
recognition systems: Case study of dysarthric speech.
Augmentative and Alternative Communication (AAC),
16(3):186 –196, January.

Kain, Alexander B., John-Paul Hosom, Xiaochuan Niu,
Jan P.H. van Santen, Melanie Fried-Oken, and Jan-
ice Staehely. 2007. Improving the intelligibil-
ity of dysarthric speech. Speech Communication,
49(9):743–759, September.

Kawahara, H. and H. Matsui. 2003. Auditory mor-
phing based on an elastic perceptual distance metric
in an interference-free time-frequency representation.
In Acoustics, Speech, and Signal Processing, 2003.
Proceedings. (ICASSP ’03). 2003 IEEE International
Conference on, volume 1, pages I–256 – I–259 vol.1,
April.

Kawahara, H., R. Nisimura, T. Irino, M. Morise, T. Taka-
hashi, and H. Banno. 2009. Temporally variable
multi-aspect auditory morphing enabling extrapolation
without objective and perceptual breakdown. In Pro-
ceedings of IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP 2009),
pages 3905–3908, April.

Kawahara, Hideki. 2006. STRAIGHT, exploitation of
the other aspect of VOCODER: Perceptually isomor-
phic decomposition of speech sounds. Acoustical Sci-
ence and Technology, 27(6):349–353.

Kawahara, Hideki, Alain de Cheveigné, Hideki Banno,
Toru Takahashi, and Toshio Irino. 2005. Nearly
Defect-Free F0 Trajectory Extraction for Expressive
Speech Modifications Based on STRAIGHT. In Pro-
ceedings of INTERSPEECH 2005, pages 537–540,
September.

Kent, Ray D. and Kristin Rosen. 2004. Motor con-
trol perspectives on motor speech disorders. In Ben

20



Maassen, Raymond Kent, Herman Peters, Pascal Van
Lieshout, and Wouter Hulstijn, editors, Speech Mo-
tor Control in Normal and Disordered Speech. Oxford
University Press, Oxford, chapter 12, pages 285–311.

Kida, Yusuke and Tatsuya Kawahara. 2005. Voice
activity detection based on optimally weighted com-
bination of multiple features. In Proceedings of
INTERSPEECH-2005, pages 2621–2624.

Levenshtein, Vladimir I. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Cyber-
netics and Control Theory, 10(8):707–710.

Messina, James J. and Constance M. Messina.
2007. Description of AAC devices. http:
//www.coping.org/specialneeds/
assistech/aacdev.htm, April.

Moulines, Eric and Francis Charpentier. 1990. Pitch-
synchronous waveform processing techniques for text-
to-speech synthesis using diphones. Speech Commu-
nication, 9:453–467, December.

Nakatani, Christine. 1993. A speech-first model for re-
pair detection and correction. In Proceedings of the
31st Annual Meeting of the Association for Computa-
tional Linguistics, pages 46–53.

Neel, Amy T. 2009. Effects of loud and amplified speech
on sentence and word intelligibility in parkinson dis-
ease. Journal of Speech, Language, and Hearing Re-
search, 52:1021–1033, August.

O’Shaughnessy, Douglas. 2000. Speech Communica-
tions – Human and Machine. IEEE Press, New York,
NY, USA.

O’Shaughnessy, Douglas. 2008. Formant estimation and
tracking. In Jacob Benesty, M. Mohan Sondhi, and
Yiteng Huang, editors, Speech Processing. Springer.

Patel, Rupal. 1998. Control of prosodic parameters by
an individual with severe dysarthria. Technical report,
University of Toronto, December.

Portnoff, Michael R. 1976. Implementation of the dig-
ital phase vocoder using the fast Fourier transform.
IEEE Transactions on Acoustics, Speech and Signal
Processing, 24(3):243–248.

Press, William H., Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: the art of scientific computing. Cambridge Uni-
versity Press, second edition.

Rosen, Kristin and Sasha Yampolsky. 2000. Automatic
speech recognition and a review of its functioning with
dysarthric speech. Augmentative & Alternative Com-
munication, 16(1):48–60, Jan.

Rudzicz, Frank. 2011. Production knowledge in the
recognition of dysarthric speech. Ph.D. thesis, Uni-
versity of Toronto, Department of Computer Science.

Rudzicz, Frank, Aravind Kumar Namasivayam, and
Talya Wolff. 2011. The TORGO database of acoustic

and articulatory speech from speakers with dysarthria.
Language Resources and Evaluation, (in press).

Schroeter, Juergen. 2008. Basic principles of speech
synthesis. In Jacob Benesty, M. Mohan Sondhi, and
Yiteng Huang, editors, Speech Processing. Springer.

Sethares, William Arthur. 2007. Rhythm and Trans-
forms. Springer.

Snell, Roy C. and Fausto Milinazzo. 1993. Formant Lo-
cation from LPC Analysis Data. IEEE Transactions
on Speech and Audio Processing, 1(2), April.

Spiegel, Murray F., Mary Jo Altom, Marian J. Macchi,
and Karen L. Wallace. 1990. Comprehensive assess-
ment of the telephone intelligibility of synthesized and
natural speech. Speech Communication, 9(4):279 –
291.

Stevens, Kenneth N. 1998. Acoustic Phonetics. MIT
Press, Cambridge, Massachussetts.

Stylianou, Yannis. 2008. Voice transformation. In Ja-
cob Benesty, M. Mohan Sondhi, and Yiteng Huang,
editors, Speech Processing. Springer.

Taylor, Paul, Alan W. Black, and Richard Caley. 1998.
The architecture of the Festival speech synthesis sys-
tem. In Proceedings of the 3rd ESCA Workshop on
Speech Synthesis, pages 147–151, Jenolan Caves, Aus-
tralia.

Toda, Tomoki, Alan W. Black, and Keiichi Tokuda. 2005.
Spectral conversion based on maximum likelihood es-
timation considering global variance of converted pa-
rameter. In Proceedings of the 2005 International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP 2005), Philadelphia, Pennsylvania.

Tolba, Hesham and Ahmed S. El Torgoman. 2009. To-
wards the improvement of automatic recognition of
dysarthric speech. In International Conference on
Computer Science and Information Technology, pages
277–281, Los Alamitos, CA, USA. IEEE Computer
Society.

Toth, Arthur R. and Alan W. Black. 2005. Cross-speaker
articulatory position data for phonetic feature predic-
tion. In Proceedings of Interspeech 2005, Lisbon, Por-
tugal.

Yan, Qin, Saeed Vaseghi, Esfandiar Zavarehei, Ben Mil-
ner, Jonathan Darch, Paul White, and Ioannis An-
drianakis. 2007. Formant tracking linear predic-
tion model usng HMMs and Kalman filters for noisy
speech processing. Computer Speech and Language,
21:543–561.

21


