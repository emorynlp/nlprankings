



















































Accelerated Training of Maximum Margin Markov Models for Sequence Labeling: A Case Study of NP Chunking


Coling 2010: Poster Volume, pages 1408–1416,
Beijing, August 2010

Accelerated Training of Maximum Margin Markov Models for Sequence
Labeling: A Case Study of NP Chunking∗

Xiaofeng YU Wai LAM

Information Systems Laboratory
Department of Systems Engineering & Engineering Management

The Chinese University of Hong Kong
{xfyu,wlam}@se.cuhk.edu.hk

Abstract
We present the first known empirical results
on sequence labeling based on maximum mar-
gin Markov networks (M3N ), which incorpo-
rate both kernel methods to efficiently deal with
high-dimensional feature spaces, and probabilistic
graphical models to capture correlations in struc-
tured data. We provide an efficient algorithm, the
stochastic gradient descent (SGD), to speedup the
training procedure of M3N . Using official dataset
for noun phrase (NP) chunking as a case study,
the resulting optimizer converges to the same qual-
ity of solution over an order of magnitude faster
than the structured sequential minimal optimization
(structured SMO). Our model compares favorably
with current state-of-the-art sequence labeling ap-
proaches. More importantly, our model can be eas-
ily applied to other sequence labeling tasks.

1 Introduction

The problem of annotating or labeling observation
sequences arises in many applications across a va-
riety of scientific disciplines, most prominently in
natural language processing, speech recognition,
information extraction, and bioinformatics. Re-
cently, the predominant formalism for modeling
and predicting label sequences has been based on
discriminative graphical models and variants.

Among such models, maximum margin
Markov networks (M3N ) and variants ( Taskar
et al. (2003); Taskar (2004); Taskar et al. (2005))
have recently gained popularity in the machine
learning community. While the M3N framework
makes extensive use of many theoretical results

∗The work described in this paper is substantially sup-
ported by grants from the Research Grant Council of the
Hong Kong Special Administrative Region, China (Project
No: CUHK4128/07) and the Direct Grant of the Fac-
ulty of Engineering, CUHK (Project Codes: 2050442 and
2050476). This work is also affiliated with the Microsoft-
CUHK Joint Laboratory for Human-centric Computing and
Interface Technologies.

available for Markov networks, it largely dis-
penses with the probabilistic interpretation. M3N
thus combines the advantages of both worlds, the
possibility to have a concise model of the relation-
ships present in the data via log-linear Markov
networks over a set of label variables and the
highly accurate predictions based on maximum
margin estimation of the model parameters.

Traditionally, M3N can be trained using
the structured sequential minimal optimization
(structured SMO), a coordinate descent method
for solving quadratic programming (QP) prob-
lems (Taskar et al., 2003). Clearly, however, the
polynomial number of constraints in the QP prob-
lem associated with the M3N can still be very
large, making the structured SMO algorithm slow
to converge over the training data. This currently
limits the scalability and applicability of M3N to
large-scale real world problems.

Stochastic gradient methods (e.g., Lecun et
al. (1998); Bottou (2004)), on the other hand,
are online and scale sub-linearly with the amount
of training data, making them very attractive for
large-scale datasets. In stochastic (or online) gra-
dient descent (SGD), the true gradient is approx-
imated by the gradient of the cost function only
evaluated on a single training example. The pa-
rameters are then adjusted by an amount propor-
tional to this approximate gradient. Therefore, the
parameters of the model are updated after each
training example. For large-scale datasets, online
gradient descent can be much faster than standard
(or batch) gradient descent.

In this paper, we marry the above two tech-
niques and show how SGD can be used to signif-
icantly accelerate the training of M3N . And we

1408



then apply our model to the well-established se-
quence labeling task: noun phrase (NP) chunking.
Experimental results show the validity and effec-
tiveness of our approach. We now summarize the
primary contributions of this paper as follows:

• We exploit M3N to NP chunking on the
standard evaluation dataset, achieving fa-
vorable performance against recent top-
performing systems. The M3N framework
allows arbitrary features of observation se-
quence, as well as the important benefits of
kernels. To the best of our knowledge, this is
the first known empirical study on NP chunk-
ing using M3N in the NLP community.

• We provide the efficient SGD algorithm to
accelerate the training procedure of M3N ,
and experimental results show that it con-
verges over an order of magnitude faster than
the structured SMO without sacrificing per-
formance.

• Our model is easily extendable to other se-
quence labeling tasks, such as part-of-speech
tagging and named entity recognition. Based
on the promising results on NP chunking,
we believe that our model will significantly
further the applicability of margin-based ap-
proaches to large-scale sequence labeling
tasks.

2 Maximum Margin Markov Networks
for Sequence Labeling

In sequence labeling, the output is a sequence of
labels y = (y1, . . . , yT ) which corresponds to an
observation sequence x = (x1, . . . , xT ). Suppose
each individual label can take values from set Σ,
then the problem can be considered as a multiclass
classification problem with |Σ|T different classes.

InM3N , a pairwise Markov network is defined
as a graph G = (Y,E). Each edge (i, j) ∈ E is
associated with a potential function

ψij(x, yi, yj) = exp(
l∑

k=1

wkφk(x, yi, yj))

= exp(w>φ(x, yi, yj)) (1)

where φ(x, yi, yj) is a pairwise basis function. All
edges in the graph denote the same type of in-
teraction, so that we can define a feature map
φk(x, y) =

∑
(i,j)∈E φk(x, yi, yj). The network

encodes the following conditional probability dis-
tribution (Taskar et al., 2003):

P (y|x) ∝
∏

(i,j)∈E
ψij(x, yi, yj) = exp(w>φ(x, y))

(2)
where φ(x, y) = [φ1φ2 . . . φ|Σ|φtrans]> is
used to learn a weight vector w. φk =∑n

i=1 φi(x)I(yi = k),∀k ∈ {1, 2, . . . , |Σ|} and
φtrans = [c11c12 . . . cTT ]

> where cij is the num-
ber of observed transitions from the ith alphabet
to the jth alphabet in Σ.

Similar to SVMs (Vapnik, 1995), M3N tries to
find a projection to maximize the margin γ. On
the other hand, M3N also attempts to minimize
‖w‖ to minimize the generalization error. Sup-
pose ∆tx(y) =

∑n
i=1∆tx(yi) =

∑n
i=1 I(yi 6=

(t(x))i) where t((x))i is the true label of the ith
sequence xi, and ∆φx(y) = φ(x, t(x)) − φ(x, y)
where t(x) is the true label of the observation se-
quence x. We can get a quadratic program (QP)
using a standard transformation to eliminate γ as
follows:

min
1

2
‖w‖2; (3)

s.t. w>∆φx(y) ≥ ∆tx(y),∀x ∈ S,∀y ∈ Σ.

However, the sequence data is often not separa-
ble by the defined hyperplane. In such cases, we
can introduce slack variables ξx which are guaran-
teed to be non-negative to allow some constraints.
Thus the complete primal form of the optimiza-
tion problem can be formulated by:

min
1

2
‖w‖2 + C

∑

x
ξx; (4)

s.t. w>∆φx(y) ≥ ∆tx(y)− ξx,∀x ∈ S,∀y ∈ Σ.

where C is called the capacity in the support vector
literature and presents a way to trade-off the train-
ing error and margin size. One should note that the
number of constraints is

∑T
i=1 |Σi|, an extremely

large number. And the corresponding dual formu-

1409



lation can be defined as:

max
∑

x,y
αx(y)∆tx(y)−

1

2
‖
∑

x,y
αx(y)∆φx(y)‖2;

s.t.
∑

y
αx(y) = C,∀x;αx(y) ≥ 0,∀x, y. (5)

where αx(y) is a dual variable.
As well as loss functions, kernels might have

substantial influence on the performance of a clas-
sification system. M3N is capable of incorpo-
rating many different kinds of kernel functions to
reduce computations in the high-dimensional fea-
ture space H. This is sometimes referred to as
the “kernel trick” (Schölkopf and Smola, 2002;
Shawe-Taylor and Cristianini, 2004). A linear
kernel can be defined as

κ((x, y), (x′, y′)) = 〈φ(x, y), φ(x′, y′)〉H (6)

For a polynomial kernel,

κ((x, y), (x′, y′))

= (s · 〈φ(x, y), φ(x′, y′)〉H + r)d, (7)

and for a neural kernel,

κ((x, y), (x′, y′))
= tanh(s · 〈φ(x, y), φ(x′, y′)〉H + r), (8)

where s, d, and r are coefficients in kernel func-
tions.

3 Stochastic Gradient Descent

For M3N optimization, Taskar et al. (2003) has
proposed a reparametrization of the dual variables
to take advantage of the network structure of the
labeling sequence problem. The dual QP is then
solved using the structured sequential minimal
optimization (structured SMO) analogous to the
SMO used for SVMs (Platt, 1998). However, the
resulting number of constraints in the QP make
the structured SMO algorithm slow to converge,
or even prohibitively expensive for large-scale real
world problems. In this section we will present
stochastic gradient descent (SGD) method, and
show SGD can significantly speedup the training
of M3N .

3.1 Regularized Loss Minimization
Recall that for M3N , the goal is to find
a linear hypothesis hw such that hw(x) =
argmaxy∈Σ w>φ(x, y). The parameters w are
learned by minimizing a regularized loss

L(w; {(xi, yi)}Ti=1, C) =
m∑

i=1

`(w, xi, yi)+
C

2
‖w‖2.

(9)
The function `measures the loss incurred in us-

ing w to predict the label of xi. Following (Taskar
et al., 2003), `(w, xi, yi) is a variant of the hinge
loss, and can be defined as follows:

`(w, xi, yi) = max
y∈Σ

[e(xi, yi, y)

− w · (φ(xi, yi)− φ(xi, y))], (10)

where e(xi, yi, y) is some non-negative measure
of the error incurred in predicting y instead of yi
as the label of xi. We assume that e(xi, yi, y) = 0
for all i, so that no loss is incurred for correct
prediction, and therefore `(w, xi, yi) is always
non-negative. This loss function corresponds to
the M3N approach, which explicitly penalizes
training examples for which, for some y 6= yi,
w · (φ(xi, yi) − φ(xi, y)) < e(xi, yi, y). And the
function L is convex in w for `(w, xi, yi). There-
fore, minimization of L can be re-cast as opti-
mization of the following dual convex problem:

w∗ = argmin
w

∑

i

max
y∈Σ

[e(xi, yi, y)

− w · (φ(xi, yi)− φ(xi, y))] +
C

2
‖w‖2. (11)

3.2 The SGD Algorithm
To perform parameter estimation, we need to min-
imize L(w; {(xi, yi)}Ti=1, C). For this purpose we
compute its gradient G(w):

G(w) =
∂

∂w
(L(w; {(xi, yi)}Ti=1, C))

=
∂

∂w
(

m∑

i=1

`(w, xi, yi) +
C

2
‖w‖2) (12)

In addition to the gradient, second-order meth-
ods based on Newton steps also require computa-
tion and inversion of the Hessian H(w). Taking

1410



the gradient of Equation 12 wrt. w yields:

H(w) =
∂

∂w
G(w) =

∂2

∂w2
L (13)

Explicitly computing the full Hessian is time
consuming. Instead we can make use of the dif-
ferential

dG(w) = H(w)dw (14)

to efficiently compute the product of the Hessian
with a chosen vector v =: dw by forward-mode
algorithmic differentiation (Pearlmutter, 1994).
These Hessian-vector products can be computed
along with the gradient at only 2-3 times the
cost of the gradient computation alone. We de-
note G(w) = ∇wL, and each iteration of the
SGD algorithm consists in drawing an example
(xi, yi) at random and applying the parameter up-
date rule (Robbins and Monroe, 1951):

wt+1 ← wt − η · ∇wL (15)

where η is the learning rate in the algorithm.
The SGD algorithm has been shown to be fast,

reliable, and less prone to reach bad local minima.
In this algorithm, the weights are updated after
the presentation of each example, according to the
gradient of the loss function (Lecun et al., 1998).
The convergence is very fast when the training ex-
amples are redundant since only a few examples
are needed to perform. This algorithm can get a
good estimation after considerably few iterations.

3.3 Choosing Learning Rate η
The learning rate η is crucial to the speed of
SGD algorithm. Ideally, each parameter weight
wi should have its own learning rate ηi. Because
of possible correlations between input variables,
the learning rate of a unit should be inversely pro-
portional to the square root of the number of in-
puts to the unit. If shared weights are used, the
learning rate of a weight should be inversely pro-
portional to the square root of the number of con-
nection sharing that weight.

For one-dimensional sequence labeling task,
the optimal learning rate yields the fastest conver-
gence in the direction of highest curvature is (Bot-
tou, 2004):

ηopt = (
∂2L
∂w2

)−1 = (H(w))−1, (16)

and the maximum learning rate is ηmax = 2ηopt.
The simple SGD update offers lots of engineer-

ing opportunities. In practice, however, at any mo-
ment during the training procedure, we can select
a small subset of training examples and try vari-
ous learning rates on the subset, then pick the one
that most reduces the cost and use it on the full
dataset.

3.4 The SGD Convergence

The convergence of stochastic algorithms actually
has been studied for a long time in adaptive signal
processing. Given a suitable choice of the learn-
ing rate ηt, the standard (batch) gradient descent
algorithm is known to converge to a local mini-
mum of the cost function. However, the random
noise introduced by SGD disrupts this determinis-
tic picture and the specific study of SGD conver-
gence usually is fairly complex (Benveniste et al.,
1987).

It is reported that for the convex case, if sev-
eral assumptions and conditions are valid, then
the SGD algorithm converges almost surely to the
optimum w∗ 1. For the general case where the
cost function is non-convex and has both local
and global minima, if four assumptions and two
learning rate assumptions hold, it is guaranteed
that the gradient ∇wL converges almost surely to
zero (Bottou, 2004). We omit the details of the
convergence theorem and corresponding proofs
due to space limitation.

3.5 SGD Speedup

Unfortunately, many of sophisticated gradient
methods are not robust to noise, and scale badly
with the number of parameters. The plain SGD
algorithm can be very slow to converge. Inspired
by stochastic meta-descent (SMD) (Schraudolph,
1999), the convergence speed of SGD can be fur-
ther improved with gradient step size adaptation
by using second-order information. SMD is a
highly scalable local optimizer. It shines when
gradients are stochastically approximated.

In SMD, the learning rate η is simultaneously

1One may argue that SGD on many architectures does
not result in a global optima. However, our goal is to obtain
good performance on future examples in learning rather than
achieving a global optima on the training set.

1411



INPUT: training set S {(x1, y1), . . . , (xT, yT)};
factor λ; number of iterations N .

INITIALIZE: w0, v0 = 0, η0.
FOR t = 1, 2, . . . , N

Choose a random example (xi, yi) ∈ S
Compute the gradient ∇t = Gt and Htvt
Set vt+1 = λvt − ηt · (Gt + λHtvt)
Update the parameter vector:
wt+1 ← wt − ηt · ∇t
Adapt the gradient step size:
ηt+1 = ηt ·max(12 , 1− µGt+1 · vt+1)

OUTPUT: wN+1

Figure 1: Pseudo-code for the SGD algorithm.

adapted via a multiplicative update with µ:

ηt+1 = ηt ·max(
1

2
, 1− µGt+1 · vt+1), (17)

where the vector v (v =: dw) captures the long-
term dependencies of parameters. v can be com-
puted by the simple iterative update:

vt+1 = λvt − ηt · (Gt + λHtvt), (18)

where the factor 0 ≤ λ ≤ 1 governs the time scale
over which long-term dependencies are taken into
account, and Htvt can be calculated efficiently
alongside the gradient by forward-mode algorith-
mic differentiation via Equation 14. This Hessian-
vector product is computed implicitly and it is the
key to SMD’s efficiency. The pseudo-code for the
SGD algorithm is shown in Figure 1.

4 Experiments: A Case Study of NP
Chunking

4.1 Data
Our data comes from the CoNLL 2000 shared task
(Sang and Buchholz, 2000). The dataset is di-
vided into a standard training set of 8,936 sen-
tences and a testing set of 2,012 sentences. This
data consists of the same partitions of the Wall
Street Journal corpus (WSJ) as the widely used
data for NP chunking: sections 15-18 as training
data (211,727 tokens) and section 20 as test data
(47,377 tokens). And the annotation of the data
has been derived from the WSJ corpus.

wt−δ = w
wt matches [A-Z]
wt matches [A-Z]+
wt matches [A-Z][a-z]+
wt matches [A-Z]+[a-z]+[A-Z]+[a-z]
wt matches .*[0-9].*
wt contains dash “-” or dash-based “-based”
wt is capitalized, all-caps, single capital letter,
or mixed capitalization
wt contains years, year-spans or fractions
wt is contained in a lexicon of words with POS
p (from the Brill tagger)
pt = p

qk(x, t+ δ) for all k and δ ∈ [−3, 3]

Table 1: Input feature template qk(x, t) for NP
chunking. In this table wt is the token (word) at
position t, pt is the POS tag at position t, w ranges
over all words in the training data, and p ranges
over all POS tags.

4.2 Features

We follow some top-performing NP chunking sys-
tems and perform holdout methodology to design
features for our model, resulting in a rich feature
set including POS features provided in the official
CoNLL 2000 dataset (generated by the Brill tag-
ger (Brill, 1995), with labeling accuracy of around
95-97%), some contextual and morphological fea-
tures. Table 1 lists our feature set for NP chunk-
ing.

4.3 Experimental Results

We trained linear-chain conditional random fields
(CRFs) (Lafferty et al., 2001) as the baseline. The
well known limited memory quasi-Newton BFGS
algorithm (L-BFGS) (Liu and Nocedal, 1989) was
applied to learn the parameters for CRFs. To
avoid over-fitting, we penalized the log-likelihood
by the commonly used zero-mean Gaussian prior
over the parameters. This gives us a competitive
baseline CRF model for NP chunking. To make
fair and accurate comparison, we used the same
set of features listed in Table 1 for both M3N and
CRFs. All experiments were performed on the
Linux platform, with a 3.2GHz Pentium 4 CPU
and 4 GB of memory.

1412



Model Training Method Kernel Function Iteration Training Time(s) P(%) R(%) Fβ=1
M3N structured SMO linear kernel: 〈a, b〉H 100 1176 94.59 94.22 94.40
M3N structured SMO polynomial(quadratic): (〈a, b〉H + 1)2 100 30792 94.88 94.49 94.68
M3N structured SMO polynomial(cubic): (〈a, b〉H + 1)3 100 30889 94.47 94.01 94.24
M3N structured SMO polynomial(biquadratic): (〈a, b〉H + 1)4 100 31556 93.90 93.77 93.83
M3N structured SMO neural kernel: tanh(0.1 · 〈a, b〉H) 20 7395 94.42 94.02 94.22
CRFs L-BFGS — 100 352 94.55 94.09 94.32

Table 2: M3N vs. CRFs: Performance and training time comparison for NP chunking on the CoNLL
2000 official dataset. M3N was trained using the structured SMO algorithm.

Model Training Method Kernel Function Iteration Training Time(s) P(%) R(%) Fβ=1
M3N SGD linear kernel: 〈a, b〉H 100 89 94.58 94.21 94.39
M3N SGD polynomial(quadratic): (〈a, b〉H + 1)2 100 1820 94.89 94.50 94.69
M3N SGD polynomial(cubic): (〈a, b〉H + 1)3 100 1831 94.47 94.01 94.24
M3N SGD polynomial(biquadratic): (〈a, b〉H + 1)4 100 1857 93.91 93.76 93.83
M3N SGD neural kernel: tanh(0.1 · 〈a, b〉H) 20 477 94.40 94.01 94.20
CRFs L-BFGS — 100 352 94.55 94.09 94.32

Table 3: M3N vs. CRFs: Performance and training time comparison for NP chunking on the CoNLL
2000 official dataset. M3N was trained using the SGD algorithm.

System Fβ=1
SVMs (polynomial kernel) (Kudo and Mat-
sumoto, 2000)

93.79

SVM combination (Kudo and Matsumoto,
2001)

94.39

Generalized winnow (Zhang et al., 2002) 94.38
Voted perceptron (Collins, 2002) 94.09
CRFs (Sha and Pereira, 2003) 94.38
Second order CRFs (McDonald et al., 2005) 94.29
Chunks from the Charniak Parser (Holling-
shead et al., 2005)

94.20

Second order latent-dynamic CRFs + improved
A* search based inference (Sun et al., 2008)

94.34

Our approach 94.69

Table 4: NP chunking: Comparison with some ex-
isting state-of-the-art systems.

Similar to other discriminative graphical mod-
els such as CRFs, the modeling flexibility of
M3N permits the feature functions to be com-
plex, arbitrary, nonindependent, and overlapping
features, allowing the multiple features described
in Table 1 to be directly exploited. Moreover,
M3N is capable of incorporating multiple kernel
functions (see Section 2) which allow the efficient
use of high-dimensional feature spaces during the
experiments.

The resulting number of features is 7,835,439,
and both M3N and CRFs were trained to predict
47,366 tokens with 12,422 noun phrases in the
testing set. For simplicity, we denote a = φ(x, y),

and b = φ(x′, y′), and the linear kernel can be
rewritten as κ(a, b) = 〈a, b〉H. We performed
holdout methodology to find optimal values for
coefficients s, d, and r in M3N kernel functions.
For polynomial kernels, we varied d from 2 to 4,
resulting in quadratic, cubic, and biquadratic ker-
nels, respectively. Finally, we chose optimized
values: s = 1, r = 1 for polynomial kernels, and
s = 0.1, r = 0 for neural kernels. The capacity C
for M3N was set to 1 in our experiments.

Table 2 shows comparative performance and
training time for M3N (trained with structured
SMO) and CRFs, while Table 3 shows compar-
ative performance and training time for M3N
(trained with SGD) and CRFs 2. For M3N , when
trained with quadratic kernel and structured SMO,
the best F-measure of 94.68 was achieved, leading
to an improvement of 0.36 compared to the CRF
baseline. What follows is the linear kernel that
obtained 94.40 F-measure. The cubic and neu-
ral kernels obtained close performance, while the
biquadratic kernel led to the worst performance.
However, the structured SMO is very computa-
tionally intensive, especially for polynomial ker-
nels. For example, CRFs converged in 352 sec-

2We used Taku Kudo’s CRF++ toolkit (available at
http://crfpp.sourceforge.net/) in our experiments. The M3N
model, and the structured SMO and SGD training algorithms
were also implemented using C++.

1413



-10 0 10 20 30 40 50 60 70 80 90 100

-240

-220

-200

-180

-160

-140

-120

-100

-80

O
bj

ec
tiv

e 
fu

nc
tio

n 
va

lu
e

Iteration
(a)

 M3N, structured SMO
 M3N, SGD

-10 0 10 20 30 40 50 60 70 80 90 100
-0.45

-0.40

-0.35

-0.30

-0.25

O
bj

ec
tiv

e 
fu

nc
tio

n 
va

lu
e

Iteration
(b)

 M3N, structured SMO
 M3N, SGD

-2 0 2 4 6 8 10 12 14 16 18 20
-220

-200

-180

-160

-140

-120

-100

O
bj

ec
tiv

e 
fu

nc
tio

n 
va

lu
e

Iteration
(c)

 M3N, structured SMO
 M3N, SGD

Figure 2: Convergence speed comparison for structured SMO and SGD algorithms. The X axis shows
number of training iterations, and the Y axis shows objective function value. (a) The M3N model was
trained using linear kernel. (b) The M3N model was trained using polynomial(quadratic) kernel. (c)
The M3N model was trained using neural kernel.

onds, whileM3N (polynomial kernels) took more
than 8.5 hours to finish training.

As can be seen in Table 3, the SGD algorithm
significantly accelerated the training procedure of
M3N without sacrificing performance. When the
linear kernel was used, M3N finished training in
89 seconds, more than 13 times faster than the
model trained with structured SMO. And it is even
much faster than the CRF model trained with L-
BFGS. More importantly, SGD obtained almost
the same performance as structured SMO with all
M3N kernel functions.

Table 4 gives some representative NP chunking
results for previous work and for our best model
on the same dataset. These results showed that our
model compares favorably with existing state-of-
the-art systems 3.

Figure 2 compares the convergence speed of
structured SMO and SGD algorithms for the
M3N model. Linear (Figure 2 (a)), polyno-
mial(quadratic) (Figure 2 (b)) and neural kernels
(Figure 2 (c)) were used 4. We calculated objec-
tive function values during effective training iter-
ations. It can be seen that both structured SMO
and SGD algorithms converge to the same objec-
tive function value for different kernels, but SGD
converges considerably faster than the structured
SMO.

Figure 3 (a) demonstrates the effect of training
set size on performance for NP chunking. We

3Note that it is difficult to compare strictly, since reported
results sometimes leave out details (e.g., feature sets, signifi-
cance tests, etc) needed for accurate comparison.

4For cubic and biquadratic kernels, the curves are very
similar to that of quadratic kernel, and we omitted them for
space.

increased the training set size from 1,000 sen-
tences to 8,000 sentences, with an incremental
step of 1,000. And the testing set was fixed to
be 2,012 sentences. The M3N models (with dif-
ferent kernels) were trained using the SGD algo-
rithm. It is particularly interesting to know that
the performance boosted for all the models when
increasing the training set size. Using linear and
quadratic kernels, M3N model significantly and
consistently outperforms the CRF model for dif-
ferent training set sizes. The cubic and neural
kernels lead to almost the same performance for
M3N , which is slightly lower than the CRF base-
line. As illustrated by the curves, M3N (trained
with quadratic kernel) achieved the best perfor-
mance and larger training set size leads to better
improvement for this model when compared to the
CRF model, whileM3N (trained with biquadratic
kernel) obtained the worst performance among all
the models.

Accordingly, Figure 3 (b) shows the impact of
increasing the training set size on training time for
NP chunking. Increasing training set size leads
to an increase in the computational complexity of
training procedure for all models. For the M3N
model, it is faster when trained with linear kernel
than the CRF model. And the three polynomial
kernels (quadratic, cubic and biquadratic) have
roughly the same training time. For CRFs and
(M3N , neural kernel), the training time is close
to each other. For example, when the training
set contains 1,000 sentences, the training time for
CRFs, (M3N , linear kernel), (M3N , quadratic
kernel), (M3N , cubic kernel), (M3N , biquadratic
kernel), and (M3N , neural kernel) is 24s, 7s, 72s,

1414



1k 2k 3k 4k 5k 6k 7k 8k 9k
90.5

91.0

91.5

92.0

92.5

93.0

93.5

94.0

94.5

95.0

F-
m

ea
su

re

Number of training sentences
(a)

 CRFs
 M3N, linear kernel 
 M3N, quadratic kernel
 M3N, cubic kernel
 M3N, biquadratic kernel
 M3N, neural kernel

1k 2k 3k 4k 5k 6k 7k 8k 9k
0

200

400

600

800

1000

1200

1400

1600

1800

2000

Tr
ai

ni
ng

 ti
m

e 
(s

)

Number of training sentences
(b)

 CRFs
 M3N, linear kernel 
 M3N, quadratic kernel
 M3N, cubic kernel
 M3N, biquadratic kernel
 M3N, neural kernel

Figure 3: (a) Effect of training set size on performance for NP chunking. The training set size was
increased from 1,000 sentences to 8,000 sentences, with an incremental step of 1,000. The testing set
contains 2,012 sentences. All the M3N models (with different kernels) were trained using the SGD
algorithm. (b) Effect of training set size on training time for NP chunking.

72s, 74s, and 30s. When trained on 8,000 sen-
tences, the numbers become 336s, 79s, 1679s,
1689s, 1712s, and 411s, respectively.

5 Related Work

The M3N framework and its variants have gen-
erated much interest and great progress has been
made, as evidenced by their promising results
evaluated in handwritten character recognition,
collective hypertext classification (Taskar et al.,
2003), parsing (Taskar et al., 2004), and XML
tag relabeling (Spengler, 2005). However, all the
above mentioned research work used structured
SMO algorithm for parameter learning, which can
be computationally intensive, especially for very
large datasets.

Recently, similar stochastic gradient methods
have been applied to train log-linear models such
as CRFs (Vishwanathan et al., 2006). However,
the maximum margin loss has a discontinuity in
its derivative, making optimization of such models
somewhat more involved than log-linear ones. We
first exploit SGD method for fast parameter learn-
ing of M3N and achieve state-of-the-art perfor-
mance on the NP chunking task in the NLP com-
munity.

Several algorithms have been proposed to
train max-margin models, including cutting plane
SMO (Tsochantaridis et al., 2005), exponenti-
ated gradient (Bartlett et al., 2004; Collins et al.,
2008), extragradient (Taskar et al., 2006), and

subgradient (Shalev-Shwartz et al., 2007). Some
methods are similar to SGD in that they all pro-
cess a single training example at a time. The
SGD methods directly optimize the primal prob-
lem, and at each update use a single example to
approximate the gradient of the primal objective
function. Some of the proposed algorithms, such
as exponentiated gradient corresponds to block-
coordinate descent in the dual, and uses the exact
gradient with respect to the block being updated.
We plan to implement and compare some of these
algorithms with SGD for M3N .

6 Conclusion and Future Work

We have presented the first known empirical study
on sequence labeling based on M3N . We have
also provided the efficient SGD algorithm and
shown how it can be applied to significantly
speedup the training procedure of M3N . As a
case study, we performed extensive experiments
on standard dataset for NP chunking, showing the
promising and competitiveness of our approach.
Several interesting issues, such as the convergence
speed of the SGD algorithm, the effect of train-
ing set size on performance for NP chunking, and
the effect of training set size on training time,
were also investigated in our experiments. For
the future work, we plan to further the scalability
and applicability of our approach and evaluate it
on other large-scale real world sequence labeling
tasks, such as POS tagging and NER.

1415



References
Peter L. Bartlett, Ben Taskar, Michael Collins, and David

Mcallester. Exponentiated gradient algorithms for large-
margin structured classification. In Proceedings of NIPS-
04, pages 113–120. MIT Press, 2004.

A. Benveniste, M. Metivier, and P. Priouret. Algorithmes
adaptatifs et approximations stochastiques. Masson,
1987.

Léon Bottou. Stochastic learning. In Olivier Bousquet and
Ulrike von Luxburg, editors, Advanced Lectures on Ma-
chine Learning, Lecture Notes in Artificial Intelligence,
LNAI 3176, pages 146–168. Springer Verlag, Berlin,
2004.

Eric Brill. Transformation-based error-driven learning and
natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543–
565, 1995.

Michael Collins, Amir Globerson, Terry Koo, Xavier Car-
reras, and Peter L. Bartlett. Exponentiated gradient al-
gorithms for conditional random fields and Max-margin
Markov networks. Journal of Machine Learning Re-
search, 9:1775–1822, 2008.

Michael Collins. Discriminative training methods for hidden
Markov models: Theory and experiments with perceptron
algorithms. In Proceedings of HLT/EMNLP-02, pages 1–
8, 2002.

Kristy Hollingshead, Seeger Fisher, and Brian Roark.
Comparing and combining finite-state and context-free
parsers. In Proceedings of HLT/EMNLP-05, pages 787–
794, Vancouver, British Columbia, Canada, 2005.

Taku Kudo and Yuji Matsumoto. Use of support vector learn-
ing for chunk identification. In Proceedings of CoNLL-
2000 and LLL-2000, pages 142–144, Lisbon, Portugal,
2000.

Taku Kudo and Yuji Matsumoto. Chunking with support vec-
tor machines. In Proceedings of HLT/NAACL-01, pages
1–8, 2001.

John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML-01, pages 282–289, 2001.

Yann Lecun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324,
Nov 1998.

Dong C. Liu and Jorge Nocedal. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45:503–528, 1989.

Ryan McDonald, Koby Crammer, and Fernando Pereira.
Flexible text segmentation with structured multilabel clas-
sification. In Proceedings of HLT/EMNLP-05, pages 987–
994, Vancouver, British Columbia, Canada, 2005.

Barak A. Pearlmutter. Fast exact multiplication by the Hes-
sian. Neural Computation, 6(1):147–160, 1994.

John C. Platt. Fast training of support vector machines us-
ing sequential minimal optimization. Advances in Kernel
Methods: Support Vector Learning, pages 41–64, 1998.

H. Robbins and S. Monroe. A stochastic approximation
method. Annals of Mathematical Statistics, 22:400–407,
1951.

Erik Tjong Kim Sang and Sabine Buchholz. Introduction to
the CoNLL-2000 shared task: Chunking. In Proceedings
of CoNLL-2000, pages 127–132, Lisbon, Portugal, 2000.

Bernhard Schölkopf and Alexander J. Smola. Learning with
Kernels. MIT Press, Cambridge, MA, 2002.

Nicol N. Schraudolph. Local gain adaptation in stochas-
tic gradient descent. In Proceedings of the 9th Interna-
tional Conference on Artificial Neural Networks, pages
569–574, 1999.

Fei Sha and Fernando Pereira. Shallow parsing with condi-
tional random fields. In Proceedings of HLT/NAACL-03,
pages 213–220, 2003.

Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pe-
gasos: Primal estimated sub-GrAdient SOlver for SVM.
In Proceedings of ICML-07, pages 807–814, New York,
NY, USA, 2007.

John Shawe-Taylor and Nello Cristianini. Kernel Methods
for Pattern Analysis. Cambridge University Press, Cam-
bridge, UK, 2004.

Alex Spengler. Maximum margin Markov networks for
XML tag relabelling. Master’s thesis, University of Karl-
sruhe, 2005.

Xu Sun, Louis-Philippe Morency, Daisuke Okanohara, and
Jun’ichi Tsujii. Modeling latent-dynamic in shallow pars-
ing: A latent conditional model with improved inference.
In Proceedings of COLING-08, pages 841–848, Manch-
ester, UK, 2008.

Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-
margin Markov networks. In Proceedings of NIPS-03.
MIT Press, 2003.

Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and
Christopher Manning. Max-margin parsing. In Proceed-
ings of HLT/EMNLP-04, pages 1–8, 2004.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos
Guestrin. Learning structured prediction models: A large
margin approach. In Proceedings of ICML-05, pages 896–
903, Bonn, Germany, 2005.

Ben Taskar, Simon Lacoste-Julien, and Michael I. Jordan.
Structured prediction via the extragradient method. In
Proceedings of NIPS-06. MIT Press, 2006.

Ben Taskar. Learning Structured Prediction Models: A Large
Margin Approach. PhD thesis, Stanford University, De-
cember 2004.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. Large margin methods for
structured and interdependent output variables. Journal
of Machine Learning Research, 6:1453–1484, 2005.

Vladimir N. Vapnik. The Nature of Statistical Learning The-
ory. Springer-Verlag, Inc., New York, USA, 1995.

S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. Accelerated training of
conditional random fields with stochastic gradient meth-
ods. In Proceedings of ICML-06, pages 969–976, Pitts-
burgh, Pennsylvania, 2006.

Tong Zhang, Fred Damerau, and David Johnson. Text chunk-
ing based on a generalization of winnow. Journal of Ma-
chine Learning Research, 2:615–637, 2002.

1416


