Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 815–823,

Beijing, August 2010

815

Nonparametric Word Segmentation for Machine Translation

ThuyLinh Nguyen Stephan Vogel Noah A. Smith

Language Technologies Institute

Carnegie Mellon University

{thuylinh,vogel,nasmith}@cs.cmu.edu

Abstract

We present an unsupervised word seg-
mentation model for machine translation.
The model uses existing monolingual seg-
mentation techniques and models the joint
distribution over source sentence segmen-
tations and alignments to the target sen-
tence. During inference,
the monolin-
gual segmentation model and the bilin-
gual word alignment model are coupled
so that the alignments to the target sen-
tence guide the segmentation of the source
sentence. The experiments show improve-
ments on Arabic-English and Chinese-
English translation tasks.

1

Introduction

In statistical machine translation, the smallest unit
is usually the word, deﬁned as a token delimited
by spaces. Given a parallel corpus of source and
target text, the training procedure ﬁrst builds a
word alignment, then extracts phrase pairs from
this word alignment. However, in some languages
(e.g., Chinese) there are no spaces between words.
The same problem arises when translating be-
tween two very different languages, such as from
a language with rich morphology like Hungarian
or Arabic to a language with poor morphology
like English or Chinese. A single word in a mor-
phologically rich language is often the composi-
tion of several morphemes, which correspond to
separate words in English.1

1We will use the terms word segmentation, morphologi-
cal analysis, and tokenization more or less interchangeably.

Often some preprocessing is applied involving
word segmentation or morphological analysis of
the source and/or target text. Such preprocess-
ing tokenizes the text into morphemes or words,
which linguists consider the smallest meaning-
bearing units of the language. Take as an ex-
ample the Arabic word “fktbwha” and its En-
glish translation “so they wrote it”. The preferred
segmentation of “fktbwha” would be “f-ktb-w-ha
(so-wrote-they-it),” which would allow for a one-
to-one mapping between tokens in the two lan-
guages. However, the translation of the phrase in
Hebrew is “wktbw ath”. Now the best segmen-
tation of the Arabic words would be “fktbw-ha,”
corresponding to the two Hebrew words. This ex-
ample shows that there may not be one correct
segmentation that can be established in a prepro-
cessing step. Rather, tokenization depends on the
language we want to translate into and needs to
be tied in with the alignment process.
In short,
we want to ﬁnd the tokenization yielding the best
alignment, and thereby the best translation sys-
tem.

We propose an unsupervised tokenization
method for machine translation by formulating a
generative Bayesian model to “explain” the bilin-
gual training data. Generation of a sentence pair
is described as follows: ﬁrst a monolingual to-
kenization model generates the source sentence,
then the alignment model generates the target sen-
tence through the alignments with the source sen-
tence. Breaking this generation process into two
steps provides ﬂexibility to incorporate existing
monolingual morphological segmentation mod-
els such as those of Mochihashi et al. (2009) or
Creutz and Lagus (2007). Using nonparametric

816

models and the Bayesian framework makes it pos-
sible to incorporate linguistic knowledge as prior
distributions and obtain the posterior distribution
through inference techniques such as MCMC or
variational inference.

As new test source sentences do not have trans-
lations which can help to infer the best segmenta-
tion, we decode the source string according to the
posterior distribution from the inference step.

In summary, our segmentation technique con-

sists of the following steps:

• A joint model of segmented source text and

its target translation.

• Inference of the posterior distribution of the

model given the training data.

• A decoding algorithm for segmenting source

text.

• Experiments in translation using the prepro-

cessed source text.

Our experiments show that the proposed seg-
mentation method leads to improvements on
Arabic-English and Chinese-English translation
tasks.

In the next section we will discuss related work.
Section 3 will describe our model in detail. The
inference will be covered in Section 4, and decod-
ing in Section 5. Experiments and results will be
presented in Section 6.

2 Related Work
The problem of segmentation for machine trans-
lation has been studied extensively in recent lit-
erature. Most of the work used some linguistic
knowledge about the source and the target lan-
guages (Nießen and Ney, 2004; Goldwater and
McClosky, 2005). Sadat and Habash (2006) ex-
perimented with a wide range of tokenization
schemes for Arabic-English translation. These
experiments further show that even for a single
language pair, different tokenizations are needed
depending on the training corpus size. The ex-
periments are very expensive to conduct and do
not generalize to other language pairs. Recently,
Dyer (2009) created manually crafted lattices for

a subset of source words as references for seg-
mentation when translating into English, and then
learned the segmentation of the source words to
optimize the translation with respect to these ref-
erences. He showed that the parameters of the
model can be applied to similar languages when
translating into English. However, manually cre-
ating these lattices is time-consuming and requires
a bilingual person with some knowledge of the un-
derlying statistical machine translation system.

There have been some attempts to apply un-
supervised methods for tokenization in machine
translation (Chung and Gildea, 2009; Xu et al.,
2008).
The alignment model of Chung and
Gildea (2009) forces every source word to align
with a target word. Xu et al. (2008) mod-
eled the source-to-null alignment as in the source
word to target word model. Their models are
special cases of our proposed model when the
source model2 is a unigram model. Like Xu et
al. (2008), we use Gibbs sampling for inference.
Chung and Gildea (2009) applied efﬁcient dy-
namic programming-based variational inference
algorithms.

We beneﬁt from existing unsupervised mono-
lingual segmentation. The source model uses the
nested Pitman-Yor model as described by Mochi-
hashi et al. (2009). When sampling each potential
word boundary, our inference technique is a bilin-
gual extension of what is described by Goldwater
et al. (2006) for monolingual segmentation.

Nonparametric models have received attention
in machine translation recently. For example,
DeNero et al. (2008) proposed a hierarchical
Dirichlet process model to learn the weights of
phrase pairs to address the degeneration in phrase
extraction. Teh (2006) used a hierarchical Pitman-
Yor process as a smoothing method for language
models.

Recent work on multilingual language learning
successfully used nonparametric models for lan-
guage induction tasks such as grammar induction
(Snyder et al., 2009; Cohen et al., 2010), morpho-
logical segmentation (Goldwater et al., 2006; Sny-
der and Barzilay, 2008), and part-of-speech tag-
ging (Goldwater and Grifﬁths, 2007; Snyder et al.,

2Note that “source model” here means a model of source

text, not a source model in the noisy channel paradigm.

817

2008).

3 Models
We start with the generative process for a source
sentence and its alignment with a target sentence.
Then we describe individual models employed by
this generation scheme.

3.1 Generative Story
A source sentence is a sequence of word tokens,
and each word is either aligned or not aligned. We
focus only on the segmentation problem and not
reordering source words; therefore, the model will
not generate the order of the target word tokens.
A sentence pair and its alignment are captured by
four components:

• a sequence of words in the source sentence,
• a set of null-aligned source tokens,
• a set of null-aligned target tokens, and
• a set of (source word to target word) align-

ment pairs.

We will start with a high-level story of how the
segmentation of the source sentence and the align-
ment are generated.

1. A source language monolingual segmenta-

tion model generates the source sentence.

2. Generate alignments:

(a) Given the sequence of words of the
source sentence already generated in
step 1, the alignment model marks each
source word as either aligned or un-
aligned. If a source word is aligned, the
model also generates the target word.
(b) Unaligned target words are generated.

The model deﬁnes the joint probability of a seg-
mented source language sentence and its align-
ment. During inference, the two parts are cou-
pled, so that the alignment will inﬂuence which
segmentation is selected. However, there are sev-
eral advantages in breaking the generation process
into two steps.

First of all, in principle the model can incor-
porate any existing probabilistic monolingual seg-
mentation to generate the source sentence. For
example,
the source model can be the nested
Pitman-Yor process as described by Mochihashi et
al. (2009), the minimum description length model
presented by Creutz and Lagus (2007), or some-
thing else. Also the source model can incorporate
linguistic knowledge from a rule-based or statisti-
cal morphological disambiguator.

The model generates the alignment after the
source sentence with word boundaries already
generated. Therefore, the alignment model can
be any existing word alignment model (Brown
et al., 1993; Vogel et al., 1996). Even though
the choices of source model or alignment model
can lead to different inference methods, the model
we propose here is highly extensible. Note that
we assume that the alignment consists of at most
one-to-one mappings between source and target
words, with null alignments possible on both
sides.

Another advantage of a separate source model
lies in the segmentation of an unseen test set. In
section 5 we will show how to apply the source
model distribution learned from training data to
ﬁnd the best segmentation of an unseen test set.

Notation and Parameters

We will use bold font for a sequence or bags
of words and regular font for an individual word.
A source sentence s is a sequence of |s| words
si: (cid:2)s1, . . . , s|s|(cid:3); the translation of sentence s is
the target sentence t of |t| words (cid:2)t1, . . . , t|t|(cid:3).

In sentence s the list of unaligned words is snal
and the list of aligned source words is sal.
In
the target sentence t the list of unaligned words
is tnal and the list of target words having one-
to-one alignment with source words sal is tal.
The alignment a of s and t is represented by
{(cid:2)si, null(cid:3) | si ∈ snal} ∪ {(cid:2)si, tai(cid:3) | si ∈ sal; tai ∈
tal} ∪ {(cid:2)null, tj(cid:3) | tj ∈ tnal} where ai denotes
the index in t of the word aligned to si.
The probability of a sequence or a set is denoted
by P (.), probability at the word level is p (.). For
example, the probability of sentence s is P (s), the
probability of a word s is p (s), the probability
that the target word t aligns to an aligned source

818

word s is p (t| s).
from the following models:

A sentence pair and its alignment are generated

• The source model generates sentence s with

probability P (s).

aligned with

independently for

cides
whether it
p (null | si )
ity:
of

• The source-to-null alignment model de-
each word s
is unaligned with probability
probabil-
or
The probability
is:
p (null | si) ×

1 − p (null | si ).
this step,

P (snal, sal | s) = (cid:4)si∈snal
(cid:4)si∈sal

(1 − p (null | si)) .

We will also refer to the source-to-null model
as the deletion model, since words in snal are
effectively deleted for the purposes of align-
ment.

for all source words,

• The source-to-target alignment model gen-
erates a bag of target words tal aligned
to the source words sal with probability:
P (tal| sal) =(cid:4)si∈sal;tai∈tal
p (tai | si ). Note
that we do not need to be concerned with
generating a explicitly, since we do not
model word order on the target side.

• The null-to-target alignment model gen-
erates the list of unaligned target words
tnal given aligned target words tal with
P (tnal| tal) as follows:

– Generate the number of unaligned tar-
get words |tnal| given the number of
aligned target words |tal| with probabil-
ity P (|tnal| | |tal|).
– Generate |tnal| unaligned words t ∈
tnal independently, each with probabil-
ity p (t | null).
resulting

null-to-target

The
bility is

therefore:

P (|tnal| | |tal|)(cid:4)t∈tnal

proba-
=

P (tnal | tal)
p (t | null) .

We also call the null-to-target model the in-
sertion model.

The above generation process deﬁnes the joint
probability of source sentence s and its alignment

a as follows:

(1)

alignment model

source model

P (s, a) = P (s)

× P (a | s)
(cid:5)(cid:6)(cid:7)(cid:8)
(cid:5) (cid:6)(cid:7) (cid:8)
P (a | s) = P (tal| sal) × P (tnal| tal)
× (cid:9)si∈snal
p (null | si) × (cid:9)si∈sal

(2)
(1 − p (null | si))

3.2 Source Model
Our generative process provides the ﬂexibility of
incorporating different monolingual models into
the probability distribution of a sentence pair.
In particular we use the existing state-of-the-art
nested Pitman-Yor n-gram language model as de-
scribed by Mochihashi et al. (2009). The proba-
bility of s is given by

P (s) = P (|s|)

|s|(cid:9)i=1

p (si| si−n, . . . , si−1)

(3)

where the n-gram probability is a hierarchical
Pitman-Yor language model using (n − 1)-gram
as the base distribution.
At the unigram level, the model uses the base
distribution p (s) as the inﬁnite-gram character-
level Pitman-Yor language model.

3.3 Modeling Null-Aligned Source Words
The probability that a source word aligns to null
p (null | s) is deﬁned by a binomial distribution
with Beta prior Beta (αp, α (1 − p)), where α
and p are model parameters. When p → 0 and
α → ∞ the probability p (null | s) converges to 0
forcing each source words align to a target word.
We ﬁxed p = 0.1 and α = 20 in our experiment.
Xu et al. (2008) view the null word as another
target word, hence in their model the probability
that a source word aligns to null can only depend
on itself.

By modeling the source-to-null alignment sep-
arately, our model lets the distribution depend
on the word’s n-gram context as in the source
model. p (null | si−n, . . . , si) stands for the prob-
ability that the word si is not aligned given its con-
text (si−n, . . . , si−1).
p (null | si−n, . . . , si)

distribution
is deﬁned similarly to

n-gram source-to-null

The

819

p (null | si) deﬁnition above in which the base
distribution p now becomes the (n − 1)-gram:
p (null | si−n+1, . . . , si).3
3.4 Source-Target Alignment Model
The probability p (t | s) that a target word t aligns
to a source word s is a Pitman-Yor process:

t | s ∼ PY (d, α, p0 (t | s))

here d and α are the input parameters, and
p0 (t | s) is the base distribution.
Let |s,·| denote the number of times s is aligned
to any t in the corpus and let |s, t| denote the num-
ber of times s is aligned to t anywhere in the cor-
pus. And let ty(s) denote the number of different
target words t the word s is aligned to anywhere
in the corpus. In the Chinese Restaurant Process
metaphor, there is one restaurant for each source
word s, the s restaurant has ty(s) tables and total
|s,·| customers; table t has |s, t| customers.
Then, at a given time in the generative process
for the corpus, we can write the probability that t
is generated by the word s as:

• if |s, t| > 0:

p (t | s) =
|s, t| − d + [α + dty(s)]p0 (t | s)

|s,·| + α

• if |s, t| = 0:

p (t | s) =

[α + dty(s)]p0 (t | s)

|s,·| + α

For language pairs with similar character sets
such as English and French, words with similar
surface form are often translations of each other.
The base distribution can be deﬁned based on
the edit distance between two words (Snyder and
Barzilay, 2008).

We are working with diverse language pairs
(Arabic-English and Chinese-English), so we
use the base distribution as the ﬂat distribution
p0 (t | s) = 1
T ; T is the number of distinct target
words in the training set. In our experiment, the
model parameters are α = 20 and d = .5.

3We also might have conditioned this decision on words
following si, since those have all been generated already at
this stage.

3.5 Modeling Null-Aligned Target Words
The null-aligned target words are modeled condi-
tioned on previously generated target words as:

P (tnal| tal) = P (|tnal| | |tal|) (cid:9)t∈tnal
• the number of unaligned target words:

This model uses two probability distributions:

p (t | null)

P (|tnal| | |tal|), and

• the probability that each word in tnal is gen-

erated by null: p (t | null).

We model

the number of unaligned target
words similarly to the distribution in the IBM3
word alignment model (Brown et al., 1993).
IBM3 assumes that each aligned target words gen-
erates a null-aligned target word with probabil-
ity p0 and fails to generate a target word with
probability 1 − p0. So the parameter p0 can
be used to control the number of unaligned tar-
get words.
In our experiments, we ﬁx p0 =
.05. Following this assumption, the probability of
|tnal| unaligned target words generated from |tal|
words is: P (|tnal| | |tal|) = (cid:2) |tal|
(1 −
The probability that a target word t aligns to
null, p (t | null), also has a Pitman-Yor process
prior. The base distribution of the model is similar
to the source-to-target model’s base distribution
which is the ﬂat distribution over target words.

|tnal|(cid:3)p|tnal|

p0)|tal|−|tnal|.

0

4 Inference
We have deﬁned a probabilistic generative model
to describe how a corpus of alignments and seg-
mentations can be generated jointly. In this sec-
tion we discuss how to obtain the posterior distri-
butions of the missing alignments and segmenta-
tions given the training corpus, using Gibbs sam-
pling.

Suppose we are provided a morphological
disambiguator for the source language such as
MADA morphology tokenization toolkit (Sadat
and Habash, 2006) for Arabic.4 The morpho-
logical disambiguator segments a source word to
4MADA provides several segmentation schemes; among
them the MADA-D3 scheme seeks to separate all mor-
phemes of each word.

820

morphemes of smallest meaning-bearing units of
the source language. Therefore, a target word is
equivalent to one or several morphemes. Given
a morphological disambiguation toolkit, we use
its output to bias our inference by not consider-
ing word boundaries after every character but only
considering potential word boundaries as a subset
of the morpheme boundaries set. In this way, the
inference uses the morphological disambiguation
toolkit to limit its search space.

The inference starts with an initial segmenta-
tion of the source corpus and also its alignment
to the target corpus. The Gibbs sampler consid-
ers one potential word boundary at a time. There
are two hypotheses at any given boundary posi-
tion of a sentence pair (s, t): the merge hypothe-
sis stands for no word boundary and the resulting
source sentence smerge has a word s spanning over
the sample point; the split hypothesis indicates the
resulting source sentence ssplit has a word bound-
ary at the sample point separating two words s1s2.
Similar to Goldwater et al. (2006) for monolingual
segmentation, the sampler randomly chooses the
boundary according to the relative probabilities of
the merge hypothesis and the split hypothesis.

The model consists of source and alignment
model variables; given the training corpora size of
a machine translation system, the number of vari-
ables is large. So if the Gibbs sampler samples
both source variables and alignment variables, the
inference requires many iterations until the sam-
pler mixes. Xu et al. (2008) ﬁxed this by repeat-
edly applying GIZA++ word alignment after each
sampling iteration through the training corpora.

Our inference technique is not precisely Gibbs
sampling. Rather than sampling the alignment or
attempting to collapse it out (by summing over
all possible alignments when calculating the rel-
ative probabilities of the merge and split hypothe-
ses), we seek the best alignment for each hypoth-
esis. In other words, for each hypothesis, we per-
form a local search for a high-probability align-
ment of the merged word or split words, given
the rest of alignment for the sentence. Up to one
word may be displaced and realigned. This “local-
best” alignment is used to score the hypothesis,
and after sampling merge or split, we keep that
best alignment.

This inference technique is motivated by run-
time demands, but we do not yet know of a the-
oretical justiﬁcation for combining random steps
with maximization over some variables. A more
complete analysis is left to future work.

5 Decoding for Unseen Test Sentences
Section 4 described how to get the model’s pos-
terior distribution and the segmentation and align-
ment of the training data under the model. We are
left with the problem of decoding or ﬁnding the
segmentation of test sentences where the transla-
tions are not available. This is needed when we
want to translate new sentences. Here, tokeniza-
tion is performed as a preprocessing step, decou-
pled from the subsequent translation steps.

The decoding step uses the model’s posterior
distribution for the training data to segment un-
seen source sentences. Because of the clear sep-
aration of the source model and the alignment
model, the source model distribution learned from
the Gibbs sampling directly represents the distri-
bution over the source language and can therefore
also handle the segmentation of unknown words
in new test sentences. Only the source model is
used in preprocessing.

The best segmentation s∗ of a string of charac-

ters c = (cid:2)c1, . . . , c|c|(cid:3) according to the n-gram

source model is:

s∗ = argmax
s from c

p (|s|)

i=|s|(cid:9)i=1

p (si| si−n, . . . , si−1)

We use a stochastic ﬁnite-state machine for de-
coding. This is possible by composition of the fol-
lowing two ﬁnite state machines:

• Acceptor Ac. The string of characters c is
represented as an ﬁnite state acceptor ma-
chine where any path through the machine
represents an unweighted segmentation of c.
• Source model weighted ﬁnite state trans-
ducer Lc. Knight and Al-Onaizan (1998)
show how to build an n-gram language
model by a weighted ﬁnite state machine.
The states of the transducer are (n − 1)-
gram history, the edges are words from the
language. The arc si coming from state

821

(si−n, . . . , si−1) to state (si−n+1, . . . , si) has
weight p (si| si−n, . . . , si−1).

The best segmentation s∗ is given as s∗ =
BestPath(Ac ◦ Lc).
6 Experiments
This section presents experimental results on
Arabic-English and Chinese-English translation
tasks using the proposed segmentation technique.

6.1 Arabic-English
As a training set we use the BTEC corpus dis-
tributed by the International Workshop on Spo-
ken Language Translation (IWSLT) (Matthias and
Chiori, 2005). The corpus is a collection of
conversation transcripts from the travel domain.
The “Supplied Data” track consists of nearly 20K
Arabic-English sentence pairs. The development
set consists of 506 sentences from the IWSLT04
evaluation test set and the unseen set consists of
500 sentences from the IWSLT05 evaluation test
set. Both development set and test set have 16 ref-
erences per Arabic sentence.

6.2 Chinese-English
The training set for Chinese-English translation
task is also distributed by the IWSLT evaluation
campaign.
It consists of 67K Chinese-English
sentence pairs. The development set and the test
set each have 489 Chinese sentences and each sen-
tence has 7 English references.

6.3 Results
We will report the translation results where the
preprocessing of the source text are our unigram,
bigram, and trigram source models and source-to-
null model.

The MCMC inference algorithm starts with an
initial segmentation of the source text into full
word forms. For Chinese, we use the original
word segmentation as distributed by IWSLT. To
get an initial alignment, we generate the IBM4
Viterbi alignments in both directions using the
GIZA++ toolkit (Och and Ney, 2003) and com-
bine them using the “grow-diag-ﬁnal-and” heuris-
tic. The output of combining GIZA++ align-
ment for a sentence pair is a sequence of si-tj

entries where i is an index of the source sen-
tence and j is an index of the target sentence.
As our model allows only one-to-one mappings
between the words in the source and target sen-
tences, we remove si-tj from the sequence if ei-
ther the source word si or target word tj is al-
ready in a previous entry of the combined align-
ment sequence. The resulting alignment is our ini-
tial alignment for the inference.

We also apply the MADA morphology seg-
mentation toolkit (Habash and Rambow, 2005) to
preprocess the Arabic corpus. We use the D3
scheme (each Arabic word is segmented into mor-
phemes in sequence [CONJ+ [PART+ [Al+ BASE
+PRON]]]), mark the morpheme boundaries, and
then combine the morphemes again to have words
in their original full word form. During inference,
we only sample over these morpheme boundaries
as potential word boundaries.
In this way, we
limit the search space, allowing only segmenta-
tions consistent with MADA-D3.

The inference samples 150 iterations through
the whole training set and uses the posterior prob-
ability distribution from the last iteration for de-
coding. The decoding process is then applied
to the entire training set as well as to the devel-
opment and test sets to generate a consistent to-
kenization across all three data sets. We used
the OpenFST toolkit (Allauzen et al., 2007) for
ﬁnite-state machine implementation and opera-
tions. The output of the decoding is the pre-
processed data for translation. We use the open
source Moses phrase-based MT system (Koehn et
al., 2007) to test the impact of the preprocessing
technique on translation quality.5

6.3.1 Arabic-English Translation Results

original

We consider the Arabic-English setting. We
use two baselines:
full word form
and MADA-D3 tokenization scheme for Arabic-
English translation. Table 1 compares the trans-
lation results of our segmentation methods with
these baselines. Our segmentation method shows
improvement over the two baselines on both the
development and test sets. According to Sadat
and Habash (2006), the MADA-D3 scheme per-

5The Moses translation alignment
GIZA++, not from our MCMC inference.

is the output of

822

Original
MADA-D3
Unigram
Bigram
Trigram

Dev.
59.21
58.28
59.44
58.88
58.76

Test
54.00
54.92
56.18
56.18
56.82

Table 1:
(BLEU).

Arabic-English translation results

forms best for their Arabic-English translation es-
pecially for small and moderate data sizes. In our
experiments, we see an improvement when using
the MADA-D3 preprocessing over using the orig-
inal Arabic corpus on the unseen test set, but not
on the development set.

The Gibbs sampler only samples on the mor-
phology boundary points of MADA-D3, so the
improvement resulting from our segmentation
technique does not come from removing unknown
words.
It is due to a better matching between
the source and target sentences by integrating seg-
mentation and alignment. We therefore expect the
same impact on a larger training data set in future
experiments.

6.3.2 Chinese-English Translation Results

Whole word
Character
Unigram
Trigram

Dev.
23.75
23.39
24.90
23.98

Test
29.02
27.74
28.97
28.20

Table 2: Chinese-English translation result in
BLEU score metric.

We next consider the Chinese-English setting.
The translation performance using our word seg-
mentation technique is shown in Table 2. There
are two baselines for Chinese-English translation:
(a) the source text in the full word form distributed
by the IWSLT evaluation and (b) no segmentation
of the source text, which is equivalent to interpret-
ing each Chinese character as a single word.

Taking development and test sets into account,
the best Chinese-English translation system re-
sults from our unigram model. It is signiﬁcantly

better than other systems on the development set
and performs almost equally well with the IWSLT
segmentation on the test set. Note that the seg-
mentation distributed by IWSLT is a manual seg-
mentation for the translation task.

Chung and Gildea (2009) and Xu et al. (2008)
also showed improvement over a simple mono-
lingual segmentation for Chinese-English trans-
lation. Our character-based translation result is
comparable to their monolingual segmentations.
Both trigram and unigram translation results out-
perform the character-based translation.

We also observe that there are no additional
gains for Chinese-English translation when using
a higher n-gram model. Our Gibbs sampler has
the advantage that the samples are guaranteed to
converge eventually to the model’s posterior dis-
tributions, but in each step the modiﬁcation to the
current hypothesis is small and local.
In itera-
tions 100–150, the average number of boundary
changes for the unigram model is 14K boundaries
versus only 1.5K boundary changes for the tri-
gram model. With 150 iterations, the inference
output of trigram model might not yet represent
its posterior distribution. We leave a more de-
tailed investigation of convergence behavior to fu-
ture work.

Conclusion and Future Work

We presented an unsupervised segmentation
method for machine translation and presented
experiments for Arabic-English and Chinese-
English translation tasks. The model can incor-
porate existing monolingual segmentation mod-
els and seeks to learn a segmenter appropriate for
a particular translation task (target language and
dataset).

Acknowledgements

We thank Kevin Gimpel for interesting discus-
sions and technical advice. We also thank the
anonymous reviewers for useful feedback. This
work was supported by DARPA Gale project,
NSF grants 0844507 and 0915187.

823

References
Allauzen, C., M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: A General and Efﬁcient
Weighted Finite-State Transducer Library. In Pro-
ceedings of the CIAA 2007, volume 4783 of Lecture
Notes in Computer Science, pages 11–23. Springer.
http://www.openfst.org.

Brown, Peter F., Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Comput. Linguist., 19(2):263–
311.

Chung, T. and D. Gildea. 2009. Unsupervised Tok-
enization for Machine Translation. In Proceedings
of EMNLP 2009, pages 718–726, Singapore, Au-
gust. Association for Computational Linguistics.

Cohen, S. B., D. M. Blei, and N. A. Smith. 2010. Vari-
In Pro-

ational Inference for Adaptor Grammars.
ceedings of NAACL-HLT, pages 564–572, June.

Creutz, Mathias and Krista Lagus.

2007. Unsu-
pervised Models for Morpheme Segmentation and
Morphology Learning. ACM Trans. Speech Lang.
Process., 4(1):1–34.

DeNero, J., A. Bouchard-Cˆot´e, and D. Klein. 2008.
Sampling Alignment Structure under a Bayesian
Translation Model.
In Proceedings of EMNLP
2008, pages 314–323, Honolulu, Hawaii, October.
Association for Computational Linguistics.

Dyer, C. 2009. Using a Maximum Entropy model to
build segmentation lattices for MT. In Proceedings
of HLT 2009, pages 406–414, Boulder, Colorado,
June.

Goldwater, S. and T. L. Grifﬁths.

2007. A Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. In Proceedings of ACL.

Goldwater, S. and D. McClosky. 2005. Improving Sta-
tistical Machine Translation Through Morphologi-
cal Analysis. In Proc. of EMNLP.

Goldwater, S., T. L. Grifﬁths, and M. Johnson. 2006.
Contextual Dependencies in Unsupervised Word
Segmentation. In Proc. of COLING-ACL.

Habash, N. and O. Rambow.

2005. Arabic Tok-
enization, Part-of-Speech Tagging, and Morpholog-
ical Disambiguation in One Fell Swoop. In Proc. of
ACL.

Knight, K. and Y. Al-Onaizan. 1998. Translation
with Finite-State Devices. In Proceedings of AMTA,
pages 421–437.

Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of ACL (demo session).

Matthias, E. and H. Chiori. 2005. Overview of the
IWSLT 2005 Evaluation Campaign. In Proceedings
of IWSLT.

Mochihashi, D., T. Yamada, and N. Ueda.

2009.
Bayesian Unsupervised Word Segmentation with
Nested Pitman-Yor Language Modeling.
In Pro-
ceedings of 47th ACL, pages 100–108, Suntec, Sin-
gapore, August.

Nießen, S. and H. Ney. 2004. Statistical Machine
Translation with Scarce Resources Using Morpho-
Syntactic Information. Computational Linguistics,
30(2), June.

Och, F. and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1).

Sadat, F. and N. Habash. 2006. Combination of Ara-
bic Preprocessing Schemes for Statistical Machine
Translation. In Proceedings of the ACL, pages 1–8.

Snyder, B. and R. Barzilay. 2008. Unsupervised Mul-
tilingual Learning for Morphological Segmentation.
In Proceedings of ACL-08: HLT, pages 737–745,
June.

Snyder, B., T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised Multilingual Learning for POS
Tagging. In Proceedings of EMNLP.

Snyder, B., T. Naseem, and R. Barzilay. 2009. Unsu-
pervised Multilingual Grammar Induction. In Pro-
ceedings of ACL-09, pages 73–81, August.

Teh, Y. W. 2006. A Hierarchical Bayesian Language
In Pro-

Model Based On Pitman-Yor Processes.
ceedings of ACL, pages 985–992, July.

Vogel, S., H. Ney, and C. Tillmann. 1996. HMM-
Based Word Alignment in Statistical Translation. In
Proceedings of COLING, pages 836–841.

Xu, J., J. Gao, K. Toutanova, and H. Ney.

2008.
Bayesian Semi-Supervised Chinese Word Segmen-
tation for Statistical Machine Translation.
In
Proceedings of (Coling 2008), pages 1017–1024,
Manchester, UK, August.

