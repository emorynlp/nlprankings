



















































A Transition-based Algorithm for AMR Parsing


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 366–375,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

A Transition-based Algorithm for AMR Parsing

Chuan Wang
Brandeis University

cwang24@brandeis.edu

Nianwen Xue
Brandeis University

xuen@brandeis.edu

Sameer Pradhan
Harvard Medical School
Sameer.Pradhan@

childrens.harvard.edu

Abstract

We present a two-stage framework to parse
a sentence into its Abstract Meaning Repre-
sentation (AMR). We first use a dependency
parser to generate a dependency tree for the
sentence. In the second stage, we design
a novel transition-based algorithm that trans-
forms the dependency tree to an AMR graph.
There are several advantages with this ap-
proach. First, the dependency parser can be
trained on a training set much larger than the
training set for the tree-to-graph algorithm, re-
sulting in a more accurate AMR parser over-
all. Our parser yields an improvement of 5%
absolute in F-measure over the best previous
result. Second, the actions that we design are
linguistically intuitive and capture the regular-
ities in the mapping between the dependency
structure and the AMR of a sentence. Third,
our parser runs in nearly linear time in practice
in spite of a worst-case complexity of O(n2).

1 Introduction

Abstract Meaning Representation (AMR) is a
rooted, directed, edge-labeled and leaf-labeled
graph that is used to represent the meaning of a sen-
tence. The AMR formalism has been used to anno-
tate the AMR Annotation Corpus (Banarescu et al.,
2013), a corpus of over 10 thousand sentences that
is still undergoing expansion. The building blocks
for an AMR representation are concepts and rela-
tions between them. Understanding these concepts
and their relations is crucial to understanding the
meaning of a sentence and could potentially bene-
fit a number of natural language applications such

as Information Extraction, Question Answering and
Machine Translation.

The property that makes AMR a graph instead of
a tree is that AMR allows reentrancy, meaning that
the same concept can participate in multiple rela-
tions. Parsing a sentence into an AMR would seem
to require graph-based algorithms, but moving to
graph-based algorithms from the typical tree-based
algorithms that we are familiar with is a big step in
terms of computational complexity. Indeed, quite a
bit of effort has gone into developing grammars and
efficient graph-based algorithms that can be used to
parse AMRs (Chiang et al., 2013).

want

police

The

arrest

to

Karras

in

Michael

Singapore

nsubj xcomp

det
aux

dobj

prep

nn

pobj

(a) Dependency tree

want-01

police

arrest-01

person

Singapore

name

“Michael” “Karras”

ARG0
ARG1

ARG0
ARG1

location

name

op1 op2

(b) AMR graph

Figure 1: Dependency tree and AMR graph for the
sentence, “The police want to arrest Micheal Karras
in Singapore.”

Linguistically, however, there are many similari-
ties between an AMR and the dependency structure
of a sentence. Both describe relations as holding be-
tween a head and its dependent, or between a parent
and its child. AMR concepts and relations abstract
away from actual word tokens, but there are regular-
ities in their mappings. Content words generally be-

366



come concepts while function words either become
relations or get omitted if they do not contribute to
the meaning of a sentence. This is illustrated in Fig-
ure 1, where ‘the’ and ‘to’ in the dependency tree
are omitted from the AMR and the preposition ‘in’
becomes a relation of type location. In AMR, reen-
trancy is also used to represent co-reference, but this
only happens in some limited contexts. In Figure 1,
‘police’ is both an argument of ‘arrest’ and ‘want’ as
the result of a control structure. This suggests that it
is possible to transform a dependency tree into an
AMR with a limited number of actions and learn a
model to determine which action to take given pairs
of aligned dependency trees and AMRs as training
data.

This is the approach we adopt in the present
work, and we present a transition-based framework
in which we parse a sentence into an AMR by tak-
ing the dependency tree of that sentence as input and
transforming it to an AMR representation via a se-
ries of actions. This means that a sentence is parsed
into an AMR in two steps. In the first step the sen-
tence is parsed into a dependency tree with a depen-
dency parser, and in the second step the dependency
tree is transformed into an AMR graph. One advan-
tage of this approach is that the dependency parser
does not have to be trained on the same data set as
the dependency to AMR transducer. This allows us
to use more accurate dependency parsers trained on
data sets much larger than the AMR Annotation Cor-
pus and have a more advantageous starting point.
Our experiments show that this approach is very ef-
fective and yields an improvement of 5% absolute
over the previously reported best result (Flanigan et
al., 2014) in F-score, as measure by the Smatch met-
ric (Cai and Knight, 2013).

The rest of the paper is as follows. In §2, we
describe how we align the word tokens in a sen-
tence with its AMR to create a span graph based
on which we extract contextual information as fea-
tures and perform actions. In §3, we present our
transition-based parsing algorithm and describe the
actions used to transform the dependency tree of a
sentence into an AMR. In §4, we present the learn-
ing algorithm and the features we extract to train the
transition model. In §5, we present experimental re-
sults. §6 describes related work, and we conclude in
§7.

2 Graph Representation

Unlike the dependency structure of a sentence where
each word token is a node in the dependency tree
and there is an inherent alignment between the word
tokens in the sentence and the nodes in the depen-
dency tree, AMR is an abstract representation where
the word order of the corresponding sentence is not
maintained. In addition, some words become ab-
stract concepts or relations while other words are
simply deleted because they do not contribute to
meaning. The alignment between the word tokens
and the concepts is non-trivial, but in order to learn
the transition from a dependency tree to an AMR
graph, we have to first establish the alignment be-
tween the word tokens in the sentence and the con-
cepts in the AMR. We use the aligner that comes
with JAMR (Flanigan et al., 2014) to produce this
alignment. The JAMR aligner attempts to greedily
align every concept or graph fragment in the AMR
graph with a contiguous word token sequence in the
sentence.

want-01

police

arrest-01

person

name

“Micheal” “Karras”

ARG0
ARG1

ARG0

ARG1

name

op1 op2

(a) AMR graph

s0,1:ROOT

s3,4:want-01

s2,3:police

s5,6:arrest-01

s6,8:
person+name

ARG0

ARG1

ARG0

ARG1

(b) Span graph

Figure 2: AMR graph and its span graph for the sen-
tence, “The police want to arrest Micheal Karras.”

We use a data structure called span graph to
represent an AMR graph that is aligned with the
word tokens in a sentence. For each sentence w =
w0, w1, . . . , wn, where token w0 is a special root
symbol, a span graph is a directed, labeled graph
G = (V,A), where V = {si,j |i, j ∈ (0, n) and j >
i} is a set of nodes, and A ⊆ V × V is a set of arcs.
Each node si,j of G corresponds to a continuous
span (wi, . . . , wj−1) in sentence w and is indexed
by the starting position i. Each node is assigned a
concept label from a set LV of concept labels and
each arc is assigned a relation label from a set LA

367



of relation labels, respectively.
For example, given an AMR graph GAMR in Fig-

ure 2a, its span graph G can be represented as Fig-
ure 2b. In span graph G, node s3,4’s sentence span
is (want) and its concept label is want-01, which
represents a single node want-01 in AMR. To sim-
plify the alignment, when creating a span graph out
of an AMR, we also collapse some AMR subgraphs
in such a way that they can be deterministically re-
stored to their original state for evaluation. For ex-
ample, the four nodes in the AMR subgraph that cor-
respond to span (Micheal, Karras) is collapsed into
a single node s6,8 in the span graph and assigned the
concept label person+name, as shown in Figure 3.
So the concept label set that our model predicts con-
sists of both those from the concepts in the original
AMR graph and those as a result of collapsing the
AMR subgraphs.

person

name

“Micheal” “Karras”

s6,8:person+name

name

op1 op2

Figure 3: Collapsed nodes

Representing AMR graph this way allows us to
formulate the AMR parsing problem as a joint learn-
ing problem where we can design a set of actions to
simultaneously predict the concepts (nodes) and re-
lations (arcs) in the AMR graph as well as the labels
on them.

3 Transition-based AMR Parsing

3.1 Transition System
Similar to transition-based dependency pars-
ing (Nivre, 2008), we define a transition system for
AMR parsing as a quadruple S = (S, T, s0, St),
where

• S is a set of parsing states (configurations).
• T is a set of parsing actions (transitions), each

of which is a function t : S → S.
• s0 is an initialization function, mapping each

input sentence w and its dependency tree D to
an initial state.

• St ⊆ S is a set of terminal states.

Each state (configuration) of our transition-based
parser is a triple (σ, β,G). σ is a buffer that stores
indices of the nodes which have not been processed
and we write σ = σ0|σ′ to indicate that σ0 is the top-
most element of σ. β is also a buffer [β0, β1, . . . , βj ]
and each element βi of β indicates the edge (σ0, βi)
which has not been processed in the partial graph.
We also write β = β0|β′ to indicate the topmost el-
ement of β is β0. We use span graph G to store the
partial parses for the input sentence w. Note that
unlike traditional transition-based syntactic parsers
which store partial parses in the stack structure and
build a tree or graph incrementally, here we use
σ and β buffers only to guide the parsing process
(which node or edge to be processed next) and the
actual tree-to-graph transformations are applied to
G.

When the parsing procedure starts, σ is initialized
with a post-order traversal of the input dependency
treeD with topmost element σ0, β is initialized with
node σ0’s children or set to null if σ0 is a leaf node.
G is initialized with all the nodes and edges of D.
Initially, all the nodes of G have a span length of
one and all the labels for nodes and edges are set
to null. As the parsing procedure goes on, the parser
will process all the nodes and their outgoing edges in
dependency treeD in a bottom-up left-right manner,
and at each state certain action will be applied to
the current node or edge. The parsing process will
terminate when both σ and β are empty.

The most important part of the transition-based
parser is the set of actions (transitions). As stated
in (Sartorio et al., 2013), the design space of possi-
ble actions is actually infinite since the set of pars-
ing states is infinite. However, if the problem is
amenable to transition-based parsing, we can design
a finite set of actions by categorizing all the possi-
ble situations we run into in the parsing process. In
§5.2 we show this is the case here and our action set
can account for almost all the transformations from
dependency trees to AMR graphs.

We define 8 types of actions for the actions set
T , which is summarized in Table 1. The action set
could be divided into two categories based on con-
ditions of buffer β. When β is not empty, parsing
decisions are made based on the edge (σ0, β0); oth-

368



Action Current state⇒ Result state Assign labels Precondition
NEXT EDGE-lr (σ0|σ′, β0|β′, G)⇒ (σ0|σ′, β′, G′) δ[(σ0, β0)→ lr]

β is not empty

SWAP-lr (σ0|σ′, β0|β′, G)⇒ (σ0|β0|σ′, β′, G′) δ[(β0, σ0)→ lr]
REATTACHk-lr (σ0|σ′, β0|β′, G)⇒ (σ0|σ′, β′, G′) δ[(k, β0)→ lr]
REPLACE HEAD (σ0|σ′, β0|β′, G)⇒ (β0|σ′, β = CH(β0, G′), G′) NONE
REENTRANCEk-lr (σ0|σ′, β0|β′, G)⇒ (σ0|σ′, β0|β′, G′) δ[(k, β0)→ lr]
MERGE (σ0|σ′, β0|β′, G)⇒ (σ̃|σ′, β′, G′) NONE
NEXT NODE-lc (σ0|σ1|σ′, [], G)⇒ (σ1|σ′, β = CH(σ1, G′), G′) γ[σ0 → lc] β is empty
DELETE NODE (σ0|σ1|σ′, [], G)⇒ (σ1|σ′, β = CH(σ1, G′), G′) NONE

Table 1: Transitions designed in our parser. CH(x, y) means getting all node x’s children in graph y.

erwise, only the current node σ0 is examined. Also,
to simultaneously make decisions on the assignment
of concept/relation label, we augment some of the
actions with an extra parameter lr or lc. We define
γ : V → LV as the concept labeling function for
nodes and δ : A→ LA as the relation labeling func-
tion for arcs. So δ[(σ0, β0) → lr] means assign-
ing relation label lr to arc (σ0, β0). All the actions
update buffer σ, β and apply some transformation
G ⇒ G′ to the partial graph. The 8 actions are de-
scribed below.

• NEXT-EDGE-lr (ned). This action assigns a
relation label lr to the current edge (σ0, β0)
and makes no further modification to the par-
tial graph. Then it pops out the top element of
buffer β so that the parser moves one step for-
ward to examine the next edge if it exists.

oppose

Korea

South and Israel

oppose

and

Korea

South Israel

op1

Figure 4: SWAP action

• SWAP-lr (sw). This action reverses the de-
pendency relation between node σ0 and β0 and
then makes node β0 as new head of the sub-
graph. Also it assigns relation label lr to the
arc (β0, σ0). Then it pops out β0 and inserts it
into σ right after σ0 for future revisiting. This
action is to resolve the difference in the choice
of head between the dependency tree and the
AMR graph. Figure 4 gives an example of ap-

plying SWAP-op1 action for arc (Korea, and)
in the dependency tree of sentence “South Ko-
rea and Israel oppose ...”.
• REATTACHk-lr (reat). This action removes

the current arc (σ0, β0) and reattaches node β0
to some node k in the partial graph. It also
assigns a relation label lr to the newly cre-
ated arc (k, β0) and advances one step by pop-
ping out β0. Theoretically, the choice of node
k could be any node in the partial graph un-
der the constraint that arc (k, β0) doesn’t pro-
duce a self-looping cycle. The intuition behind
this action is that after swapping a head and
its dependent, some of the dependents of the
old head should be reattached to the new head.
Figure 5 shows an example where node Israel
needs to be reattached to node and after a head-
dependent swap.

oppose

and

Korea

South Israel

op1 reattach

oppose

and

Korea

South

Israel

op1 op2

Figure 5: REATTACH action

• REPLACE-HEAD (rph). This action removes
node σ0, replaces it with node β0. Node β0 also
inherits all the incoming and outgoing arcs of
σ0. Then it pops out β0 and inserts it into the
top position of buffer σ. β is re-initialized with
all the children of β0 in the transformed graph
G′. This action targets nodes in the dependency
tree that do not correspond to concepts in AMR

369



graph and become a relation instead. An exam-
ple is provided in Figure 6, where node in, a
preposition, is replaced with node Singapore,
and in a subsequent NEXT-EDGE action that
examines arc (live, Singapore), the arc is la-
beled location.

live

in

Singapore

live

Singapore

Figure 6: REPLACE-HEAD action

• REENTRANCEk-lr (reen). This is the action
that transforms a tree into a graph. It keeps the
current arc unchanged, and links node β0 to ev-
ery possible node k in the partial graph that can
also be its parent. Similar to the REATTACH
action, the newly created arc (k, β0) should not
produce a self-looping cycle and parameter k is
bounded by the sentence length. In practice, we
seek to constrain this action as we will explain
in §3.2. Intuitively, this action can be used to
model co-reference and an example is given in
Figure 7.

want

police arrest

reentrance

want

police arrestARG0

Figure 7: REENTRANCE action

• MERGE (mrg). This action merges nodes σ0
and β0 into one node σ̃ which covers multiple
words in the sentence. The new node inher-
its all the incoming and outgoing arcs of both
nodes σ0 and β0. The MERGE action is in-
tended to produce nodes that cover a continu-
ous span in the sentence that corresponds to a
single name entity in AMR graph. see Figure 8
for an example.

arrest

Michael

Karras

arrest

Michael,Karras

Figure 8: MERGE action

When β is empty, which means all the outgoing arcs
of node σ0 have been processed or σ0 has no outgo-
ing arcs, the following two actions can be applied:

• NEXT-NODE-lc (nnd). This action first as-
signs a concept label lc to node σ0. Then it
advances the parsing procedure by popping out
the top element σ0 of buffer σ and re-initializes
buffer β with all the children of node σ1 which
is the current top element of σ. Since this action
will be applied to every node which is kept in
the final parsed graph, concept labeling could
be done simultaneously through this action.
• DELETE-NODE (dnd). This action simply

deletes the node σ0 and removes all the arcs as-
sociated with it. This action models the fact
that most function words are stripped off in the
AMR of a sentence. Note that this action only
targets function words that are leaves in the de-
pendency tree, and we constrain this action by
only deleting nodes which do not have outgo-
ing arcs.

When parsing a sentence of length n (excluding
the special root symbol w0), its corresponding de-
pendency tree will have n nodes and n − 1 arcs.
For projective transition-based dependency parsing,
the parser needs to take exactly 2n − 1 steps or ac-
tions. So the complexity is O(n). However, for
our tree-to-graph parser defined above, the actions
needed are no longer linearly bounded by the sen-
tence length. Suppose there are no REATTACH,
REENTRANCE and SWAP actions during the pars-
ing process, the algorithm will traverse every node
and edge in the dependency tree, which results in
2n actions. However, REATTACH and REEN-
TRANCE actions would add extra edges that need
to be re-processed and the SWAP action adds both
nodes and edges that need to be re-visited. Since the

370



space of all possible extra edges is (n − 2)2 and re-
visiting them only adds more actions linearly, the to-
tal asymptotic runtime complexity of our algorithm
is O(n2).

In practice, however, the number of applications
of the REATTACH action is much less than the
worst case scenario due to the similarities between
the dependency tree and the AMR graph of a sen-
tence. Also, nodes with reentrancies in AMR only
account for a small fraction of all the nodes, thus
making the REENTRANCE action occur at constant
times. These allow the tree-to-graph parser to parse
a sentence in nearly linear time in practice.

3.2 Greedy Parsing Algorithm

Algorithm 1 Parsing algorithm
Input: sentence w = w0 . . . wn and its dependency

tree Dw
Output: parsed graph Gp

1: s← s0(Dw, w)
2: while s /∈ St do
3: T ← all possible actions according to s
4: bestT ← arg maxt∈T score(t, c)
5: s← apply bestT to s
6: end while
7: return Gp

Our parsing algorithm is similar to the parser in
(Sartorio et al., 2013). At each parsing state s ∈ S,
the algorithm greedily chooses the parsing action
t ∈ T that maximizes the score function score().
The score function is a linear model defined over
parsing action t and parsing state s.

score(t, s) = ~ω · φ(t, s) (1)

where ~ω is the weight vector and φ is a function
that extracts the feature vector representation for one
possible state-action pair 〈t, s〉.

First, the algorithm initializes the state s with the
sentence w and its dependency tree Dw. At each
iteration, it gets all the possible actions for current
state s (line 3). Then, it chooses the action with the
highest score given by function score() and applies
it to s (line 4-5). When the current state reaches a
terminal state, the parser stops and returns the parsed
graph.

As pointed out in (Bohnet and Nivre, 2012), con-
straints can be added to limit the number of possible
actions to be evaluated at line 3. There could be for-
mal constraints on states such as the constraint that
the SWAP action should not be applied twice to the
same pair of nodes. We could also apply soft con-
straints to filter out unlikely concept labels, relation
labels and candidate nodes k for REATTACH and
REENTRANCE. In our parser, we enforce the con-
straint that NEXT-NODE-lc can only choose from
concept labels that co-occur with the current node’s
lemma in the training data. We also empirically set
the constraint that REATTACHk could only choose
k among σ0’s grandparents and great grandparents.
Additionally, REENTRANCEk could only choose k
among its siblings. These constraints greatly reduce
the search space, thus speeding up the parser.

4 Learning

4.1 Learning Algorithm
As stated in section 3.2, the parameter of our model
is weight vector ~ω in the score function. To train the
weight vector, we employ the averaged perceptron
learning algorithm (Collins, 2002).

Algorithm 2 Learning algorithm
Input: sentence w = w0 . . . wn, Dw, Gw
Output: ~ω

1: s← s0(Dw, w)
2: while s /∈ St do
3: T ← all possible actions according to s
4: bestT ← arg maxt∈T score(t, s)
5: goldT ← oracle(s,Gw)
6: if bestT 6= goldT then
7: ~ω ← ~ω − φ(bestT, s) + φ(goldT, s)
8: end if
9: s← apply goldT to s

10: end while

For each sentence w and its corresponding AMR
annotation GAMR in the training corpus, we could
get the dependency tree Dw of w with a dependency
parser. Then we represent GAMR as span graph
Gw, which serves as our learning target. The learn-
ing algorithm takes the training instances (w, Dw,
Gw), parses Dw according to Algorithm 1, and get
the best action using current weight vector ~ω. The

371



gold action for current state s is given by consulting
span graph Gw, which we formulate as a function
oracle() (line 5). If the gold action is equal to the
best action we get from the parser, then the best ac-
tion is applied to current state; otherwise, we update
the weight vector (line 6-7) and continue the parsing
procedure by applying the gold action.

4.2 Feature Extraction

Single node features
σ̄0.w, σ̄0.lem, σ̄0.ne, σ̄0.t, σ̄0.dl, σ̄0.len
β̄0.w, β̄0.lem, β̄0.ne, β̄0.t, β̄0.dl, β̄0.len
k̄.w, k̄.lem, k̄.ne, k̄.t, k̄.dl, k̄.len
σ̄0p.w, σ̄0p.lem, σ̄0p.ne, σ̄0p.t, σ̄0p.dl

Node pair features
σ̄0.lem+ β̄0.t, σ̄0.lem+ β̄0.dl
σ̄0.t+ β̄0.lem, σ̄0.dl + β̄0.lem
σ̄0.ne+ β̄0.ne, k̄.ne+ β̄0.ne
k̄.t+ β̄0.lem, k̄.dl + β̄0.lem
Path features
σ̄0.lem+ β̄0.lem+ pathσ0,β0
k̄.lem+ β̄0.lem+ pathk,β0
Distance features
distσ0,β0
distk,β0
distσ0,β0 + pathσ0,β0
distσ0,β0 + pathk,β0
Action specific features
β̄0.lem+ β̄0.nswp
β̄0.reph

Table 2: Features used in our parser. σ̄0, β̄0, k̄, σ̄0p
represents elements in feature context of nodes
σ0, β0, k, σ0p, separately. Each atomic feature is
represented as follows: w - word; lem - lemma; ne -
name entity; t - POS-tag; dl - dependency label; len
- length of the node’s span.

For transition-based dependency parsers, the fea-
ture context for a parsing state is represented by the
neighboring elements of a word token in the stack
containing the partial parse or the buffer containing
unprocessed word tokens. In contrast, in our tree-
to graph parser, as already stated, buffers σ and β
only specify which arc or node is to be examined
next. The feature context associated with current arc

or node is mainly extracted from the partial graph
G. As a result, the feature context is different for
the different types of actions, a property that makes
our parser very different from a standard transition-
based dependency parser. For example, when evalu-
ating action SWAP we may be interested in features
about individual nodes σ0 and β0 as well as features
involving the arc (σ0, β0). In contrast, when evaluat-
ing action REATTACHk, we want to extract not only
features involving σ0 and β0, but also information
about the reattached node k. To address this prob-
lem, we define the feature context as 〈σ̄0, β̄0, k̄, σ̄0p〉,
where each element x̄ consists of its atomic features
of node x and σ0p denotes the immediate parent of
node σ0. For elements in feature context that are not
applicable to the candidate action, we just set the el-
ement to NONE and only extract features which are
valid for the candidate action. The list of features we
use is shown in Table 2.

Single node features are atomic features concern-
ing all the possible nodes involved in each candi-
date state-action pair. We also include path features
and distance features as described in (Flanigan et al.,
2014). A path feature pathx,y is represented as the
dependency labels and parts of speech on the path
between nodes x and y in the partial graph. Here
we combine it with the lemma of the starting and
ending nodes. Distance feature distx,y is the num-
ber of tokens between two node x, y’s spans in the
sentence. Action-specific features record the his-
tory of actions applied to a given node. For exam-
ple, β̄0.nswp records how many times node β0 has
been swapped up. We combine this feature with the
lemma of node β0 to prevent the parser from swap-
ping a node too many times. β̄0.reph records the
word feature of nodes that have been replaced with
node β0. This feature is helpful in predicting rela-
tion labels. As we have discussed above, in an AMR
graph, some function words are deleted as nodes but
they are crucial in determining the relation label be-
tween its child and parent.

5 Experiments

5.1 Experiment Setting

Our experiments are conducted on the
newswire section of AMR Annotation Cor-
pus (LDC2013E117) (Banarescu et al., 2013).

372



We follow Flanigan et al. (2014) in setting up the
train/development/test splits1 for easy comparison:
4.0k sentences with document years 1995-2006
as the training set; 2.1k sentences with document
year 2007 as the development set; 2.1k sentences
with document year 2008 as the test set, and only
using AMRs that are tagged ::preferred.
Each sentence w is preprocessed with the Stanford
CoreNLP toolkit (Manning et al., 2014) to get part-
of-speech tags, name entity information, and basic
dependencies. We have verified that there is no
overlap between the training data for the Stanford
CoreNLP toolkit2 and the AMR Annotation Corpus.
We evaluate our parser with the Smatch tool (Cai
and Knight, 2013), which seeks to maximize the
semantic overlap between two AMR annotations.

5.2 Action Set Validation

One question about the transition system we pre-
sented above is whether the action set defined here
can cover all the situations involving a dependency-
to-AMR transformation. Although a formal theo-
retical proof is beyond the scope of this paper, we
can empirically verify that the action set works well
in practice. To validate the actions, we first run the
oracle() function for each sentencew and its depen-
dency tree Dw to get the “pseudo-gold” G′w. Then
we compare G′w with the gold-standard AMR graph
represented as span graph Gw to see how similar
they are. On the training data we got an overall 99%
F-score for all 〈G′w, Gw〉 pairs, which indicates that
our action set is capable of transforming each sen-
tence w and its dependency tree Dw into its gold-
standard AMR graph through a sequence of actions.

5.3 Results

Table 3 gives the precision, recall and F-score of our
parser given by Smatch on the test set. Our parser
achieves an F-score of 63% (Row 3) and the result
is 5% better than the first published result reported
in (Flanigan et al., 2014) with the same training and
test set (Row 2). We also conducted experiments on
the test set by replacing the parsed graph with gold

1A script to create the train/dev/test partitions is available at
the following URL: http://goo.gl/vA32iI

2Specifically we used CoreNLP toolkit v3.3.1 and parser
model wsjPCFG.ser.gz trained on the WSJ treebank sections
02-21.

relation labels or/and gold concept labels. We can
see in Table 3 that when provided with gold concept
and relation labels as input, the parsing accuracy im-
proves around 8% F-score (Row 6). Rows 4 and 5
present results when the parser is provided with just
the gold relation labels (Row 4) or gold concept la-
bels (Row 5), and the results are expectedly lower
than if both gold concept and relation labels are pro-
vided as input.

Precision Recall F-score
JAMR .52 .66 .58

Our parser .64 .62 .63
Our parser +lgr .68 .65 .67
Our parser +lgc .69 .67 .68
Our parser +lgrc .72 .70 .71

Table 3: Results on the test set. Here, lgc - gold
concept label; lgr - gold relation label; lgrc - gold
concept label and gold relation label.

5.4 Error Analysis

Figure 9: Confusion Matrix for actions 〈tg, t〉. Ver-
tical direction goes over the correct action type, and
horizontal direction goes over the parsed action type.

Wrong alignments between the word tokens in the
sentence and the concepts in the AMR graph ac-
count for a significant proportion of our AMR pars-
ing errors, but here we focus on errors in the tran-
sition from the dependency tree to the AMR graph.
Since in our parsing model, the parsing process has
been decomposed into a sequence of actions ap-
plied to the input dependency tree, we can use the
oracle() function during parsing to give us the cor-

373



rect action tg to take for a given state s. A compar-
ison between tg and the best action t actually taken
by our parser will give us a sense about how accu-
rately each type of action is applied. When we com-
pare the actions, we focus on the structural aspect of
AMR parsing and only take into account the eight
action types, ignoring the concept and edge labels at-
tached to them. For example, NEXT-EDGE-ARG0
and NEXT-EDGE-ARG1 would be considered to be
the same action and counted as a match when we
compute the errors even though the labels attached
to them are different.

Figure 9 shows the confusion matrix that presents
a comparison between the parser-predicted actions
and the correct actions given by oracle() func-
tion. It shows that the NEXT-EDGE (ned), NEXT-
NODE (nnd), and DELETENODE (dnd) actions ac-
count for a large proportion of the actions. These
actions are also more accurately applied. As ex-
pected, the parser makes more mistakes involving
the REATTACH (reat), REENTRANCE (reen) and
SWAP (sw) actions. The REATTACH action is of-
ten used to correct PP-attachment errors made by the
dependency parser or readjust the structure result-
ing from the SWAP action, and it is hard to learn
given the relatively small AMR training set. The
SWAP action is often tied to coordination structures
in which the head in the dependency structure and
the AMR graph diverges. In the Stanford depen-
dency representation which is the input to our parser,
the head of a coordination structure is one of the
conjuncts. For AMR, the head is an abstract con-
cept signaled by one of the coordinating conjunc-
tions. This also turns out to be one of the more dif-
ficult actions to learn. We expect, however, as the
AMR Annotation Corpus grows bigger, the parsing
model trained on a larger training set will learn these
actions better.

6 Related Work

Our work is directly comparable to JAMR (Flanigan
et al., 2014), the first published AMR parser. JAMR
performs AMR parsing in two stages: concept iden-
tification and relation identification. They treat con-
cept identification as a sequence labeling task and
utilize a semi-Markov model to map spans of words
in a sentence to concept graph fragments. For rela-

tion identification, they adopt the graph-based tech-
niques for non-projective dependency parsing. In-
stead of finding maximum-scoring trees over words,
they propose an algorithm to find the maximum
spanning connected subgraph (MSCG) over concept
fragments obtained from the first stage. In con-
trast, we adopt a transition-based approach that finds
its root in transition-based dependency parsing (Ya-
mada and Matsumoto, 2003; Nivre, 2003; Sagae
and Tsujii, 2008), where a series of actions are per-
formed to transform a sentence to a dependency tree.
As should be clear from our description, however,
the actions in our parser are very different in na-
ture from the actions used in transition-based depen-
dency parsing.

There is also another line of research that attempts
to design graph grammars such as hyperedge re-
placement grammar (HRG) (Chiang et al., 2013) and
efficient graph-based algorithms for AMR parsing.
Existing work along this line is still theoretical in
nature and no empirical results have been reported
yet.

7 Conclusion and Future Work

We presented a novel transition-based parsing algo-
rithm that takes the dependency tree of a sentence
as input and transforms it into an Abstract Mean-
ing Representation graph through a sequence of ac-
tions. We show that our approach is linguistically
intuitive and our experimental results also show that
our parser outperformed the previous best reported
results by a significant margin. In future work we
plan to continue to perfect our parser via improved
learning and decoding techniques.

Acknowledgments

We want to thank the anonymous reviewers for their
suggestions. We also want to thank Jeffrey Flanigan,
Xiaochang Peng, Adam Lopez and Giorgio Satta
for discussion about ideas related to this work dur-
ing the Fred Jelinek Memorial Workshop in Prague
in 2014. This work was partially supported by the
National Science Foundation via Grant No.0910532
entitled Richer Representations for Machine Trans-
lation. All views expressed in this paper are those
of the authors and do not necessarily represent the
view of the National Science Foundation.

374



References

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider, 2013. Abstract Meaning Representation
for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, pages 178–186. Association for Computa-
tional Linguistics.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 1455–1465.
Association for Computational Linguistics.

Shu Cai and Kevin Knight. 2013. Smatch: an evaluation
metric for semantic feature structures. In Proceedings
of the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers), pages
748–752. Association for Computational Linguistics.

David Chiang, Jacob Andreas, Daniel Bauer, Karl Moritz
Hermann, Bevan Jones, and Kevin Knight. 2013.
Parsing graphs with hyperedge replacement gram-
mars. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 924–932, Sofia, Bulgaria,
August. Association for Computational Linguistics.

Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111. Association for
Computational Linguistics.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing-Volume 10, pages 1–8. Asso-
ciation for Computational Linguistics.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris
Dyer, and Noah A. Smith. 2014. A discriminative
graph-based parser for the abstract meaning represen-
tation. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1426–1436, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.

Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Associ-
ation for Computational Linguistics.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.

Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT). Citeseer.

Joakim Nivre. 2007. Incremental non-projective de-
pendency parsing. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 396–403.
Association for Computational Linguistics.

Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513–553.

Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1-Volume
1, pages 351–359. Association for Computational Lin-
guistics.

Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce de-
pendency dag parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, pages 753–760. Association for Computa-
tional Linguistics.

Francesco Sartorio, Giorgio Satta, and Joakim Nivre.
2013. A transition-based dependency parser using a
dynamic parsing strategy. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 135–144.
Association for Computational Linguistics.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, volume 3.

375


