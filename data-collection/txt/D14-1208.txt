



















































Noisy Or-based model for Relation Extraction using Distant Supervision


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1937–1941,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Noisy Or-based model for Relation Extraction using Distant Supervision

Ajay Nagesh1,2,3
1IITB-Monash Research Academy

ajaynagesh@cse.iitb.ac.in

Gholamreza Haffari
2Faculty of IT, Monash University

gholamreza.haffari@monash.edu

Ganesh Ramakrishnan
3Dept. of CSE, IIT Bombay

ganesh@cse.iitb.ac.in

Abstract

Distant supervision, a paradigm of rela-
tion extraction where training data is cre-
ated by aligning facts in a database with a
large unannotated corpus, is an attractive
approach for training relation extractors.
Various models are proposed in recent lit-
erature to align the facts in the database
to their mentions in the corpus. In this
paper, we discuss and critically analyse a
popular alignment strategy called the “at
least one” heuristic. We provide a sim-
ple, yet effective relaxation to this strat-
egy. We formulate the inference proce-
dures in training as integer linear program-
ming (ILP) problems and implement the
relaxation to the “at least one ” heuris-
tic via a soft constraint in this formulation.
Empirically, we demonstrate that this sim-
ple strategy leads to a better performance
under certain settings over the existing ap-
proaches.

1 Introduction

Although supervised approaches to relation ex-
traction (GuoDong et al., 2005; Surdeanu and Cia-
ramita, 2007) achieve very high accuracies, they
do not scale as they are data intensive and the cost
of creating annotated data is quite high. To alle-
viate this problem, Mintz et al. (2009) proposed
relation extraction in the paradigm of distant su-
pervision. In this approach, given a database of
facts (e.g. Freebase1) and an unannotated docu-
ment collection, the goal is to heuristically align
the facts in the database to the sentences in the
corpus which contain the entities mentioned in the
fact. This is done to create weakly labeled train-
ing data to train a classifier for relation extraction.
The underlying assumption is that all mentions of

1www.freebase.com

an entity pair2 (i.e. sentences containing the en-
tity pair) in the corpus express the same relation
as stated in the database.

The above assumption is a weak one and is
often violated in natural language text. For in-
stance, the entity pair, (Barack Obama, United
States) participate in more than one relation:
citizenOf, presidentOf, bornIn and every men-
tion expresses either one of these fixed set of rela-
tions or none of them.

Consequently, a number of models have been
proposed in literature to provide better heuristics
for the mapping between the entity pair in the
database and its mentions in the sentences of the
corpus. Riedel et al. (2010) tightens the assump-
tion of distant supervision in the following man-
ner: “Given a pair of entities and their mentions in
sentences from a corpus, at least one of the men-
tions express the relation given in the database”.
In other words, it models the problem as that of
multi-instance (mentions) single-label (relation)
learning. Following this, Hoffmann et al. (2011)
and Surdeanu et al. (2012) propose models that
consider the mapping as that of multi-instance
multi-label learning. The instances are the men-
tions of the entity pair in the sentences of the cor-
pus and the entity pair can participate in more than
one relation.

Although, these models work very well in prac-
tice, they have a number of shortcomings. One
of them is the possibility that during the align-
ment, a fact in the database might not have an in-
stantiation in the corpus. For instance, if our cor-
pus only contains documents from the years 2000
to 2005, the fact presidentOf(Barack Obama,
United States) will not be present in the corpus.
In such cases, the distant supervision assumption
fails to provide a mapping for the fact in the cor-
pus.

In this paper, we address this situation with a

2In this paper we restrict ourselves to binary relations

1937



noisy-or model (Srinivas, 2013) in training the re-
lation extractor by relaxing the “at least one” as-
sumption discussed above. Our contributions in
this paper are as follows: (i) We formulate the in-
ference procedures in the training algorithm as in-
teger linear programming (ILP) problems, (ii) We
introduce a soft-constraint in the ILP objective to
model noisy-or in training, and (iii) Empirically,
our algorithm performs better than Hoffmann et
al. (2011) procedure under certain settings on two
benchmark datasets.

Our paper is organized as follows. In Section 2,
we discuss our methodology. We review the ap-
proach of Hoffmann et al. (2011) and explain our
modifications to it. In Section 3, we discuss re-
lated work. In Section 4, we discuss the experi-
mental setup and our preliminary results. We con-
clude in Section 4.

2 Methodology

Our work extends the work of Hoffmann et al.
(2011). So, we recapitulate Hoffmann’s model in
the following subsection. Following which our ad-
ditions to this model is explained in detail.

Hoffmann’s model

Hoffmann et al. (2011) present a multi-instance
multi-label model for relation extraction through
distant supervision. In this model, a pair of enti-
ties have multiple mentions (sentence containing
the entity pair) in the corpus. An entity pair can
have one or more relation labels (obtained from
the database).

Objective function

Consider an entity pair (e1, e2) denoted by the in-
dex i. The set of sentences containing the entity
pair is denoted xi and the set of relation labels for
the entity pair from the database is denoted by yi.
The mention-level labels are denoted by the latent
variable z (there is one variable zj for each sen-
tence j).

To learn the parameters θ, the training objective
to maximize is the likelihood of the facts observed
in the database conditioned on the sentences in the
text corpus.

θ∗ = arg max
θ

∏
i

Pr(yi|xi; θ)

= arg max
θ

∏
i

∑
z

Pr(yi, z|xi; θ)

The expression Pr(yi, z|xi) for a given entity
pair is defined by two types of factors in the factor
graph. They are extract factors for each mention
and mention factors between a relation label and
all the mentions.

The extract factors capture the local signal for
each mention and consists of a bunch of lexical
and syntactic features like POS tags, dependency
path between the entities and so on (Mintz et al.,
2009).

The mention factors capture the dependency be-
tween relation label and its mentions. Here, the at
least one assumption that was discussed in Section
1 is modeled. It is implemented as a simple deter-
ministic OR operator as given below:

fmention(yr, z) =
{

1 if yr is true ∧∃i : zi = r
0 otherwise

Training algorithm

The learning algorithm is a perceptron-style
parameter update scheme with 2 modifications:
i) online learning ii) Viterbi approximation. The
inference is shown to reduce to the well-known
weighted edge-cover problem which can be
solved exactly, although Hoffmann et al. (2011)
provide an approximate solution.

Algorithm 1: Hoffmann et al. (2011) : Train-
ing

Input : i) Σ: set of sentences, ii) E: set of entities
mentioned in the sentences, iii) R: set of
relation labels, iv) ∆: database of facts

Output: Extraction model : Θ
begin

for t← 1 to T ; /* training iterations */
do

for i← 1 to N ; /* No. of entity pairs */
do

ŷ, ẑ, = arg max
y,z

Pr
(
y, z
∣∣xi; Θ

)

if ŷ! = yi then
z∗ = arg max

z
Pr
(
z
∣∣yi,xi; Θ

)

Θnew = Θold+Φ(xi, z
∗)−Φ(xi, ẑ)

end

Our additions to Hoffmann’s model

In the training algorithm described above, there
are two MAP inference procedures. Our con-
tributions in this space is two-fold. Firstly, we

1938



have formulated these as ILP problems. As a re-
sult of this, the approximate inference therein is
replaced by an exact inference procedure. Sec-
ondly, we replace the deterministic-or by a noisy-
or which provides a soft-constraint instead of the
hard-constraint of Hoffmann. (“at least one” as-
sumption)

ILP formulations

Some notations:

� zji : The mention variable zj (or jth sen-
tence) taking the relation value i

� sji : Score for zj taking the value of i. Scores
are computed from the extract factors

� yi : relation label being i
� m : number of mentions (sentences) for the

given entity pair
� R: total number of relation labels (excluding

the nil label)

Deterministic OR

The following is the ILP formulation for the exact
inference arg max Pr(y, z|xi) in the model based
on the deterministic-or:

max
Z,Y

{
m∑

j=1

∑

i∈{R,nil}

[
zjisji

]}

s.t 1.
∑

i∈{R,nil}
zji = 1 ∀j

2. zji ≤ yi ∀j, ∀i

3. yi ≤
m∑

j=1

zji ∀i

where zji ∈ {0, 1}, yi ∈ {0, 1}

The first constraint restricts a mention to have
only one label. The second and third constraints
impose the at least one assumption. This is the
same formulation as Hoffmann but expressed as
an ILP problem. However, posing the inference as
an ILP allows us to easily add more constraints to
it.

Noisy OR

As a case-study, we add the noisy-or soft-
constraint in the above objective function. The
idea is to model the situation where a fact is
present in the database but it is not instantiated in
the text. This is a common scenario, as the facts
populated in the database and the text of the corpus

can come from different domains and there might
not be a very good match.

max
Z,Y,ǫ

{(
m∑

j=1

∑

i∈{R,nil}

[
zjisji

])

−
(
∑

i∈R
ǫi

)}

s.t 1.
∑

i∈{R,nil}
zji = 1 ∀j

2. zji ≤ yi ∀j, ∀i

3. yi ≤
m∑

j=1

zji + ǫi ∀i

where zji ∈ {0, 1}, yi ∈ {0, 1}, ǫi ∈ {0, 1}

In the above formulation, the objective function
is augmented with a soft penalty. Also the third
constraint is modified with this penalty term. We
call this new term ǫi and it is a binary variable to
model noise. Through this term we encourage at
least one type of configuration but will not disal-
low a configuration that does not conform to this.
Essentially, the consequence of this is to allow the
case where a fact is present in the database but is
not instantiated in the text.

3 Related Work

Relation Extraction in the paradigm of distant su-
pervision was introduced by Craven and Kum-
lien (1999). They used a biological database as
the source of distant supervision to discover rela-
tions between biological entities. The progression
of models for information extraction using distant
supervision was presented in Section 1.

Surdeanu et al. (2012) discuss a noisy-or
method for combining the scores of various sen-
tence level models to rank a relation during evalu-
ation. In our approach, we introduce the noisy-or
mechanism in the training phase of the algorithm.

Our work is inspired from previous works
like Roth and tau Yih (2004). The use of ILP
for this problem facilitates easy incorporation of
different constraints and to the best of our knowl-
edge, has not been investigated by the community.

4 Experiments

The experimental runs were carried out using the
publicly available Stanford’s distantly supervised
slot-filling system3 (Surdeanu et al., 2011) and
Hoffmann et al. (2011) code-base4.

3http://nlp.stanford.edu/software/
mimlre.shtml

4http://www.cs.washington.edu/ai/
raphaelh/mr/

1939



Datasets and Evaluation

We report results on two standard datasets used as
benchmarks by the community namely KBP and
Riedel datasets. A complete description of these
datasets is provided in Surdeanu et al. (2012).

The evaluation setup and module is the same
as that described in Surdeanu et al. (2012). We
also use the same set of features used by the var-
ious systems in the package to ensure that the ap-
proaches are comparable. As in previous work, we
report precision/recall (P/R) graphs to evaluate the
various techniques.

We used the publicly available lp solve pack-
age5 to solve our inference problems.

Performance of ILP

Use of ILP raises concerns about performance as
it is NP-hard. In our problem we solve a separate
ILP for every entity pair. The number of variables
is limited by the number of mentions for the given
entity pair. Empirically, on the KBP dataset (larger
of the two datasets), Hoffmann takes around 1hr
to run. Our ILP formulation takes around 8.5hrs.
However, MIMLRE algorithm (EM-based) takes
around 23hrs to converge.

Results

We would primarily like to highlight two settings
on which we report the P/R curves and contrast
it with Hoffmann et al. (2011). Firstly, we re-
place the approximate inference in that work with
our ILP-based exact inference; we call this set-
ting the hoffmann-ilp. Secondly, we replace the
deterministic-or in the model with a noisy-or, and
call this setting the noisy-or. We further compare
our approach with Surdeanu et al. (2012). The
P/R curves for the various techniques on the two
datasets are shown in Figures 1 and 2.

We further report the highest F1 point in the P/R
curve for both the datasets in Tables 1 and 2.

Table 1 : Highest F1 point in P/R curve : KBP Dataset
Precision Recall F1

Hoffmann 0.306451619 0.197916672 0.2405063349
MIMLRE 0.28061223 0.286458343 0.2835051518
Noisy-OR 0.297002733 0.189236104 0.2311770916
Hoffmann-ilp 0.293010741 0.189236104 0.2299577976

Discussion

We would like to discuss the results in the above
two scenarios.

5http://lpsolve.sourceforge.net/5.5/

 0.2

 0.3

 0.4

 0.5

 0.6

 0.7

 0  0.05  0.1  0.15  0.2  0.25  0.3

pr
ec

is
io

n

recall

hoffmann
noisyOr

hoffmann_ilp
mimlre

Figure 1: Results : KBP dataset

 0.2

 0.3

 0.4

 0.5

 0.6

 0.7

 0.8

 0.9

 0  0.05  0.1  0.15  0.2  0.25  0.3

pr
ec

is
io

n

recall

hoffmann
noisyOr

hoffmann_ilp
mimlre

Figure 2: Results : Riedel dataset

1. Performance of hoffmann-ilp

On the KBP dataset, we observe that
hoffmann-ilp has higher precision in the
range of 0.05 to 0.1 at lower recall (0 to 0.04).
In other parts of the curve it is very close to
the baseline (although hoffmann’s algorithm
is slightly better). In Table 1, we notice that
recall of hoffmann-ilp is lower in comparison
with hoffmann’s algorithm.

On the Riedel dataset, we observe that
hoffmann-ilp has better precision (0.15 to
0.2) than MIMLRE within recall of 0.1.
At recall > 0.1, precision drops drastically.
This is because, hoffmann-ilp predicts signif-
icantly more nil labels. However, nil labels
are not part of the label-set in the P/R curves
reported in the community. In Table 2, we see
that hoffmann-ilp has higher precision (0.04)
compared to Hoffmann’s algorithm.

2. Performance of noisy-or

1940



Table 2 : Highest F1 point in P/R curve : Riedel Dataset
Precision Recall F1

Hoffmann 0.32054795 0.24049332 0.27480916
MIMLRE 0.28061223 0.28645834 0.28350515
Noisy-OR 0.317 0.18139774 0.23075178
Hoffmann-ilp 0.36701337 0.12692702 0.18862161

In Figure 1 we see that there is a big jump
in precision (around 0.4) of noisy-or com-
pared to Hoffmann’s model in most parts of
the curve on the KBP dataset. However, in
Figure 2 (Riedel dataset), we do not see such
a trend. Although, we do perform better than
MIMLRE (Surdeanu et al., 2012) (precision
> 0.15 for recall < 0.15).

On both datasets, noisy-or has higher preci-
sion than MIMLRE, as seen from Tables 1
and 2. However, the recall reduces. More in-
vestigation in this direction is part of future
work.

5 Conclusion

In this paper we described an important addition to
Hoffmann’s model by the use of the noisy-or soft
constraint to further relax the at least one assump-
tion. Since we posed the inference procedures in
Hoffmann using ILP, we could easily add this con-
straint during the training and inference.

Empirically, we showed that the resulting P/R
curves have a significant performance boost over
Hoffmann’s algorithm as a result of this newly
added constraint. Although our system has a lower
recall when compared to MIMLRE (Surdeanu et
al., 2012), it performs competitively w.r.t the pre-
cision at low recall.

As part of immediate future work, we would
like to improve the system recall. Our ILP for-
mulation provides a good framework to add new
type of constraints to the problem. In the future,
we would like to experiment with other constraints
like modeling the selectional preferences of entity
types.

References

Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology, pages 77–86. AAAI Press.

Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting

on Association for Computational Linguistics, ACL
’05, pages 427–434, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ’11, pages 541–550,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ’09, pages 1003–1011,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Proceedings of the
2010 European conference on Machine learning
and knowledge discovery in databases: Part III,
ECML PKDD’10, pages 148–163, Berlin, Heidel-
berg. Springer-Verlag.

Dan Roth and Wen tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In In Proceedings of CoNLL-2004,
pages 1–8.

Sampath Srinivas. 2013. A generalization of the noisy-
or model. CoRR, abs/1303.1479.

Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
Proceedings of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), March.

Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D. Manning. 2011. Stanford’s
distantly-supervised slot-filling system. In Proceed-
ings of the Fourth Text Analysis Conference (TAC
2011), Gaithersburg, Maryland, USA, November.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 455–465, Stroudsburg, PA, USA.
Association for Computational Linguistics.

1941


