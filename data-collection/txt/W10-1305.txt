










































A Platform for Automated Acoustic Analysis for Assistive Technology


Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 37–43,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

A Platform for Automated Acoustic Analysis for Assistive Technology 
 
 

Suzanne Boyce Harriet Fell 
Department of Communication Sciences and 

Disorders 
College of Computer and Information 

Science 
University of Cincinnati Northeastern University 

Cincinnati, Ohio, 45267, USA Boston, Massachusetts, 02115, USA 
boycese@ucmail.uc.edu fell@ccs.neu.edu 

  
Joel MacAuslan Lorin Wilde 

Speech Technology and Applied Research Boston University 
54 Middlesex Turnpike  

Bedford, Massachusetts,  , USA Boston, Massachusetts, 02180, USA 
joelm@staranalyticalservices.com wildercom@gmail.com 

 
 

 

 

Abstract 

The use of speech production data has been 
limited by a steep learning curve and the need 
for laborious hand measurement. We are 
building a tool set that provides summary sta-
tistics for measures designed by clinicians to 
screen, diagnose or provide training to assis-
tive technology users.  This will be achieved 
by extending an existing shareware software 
platform with “plug-ins” that perform specific 
measures and report results to the user. The 
common underlying basis for this tool set is a 
Stevens’ paradigm of landmarks, points in an 
utterance around which information about ar-
ticulatory events can be extracted. 

1 Introduction 

 
To date, the use of speech production data has been 
limited by a steep learning curve and the need for 
laborious hand measurement.  Many speech-related 
studies result in voluminous acoustic data. Many 
clinicians who design and use assistive technology 
would like to incorporate acoustic analysis, but 
have been discouraged because of these technical 
challenges. We are in the process of developing a 
set of tools that considerably streamlines the proc-
ess of analyzing speech production details. 

 
We are building a tool set to provide summary sta-
tistics for measures designed by clinicians to 
screen, diagnose or provide training to patients.  
This will be achieved by extending an existing 
shareware software platform with “plug-ins” that 
perform specific measures and report results to the 
user. At present, our goal is to use the existing 
shareware software tool Wavesurfer (Wavesurfer, 
2005). The new modules will be set up to report 
data from a single audio file, or groups of audio 
files in a standard table format, for easy input to 
statistical or other analysis software.  For example, 
the data may be imported into a program that cor-
relates speech data with scalp electrode and medi-
cation data.   
 
Our tool will include alternative and independently 
tested algorithms for clinically relevant measures, 
as well as guidance as to what the speech data may 
mean.  
 
The common underlying basis for this tool set is a 
focused set of landmarks derived from Stevens’ 
Lexical Access from Features (LAFF) paradigm 
(Stevens, 1992, 2002; Liu, 1995; Slifka et al., 
2004).  In this approach, landmarks are points in an 
utterance around which information about articula-
tory events can be extracted.   
 

37



In what follows, we will describe (1) the theoreti-
cal rationale of landmarks, (2) the general utility of 
landmark processing and several examples of clin-
ically related measures, and (3) our current work 
on developing tools to make landmark analysis 
more widely available.  
   

2 Landmarks reflect articulation 

Landmark analysis is based on the fact that differ-
ent sounds produce different patterns of abrupt 
changes in the acoustic signal simultaneously 
across wide frequency ranges.   For instance, the 
abrupt increase in amplitude for a broad range of 
frequencies above 3 kHz can be used to indicate 
the onset of bursts.  Likewise, an abrupt decrease 
in the same frequency bands can be used to indi-
cate the end of frication.  The use of onset and off-
set data in other frequency bands can be used to 
indicate sonorancy; i.e., intervals when the oral 
cavity is relatively unconstricted.   Examples based 
on Liu [1995] are listed below. 

 
g(lottis): marks the onset (+g) or offset (-g) of 
voicing. 

 
s(yllabicity): marks the onset (+s) or offset (-s) of 
syllabicity, i.e. onsets and releases of voiced sono-
rant consonants such as /l/ or /r/, vocal tract clo-
sures due to voiced stop consonants such as /b/ or 
/d/. 

 
b(urst): marks the onset (+b) of the burst of air 
following stop or affricate consonant release, or the 
onset of frication noise for fricative consonants. 
Offsets (-b) mark points where aspiration or frica-
tion noise ends abruptly due to a stop closure. 

 
V(owel):  marks points of peak amplitude in a so-
norant region—that is, a region where voicing is 
evident [Howitt, 2000].   

 
Although much of the past work using landmark 
processing has been focused on employing a wide 
variety of landmarks to recognize the lexical con-
tent of speech [Juneja and Espy-Wilson 2003, Slif-
ka, et al. 2004], the power of these measures is 
even more apparent when applied to non-lexical 
attributes. 

3 Applications of Landmark Analysis to 
Assistive Technology 

3.1 Tracking Articulatory Precision 

Measuring articulatory precision is important to 
evaluating efficacy of a treatment or in monitoring 
disease progression, e.g. in Parkinson’s disease. 

 
Given that landmarks reflect articulation, tools 
based on landmarks may be useful for measuring 
and monitoring articulatory precision [Boyce et al. 
2005, 2007]. The technique relies on setting em-
pirically derived thresholds for the detection of 
abrupt acoustic changes in specified frequency 
bands.  Recall that changes in the acoustic signal 
occur simultaneously across wide frequency 
ranges. When the onset of energy does not exceed 
threshold in a particular frequency band, i.e., not 
quite abrupt enough to trigger the detection of a 
landmark, then no landmark may be assigned. 
However, since different sounds produce different 
patterns, changes detected in other bands at that 
point in time are either a) assigned to a different 
landmark, or b) considered to be extraneous. Thus, 
small acoustic differences in the way speech is 
produced can be tracked as different patterns of 
landmarks.  

 
In addition to requirements that a tool for general 
clinical use must be fast and robust, it must be able 
to handle a wide variety of speaking styles, dia-
lects, and voices.  By focusing on landmarks that 
specify syllable structure and broad phoneme 
classes, distinctive differences between phonemes 
can be ignored.  Therefore, the tool is less likely to 
break down due to problems recognizing specific 
vocabulary while remaining sensitive to changes in 
the acoustic signal that reflect articulatory preci-
sion of speech.  

3.2 Evaluating phonological complexity 

Development of speech in early infancy includes 
the ability to produce increasingly complex 
phonological structure.   Patterns of syllable struc-
ture in speech output can be tracked using land-
marks, again without reference to specific 
phonemes or words.   In Fell et al. [2002], land-
marks were grouped into standard syllable patterns 
and syllables were grouped into utterances.   Statis-

38



tics based on these patterns were then reported to 
the clinician for various uses in training, screening 
or diagnosis.  Patterns of syllable complexity were 
used to compute a "vocalization age."  This was 
used in turn to derive screening rules that clinically 
distinguish infants who may be at risk for later 
communication or other developmental problems 
from typically developing infants. 

 

3.3 Measuring and Evaluating “Clear 
Speech” 

“Clear Speech” is an intelligibility-enhancing style 
of speech that is used to improve communication 
outcomes.  Listeners with hearing impairment de-
rive significant benefit from being addressed with 
clearly articulated speech. Speech that is more 
clearly articulated contains more abrupt acoustic 
changes. The result is that speech with different 
levels of intelligibility shows different numbers 
and combinations of landmarks [Boyce et al. 2005, 
2007].  

3.4 Other Applications 

In the UCARE project [1995], Cress reported ana-
lyzing 40 hours of pre-existing [2005] videotaped 
sessions of children with physical or neurological 
impairments using landmark-based tools. 
 
Fell et al. [2004] reported using landmark analysis 
to follow the progress of several children with se-
vere speech delays.  In this project, 10-minute, in-
home audio recordings were processed in real-time 
on a 2002-era PC laptop. 
 
Wade and Möbius [2007] used automated land-
mark analysis to study speaking rate effects as a 
measure of disease progression in Parkinson's dis-
ease. 
 
DiCicco and Patel [2008] used automatic landmark 
analysis on dysarthric speech. This study provides 
quantitative support for the hypothesis [Deller 
1991] that dysarthric speech includes erroneous 
additional acoustic cues, not only malformed or 
missing ones. 

4 Potential Benefits of Landmark Appli-
cations 

In a small study, Warner-Czyz and Davis [2010] 
compared consonant–vowel syllable accuracy in 
early words of children with normal hearing and 
children with hearing loss who received cochlear 
implantation.  They found and evaluated, via man-
ual coding, approximately 4000 syllables from 48 
hours of recordings.  This is a project where auto-
matic landmark analysis might have greatly re-
duced the effort. 
 
Similarly, in a study on tongue-twisters, Matthew 
Goldrick (Northwestern University) collected 100 
hours of data comprising 20,000 tokens in less than 
three weeks, but found that it required another 600 
hours merely to segment and label the data for fur-
ther analysis.  In personal correspondence about 
another study on single words, he stated:  

A major ‘choke point’ for speech production 
research is the need to manually analyze 
speech data. Given that many thousands of 
data points are typically required to gain accu-
rate estimates of probability density functions 
along phonetic dimensions, hundreds of per-
son-hours are typically required to analyze da-
ta from a single simple experiment…. If we 
could gain access to reliable, highly accurate 
automated tools, we could change the speed of 
research by an order of magnitude. 

 
Researchers who currently want to use speech 
analysis as a tool must accept long periods of hand 
measurements. This discourages researchers who 
may be more interested in a particular neurological 
disease or process than in speech research per se.  
It is notoriously difficult to quantify projects not 
undertaken, or papers not written, but it is telling 
that, although each of the studies cited above re-
ported positive results from a study of speech ar-
ticulation, they exist as relative islands in their 
respective disciplines.  We contend that this situa-
tion exists largely because of barriers to entry; that 
is, we believe that many scientists would like to 
use speech assessment as part of their research, but 
elect not to do for lack of a convenient tool.  The 
existence of a convenient tool to detect, measure 
and track subtle changes in speech articulation 
would constitute an enabling technology.   
 

39



5 Tools 

5.1 Description 

In our own work, we have developed an automatic 
tool for detecting, counting and analyzing acoustic 
events in the speech signal that are commonly used 
by scientists to measure differences in speech ar-
ticulation. 
 
We are now integrating our system with Wave-
surfer for certain researchers (linguists, speech-
language pathologists, certain engineering and 
cognitive-science researchers) with a primary in-
terest in inspecting and interpreting the articula-
tion-related features in the waveforms of a corpus: 
e.g., the placement of landmarks of each type, pat-
terns of clustering, or identification of non-speech 
sounds to be excised.  (See Figure 1).) 
 

For this version, we are implementing user controls 
(“widgets”) to produce automated measures or 
types of analyses for speech research such as: 
• Voice-onset time, VOT. 

• Detection of non-harmonic (and harmonic) 
voicing. 

• Identification and suppression or removal of 
stray sounds, i.e., non-speech. 

• Grouping of landmarks into syllable-like clus-
ters. 

 (Note that Wavesurfer already provides a general 
pitch-tracking capability for harmonic voicing.) 
 
The Wavesurfer plug-in will also allow the user to 
output information about an audio file or a direc-
tory of audio files, e.g. all the recordings of a child. 
This information will be in a tab-delimited text file 
or a spreadsheet.  This will allow the speech scien-
tist

 

 
 
Figure 1: Wavesurfer with landmarks/waveform pane filtered to show only +/-g landmarks, and tran-
scription pane (top) with +/-g and +/-s landmarks 

40



This information will be in a tab-delimited text file 
or a spreadsheet.  This will allow the speech scien-
tist to analyze the output and, for example, to 
summarize and compare the typically developing 
children to those diagnosed with autism. 

 

5.2 User Testing 

We are currently recruiting potential users to test 
the system including graduate students and senior 
researchers in neurosciences and speech-related 
sciences.  So that these users can test the system on 
a realistic problem, we will provide them with a 
corpus of annotated, de-identified recordings of 
children with and without a diagnosis of autism.  
This will provide context for specific training tasks 
that we ask of the users and enable them to formu-
late their own appropriate, if small, research ques-
tions that the system can help to answer.  We will 
probe their experiences by logging the questions 
they have about the system, watching their actions 
as they attempt to answer the research questions, 
and asking their opinions of the experience after-
ward. 

6 Requested Features 

In an early trial of our Waversurfer plug-in, a user 
requested the VOT (voice-onset time) measure.  In 
response to this request, we are now adding a 
VOT-transcription pane to display the automati-
cally computed voice onset times aligned with the 
waveform, spectrogram, and displayed informa-
tion.  The information in this pane is also auto-
matically saved to a text file that can be analyzed 
with other software. 
 
This request also led us to include a popup window 
to show the vowel-space in a recording. Vowel-
space measures are conventionally labor-intensive, 
thus limited to a few instances of specific vowels, 
and require that the researcher first identify spe-
cific instances of these vowels.  On the other hand,  
vocalic landmarks identify the instants where for-
mant frequencies may be reliably estimated, so our 
tools can quickly and automatically evaluate the 
full vowel space of a passage.  (See Figure 2.)  
 

Figure 2: Automatic Vowel-Space Evaluation.  Com-
puting the resonant frequencies (formants) at vowel 
landmarks allows plotting the vowel space, i.e., the scat-
ter of the first two formants against each other.  In this 
case, a female read the complete Rainbow Passage (a 
standard passage of 3 paragraphs, approx. 90 sec of 
reading).  The system automatically identified all the 
consonantal and vocalic landmarks, evaluated the for-
mants at ~ 140 stressed vowels, and computed the con-
vex hull (“rubber-band”) area, 0.88 kHz2. Total 
computation time on a commodity 3 GHz PC was 143 
sec (and is directly proportional to the duration of the 
passage). 

7 Challenges for Software development, 
Challenges for availability 

Our algorithms are implemented in MATLAB.  
Though toolkits that run in MATLAB might be 
available free, or for a modest price, the MATLAB 
platform itself is costly, especially for non-
academic users. On the other hand, shareware or 
freeware may have minimal documentation; sup-
port that depends entirely on the presence (or ab-
sence!) of a knowledgeable user community; and 
variable standards for testing, correctness, and per-
formance.   

 
A critical hidden cost for any system is the learn-
ing curve.  For those systems with little documen-
tation and training, this can dwarf the overt costs.  
Our goal is to make learning easier by creating 
landmark-processing plug-ins that people can use 
within software that they already employ. 

 

41



Such a plan requires a careful balance between the 
flexibility of a general, extensible system and the 
simplicity of a small, fixed set of easily docu-
mented plug-in capabilities.  Our project therefore 
includes both a small set of simple functions, such 
as VOT, and software design centered on the needs 
identified by users from the appropriate research 
communities.  Our design relies on an iterative 
process of structured interviews and web-based 
surveys, combined with observations of user expe-
riences with our plug-ins. 

 
This user study extends beyond the matter of func-
tionality and documentation.  It also addresses the 
expectations or requirements for convenient avail-
ability, training, and support, and the costs that 
these imply. 

8 Future work 

8.1 R – statistical analysis system 

We will integrate our software with R 
(http://www.r-project.org/) for those with a pri-
mary interest instead in the derived articulatory-
precision information: e.g., syllable production 
rate, fraction of syllables of a given complexity, or 
range of vowels. 
 
For this platform, we will implement further user-
level functions, with corresponding graphical user 
interfaces as appropriate, to produce:  
• Number of landmarks, optionally excluding 

those that are automatically detected as noise-
related. 

• Syllable complexity and statistics of same. 

• Utterance complexity. 

• Syllable production rate. 

• Articulatory precision. 

• Vowel space measures. 

8.2 Other Platforms 

We plan to expand our work to include plugins or 
packages for integration with a wider (and more 
powerful) collection of research tools, for example 
PRAAT, CSL, or even Excel. 

8.3 Other Features 

We are soliciting input from user communities 
about the features they would like to see in these 
tools.   

Acknowledgments 
This work was funded in part by NIH grant R43 
DC010104. 

References  
Suzanne Boyce, Joel MacAuslan, Ann Bradlow, and 

Rajka Smiljanič. 2007. Automatic Detection of Dif-
ferences Between Clear & Conversational Speech, 
poster presented at American Speech-Language-
Hearing Convention. 

Suzanne Boyce, Ann Bradlow, and Joel MacAuslan. 
2005. Landmark analysis of clear and conversational 
speaking styles, 150th meeting of the Acoustical So-
ciety of America. 

Thomas DiCicco and Rupal Patel. 2008. Automatic 
Landmark Analysis of Dysarthric Speech, Journal of 
Medical Speech-Language Pathology, 16(4):213-
219. 

Cynthia J. Cress, S. Unrein, A. Weber, S. Krings, H. 
Fell, J. MacAuslan, and J. Gong. 2005. Vocal Devel-
opment Patterns in Children at Risk for Being Non-
speaking. ASHA 2005. 

Cynthia J. Cress. 1995. Communicative and symbolic 
precursors of AAC, Unpublished NIH CIDA Grant: 
University of Nebraska-Lincoln. 

Jack R. Deller,  D. Hsu, and Linda J. Ferrier. 1991. On 
the Use of Hidden Markov Modeling for Recognition 
of Dysarthric Speech, Computer Methods and Pro-
grams in Biomedicine. (35)2:125-139. 

Harriet J. Fell, Joel MacAuslan, Linda J. Ferrier, Susan 
G. Worst, and Karen Chenausky. 2002. Vocalization 
Age as a Clinical Tool. Procroceedings of the Inter-
national Conference on Speech and Language Proc-
essing. 

Harriet J. Fell, Joel MacAuslan, Cynthia. Cress, Linda J. 
Ferrier. 2004. visiBabble for Reinforcement of Early 
Vocalization, Proceedings of ASSETS 2004. 161-168. 

Wilson Howitt. 2000. Unpublished Ph.D. dissertation, 
Massachusetts Institute of Technology. 

Amit Juneja and Carol Espy-Wilson. 2003, Speech 
Segmentation Using Probabilistic Phonetic Feature 
Hierarchy and Support Vector Machines. Proceed-
ings of the International Joint Conference on Neural 
Networks. 

Sharlene A. Liu. 1995. Landmark Detection for Distinc-
tive Feature-Hyphen Based Speech Recognition, 
M.I.T. Doctoral Thesis. 

R, http://www.r-project.org/ 

42



Janet Slifka, Kenneth N. Stevens, Sharon Manuel, and 
Stefanie Shattuck-Hufnagel. 2004. A Landmark-
Based Model of Speech Perception: History and Re-
cent Developments. From Sound to Sense, 85-90. 

Kenneth N. Stevens, 2000. Acoustic Phonetics, The 
MIT Press, Cambridge, Massachusetts. 

Kenneth N. Stevens. 2002. Toward a model for lexical 
access based on acoustic landmarks and distinctive 
features, Journal of the Acoustic Society of America. 
111(4):1872-1891. 

Kenneth N. Stevens, Sharon Manuel, Stefanie Shattuck-
Hufnagel, and Sharlene Liu. 1992. Implementation of 
a model for lexical access based on features,  Proced-
ings ICSLP (Int. Conf. on Speech & Language Proc-
essing). 499-502. 

Travis Wade, Bernd Möbius. 2007. Speaking rate ef-
fects in a landmark-based phonetic exemplar model, 
Interspeech 2007. 402-405. 

Wavesurfer.2005. http://www.speech.kth.se/wavesurfer/  
 

43


