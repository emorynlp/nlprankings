










































Exemplar-Based Word-Space Model for Compositionality Detection: Shared Task System Description


Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 54–60,

Portland, Oregon, 24 June 2011. c©2011 Association for Computational Linguistics

Exemplar-based Word-Space Model for Compositionality Detection: Shared
task system description

Siva Reddy
University of York, UK
siva@cs.york.ac.uk

Suresh Manandhar
University of York, UK
suresh@cs.york.ac.uk

Diana McCarthy
Lexical Computing Ltd, UK

diana@dianamccarthy.co.uk

Spandana Gella
University of York, UK

spandana@cs.york.ac.uk

Abstract

In this paper, we highlight the problems of
polysemy in word space models of compo-
sitionality detection. Most models represent
each word as a single prototype-based vec-
tor without addressing polysemy. We propose
an exemplar-based model which is designed
to handle polysemy. This model is tested for
compositionality detection and it is found to
outperform existing prototype-based models.
We have participated in the shared task (Bie-
mann and Giesbrecht, 2011) and our best per-
forming exemplar-model is ranked first in two
types of evaluations and second in two other
evaluations.

1 Introduction

In the field of computational semantics, to represent
the meaning of a compound word, two mechanisms
are commonly used. One is based on the distribu-
tional hypothesis (Harris, 1954) and the other is on
the principle of semantic compositionality (Partee,
1995, p. 313).

The distributional hypothesis (DH) states that
words that occur in similar contexts tend to have
similar meanings. Using this hypothesis, distribu-
tional models like the Word-space model (WSM,
Sahlgren, 2006) represent a target word’s meaning
as a context vector (location in space). The simi-
larity between two meanings is the closeness (prox-
imity) between the vectors. The context vector of a
target word is built from its distributional behaviour
observed in a corpus. Similarly, the context vector of
a compound word can be built by treating the com-

pound as a single word. We refer to such a vector as
a DH-based vector.

The other mechanism is based on the principle of
semantic compositionality (PSC) which states that
the meaning of a compound word is a function of,
and only of, the meaning of its parts and the way
in which the parts are combined. If the meaning of
a part is represented in a WSM using the distribu-
tional hypothesis, then the principle can be applied
to compose the distributional behaviour of a com-
pound word from its parts without actually using the
corpus instances of the compound. We refer to this
as a PSC-based vector. So a PSC-based is composed
of component DH-based vectors.

Both of these two mechanisms are capable of de-
termining the meaning vector of a compound word.
For a given compound, if a DH-based vector and
a PSC-based vector of the compound are projected
into an identical space, one would expect the vec-
tors to occupy the same location i.e. both the vectors
should be nearly the same. However the principle
of semantic compositionality does not hold for non-
compositional compounds, which is actually what
the existing WSMs of compositionality detection ex-
ploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006;
Schone and Jurafsky, 2001). The DH-based and
PSC-based vectors are expected to have high simi-
larity when a compound is compositional and low
similarity for non-compositional compounds.

Most methods in WSM (Turney and Pantel, 2010)
represent a word as a single context vector built from
merging all its corpus instances. Such a representa-
tion is called the prototype-based modelling (Mur-
phy, 2002). These prototype-based vectors do not

54



distinguish the instances according to the senses of
a target word. Since most compounds are less am-
biguous than single words, there is less need for dis-
tinguishing instances in a DH-based prototype vec-
tor of a compound and we do not address that here
but leave ambiguity of compounds for future work.
However the constituent words of the compound are
more ambiguous. When DH-based vectors of the
constituent words are used for composing the PSC-
based vector of the compound, the resulting vec-
tor may contain instances, and therefore contexts,
that are not relevant for the given compound. These
noisy contexts effect the similarity between the PSC-
based vector and the DH-based vector of the com-
pound. Basing compositionality judgements on a
such a noisy similarity value is no longer reliable.

In this paper, we address this problem of pol-
ysemy of constituent words of a compound using
an exemplar-based modelling (Smith and Medin,
1981). In exemplar-based modelling of WSM (Erk
and Padó, 2010), each word is represented by all its
corpus instances (exemplars) without merging them
into a single vector. Depending upon the purpose,
only relevant exemplars of the target word are acti-
vated and then these are merged to form a refined
prototype-vector which is less-noisy compared to
the original prototype-vector. Exemplar-based mod-
els are more powerful than prototype-based ones be-
cause they retain specific instance information.

We have evaluated our models on the validation
data released in the shared task (Biemann and Gies-
brecht, 2011). Based on the validation results, we
have chosen three systems for public evaluation and
participated in the shared task (Biemann and Gies-
brecht, 2011).

2 Word Space Model

In this section, construction of WSM for all our ex-
periments is described. We use Sketch Engine1 (Kil-
garriff et al., 2004) to retrieve all the exemplars for
a target word or a pattern using corpus query lan-
guage. Let w1 w2 be a compound word with con-
stituent words w1 and w2. Ew denotes the set of
exemplars of w. Vw is the prototype vector of the
word w, which is built by merging all the exemplars
in Ew

1Sketch Engine http://www.sketchengine.co.uk

For the purposes of producing a PSC-based vector
for a compound, a vector of a constituent word is
built using only the exemplars which do not contain
the compound. Note that the vectors are sensitive
to a compound’s word-order since the exemplars of
w1 w2 are not the same as w2 w1.

We use other WSM settings following Mitchell
and Lapata (2008). The dimensions of the WSM
are the top 2000 content words in the given corpus
(along with their coarse-grained part-of-speech in-
formation). Cosine similarity (sim) is used to mea-
sure the similarity between two vectors. Values at
the specific positions in the vector representing con-
text words are set to the ratio of the probability of
the context word given the target word to the overall
probability of the context word. The context window
of a target word’s exemplar is the whole sentence of
the target word excluding the target word. Our lan-
guage of interest is English. We use the ukWaC cor-
pus (Ferraresi et al., 2008) for producing out WSMs.

3 Related Work

As described in Section 1, most WSM models for
compositionality detection measure the similarity
between the true distributional vector Vw1w2 of the
compound and the composed vector Vw1⊕w2 , where
⊕ denotes a compositionality function. If the simi-
larity is high, the compound is treated as composi-
tional or else non-compositional.

Giesbrecht (2009); Katz and Giesbrecht (2006);
Schone and Jurafsky (2001) obtained the compo-
sitionality vector of w1 w2 using vector addition
Vw1⊕w2 = aVw1 + bVw2 . In this approach, if
sim(Vw1⊕w2 , Vw1w2) > γ, the compound is clas-
sified as compositional, where γ is a threshold for
deciding compositionality. Global values of a and b
were chosen by optimizing the performance on the
development set. It was found that no single thresh-
old value γ held for all compounds. Changing the
threshold alters performance arbitrarily. This might
be due to the polysemous nature of the constituent
words which makes the composed vector Vw1⊕w2
filled with noisy contexts and thus making the judge-
ment unpredictable.

In the above model, if a=0 and b=1, the result-
ing model is similar to that of Baldwin et al. (2003).
They also observe similar behaviour of the thresh-

55



old γ. We try to address this problem by addressing
the polysemy in WSMs using exemplar-based mod-
elling.

The above models use a simple addition based
compositionality function. Mitchell and Lapata
(2008) observed that a simple multiplication func-
tion modelled compositionality better than addi-
tion. Contrary to that, Guevara (2011) observed
additive models worked well for building composi-
tional vectors. In our work, we try using evidence
from both compositionality functions, simple addi-
tion and simple multiplication.

Bannard et al. (2003); McCarthy et al. (2003) ob-
served that methods based on distributional similar-
ities between a phrase and its constituent words help
when determining the compositionality behaviour of
phrases. We therefore also use evidence from the
similarities between each constituent word and the
compound.

4 Our Approach: Exemplar-based Model

Our approach works as follows. Firstly, given a
compound w1 w2, we build its DH-based proto-
type vector Vw1w2 from all its exemplars Ew1w2 .
Secondly, we remove irrelevant exemplars in Ew1
and Ew2 of constituent words and build the refined
prototype vectors Vwr1 and Vwr2 of the constituent
words w1 and w2 respectively. These refined vec-
tors are used to compose the PSC-based vectors 2 of
the compound. Related work to ours is (Reisinger
and Mooney, 2010) where exemplars of a word are
first clustered and then prototype vectors are built.
This work does not relate to compositionality but to
measuring semantic similarity of single words. As
such, their clusters are not influenced by other words
whereas in our approach for detecting composition-
ality, the other constituent word plays a major role.

We use the compositionality functions, sim-
ple addition and simple multiplication to build
Vwr1+wr2 and Vwr1×wr2 respectively. Based on
the similarities sim(Vw1w2 , Vwr1), sim(Vw1w2 , Vwr2),
sim(Vw1w2 , Vwr1+wr2) and sim(Vw1w2 , Vwr1×wr2), we
decide if the compound is compositional or non-
compositional. These steps are described in a little
more detail below.

2Note that we use two PSC-based vectors for representing a
compound.

4.1 Building Refined Prototype Vectors

We aim to remove irrelevant exemplars of one con-
stituent word with the help of the other constituent
word’s distributional behaviour. For example, let
us take the compound traffic light. Light occurs
in many contexts such as quantum theory, optics,
lamps and spiritual theory. In ukWaC, light has
316,126 instances. Not all these exemplars are rel-
evant to compose the PSC-based vector of traffic
light. These irrelevant exemplars increases the se-
mantic differences between traffic light and light and
thus increase the differences between Vtraffic⊕light
and Vtraffic light. sim(Vlight, Vtraffic light) is found to be
0.27.

Our intuition and motivation for exemplar re-
moval is that it is beneficiary to choose only the
exemplars of light which share similar contexts of
traffic since traffic light should have contexts sim-
ilar to both traffic and light if it is compositional.
We rank each exemplar of light based on common
co-occurrences of traffic and also words which are
distributionally similar to traffic. Co-occurrences of
traffic are the context words which frequently occur
with traffic, e.g. car, road etc. Using these, the
exemplar from a sentence such as “Cameras capture
cars running red lights . . .” will be ranked higher
than one which does not have contexts related to
traffic. The distributionally similar words to traffic
are the words (like synonyms, antonyms) which are
similar to traffic in that they occur in similar con-
texts, e.g. transport, flow etc. Using these distri-
butionally similar words helps reduce the impact of
data sparseness and helps prioritise contexts of traf-
fic which are semantically related. We use Sketch
Engine to compute the scores of a word observed
in a given corpus. Sketch Engine scores the co-
occurrences (collocations) using logDice motivated
by (Curran, 2003) and distributionally related words
using (Rychlý and Kilgarriff, 2007; Lexical Com-
puting Ltd., 2007). For a given word, both of these
scores are normalised in the range (0,1)

All the exemplars of light are ranked based on
the co-occurrences of these collocations and distri-
butionally related words of traffic using

strafficE ∈ Elight =
∑
c ∈ E

xEc × ytrafficc (1)

where strafficE ∈ Elight stands for the relevance score of the

56



exemplar E w.r.t. traffic, c for context word in the
exemplar E, xEc is the coordinate value (contextual
score) of the context word c in the exemplar E and
ytrafficc is the score of the context word c w.r.t. traffic.

A refined prototype vector of light is then built by
merging the top n exemplars of light

Vlightr =
n∑

ei∈Etrafficlight ;i=0

ei (2)

where Etrafficlight are the set of exemplars of light
ranked using co-occurrence information from the
other constituent word traffic. n is chosen such that
sim(Vlightr , Vtraffic light) is maximised. This similar-
ity is observed to be greatest using just 2286 (less
than 1%) of the total exemplars of light. After ex-
emplar removal, sim(Vlightr , Vtraffic light) increased to
0.47 from the initial value of 0.27. Though n is cho-
sen by maximising similarity, which is not desirable
for non-compositional compounds, the lack of simi-
larity will give the strongest possible indication that
a compound is not compositional.

4.2 Building Compositional Vectors

We use the compositionality functions, simple ad-
dition and simple multiplication to build composi-
tional vectors Vwr1+wr2 and Vwr1×wr2 . These are as de-
scribed in (Mitchell and Lapata, 2008). In model ad-
dition, Vw1⊕w2 = aVw1 + bVw2 , all the previous ap-
proaches use static values of a and b. Instead, we use
dynamic weights computed from the participating
vectors using a = sim(Vw1w2 ,Vw1 )sim(Vw1w2 ,Vw1 )+sim(Vw1w2 ,Vw2 )
and b = 1−a. These weights differ from compound
to compound.

4.3 Compositionality Judgement

To judge if a compound is compositional or non-
compositional, previous approaches (see Section 3)
base their judgement on a single similarity value. As
discussed, we base our judgement based on the col-
lective evidences from all the similarity values using
a linear equation of the form

α(Vwr1 , Vwr2) = a0 + a1.sim(Vw1w2 , Vwr1)

+ a2.sim(Vw1w2 , Vwr2) (3)

+ a3.sim(Vw1w2 , Vwr1+wr2)

+ a4.sim(Vw1w2 , Vwr1×wr2)

Model APD Acc.
Exm-Best 13.09 88.0
Pro-Addn 15.42 76.0
Pro-Mult 17.52 80.0
Pro-Best 15.12 80.0

Table 1: Average Point Difference (APD) and Av-
erage Accuracy (Acc.) of Compositionality Judge-
ments

where the value of α denotes the compositionality
score. The range of α is in between 0-100. If α ≤
34, the compound is treated as non-compositional,
34 < α < 67 as medium compositional and α ≥
67 as highly compositional. The parameters ai’s
are estimated using ordinary least square regression
by training over the training data released in the
shared task (Biemann and Giesbrecht, 2011). For
the three categories – adjective-noun, verb-object
and subject-verb – the parameters are estimated sep-
arately.

Note that if a1 = a2 = a4 = 0, the model bases
its judgement only on addition. Similarly if a1 =
a2 = a3 = 0, the model bases its judgement only on
multiplication.

We also experimented with combinations such as
α(Vwr1 , Vw2) and α(Vw1 , Vwr2) i.e. using refined vec-
tor for one of the constituent word and the unrefined
prototype vector for the other constituent word.

4.4 Selecting the best model

To participate in the shared task, we have selected
the best performing model by evaluating the mod-
els on the validation data released in the shared task
(Biemann and Giesbrecht, 2011). Table 1 displays
the results on the validation data. The average point
difference is calculated by taking the average of the
difference in a model’s score α and the gold score
annotated by humans, over all compounds. Table 1
also displays the overall accuracy of coarse grained
labels – low, medium and high.

Best performance for verb(v)-object(o) com-
pounds is found for the combination α(Vvr , Vor) of
Equation 3. For subject(s)-verb(v) compounds, it is
for α(Vsr , Vvr) and a3 = a4 = 0. For adjective(j)-
noun(n) compounds, it is α(Vjr , Vn). We are not
certain of the reason for this difference, perhaps
there may be less ambiguity of words within specific
grammatical relationships or it may be simply due to

57



TotPrd Spearman ρ Kendalls τ
Rand-Base 174 0.02 0.02
Exm-Best 169 0.35 0.24
Pro-Best 169 0.33 0.23
Exm 169 0.26 0.18
SharedTaskNextBest 174 0.33 0.23

Table 2: Correlation Scores

the actual compounds in those categories. We leave
analysis of this for future work. We combined the
outputs of these category-specific models to build
the best model Exm-Best.

For comparison, results of standard mod-
els prototype addition (Pro-Addn) and prototype-
multiplication (Pro-Mult) are also displayed in Table
1. Pro-Addn can be represented as α(Vw1 , Vw2) with
a1 = a2 = a4 = 0. Pro-Mult can be represented as
α(Vw1 , Vw2) with a1 = a2 = a3 = 0. Pro-Best is
the best performing model in prototype-based mod-
elling. It is found to be α(Vw1 , Vw2). (Note: De-
pending upon the compound type, some of the ai’s
in Pro-Best may be 0).

Overall, exemplar-based modelling excelled in
both the evaluations, average point difference and
coarse-grained label accuracies. The systems Exm-
Best, Pro-Best and Exm α(Vwr1 , Vwr2) were submit-
ted for the public evaluation in the shared task. All
the model parameters were estimated by regression
on the task’s training data separately for the 3 com-
pound types as described in Section 4.3. We perform
the regression separately for these classes to max-
imise performance. In the future, we will investigate
whether these settings gave us better results on the
test data compared to setting the values the same re-
gardless of the category of compound.

5 Shared Task Results

Table 2 displays Spearman ρ and Kendalls τ corre-
lation scores of all the models. TotPrd stands for
the total number of predictions. Rand-Base is the
baseline system which randomly assigns a compo-
sitionality score for a compound. Our model Exm-
Best was the best performing system compared to
all other systems in this evaluation criteria. Shared-
TaskNextBest is the next best performing system
apart from our models. Due to lemmatization er-
rors in the test data, our models could only predict
judgements for 169 out of 174 compounds.

All ADJ-NN V-SUBJ V-OBJ
Rand-Base 32.82 34.57 29.83 32.34
Zero-Base 23.42 24.67 17.03 25.47
Exm-Best 16.51 15.19 15.72 18.6
Pro-Best 16.79 14.62 18.89 18.31
Exm 17.28 15.82 18.18 18.6
SharedTaskBest 16.19 14.93 21.64 14.66

Table 3: Average Point Difference Scores

All ADJ-NN V-SUBJ V-OBJ
Rand-Base 0.297 0.288 0.308 0.30
Zero-Base 0.356 0.288 0.654 0.25
Most-Freq-Base 0.593 0.673 0.346 0.65
Exm-Best 0.576 0.692 0.5 0.475
Pro-Best 0.567 0.731 0.346 0.5
Exm 0.542 0.692 0.346 0.475
SharedTaskBest 0.585 0.654 0.385 0.625

Table 4: Coarse Grained Accuracy

Table 3 displays average point difference scores.
Zero-Base is a baseline system which assigns a score
of 50 to all compounds. SharedTaskBest is the over-
all best performing system. Exm-Best was ranked
second best among all the systems. For ADJ-NN
and V-SUBJ compounds, the best performing sys-
tems in the shared task are Pro-Best and Exm-Best
respectively. Our models did less well on V-OBJ
compounds and we will explore the reasons for this
in future work.

Table 4 displays coarse grained scores. As above,
similar behaviour is observed for coarse grained ac-
curacies. Most-Freq-Base is the baseline system
which assigns the most frequent coarse-grained la-
bel for a compound based on its type (ADJ-NN, V-
SUBJ, V-OBJ) as observed in training data. Most-
Freq-Base outperforms all other systems.

6 Conclusions

In this paper, we examined the effect of polysemy
in word space models for compositionality detec-
tion. We showed exemplar-based WSM is effective
in dealing with polysemy. Also, we use multiple
evidences for compositionality detection rather than
basing our judgement on a single evidence. Over-
all, performance of the Exemplar-based models of
compositionality detection is found to be superior to
prototype-based models.

58



References

Baldwin, T., Bannard, C., Tanaka, T., and Widdows,
D. (2003). An empirical model of multiword ex-
pression decomposability. In Proceedings of the
ACL 2003 workshop on Multiword expressions:
analysis, acquisition and treatment - Volume 18,
MWE ’03, pages 89–96, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Bannard, C., Baldwin, T., and Lascarides, A. (2003).
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 work-
shop on Multiword expressions: analysis, ac-
quisition and treatment - Volume 18, MWE ’03,
pages 65–72, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Biemann, C. and Giesbrecht, E. (2011). Distri-
butional semantics and compositionality 2011:
Shared task description and results. In Pro-
ceedings of DISCo-2011 in conjunction with ACL
2011.

Curran, J. R. (2003). From distributional to semantic
similarity. Technical report, PhD Thesis, Univer-
sity of Edinburgh.

Erk, K. and Padó, S. (2010). Exemplar-based mod-
els for word meaning in context. In Proceed-
ings of the ACL 2010 Conference Short Papers,
ACLShort ’10, pages 92–97, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the WAC4 Workshop at
LREC 2008, Marrakesh, Morocco.

Giesbrecht, E. (2009). In search of semantic com-
positionality in vector spaces. In Proceedings
of the 17th International Conference on Concep-
tual Structures: Conceptual Structures: Leverag-
ing Semantic Technologies, ICCS ’09, pages 173–
184, Berlin, Heidelberg. Springer-Verlag.

Guevara, E. R. (2011). Computing semantic com-
positionality in distributional semantics. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics, IWCS ’2011.

Harris, Z. S. (1954). Distributional structure. Word,
10:146–162.

Katz, G. and Giesbrecht, E. (2006). Automatic
identification of non-compositional multi-word
expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underly-
ing Properties, MWE ’06, pages 12–19, Strouds-
burg, PA, USA. Association for Computational
Linguistics.

Kilgarriff, A., Rychly, P., Smrz, P., and Tugwell, D.
(2004). The sketch engine. In Proceedings of EU-
RALEX.

Lexical Computing Ltd. (2007). Statistics used in
the sketch engine.

McCarthy, D., Keller, B., and Carroll, J. (2003).
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis,
acquisition and treatment - Volume 18, MWE ’03,
pages 73–80, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceed-
ings of ACL-08: HLT, pages 236–244, Columbus,
Ohio. Association for Computational Linguistics.

Murphy, G. L. (2002). The Big Book of Concepts.
The MIT Press.

Partee, B. (1995). Lexical semantics and compo-
sitionality. L. Gleitman and M. Liberman (eds.)
Language, which is Volume 1 of D. Osherson (ed.)
An Invitation to Cognitive Science (2nd Edition),
pages 311–360.

Reisinger, J. and Mooney, R. J. (2010). Multi-
prototype vector-space models of word mean-
ing. In Proceedings of the 11th Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-
2010), pages 109–117.

Rychlý, P. and Kilgarriff, A. (2007). An efficient
algorithm for building a distributional thesaurus
(and other sketch engine developments). In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Ses-
sions, ACL ’07, pages 41–44, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Sahlgren, M. (2006). The Word-Space Model: Us-
ing distributional analysis to represent syntag-

59



matic and paradigmatic relations between words
in high-dimensional vector spaces. PhD thesis,
Stockholm University.

Schone, P. and Jurafsky, D. (2001). Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem? In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ’01.

Smith, E. E. and Medin, D. L. (1981). Categories
and concepts / Edward E. Smith and Douglas L.
Medin. Harvard University Press, Cambridge,
Mass. :.

Turney, P. D. and Pantel, P. (2010). From frequency
to meaning: vector space models of semantics. J.
Artif. Int. Res., 37:141–188.

60


