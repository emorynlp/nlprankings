










































Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields


Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1035–1045,
MIT, Massachusetts, USA, 9-11 October 2010. c©2010 Association for Computational Linguistics

Extracting Opinion Targets in a Single- and Cross-Domain Setting
with Conditional Random Fields

Niklas Jakob
Technische Universität Darmstadt

Hochschulstraße 10
64289 Darmstadt, Germany

Iryna Gurevych
Technische Universität Darmstadt

Hochschulstraße 10
64289 Darmstadt, Germany

http://www.ukp.tu-darmstadt.de/people

Abstract

In this paper, we focus on the opinion tar-
get extraction as part of the opinion min-
ing task. We model the problem as an in-
formation extraction task, which we address
based on Conditional Random Fields (CRF).
As a baseline we employ the supervised al-
gorithm by Zhuang et al. (2006), which rep-
resents the state-of-the-art on the employed
data. We evaluate the algorithms comprehen-
sively on datasets from four different domains
annotated with individual opinion target in-
stances on a sentence level. Furthermore, we
investigate the performance of our CRF-based
approach and the baseline in a single- and
cross-domain opinion target extraction setting.
Our CRF-based approach improves the perfor-
mance by 0.077, 0.126, 0.071 and 0.178 re-
garding F-Measure in the single-domain ex-
traction in the four domains. In the cross-
domain setting our approach improves the per-
formance by 0.409, 0.242, 0.294 and 0.343 re-
garding F-Measure over the baseline.

1 Introduction

The automatic extraction and analysis of opinions
has been approached on several levels of granular-
ity throughout the last years. As opinion mining is
typically an enabling technology for another task,
this overlaying system defines requirements regard-
ing the level of granularity. Some tasks only require
an analysis of the opinions on a document or sen-
tence level, while others require an extraction and
analysis on a term or phrase level. Amongst the
tasks which require the finest level of granularity

are: a) Opinion question answering - i.e. with ques-
tions regarding an entity as in “What do the people
like / dislike about X?”. b) Recommender systems
- i.e. if the system shall only recommend entities
which have received good reviews regarding a cer-
tain aspect. c) Opinion summarization - i.e. if one
wants to create an overview of all positive / negative
opinions regarding aspect Y of entity X and cluster
them accordingly. All of these tasks have in com-
mon that in order to fulfill them, the opinion min-
ing system must be capable of identifying what the
opinions in the individual sentences are about, hence
extract the opinion targets.

Our goal in this work is to extract opinion tar-
gets from user-generated discourse, a discourse type
which is quite frequently encountered today, due to
the explosive growth of Web 2.0 community web-
sites. Typical sentences which we encounter in this
discourse type are shown in the following examples.
The opinion targets which we aim to extract are un-
derlined in the sentences, the corresponding opinion
expressions are shown in italics.

(1) While none of the features are
earth-shattering, eCircles does provide a great
place to keep in touch.

(2) Hyundai’s more-than-modest refresh has
largely addressed all the original car’s
weaknesses while maintaining its price
competitiveness.

The extraction of opinion targets can be consid-
ered as an instance of an information extraction
(IE) task (Cowie and Lehnert, 1996). Conditional

1035



Random Fields (CRF) (Lafferty et al., 2001) have
been successfully applied to several IE tasks in
the past (Peng and McCallum, 2006). A recur-
ring problem, which arises when working with su-
pervised approaches, concerns the domain portabil-
ity. In the opinion mining context this question has
been prominently investigated with respect to opin-
ion polarity analysis (sentiment analysis) in previ-
ous research (Aue and Gamon, 2005; Blitzer et al.,
2007). Terms as “unpredictable” can express a pos-
itive opinion when uttered about the storyline of a
movie but a negative opinion when the handling of
a car is described. Hence the effects of training and
testing a machine learning algorithm for sentiment
analysis on data from different domains have been
analyzed in previous research. However to the best
of our knowledge, these effects have not been inves-
tigated regarding the extraction of opinion targets.

The contribution of this paper is a CRF-based ap-
proach for opinion targets extraction which tackles
the problem of domain portability. We first evalu-
ate our approach in three different domains against
a state-of-the art baseline system and then evaluate
the performance of both systems in a cross-domain
setting. We show that our CRF-based approach out-
performs the baseline in both settings, and how the
diffrerent combinations of features we introduce in-
fluence the results of our CRF-based approach. The
remainder of this paper is structured as follows: In
Section 2 we discuss the related work, and in Sec-
tion 3 we describe our CRF-based approach. Sec-
tion 4 comprises our experimental setup including
the description of the dataset we employ in our ex-
periments in Section 4.1 and the baseline system in
Section 4.2. The results of our experiments and their
discussion follow in Section 5. Finally we draw our
conclusions in Section 6.

2 Related Work

In the following we will discuss the related work re-
garding opinion target extraction and domain adap-
tation in opinion mining. The discussion of the re-
lated work on opinion target extraction is separated
in supervised and unsupervised approaches. We
conclude with a discussion of the related work on
domain adaptation in opinion mining.

2.1 Unsupervised Opinion Target Extraction
The first work on opinion target extraction was done
on customer reviews of consumer electronics. Hu
and Liu (2004) introduce the task of feature based
summarization, which aims at creating an overview
of the product features commented on in the re-
views. Their approach relies on a statistical analysis
of the review terms based on association mining. A
dataset of customer reviews from five domains was
annotated by the authors regarding mentioned prod-
uct features with respective opinion polarities. The
association mining based algorithm yields a preci-
sion of 0.72 and a recall of 0.80 in the extraction
of a manually selected subset of product features.
The same dataset of product reviews was used in the
work of Yi et al. (2003). They present and evalu-
ate a complete system for opinion extraction which
is based on a statistical analysis based on the Like-
lihood Ratio Test for opinion target extraction. The
Likelihood Ratio Test yields a precision of 0.97 and
1.00 in the task of opinion target (product feature)
extraction, recall values are not reported.

Popescu and Etzioni (2005) present the OPINE
system for opinion mining on product reviews.
Their algorithm is based on an information extrac-
tion system, which uses the pointwise mutual infor-
mation based on the hitcounts of a web-search en-
gine as an input. They evaluate the opinion target
extraction separately on the dataset by Hu and Liu
(2004). OPINE’s precision is on average 22% higher
than the association mining based approach, while
having an average 3% lower recall.

Bloom et al. (2007) manually create taxonomies
of opinion targets for two datasets. With a hand-
crafted set of dependency tree paths their algorithm
identifies related opinion expressions and targets.
Due to the lack of a dataset annotated with opinion
expressions and targets, they just evaluate the accu-
racy of several aspects of their algorithm by man-
ually assessing an output sample. Their algorithm
yields an accuracy of 0.75 in the identification of
opinion targets.

Kim and Hovy (2006) aim at extracting opinion
holders and opinion targets in newswire with se-
mantic role labeling. They define a mapping of the
semantic roles identified with FrameNet to the re-
spective opinion elements. As a baseline, they im-

1036



plement an approach based on a dependency parser,
which identifies the targets following the dependen-
cies of opinion expressions. They measure the over-
lap between two human annotators and their algo-
rithm as well as the baseline system. The algorithm
based on semantic role labeling yields an F-Measure
of 0.315 with annotator1 and 0.127 with annotator2,
while the baseline yields an F-Measure of 0.107 and
0.109 regarding opinion target extraction

2.2 Supervised Opinion Target Extraction
Zhuang et al. (2006) present a supervised algorithm
for the extraction of opinion expression - opinion
target pairs. Their algorithm learns the opinion tar-
get candidates and a combination of dependency and
part-of-speech paths connecting such pairs from an
annotated dataset. They evaluate their system in a
cross validation setup on a dataset of user-generated
movie reviews and compare it to the results of the Hu
and Liu (2004) system as a baseline. Thereby, the
system by Zhuang et al. (2006) yields an F-Measure
of 0.529 and outperforms the baseline which yields
an F-Measure of 0.488 in the task of extracting opin-
ion target - opinion expression pairs.

Kessler and Nicolov (2009) solely focus on iden-
tifying which opinion expression is linked to which
opinion target in a sentence. They present a dataset
of car and camera reviews in which opinion expres-
sions and opinion targets are annotated. Starting
with this information, they train a machine learn-
ing classifier for identifying related opinion expres-
sions and targets. Their algorithm receives the opin-
ion expression and opinion target annotations as in-
put during runtime. The classifier is evaluated us-
ing the algorithm by Bloom et al. (2007) as a base-
line. The support vector machine based approach
by Kessler and Nicolov (2009) yields an F-Measure
of 0.698, outperforming the baseline which yields an
F-Measure of 0.445.

2.3 Domain Adaptation in Opinion Mining
The task of creating a supervised algorithm, which
when trained on data from domain A, also performs
well on data from another domain B, is a domain
adaptation problem (Daumé III and Marcu, 2006;
Jiang and Zhai, 2007). Aue and Gamon (2005) have
investigated this challenge very early in the task of
document level sentiment classification (positive /

negative). They observe that increasing the amount
of training data raises the classification accuracy, but
only if the training data is from one source domain.
Increasing the training data by mixing domains does
not yield any consistent improvements. Blitzer et
al. (2007) introduce an extension to a structural cor-
respondence learning algorithm, which was specifi-
cally designed to address the task of domain adap-
tation. Their enhancement aims at identifying pivot
features, which are stable across domains. In a series
of experiments in document level sentiment classi-
fication they show that their extension outperforms
the original structural correspondence learning ap-
proach. In their error analysis, the authors observe
the best results were reached when the training - test-
ing combinations were Books - DVDs or Electronics
- Kitchen appliances. They conclude that the topi-
cal relatedness of the domains is an important factor.
Furthermore they observe that training the algorithm
on a smaller amount of data from a similar domain is
more effective than increasing the amount of train-
ing data by mixing domains.

3 CRF-based Approach for Opinion
Target Extraction

In the following we will describe the features we
employ as input for our CRF-based approach. As the
development data, we used 29 documents from the
movies dataset, 23 documents from the web-services
dataset and 15 documents from the cars & cameras
datasets.

Token
This feature represents the string of the current token
as a feature. Even though this feature is rather ob-
vious, it can have considerable impact on the target
extraction performance. If the vocabulary of targets
is rather compact for a certain domain (correspond-
ing to a low target type / target ratio), the training
data is likely to contain the majority of the target
types, which should hence be a good indicator. We
will refer to this feature as tk in our result tables.

POS
This feature represents the part-of-speech tag of the
current token as identified by the Stanford POS Tag-
ger1. It can provide some means of lexical disam-

1http://nlp.stanford.edu/software/tagger.shtml

1037



biguation, e.g. indicate that the token “sounds” is
a noun and not a verb in a certain context. At the
same time, the CRF algorithm is provided with ad-
ditional information to extract opinion targets which
are multiword expressions, i.e. noun combinations.
We will refer to this feature as pos in our result ta-
bles.

Short Dependency Path
Previous research has successfully employed paths
in the dependency parse tree to link opinion expres-
sions and the corresponding targets (Zhuang et al.,
2006; Kessler and Nicolov, 2009). Both works iden-
tify direct dependency relations such as “amod” and
“nsubj” as the most frequent and at the same time
highly accurate connections between a target and an
opinion expression. We hence label all tokens which
have a direct dependency relation to an opinion ex-
pression in a sentence. The Stanford Parser2 is em-
ployed for the constituent and dependency parsing.
We will refer to this feature as dLn in our result ta-
bles.

Word Distance
From the work of Zhuang et al. (2006) we can infer
that opinion expressions and their target(s) are not
always connected via short paths in the dependency
parse tree. Since we cannot capture such paths with
the abovementioned feature we introduce another
feature which acts as heuristic for identifying the
target to a given opinion expression. Hu and Liu
(2004) and Yi et al. (2003) have shown that (base)
noun phrases are good candidates for opinion targets
in the datasets of product reviews. We therefore la-
bel the token(s) in the closest noun phrase regarding
word distance to each opinion expression in a sen-
tence. We will refer to this feature as wrdDist in our
result tables.

Opinion Sentence
With this feature, we simply label all tokens occur-
ring in a sentence containing an opinion expression.
This feature shall enable the CRF algorithm to
distinguish between the occurence of a certain
token in a sentence which contains an opinion vs. a
sentence without an opinion. We will refer to this
feature as sSn in our result tables.

2http://nlp.stanford.edu/software/lex-parser.shtml

Our goal is to extract individual instances of opinion
targets from sentences which contain an opinion
expression. This can be modeled as a sequence
segmentation and labeling task. The CRF algorithm
receives a sequence of tokens t1...tn for which
it has to predict a sequence of labels l1...ln. We
represent the possible labels following the IOB
scheme: B-Target, identifying the beginning of an
opinion target, I-Target identifying the continuation
of a target, and O for other (non-target) tokens. We
model the sentences as a linear chain CRF, which
is based on an undirected graph. In the graph, each
node corresponds to a token in the sentence and
edges connect the adjacent tokens as they appear in
the sentence. In our experiments, we use the CRF
implementation from the Mallet toolkit3.

4 Experimental Setup

4.1 Datasets
In our experiments, we employ datasets from three
different sources, which span four domains in total
(see Table 1). All of them consist of reviews col-
lected from Web 2.0 sites. The first dataset con-
sists of reviews for 20 different movies collected
from the Internet Movie Database. It was presented
in Zhuang et al. (2006) and annotated regarding
opinion target - opinion expression pairs. The sec-
ond dataset consists of 234 reviews for two different
web-services collected from epinions.com, as de-
scribed in Toprak et al. (2010). The third dataset is
an extended version of the data presented in Kessler
and Nicolov (2009). The authors have provided us
with additional documents, which have been anno-
tated in the meantime. The version of the dataset
used in our experiments consists of 179 blog post-
ings regarding different digital cameras and 336 re-
views of different cars. In the description of their
annotation guidelines, Kessler and Nicolov (2009)
refer to opinion targets as mentions. Mentions are
all aspects of the review topic, which can be targets
of expressed opinions. However, not only mentions
which occur as opinion targets were originally anno-
tated, but also mentions which occur in non-opinion
sentences. In our experiments, we only use the men-
tions which occur as targets of opinion expressions.

3http://mallet.cs.umass.edu/

1038



All three datasets contain annotations regarding
the antecedents of anaphoric opinion targets. In our
experimental setup, we do not require the algorithms
to also correctly resolve the antecedent of an opin-
ion target representy by a pronoun, as we are solely
interested in evaluating the opinion target extraction
not any anaphora resolution.

As shown in rows 4 and 5 of Table 1, the docu-
ments from the cars and the cameras datasets exhibit
a much higher density of opinions per document.
53.5% of the sentences from the cars dataset contain
an opinion and in the cameras dataset even 56.1%
of the sentences contain an opinion, while in the
movies and the web-services reviews just 22.1% and
22.4% of the sentences contain an opinion. Further-
more in the cars and the cameras datasets the lexical
variability regarding the opinion targets is substan-
tially larger than in the other two datasets: We calcu-
late target types by counting the number of distinct
opinion targets in a dataset. We divide this by the
sum of all opinion target instances in the dataset. For
the cars dataset this ratio is 0.440 and for the cam-
eras dataset it is 0.433, while for the web-services
dataset it is 0.306 and for the movies dataset only
0.122. In terms of reviews this means, that in the
movie reviews the same movie aspects are repeat-
edly commented on, while in the cars and the cam-
eras datasets many different aspects of these entities
are discussed, which in turn each occur infrequently.

Table 1: Dataset Statistics
movies web- cars camerasservices

Documents 1829 234 336 179
Sentences 24555 6091 10969 5261
Tokens /

20.3 17.5 20.3 20.4
sentence

Sentences with
21.4% 22.4% 51.1% 54.0%

target(s)
Sentences with

21.4% 22.4% 53.5% 56.1%
opinion(s)

Targets 7045 1875 8451 4369
Target types 865 574 3722 1893

Tokens / target 1.21 1.35 1.29 1.42
Avg. targets /

1.33 1.37 1.51 1.53
opinion sent.

4.2 Baseline System
In the task of opinion target extraction the super-
vised algorithm by Zhuang et al. (2006) represents
the state-of-the-art on the movies dataset we also
employ in our experiments. We therefore use it as
a baseline. The algorithm learns two aspects from
the labeled training data:

1. A set of opinion target candidates

2. A set of paths in a dependency tree which iden-
tify valid opinion target - opinion expression
pairs

In our experiments, we learn the full set of opin-
ion targets from the labeled training data in the first
step. This is slightly different from the approach
in (Zhuang et al., 2006), but we expect that this mod-
ification should be beneficial for the overall perfor-
mance in terms of recall, as we do not remove any
learned opinion targets from the candidate list. In
the second step, the annotated sentences are parsed
and a graph containing the words of a sentence is
created, which are connected by the dependency re-
lations between them. For each opinion target -
opinion expression pair from the gold standard, the
shortest path connecting them is extracted from the
dependency graph. A path consists of the part-of-
speech tags of the nodes and the dependency types
of the edges. Example 3 shows a typical dependency
path.

(3) NN - nsubj - NP - amod - JJ

During runtime, the algorithm identifies opinion tar-
gets from the candidate list in the training data. The
opinion expressions are directly taken from the gold
standard, as we focus on the opinion target extrac-
tion aspect in this work. The sentences are then
parsed and if a valid path between a target and
an opinion expression is found in the list of possi-
ble paths, then the pair is extracted. Since the de-
pendency paths only identify pairs of single word
target and opinion expression candidates, we em-
ploy a merging step. Extracted target candidates are
merged into a multiword target if they are adjacent
in a sentence. Thereby, the baseline system is also
capable of extracting multiword opinion targets.

1039



4.3 Metrics
We employ the following requirements in our eval-
uation of the opinion target extraction: An opin-
ion target must be extracted with exactly the span
boundaries as annotated in the gold standard. This
is especially important regarding multiword tar-
gets. Extracted targets which partially overlap with
the annotated gold standard are counted as errors.
Hence a target extracted by the algorithm which
does not exactly match the boundaries of a target
in the gold standard is counted as a false positive
(FP), e.g. if “battery life” is annotated as the tar-
get in the gold standard, only “battery” or “life”
extracted as targets will be counted as FPs. Exact
matches between the targets extracted by the algo-
rithm and the gold standard are true positives (TP).
We refer to the number of annotated targets in the
gold standard as TGS . Precision is calculated as
Precision = TPTP+FP , and recall is calculated as
Recall = TPTGS . F-Measure is the harmonic mean of
precision and recall.

5 Results and Discussion

We investigate the performance of the baseline and
the CRF-based approach for opinion target extrac-
tion in a single- and cross-domain setting. The
single-domain approach assumes that there is a set
of training data available for the same domain as
the domain the algorithm is being tested on. In this
setup, we will both run the baseline and our CRF
based system in a 10-fold cross-validation and report
results macro averaged over all runs. In the cross-
domain approach, we will investigate how the algo-
rithm performs if given training data from domain A
while being tested on another domain B. In this set-
ting, we will train the algorithm on the entire dataset
A, and test it on the entire dataset B, we hence report
one micro averaged result set. In Subsection 5.1 we
present the results of both the baseline system and
our CRF-based approach in the single-domain set-
ting, in Subsection 5.2 we present the results of the
two systems in the cross-domain opinion target ex-
traction.

Table 2: Single-Domain Extraction with Zhuang Baseline
Dataset Precision Recall F-Measure
movies 0.663 0.592 0.625

web-services 0.624 0.394 0.483
cars 0.259 0.426 0.322

cameras 0.423 0.431 0.426

5.1 Single-Domain Results
5.1.1 Zhuang Baseline

As shown in Table 2, the state-of-the-art algo-
rithm of Zhuang et al. (2006) performs best on the
movie review dataset and worst on the cars dataset.
The results on the movie dataset are higher than
originally reported in (Zhuang et al., 2006) (Preci-
sion 0.483, Recall 0.585, F-Measure 0.529). We as-
sume that this is due to two reasons: 1. In our task,
the algorithm uses the opinion expression annotation
from the gold standard. 2. We do not remove any
learned opinion target candidates from the training
data (See Section 4.2).

During training we observed that for each dataset
the lists of possible dependency paths (see Exam-
ple 3) contained several hundred entries, many of
them only occurring once. We assume that the re-
call of the algorithm is limited by a large variety
of possible dependency paths between opinion tar-
gets and opinion expressions, since the algorithm
cannot link targets and opinion expressions in the
testing data if there is no valid candidate depen-
dency path. Furthermore, we observe that for the
cars dataset the size of the dependency path candi-
date list (6642 entries) was approximately five times
larger than the dependency graph candidate list for
the web-services dataset (1237 entries), which has a
comparable size regarding documents. At the same
time, the list of target candidates of the cars dataset
was approximately eight times larger than the tar-
get candidate list for the web-services dataset. We
assume that a large number of both the target can-
didates as well as the dependency path candidates
introduces many false positives during the target ex-
traction, hence lowering the precision of the algo-
rithm on the cars dataset considerably.

1040



Table 3: Single-Domain Extraction with our CRF-based Approach
movies web-services cars cameras

Features Prec Rec F-Me Prec Rec F-Me Prec Rec F-Me Prec Rec F-Me
tk, pos 0.639 0.133 0.220 0.500 0.051 0.093 0.438 0.110 0.175 0.300 0.085 0.127
tk, pos, wDs 0.542 0.181 0.271 0.451 0.272 0.339 0.570 0.354 0.436 0.549 0.375 0.446
tk, pos, dLn 0.777 0.481 0.595 0.634 0.380 0.475 0.603 0.372 0.460 0.569 0.376 0.453
tk, pos, sSn 0.673 0.637 0.653 0.604 0.397 0.476 0.453 0.180 0.257 0.398 0.172 0.238
tk, pos, dLn, wDs 0.792 0.481 0.598 0.620 0.354 0.450 0.603 0.389 0.473 0.596 0.425 0.496
tk, pos, sSn, wDs 0.662 0.656 0.659 0.664 0.461 0.544 0.564 0.370 0.446 0.544 0.381 0.447
tk, pos, sSn, dLn 0.791 0.477 0.594 0.654 0.501 0.568 0.598 0.384 0.467 0.586 0.391 0.468
tk, pos, sSn, dLn, wDs 0.749 0.661 0.702 0.722 0.526 0.609 0.622 0.414 0.497 0.614 0.423 0.500
pos, sSn, dLn, wDs 0.672 0.441 0.532 0.612 0.322 0.422 0.612 0.369 0.460 0.674 0.398 0.500

5.1.2 Our CRF-based Approach
Table 3 shows the results of the opinion target ex-

traction using the CRF algorithm. Row 8 contains
the results of the feature configuration, which yields
the best performance regarding F-Measure across all
datasets. We observe that our aproach outperforms
the Zhuang baseline on all datasets. The gain in F-
Measure is between 0.077 in the movies domain and
0.175 in the cars domain. Although the CRF-based
approach clearly outperforms the baseline system
on all four datasets, we also observe the same gen-
eral trend regarding the individual results: The CRF
yields the best results on the movies dataset and the
worst results on the cars & cameras dataset.

As shown in the first row, the results when using
just the token string and part-of-speech tags as fea-
tures are very low, especially regarding recall. We
observe that the higher the lexical variability of the
opinion targets is in a dataset, the lower the results
are. If we add the feature based on word distance
(row 2), the recall is improved on all datasets, while
the precision is slightly lowered on the movies and
web-services datasets. The dependency path based
feature performs better compared to the word dis-
tance heuristic as shown in row 3. The precision is
considerably increased on all datasets, on the movies
and cars & cameras datasets even reaching the over-
all highest value. At the same time, we observe
an increase of recall on all datasets. The obser-
vation made in previous research that short paths
in the dependency graph are a high precision indi-
cator of related opinion expressions - opinion tar-
gets (Kessler and Nicolov, 2009) is confirmed on all
datasets. Adding the information regarding opinion
sentences to the basic features of the token string and

the part-of-speech tag (row 4) yields the biggest im-
provements regarding F-Measure on the movies and
web-services dataset (+0.433 / +0.383). On the cars
& cameras dataset the recall is relatively low again.
We assume that this is again due to the high lexical
variability, so that the CRF algorithm will encounter
many actual opinion targets in the testing data which
have not occurred in the training data and will hence
not be extracted.

As shown in row 5, if we combine the dependency
graph based feature with the word distance heuris-
tic, the results regarding F-Measure are consistently
higher than the results of these features in isolation
(rows 2 - 4) on all datasets. We conclude that these
two features are complementary, as they apparently
indicate different kinds of opinion targets which are
then correctly extracted by the CRF. If we combine
each of the opinion expression related features with
the label which identifies opinion sentences in gen-
eral (rows 6 & 7), we observe that this feature is
also complementary to the others. On all datasets the
results regarding F-Measure are consistently higher
compared to the features in isolation (rows 2 - 4).
Row 8 shows the results of all features in combina-
tion. Again, we observe the complementarity of the
features, as the results of this feature combination
are the best regarding F-Measure across all datasets.

In row 9 of the results, we exclude the token
string as a feature. In comparison to the full fea-
ture combination of row 8 we observe a significant
decrease of F-Measure on the movies and the web-
services dataset. On the cars dataset we only observe
a slight decrease of recall. Interestingly on the cam-
eras dataset we even observe a slight increase of pre-
cision which compensates a slight decrease of recall,

1041



in turn resulting in stable F-Measure of 0.500 as in
the full feature set of row 8.

We have run some additional experiments in
which we did not rely on the annotated opinion ex-
pressions, but employed a general pupose subjectiv-
ity lexicon4. Already in the single-domain extrac-
tion, we observed that the results declined substan-
tially (e.g. web-services F-Measure: 0.243, movies
F-Measure: 0.309, cars F-Measure: 0.192 and cam-
eras F-Measure: 0.198).

We performed a quantitative error analysis on the
results of the CRF-based approach in the single-
domain setting. In doing so, we focused on misclas-
sifications of B-Target and I-Target instances, as the
recall is consistently lower than the precision across
all datasets. We observe that most of the recall errors
result from one-word opinion targets or the begin-
ning of opinion targets (B-Targets) being missclassi-
fied as non-targets (movies 83%, web-services 73%,
cars 68%, cameras 64%). For the majority of these
missclassifications neither the dLn nor the wDs fea-
tures were present (movies 82%, web-services 56%,
cars 64%, cameras 61%). We assume that our fea-
tures cannot capture the structure of more complex
sentences very well. Our results indicate that the
dLn and wDs features are complementary, but appar-
ently there are quite a few cases in which the opin-
ion target is neither directly related to the opinion
expression in the dependency graph nor close to it
in the sentence. One of these sentences, in this case
from a camera review, in shown in Example 4.

(4) A lens cap and a strap may not sound very
important, but it makes a huge difference in the
speed and usability of the camera.

In this sentence, the dLn and wDs features both la-
beled “speed” which was incorrectly extracted as the
target of the opinion. None of the actual targets “lens
cap”, “strap” and “camera” have a short dependency
path to the opinion expression and “speed” is sim-
ply the closest noun to it. Note that although both
“speed” and “usability” are attributes of a camera,
the opinion in this sentence is about the “lens cap”
and “strap”, hence only these attributes are anno-
tated as targets.

4http://www.cs.pitt.edu/mpqa/

5.2 Cross-Domain Results
5.2.1 Zhuang Baseline

Table 4 shows the results of the opinion target ex-
traction with the state-of-the-art system in the cross-
domain setting. We observe that the results on all
domain combinations are very low. A quantitative
error analysis has revealed that there is hardly any
overlap in the opinion target candidates between do-
mains, as reflected by the low recall in all config-
urations. The vocabularies of the opinion targets
are too different, hence the performance of the algo-
rithm by Zhuang et al. (2006) is so low. The overlap
regarding the dependency paths between domains
was however higher. Especially identical short paths
could be found across domains which at the same
time typically occured quite often. For future work
it might be interesting to investigate how the algo-
rithm by Zhuang et al. (2006) performs in the cross-
domain setting if the target candidate learning is per-
formed differently, e.g. with a statistical approach as
outlined in Section 2.1.

5.2.2 CRF-based Approach
The results of the cross-domain target extraction

with the CRF-based algorithm are shown in Table 5.
Due to the increase of system configurations intro-
duced by the training - testing data combinations,
we had to limit results of the feature combinations
reported in the Table. The feature combination pos,
sSn, wDs, dLn yielded the best results regarding F-
Measure. Hence, we report its result as the basic fea-
ture set. When comparing the results of the best per-
forming feature / training data combination of our
CRF-based approach with the baseline, we observe
that our approach outperforms the baseline on all
four domains. The gain in F-Measure is 0.409 in the
movies domain, 0.242 in the web-services domain,
0.294 in the cars domain and 0.343 in the cameras
domain.

Effects of Features
Interestingly with the best performing feature com-
bination from the single-domain extraction, the re-
sults regarding recall in the cross-domain extraction
are very low5. This is due to the fact that the CRF at-
tributed a relatively large weight to the token string

5Not shown in any result table due to limited space.

1042



Table 4: Cross-Domain Extraction with Zhuang Baseline
Training Testing Precision Recall F-Measure

web-services movies 0.194 0.032 0.055
cars movies 0.032 0.034 0.033

cameras movies 0.155 0.084 0.109
cars + cameras movies 0.071 0.104 0.084

web-services + cars + cameras movies 0.070 0.103 0.083
movies web-services 0.311 0.073 0.118

cars web-services 0.086 0.091 0.089
cameras web-services 0.164 0.081 0.108

cars + cameras web-services 0.086 0.104 0.094
movies + cars + cameras web-services 0.074 0.100 0.080

movies cars 0.182 0.014 0.026
web-services cars 0.218 0.028 0.049

cameras cars 0.250 0.121 0.163
cameras + web-services cars 0.247 0.131 0.171
movies + web-services cars 0.246 0.045 0.076

movies cameras 0.108 0.012 0.022
web-services cameras 0.268 0.048 0.082

cars cameras 0.125 0.160 0.140
cars + web-services cameras 0.119 0.157 0.136

movies + web-services cameras 0.245 0.063 0.100

feature. As we also observed in the analysis of the
baseline results, the overlap of the opinion target vo-
cabularies between domains is low, which resulted
in a very small number of targets extracted by the
CRF. As shown in Table 5 the results are promising
regarding F-Measure if we just leave the token fea-
ture out of the configuration.

Effects of Training Data
When analyzing the results of the different training
- testing domain configurations we observe the fol-
lowing: In isolation training data from the cameras
domain consistently yields the best results regarding
F-Measure when the algorithm is run on the datasets
from the other three domains. This is particularly
interesting since the cameras dataset is the smallest
of the four (see Table 1). We investigated whether
the CRF algorithm was overfitting to the training
datasets by reducing their size to the size of the cam-
eras dataset. However, the reduction of the train-
ing data sizes never improved the extraction results
regarding F-Measure for the movies, web-serviecs
and cars datasets. The good results when training
on the cameras dataset are in line with our obser-
vations from Section 5.1.2. We noticed that on the
cameras dataset the results regarding F-Measure re-
mained stable if the token feature is not used in the
training.

In isolation, training only on the cars data yields
the second highest results on the movies and web-
services datasets and the highest results regarding
F-Measure on the cameras data. However, the re-
sults of the cars + cameras training data combination
indicate that the cameras data does not contribute
any additional information during the learning, since
the results on both the movies and the web-services
datasets are lower than when training only on the
cameras data.

Our results also confirm the insights gained
by Blitzer et al. (2007), who observed that in cross-
domain polarity analysis adding more training data
is not always beneficial. Apparently even the small-
est training dataset (cameras) contain enough feature
instances to learn a model which performs well on
the testing data.

We observe that the results of the cross-domain
extraction regarding F-Measure come relatively
close to the results of the single-domain setting, es-
pecially if the token string feature is removed there
(see Table 3 row 9). On the cars and the cameras
dataset the cross-domain results are even closer to
the single-domain results. The features we employ
seem to scale well across domains and compensate
the difference between training and testing data and
the lack of information regarding the target vocabu-

1043



Table 5: Cross-Domain Extraction with our CRF-based Approach
Testing

web-services movies
Pre Rec F-Me Pre Rec F-Me

Tr
ai

ni
ng

web-services - - - 0.560 0.339 0.422
movies 0.565 0.219 0.316 - - -

cars 0.538 0.248 0.340 0.642 0.382 0.479
cameras 0.529 0.256 0.345 0.642 0.408 0.499

movies + cars 0.554 0.249 0.344 - - -
movies + cameras 0.530 0.273 0.360 - - -

movies + cars + cameras 0.562 0.250 0.346 - - -
cars + cameras 0.538 0.254 0.345 0.641 0.395 0.489

web-services + cars - - - 0.651 0.396 0.492
web-services + cameras - - - 0.642 0.435 0.518

web-services + cars + cameras - - - 0.639 0.405 0.496
cars cameras

Pre Rec F-Me Pre Rec F-Me
web-services 0.391 0.277 0.324 0.505 0.330 0.399

movies 0.512 0.307 0.384 0.550 0.303 0.391
cars - - - 0.665 0.369 0.475

cameras 0.589 0.384 0.465 - - -
cameras + movies 0.567 0.394 0.465 - - -

cameras + web-services 0.572 0.381 0.457 - - -
movies + web-services 0.489 0.327 0.392 0.553 0.339 0.421

movies + cars - - - 0.634 0.376 0.472
web-services + cars - - - 0.678 0.376 0.483

web-services + movies + cars - - - 0.635 0.378 0.474
movies + web-services + cameras 0.549 0.381 0.450 - - -

lary.

6 Conclusions

In this paper, we have shown how a CRF-based
approach for opinion target extraction performs in
a single- and cross-domain setting. We have pre-
sented a comparative evaluation of our approach
on datasets from four different domains. In the
single-domain setting, our CRF-based approach out-
performs a supervised baseline on all four datasets.
Our error analysis indicates that additional features,
which can capture opinions in more complex sen-
tences, are required to improve the performance of
the opinion target extraction. Our CRF-based ap-
proach also yields promising results in the cross-
domain setting. The features we employ scale well
across domains, given that the opinion target vocab-
ularies are substantially different. For future work,
we might investigate how machine learning algo-
rithms, which are specifically designed for the prob-
lem of domain adaptation (Blitzer et al., 2007; Jiang
and Zhai, 2007), perform in comparison to our ap-
proach. Since three of the features we employed in

our CRF-based approach are based on the respec-
tive opinion expressions, it is to investigate how to
mitigate the possible negative effects introduced by
errors in the opinion expression identification if they
are not annotated in the gold standard. We observe
similar challenges as Choi et al. (2005) regarding the
analysis of complex sentences. Although our data is
user-generated from Web 2.0 communities, a man-
ual inspection has shown that the documents were
of relatively high textual quality. It is to investigate
to which extent the approaches taken in the analysis
of newswire, such as identifying targets with coref-
erence resolution, can also be applied to our task on
user-generated discourse.

Acknowledgments

The project was funded by means of the German Fed-
eral Ministry of Economy and Technology under the
promotional reference “01MQ07012”. The authors take
the responsibility for the contents. This work has been
supported by the Volkswagen Foundation as part of
the Lichtenberg-Professorship Program under grant No.
I/82806.

1044



References
Anthony Aue and Michael Gamon. 2005. Customizing

sentiment classifiers to new domains: A case study.
In Proceedings of the 5th International Conference
on Recent Advances in Natural Language Processing,
Borovets, Bulgaria, September.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440–447,
Prague, Czech Republic, June.

Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In Proceed-
ings of Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 308–
315, Rochester, New York, USA, April.

Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 355–362, Vancou-
ver, Canada, October.

James R. Cowie and Wendy G. Lehnert. 1996. In-
formation extraction. Communications of the ACM,
39(1):80–91.

Hal Daumé III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research (JAIR), 26:101–126.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168–177,
Seattle, Washington, USA, August.

Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 264–271, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.

Jason Kessler and Nicolas Nicolov. 2009. Targeting sen-
timent expressions through supervised ranking of lin-
guistic configurations. In Proceedings of the Third In-
ternational AAAI Conference on Weblogs and Social
Media, pages 90–97, San Jose, California, USA, May.

Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the ACL Workshop
on Sentiment and Subjectivity in Text, pages 1–8, Syd-
ney, Australia, July.

John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the 18th International Conference
on Machine Learning, pages 282–289, Williamstown,
MA, USA, June.

Fuchun Peng and Andrew McCallum. 2006. Information
extraction from research papers using conditional ran-
dom fields. Information Processing and Management,
42(4):963–979, July.

Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 339–346, Vancouver,
Canada, October.

Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and expression level annotation of
opinions in user-generated discourse. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 575–584, Uppsala,
Sweden, July.

Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing techniques. In Proceedings of the
3rd IEEE International Conference on Data Mining,
pages 427–434, Melbourne, Florida, USA, December.

Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the ACM 15th Conference on Information and Knowl-
edge Management, pages 43–50, Arlington, Virginia,
USA, November.

1045


