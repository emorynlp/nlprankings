










































Multimodal Sentiment Analysis


Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, page 1,
Jeju, Republic of Korea, 12 July 2012. cÂ©2012 Association for Computational Linguistics

Multimodal Sentiment Analysis
(Abstract of Invited Talk)

Rada Mihalcea
Department of Computer Science and Engineering

University of North Texas
P. O. Box 311366

Denton, TX 76203-6886, U.S.A.
rada@cs.unt.edu

Abstract

With more than 10,000 new videos posted
online every day on social websites such as
YouTube and Facebook, the internet is be-
coming an almost infinite source of informa-
tion. One important challenge for the com-
ing decade is to be able to harvest relevant
information from this constant flow of mul-
timodal data. In this talk, I will introduce
the task of multimodal sentiment analysis, and
present a method that integrates linguistic, au-
dio, and visual features for the purpose of
identifying sentiment in online videos. I will
first describe a novel dataset consisting of
videos collected from the social media web-
site YouTube, which were annotated for senti-
ment polarity. I will then show, through com-
parative experiments, that the joint use of vi-
sual, audio, and textual features greatly im-
proves over the use of only one modality at
a time. Finally, by running evaluations on
datasets in English and Spanish, I will show
that the method is portable and works equally
well when applied to different languages.

This is joint work with Veronica Perez-Rosas
and Louis-Philippe Morency.

1


