



















































Time to Change the "D" in "DEL"


Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 100–107,
Baltimore, Maryland, USA, 26 June 2014. c©2014 Association for Computational Linguistics

 

 

Time to change the “D” in “DEL” 

 
Stephen Beale 

Linguist’s Assistant 
Baltimore, MD 

stephenbeale42@gmail.com 
 
 
 
 

Abstract 

The “D” in “DEL” stands for “documenting” 
– a code word for linguists that means the 
collection of linguistic data in audio and writ-
ten form. The DEL (Documenting Endan-
gered Languages) program run by the NSF 
and NEH is thus centered around building 
and archiving data resources for endangered 
languages. This paper is an argument for ex-
tending the ‘D’ to include “describing” lan-
guages in terms of lexical, semantic, morpho-
logical and grammatical knowledge. We pre-
sent an overview of descriptive computa-
tional tools aimed at endangered languages 
along with a longer summary of two particu-
lar computer programs: Linguist’s Assistant 
and Boas. These two programs, respectively, 
represent research in the areas of: A) compu-
tational systems capable of representing lexi-
cal, morphological and grammatical struc-
tures and using the resulting computational 
models for translation in a minority language 
context, and B) tools for efficiently and accu-
rately acquiring linguistic knowledge. A 
hoped-for side effect of this paper is to pro-
mote cooperation between these areas of re-
search in order to provide a total solution to 
describing endangered languages.  

1    Introduction 

The “D” in “DEL” stands for “documenting” – a 
code word for linguists that means the collection 
of linguistic data in audio and written form. The 
DEL (Documenting Endangered Languages) 
program run by the NSF and NEH is thus cen-
tered around building and archiving data re-
sources for endangered languages. Furthermore, 
the recent change in the program to include com-
putational tools hasn’t changed the central focus 
on documentation, with one notable exception: 
the research headed by Emily Bender (Bender, et 
al. 2013) to automatically extract grammatical 

information from interlinear text. This paper is 
an argument for extending the ‘D’ to include 
“describing” languages in terms of lexical, se-
mantic, morphological and grammatical knowl-
edge. We present an overview of descriptive 
computational tools aimed at endangered lan-
guages along with a longer summary of two par-
ticular computer programs: Linguist’s Assistant 
and Boas. These two programs, respectively, rep-
resent research in the areas of A) computational 
systems capable of representing and translating 
minority languages, and B) tools for efficiently 
and accurately acquiring linguistic knowledge. A 
hoped-for side effect of this paper is to promote 
cooperation between these areas of research in 
order to provide a total solution to describing 
endangered languages.  

2    Documenting versus Describing 

The code word “documenting” implies data. The 
DEL program is primarily interested in procuring 
data about languages that are disappearing. The 
rationale behind this is obvious: we need to 
quickly gather data from languages before they 
become extinct. Data in the form of transcribed 
audio recordings and texts is certainly invalu-
able. However, consider the impact of such data 
in two areas: 1) future analysis by linguists, and 
2) revitalization and language promotion today. 

Think ahead 50 or 100 years. By all accounts, 
a majority of the world’s languages will be ex-
tinct. What resources will be available to the 22nd 
century linguist? The DEL program seeks to ar-
chive audio and textual data for use in the future. 
While this data is certainly valuable, how useful 
will it be?  Without a living speaker of the lan-
guage, extracting a useful, accurate and broad-
coverage description of the language from ar-
chived data will be extremely time consuming 

100



 

 

and probably impossible in most cases.1 Al-
though such data could be used for other pur-
poses, Gippert et al. (2006) agree with the gen-
eral premise that “without theoretical grounding 
language documentation is in danger of produc-
ing ‘data graveyards’, i.e. large heaps of data 
with little or no use to anyone.” This is a shame, 
and quite possibly a non-optimal use of our cur-
rent linguistic talent pool. On the other hand, if a 
linguist working today with a living informant 
and using appropriate computational tools and 
programs could efficiently and accurately de-
scribe these languages at a lexical, semantic, 
morphological and grammatical level, then the 
usefulness of such research 100 years from now 
would be considerably greater. 

That is looking ahead. What about now? What 
kind of work could help revitalize endangered 
languages so that they will not become extinct in 
the first place? My experience in language pro-
jects in the South Pacific leads me to the conclu-
sion that descriptive work - and the resulting 
computational and non-computational projects 
that are enabled by it - have a much greater im-
pact on current language populations than docu-
mentary efforts. The community I worked with 
for three years were the recipients of dictionaries 
and story books that documented linguistic re-
search. These efforts bore fruit: there was ini-
tially quite a bit of interest about them. However, 
this kind of work quickly lost appeal. On the 
other hand, descriptive work quickly led to the 
production of educational materials and interest 
in translation. Automatic and manual translations 
followed, especially of songs, religious and 
health-related materials. A knowledge of how the 
language works leads to an empowerment with 
the language.  

3  Research in Describing Endangered 
Languages: knowledge acquisition meth-
odologies  

In this section we present an overview of current 
and past descriptive computational tools aimed at 
endangered languages. In general, the field can 
be divided into two parts: A) computational sys-
tems capable of representing and translating mi-
nority languages, and B) tools for efficiently and 

                                                
1 Our experience backs up this claim. We have attempted to 
use Linguist’s Assistant to describe languages using only 
transcribed texts without a human informant; these experi-
ments failed miserably. 

accurately acquiring linguistic knowledge. Up 
until recently, research has focused on the latter. 

The most widespread line of computational re-
search in category B can be categorized as 
grammatical typology questionnaires. These fol-
low in the path of traditional, non-computational 
linguistic fieldwork methods characterized by 
Longacre (1964) and Comrie and Smith (1977). 
Boas (McShane, et al. 2002), the LinGO Gram-
mar Matrix (Bender, et al. 2010) and PAWS 
(Black & Black 2009) all fit into this paradigm. 
All these systems extract salient properties of a 
language through typological questionnaires and 
then produce computational resources of varying 
utility. This work must be applauded, and we 
argue that it is indispensable for a complete solu-
tion for describing endangered languages. How-
ever, the typology questionnaire approach is lim-
ited to creating approximate grammars. Bender 
et al. (2010) describe the LinGO Grammar Ma-
trix as a ‘rapid prototyping’ tool. Such a tool is 
useful, but more is needed to thoroughly describe 
a language and enable machine translation capa-
bilities. Linguist’s Assistant (LA, described be-
low) promotes such a thorough description; 
however, it comes at a cost. LA is able to repre-
sent the kinds of knowledge that is typically ex-
tracted by the grammatical typology question-
naire approach, such as rules to represent phrase 
structure word ordering and phenomena such as 
case, agreement, nominal declensions and the 
like.  But it is more flexible and able to describe 
additional linguistic phenomena that are not as 
easily described using a typological approach 
(see below for details). But the rules in LA cur-
rently must be entered manually by a computa-
tional linguist. Thus, the tradeoff: quick  descrip-
tions (using well thought-out typologies) that fall 
short of broad and deep coverage vs. adequate 
depth and breadth of coverage at a higher cost. 

It is perfectly clear that some linguistic phe-
nomena can be most efficiently described using 
the techniques of the typology questionnaire 
paradigm. However, the computational grammar 
and lexicon produced in an LA-type language 
description project are meant to be comprehen-
sive and complete insofar as they will be able to 
be used in a text generator to produce accurate 
translations. It is exactly this completeness and 
the resulting usefulness of the description (espe-
cially in language revitalization) that might be a 
prime factor in securing research funding from 
organizations that are interested in endangered 
languages. Therefore, we argue for: 1) continued 
research in typology questionnaire methods for 

101



 

 

efficiently acquiring the linguistic knowledge 
appropriate to that paradigm, 2) further devel-
opment of complete description paradigms like 
LA, 3) a greater cooperation between the two 
paradigms, and 4) the resurrection of machine 
learning, example-based techniques to minimize 
and semi-automate the comprehensive gram-
matical and semantic description process needed 
by systems like LA. 

A prime example of this latter point was the 
Avenue Project at Carnegie Mellon University 
(Probst, et al. 2003). The Avenue project was a 
machine translation system oriented towards 
low-density languages. It consisted of two cen-
tral parts: 1) the pre-run-time module that han-
dles the elicitation of data and the subsequent 
automatic creation of transfer rules, and 2) the 
actual translation engine. We are especially in-
terested in the former: 

 
“The purpose of the elicitation system is to col-
lect a high-quality, word-aligned parallel cor-
pus. Because a human linguist may not be 
available to supervise the elicitation, a user in-
terface presents sentences to the informants. 
The informants must be bilingual and fluent in 
the language of elicitation and the language be-
ing elicited, but do not need to have training in 
linguistics or computational linguistics. They 
translate phrases and sentences from the elicita-
tion language into their language and specify 
word alignments graphically. 

The rule-learning system takes the elicited, 
word-aligned data as input. Based on this in-
formation, it infers syntactic transfer rules.... 
The system also learns the composition of sim-
pler rules into more complicated rules, thus re-
ducing their complexity and capturing the com-
positional makeup of a language (e.g., NP rules 
can be plugged into sentence-level rules). The 
output of the rule-learning system is a set of 
transfer rules that then serve as a transfer 
grammar in the run-time system.” (Probst, et al. 
2003:247–248) 

 
At a high level, this is exactly the approach 

that LA advocates. However, LA differs from 
Avenue in several important features, most nota-
bly the underlying semantic representation in LA 
as opposed to Avenue’s transfer (source surface 
language to target surface language) approach. 
LA attains a greater practicality than Avenue 
primarily because of this difference, because in-
terlingual-based language description and text 
generation is an order of magnitude simpler and 
less prone to error than transfer-based ap-
proaches. But again, this benefit comes at a cost: 

the grammar description modules and all subse-
quent texts to be translated must be encoded in 
the semantic representation (as opposed to a 
natural language like English for transfer-based 
approaches). See the next section on Document 
Authoring for more details for how this limita-
tion can be minimized. 

Bender et al. (2013) also provide a machine-
learning component for their LinGO Grammar 
Matrix (Bender, et al. 2013). That is the project 
that is the exception to the “D” word problem. 
And that exceptional nature (it was funded!) 
should be instructional for all of us. 

The missing ingredient in LA (besides the in-
clusion of grammar typology techniques such as 
LinGO and BOAS) is the sort of machine learn-
ing capability seen in the Avenue project and 
Bender’s project. The latter system learns LinGO 
rules from interlinear text. Obviously, that is ex-
citing work and has the added benefit of being 
able to be used directly in the DEL’s data-centric 
context. However, it has limitations. We argue 
for a similar type of interlinear machine learning 
system, but one that is grounded in semantics 
and works over carefully prepared texts that will 
maximize the learning capabilities and allow for 
broad coverage of semantic phenomena. For ex-
ample, assume we have the following sentences 
semantically represented: 

 
John hit the tree. 
John began to hit the tree. 
John finished hitting the tree. 
etc… 

 
After a native speaker translates these sentences, 
a machine learning system could be employed to 
learn a grammar of inceptives, completives, etc., 
by comparing the semantic representations of the 
sentences in the module to find the differences 
(i.e. the addition of a “inceptive” property on the 
event) and then mapping those differences to the 
differences found in the translated texts (for ex-
ample, added words, affixes or changes in word 
order). Example elicitation modules have been 
prepared (including their semantic representa-
tions) for a large variety of semantically-based 
phenomena. Similar techniques are also used to 
probe different semantic case frame realizations. 
Such a semantically-based “grammar discovery 
procedure” is the means currently employed in 
LA. This grammar discovery procedure can be 
used to quickly describe how a particular lan-
guage encodes a wide range of meaning-based 

102



 

 

communication. The resulting computational 
description can then be used in the embedded 
text generation system to enable automatic trans-
lation. A grammar discovery procedure guided 
by semantics will obviously not yield a complete 
description of a language. It will not document 
everything that can be said in the language; how-
ever, we argue that it produces a practical de-
scription that will enable future generations to 
answer the question, “How do you say … in this 
language?” The approach is also very efficient in 
terms of the number of man-hours of linguistic 
work required. Our experience is that (under the 
right circumstances) a field linguist will require 
less than a month to complete the process. We 
expect this timeframe to decrease further as addi-
tional techniques such as those used in BOAS 
and LinGO are added to LA.2 This type of 
grammar discovery is also very suitable for a 
workshop situation where many languages 
within a single language family could work to-
gether. 

One valid argument against such an approach 
comes from linguistic circles. The current trend 
in linguistic research discourages elicitation, re-
lying instead on the analysis of naturally occur-
ring texts and dialogues. For example, a re-
spected linguist involved in and relatively sup-
portive of LA commented that “I am, in general, 
a bit reluctant to use ready-made questionnaires, 
for all sorts of reasons - some of which you men-
tion yourself. It so happens that my personal in-
terest has always been on naturalistic speech... I 
have always paid a lot of attention to what actu-
ally shows up in everyday spoken speech…” 
(Alex François, personal communication). We 
understand and accept this inclination towards 
naturally occurring texts over elicited texts, and 
in a “normal” situation we would completely 
agree. However, with the extinction of thousands 
of languages imminent, more radical techniques 
are needed. Elicitation techniques are also sup-
ported in the linguistic literature, for example, 
Ameka et al. (2006) state that ‘limiting what the 
grammar should account for to a corpus [of natu-
rally occurring texts] also overlooks the fact that 
speakers may have quite clear and revealing 
judgements’ and ‘the view...that grammars 
should be answerable just to a published corpus 

                                                
2 The discovery process itself as well as the underlying 
semantic representation language need to be refined and 
validated by our colleagues; we expect such refinements to 
also improve efficiency. 

seems an extreme position in practical terms.’ 
And again, Gippert et al. (2006) add their warn-
ing that ‘without theoretical grounding language 
documentation is in the danger of producing 
‘data graveyards’, i.e. large heaps of data with 
little or no use to anyone.’ We believe that the 
semantic-based grammar discovery methodology 
adds this theoretical grounding. 

We also add the argument that “the proof is in 
the pudding.” Allman, et al. (2012) documents 
that a grammar discovery procedure such as de-
scribed above combined with a capable knowl-
edge acquisition and text generation environment 
such as found in LA can produce translations 
that are as accurate and readable to native speak-
ers as manual translations and that these results 
indicate that the underlying language description 
is accurate, natural and broad-coverage.  

4    Document authoring: a bridge to prac-
tical MT (and language description) in 
endangered languages 

We have already argued that a semantically-
based language description environment is supe-
rior to a transfer-based system. We will try to 
bolster that argument here. In terms of machine 
translation, the analysis of a source text will al-
ways be the bottleneck in terms of translation 
quality. On the other hand, an interlingual text 
generation process is relatively simple and accu-
rate - assuming the presence of an accurate se-
mantic description of the input text. Furthermore, 
a semantic description “language” is much sim-
pler than natural languages since it has no ambi-
guity, fewer atoms (concepts vs. words), and 
fewer “syntactic” combinations. This leads to an 
economy when trying to describe how a particu-
lar language encodes it (as opposed to trying to 
describe how a language would encode arbitrary 
free text from a source language). And finally, as 
described above, a semantic-based description 
provides the framework for efficient and poten-
tially machine learnable acquisition of grammar 
via an organized grammar discovery procedures. 

The glue that holds this together is the concept 
of “document authoring.”  Authoring a semantic 
description of a text (or of the elicitation mod-
ules) can be accomplished through a semi-
automatic authoring interface. Such an interface 
typically accepts a standardized (or “controlled”) 
subset of a natural language as its input. The in-
put is run through an analyzer and the results are 
visually presented to the user, who checks and/or 
assigns semantic concepts and relationships. The 

103



 

 

steps in preparing a semantic analysis of a text or 
set of elicitation sentences is thus: 1) manually 
“translate” the text into the controlled language, 
2) run this through the automatic analyzer, and 3) 
manually check and correct the resulting seman-
tic analysis. Although unlimited free text cannot 
be translated in an LA language project, a wide 
variety of texts can be semantically authored. 
This process only needs to be done once and the 
results can then be used for any language. See 
(Beale, et al. 2005) for more information on 
document authoring in the context of endangered 
languages. 

We believe that a semantically-based descrip-
tion of a language is the key to the practical de-
scription of endangered languages. It provides an 
inherently efficient framework for language de-
scription in the field. The resulting description 
not only provides invaluable data for future lin-
guists, but also enables present-day translation 
capabilities that can aid in language revitaliza-
tion. A document authoring system provides the 
means for overcoming one of the main draw-
backs to a semantically-based system in that it 
allows for a relatively quick, once-for-all prepa-
ration of semantic representations that can be 
used in a grammar discovery procedure and in 
machine translation of texts. 

We now present longer summaries of Lin-
guist’s Assistant and BOAS.  

5    Linguist’s Assistant 

The Linguist’s Assistant (LA) is a practical com-
putational paradigm for describing languages. 
LA is built on a comprehensive semantic founda-
tion. We combine a conceptual, ontological 
framework with detailed semantic features that 
cover (or is a beginning towards the goal of cov-
ering) the range of human communication. An 
elicitation procedure has been built up around 
this central, semantic core that systematically 
guides the linguist through the language descrip-
tion process, during which the linguist builds a 
grammar and lexicon that ‘describes’ how to 
generate target language text from the semantic 
representations of the elicitation corpus. The re-
sult is a meaning-based ‘how to’ guide for the 
language: how does one encode given semantic 
representations in the language? 

Underlying this approach to knowledge acqui-
sition in LA is a visual, semi-automatic interface 
for recording grammatical rules and lexical in-
formation. Figure 1 shows an example of one 
kind of visual interface used for “theta-grid ad-

justment rules.” The figure shows an English 
rule used to adjust the “theta grid” or “case 
frame” of an English verb. Grammatical rules 
typically describe how a given semantic structure 
is realized in the language. The whole gamut of 
linguistic phenomena is covered, from morpho-
logical alternations (Figure 2) to case frame 
specifications to phrase structure ordering (Fig-
ure 3) to lexical collocations – and many others. 
These grammatical rules interplay with a rich 
lexical description interface that allows for as-
signment of word-level features and the descrip-
tion of lexical forms associated with individual 
roots (Figure 4). As stated above, the user is cur-
rently responsible for the creation of rules, albeit 
with a natural, visual interface that often is able 
to set up the requisite input semantic structures 
automatically. As mentioned, we also seek to 
collaborate with researchers to enable semi-
automatic generation of rules similar to what can 
be found in the Boas (McShane, et al., 2002), 
LinGO (Bender, et al., 2010), PAWS (Black and 
Black, 2009) and Avenue (Probst, et al., 2003) 
projects. Such extensions will make LA accessi-
ble to a larger pool of linguists and will shorten 
the time needed for documenting languages. 

 

 
Figure 1. Visual interface for grammatical rules 

 

 
Figure 2. Morphological alternation rule 

 
Integrated with these elicitation and descrip-

tion tools is a text generator that allows for im-
mediate confirmation of the validity of gram-
matical rules and lexical information. We also 

104



 

 

provide an interface for tracking the scope and 
examples of grammatical rules. This minimizes 
the possibility of conflicting or duplicate rules 
while providing the linguist a convenient index 
into the work already accomplished. And finally, 
we provide a utility for producing a written de-
scription of the language - after all, a computa-
tional description of a language is of no practical 
use (outside of translation applications) unless it 
can be conveniently referenced. Refer to Beale 
(2012) for a comprehensive description of Lin-
guist’s Assistant. 

 

 
Figure 3. Phrase structure ordering rule 

 

 
Figure 4. Lexical forms for Spanish 

 
LA has been used to produce extensive gram-

mars and lexicons for Jula (a Niger-Congo lan-
guage), Kewa (Papua New Guinea), North Tanna 
(Vanuatu), Korean and English. Work continues 
in two languages of Vanuatu (and a new avenue 
of research has recently opened as a result of a 
partnership with De La Salle University in the 
Philippines). The resulting computational lan-
guage descriptions have been used in LA’s em-
bedded text generation system to produce a sig-
nificant amount of high-quality translations. Fig-
ures 5 and 6 present translations of a section of a 
medical text on AIDS into English and Korean. 
Please reference Beale et al. (2005) and Allman 
and Beale (2004; 2006) and Allman et al. (2012) 
for more information on using LA in translation 

projects and for documentation on the evalua-
tions of the translations produced. We argue that 
the high quality achieved in translation projects 
demonstrate the quality and coverage of the un-
derlying language description that LA produces. 

 

 
Figure 5. English translation of a medical text 
 

 
Figure 6. Korean translation of a medical text 

 

6    BOAS 

Boas (McShane et al. 2002) is an example of a 
typology-based questionnaire approach that can 
be useful for quickly eliciting certain properties 
of a language. This section is meant as an over-
view that is representative of this class of pro-
grams. The author has no direct connection with 
the Boas system; permission was given to use the 
following description.  

Boas is used to extract knowledge about a lan-
guage, L, from an informant with no knowledge 
engineer present. Boas itself leads the informant 
through the process of supplying the necessary 
information in a directly usable way. In order to 
do this, the system must be supplied with meta-
knowledge about language – not L, but language 
in general – which is organized into a typologi-
cally and cross-linguistically motivated inven-
tory of parameters, their potential value sets, and 
modes of realizing the latter. The inventory takes 

105



 

 

into account phenomena observed in a large 
number of languages. Particular languages would 
typically feature only a subset of parameters, 
values and means of realization. The parameter 
values employed by a particular language, and 
the means of realizing them, differentiate one 
language from another and can, in effect, act as 
the formal “signature” for the language. Exam-
ples of parameters, values and their realizations 
that play a role in the Boas knowledge-elicitation 
process are shown in Table 1. The first block 
illustrates inflection, the second, closed-class 
meanings, the third, ecology and the fourth, syn-
tax. 

In the elicitation process, the parameters (left 
column) represent categories of phenomena that 
need to be covered in the description of L, the 
values (middle column) represent choices that 
orient what might be included in the description 
of that phenomenon for L, and the realization 
options (right column) suggest the kinds of ques-
tions that must be asked to gather the relevant 
information. 

 

 
Table 1: Sample parameters, values and means of 

their realization 
 

The selection of parameters and values in 
Boas is made similar to a multiple choice test 
which, with the necessary pedagogical support, 
can be carried out even by an informant not 
trained in linguistics. This turns out to be a cru-
cial aspect of knowledge elicitation for rare lan-
guages, since one must prepare for the case when 
available informants lack formal linguistic train-

ing. Boas also allows a maximum of flexibility 
and economy of effort. Certain decisions on the 
part of the user cause the system to reorganize 
the process of acquisition by removing some in-
terface pages and/or reordering those that re-
main. This means that the system is more flexi-
ble than static acquisition interfaces that require 
the user to walk through the same set of pages 
irrespective of context and prior decisions. 

The five major modules of the Boas system 
are: 

 
Ecology: 

• inventory of characters 
• inventory and use of punctuation marks 
• proper name conventions 
• transliteration 
• dates and numbers 
• list of common abbreviations, geographi-
cal entities, famous people, etc. (which 
can be expanded indefinitely) 

Morphology: 
• selecting language type: flective, aggluti-
nating, mixed 

• paradigmatic inflectional morphology, if 
needed 

• non-paradigmatic inflectional morphol-
ogy, if needed 

• derivational morphology 
Syntax: 

• structure of the noun phrases: NP com-
ponents, word order, etc. 

• grammatical functions: subject, direct ob-
ject, etc. 

• realization of sentence types: declarative, 
interrogative, etc. 

• special syntactic structures: topic front-
ing, affix hopping, etc. 

Closed-Class Lexical Acquisition: 
Provide L translations of some 150 closed-
class meanings, which can be realized as 
words, phrases, affixes or features (e.g., In-
strumental Case used to realize instrumental 
‘with’, as in hit with a stick). Inflecting 
forms of any of the first three realizations 
must be provided as well, as applicable. 

Open-Class Lexical Acquisition: 
Build a L-to-English lexicon by a) translat-
ing entries from an English seed lexicon, b) 
importing then supplementing an on-line bi-
lingual lexicon, c) composing lists of words 
in L and translating them into English, or d) 
any combination of the above. Grammati-
cally important inherent features and irregu-
lar inflectional forms must be provided. 

106



 

 

Associated with each of these tasks are 
knowledge elicitation “threads”—i.e., series of 
pages that combine questions with background 
information and instruction. If, for example, a 
user indicates that nouns in L inflect for number, 
the page shown in Figure 7 will be accessed. Ex-
planatory support for decision-making is pro-
vided in help links at the bottom of the page.  

Boas offers a good example of an advanced 
elicitation system by combining extensive and 
parameterized descriptive material about lan-
guage, a rich set of expressive means in the user 
interface, and extensive pedagogical resources.  

 

 
Figure 7: Selecting the values for 
number for which nouns inflect 

7  Conclusion 

A quick perusal of the grants awarded by 
NSF/NEH in the DEL program over the last five 
years confirms the underlying assumption of this 
paper: the DEL program funds projects that pro-
duce or aid audio and textual documentation (i.e. 
data) on endangered languages. We argued that 
descriptive work might return a higher payback 
as regards to potential linguistic utilization in the 
future. We also argued that the value of descrip-
tive work in revitalizing languages today exceeds 
that of purely documentary work. Furthermore, 
we described several lines of research that would 
allow such descriptive work to proceed, along 
with a rationale for continued research to im-
prove the computational tools employed in such 
work. Linguist’s Assistant and Boas represent 
two sides of the same coin for descriptive work 
in minority languages. Cooperation between the 
various research programs that represent each 
side of that coin is critical to attaining a total so-
lution to describing endangered languages. 

References 
Tod Allman, Stephen Beale and Richard Denton. 

2012. Linguist’s Assistant: A Multi-Lingual Natu-
ral Language Generator based on Linguistic Uni-

versals, Typologies, and Primitives. In Proceedings 
of 7th International Natural Language Generation 
Conference (INLG-12), Utica, IL.  

Tod Allman and Stephen Beale. 2006. A natural lan-
guage generator for minority languages. In Pro-
ceedings of SALTMIL, Genoa, Italy. 

Tod Allman and Stephen Beale. 2004. An environ-
ment for quick ramp-up multi-lingual authoring. 
International Journal of Translation 16(1). 

Felix Ameka, Alan Dench & Nicholas Evans. 2006. 
Catching language: the standard challenge of 
grammar writing. Berlin: Mouton de Gruyter. 

Stephen Beale. 2012. Documenting endangered lan-
guages with Linguist’s Assistant. Language Docu-
mentation and Conservation 6(1), pp. 104-134. 

Stephen Beale, S. Nirenburg, M. McShane, and Tod 
Allman. 2005. Document authoring the Bible for 
minority language translation. In Proceedings of 
MT-Summit, Phuket, Thailand. 

Emily Bender, Michael Wayne Goodman, Joshua 
Crowgey and Fei Xia. 2013. Towards creating pre-
cision grammars from interlinear glossed text: in-
ferring large-scale typological properties. In Pro-
ceedings of the ACL 2013 workshop on Language 
Technology for Cultural Heritage, Social Sciences 
and Humanities. 

Emily Bender, S. Drellishak, A. Fokkens, M. Good-
man, D. Mills, L. Poulson, and S. Saleem. 2010. 
Grammar prototyping and testing with the LinGO 
grammar matrix customization system. In Proceed-
ings of the ACL 2010 System Demonstrations. 

Sheryl Black and Andrew Black. 2009. PAWS: parser 
and writer for syntax: drafting syntactic grammars 
in the third wave.  
http://www.sil.org/silepubs/PUBS/51432/SILForu
m2009-002.pdf. 

B. Comrie and N. Smith. 1977. Lingua descriptive 
questionnaire. Lingua 42. 

Jost Gippert, Nikolaus Himmelmann & Ulrike Mosel. 
2006. Essentials of language documentation. Ber-
lin: Mouton de Gruyter. 

R.E. Longacre. 1964. Grammar Discovery Proce-
dures. Mouton: The Hague. 

Marjorie McShane, Sergei Nirenburg, Jim Cowie, and 
Ron Zacharski. 2002. Embedding knowledge elici-
tation and MT systems within a single architecture. 
Machine Translation 17(4), pp. 271-305. 

Katharina Probst, Lori Levin, Erik Petersen, Alon 
Lavie and Jaime Carbonell. 2003. MT for minority 
languages using elicitation-based learning of syn-
tactic transfer rules. Machine Translation 17(4), 
pp. 245-270. 

107


