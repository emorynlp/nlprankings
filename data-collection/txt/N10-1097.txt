










































Making Conversational Structure Explicit: Identification of Initiation-response Pairs within Online Discussions


Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 673–676,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics

Making Conversational Structure Explicit:  

Identification of Initiation-response Pairs within Online Discussions 

    

Yi-Chia Wang Carolyn P. Rosé 
Language Technologies Institute  Language Technologies Institute  

Carnegie Mellon University Carnegie Mellon University 

Pittsburgh, PA 15213, USA Pittsburgh, PA 15213, USA 
yichiaw@cs.cmu.edu cprose@cs.cmu.edu 

 
 

Abstract 

In this paper we investigate how to identify 

initiation-response pairs in asynchronous, 

multi-threaded, multi-party conversations.  

We formulate the task of identifying initia-

tion-response pairs as a pairwise ranking 

problem. A novel variant of Latent Semantic 

Analysis (LSA) is proposed to overcome a li-

mitation of standard LSA models, namely that 

uncommon words, which are critical for sig-

naling initiation-response links, tend to be 

deemphasized as it is the more frequent terms 

that end up closer to the latent factors selected 

through singular value decomposition. We 

present experimental results demonstrating 

significantly better performance of the novel 

variant of LSA over standard LSA.  

1 Introduction 

In recent years, research in the analysis of social 

media (e.g., weblogs, discussion boards, and mes-

sengers) has grown in popularity. Unlike exposito-

ry text, the data produced through use of social 

media is often conversational, multi-threaded, and 

more complex because of the involvement of nu-

merous participants who are distributed both across 

time and across space. Recovering the multi-

threaded structure is an active area of research. 

In this paper, we form the foundation for a 

broader study of this type of data by investigating 

the basic unit of interaction, referred to as an initi-
ation-response pair (Schegloff, 2007). Initiation-
response pairs are pairs of utterances that are typi-

cally contributed by different participants, and 

where the first pair part sets up an expectation for 

the second pair part. Types of common initiation-

response pairs include question-answer, assess-

ment-agreement, blame-denial, etc. Note that al-

though sometimes discussion forum interfaces 

make the thread structure of the interaction expli-

cit, these affordances are not always present. And 

even in forums that have these affordances, the 

apparent structure of the discourse as represented 

through the interface may not capture all of the 

contingencies between contributions in the unfold-

ing conversation. Thus, the goal of this investiga-

tion is to investigate approaches for automatically 

identifying initiation-response pairs in conversa-

tions.   

One of the challenges in identifying initiation-

response pairs is that the related messages are not 

necessarily adjacent to each other in the stream of 

contributed messages, especially within the asyn-

chronous environment of social media. Further-

more, individual differences related to writing style 

or creative expression of self may also complicate 

the identification of the intended connections be-

tween contributions. Identification of initiation-

response pairs is an important step towards auto-

matic processing of conversational data. One po-

tential application of this work is conversation 

summarization. A summary should include both 

the initiation and response as a coherent unit or it 

may fail to capture the intended meaning. 

We formulate the task of identifying initiation-

response pairs as a pairwise ranking problem. The 

goal is to distinguish message pairs that constitute 

an initiation-response pair from those that do not. 

We believe a ranking approach, where the degree 

of relatedness between a message pair can be con-

sidered in light of the relatedness between each of 

them and the surrounding messages within the 

same thread, is a more suitable paradigm for this 

task than a discrete classification-based paradigm.    

 Previous work on recovering conversational 

structure has relied on simple lexical cohesion 

673



measures (i.e., cosine similarity), temporal infor-

mation (Lewis and Knowles, 1997; Wang et al., 

2008), and meta-data (Minkov et al., 2006). How-

ever, relatively little work has investigated the im-

portance of specifically in-focus connections 

between initiation-response pairs and utilized them 

as clues for the task. Consider, for example, the 

following excerpt discussing whether congress 

should pass a bill requiring the use of smaller cars 

to save the environment:   
a) Regressing to smaller vehicles would discourage 

business from producing more pollution. 

b) If CO2 emissions are lowered, wouldn't tax revenues 
be lowered as well? Are the democrats going to wil-

lingly give up Medicaid and social security? 

Although segment (b) is a reply to segment (a), the 

amount of word overlap is minimal. Nonetheless, 

we can determine that (b) is a response to (a) by 

recognizing the in-focus connections, such as "ve-

hicles-CO2" and "pollution-CO2." To properly 

account for connections between initiations and 

responses, we introduce a novel variant of Latent 

Semantic Analysis (LSA) into our ranking model.  

In section 2, we describe the Usenet data and 

how we extract a large corpus of initiation-

response pairs from it. Section 3 explains our rank-

ing model as well as the proposed novel LSA vari-

ation. The experimental results and discussion are 

detailed in Section 4 and Section 5, respectively. 

2 Usenet and Generation of Data 

The experiment for this paper was conducted using 

data crawled from the alt.politics.usa Usenet (User 

Network) discussion forum, including all posts 

from the period between June 2003 and June 2008.  

The resulting set contains 784,708 posts. The posts 

in this dataset also contain meta-data that makes 

parent-child relationships explicit (i.e., through the 

References field). Thus, we know 625,116 of the 

posts are explicit responses to others posts. The 

messages are organized into a total of 77,985 dis-

cussion threads, each of which has 2 or more posts.   

In order to evaluate the quality of using the ex-

plicit reply structure as our gold standard for initia-

tion-response links, we asked human judges to 

annotate the response structure of a random-

selected medium-length discussion (19 posts) 

where we had removed the meta-data that indi-

cated the initiation-reply structure. The result 

shows the accuracy of our gold standard is 0.89. 

To set up the data as a pairwise ranking prob-

lem, we arranged the posts in the corpus into in-

stances containing three messages each, one of 

which is a response message, one of which is the 

actual initiating message, and the other of which is 

a foil selected from the same thread. The idea is 

that the ranking model will be trained to prefer the 

actual initiating message in contrast to the foil.   

The grain size of our examples is finer than 

whole messages. More specifically, positive exam-

ples are pairs of spans of text that have an initia-

tion-reply relationship. We began the process with 

pairs of messages where the meta-data indicates 

that an initiation-reply relationship exits, but we 

didn’t stop there. For our task it is important to 

narrow down to the specific spans of text that have 

the initiation-response relation. For this, we used 

the indication of quoted material within a message. 

We observed that when users explicitly quote a 

portion of a previously posted message, the portion 

of text immediately following the quoted material 

tends to have an explicit discourse connection with 

it. Consider the following example:  
>> Why is the quality of life of the child, mother,  
>> and society at large, more important than the 
>> sanctity of life? 
> Because in the case of anencephaly at least, 
> the life is ended before it begins. 
We disagree on this point. Why do you refuse to 
provide your very own positive definition of life?  
Do you believe life begins before birth?  At birth?  
After birth?  Never? 

In this thread, the reply expresses an opinion 

against the first level quote, but not the second lev-

el quote. Thus, we used segments of text with sin-

gle quotes as an initiation and the immediately 

following non-quoted text as the response. We ex-

tracted positive examples by scanning each post to 

locate the first level quote that is immediately fol-

lowed by unquoted content. If such quoted material 

was found, the quoted material and the unquoted 

response were both extracted to form a positive 

example. Otherwise, the message was discarded. 

For each post P where we extracted a positive 

example, we also extracted a negative example by 

picking a random post R from the same thread as 

P. We selected the negative example in such a way 

to make the task difficult in a realistic way. Choos-

ing R from other threads would make the task too 

easy because the topics of P and R would most 

likely be different. We also stipulated that R cannot 

be the parent, grandparent, sibling, or child of P. 

674



Together the non-quoted text of P and R forms a 

negative instance. Thus, the final dataset consists 

of pairs of message pairs ((pi, pj), (pi, pk)), where 

they have the same reply message pi, and pj is the 

correct quote message of pi, but pk is not. In other 

words, (pi, pj) is considered as a positive example; 

(pi, pk) is a negative example. We constructed a 

total of 100,028 instances for our dataset, 10,000 

(~10%) of which were used for testing, and 90,028 

(~90%) of which were the learning set used to con-

struct the LSA space described in the next section. 

3 Ranking Models for Identification of  
Initiation-Response Pairs 

Our pairwise ranking model
1
 takes as input an or-

dered pair of message pairs ((pi, pj), (pi, pk)) and 

computes their relatedness using a similarity func-

tion sim. Specifically, 

( xij, xik ) = ( sim (pi, pj), sim (pi, pk) ) 

where xij is the similarity value between post pi and 

pj; xik is the similarity value between post pi and pk.   

To determine which of the two message pairs ranks 

higher regarding initiation-response relatedness, 

we use the following scoring function to compare 

their corresponding similarity values: 

score (xij, xik) = xij – xik  

If the score is positive, the model ranks (pi, pj) 

higher than (pi, pk) and vice versa. A message pair 

ranked higher means it has more evidence of being 

an initiation-reply link, compared to the other pair. 

3.1 Alternative Similarity Functions 

We introduce and motivate 3 alternative similarity 

functions, where the first two are considered as 

baseline approaches and the third one is a novel 

variation of LSA. We argue that the proposed LSA 

variation is an appropriate semantic similarity 

measurement for identifying topic continuation and 

initiation-reply pairs in online discussions.  

Cosine Similarity (cossim).  We choose an ap-

proach that uses only lexical cohesion as our base-

line. Previous work (Lewis and Knowles, 1997; 

Wang et al., 2008) has verified its usefulness for 

the thread identification task. In this case, 

                                                           
1 We cast the problem as a pairwise ranking problem in order 

to focus specifically on the issue of characterizing how initia-

tion-response links are encoded in language through lexical 

choice.  Note that once trained, pairwise ranking models can 

be used to rank multiple instances. 

sim(pi,pj) = cossim(pi,pj)  

where cossim(pi,pj) computes the cosine of the an-

gle between two posts pi and pj while they are 

represented as term vectors. 

LSA Average Similarity (lsaavg).  LSA is a well-

known method for grouping semantically related 

words (Landauer et al., 1998). It represents word 

meanings in a concept space with dimensionality k. 

Before we describe how to compute average simi-

larity given an LSA space, we explain how the 

LSA space was constructed in our work. First, we 

construct a term-by-document matrix, where we 

use the 90,028 message learning set mentioned at 

the end of Section 2. Next, LSA applies singular 

value decomposition to the matrix, and reduces the 

dimensionality of the feature space to a k dimen-

sional concept space. This generated LSA space is 

used by both lsaavg and lsacart later. 

For lsaavg, we follow Foltz et al. (1998):  

 
 

 

 

The meaning of each post is represented as a vec-

tor in the LSA space by averaging across the LSA 

representations for each of its words. The similari-

ty between the two posts is then determined by 

computing the cosine value of their LSA vectors.  

This is the typical method for using LSA in text 

similarity comparisons. However, note that not all 

words carry equal weight within the vector that 

results from this averaging process. Words that are 

closer to the "semantic prototypes" represented by 

each of the k dimensions of the reduced vector 

space will have vectors with longer lengths than 

words that are less prototypical. Thus, those words 

that are closer to those prototypes will have a larg-

er effect on the direction of the resulting vector and 

therefore on the comparison with other texts. An 

important consideration is whether this is a desira-

ble effect. It would lead to deemphasizing those 

unusual types of information that might be being 

discussed as part of a post. However, one might 

expect that those things that are unusual types of 

information might actually be more likely to be the 

in-focus information within an initiation that res-

ponses may be likely to refer to. In that case, for 

our purposes, we would not expect this typical me-

thod for applying LSA to work well. 

LSA Cartesian Similarity (lsacart).  To properly 
account for connections between initiations and 

( ) ( )


















==

∑∑
∈∈

j

pt

b

i

pt

a

jiji
p

t

p

t

pplsaavgppsim jbia ,cos,,

675



responses that include unusual words, we introduce 

the following similarity function:  

 
 

 

 

where we take the mean of the cosine values for all 

the word pairs in the Cartesian product of posts pi 

and pj. Note that in this formulation, all words have 

an equal chance to affect the overall similarity be-

tween vectors since it is the angle represented by 

each word in a pair that comes to play when cosine 

distance is applied to a word pair. Length is no 

longer a factor. Moreover, the averaging is across 

cosine similarity scores rather than LSA vectors. 

4 Experimental Results 

The results are found in Table 1. For comparison, 

we also report the random baseline (0.50).  

 
  Random 

Baseline 

Cos- 

Similarity 

LSA- 

Average 

LSA- 

Cart 

Accuracy 0.50 0.66 0.60 0.71 

Table 1. Overview of results 

Besides the random baseline, LSA-Average per-

forms the worst (0.60), with simple Cosine similar-

ity (0.66) in the middle, and LSA-Cart (0.71) the 

best, with each of the pairwise contrasts being sta-

tistically significant. We believe the reason why 

LSA-Average performs so poorly on this task is 

precisely because, as discussed in last section, it 

deemphasizes those words that contribute the most 

unusual content. LSA-Cart addresses this issue. 

To further understand this effect, we conducted 

an error analysis. We divided the instances into 4 

sets based on the lexical cohesion between the re-

sponse and the true initiation and between the re-

sponse and the foil, by taking the median split on 

the distributions of these two cohesion scores.  Our 

finding is that model performances vary by subset. 

In particular, we find that it is only in cases where 

the positive example has low lexical cohesion (e.g. 

our "vehicles-CO2" and "pollution-CO2" example 

from the earlier section), that we see the benefit of 

the LSA-Cart approach.  In other cases, where the 

cohesion between the reply and the true initiation 

is high, Cos-Similarity performs best. 

5 Discussion and Conclusion 

We have argued why the task of detecting initia-

tion-response pairs in multi-party discussions is 

important and challenging. We proposed a method 

for acquiring a large corpus for use to identify init-

iation-response pairs. In our experiments, we have 

shown that the ranking model using a variant of 

LSA performs best, which affirms our hypothesis 

that unusual information and uncommon words 

tends to be the focus of ongoing discussions and 

therefore to be the key in identifying initiation-

response links. 

In future work, we plan to further investigate the 

connection between an initiation-response pairs 

from multiple dimensions, such as topical cohe-

rence, semantic relatedness, conversation acts, etc. 

One important current direction is to develop a 

richer operationalization of the interaction that ac-

counts for the way posts sometimes respond to a 

user, a collection of users, or a user’s posting histo-

ry, rather than specific posts per se. 

Acknowledgments 

We thank Mary McGlohon for sharing her data 

with us.  This research was funded through NSF 

grant DRL-0835426. 

References  

David D. Lewis and Kimberly A. Knowles. 1997. 

Threading electronic mail: A preliminary study. In-

formation Processing and Management, 33(2), 209–

217. 

Einat Minkov, William W. Cohen, Andrew Y. Ng. 

2006. Contextual Search and Name Disambiguation 

in Email using Graphs. In Proceedings of the Inter-

national ACM Conference on Research and Devel-

opment in Information Retrieval (SIGIR), pages 35–

42. ACM Press, 2006. 

Peter W. Foltz, Walter Kintsch, Thomas K. Landauer. 

1998. Textual coherence using latent semantic analy-

sis. Discourse Processes, 25, 285–307. 

Thomas K. Landauer, Peter W. Foltz, and Darrell La-

ham. 1998. Introduction to latent semantic analysis. 

Discourse Processes, 25, 259-284.  

Schegloff, E. 2007.  Sequence Organization in Interac-

tion: A Primer in Conversation Analysis, Cambridge 

University Press. 

Yi-Chia Wang, Mahesh Joshi, William W. Cohen, Ca-

rolyn P. Rosé. 2008. Recovering Implicit Thread 

Structure in Newsgroup Style Conversations. In Pro-

ceedings of the 2nd International Conference on 

Weblogs and Social Media (ICWSM II), Seattle, 

USA. 

( ) ( )
( )

ji

pptt

ba

jiji
pp

tt

pplsacartppsim
jiba

 

,cos

,,
),(

∑
×∈

==

676


