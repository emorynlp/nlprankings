










































Effective Use of Function Words for Rule Generalization in Forest-Based Translation


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 22–31,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics

Effective Use of Function Words for Rule Generalization
in Forest-Based Translation

Xianchao Wu† Takuya Matsuzaki† Jun’ichi Tsujii†‡∗

†Department of Computer Science, The University of Tokyo
‡School of Computer Science, University of Manchester

∗National Centre for Text Mining (NaCTeM)
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp

Abstract

In the present paper, we propose the ef-
fective usage of function words to generate
generalized translation rules for forest-based
translation. Given aligned forest-string pairs,
we extract composed tree-to-string translation
rules that account for multiple interpretations
of both aligned and unaligned target func-
tion words. In order to constrain the ex-
haustive attachments of function words, we
limit to bind them to the nearby syntactic
chunks yielded by a target dependency parser.
Therefore, the proposed approach can not
only capture source-tree-to-target-chunk cor-
respondences but can also use forest structures
that compactly encode an exponential num-
ber of parse trees to properly generate target
function words during decoding. Extensive
experiments involving large-scale English-to-
Japanese translation revealed a significant im-
provement of 1.8 points in BLEU score, as
compared with a strong forest-to-string base-
line system.

1 Introduction

Rule generalization remains a key challenge for
current syntax-based statistical machine translation
(SMT) systems. On the one hand, there is a ten-
dency to integrate richer syntactic information into
a translation rule in order to better express the trans-
lation phenomena. Thus, flat phrases (Koehn et al.,
2003), hierarchical phrases (Chiang, 2005), and syn-
tactic tree fragments (Galley et al., 2006; Mi and
Huang, 2008; Wu et al., 2010) are gradually used in
SMT. On the other hand, the use of syntactic phrases
continues due to the requirement for phrase cover-
age in most syntax-based systems. For example,

Mi et al. (2008) achieved a 3.1-point improvement
in BLEU score (Papineni et al., 2002) by including
bilingual syntactic phrases in their forest-based sys-
tem. Compared with flat phrases, syntactic rules are
good at capturing global reordering, which has been
reported to be essential for translating between lan-
guages with substantial structural differences, such
as English and Japanese, which is a subject-object-
verb language (Xu et al., 2009).

Forest-based translation frameworks, which make
use of packed parse forests on the source and/or tar-
get language side(s), are an increasingly promising
approach to syntax-based SMT, being both algorith-
mically appealing (Mi et al., 2008) and empirically
successful (Mi and Huang, 2008; Liu et al., 2009).
However, forest-based translation systems, and, in
general, most linguistically syntax-based SMT sys-
tems (Galley et al., 2004; Galley et al., 2006; Liu
et al., 2006; Zhang et al., 2007; Mi et al., 2008;
Liu et al., 2009; Chiang, 2010), are built upon word
aligned parallel sentences and thus share a critical
dependence on word alignments. For example, even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, and un-
aligned words can result in an exponentially large
set of extractable rules for the interpretation of these
unaligned words (Galley et al., 2006).

What makes word alignment so fragile? In or-
der to investigate this problem, we manually ana-
lyzed the alignments of the first 100 parallel sen-
tences in our English-Japanese training data (to be
shown in Table 2). The alignments were generated
by running GIZA++ (Och and Ney, 2003) and the
grow-diag-final-and symmetrizing strategy (Koehn
et al., 2007) on the training set. Of the 1,324 word
alignment pairs, there were 309 error pairs, among

22



which there were 237 target function words, which
account for 76.7% of the error pairs1. This indicates
that the alignments of the function words are more
easily to be mistaken than content words. More-
over, we found that most Japanese function words
tend to align to a few English words such as ‘of’
and ‘the’, which may appear anywhere in an English
sentence. Following these problematic alignments,
we are forced to make use of relatively large English
tree fragments to construct translation rules that tend
to be ill-formed and less generalized.

This is the motivation of the present approach of
re-aligning the target function words to source tree
fragments, so that the influence of incorrect align-
ments is reduced and the function words can be gen-
erated by tree fragments on the fly. However, the
current dominant research only uses 1-best trees for
syntactic realignment (Galley et al., 2006; May and
Knight, 2007; Wang et al., 2010), which adversely
affects the rule set quality due to parsing errors.
Therefore, we realign target function words to a
packed forest that compactly encodes exponentially
many parses. Given aligned forest-string pairs, we
extract composed tree-to-string translation rules that
account for multiple interpretations of both aligned
and unaligned target function words. In order to con-
strain the exhaustive attachments of function words,
we further limit the function words to bind to their
surrounding chunks yielded by a dependency parser.
Using the composed rules of the present study in
a baseline forest-to-string translation system results
in a 1.8-point improvement in the BLEU score for
large-scale English-to-Japanese translation.

2 Backgrounds

2.1 Japanese function words

In the present paper, we limit our discussion
on Japanese particles and auxiliary verbs (Martin,
1975). Particles are suffixes or tokens in Japanese
grammar that immediately follow modified con-
tent words or sentences. There are eight types of
Japanese function words, which are classified de-
pending on what function they serve: case markers,
parallel markers, sentence ending particles, interjec-

1These numbers are language/corpus-dependent and are not
necessarily to be taken as a general reflection of the overall qual-
ity of the word alignments for arbitrary language pairs.

tory particles, adverbial particles, binding particles,
conjunctive particles, and phrasal particles.

Japanese grammar also uses auxiliary verbs to
give further semantic or syntactic information about
the preceding main or full verb. Alike English, the
extra meaning provided by a Japanese auxiliary verb
alters the basic meaning of the main verb so that the
main verb has one or more of the following func-
tions: passive voice, progressive aspect, perfect as-
pect, modality, dummy, or emphasis.

2.2 HPSG forests

Following our precious work (Wu et al., 2010), we
use head-drive phrase structure grammar (HPSG)
forests generated by Enju2 (Miyao and Tsujii, 2008),
which is a state-of-the-art HPSG parser for English.
HPSG (Pollard and Sag, 1994; Sag et al., 2003) is a
lexicalist grammar framework. In HPSG, linguistic
entities such as words and phrases are represented
by a data structure called a sign. A sign gives a
factored representation of the syntactic features of
a word/phrase, as well as a representation of their
semantic content. Phrases and words represented by
signs are collected into larger phrases by the appli-
cations of schemata. The semantic representation of
the new phrase is calculated at the same time. As
such, an HPSG parse forest can be considered to
be a forest of signs. Making use of these signs in-
stead of part-of-speech (POS)/phrasal tags in PCFG
results in a fine-grained rule set integrated with deep
syntactic information.

For example, an aligned HPSG forest3-string pair
is shown in Figure 1. For simplicity, we only draw
the identifiers for the signs of the nodes in the HPSG
forest. Note that the identifiers that start with ‘c’ de-
note non-terminal nodes (e.g., c0, c1), and the iden-
tifiers that start with ‘t’ denote terminal nodes (e.g.,
t3, t1). In a complete HPSG forest given in (Wu et
al., 2010), the terminal signs include features such
as the POS tag, the tense, the auxiliary, the voice of
a verb, etc.. The non-terminal signs include features
such as the phrasal category, the name of the schema

2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
3The forest includes three parse trees rooted at c0, c1, and

c2. In the 1-best tree, ‘by’ modifies the passive verb ‘verified’.
Yet in the 2- and 3-best tree, ‘by’ modifies ‘this result was ver-
ified’. Furthermore, ‘verified’ is an adjective in the 2-best tree
and a passive verb in the 3-best tree.

23



 

 

jikken niyotte kono kekka ga sa re ta kensyou 

Realign target function words 

実験 0 
 

によって 1 
 

この 2 
 

結果 3 
 

が 4 さ 6 
 
れ 7 
 

た 8 
 

検証 5 
 

this 
 

result 
 

was 
 

verified 
 

by 
 

the 
 

experiments 
 

t3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9 

c9 c10 c16 c22 c4 c21 c12 c18 c19 c14 c15

c23 

c8 

c13 

c5 c17 

c3 

c6 

c2 

c7 

c11 

c0 

c20 

c1 

1-best tree 
2-best tree 3-best tree 

experiments 
 

by 
 

this 
 

result 
 

 
 

verified 
 

c1 

実験 0 

 

によって 1 

 

この 2 

 

結果 3 

 

が 4 
 

さ 6 

 

れ 7 

 

た 8 

 

検証 5 

 C1 C2 C3 C4 

this 

 

result 

 

was 

 

verified 

 

by 

 

the 

 

experiments 
 

t3 t1 t4 t8 t10 
t7 

t0 t6 t5 t2 t9 

c9 c16 c22 
c45-7 | 5-8 c125-7 | 5-8  c18 c19 c14 c15

c2 c0 

c215-7 | 5-8 

c23 
c8 

c13 

c5 
c17 

c3 

c6 

c7 

c11 c20 

c103 | 3-4 

Figure 1: Illustration of an aligned HPSG forest-string pair for English-to-Japanese translation. The chunk-level
dependency tree for the Japanese sentence is shown as well.

applied in the node, etc..

3 Composed Rule Extraction

In this section, we first describe an algorithm that
attaches function words to a packed forest guided
by target chunk information. That is, given a triple
⟨FS , T, A⟩, namely an aligned (A) source forest
(FS) to target sentence (T ) pair, we 1) tailor the
alignment A by removing the alignments for tar-
get function words, 2) seek attachable nodes in the
source forest FS for each function word, and 3) con-
struct a derivation forest by topologically travers-

ing FS . Then, we identify minimal and composed
rules from the derivation forest and estimate the
probabilities of rules and scores of derivations us-
ing the expectation-maximization (EM) (Dempster
et al., 1977) algorithm.

3.1 Definitions
In the proposed algorithm, we make use of the fol-
lowing definitions, which are similar to those de-
scribed in (Galley et al., 2004; Mi and Huang, 2008):

• s(·): the span of a (source) node v or a (target)
chunk C, which is an index set of the words that

24



v or C covers;

• t(v): the corresponding span of v, which is an
index set of aligned words on another side;

• c(v): the complement span of v, which is the
union of corresponding spans of nodes v′ that
share an identical parse tree with v but are nei-
ther antecedents nor descendants of v;

• PA: the frontier set of FS , which contains
nodes that are consistent with an alignment A
(gray nodes in Figure 1), i.e., t(v) ̸= ∅ and
closure(t(v)) ∩ c(v) = ∅.

The function closure covers the gap(s) that may
appear in the interval parameter. For example,
closure(t(c3)) = closure({0-1, 4-7}) = {0-7}.
Examples of the applications of these functions can
be found in Table 1. Following (Galley et al.,
2006), we distinguish between minimal and com-
posed rules. The composed rules are generated by
combining a sequence of minimal rules.

3.2 Free attachment of target function words

3.2.1 Motivation
We explain the motivation for the present research

using an example that was extracted from our train-
ing data, as shown in Figure 1. In the alignment of
this example, three lines (in dot lines) are used to
align was and the with ga (subject particle), and was
with ta (past tense auxiliary verb). Under this align-
ment, we are forced to extract rules with relatively
large tree fragments. For example, by applying the
GHKM algorithm (Galley et al., 2004), a rule rooted
at c0 will take c7, t4, c4, c19, t2, and c15 as the
leaves. The final tree fragment, with a height of 7,
contains 13 nodes. In order to ensure that this rule
is used during decoding, we must generate subtrees
with a height of 7 for c0. Suppose that the input for-
est is binarized and that |E| is the average number
of hyperedges of each node, then we must generate
O(|E|26−1) subtrees4 for c0 in the worst case. Thus,

4For one (binarized) hyperedge e of a node, suppose there
are x subtrees in the left tail node and y subtrees in the right tail
node. Then the number of subtrees guided by e is (x + 1) ×
(y +1). Thus, the recursive formula is Nh = |E|(Nh−1 +1)2,
where h is the height of the hypergraph and Nh is the number
of subtrees. When h = 1, we let Nh = 0.

the existence of these rules prevents the generaliza-
tion ability of the final rule set that is extracted.

In order to address this problem, we tailor the
alignment by ignoring these three alignment pairs in
dot lines. For example, by ignoring the ambiguous
alignments on the Japanese function words, we en-
large the frontier set to include from 12 to 19 of the
24 non-terminal nodes. Consequently, the number
of extractable minimal rules increases from 12 (with
three reordering rules rooted at c0, c1, and c2) to
19 (with five reordering rules rooted at c0, c1, c2,
c5, and c17). With more nodes included in the fron-
tier set, we can extract more minimal and composed
monotonic/reordering rules and avoid extracting the
less generalized rules with extremely large tree frag-
ments.

3.2.2 Why chunking?
In the proposed algorithm, we use a target chunk

set to constrain the attachment explosion problem
because we use a packed parse forest instead of a 1-
best tree, as in the case of (Galley et al., 2006). Mul-
tiple interpretations of unaligned function words for
an aligned tree-string pair result in a derivation for-
est. Now, we have a packed parse forest in which
each tree corresponds to a derivation forest. Thus,
pruning free attachments of function words is prac-
tically important in order to extract composed rules
from this “(derivation) forest of (parse) forest”.

In the English-to-Japanese translation test case of
the present study, the target chunk set is yielded
by a state-of-the-art Japanese dependency parser,
Cabocha v0.535 (Kudo and Matsumoto, 2002). The
output of Cabocha is a list of chunks. A chunk con-
tains roughly one content word (usually the head)
and affixed function words, such as case markers
(e.g., ga) and verbal morphemes (e.g., sa re ta,
which indicate past tense and passive voice). For
example, the Japanese sentence in Figure 1 is sepa-
rated into four chunks, and the dependencies among
these chunks are identified by arrows. These arrows
point out the head chunk that the current chunk mod-
ifies. Moreover, we also hope to gain a fine-grained
alignment among these syntactic chunks and source
tree fragments. Thereby, during decoding, we are
binding the generation of function words with the
generation of target chunks.

5http://chasen.org/∼taku/software/cabocha/
25



Algorithm 1 Aligning function words to the forest
Input: HPSG forest FS , target sentence T , word alignment

A = {(i, j)}, target function word set {fw} appeared in
T , and target chunk set {C}

Output: a derivation forest DF

1: A′ ← A \ {(i, s(fw))} ◃ fw ∈ {fw}
2: for each node v ∈ PA′ in topological order do
3: Tv ← ∅ ◃ store the corresponding spans of v
4: for each function word fw ∈ {fw} do
5: if fw ∈ C and t(v)∩(C) ̸= ∅ and fw are not attached

to descendants of v then
6: append t(v) ∪ {s(fw)} to Tv
7: end if
8: end for
9: for each corresponding span t(v) ∈ Tv do

10: R ← IDENTIFYMINRULES(v, t(v), T ) ◃ range
over the hyperedges of v, and discount the factional
count of each rule r ∈ R by 1/|Tv|

11: create a node n in DF for each rule r ∈ R
12: create a shared parent node ⊕ when |R| > 1
13: end for
14: end for

3.2.3 The algorithm

Algorithm 1 outlines the proposed approach to
constructing a derivation forest to include multiple
interpretations of target function words. The deriva-
tion forest is a hypergraph as previously used in
(Galley et al., 2006), to maintain the constraint that
one unaligned target word be attached to some node
v exactly once in one derivation tree. Starting from
a triple ⟨FS , T, A⟩, we first tailor the alignment A
to A′ by removing the alignments for target function
words. Then, we traverse the nodes v ∈ PA′ in topo-
logical order. During the traversal, a function word
fw will be attached to v if 1) t(v) overlaps with the
span of the chunk to which fw belongs, and 2) fw
has not been attached to the descendants of v.

We identify translation rules that take v as the root
of their tree fragments. Each tree fragment is a fron-
tier tree that takes a node in the frontier set PA′
of FS as the root node and non-lexicalized frontier
nodes or lexicalized non-frontier nodes as the leaves.
Also, a minimal frontier tree used in a minimal rule
is limited to be a frontier tree such that all nodes
other than the root and leaves are non-frontier nodes.
We use Algorithm 1 described in (Mi and Huang,
2008) to collect minimal frontier trees rooted at v in
FS . That is, we range over each hyperedges headed
at v and continue to expand downward until the cur-

A → (A′)
node s(·) t(·) c(·) consistent
c0 0-6 0-8(0-3,5-7) ∅ 1
c1 0-6 0-8(0-3,5-7) ∅ 1
c2 0-6 0-8(0-3,5-7) ∅ 1
c3 3-6 0-1,4-7(0-1, 5-7) 2,8 0
c4 3 5-7 0,8(0-3) 1
c5* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c6* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c7 0-1 2-3 0-1,4-8(0-1,5-7) 1
c8* 2-3 4-8(5-7) 0-4(0-3) 0(1)
c9 0 2 0-1,3-8(0-1,3,5-7) 1
c10 1 3 0-2,4-8(0-2,5-7) 1
c11 2-6 0-1,4-8(0-1,5-7) 2-3 0
c12 3 5-7 0,8(0-3) 1
c13* 5-6 0,4(0) 1-8(1-3,5-7) 0(1)
c14 5 4(∅) 0-8(0-3,5-7) 0
c15 6 0 1-8(1-3,5-7) 1
c16 2 4,8(∅) 0-7(0-3,5-7) 0
c17* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c18 4 1 0,2-8(0,2-3,5-7) 1
c19 4 1 0,2-8(0,2-3,5-7) 1
c20* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c21 3 5-7 0,8(0-3) 1
c22 2 4,8(∅) 0-7(0-3,5-7) 0
c23* 2-3 4-8(5-7) 0-4(0-3) 0(1)

Table 1: Change of node attributes after alignment modi-
fication from A to A′ of the example in Figure 1. Nodes
with * superscripts are consistent with A′ but not consis-
tent with A.

rent set of hyperedges forms a minimal frontier tree.
In the derivation forest, we use ⊕ nodes to man-

age minimal/composed rules that share the same
node and the same corresponding span. Figure 2
shows some minimal rule and ⊕ nodes derived from
the example in Figure 1.

Even though we bind function words to their
nearby chunks, these function words may still be at-
tached to relative large tree fragments, so that richer
syntactic information can be used to predict the
function words. For example, in Figure 2, the tree
fragments rooted at node c0−80 can predict ga and/or
ta. The syntactic foundation behind is that, whether
to use ga as a subject particle or to use wo as an ob-
ject particle depends on both the left-hand-side noun
phrase (kekka) and the right-hand-side verb (kensyou
sa re ta). This type of node v′ (such as c0−80 ) should
satisfy the following two heuristic conditions:

• v′ is included in the frontier set PA′ of FS , and

• t(v′) covers the function word, or v′ is the root
node of FS if the function word is the beginning
or ending word in the target sentence T .

Starting from this derivation forest with minimal
26



 

c103-4 

t13: result 

kekka ga  

 

* c103 

t13: result 

kekka  

 

c92 

t32: the 

kono  

 

c72-3 

c103 c92 

x0 x1 

 x0 
 

x1 
 

c72-4 

c103-4 c92 
x0 x1 

 x0 
 

x1 
 * 

c62-7 

c85-7 c72-3 

x0 ga x1  

 x0 
 

x1 
 

* c62-7 

c85-7 c72-4 
x0 x1  

 x0 
 

x1 
 * 

c00-8 

c16 

c45-7 c50-1 

c3 

c72-4 c11 x2 x0 x1 ta 

 

x0 
 

x1 
 

x2 
 

+ 

* 

c00-8 

c16 

c45-8 c50-1 

c3 

c72-4 c11 
x2 x0 x1  

 

x0 
 

x1 
 

x2 
 + 

* 

c00-8 

c16 

c45-7 c50-1 

c3 

c72-3 c11 
x2 x0 ga x1 ta 

 
x0 
 

x1 
 

x2 
 

* + c0
0-8 

c16 

c45-8 c50-1 

c3 

c72-3 c11 
x2 x0 ga x1 

 
x0 
 

x1 
 

x2 
 

* 

+ 

t4{}:was t4{}:was t4{}:was t4{}:was 

Figure 2: Illustration of a (partial) derivation forest. Gray nodes include some unaligned target function word(s).
Nodes annotated by “*” include ga, and nodes annotated by “+” include ta.

rules as nodes, we can further combine two or more
minimal rules to form composed rules nodes and can
append these nodes to the derivation forest.

3.3 Estimating rule probabilities

We use the EM algorithm to jointly estimate 1)
the translation probabilities and fractional counts of
rules and 2) the scores of derivations in the deriva-
tion forests. As reported in (May and Knight, 2007),
EM, as has been used in (Galley et al., 2006) to es-
timate rule probabilities in derivation forests, is an
iterative procedure and prefers shorter derivations
containing large rules over longer derivations con-
taining small rules. In order to overcome this bias
problem, we discount the fractional count of a rule
by the product of the probabilities of parse hyper-
edges that are included in the tree fragment of the
rule.

4 Experiments

4.1 Setup

We implemented the forest-to-string decoder de-
scribed in (Mi et al., 2008) that makes use of forest-
based translation rules (Mi and Huang, 2008) as
the baseline system for translating English HPSG
forests into Japanese sentences. We analyzed the
performance of the proposed translation rule sets by

Train Dev. Test
# sentence pairs 994K 2K 2K
# En 1-best trees 987,401 1,982 1,984
# En forests 984,731 1,979 1,983
# En words 24.7M 50.3K 49.9K
# Jp words 28.2M 57.4K 57.1K
# Jp function words 8.0M 16.1K 16.1K

Table 2: Statistics of the JST corpus. Here, En = English
and Jp = Japanese.

using the same decoder.
The JST Japanese-English paper abstract corpus6

(Utiyama and Isahara, 2007), which consists of one
million parallel sentences, was used for training,
tuning, and testing. Table 2 shows the statistics of
this corpus. Note that Japanese function words oc-
cupy more than a quarter of the Japanese words.
Making use of Enju 2.3.1, we generated 987,401
1-best trees and 984,731 parse forests for the En-
glish sentences in the training set, with successful
parse rates of 99.3% and 99.1%, respectively. Us-
ing the pruning criteria expressed in (Mi and Huang,
2008), we continue to prune a parse forest by set-
ting pe to be 8, 5, and 2, until there are no more than
e10 = 22, 026 trees in a forest. After pruning, there
are an average of 82.3 trees in a parse forest.

6http://www.jst.go.jp

27



C3-T M&H-F Min-F C3-F
free fw Y N Y Y
alignment A′ A A′ A′

English side tree forest forest forest
# rule 86.30 96.52 144.91 228.59
# reorder rule 58.50 91.36 92.98 162.71
# tree types 21.62 93.55 72.98 120.08
# nodes/tree 14.2 42.1 26.3 18.6
extract time 30.2 52.2 58.6 130.7
EM time 9.4 - 11.2 29.0
# rules in dev. 0.77 1.22 1.37 2.18
# rules in test 0.77 1.23 1.37 2.15
DT(sec./sent.) 2.8 15.7 22.4 35.4
BLEU (%) 26.15 27.07 27.93 28.89

Table 3: Statistics and translation results for four types of
tree-to-string rules. With the exception of ‘# nodes/tree’,
the numbers in the table are in millions and the time is in
hours. Here, fw denotes function word, and DT denotes
the decoding time, and the BLEU scores were computed
on the test set.

We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al., 2007) on the training set to obtain
alignments. The SRI Language Modeling Toolkit
(Stolcke, 2002) was employed to train a five-gram
Japanese LM on the training set. We evaluated the
translation quality using the BLEU-4 metric (Pap-
ineni et al., 2002).

Joshua v1.3 (Li et al., 2009), which is a
freely available decoder for hierarchical phrase-
based SMT (Chiang, 2005), is used as an external
baseline system for comparison. We extracted 4.5M
translation rules from the training set for the 4K En-
glish sentences in the development and test sets. We
used the default configuration of Joshua, with the ex-
ception of the maximum number of items/rules, and
the value of k (of the k-best outputs) is set to be 200.

4.2 Results

Table 3 lists the statistics of the following translation
rule sets:

• C3-T: a composed rule set extracted from the
derivation forests of 1-best HPSG trees that
were constructed using the approach described
in (Galley et al., 2006). The maximum number
of internal nodes is set to be three when gen-
erating a composed rule. We free attach target
function words to derivation forests;

0

5

10

15

20

25

2 12 22 32 42 52 62 72 82 92

#
 o
f 
ru
le
s 
(M

)

# of tree nodes in rule

M&H-F

Min-F

C3-T

C3-F

Figure 3: Distributions of the number of tree nodes in the
translation rule sets. Note that the curves of Min-F and
C3-F are duplicated when the number of tree nodes being
larger than 9.

• M&H-F: a minimal rule set extracted from
HPSG forests using the extracting algorithm of
(Mi and Huang, 2008). Here, we make use of
the original alignments. We use the two heuris-
tic conditions described in Section 3.2.3 to at-
tach unaligned words to some node(s) in the
forest;

• Min-F: a minimal rule set extracted from the
derivation forests of HPSG forests that were
constructed using Algorithm 1 (Section 3).

• C3-F: a composed rule set extracted from the
derivation forests of HPSG forests. Similar to
C3-T, the maximum number of internal nodes
during combination is three.

We investigate the generalization ability of these
rule sets through the following aspects:

1. the number of rules, the number of reordering
rules, and the distributions of the number of
tree nodes (Figure 3), i.e., more rules with rel-
atively small tree fragments are preferred;

2. the number of rules that are applicable to the
development and test sets (Table 3); and

3. the final translation accuracies.

Table 3 and Figure 3 reflect that the generalization
abilities of these four rule sets increase in the or-
der of C3-T < M&H-F < Min-F < C3-F. The ad-
vantage of using a packed forest for re-alignment is
verified by comparing the statistics of the rules and

28



0

10

20

30

40

50

0.0

0.5

1.0

1.5

2.0

2.5

C3-T M&H-F Min-F C3-F

D
ec

o
d
in

g
 t
im

e 
(s
ec

./
se

n
t.
)

#
 o
f 
ru

le
s 
(M

)
# rules (M)

DT

Figure 4: Comparison of decoding time and the number
of rules used for translating the test set.

the final BLEU scores of C3-T with Min-F and C3-
F. Using the composed rule set C3-F in our forest-
based decoder, we achieved an optimal BLEU score
of 28.89 (%). Taking M&H-F as the baseline trans-
lation rule set, we achieved a significant improve-
ment (p < 0.01) of 1.81 points.

In terms of decoding time, even though we used
Algorithm 3 described in (Huang and Chiang, 2005),
which lazily generated the N-best translation can-
didates, the decoding time tended to be increased
because more rules were available during cube-
pruning. Figure 4 shows a comparison of decoding
time (seconds per sentence) and the number of rules
used for translating the test set. Easy to observe that,
decoding time increases in a nearly linear way fol-
lowing the increase of the number of rules used dur-
ing decoding.

Finally, compared with Joshua, which achieved
a BLEU score of 24.79 (%) on the test set with
a decoding speed of 8.8 seconds per sentence, our
forest-based decoder achieved a significantly better
(p < 0.01) BLEU score by using either of the four
types of translation rules.

5 Related Research

Galley et al. (2006) first used derivation forests of
aligned tree-string pairs to express multiple inter-
pretations of unaligned target words. The EM al-
gorithm was used to jointly estimate 1) the trans-
lation probabilities and fractional counts of rules
and 2) the scores of derivations in the derivation
forests. By dealing with the ambiguous word align-
ment instead of unaligned target words, syntax-
based re-alignment models were proposed by (May

and Knight, 2007; Wang et al., 2010) for tree-based
translations.

Free attachment of the unaligned target word
problem was ignored in (Mi and Huang, 2008),
which was the first study on extracting tree-to-string
rules from aligned forest-string pairs. This inspired
the idea to re-align a packed forest and a target sen-
tence. Specially, we observed that most incorrect or
ambiguous word alignments are caused by function
words rather than content words. Thus, we focus on
the realignment of target function words to source
tree fragments and use a dependency parser to limit
the attachments of unaligned target words.

6 Conclusion

We have proposed an effective use of target function
words for extracting generalized transducer rules for
forest-based translation. We extend the unaligned
word approach described in (Galley et al., 2006)
from the 1-best tree to the packed parse forest. A
simple yet effective modification is that, during rule
extraction, we account for multiple interpretations
of both aligned and unaligned target function words.
That is, we chose to loose the ambiguous alignments
for all of the target function words. The consider-
ation behind is in order to generate target function
words in a robust manner. In order to avoid gener-
ating too large a derivation forest for a packed for-
est, we further used chunk-level information yielded
by a target dependency parser. Extensive experi-
ments on large-scale English-to-Japanese translation
resulted in a significant improvement in BLEU score
of 1.8 points (p < 0.01), as compared with our
implementation of a strong forest-to-string baseline
system (Mi et al., 2008; Mi and Huang, 2008).

The present work only re-aligns target function
words to source tree fragments. It will be valuable
to investigate the feasibility to re-align all the tar-
get words to source tree fragments. Also, it is in-
teresting to automatically learn a word set for re-
aligning7. Given source parse forests and a target
word set for re-aligning beforehand, we argue our
approach is generic and applicable to any language
pairs. Finally, we intend to extend the proposed
approach to tree-to-tree translation frameworks by

7This idea comes from one reviewer, we express our thank-
fulness here.

29



re-aligning subtree pairs (Liu et al., 2009; Chiang,
2010) and consistency-to-dependency frameworks
by re-aligning consistency-tree-to-dependency-tree
pairs (Mi and Liu, 2010) in order to tackle the rule-
sparseness problem.

Acknowledgments

The present study was supported in part by a Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan), by the Japanese/Chinese Machine Transla-
tion Project through Special Coordination Funds for
Promoting Science and Technology (MEXT, Japan),
and by Microsoft Research Asia Machine Transla-
tion Theme.

Wu (wu.xianchao@lab.ntt.co.jp) has
moved to NTT Communication Science Laborato-
ries and Tsujii (junichi.tsujii@live.com)
has moved to Microsoft Research Asia.

References
David Chiang. 2005. A hierarchical phrase-based model

for statistical machine translation. In Proceedings of
ACL, pages 263–270, Ann Arbor, MI.

David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443–1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
39:1–38.

Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings of HLT-NAACL.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961–968, Sydney.

Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,

Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondřej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177–180.

Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL-2002, pages 63–69. Taipei, Taiwan.

Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Demonstration of joshua: An open source
toolkit for parsing-based machine translation. In Pro-
ceedings of the ACL-IJCNLP 2009 Software Demon-
strations, pages 25–28, August.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609–616, Sydney, Australia.

Yang Liu, Yajuan Lü, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558–566, August.

Samuel E. Martin. 1975. A Reference Grammar of
Japanese. New Haven, Conn.: Yale University Press.

Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 360–368, Prague, Czech Republic,
June. Association for Computational Linguistics.

Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206–214, October.

Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1433–1442, Uppsala, Swe-
den, July. Association for Computational Linguistics.

Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192–199, Columbus, Ohio.

Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35–80.

Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311–318.

30



Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.

Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.

Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901–904.

Masao Utiyama and Hitoshi Isahara. 2007. A japanese-
english patent parallel corpus. In Proceedings of MT
Summit XI, pages 475–482, Copenhagen.

Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247–277.

Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 325–334, Uppsala, Sweden, July. Association
for Computational Linguistics.

Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceedings
of HLT-NAACL, pages 245–253.

Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of MT Summit XI, pages 535–542, Copen-
hagen, Denmark, September.

31


