



















































Chinese Event Coreference Resolution: An Unsupervised Probabilistic Model Rivaling Supervised Resolvers


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1097–1107,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Chinese Event Coreference Resolution: An Unsupervised Probabilistic
Model Rivaling Supervised Resolvers

Chen Chen and Vincent Ng
Human Language Technology Research Institute

University of Texas at Dallas
Richardson, TX 75083-0688

{yzcchen,vince}@hlt.utdallas.edu

Abstract

Recent work has successfully leveraged the
semantic information extracted from lexi-
cal knowledge bases such as WordNet and
FrameNet to improve English event corefer-
ence resolvers. The lack of comparable re-
sources in other languages, however, has made
the design of high-performance non-English
event coreference resolvers, particularly those
employing unsupervised models, very diffi-
cult. We propose a generative model for the
under-studied task of Chinese event corefer-
ence resolution that rivals its supervised coun-
terparts in performance when evaluated on the
ACE 2005 corpus.

1 Introduction

Event coreference resolution is the task of deter-
mining which event mentions in a text refer to the
same real-world event. Compared to entity corefer-
ence, event coreference is not only much less stud-
ied, but it is arguably more challenging. Recall that
for two event mentions to be coreferent, both their
triggers (i.e., the words realizing the occurrence of
the events) and their corresponding arguments (e.g.,
the times, places, and people involved in them) have
to be compatible. However, identifying potential ar-
guments (which is typically performed by an entity
extraction system), linking arguments to their event
mentions (which is typically performed by an event
extraction system), and determining the compatibil-
ity between two event arguments (which is the job
of an entity coreference resolver), are all non-trivial
tasks. In other words, end-to-end event coreference

resolution is complicated in part by the fact that an
event coreference resolver has to rely on the noisy
outputs produced by its upstream components in the
standard information extraction (IE) pipeline.
In this paper, we examine Chinese event coref-

erence resolution. While English event coreference
is under-investigated, Chinese event coreference is
much less studied than English event coreference.
In terms of task definition, there is no difference be-
tween English and Chinese event coreference. How-
ever, the design of high-performance Chinese event
coreference resolvers is complicated in part by the
lack of large-scale lexical knowledge bases. Re-
cent work by Bejan and Harabagiu (2010; 2014) has
shown that the semantic information extracted from
WordNet (Fellbaum, 1998) and FrameNet (Baker
et al., 1998) significantly contributed to the perfor-
mance of their English event coreference resolver.
While the lack of comparable lexical knowledge

bases in Chinese can be mitigated in part by the use
of event coreference annotated data, we focus on a
challenging version of the task --- unsupervised Chi-
nese event coreference resolution. Specifically, our
goal is to learn an event coreference model without
using data annotated with event coreference links.
When evaluated on the Chinese portion of the ACE
2005 corpus, our unsupervised probabilistic model
for event coreference resolution rivals its state-of-
the-art supervised counterpart in performance. This,
together with the fact that its underlying genera-
tive process is not language-dependent and does not
rely on features extracted from lexical knowledge
bases, potentially enables it to be applied to lan-
guages where neither annotated data nor large-scale

1097



knowledge bases are available.
Another feature of our model that deserves men-

tion is that it performs joint event coreference res-
olution and anaphoricity determination. Anaphoric-
ity determination, the task of determining whether
a mention is anaphoric and hence needs to be re-
solved, is an issue common to both entity and event
coreference resolution. However, determining the
anaphoricity of an event mention is arguably more
difficult than determining the anaphoricity of a pro-
noun. The reason is that while there exist lexical and
syntactic cues that can be used to reliably identify
pleonastic pronouns (Bergsma and Yarowsky, 2011),
the lack of such cues in event mentions makes the
identification of anaphoric event mentions challeng-
ing even in a supervised manner, let alone in an un-
supervised manner. Note that ignoring anaphoric-
ity determination and having our model attempt to
resolve every event mention is not a viable option,
as only 24.4% of the Chinese event mentions in our
evaluation corpus (ACE 2005) are anaphoric. Our
decision to jointly model anaphoricity determination
and event coreference resolution was inspired by the
difficulty of designing a standalone system for deter-
mining the anaphoricity of event mentions.

2 Related Work

Almost all existing approaches to event coreference
are developed for English. These approaches can
broadly be divided into three categories.
Within-document coreference is the most pop-

ularly investigated and arguably the most important
event coreference task. While early work in MUC
(e.g., Humphreys et al. (1997)) is limited to sev-
eral scenarios, ACE takes a further step towards pro-
cessing more fine-grained events. Most ACE event
coreference resolvers are supervised, training a pair-
wise model to determine whether two event men-
tions are coreferent (e.g., Ahn (2006)).
Improvements to this standard approach include

the use of feature weighting to train a better model
(McConky et al., 2012), and graph-based cluster-
ing algorithms to produce event coreference clusters
(Chen and Ji, 2009; Sangeetha and Arock, 2012).
Chen et al. (2011) train multiple classifiers to han-
dle coreference between event mentions of different
syntactic types (e.g., verb-noun coreference, noun-

noun coreference) on the OntoNotes corpus (Prad-
han et al., 2007). However, OntoNotes is only par-
tially annotated with event coreference links, and
Chen et al. further make the simplifying assumption
that event coreference chains are all and only those
coreference chains that involve at least one verb.
More recently, Cybulska and Vossen (2012) and

Goyal et al. (2013) have performed event corefer-
ence using semantic relations (e.g., hyponymy rela-
tions extracted fromWordNet) and distributional se-
mantic information, respectively, on the Intelligence
Community (IC) corpus (Hovy et al., 2013). The IC
corpus, which at the time of writing is not yet pub-
licly available, is different from the MUC and ACE
corpora in that it is annotated with not only full event
coreference relations but also partial event corefer-
ence relations. Partial coreference is a term coined
by Hovy et al. to refer to event relations that exhibit
subtle deviation from the perfect identity of events
(e.g., the subset relation, the membership relation).
While all of the aforementioned work addresses the
full event coreference task, a two-stage approach is
recently proposed by Araki et al. (2014) to identify
subevent relations from the IC corpus.
Cross-document coreference is first investigated

by Bagga and Baldwin (1999), who represent an
eventmention as a vector of its context words and de-
termine whether two event mentions are coreferent
based on the cosine similarity of their vectors. Be-
jan and Harabagiu (2010; 2014) and Lee et al. (2012)
propose nonparametric models and a joint entity and
event coreference model respectively for within- and
cross-document event coreference, evaluating their
models on the ECB corpus. However, ECB "is an-
notated mainly for cross-document coreference" and
many difficult cases of within-document coreference
are not annotated (Liu et al., 2014).
Naughton (2009) and Elkhlifi and Faiz (2009)

have worked on sentence-level event coreference,
where the goal is to determinewhether two sentences
containing event mentions are coreferent. Some-
what unfortunately, simplifying assumptions have to
be made when a sentence containing multiple non-
coreferent event mentions is encountered.
Compared to English event coreference, there has

been much less work on Chinese event coreference.
SinoCoreferencer (Chen and Ng, 2014), a publicly-
available ACE-style within-document event corefer-

1098



ence resolver for Chinese that achieves state-of-the-
art results, employs a supervised approach where a
classifier is trained to determine whether two event
mentions are coreferent. We will compare our unsu-
pervised model against this supervised resolver.

3 ACE Event Coreference

In this section, we overview the ACE 2005 event
coreference task, which is the version of the within-
document event coreference task we focus on.
The ACE 2005 event coreference task requires

that an event coreference resolver perform corefer-
ence on event mentions belonging to one of the ACE
event types. More specifically, an event mention is
composed of a trigger (i.e., the word realizing the
event's occurrence) and a set of arguments (i.e., the
event's participants). Each event trigger has a type
and a subtype. In ACE 2005, eight event types are
defined, which are further subcategorized into 33
subtypes. Each event argument has a semantic role.
In ACE 2005, a set of argument roles is defined for
each event type. That is, an event's type determines
what roles its mentions' arguments can assume. Not
surprisingly, two event mentions cannot be corefer-
ent if their triggers have different subtypes or they
have incompatible arguments (e.g., their dates or lo-
cations are different).
To better understand the ACE 2005 event corefer-

ence task, consider the sentence in Figure 1, which
is taken from the ACE 2005 corpus. This example
contains three event mentions belonging to the ACE
event types. Specifically, these three mentions are
triggered by the words 离 (leaving), 暗杀 (assassi-
nated) and攻击 (attack). 暗杀 and攻击 have type
Life and subtype Die, whereas离 has type Move-
ment and subtype Transport. Note that 暗杀
and攻击 refer to the same real-world event and are
therefore coreferent.

4 The Generative Model

In this section, we present our generative model.

4.1 Notation
We begin by introducing the notation that we use in
the rest of this paper. We denote e to be the current
event mention to be resolved (henceforth the active
event mention). C, the set of candidate antecedents

沙米里与其子在上午交通尖峰时间 [离]家时，遭到
[暗杀]。这次 [攻击]再次显示叛乱分子能力。

Shameri and his son were [assassinated] during morning
rush hour when [leaving] home. This [attack] once again
demonstrated the insurgents' ability.

Figure 1: An excerpt from a Chinese document in the
ACE 2005 corpus with the corresponding English trans-
lation. The event mentions are bracketed.

of e, contains all the event mentions preceding e in
the associated text as well as a dummy candidate an-
tecedent d (to which e will be resolved if it is non-
anaphoric). Also, we define k to be the context sur-
rounding e as well as every candidate antecedent c in
C, and kc to be the context surrounding e and can-
didate antecedent c. Moreover, we define l to be a
binary variable indicating whether c is the correct
antecedent of e. Finally, et and ct denote e and c's
respective trigger words.

4.2 Training

Our model estimates P (e, k, c, l), the probability of
seeing (1) the active event mention e; (2) the con-
text k surrounding e and its candidate antecedents;
(3) a candidate antecedent c of e; and (4) l, a binary
value indicating whether c is e's correct antecedent.
Since we estimate this probability from a raw, unan-
notated corpus, we are effectively treating e, k, and
c as observed data and l as hidden data.
Owing to the presence of hidden data, we esti-

mate the model parameters using the Expectation-
Maximization (EM) algorithm (Dempster et al.,
1977). Specifically, we use EM to iteratively esti-
mate the model parameters from data in which each
event mention is labeled with the probability that it
corefers with each of its candidate antecedents, and
apply the resulting model to relabel each event men-
tion with the probability that it corefers with each of
its candidate antecedents. Below we describe the de-
tails of the E-step and the M-step.

4.2.1 E-Step
The goal of the E-step is to compute P (l=1|e, k, c),
the probability that a candidate antecedent c is the
correct antecedent of e given context k. Assuming
that exactly one of the e's candidate antecedents is

1099



its correct antecedent, we can rewrite P (l=1|e, k, c)
as follows:

P (l=1|e, k, c) = P (e, k, c, l=1)∑
c′∈C P (e, k, c′, l=1)

(1)

As we can see from Equation (1), to compute
P (l=1|e, k, c), we need to compute P (e, k, c, l=1),
which can be rewritten using Chain Rule:

P (e, k, c, l=1) = P (e|k, c, l=1) ∗ P (l=1|k, c)
∗ P (c|k) ∗ P (k)

(2)
Next, given l = 1 (i.e., c is the antecedent of e),

we assume that we can generate e from c without
looking at the context. Using this assumption and
approximating e and c by their trigger words, we can
rewrite P (e|k, c, l=1) as follows:

P (e|k, c, l=1) ≈ P (et|ct, l=1) (3)
Moreover, we assume that (1) given e and c's con-

text, the probability of c being the antecedent of e
is not affected by the context of the other candidate
antecedents; and (2) kc is sufficient for determining
whether c is the antecedent of e. So,

P (l=1|k, c) ≈ P (l=1|kc, c) ≈ P (l=1|kc) (4)
Next, applying Bayes Rule to P (l=1|kc), we get:

P (kc|l=1)P (l=1)
P (kc|l=1)P (l=1) + P (kc|l=0)P (l=0) (5)

Representing kc as a set of n features f1c , . . . fnc
and assuming that each f ic is conditionally indepen-
dent given l, we can approximate Expression (5) as:

∏
i P (f

i
c|l=1)P (l=1)∏

i P (f ic|l=1)P (l=1) +
∏

i P (f ic|l=0)P (l=0)
(6)

Given Equations (2), (3), (4) and (6), we can
rewrite P (l=1|e, k, c) as follows:

P (l=1|e, k, c) = P (e, k, c, l=1)∑
c′∈C P (e, k, c′, l=1)

≈ P (et|ct, l=1) ∗
∏

i P (f
i
c|l=1)

Zc
∗ P (c|k)∑

c′∈C P (et|c′t, l=1) ∗
∏

i P (f
i
c′ |l=1)

Zc′
∗ P (c′|k)

(7)

where

Zx =
∏

i

P (f ix|l=1)P (l=1)+
∏

i

P (f ix|l=0)P (l=0)
(8)

As we can see from Equation (7), our model has
four groups of parameters, namely P (et|ct, l=1),
P (f ic=1|l), P (l) andP (c|k). With these four groups
of parameters, we can apply Equation (7) to effi-
ciently compute P (l=1|e, k, c).
Two points deserve mention before we describe

our M-step. First, among the four groups of param-
eters, P (et|ct, l=1) and P (f ic|l) are estimated in the
M-step described below; P (l) is estimated in param-
eter initialization and used throughout the EM itera-
tions (details on parameter initialization appear after
the M-step); and P (c|k) is computed heuristically.
Intuitively, P (c|k) is the prior probability of a can-
didate antecedent c given context k. The simplest
way to model P (c|k) is to assume that every candi-
date antecedent is equally likely given the context. In
practice, however, some candidate antecedents are
implausible given the context. To identify such can-
didate antecedents, we employ a simple heuristic,
which considers a candidate antecedent implausible
if its event subtype is different from that of e. Con-
sequently, we model P (c|k) as follows: if c is im-
plausible, we set P (c|k) to 0 and distribute the prob-
ability mass uniformly over all and only the plausi-
ble candidate antecedents. Since this heuristic is not
applicable to dummy candidates, we assume for sim-
plicity that they are all plausible.
Second, by including d as a dummy candidate an-

tecedent for each e, we model anaphoricity determi-
nation and event coreference in a joint fashion. If the
model resolves e to d, it means that the model posits
e as non-anaphoric; on the other hand, if the model
resolves e to a non-dummy candidate antecedent c,
it means that the model posits e as anaphoric and
c as e's correct antecedent. This joint modeling
method has proven effective in earlier work on su-
pervised entity coreference resolution (e.g., Rahman
and Ng (2009; 2011)).

4.2.2 M-Step
Given P (l=1|e, k, c), the goal of the M-step is
to (re)estimate two of the four groups of param-
eters mentioned above, namely P (et|ct, l=1) and
P (f ic|l), using maximum likelihood estimation.

1100



Specifically, P (et|ct, l=1) is estimated as fol-
lows:

P (et|ct, l=1) = Count(et, ct, l=1) + θ
Count(ct, l=1) + θ ∗ |t| (9)

where Count(ct, l=1) is the expected number of
times c has trigger word ct when it is the antecedent
of an event mention; and |t| is the number of possible
trigger words in the training data (we treat the "trig-
ger word" of a dummy candidate antecedent as an
unseen word). Also, θ is the Laplace smoothing pa-
rameter, which we set to 1, andCount(et, ct, l=1) is
the expected number of times e has et as its trigger
when its antecedent c has trigger ct. Given trigger
words e′t and c′t, we compute Count(e′t, c′t, l=1) as
follows:

Count(e′t, c
′
t, l=1) =

∑
e,c:et=e′t,ct=c′t

P (l=1|e, k, c)

(10)
The remaining group of parameters, P (f ic|l), can

be estimated in a similar fashion.
To start the induction process, we initialize all

parameters with uniform values. Specifically,
P (et|ct, l=1) is set to 1|t| , and P (l=1|kc) is set to
0.5. As noted before, P (l) is also initialized here
and used throughout the EM iterations. Recall that
P (l=1) is the fraction of event pairs that are corefer-
ent. Since we assumed earlier that each event men-
tion has exactly one (dummy or non-dummy) an-
tecedent, P (l=1) can be computed as the number of
event mentions divided by the total number of event
pairs. After initialization, we iteratively run the E-
step and the M-step until convergence.
There is an important question we have not ad-

dressed: what features f ic should we use to represent
context kc, which we need to estimate P (f ic|l)? We
defer the discussion of this question to Section 5.

4.3 Inference
After training, we can apply the resulting model to
resolve event mentions. Given an event mention e,
we determine its antecedent as follows:

ĉ = argmax
c∈C

P (l=1|e, k, c) (11)

where C is the set of candidate antecedents of e. In
other words, we apply Equation (11) to each of e's

candidate antecedents, and select the one that yields
the largest probability. If c is a non-dummy candi-
date antecedent, we posit c as the antecedent of e;
otherwise, we posit e as non-anaphoric.

5 Context Features

As mentioned at the end of Section 4.2.2, to fully
specify our model, we need to describe the features
f ic used to represent kc, which is needed to com-
pute P (f ic|l). Recall that kc encodes the context
surrounding candidate antecedent c and active event
mention e. We represent kc using six features that en-
code the relationship between c and e, some of which
are motivated by previous work on supervised event
coreference resolution (e.g., Chen and Ji (2009)).
Below we describe these six features, which can be
broadly divided into three categories.

5.1 Trigger-Based Features
We employ two trigger-based features (Features 1
and 2), both of which are binary-valued and are com-
puted based on e's and c's triggers.
Feature 1 encodes whether ct and et, the trigger

words of c and e, satisfy any of the following three
conditions:

1. ct and et are lexically identical;
2. ct and et contain the same basic verb (BV) and

their verb structures are compatible;
3. the similarity between ct and et is greater than

a certain threshold (which we set to 0.8 in our
experiments).

Intuitively, Feature 1 is a recall-enhancing feature:
it encodes a condition whose satisfaction can help
discover many event coreference links. However, it
is not designed to be precision-oriented, as it is com-
puted based solely on the triggers and not their sur-
rounding contexts. Below we explain conditions 2
and 3 in more detail.
Recall that condition 2 encodes our observation

that an event coreference relation may exist between
two non-identical trigger words having the same BV
if their verb structures are compatible. To under-
stand this condition, let us explain the notion of BVs
and how we determine the compatibility of two verb
structures. A BV is a single-character Chinese verb,
which is the building block of all Chinese verbs.

1101



Specifically, Li et al. (2012) observe that, with a few
exceptions, a Chinese verb constructed out of a ba-
sic verb bv possesses one of six main verb struc-
tures: (1) bv (e.g., 逮 (arrest)); (2) bv + verb (e.g.,
送到 (deliver), where bv is送); (3) verb + bv (e.g.,
离开 (leave), where bv is开); (4) bv + complemen-
tation (e.g., 进了 (enter), where bv is 进); (5) bv +
noun/adjective (e.g., 开枪 (shoot), where bv is开);
(6) noun/adjective + bv (e.g., 轻伤 (slight wound),
where bv is 伤). Now, assuming that t1 and t2 are
two lexically different trigger words containing the
same BV (bv), we say that their verb structures (de-
noted as vs1 and vs2) are incompatible if one of the
following conditions is satisfied: (1) bv appears in
different positions in t1 and t2 (e.g., 开枪 (shoot)
and离开 (leave), where bv is开); (2) both vs1 and
vs2 have bv + verb or verb + bv as their verb struc-
ture (e.g.,送到 (deliver) and赶到 (reach), where bv
is到); or (3) both vs1 and vs2 have noun/adjective
+ bv or bv + noun/adjective as their verb structure
(e.g.,轻伤 (slight wound) and重伤 (severe wound),
where bv is伤). Note that these three incompatibil-
ity conditions encode our commonsense knowledge
of when two Chinese verbs having the same BV can-
not have the same meaning.
Next, we explain how we compute the similar-

ity between two trigger words in condition 3. To
capture their semantic similarity, we first apply
word2vec (Mikolov et al., 2013) to the Chinese Gi-
gaword corpus (Parker et al., 2009) to obtain a vec-
tor representation of each word and then compute the
cosine similarity between the two word vectors.
Feature 2, our second trigger-based feature, en-

codes whether two nominal event mentions are in-
compatible w.r.t. number. Specifically, its value is
True if and only if (1) c and e are both nouns, and (2)
one is singular and the other is plural. Intuitively,
this feature encodes a non-coreference condition.

5.2 Argument-Based Features

We employ three argument-based features (Fea-
tures 3−5), all of which are binary-valued and are
computed based on c's and e's arguments.
Feature 3 encodes whether c and e possess two ar-

guments that have the same semantic role but dif-
ferent semantic classes.1 Intuitively, Feature 3 en-

1The possible semantic classes are the ACE 2005 entity

codes a non-coreference condition: c and e cannot
be coreferent if such arguments exist.
Feature 4 can be viewed as a generalized version

of Feature 3, encoding whether c and e possess two
arguments that have the same semantic role but are
not coreferent.
Feature 5 encodes whether c and e possess two

named entity (NE) arguments that both have Value
as their NE type but are lexically different. Such
event mentions have a good chance of being not
coreferent.

5.3 Distance Feature
We employ one distance feature (Feature 6) that en-
codes how far c and e are apart from each other in
terms of the number of event mentions. To reduce
data sparseness during parameter estimation, how-
ever, we quantize the distance as follows. Let d be
the distance between the first event mention and the
last event mention in the document for which the dis-
tance feature will be computed. Note that the dis-
tance between an arbitrary pair of event mentions in
this document will be between 0 and d. We divide
the interval [0,d] into four equal-sized regions, and
set the value of the distance feature based on which
of the four bins it falls into.

5.4 Features for Dummy Candidates
Now that we can compute the aforementioned six
features for a non-dummy candidate antecedent, we
next specify how we compute these features for a
dummy candidate antecedent d of active event men-
tion e. For Feature 1, we set the feature value of d
to True, whereas for Features 2−5, we set the fea-
ture value of d to False. To understand why these
values are chosen, note that for each of these fea-
tures the opposite value could be a strong indicator
of non-coreference, potentially causing the model to
have an overly strong bias against selecting d as the
antecedent of e.
Finally, to compute Feature 6, we assume that d

is the zero-th event mention of the associated docu-
ment, and then compute the distance feature in the
same way as described above. By letting d be the
zero-th event mention, we make the probability of
picking d as the correct antecedent (the probability of

types, i.e., Person,Organization,GPE, Facility, and Lo-
cation.

1102



classifying e as non-anaphoric) depend on e's posi-
tion in the associated text. This makes sense because
in general, the probability of e being non-anaphoric
tends to be larger (smaller) when it appears earlier
(later) in the document.

6 Evaluation

6.1 Experimental Setup
Dataset. For evaluation, we conduct five-fold
cross-validation experiments on the 633 Chinese
documents of the ACE 2005 training corpus. Statis-
tics on the corpus are shown in Table 1.
Evaluation measures. We report results in terms
of recall (R), precision (P), and F-score (F) using the
commonly-used coreference evaluation measures
given by the CoNLL scorer, namely the link-based
MUC scorer (Vilain et al., 1995), the mention-based
B3 scorer (Bagga and Baldwin, 1998), the entity-
based version of the CEAF scorer (Luo, 2005), and
the Rand index-based BLANC scorer (Recasens and
Hovy, 2011), after singleton event mentions are re-
moved from the coreference partitions produced by
our resolver. We use the latest version (version 8)
of the CoNLL scorer2, which fixes a bug in pre-
vious versions (Pradhan et al., 2014). In addition,
we report the CoNLL score (Pradhan et al., 2011),
which is the unweighted average of the MUC, B3,
and CEAF F-scores.
Evaluation setting. We perform an end-to-end
evaluation, as it can more accurately reflect the per-
formance of an event coreference resolver when it is
used in practice.
More specifically, to extract the event mentions

used in our evaluation, we employ SinoCorefer-
encer3, which, as mentioned before, is an end-to-end
ACE-style Chinese IE system that achieves state-of-
the-art event coreference results. Specifically, the
event triggers needed to compute the trigger-based
context features are extracted using SinoCorefer-
encer's event extraction subsystem. The event sub-
types needed to identify and filter out implausible
candidate antecedents are also provided by its event
extraction subsystem. The event arguments needed
to compute the argument-based context features are

2conll.github.io/reference-coreference-scorers/
3Downloadable from http://www.hlt.utdallas.edu/

~yzcchen/coreference/

Documents 633
Sentences 9,967

Event mentions 3,333
Event coreference chains 2,521

Table 1: Statistics on the ACE 2005 Chinese corpus.

first extracted and typed by its entity extraction sub-
system, and then linked to their triggers by its event
extraction subsystem. Finally, the entity coreference
links and the semantic roles needed to compute Fea-
ture 4 are provided by its entity coreference sub-
system and its event extraction subsystem, respec-
tively.4 Details of each of these subsystems can be
found in Chen and Ng (2014).

6.2 Results

We employ two supervised resolvers as baseline sys-
tems. The first baseline employs rote learning, sim-
ply positing two event mentions as coreferent if their
corresponding triggers are annotated as coreferent in
the training data. The second baseline is SinoCoref-
erencer, which has produced the best Chinese event
coreference results to date on the ACE corpus.
Row 1 of Table 2 shows the results of the base-

line that employs rote learning. As we can see, this
baseline achieves a CoNLL score of 37.9. Row 2
shows the results of SinoCoreferencer. It performs
significantly better than the rote-learning baseline
w.r.t. all five scoring measures5, achieving a CoNLL
score of 39.2. Finally, row 3 shows the results of our
model. Despite being unsupervised, it significantly
outperforms the better baseline, SinoCoreferencer,
w.r.t. all five scoring measures, achieving a CoNLL
score of 41.5, which is 2.3 points higher than that of
SinoCoreferencer. These results suggest that a gen-
erative approach to unsupervised event coreference
holds promise.

6.3 Ablation Experiments

Recall that in our model eight probability terms play
a major role: P (et|ct), P (c|k), and P (f ic|l) for each

4We employ only those semantic roles that can be reliably
determined by SinoCoreferencer's event extraction subsystem,
namely, Agent, Adjudicator, Defendant, Giver, Per-
son, Place, Position, Organization, Origin, and Re-
cipient.

5All significance tests are paired t-tests, with p < 0.05.

1103



MUC B3 CEAFe BLANC CoNLL
System R P F R P F R P F R P F F
Rote learning 42.6 36.4 39.3 41.4 32.3 36.3 37.0 39.7 38.3 27.4 20.0 23.1 37.9
SinoCoreferencer 42.7 38.3 40.4 41.5 34.7 37.8 39.9 39.2 39.5 28.1 23.7 25.7 39.2
Our model 43.1 42.4 42.8 41.4 39.1 40.2 40.7 42.6 41.6 27.5 26.4 26.9 41.5

Table 2: Five-fold cross-validation event coreference results on the ACE 2005 corpus.

of the six context features. To investigate the con-
tribution of each probability term to overall perfor-
mance, we conduct ablation experiments. Specifi-
cally, in each ablation experiment, we remove ex-
actly one term from the model and retrain it.
Ablation results are shown in Table 3. Each row

contains the F-scores obtained via the five evaluation
measures. To facilitate comparison, the scores of the
model in which all eight probability terms are used is
shown in row 1. As we can see, Feature 1 is the most
useful feature: its removal causes the CoNLL score
to drop significantly by 5.3 points. A closer exam-
ination reveals that the drop in the CoNLL score is
caused by a significant drop in recall w.r.t. all scor-
ers. Recall that this feature encodes the conditions
under which two triggers are likely to be coreferent.
It is perhaps not surprising that its removal causes a
significant drop in recall.
The second most useful feature is P (c|k), which

places zero probability mass on candidate an-
tecedents whose event subtypes are different from
that of the active event mention. Its removal causes
the CoNLL score to drop significantly by 1.6 points.
The removal of each other feature resulted in a small,
insignificant drop in the CoNLL score.

6.4 Error Analysis

It is somewhat surprising that our unsupervised event
coreference model outperforms the better supervised
baseline, SinoCoreferencer. To understand why, we
analyze the errors made by the two resolvers.
Our analysis proceeds as follows. First, to gain in-

sights into the differences between the two resolvers,
we examine those candidate event mentions that are
correctly handled by one model but not the other
(Section 6.4.1). Specifically, we consider a candi-
date event mention e correctly handled if (1) e is a
correctly resolved anaphoric event mention; (2) e is
an unresolved singleton event mention; or (3) e is an
unresolved non-event mention (i.e., not a true event

System MUC B3 CEAFe BLANCCoNLL
Full model 42.8 40.2 41.6 26.9 41.5
− P (et|ct) 42.9 39.8 40.9 26.9 41.2
− P (c|k) 41.2 38.6 39.8 24.9 39.9
− Feature 1 37.5 32.9 38.2 20.8 36.2
− Feature 2 42.5 39.9 41.4 26.6 41.3
− Feature 3 42.4 40.0 41.3 26.9 41.2
− Feature 4 42.5 40.1 41.7 27.0 41.4
− Feature 5 42.4 40.0 41.4 26.5 41.3
− Feature 6 42.3 39.6 40.9 26.8 40.9

Table 3: Ablation results in terms of F-scores.

mention). Second, to understand how to improve
event coreference, we identify the major sources of
error made by both resolvers (Section 6.4.2).

6.4.1 Common Sources of Disagreement
There are 323 candidate event mentions that are cor-
rectly handled by one model but not the other in our
dataset. Among these 323 cases, 205 (50 anaphoric +
79 singletons + 76 non-event) are correctly handled
by the unsupervised model, and 118 (42 anaphoric +
52 singletons + 24 non-event) are correctly handled
by SinoCoreferencer.
From these numbers, we can see that the unsuper-

vised model performs far better than SinoCorefer-
encer in not resolving the singletons and the non-
event mentions. This is perhaps not surprising
given the unsupervised model's relatively stricter
conditions on resolving a candidate event mention.
Specifically, it is unlikely to posit two candidate
event mentions as coreferent unless (1) their trig-
gers have a BV match or a large word2vec similar-
ity value and (2) none of the non-coreference condi-
tions are satisfied. Overall, these results explain why
the unsupervised model has a much higher precision
than SinoCoreferencer.
Not only does the unsupervised model perform

much better in not resolving singletons and non-
event mentions, but it is also slightly more accurate

1104



in resolving the anaphoric event mentions, which ul-
timately enables it to achieve a higher recall than
SinoCoreferencer. In particular, it correctly resolves
50 anaphoric mentions that are incorrectly handled
by SinoCoreferencer. The successful resolution of
these anaphoric mentions can be attributed largely to
its use of BV and word2vec, neither of which is ex-
ploited by SinoCoreferencer. However, while a BV
match or a high word2vec similarity value is a good
indicator of event coreference, they are by no means
perfect. This partly explains why there are singletons
and non-event mentions that are correctly handled by
SinoCoreferencer but not the unsupervised model.
Despite the fact that SinoCoreferencer slightly

lags behind the unsupervised model in resolv-
ing anaphoric mentions, it correctly resolves 42
anaphoric event mentions that are incorrectly han-
dled by the unsupervised model. These are cases
that cannot be handled simply by relying on BV
match or word2vec similarity. More specifically,
two of the unique features of SinoCoreferencer are
primarily responsible for its successful resolution of
these event mentions. First, it learns coreferent trig-
ger pairs from the training data. These pairs proved
to be useful for event coreference, as we saw from
the competitive results provided by the rote-learning
baseline. Second, unlike the unsupervised model,
SinoCoreferencer can posit two event mentions as
coreferent without considering their triggers. More
specifically, SinoCoreferencer may posit two event
mentions as coreferent if the corresponding argu-
ments of the two event mentions (i.e., arguments
having the same role) are coreferent. Neither of these
two recall-enhancing features of SinoCoreferencer
is a precise indicator of event coreference. In other
words, employing themwidens the precision gap be-
tween the two resolvers.

6.4.2 Common Sources of Error
Next, we discuss the major sources of error made
by both our unsupervised model and SinoCorefer-
encer. Broadly, the errors can be divided into two
categories, precision errors and recall errors.
Precision errors arise primarily from erroneous

coreference links established between (1) one or
more candidate eventmentions that are not true event
mentions; (2) two event mentions with incompati-
ble latent attributes such asModality, Polarity,

Genericity, and Tense, since these attributes are
not exploited by the two resolvers; (3) two event
mentions with incompatible arguments, since these
arguments fail to be extracted by the argument iden-
tification component; (4) two mentions representing
events that occur at different times, since the event
mentions are not timestamped6; and (5) two event
mentions whose corresponding arguments are incor-
rectly posited by the entity coreference subsystem as
coreferent.
On the other hand, recall errors arise primarily

from missing coreference links attributed to (1) the
trigger identification component's failure to detect
one or both of the triggers involved in an event coref-
erence link; (2) the entity coreference subsystem's
failure to establish the link(s) between the corre-
sponding arguments of two coreferent event men-
tions; (3) the lack of positive evidence of event coref-
erence, such as BV match, high word2vec similar-
ity, and coreferent arguments; and (4) the argument
identification component's failure to extract one or
more arguments of an event mention.

7 Conclusions

We presented a generative model for the rela-
tively under-studied task of unsupervised Chinese
event coreference resolution whose parameters were
learned using EM from an unannotated corpus.
When evaluated on the ACE 2005 corpus, our model
significantly outperforms SinoCoreferencer, a state-
of-the-art Chinese event coreference resolver.
Since the performance of our resolver is limited in

part by the errors made by SinoCoreferencer's sub-
systems, we plan to mitigate this problem by per-
forming joint inference for entity coreference, event
extraction and event coreference in future work.

Acknowledgments

We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of this paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142.

6Not all of these errors can be fixed by exploiting the Tense
attribute, as Tense is only a crude approximation of time. For
instance, in the phrases 首度被捕 (arrested for the first time)
and 再度被捕 (arrested again), the two occurrences of 被捕
(arrested) are associated with different timestamps despite the
fact that they have the same Tense.

1105



References
David Ahn. 2006. The stages of event extraction. In

Proceedings of the Workshop on Annotating and Rea-
soning about Time and Events, pages 1--8.

Jun Araki, Zhengzhong Liu, Eduard Hovy, and Teruko
Mitamura. 2014. Detecting subevent structure
for event coreference resolution. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation, pages 4553--4558.

Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of the
Linguistic Coreference Workshop at the First Interna-
tional Conference on Language Resources and Evalu-
ation, page 563--566.

Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: Annotation, experiments, and ob-
servations. In Proceedings of the ACL Workshop on
Coreference and Its Applications, pages 1--9.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics and the 17th International
Conference on Computational Linguistics, Volume 1,
pages 86--90.

Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguistic
features. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1412--1422.

Cosmin Bejan and Sanda Harabagiu. 2014. Unsuper-
vised event coreference resolution. Computational
Linguistics, 40(2):311--347.

Shane Bergsma and David Yarowsky. 2011. NADA:
A robust system for non-referential pronoun detection.
In Proceedings of the 8th Discourse Anaphora and
Anaphor Resolution Colloquium, pages 12--23.

Zheng Chen and Heng Ji. 2009. Graph-based event
coreference resolution. In Proceedings of the 2009
Workshop on Graph-based Methods for Natural Lan-
guage Processing (TextGraphs-4), pages 54--57.

Chen Chen andVincent Ng. 2014. SinoCoreferencer: An
end-to-end Chinese event coreference resolver. InPro-
ceedings of the Ninth International Conference on Lan-
guage Resources and Evaluation, pages 4532--4538.

Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A unified event coreference resolution by inte-
grating multiple resolvers. In Proceedings of the 5th
International Joint Conference on Natura Language
Processing, pages 102--110.

Agata Cybulska and Piek Vossen. 2012. Using se-
mantic relations to solve event coreference in text.
In Proceedings of the LREC Workshop on Semantic

Relations-II Enhancing Resources and Applications
(SemRel 2012), pages 60--67.

Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39:1--38.

Aymen Elkhlifi and Rim Faiz. 2009. Automatic anno-
tation approach of events in news articles. Interna-
tional Journal of Computing and Information Sciences,
7(1):40--50.

Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.

Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard Hovy.
2013. A structured distributional semantic model for
event co-reference. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics: Volume 2 (Short Papers), pages 467--473.

Eduard Hovy, Teruko Mitamura, Felisa Verdejo, Jun
Araki, and Andrew Philpot. 2013. Events are not
simple: Identity, non-identity, and quasi-identity. In
Proceedings of the NAACL-HLT Workshop on Events:
Definition, Detection, Coreference, and Representa-
tion, pages 21--28.

Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information ex-
traction. In Proceedings of the ACL/EACL Workshop
on Operational Factors in Practical, Robust Anaphora
Resolution for Unrestricted Texts, pages 75--81.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489--500.

Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Libin
Hou. 2012. Employing compositional semantics
and discourse consistency in Chinese event extrac-
tion. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
andComputational Natural Language Learning, pages
1006--1016.

Zhengzhong Liu, Jun Araki, Eduard Hovy, and Teruko
Mitamura. 2014. Supervised within-document event
coreference using information propagation. In Pro-
ceedings of the Ninth International Conference on Lan-
guage Resources and Evaluation, pages 4539--4544.

Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 25--
32.

1106



Katie McConky, Rakesh Nagi, Moises Sudit, and
William Hughes. 2012. Improving event co-reference
by context extraction and dynamic feature weighting.
In Proceedings of the 2012 IEEE International Multi-
Disciplinary Conference on Cognitive Methods in Sit-
uation Awareness and Decision Support, pages 38--43.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality.
In Proceedings of the Advances in Neural Information
Processing Systems, pages 3111--3119.

Martina Naughton. 2009. Sentence Level Event Detec-
tion and Coreference Resolution. Ph.D. thesis, Na-
tional University of Ireland, Dublin, Ireland.

Robert Parker, David Graff, Ke Chen, Junbo Kong, and
Kazuaki Maeda. 2009. Chinese Gigaword fourth edi-
tion. Linguistic Data Consortium, Philadelphia, PA.

Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In Proceedings of the International Con-
ference on Semantic Computing, pages 446--453.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1--27.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted mentions:
A reference implementation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
30--35.

Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968--977.

Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. Journal of Artificial Intelligence Re-
search, 40:469--521.

Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand Index for coreference evalua-
tion. Natural Language Engineering, 17(4):485--510.

S. Sangeetha and Michael Arock. 2012. Event corefer-
ence resolution using mincut based graph clustering.
In Proceedings of the Fourth International Workshop
on Computer Networks & Communications, pages
253--260.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings

of the SixthMessageUnderstanding Conference, pages
45--52.

1107


