630

Coling 2010: Poster Volume, pages 630–638,

Beijing, August 2010

Enhancing Multi-lingual Information Extraction

via Cross-Media Inference and Fusion 

Adam Lee, Marissa Passantino, Heng Ji 

Computer Science Department 

Queens College and Graduate Center 

City University of New York 

hengji@cs.qc.cuny.edu

Guojun Qi, Thomas Huang 

Department of Electrical and Computer  

Engineering & Beckman Institute 

University of Illinois at Urbana-Champaign 

huang@ifp.uiuc.edu

Abstract

We  describe  a  new  information  fusion 
approach  to  integrate  facts  extracted 
from  cross-media  objects  (videos  and 
texts) into a coherent common represen-
tation  including  multi-level  knowledge 
(concepts, relations and events). Beyond 
standard  information  fusion,  we  ex-
ploited  video  extraction  results  and  sig-
nificantly improved text Information Ex-
traction. We further extended our meth-
ods  to  multi-lingual  environment  (Eng-
lish, Arabic and Chinese) by presenting 
a case study on cross-lingual comparable 
corpora acquisition based on video com-
parison.

the 

signs 

Introduction 

1
An  enormous  amount  of  information  is  widely 
available in various data modalities (e.g. speech, 
text,  image  and  video).  For  example,  a  Web 
news  page  about  “Health  Care  Reform  in 
America”  is  composed  with  texts  describing 
some  events  (e.g.,  Final  Senate  vote  for  the 
reform  plans,  Obama 
reform 
agreement),  images  (e.g.,  images  about  various 
government  involvements  over  decades)  and 
videos/speech (e.g. Obama’s speech video about 
the decisions) containing additional information 
regarding  the  real  extent  of  the  events  or 
providing  evidence  corroborating  the  text  part. 
These  cross-media  objects  exist  in  redundant 
and  complementary  structures,  and  therefore  it 
is  beneficial  to  fuse  information  from  various 
data  modalities.  The  goal  of  our  paper  is  to 
investigate this task from both mono-lingual and 
cross-lingual perspectives. 

image/video 

  The  processing  methods  of 
texts  and 
images/videos  are  typically  organized  into  two 
separate  pipelines.  Each  pipeline  has  been 
studied separately and quite intensively over the 
past  decade.  It  is  critical  to  move  away  from 
single  media  processing,  and  instead  toward 
methods  that  make  multiple  decisions  jointly 
using cross-media inference. For example, video 
analysis  allows  us  to  find  both  entities  and 
events  in  videos,  but  it’s  very  challenging  to 
specify  some  fine-grained  semantic  types  such 
as  proper  names  (e.g.  “Obama  Barack”)  and 
relations  among  concepts;  while  the  speech 
embedded  and  the  texts  surrounding  these 
videos can significantly enrich such analysis. On 
the  other  hand, 
features  can 
enhance  text  extraction.    For  example,  entity 
gender detection from speech recognition output 
is  challenging  because  of  entity  mention 
recognition  errors.  However,  gender  detection 
from  corresponding  images  and  videos  can 
achieve above 90% accuracy (Baluja and Rowley, 
2006). In this paper, we present a case study on 
gender  detection  to  demonstrate  how  text  and 
video extractions can boost each other. 
  We  can  further  extend  the  benefit  of  cross-
media  inference  to  cross-lingual  information 
extraction  (CLIE).  Hakkani-Tur  et  al.  (2007) 
found that CLIE performed notably worse than 
monolingual  IE,  and  indicated  that  a  major 
cause was the low quality of machine translation 
(MT).  Current  statistical  MT  methods  require 
large  and  manually  aligned  parallel  corpora  as 
input  for  each  language  pair  of  interest.  Some 
recent work (e.g. Munteanu and Marcu, 2005; Ji, 
2009)  found  that  MT  can  benefit  from  multi-
lingual  comparable  corpora  (Cheung  and  Fung, 
2004), but it is time-consuming to identify pairs 
of  comparable  texts;  especially  when  there  is 

631

three 

lack of parallel information such as news release 
dates  and  topics.  However,  the  images/videos 
embedded  in  the  same  documents  can  provide 
additional  clues  for  similarity  computation 
because  they  are  ‘language-independent’.  We 
will  show  how  a  video-based  comparison 
approach  can  reliably  build  large  comparable 
text  corpora  for 
languages:  English, 
Chinese and Arabic. 
2 Baseline Systems 
We apply the following state-of-the-art text and 
video  information  extraction  systems  as  our 
baselines.  Each  system  can  produce  reliable 
confidence values based on statistical models. 
2.1 Video Concept Extraction 
The  video  concept  extraction  system  was 
the  TREC  Video 
developed  by  IBM  for 
Retrieval 
Evaluation 
(TRECVID-2005) 
(Naphade et al., 2005). This system can extract 
2617  concepts  defined  by  TRECVID,  such  as 
"Hospital",  "Airplane"  and  "Female-Person".  It 
uses  support  vector  machines  to  learn  the 
mapping  between  low  level  features  extracted 
from visual modality as well as from transcripts 
and  production  related  meta-features.  It  also 
exploits a Correlative Multi-label Learner (Qi et 
al., 2007), a Multi-Layer Multi-Instance Kernel 
(Gu et al., 2007) and Label Propagation through 
Linear  Neighborhoods  (Wang  et  al.,  2006)  to 
extract  all  other  high-level  features.  For  each 
classifier,  different  models  are  trained  on  a  set 
of different modalities (e.g., the color moments, 
wavelet textures, and edge histograms), and the 
predictions  made  by 
these  classifiers  are 
combined  together  with  a  hierarchical  linearly-
weighted 
strategy  across  different 
modalities and classifiers. 
2.2
We  use  a  state-of-the-art  IE  system  (Ji  and 
Grishman,  2008)  developed  for  the  Automatic 
Content  Extraction  (ACE)  program1 to  process 
texts  and  automatic  speech  recognition  output. 
The  pipeline  includes  name  tagging,  nominal 
mention  tagging,  coreference  resolution,  time 
expression  extraction  and  normalization,  rela-
tion  extraction  and  event  extraction.  Entities 

Text Information Extraction 

fusion 

1 http://www.nist.gov/speech/tests/ace/ 

include coreferred persons, geo-political entities 
(GPE), locations, organizations, facilities, vehi-
cles  and  weapons;  relations  include  18  types 
(e.g. “a town some 50 miles south of Salzburg”
indicates a located relation.); events include the 
33  distinct  event  types  defined  in  ACE  2005 
(e.g.  “Barry Diller on Wednesday quit as chief 
of Vivendi Universal Entertainment.” indicates a 
“personnel-start”  event).  Names  are  identified 
and  classified  using  an  HMM-based  name  tag-
ger.  Nominals  are  identified  using  a  maximum 
entropy-based  chunker  and  then  semantically 
classified  using  statistics  from  ACE  training 
corpora. Relation extraction and event extraction 
are  also  based  on  maximum  entropy  models, 
incorporating diverse lexical, syntactic, semantic 
and ontological knowledge. 
3 Mono-lingual 
and Inference 

Information  Fusion 

3.1 Mono-lingual System Overview 

Multi-media  
Document 

                                                 

ASR

Speech

Videos/Images 

                        

Texts

                         

Partitioning 

Text Information

Extraction

Video Concept 

Extraction 

Entities/ 

Relations/Events

Concepts 

Multi-level Concept Fusion 

Global Inference 

Enhanced  

Concepts/Entities 
Relations/Events

Figure 1. Mono-lingual Cross-Media  

Information Fusion and Inference Pipeline 

Figure  1  depicts  the  general  procedure  of  our 
mono-lingual  information  fusion  and  inference 

632

For 

and 

focus. 

in  ACE 

is  mapped 

approach. After we apply two baseline systems 
to  the  multi-media  documents,  we  use  a  novel 
multi-level concept fusion approach to extract a 
common  knowledge  representation  across  texts 
and videos (section 3.2), and then apply a global 
inference  approach  to  enhance  fusion  results 
(section 3.3). 
3.2 Cross-media Information Fusion 
• Concept Mapping 
For each input video, we apply automatic speech 
recognition to obtain background texts. Then we 
use the baseline IE systems described in section 
2 to extract concepts from texts and videos. We 
construct  mappings  on  the  overlapped  facts 
across  TRECVID  and  ACE.  For  example, 
“LOC.Water-Body” 
to 
“Beach,  Lakes,  Oceans,  River,  River_Bank”  in 
TRECVID.
  Due  to  different  characteristics  of  video  clips 
and  texts,  these  two  tasks  have  quite  different 
granularities 
example, 
“PER.Individual” in ACE is an open set includ-
ing arbitrary names, while TRECVID only cov-
ers  some 
famous  proper  names  such  as 
“Hu_Jintao”  and  “John_Edwards”.  Geopolitical 
entities appear very rarely in TRECVID because 
they  are  more  explicitly  presented  in  back-
ground texts. On the other hand, TRECVID de-
fined  much  more  fine-grained  nominals  than 
ACE, for example, “FAC.Building-Grounds” in 
ACE  can  be  divided  into  52  possible  concept
types  such  as  “Conference_Buildings”  and 
“Golf_Course” because they can be more easily 
detected based on video features. We also notice 
that  TRECVID  concepts  can  include  multiple 
levels 
example 
“WEA_Shooting” concept can be separated into 
“weapon”  entities  and  “attack”  events  in  ACE. 
These  different  definitions  bring  challenges  to 
cross-media fusion but also opportunities to ex-
ploit  complementary  facts  to  refine  both  pipe-
lines.  We  manually  resolved  these  issues  and 
obtained 20 fused concept sets. 
• Time-stamp based Multi-level Projection 
After extracting facts from videos and texts, we 
conduct information fusion at all possible levels: 
name,  nominal,  coreference  link,  relation  or 
event mention. We rely on the timestamp infor-
mation associated with video keyframes or shots 

of  ACE 

facts, 

for 

(sequential  keyframes)  and  background  speech 
to align concepts. During this fusion process, we 
compare the normalized confidence values pro-
duced from two pipelines to resolve the follow-
ing three types of cases:
• Contradiction  –  A  video  fact  contradicts  a 
text fact; we only keep the fact with higher 
confidence.

• Redundancy  –  A  video  fact  conveys  the 
same content as (or entails, or is entailed by) 
a text fact; we only keep the unique parts of 
the facts. 

• Complementary  –  A  video  fact  and  a  text 
fact  are  complementary;  we  merge  these 
two to form more complete fact sets. 

• A Common Representation 
In order to effectively extract compact informa-
tion from large amounts of heterogeneous data, 
we  design  an  integrated  XML  format  to  repre-
sent  the  facts  extracted  from  the  above  multi-
level fusion. We can view this representation as 
a  set  of  directed  “information  graphs”  G={Gi
(Vi, Ei)}, where Vi is the collection of concepts 
from both texts and videos, and Ei is the collec-
tion  of  edges  linking  one  concept  to  the  other, 
labeled by relation or event attributes. An exam-
ple is presented in Figure 2. This common rep-
resentation is applied in both mono-lingual and 
multi-lingual information fusion tasks described 
in next sections. 

Amina Abbas 
Spouse

Mahmoud 

Abbas

Child 

Elected/
2008-11-23 

Located

PLO

Leader

PLO

Birth-Place
Safed

Yasser
 Abbas

British 

Mandate of
Palestine 

Figure 2. An example for cross-media common 

fact representation  

3.3 Cross-media Information Inference 
• Uncertainty Problem in Cross-Media Fu-

sion

However, such a simple merging approach usu-
ally  leads  to  unsatisfying  results  due  to  uncer-
tainty.  Uncertainty  in  multimedia  is  induced 
from  noise  in  the  data  acquisition  procedure 

633

(e.g.,  noise  in  automatic  speech  recognition  re-
sults  and  low-quality  camera  surveillance  vid-
eos)  as  well  as  human  errors  and  subjectivity. 
Unstructured  texts,  especially  those  translated 
from foreign languages, are difficult to interpret. 
In addition, automatic IE systems for both vid-
eos and texts tend to produce errors.  
• Case Study on Mention Gender Detection 
We  employ  cross-media  inference  methods  to 
reduce  uncertainty.  We  will  demonstrate  this 
approach on a case study of gender detection for 
persons.  Automatic  gender  detection  is  crucial 
to many natural language processing tasks such 
as  pronoun  reference  resolution  (Bergsma, 
2005).  Gender  detection  for  last  names  has 
proved challenging; Gender for nominals can be 
highly ambiguous in various contexts. Unfortu-
nately most state-of-the-art approaches discover 
gender information without considering specific 
contexts  in  the  document.  The  results  were 
stored  either  as  a  knowledge  base  with  prob-
abilities  (e.g.  Ji  and  Lin,  2009)  or  as  a  static 
gazetteer (e.g. census data). Furthermore, speech 
recognition normally performs poorly on names, 
which  brings  more  challenges  to  gender  detec-
tion for mis-spelled names.   
  We consider two approaches as our baselines. 
The  first  baseline  is  to  discover  gender  knowl-
edge from Google N-grams using specific lexi-
cal 
and 
his/her/its/their”)  (Ji  and  Lin,  2009).  The  other 
baseline is a gazetteer matching approach based 
on census data including person names and gen-
der  information,  as  used  in  typical  text  IE  sys-
tems.  
  We  introduce  the  third  method  based  on 
male/female concept extraction from associated 
background videos. These concepts are detected 
from  context-dependent  features  (e.g.  face  rec-
ognition).  If  there  are  multiple  persons  in  one 
snippet  associated  with  one  shot,  we  propagate 
gender information to all instances.
  We then linearly combine these three methods 
based  on  confidence  values.  For  example,  the 
confidence of predicting a name mention n as a 
male (M) can be computed by combining prob-
abilities P(n, M, method):
confidence(n,male)=(cid:554)1*P(n,M,ngram)+(cid:3)(cid:3)
(cid:554)2*P(n,M,census) +(cid:554)3*P(n,M,video)

“[mention] 

patterns 

(e.g. 

this  paper  we  used(cid:172)1=0.1,(cid:3) (cid:172)2=0.1 
  In 
and(cid:172)3=0.8 which are optimized from a devel-
opment set. 
4 Cross-lingual  Comparable  Corpora 

Acquisition

In this section we extend the information fusion 
approach  to  a  task  of  discovering  comparable 
corpora.
4.1 Comparable Documents  
Figure  3  presents  an  example  of  cross-lingual 
comparable documents. They are both about the 
rescue activities for the Haiti earthquake. 

Figure 3. An example for cross-lingual  
multi-media comparable documents 

Multi-media 
Document in
Language i

Multi-media  
Document in 
Language j

Text T i

Video V i

Video V i 

Text T j

Concept 
Extraction 

Concept 
Extraction 

Facts-Vi

Facts-Vj

Similarity Computation 

Similarity>δ?

Comparable Docu-

ments <Ti, Tj>

Figure 4. Cross-lingual Comparable Text  

Corpora Acquisition based on
Video Similarity Computation 

634

  Traditional text translation based methods tend 
to miss such pairs due to poor translation quality 
of informative words (Ji et al., 2009). However, 
the background videos and images are language-
independent and  thus  can  be  exploited  to  iden-
tify such comparable documents. This provides 
a  cross-media  approach  to  break  language  bar-
rier.
4.2 Cross-lingual System Overview 
Figure 4 presents the general pipeline of discov-
ering cross-lingual comparable documents based 
on  background  video  comparison.  The  detailed 
video  similarity  computation  method  is  pre-
sented in next section. 
4.3 Video Concept Similarity Computation 
Most document clustering systems use represen-
tations built out of the lexical and syntactic at-
tributes.  These  attributes  may  involve  string 
matching,  agreement,  syntactic  distance,  and 
document  release  dates.  Although  gains  have 
been made with such methods, there are clearly 
cases where shallow information will not be suf-
ficient to resolve clustering correctly. Therefore, 
we  should  therefore  expect  a  successful  docu-
ment  comparison  approach  to  exploit  world 
knowledge,  inference,  and  other  forms  of  se-
mantic  information  in  order  to  resolve  hard 
cases.  For  example,  if  two  documents  include 
concepts  referring  to  male-people,  earthquake 
event,  rescue  activities,  and  facility-grounds 
with similar frequency information, we can de-
termine they are likely to be comparable. In this 
paper  we  represent  each  video  as  a  vector  of 
semantic  concepts  extracted  from  videos  and 
then  use  standard  vector  space  model  to  com-
pute similarity.  
  Let  A=(a1,  …a|(cid:283)|)  and  B=(b1,  …b|(cid:283)|)  be  such 
vectors for a pair of videos, then we use cosine 
similarity to compute similarity: 

cos(

A B
,

)

=

|

(cid:166)
|
(cid:166)
=
i
1
(cid:166)
|
a
2
i
=
i
1

|

a b
i
i
(cid:166)

(cid:166)

,

|

(cid:166)
|
=
i
1

b
2
i

where  |(cid:283)|  contains  all  possible  concepts.  We 
use traditional TF-IDF (Term Frequency-Inverse 
Document  Frequency)  weights  for  the  vector 
elements ai and bi. Let C be a unique concept, V

is a video consisting of a series of k shots V = 
{S1, …, Sk}, then: 

tf C V
,

(

)

= (cid:166)

k
=
i
1

tf C S
,

(

)

i

k

Let p(C, Si) denote the probability that C is ex-
tracted from Si, we define two different ways to 
compute term frequency tf (C, Si):

(1) 
and
(2) 

tf C S
,

(

=

)

i

confidence C S
,

(

)

i

tf C S α=

(

)

,

i

confidence C S
,

(

)

i

Where Confidence (C, Si) denotes the probabil-
ity of detecting a concept C in a shot Si:

confidence C S
,
                                       otherwise 0. 

p C S
(
,

if

(

)

)

i

i

=

p C S δ> ,
(

,

)i

,

(

) 1

df C S = if

Let:
assuming there are j shots in the entire corpus, 
we calculate idf as follows: 

p C S δ> , otherwise 0, 
(

)i

,

i

idf C V
,

(

)

=

log

j

/

(cid:167)
(cid:168)
(cid:169)

j

(cid:166)

=
1

i

df C S
,

(

)

i

(cid:183)
(cid:184)
(cid:185)

corresponding 

5 Experimental Results 
This section presents experimental results of all 
the three tasks described above. 
5.1 Data
We used 244 videos from TRECVID 2005 data 
set as our test set. This data set includes 133,918 
keyframes,  with 
automatic 
speech  recognition  and  translation  results  (for 
foreign languages) provided by LDC.   
5.2
Table  1  shows  information  fusion  results  for 
English, Arabic and Chinese on multiple levels. 
It  indicates  that  video  and  text  extraction  pipe-
lines  are  complementary  –  almost  all  of  the 
video  concepts  are  about  nominals  and  events; 
while  text  extraction  output  contains  a  large 
amount  of  names  and  relations.  Therefore  the 
results  after  information  fusion  produced  much 
richer knowledge. 

Information Fusion Results 

635

Annotation Lev-

els

# of videos 

Video  Concept 
Name 
Nominal 
Relation 
Event 

Text

English  Chinese  Arabic 

104 

250880 
17350 
31528 
9645 
31132 

84 

221898 
22154 
21852 
20880 
10348 

56 

197233 
20057 
16253 
16584 
7148 

Table 1. Information Fusion Results  

  It’s also worth noting that the number of con-
cepts  extracted  from  videos  is  similar  across 
languages,  while  much  fewer  events  are  ex-
tracted  from  Chinese  or  Arabic  because  of 
speech  recognition  and  machine  translation  er-
rors. We took out 1% of the results to measure 
accuracy against ground-truth in TRECVID and 
ACE training data respectively; the mean aver-
age  precision  for  video  concept  extraction  is 
about  33.6%.  On  English ASR  output  the  text-
IE system achieved about 82.7% F-measure on 
labeling  names,  80.5%  F-measure  on  nominals 
(regardless  of  ASR  errors),  66%  on  relations 
and 64% on events. 
5.3
From  the  test  set,  we  chose  650  persons  (492 
males and 158 females) to evaluate gender dis-
covery.  For  baselines,  we  used  Google  n-gram 
(n=5) corpus Version II including 1.2 billion 5-
grams extracted from about 9.7 billion sentences 
(Lin  et  al.,  2010)  and  census  data  including  
5,014 person names with gender information. 
  Since  we  only  have  gold-standard  gender  in-
formation  on  shot-level  (corresponding  to  a 
snippet in ASR output), we asked a human an-
notator to associate ground-truth with individual 
persons.  Table  2  presents  overall  precision  (P), 
recall (R) and F-measure (F).  

Information Inference Results 

Methods 

P 

R 

F 

Census 

Google N-gram 

89.1%  70.2%  78.5%
96.2%  19.4%  32.4%
Video Extraction  88.9%  73.8%  80.6%
89.3%  80.4%  84.6%

Combined 

Table 2.  Gender Discovery Performance 

combined 

  Table 2 shows that video extraction based ap-
proach can achieve the highest recall among all 
three  methods.  The 
approach 
achieved  statistically  significant  improvement 
on recall. 
  Table  3  presents  some  examples  (“F”  for  fe-
male  and  “M”  for  male).  We  found  that  most 
speech  name  recognition  errors  are  propagated 
to gender detection in the baseline methods, for 
example, “Sala Zhang” is mis-spelled in speech 
recognition  output  (the  correct  spelling  should 
be  “Sarah  Chang”)  and  thus  Google  N-gram 
approach  mistakenly  predicted  it  as  a  male. 
Many  rare  names  such  as  “Wu  Ficzek”, 
“Karami” cannot be predicted by the baselines,  
  Error  analysis  on  video  extraction  based  ap-
proach  showed  that  most  errors  occur  on  those 
shots  including  multiple  people  (males  and  fe-
males).  In  addition,  since  the  data  set  is  from 
news domain, there were  many shots including 
reporters  and  target  persons  at  the  same  time. 
For  example,  “Jiang  Zemin”  was  mistakenly 
associated  with  a  “female”  gender  because  the 
reporter is a female in that corresponding shot. 
5.4 Comparable  Corpora  Acquisition  Re-

sults

For  comparable  corpora  acquisition,  we  meas-
ured  accuracy  for  the  top  50  document  pairs. 
Due  to  lack  of  answer-keys,  we  asked  a  bi-
lingual human annotator to judge results manu-
ally.  The  evaluation  guideline  generally  fol-
lowed  the  definitions  in  (Cheung  and  Fung, 
2004). A pair of documents is judged as compa-
rable if they share a certain amount of informa-
tion (e.g. entities, events and topics). 

Without using IDF, for different parameter (cid:302)
and δ  in  the  similarity  metrics,  the  results  are 
summarized  in  Figure  5.  For  comparison  we 
present  the  results  for  mono-lingual  and  cross- 
lingual separately. Figure 5 indicates that as the 
threshold and normalization values increase, the 
accuracy generally improves. It’s not surprising 
that  mono-lingual  results  are  better  than  cross-
lingual  results,  because  generally  more  videos 
with comparable topics are in the same language.   

636

Mention Google

Zhang
Sala

Peter
Wu
Ficzek
President M: .953 
F: 0.047 

-

Video

-

N-gram  Census
Extraction
M: 1 
F: 0.699 
F: 0 
M: 0.301 
M: .979 
F: 0.021  M: 1  M: 0.699 
F: 0.301 
M: 0.699 
F: 0.301 
M: 0.704 
F: 0.296 
F: 0.787 
M: 0.213 

M: 1 
F: 0 

-

-

M: 1 
F: 0 

-

M: 0.694 
F: 0.306 

Jiang
Zemin 

Karami 

Correct
Answer

F

M

M

M

M

M

Context Sentence 

World 
famous  meaning  violin  soloist 
Zhang Sala recently again to Toronto sym-
phony orchestra... 
Iraq, there are in Lebanon Paris pass Peter
after 10 five Dar exile without peace... 
If  you  want  to  do  a  good  job  indeed  Wu
Ficzek
Labor  union  of  Arab  heritage  publishers 
president  to  call  for  the  opening  of  the 
Arab Book Exhibition. 
It has never stopped the including the for-
mer CPC General Secretary Jiang Zemin… 
all  the  Gamal  Ismail  introduced  the  needs 
of the Akkar region, referring to the desire 
on  the  issue  of  the  President  Karami  to 
give priority disadvantaged areas 

Table 3. Examples for Mention Gender Detection 

Figure 5. Comparable Corpora Acquisition  

without IDF 

Figure 6. Comparable Corpora Acquisition with 

IDF (δ=0.6)

  We then added IDF to the optimized threshold 
and  obtained  results  in  Figure  6.  The  accuracy 
for both languages was further enhanced. We can 
see  that  under  any  conditions  our  approach  can 
discover  comparable  documents  reliably.  In  or-
der to measure the impact of concept extraction 
errors,  we  also  evaluated  the  results  for  using 
ground-truth concepts as shown in Figure 6. Sur-
prisingly it didn’t provide much higher accuracy 
than  automatic  concept  extraction,  mainly  be-
cause  the  similarity  can  be  captured  by  some 
dominant video concepts. 

6 Related Work 
A large body of prior work has focused on multi-
media information retrieval and document classi-
fication  (e.g.  Iria  and  Magalhaes,  2009).    State-
of-the-art  information  fusion  approaches  can  be 
divided  into  two  groups:  formal  “top-down” 
methods  from  the  generic  knowledge  fusion 
community  and  quantitative  “bottom-up”  tech-
niques from the Semantic Web community (Ap-
priou  et  al.,  2001;  Gregoire,  2006).  However, 
very  limited  research  methods  have  been  ex-

637

plored to fuse automatically extracted facts from 
texts and videos/images. Our idea of conducting 
information fusion on multiple semantic levels is 
similar to the kernel method described in (Gu et 
al., 2007). 
  Most previous work on cross-media information 
extraction focused on one single domain (e.g. e-
Government  (Amato  et  al.,  2010);  soccer  game 
(Pazouki  and  Rahmati,  2009))  and  struc-
tured/semi-structured  texts  (e.g.  product  cata-
logues  (Labsky  et  al.,  2005)).  Saggion  et  al. 
(2004)  described  a  multimedia  extraction  ap-
proach  to  create  composite  index  from  multiple 
and multi-lingual sources. We expand the task to 
the more general news domain including unstruc-
tured texts and use cross-media inference to en-
hance extraction performance. 
  Some recent work has exploited analysis of as-
sociated texts to improve image annotation (e.g. 
Deschacht  and  Moens,  2007;  Feng  and  Lapata, 
2008). Some recent research demonstrated cross-
modal  integration  can  provide  significant  gains 
in  improving  the  richness  of  information.  For 
example, Oviatt et al. (1997) showed that speech 
and pen-based gestures can provide complemen-
tary capabilities because basic subject, verb, and 
object  constituents  almost  always  are  spoken, 
whereas  those  describing  locative  information 
invariably are written or gestured. However, not 
much work demonstrated an effective method of 
using  video/image  annotation  to  improve  text 
extraction.  Our  experiments  provide  some  case 
studies in this new direction. Our work can also 
be  considered  as  an  extension  of  global  back-
ground inference (e.g. Ji and Grishman, 2008) to 
cross-media paradigm. 
  Extensive research has been done on video clus-
tering. For example, Cheung and Zakhor (2000)
used meta-data extracted from textual and hyper-
link  information  to  detect  similar  videos  on  the 
web; Magalhaes et al. (2008) described a seman-
tic  similarity  metric  based  on  key  word  vectors 
for  multi-media  fusion.  We  extend  such  video 
similarity  computing  approaches  to  a  multi-
lingual environment. 
7 Conclusion and Future Work 
Traditional  Information  Extraction  (IE)  ap-
proaches  focused  on  single  media  (e.g.  texts), 
with  very  limited  use  of  knowledge  from  other 
data modalities in the background. In this paper 

we propose a new approach to integrate informa-
tion extracted from videos and texts into a coher-
ent common representation including multi-level 
knowledge  (concepts,  relations  and  events).  Be-
yond  standard  information  fusion,  we attempted 
global  inference  methods  to  incorporate  video 
extraction  and  significantly  enhanced  the  per-
formance  of  text  extraction.  Finally,  we  extend 
our methods to multi-lingual environment (Eng-
lish,  Arabic  and  Chinese)  by  presenting  a  case 
study on cross-lingual comparable corpora acqui-
sition.
  We  used  a  dataset  which  includes  videos  and 
associated speech recognition output (texts), but 
our approach is applicable to any cases in which 
texts and videos appear together (from associated 
texts, captions etc.). The proposed common rep-
resentation  will  provide  a  framework  for  many 
byproducts. For example, the monolingual fused 
information  graphs  can  be  used  to  generate  ab-
stractive summaries. Given the fused information 
we can also visualize the facts from background 
texts effectively. We are also interested in using 
video information to discover novel relations and 
events which are missed in the text IE task. 
Acknowledgement 
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement 
Number  W911NF-09-2-0053, 
the  U.S.  NSF 
CAREER  Award  under  Grant  IIS-0953149, 
Google,  Inc.,  DARPA  GALE  Program,  CUNY 
Research  Enhancement  Program,  PSC-CUNY 
Research Program, Faculty Publication Program 
and  GRTI  Program.  The  views  and  conclusions 
contained  in  this  document  are  those  of  the  au-
thors and should not be interpreted as represent-
ing  the  official  policies,  either  expressed  or  im-
plied,  of  the  Army  Research  Laboratory  or  the 
U.S.  Government.  The  U.S.  Government  is  au-
thorized  to  reproduce  and  distribute  reprints  for 
Government purposes notwithstanding any copy-
right notation here on. 

References 
Amato, F., Mazzeo, A.,  Moscato, V. and Picariello, 
A.  2010. Information Extraction from Multimedia 
Documents 
e-Government  Applications. 
Information Systems: People, Organizations, Insti-
tutions, and Technologies. pp. 101-108. 

for 

638

Appriou  A.,  A.  Ayoun,  Benferhat,  S.,  Besnard,  P., 
Cholvy,  L.,  Cooke,  R.,  Cuppens,  F.,  Dubois,  D., 
Fargier,  H.,  Grabisch,  M.,  Kruse,  R.,  Lang,  J. 
Moral, S., Prade, H., Saffiotti, A., Smets, P., Sos-
sai, C. 2001. Fusion: General concepts and charac-
teristics. International  Journal  of  Intelligent  Sys-
tems 16(10).

Baluja, S. and Rowley, H. 2006. Boosting Sex Identi-
fication  Performance.  International  Journal  of 
Computer Vision. 

Bergsma,  S.  2005.  Automatic  Acquisition  of  Gender 
Information for Anaphora Resolution. Proc. Cana-
dian AI 2005. 

Cheung, P. and Fung P. 2004. Sentence Alignment in 
Parallel,  Comparable,  and  Quasi-comparable  Cor-
pora. Proc.  LREC 2004. 

Cheung, S.-C.  and Zakhor, A. 2000. Efficient video 
similarity measurement and search. Proc. IEEE In-
ternational Conference on Image Processing. 

Deschacht K. and Moens M. 2007. Text Analysis for 

Automatic Image Annotation. Proc. ACL 2007. 

Feng, Y. and Lapata, M. 2008. Automatic Image An-
notation  Using  Auxiliary  Text  Information.  Proc. 
ACL 2008. 

Gregoire,  E.  2006.  An  unbiased  approach  to  iterated 

fusion by weakening. Information Fusion. 7(1). 

Gu,  Z.,  Mei,  T.,  Hua,  X.,  Tang,  J.,  Wu,  X.  2007. 
Multi-Layer Multi-Instance Kernel for Video Con-
cept Detection. Proc. ACM Multimedia 2007. 

Hakkani-Tur, D., Ji, H. and Grishman, R. 2007. Using 
Information  Extraction  to  Improve  Cross-lingual 
Document Retrieval. Proc. RANLP 2007 Workshop 
on Multi-Source Multi-lingual Information Extrac-
tion and Summarization.

Iria,  J.  and  Magalhaes,  J.  2009.  Exploiting  Cross-
Media  Correlations  in  the  Categorization  of  Mul-
timedia Web Documents. Proc. CIAM 2009. 

DARPA  Global  Autonomous  Language  Exploita-
tion. Springer. 

Ji, H. and Lin, D. 2009. Gender and Animacy Knowl-
edge Discovery from Web-Scale N-Grams for Un-
supervised  Person  Mention  Detection.  Proc.  PA-
CLIC 2009. 

Oviatt,  S. L., DeAngeli,  A., &  Kuhn, K. 1997. Inte-
gration and synchronization of input modes during 
multimodal human-computer interaction. Proceed-
ings of Conference on Human Factors in Comput-
ing Systems (CHI’97), 415-422. New York: ACM 
Press.

Labsky,  M.,  Praks,  P.,  Sv´atek1,  V.,  and  Svab,  O. 
2005.  Multimedia  Information  Extraction  from 
2005 
HTML 
IEEE/WIC/ACM International Conference on Web 
Intelligence. pp. 401 – 404.   

Product  Catalogues.  Proc. 

Lin, D., Church, K.,  Ji, H., Sekine, S., Yarowsky, D.,  
Bergsma, S., Patil, K., Pitler, E., Lathbury, R., Rao, 
V.,  Dalwani, K. and Narsale,  S. 2010.  New Data, 
Tags  and  Tools  for  Web-Scale  N-grams.  Proc. 
LREC 2010. 

Magalhaes, J., Ciravegna, F. and Ruger, S. 2008. Ex-
ploring  Multimedia  in  a  Keyword  Space.  Proc. 
ACM Multimedia 2008. 

Munteanu, D. S. and Marcu D. 2005. Improving Ma-
chine Translation Performance by Exploiting Non-
Parallel  Corpora.  Computational  Linguistics.  Vol-
ume 31, Issue 4. pp. 477-504. 

Naphade, M. R., Kennedy, L., Kender, J. R., Chang, 
S.-F., Smith, J. R., Over, P., and Hauptmann, A. A 
light scale concept ontology for multimedia under-
standing  for  TRECVID  2005.  Technical  report, 
IBM, 2005. 

Pazouki, E. and Rahmati, M. 2009. A novel multime-
dia data mining framework for information extrac-
tion  of  a  soccer  video  stream.  Intelligent  Data 
Analysis. pp. 833-857. 

Ji, H. and Grishman, R. 2008. Refining Event Extrac-
tion  Through  Cross-document  Inference.  Proc. 
ACL 2008. 

Qi,G.-J.,  Hua,X.-S.,  Rui,  Y.,  Tang,  J.,  Mei,  T.,  and 
Zhang,H.-J.  2007.  Correlative  Multi-label  Video 
Annotation. Proc. ACM Multimedia 2007.

Ji, H. 2009. Mining Name Translations from Compa-
rable  Corpora  by  Creating  Bilingual  Information 
Networks.  Proc.  ACL-IJCNLP  2009  workshop  on 
Building  and  Using  Comparable  Corpora  (BUCC 
2009): from parallel to non-parallel corpora.

Ji,  H.,  Grishman,  R.,  Freitag,  D.,  Blume,  M.,  Wang, 
J., Khadivi, S., Zens, R., and Ney, H. 2009. Name 
Translation  for  Distillation.  Handbook  of  Natural 
Language  Processing  and  Machine  Translation: 

Saggion,  H.,  Cunningham,  H.,  Bontcheva,  K., 
Maynard, D., Hamza, O., and Wilks, Y. 2004. Mul-
timedia  indexing  through  multi-source  and  multi-
language information extraction: the MUMIS pro-
ject. Data  Knowlege  Engineering,  48,  2,  pp.  247-
264. 

Wang,  F.  and  Zhang,  C.  2006.  Label  propagation 

through linear neighborhoods. Proc. ICML 2006.

