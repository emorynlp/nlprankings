869

Coling 2010: Poster Volume, pages 869–875,

Beijing, August 2010

Support or Oppose?

Classifying Positions in Online Debates

from Reply Activities and Opinion Expressions

Akiko Murakami1,2

Rudy Raymond1

1 IBM Research - Tokyo

2 Graduate School of Interdisciplinary Information Studies, The University of Tokyo

{akikom,raymond}@jp.ibm.com

Abstract

We propose a method for the task of iden-
tifying the general positions of users in
online debates, i.e., support or oppose the
main topic of an online debate, by ex-
ploiting local information in their remarks
within the debate. An online debate is
a forum where each user post an opin-
ion on a particular topic while other users
state their positions by posting their re-
marks within the debate. The supporting
or opposing remarks are made by directly
replying to the opinion, or indirectly to
other remarks (to express local agreement
or disagreement), which makes the task of
identifying users’ general positions difﬁ-
cult. A prior study has shown that a link-
based method, which completely ignores
the content of the remarks, can achieve
higher accuracy for the identiﬁcation task
than methods based solely on the contents
of the remarks.
In this paper, we show
that utilizing the textual content of the
remarks into the link-based method can
yield higher accuracy in the identiﬁcation
task.

1 Introduction

Social computing tools, such as a SNS (So-
cial Network Service) or an online discussion
board have become very powerful communication
tools for discussing topics with people around the
world. Many companies use these kinds of social
computing tools to understand their customers’ re-
quirements and their marketing activities. Social

computing tools are very useful not only for ag-
gregating customers’ opinions outside the com-
panies, but also for aggregating their employees’
ideas. For example, IBM has held Jam1 sessions,
which are short-term online discussions to aggre-
gate ideas from employees and customers. The re-
sults of Jam sessions help management decisions,
for instance the technology areas to invest.

Not just enterprises, but some nations are try-
ing to aggregate their citizens’ ideas in the Internet
and provide systems for discussions at the people-
to-people levels as part of the movement for open
government. The United States government has
the Idea Factory2 website for collecting ideas to
enhance activities of Department of Homeland
Security (DHS) and the Open For Questions3 to
collect requests for the US government.

The motivation for creating these kinds of on-
line discussions is not limited to collecting ideas
but also to help understand the trends of opinions
about the ideas or topics. This means that getting
a quick overview of opinions about ideas is a key
point for the success of online discussions.

In this paper we propose a method to show
quick overview of participants’ positions, “Sup-
port” or “Oppose” for the main idea or topic in
online debates. It is difﬁcult to identify each per-
son’s position for a topic directly, since most of
opinionative expressions are made not for main
topic but for adjacent remarks. This causes a dif-
ﬁculty in building answer sets for classiﬁer. The
following example shows opinion expressions for
a main topic focused on an adjacent remark in a

1https://www.collaborationjam.com
2http://www.whitehouse.gov/open/innovations/IdeaFactory
3http://www.whitehouse.gov/OpenForQuestions/

870

Figure 1: Identifying users’ positions from their opinions about previous remarks

debate. In this example, The main topic is “Travel
and F2F (face-to-face) meeting is fundamental to
business”.

Remark A Travel isn’t necessary be-
cause besides the high cost of trav-
els around the world,
today we
have a lot of communication tools,
for instance web conference, video
chat that can easily contribute to
join leaders around the world in a
cheaper way.

Remark B I disagree. Without travel
and F2F meetings global integra-
tion does not work as well or as
quickly.
It doesn’t mean that ev-
erybody has to travel all the time,
but at least some meetings are key
to success.

The author of Remark A mentions that travel is
not necessary to business. This opinion opposed
to the main topic, so that the position for the main
topic is “Oppose”. In contrast, the opinion expres-
sion in Remark B is not an opinion about the main
topic, but relates to the previous opinion in Re-
mark A. This opinion expression indicates that the
author of Remark B disagrees with the opinion of
Remark A, and indirectly implies agreement with
the main topic. Thus, although it is hard to infer
the global position of Remark B from only the sur-

face expressions, it is straightforward to infer that
an opinion in Remark B about Remark A is nega-
tive (i.e., Remark B expresses disagreement with
Remark A).

In this paper, positions with regards to the main
topic (global positions) are classiﬁed into two
classes: support and oppose, while opinions about
the previous remarks (local positions) are classi-
ﬁed into three: agree, disagree, and neutral. For
example, let us consider the case in Fig. 1, where
Remark “a” is the main topic, and Remark “b” is
the reply to Remark “a” and Remark “c” is the
reply to Remark “b”. Here, let b(a) be the local
position, that is, opinion (agree/disagree/neutral)
in Remark “b” on the topic in Remark “a”. For
example, if b(a) and c(b) are disagree, one can de-
termine that the authors of the corresponding re-
marks are in the opposition. That is, the author of
Remark “c” agrees with Author A (the author of
Remark “a”), that is, the main topic, while Author
B is against the others. On the contrary, if b(a) is
disagree and c(b) is agree, then Author C agrees
with Author B and therefore it implies that Author
C is against Author A. In this case, only Author A
supports the main topic while Author B and Au-
thor C oppose to the main topic.

To infer supporting or opposing positions with
regards to the main topic, two steps are used. First,
the degree of disagreement between any two users
is computed from the link structure and the text of

871

each pair of their adjacent replies. This is used as
the link weight between nodes (which correspond
to users in a debate) in the network. Second, the
bipartition of the users in the weighted network is
computed by ﬁnding a bipartition that induces the
maximum cut of the network, a partition of nodes
into two disjoint sets that maximizes the sum of
the weights of the links connecting nodes in dif-
ferent sets. Since the weight of the links is higher
(more positive) when the degree of disagreement
is higher, the bipartition is expected to express two
groups of opposing positions.

In order to evaluate the performance of our
method, we conducted some experiments to iden-
tify the supporting and opposing positions of par-
ticipants in online debates. The experimental re-
sults indicate that our method leads to higher pre-
cision than the baseline method, which is de-
scribed in (Agrawal et al., 2003).

The rest of this paper is organized as follows.
First we describe related work in Section 2, and
in Section 3 we propose our method for identify-
ing participants’ positions from their reply activi-
ties and text contents. In Section 4 we explain the
data sets used for the evaluations and show the ex-
perimental results of an opinion classiﬁer for ad-
jacent remarks and a support/oppose classiﬁer for
the participants in online debates. We conclude
this paper and describe future work in Section 5.

2 Related Work

There are some research papers published on anal-
ysis of online discussions. Some researches re-
ported on how to analyze and navigate IBM Jam
sessions. Millen et al. pointed out the importance
of supporting the participants in discussions and
demonstrated the effectiveness of their methods
in one of these jams (Millen and Fontaine, 2003).
Dave et al. described ways for jam participants to
navigate using visualization techniques (Dave et
al., 2004). One of the authors previously also pro-
posed a method to mine discussion records using
XML annotations (Murakami et al., 2001) and a
method to ﬁnd important remarks in a discussion
thread based on the reply-to structure and partici-
pants’ opinions (Murakami et al., 2007).

Classifying agree/disagree opinions in conver-
sational debates using Bayesian networks was

presented in (Galley et al., 2004). Agrawal et al.
described an observation that reply activities show
disagreement with previous authors, and showed a
method to classify the supporting/opposing posi-
tion of users based on this observation in (Agrawal
et al., 2003). Thomas et al. (Thomas et al., 2006)
introduced some constraints that a single speaker
retains the same position for the classiﬁcation
of participants’ positions from ﬂoor-debate tran-
scripts.

3 Proposed Method

3.1 Calculating the Reaction Coefﬁcient

between participants

We call the degree of divergence in the opinions
between participants a reaction coefﬁcient. This
reaction coefﬁcient is deﬁned as a function of the
participants i, j, represented as r(i, j). To calcu-
late reaction coefﬁcients, we extracted pairs of a
remark and its reply remark, and assigned “local
position ﬂags” to the pairs. There are three lo-
cal position ﬂags, “agree”, “disagree”, and “neu-
tral”. The reaction coefﬁcient r(i, j) between par-
ticipants i and j is deﬁned as:

r(i, j) = αNdisagree(i, j)+βNneutral(i, j)+γNagree(i, j),
(1)
where Nopinion(i, j) is the number of remark pairs
with opinion as the corresponding local position
ﬂag between participants i and j.

Typically we assign a positive value to α, a
slightly positive value to β, and a negative value
to γ. This means that r(i, j) is positive when there
are only neutral remarks between user i and j.
This is based on the hypothesis in (Agrawal et al.,
2003) that replies usually indicate disagreement
with previous remarks. There is no directionality
in reaction coefﬁcients so that r(i, j) = r(j, i).

3.2 Classiﬁcation of Participants’ Positions

based on the Max Cut Problem

Let the graph corresponding to the activity net-
work of the participants in an online debate be
G(V, E), where V is the set of nodes that corre-
sponds to participants and E is the set of edges
each of which links participants that exchanged
remarks. For any i, j ∈ V , let r(i, j) be the weight
of the link between i and j. A partition of the

872

Table 1: Ideas and Number of Comments and Participants for the Ideas

Idea ID Title

# of
Comments

# of
Participant

# of Remarks
per Participant

1

2

3
4
5

Making “IT” Education as a Compulsory
Subject in Schools
Making Personal-Computer Makers to
Supplying Service Parts
Adoption of “Basic Income”
Votes in elections using Closed Networks
Computerized Books in Libraries

75

130

118
108
50

45

21

57
40
12

1.7

6.2

2.1
2.7
4.2

participants into supporting and opposing parties,
Ssup and Sopp respectively, is computed by solv-
ing the max cut problem on G(V, E) deﬁned as
follows.

imized.

[Max cut problem] Given G(V, E) as above,
ﬁnd a bipartition of V into Ssup and Sopp =

V \ Ssup so that∑i∈Ssup,j∈Sopp r(i, j) is max-

The max cut problem is known to be NP-hard,
and thus in general is difﬁcult to solve. How-
ever, good approximation algorithms based on
Linear Programming and Semideﬁnite Program-
ming have been developed recently, and combined
with branch-and-bound techniques a good exact
max-cut solver called BiqMac exists (Rendl et al.,
2010). We used BiqMac for solving the max cut
problem exactly on the activity network. Although
a faster approximate max cut solver is used in
(Agrawal et al., 2003), it is based on the limiting
assumption that the size of Sopp is approximately
the same as Sopp. This cannot be assumed for the
networks in this paper.

4 Experiments

4.1 Corpus
The Ministry of Economy, Trade and Industry
in Japan (METI) was accepting public opinions
on e-government programs via the “e-METI Idea
Box4” from February 23 to March 15 2010. Par-
ticipants could show their positions for the ideas
since the site accepted comments on the main idea
and other comments, so this discussion can be re-
garded as a kind of debate. We used this data

4http://www.meti.go.jp/policy/it policy/open-meti/

to evaluate our proposed method. The ideas and
comments were written in Japanese and the data
is available at the METI website.

For the 936 ideas that were posted to the Idea
Box, we examined 17 ideas with more than 40
comments. Finally we selected ﬁve ideas for the
evaluation. The numbers of remarks (a main idea
and comments), participants, and remarks per par-
ticipants are shown in Table 1.

We extracted the reply-to structure information
in textual contents. The Idea Box system had a
capability to adding a comment to a main topic
or the other comment, and the system inserted
an identiﬁer in comment’s text. Each identiﬁer
started with “#” and the IDs of the previous com-
ments followed the identiﬁer, such as “#003”
(with #001 referring to the main topic in the
thread). An idea or comments may have several
comments as replies, so this reply-to structure in a
debate is a tree structure whose root node is the
main topic. A typical reply-to tree structure is
shown in Fig 2.

4.2 Agree/Disagree Classiﬁcation
To calculate the reaction coefﬁcients, we need to
extract the reply-to pairs and classify these pairs
into the agree/disagree/neutral classes. To classify
these remark pairs we use opinionative and senti-
ment expressions. If a reply remark contains an
expression of “I agree with you” then it should be
classiﬁed into the agree class. Another example
of expressions of the agree class would be “That’s
a good idea!”.

To extract expressions of opinion, we cre-
ated a simple pattern dictionaries that contains

873

Table 2: Accuracy of opinion classiﬁcation for
reply-to pairs

Idea ID Precision Recall
0.25
0.14
0.38
0.26

0.63
0.62
0.44
0.56

Ave.

1
2
3

Figure 2: Reply-to Structure of a Debate

agree/disagree expressions. For instance, “I dis-
agree with your idea” and “I don’t agree with you”
are in the disagree pattern dictionary. At the same
time we use a sentiment analysis tool to extract
sentiment expressions. The tool we used for sen-
timent expression extractions is the same as de-
scribed in (Kanayama et al., 2004), which use ma-
chine translation techniques to identify sentiment
expressions in text. The tool returns sentiment ex-
pressions with a sentiment label, favorable or un-
favorable.

After identifying opinionative and sentiment
expressions in the remarks, scores for the opin-
ion classiﬁcation are calculated. The score of each
reply-to pair is the number of agreeing and favor-
able expressions minus the number of disagreeing
and unfavorable expressions in the reply remark.
When the score is positive, the opinion of the pair
is identiﬁed as agree, and if the score is negative
then the opinion of the pair identiﬁed as disagree.
If the score equals zero, then the opinion is iden-
tiﬁed as neutral.

To evaluate this opinion classiﬁer, we did an
experiment with the METI data, which was man-
ually assigned agree/disagree/neutral ﬂags. The
answers for these evaluation were created by us
for three of the idea threads (Idea IDs #1,#2 and
#3). Since most remarks do not have agree or dis-
agree expressions, most reply-to pairs are classi-
ﬁed into the neutral class. This means that cal-
culating precision and recall for the neutral class
are not important. For the evaluation of the clas-

siﬁer we calculated precisions and recalls only for
agree and disagree classes. The results are shown
in Table 2.

4.3 Support/Oppose Classiﬁcation
Using the numbers of agree/disagree/neutral
reply-to pairs, we can calculate the reaction co-
efﬁcients for each pair of participants. After cal-
culating the reaction coefﬁcients for all of the par-
ticipants’ pairs, we can classify each participant
into support or oppose sets using the max cut tech-
nique. In this subsection, we explain how to eval-
uate our proposed method and show experimental
results.

4.3.1 Answer Sets for Global Position

Classiﬁcation

To evaluate our method we created answer sets
for a global position classiﬁer, consisting of par-
ticipant sets with the position labels Support or
Oppose. We identiﬁed the positions of the par-
ticipants’ remarks with contexts, but we assigned
the “Unclear” label for some participants since
their remarks did not contain enough information
to classify their global positions. For showing the
validity of the answer sets, two annotators anno-
tated three ideas and calculated a κ value. The κ
value is 0.69 so that this answer set is appropri-
ate as an evaluation set. The use of the answer set
annotated by a single annotator for the evaluation
of support/oppose classiﬁcation is justiﬁed since
the agreement rate (the κ value) is enough for the
evaluation.

4.3.2 Evaluation Index for Position

Classiﬁcation

For evaluation we deﬁned the estimation index
accuracy since the number of participants in the
Support position is not always the same as the

874

number of participants in the Oppose position.
If the answers are grossly one-sided, the general
accuracy does not work well, since the system
can lead to a high score when it classiﬁes all of
the participants into the larger side. To minimize
this potential bias, we deﬁned an estimation in-
dex accuracy using the average of the accuracies
for the Support/Oppose sets. The estimation index
accuracy is deﬁned as:

Table 3: Accuracy of Support/Oppose position
classiﬁcation

ID Baseline
1
47.86
66.43
2
46.47
3
4
53.19
66.67
5

(1,0,0)
66.67
76.43
48.88
51.52
58.33

(1,0.5,0)
54.52
76.43
42.63
55.36
66.67

(1,0.5,-1)
54.05
89.29
55.45
77.60
75.00

accuracy =

1

2(|Asup ∩ Ssup|

|Asup|

+ |Aopp ∩ Sopp|

|Aopp|

) ,

(2)

where Asup and Aopp are the Support and Op-
pose participant sets in the answer set and Ssup
and Sopp are the Support and Oppose participant
sets generated by the system, respectively. For
accuracy, we ignore “Unclear” users since the
system is a two-class classiﬁer.

4.3.3 Experimental Results

In the experiments we use the reaction coefﬁ-
cients r(i, j) calculated based on the results of the
agree/disagree/neutral Classiﬁer, and classify par-
ticipants into Support/Oppose position sets using
BiqMac. Since we assumed that the main topic of
the debate is the ﬁrst remark of the debate thread,
we assume that the set which includes the author
of the ﬁrst remark as the “Support” set and the
other set as the “Oppose” set5.

We conducted experiments for (α, β, γ) =
(1, 0, 0), (1, 0.5, 0), (1, 0.5,−1) in Eq. (1) to ex-
amine the dependency of the accuracy on the co-
efﬁcients r(i, j). We also conducted an experi-
ment for (1, 1, 1), which is regarded as a baseline
method described in (Agrawal et al., 2003), since
all of the reply actions represent “disagree” opin-
ions for the previous remarks with these parame-
ter. The experimental results are shown in Table
3.

The ideas other than ID 1 show better accu-
racy than the baseline and their accuracies tend
to increase in the order of (1, 0, 0), (1, 0.5, 0),
(1, 0.5,−1). This result shows that the effec-
tiveness of distinguishing between “disagree” and
“agree” replies. This distinction makes it possible
to introduce the constraint in which the user pairs

5For this reason, the values of the accuracies can be lower

than 0.5.

with “disagree” and “neutral” should be classi-
ﬁed into opposing positions and user pairs with
“agree” should be classiﬁed into same position in
the Support/Oppose user sets.

At the same time, ID 1 shows lower accuracy
for (1, 0.5, 0), (1, 0.5,−1) even though the accu-
racy of agree/disagree classiﬁer is good. In idea
ID 1, the number of remarks per participant is the
lowest in data sets, so the errors of the Agree-
ment/Disagreement classiﬁer strongly affect the
results of the Support/Oppose classiﬁer.

5 Conclusion and Future Work

We have shown how to classify users in an online
debate based on their general positions with re-
gards to the main topic by the textual contents of
their remarks and the link structure of their replies.
The previous work used the assumption that the
replies are usually disagreements and based on
this assumption used a link-based method to clas-
sify the participants. However, in an online debate
the replies are also used for clarifying previous
remarks and quite often for supporting the previ-
ous ones. Our proposed method uses not only the
link structure of the replies, but also the textual
contents of the local agreement/disagreement po-
sitions between the remarks to boost the accuracy
of the task of classifying users into the supporting
and opposing parties.

The proposed method is based on the observa-
tion that it is easier to use the textual contents for
classifying the local positions of a user’s replies
with regards to the previous remarks, than to use
them (e.g., by aggregating them) for classifying
his/her global position with regards to the main
topic of the debate. In our experiments, we used
a rule-based classiﬁer to classify the replies into

875

agree, disagree, and neutral (with regards to the
previous replies) and used these classiﬁer’s result
to determine the weight of the corresponding links
in the link structure of the reply network. The
max cut algorithm is then applied to the network,
which results in a classiﬁcation of the users into
supporting or opposing parties (with regards to the
main topic of the debate). The experiments indi-
cate that the accuracies of the link-based method
of (Agrawal et al., 2003) can be signiﬁcantly in-
creased by considering the textual contents of the
replies.

There are several directions to extend our
method. When an expression of opinion appears
in a reply, we have to locate the target of the opin-
ion.
In the current method the target is deter-
mined by the ID of the remark pointed by the re-
ply. When the ID is not available, we assume that
the reply is with regards to the main topic. How-
ever, we also observed that even though a reply
was directed to a particular remark, it often also
contained opinions about the main topic. Identi-
fying such replies can be used to yield higher ac-
curacy in the classiﬁcation task.

Much work remains for ultimate understanding
of the participants’ opinions in debate corpus. Un-
derstanding the reasons for the position for the
main topic is one of the ways to understand their
opinions and it may help to decide the next steps
for companies or governments which held the de-
bate sessions. An integrated system that includes
a discussion system and an analysis system show-
ing the ratio of positions and the reasons would
support such purposes.

Acknowledgments

The authors would like to acknowledge Kenji Hi-
ramoto and Manabu Morita, who are responsible
for the IdeaBox, for their helpful comments and
conversation.

References
Agrawal, Rakesh, Sridhar Rajagopalan, Ramakrish-
nan Srikant, and Yirong Xu. 2003. Mining news-
groups using networks arising from social behav-
ior. In WWW ’03: Proceedings of the 12th interna-
tional conference on World Wide Web, pages 529–
535, New York, NY, USA. ACM.

Dave, Kushal, Martin Wattenberg, and Michael Muller.
2004. Flash forums and forumreader: navigating a
new kind of large-scale online discussion. In CSCW
’04: Proceedings of the 2004 ACM conference on
Computer supported cooperative work, pages 232–
241, New York, NY, USA. ACM.

Galley, Michel, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004.
Identifying agree-
ment and disagreement in conversational speech:
use of bayesian networks to model pragmatic de-
pendencies. In ACL ’04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, pages 669–676, Morristown, NJ, USA.
Association for Computational Linguistics.

Kanayama, Hiroshi, Tetsuya Nasukawa, and Hideo
Watanabe. 2004. Deeper sentiment analysis using
machine translation technology.
In COLING ’04:
Proceedings of the 20th international conference on
Computational Linguistics, page 494, Morristown,
NJ, USA. Association for Computational Linguis-
tics.

Millen, David R. and Michael A. Fontaine.

2003.
Multi-team facilitation of very large-scale dis-
tributed meetings.
In ECSCW’03: Proceedings of
the eighth conference on European Conference on
Computer Supported Cooperative Work, pages 259–
275, Norwell, MA, USA. Kluwer Academic Pub-
lishers.

Murakami, Akiko, Katashi Nagao, and Koichi Takeda.
2001. Discussion Mining: Knowledge discovery
from online discussion records.
In NLPRS Work-
shop XML and NLP, 2001.

Murakami, Akiko, Tetsuya Nasukawa, Fusashi Naka-
mura, Hironori Takeuchi, Risa Nishiyama, Pnina
Veisberg, and Hideo Watanabe. 2007. Innovation-
Jam: Analysis of online discussion records using
text mining technology. In International Workshop
on Intercultual Collaboration 2007 (IWIC2007).

Rendl, Franz, Giovanni Rinaldi,

and Angelika
Wiegele. 2010. Solving Max-Cut to optimality
by intersecting semideﬁnite and polyhedral relax-
ations. Math. Programming, 121(2):307.

Thomas, Matt, Bo Pang, and Lillian Lee.

2006.
Get out the vote: Determining support or opposi-
tion from congressional ﬂoor-debate transcripts. In
Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages
327–335, Sydney, Australia, July. Association for
Computational Linguistics.

