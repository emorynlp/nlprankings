



















































Is artificial data useful for biomedical Natural Language Processing algorithms?


Proceedings of the BioNLP 2019 workshop, pages 240–249
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

240

Is artificial data useful for biomedical Natural Language Processing
algorithms?

Zixu Wang1, Julia Ive2, Sumithra Velupillai3 and Lucia Specia1
1Department of Computing, Imperial College London, UK

2DCS, University of Sheffield, UK
3IoPPN, King’s College London, UK and KTH, Sweden

zixu.wang18@imperial.ac.uk
j.ive@sheffield.ac.uk

sumithra.velupillai@kcl.ac.uk
l.specia@imperial.ac.uk

Abstract

A major obstacle to the development of Natu-
ral Language Processing (NLP) methods in the
biomedical domain is data accessibility. This
problem can be addressed by generating medi-
cal data artificially. Most previous studies have
focused on the generation of short clinical text,
and evaluation of the data utility has been lim-
ited. We propose a generic methodology to
guide the generation of clinical text with key
phrases. We use the artificial data as additional
training data in two key biomedical NLP tasks:
text classification and temporal relation extrac-
tion. We show that artificially generated train-
ing data used in conjunction with real training
data can lead to performance boosts for data-
greedy neural network algorithms. We also
demonstrate the usefulness of the generated
data for NLP setups where it fully replaces real
training data.

1 Introduction

Data availability is a major obstacle in the devel-
opment of more powerful Natural Language Pro-
cessing (NLP) methods in the biomedical domain.
In particular, current state-of-the-art (SOTA) neu-
ral techniques used for NLP rely on substantial
amounts of training data.

In the NLP community, this low-resource prob-
lem is typically addressed by generating comple-
mentary data artificially (Poncelas et al., 2018;
Edunov et al., 2018). This approach is also gaining
attention in biomedical NLP. Most of these stud-
ies present work on the generation of short text
(typically under 20 tokens), given structural infor-
mation to guide this generation (e.g., chief com-
plaints using basic patient and diagnosis informa-
tion (Lee, 2018)). Evaluation scenarios for the
utility of the artificial text usually involve a sin-
gle downstream NLP task (typically, text classifi-
cation).

SOTA approaches tackle other language gener-
ation tasks by applying neural models: variations
of the encoder-decoder architecture (ED) model
(Sutskever et al., 2014; Bahdanau et al., 2015),
a.k.a sequence to sequence (seq2seq), e.g., the
Transformer model (Vaswani et al., 2017). In
this work, we follow these approaches and guide
the generation process with key phrases in the
Transformer model.

Our main contribution is thus twofold: (1) a
single methodology to generate medical text for
a series of downstream NLP tasks; (2) an assess-
ment of the utility of the generated data as comple-
mentary training data in two important biomedical
NLP tasks: text classification (phenotype classi-
fication) and temporal relation evaluation. Addi-
tionally, we thoroughly study the usefulness of the
generated data in a set of scenarios where it fully
replaces real training data.

2 Related Work

Natural Language Generation. Natural lan-
guage generation is an NLP area with a range
of applications such as dialogue generation,
question-answering, machine translation (MT),
summarisation, simplification, storytelling, etc.

SOTA approaches attempt to solve these tasks
by using neural models. One of the most widely
used models is the encoder-decoder architecture
(ED) (Sutskever et al., 2014; Bahdanau et al.,
2015). In this architecture, the decoder is a con-
ditional language model. It generates a new word
at a timestep taking into account the previously
generated words, as well as the information pro-
vided by the encoder (a sequence of hidden states,
roughly speaking, a set of automatically learned
features).

For different tasks, the input to the encoder may
be different: questions for question-answering,



241

source text for MT, story prompts for story gen-
eration, etc.

Long text generation. One of the main chal-
lenges of the ED architecture remains the genera-
tion of long coherent text. In this work, we con-
sider paragraphs as long text. Other NLP tasks
may target documents, or even group of docu-
ments (e.g., multi-document summarisation sys-
tems).

Existing vanilla ED models mainly focus on lo-
cal lexical decisions which limits their ability to
model the global integrity of the text. This issue
can be tackled by varying the generation condi-
tions: e.g., guiding the generation with prompts
(Fan et al., 2018), with named entities (Clark
et al., 2018) or template-based generation (Wise-
man et al., 2018). All these conditions serve as
binding elements to relate generated sentences and
ensure the cohesion of the resulting text.

In this work, we follow the approach of Peng
et al. (2018) and guide the generation of Elec-
tronic Health Record (EHR) notes with the help of
key phrases (phrases composed of frequent con-
tent words often co-occurring with other content
words). These key phrases are sense-bearing el-
ements extracted at the paragraph level. Using
them as guidance ensures semantic integrity and
relevance of the generated text. We experiment
with the SOTA ED Transformer model. The
model is based on multi-head attention mecha-
nisms. Such mechanisms decide which parts of
input and previously generated output are rele-
vant for the next generation decision. Heads are
designed to attend to information from different
representation subspaces. Recent studies show
that their roles are potentially linguistically in-
tepretable: e.g., attending to syntactic dependen-
cies or rare words (Voita et al., 2019).

Usage of artificial data in NLP In MT, artifi-
cial data has been successfully used in addition to
real data for training ED models. There have also
been attempts to build MT models in low-resource
conditions only with artificial data (Poncelas et al.,
2018). In this work, we investigate the usefulness
of the generated data both in the complementary
setting and in the full replacement setting.

Medical text generation. The generation of
medical data destined to help clinicians has been
addressed e.g. through generation of imaging re-
ports by Jing et al. (2018); Liu (2018).

However, to our knowledge, there have been

very few attempts to create artificial medical data
to help NLP. One attempt to create such data can
be found in (Suominen et al., 2015), where nurs-
ing handover data is generated in a very costly way
with the help of a clinical professional who wrote
imaginary text.

The attempt closest to ours is the one of
Lee (2018). They generate short-length (under
20 tokens) chief complaints using diagnosis and
patient- and admission-related information as con-
ditions in the conditional LM. The authors inves-
tigate the clinical validity of the generated text by
using it as test data for NLP models built with real
data. But they do not look into the utility of the
generated data for building NLP models.

3 Methodology

As mentioned before, in our attempt to find an op-
timal way to generate synthetic EHRs we exper-
iment with the Transformer architecture. We
extract key phrases at the paragraph level, match
them at the sentence level and further use them as
inputs into our generation model (see Figure 1).
Thus, each paragraph is generated sentence by
sentence but taking the information ensuring its in-
tegrity into account.

Figure 1: Our generation methodology to guide the
generation with key phrases.

The intrinsic evaluation of the generated data
is performed with a set of metrics standard for
text generation tasks: ROUGE-L (Lin, 2004) and
BLEU (Papineni et al., 2002). ROUGE-Lmeasures
the n-gram recall, BLEU– the n-gram precision.
We also assess the length of the generated text.

At the extrinsic evaluation step, we use gener-
ated data as training data in a phenotype classifi-



242

cation task and a temporal relation extraction task.
For each task, we experiment with neural models.
We compare performance of three models: one
trained with real data, one trained using upsam-
pled real data (the real dataset repeated twice) and
one built using real data augmented with generated
data for real test sets (see Figure 2). Development
sets are also real across setups. By upsampling the
real data twice we create a baseline mimicking a
very bad generation model simply reproducing the
original data without adding any variation to it.

Figure 2: Our extrinsic evaluation procedure with real
test data.

We further investigate the actual contribution of
the artificial data to the classification process in
experiments where we fully replace the real train-
ing data with the artificial training data for both
neural and non-neural algorithms. Useful artifi-
cial data models should demonstrate similar per-
formance results to real models. And, most impor-
tantly, those artificial data models should correctly
preserve any differences between classification al-
gorithms trained using the real data.

4 Experimental Setup

In what follows, we describe the data used in ex-
periments (Subsection 4.1), details of generation
models (Subsection 4.2) and classification models
(Subsection 4.3) we use.

4.1 Data
In our study we use EHRs from the publicly avail-
able MIMIC-III database (Johnson et al., 2016;
Johnson and Pollard, 2016). MIMIC-III contains
de-identified clinical data of around 50K adult pa-
tients to the intensive care units (ICU) at the Beth
Israel Deaconess Medical Center from 2001 to
2012. The dataset comprises several types of clini-
cal notes, including discharge summaries, nursing
notes, radiology and ECG reports.

Text generation dataset. For the text gener-
ation experiments, we extract all the MIMIC-III

discharge summaries of the patients with the 3
first diagnoses (ordered by their priority, repre-
sented by 2 first characters of each respective ICD-
9 code) matching at least one sequence of the 3
first diagnoses for the patients from our pheno-
typing dataset (used later in our phenotype clas-
sification experiments). Thus, our text generation
dataset do not contain the patients from the pheno-
typing dataset.

From all the extracted data we randomly se-
lect records of 126 patients for development pur-
poses. This results in two subsets: train-gen
and val-gen (see Table 1). As our test
sets we used parts of the phenotyping dataset
(test-gen-pheno) and of the temporal rela-
tions dataset (test-gen-temp) described be-
low.

set #, patient ID #, admission ID #, lines #, tok.

train-gen 9767 10926 1.2M 20M
val-gen 126 132 13K 224K

Table 1: Statistics over train-gen, and val-gen.
# denotes number.

Our preprocessing pipeline including sentence
detection uses the spaCy-2.0.18 toolkit.1 We low-
ercase all texts. In addition, we replace dates with
a placeholder date. We discard all the sentences
with length under 5 words.

Phenotyping dataset. In our text classification
experiments we use the phenotyping dataset from
MIMIC-III database released by Gehrmann et al.
(2018). Phenotyping is the task of determining
whether a patient has a medical condition or is at
risk for developing one. The dataset includes dis-
charge summaries annotated with 13 phenotypes
(e.g., advanced cancer, advanced heart disease, ad-
vanced lung disease, etc.)2

The phenotyping dataset used in our exper-
iments contains 1,600 discharge summaries of
1,561 patients (around 180K sentences). We fol-
low Gehrmann et al. (2018) and randomly se-
lect 10% and 20% of this data for development
and test purposes respectively (dev-pheno and
test-pheno). The rest 70% is used as the
test set for the generation experiments and as the
training set for the phenotype classification exper-
iments (test-gen-pheno).3

1https://spacy.io
2https://github.com/sebastianGehrmann/

phenotyping
3Because of structural differences between MIMIC-III

https://spacy.io
https://github.com/sebastianGehrmann/phenotyping
https://github.com/sebastianGehrmann/phenotyping


243

Temporal relations dataset. In the temporal
relation classification experiments, we use the data
set from the 2012 i2b2 temporal relations shared
task (Sun et al., 2013b). The task focuses on de-
termining the relative ordering of the events in
medical history with respect to each other and
to time expressions. The dataset contains texts
of discharge summaries from MIMIC-II. Various
textual segments in these summaries are manu-
ally annotated for events (EVENT), time expres-
sions (TIMEX3) and eight temporal relations be-
tween them (TLINK). In this study we focus only
on detecting the presence of the most frequent
OVERLAP temporal relation between events (33%
of the annotated relations). OVERLAP indicates
that two related events happen almost the same
time, but not exactly (Sun et al., 2013a) (see Fig-
ure 3).

Figure 3: Example of an OVERLAP temporal relation
(paraphrased).

The original training set includes 190 discharge
summaries. We experiment with this dataset to
demonstrate the transferability of our generation
methodology. Hence, we do not modify our gen-
eration model but instead filter out the discharge
summaries in the 2012 i2b2 dataset that overlap in
their content with train-gen (according to the
≥ 10 sentences criteria).

For the 2012 i2b2 data, we condition the gen-
eration using the textual segments annotated as
EVENT. These could also be seen as binding el-
ements of parts of longer text. Moreover, textual
segments given in the input are mostly preserved
in the generated output. The advantage of this ap-
proach is that in most of the cases we do not need
to redo human annotation in the generated text be-
cause they are preserved if given in the input. Ta-
ble 2 reports the statistics of the original (all)
data versus the data (reduced) for which the an-
notations are preserved.

10% of the data is randomly selected for devel-

and MIMIC-II database that was initially used to collect
the phenotyping dataset, we could not correctly identify text
fields for records with duplicated admission IDs. We simply
merged those records together giving preferences to annota-
tions with a higher rate of positive labels. This resulted in a
small reduction of the initial dataset (less than 1%).

opment purposes (dev-temp). The rest of the
data is again used as the test data for the generation
task and as the training data for the temporal clas-
sification models (test-gen-temp). The test
set provided with the 2012 i2b2 temporal relations
shared task was used as is for temporal classifica-
tion models (test-temp).

#, docs #, lines #, tok. %, OVERLAP.

all 190 7447 97K 33.0
reduced 175 6762 89K 33.6

Table 2: Statistics over test-gen-temp. The all
dataset corresponds to the one provided by the organ-
isers. The reduced dataset is the one for which the
annotations are preserved by the generation model. #
denotes number.

4.2 Text Generation Models

In our text generation experiments we use the
Transformer model, which generates text sen-
tence by sentence. To ensure the semantic in-
tegrity of paragraphs resulting from the concate-
nation of generated sentences, we guide the gener-
ation with key phrases. Key phrases are extracted
from each original paragraph of train-gen. For
this, we use the Rake algorithm (Rose et al.,
2010)4 and take the highest scored 50% per para-
graph. We further generate a paragraph sen-
tence by sentence using as inputs only those ex-
tracted key phrases that are present in each partic-
ular sentence. This results in approximately 2.4
key phrases with an average length of 1.7 words
per sentence (as computed for train-gen).5

Boundaries of key phrases in the input to mod-
els are fixed by a reserved token. During train-
ing, the model is learned to restore real text from
key phrases, basically by filling the gaps between
those key phrases.

We trained our Transformer models as pro-
vided by the OpenNMT toolkit (Klein et al., 2017)
with default parameters. In train-gen we re-
placed all the words with frequency 1 with a place-
holder. This resulted in a vocabulary of around
50K words. Each model was trained for 30K
epochs.6 Outputs are produced with the standard
beam decoding procedure with a beam size of 5.

4The algorithm selects phrases composed of frequent con-
tent words co-occurring with other content words.

5We used the implementation available at https://
github.com/csurfer/rake-nltk

6We noticed that this quantity of epochs is necessary for

https://github.com/csurfer/rake-nltk
https://github.com/csurfer/rake-nltk


244

test-gen-pheno

1 gen a ct was obtained which revealed a very poor study but no evidence of a brain injury .
real ct was a poor study but did not reveal a brain injury .

2 gen he had a walk of losing blood .
real she is unable to walk without losing blood .

test-gen-temp

3 gen he was treated with increasing doses of rosuvastatin and atorvastatin .
real he has been on increasing doses of rosuvastatin receiving atorvastatin in addition on a basis .

4 gen he was started on ibuprofen and his wife back pain was improved .
real the patient was initially treated with ibuprofen which was stopped after his back pain improved .

Table 3: Examples of real and generated text. The underlined text highlights “good” (examples 1 and 3) or “bad”
(examples 2 and 4) modifications. All sentences have been paraphrased.

4.3 Models for Phenotype Classification
For the phenotype classification task, we train two
standard NLP models:

1. Convolutional Neural Network (CNNs)
model inspired by (Kim, 2014). The CNN
model is built with 3 convolutional layers
with window sizes of 3, 4 and 8 respectively.
The word embedding dimensionality is 300,
both convolution layers have 100 filters. The
size of the hidden units of the dense layer
is 100. We also use a dropout layer with
a probability of 0.5. The network is imple-
mented using the Pytorch toolkit7 with the
pre-trained GloVe word embeddings (Pen-
nington et al., 2014).

2. Word-level bag-of-words (BoW) model
trained with the Naive Bayes (NB) algo-
rithm. We applied the MultinomialNB im-
plementation from Scikit-learn (Pe-
dregosa et al., 2011).

We cast the task as a binary classification task
and evaluate the detection of each phenotype com-
puting the F1-score of the positive class.

4.4 Models for Temporal Relations
Extraction

Inspired by the SOTA approaches for the
task (Tourille et al., 2017), we build a Bidirec-
tional Long Short-Term Memory (BiLSTM) clas-
sifier (Hochreiter and Schmidhuber, 1997). The
BiLSTM model is constructed with two hidden
layers of opposite directions. The size of hidden
LSTM units is 512. We use a dropout layer before
the output layer with a probability of 0.2 and the

stabilization of the model perplexity.
7https://pytorch.org

ROUGE-L BLEU avg. sent. l
(gen./real)

test-gen-pheno 67.74 40.62 13.27 / 17.50
test-gen-temp 48.47 20.91 18.61 / 16.81

Table 4: Qualitative evaluation and average sentence
lengths.

concatenation of the last hidden states of both lay-
ers goes into the ouput layer. We train our network
with the Adam (Kingma and Ba, 2014) optimiza-
tion algorithm with a batch size of 64 and a learn-
ing rate of 0.001. We use again the pre-trained
GloVe word embeddings. The classifier is imple-
mented using Pytorch. As for a non-neural model,
we use again the NB model as for the phenotype
classification task.

We cast the task as a binary classification task
(for each event-event pair, classify as OVERLAP
or not) and evaluate the result by computing the
F1-score of the positive decision.

5 Experimental Results

In this section we present results of our experi-
ments, first of the intrinsic evaluation of the qual-
ity of generated text (Section 5.1) and then of the
extrinsic evaluation of its utility for NLP (text clas-
sification and temporal relation extraction tasks,
Section 5.2).

5.1 Intrinsic Evaluation

Table 4 shows the intrinsic evaluation results
for both generated test-gen-pheno and
test-gen-temp. The BLEU and ROUGE-L
are computed between the original text (the one
used to extract key phrases) and the generated text.
We also compare the average lengths sentences for
those two texts.

https://pytorch.org


245

O
be

si
ty

N
on

A
dh

er
en

ce

D
ev

el
op

m
en

ta
l.D

el
ay

.R
et

ar
da

tio
n

A
dv

.H
ea

rt
D

is
ea

se

A
dv

.L
un

g
D

is
ea

se

Sc
hi

zo
an

d
ot

he
rP

sy
ch

.D
is

or
de

rs

A
lc

oh
ol

A
bu

se

O
th

er
Su

bs
ta

nc
e

A
bu

se

C
hr

.P
ai

n
Fi

br
om

ya
lg

ia

C
hr

.N
eu

ro
lo

gi
ca

lD
ys

tr
op

hi
es

A
dv

.C
an

ce
r

D
ep

re
ss

io
n

D
em

en
tia

av
g.

freq, % 8 9 3 17 10 18 12 10 20 23 10 29 7

CNN

real + gen 0.3257 0.3394 0.3636 0.6384 0.5333 0.3664 0.7428 0.5714 0.3846 0.5574 0.6173 0.4373 0.5714 0.4961
real 0.3789 0.3589 0.2500 0.6019 0.5085 0.2909 0.7200 0.4912 0.4000 0.4782 0.5567 0.4623 0.5667 0.4665

2 × real 0.3636 0.3333 0.2857 0.5347 0.5758 0.3057 0.7435 0.4789 0.4040 0.4580 0.5667 0.4162 0.6341 0.4692

gen 0.2500 0.3656 0.2000 0.4667 0.5574 0.3221 0.7297 0.4478 0.3978 0.4564 0.6575 0.4598 0.3273 0.4337
gen-key 0.1365 0.2443 0.0252 0.5200 0.1429 0.2978 0.2581 0.1914 0.3781 0.3740 0.3778 0.4262 0.0800 0.2656

NB

real 0.2000 0.4722 0.0000 0.5812 0.4838 0.5614 0.6756 0.5000 0.4109 0.5270 0.6779 0.5700 0.3846 0.4650
gen 0.2424 0.4719 0.0000 0.5893 0.4687 0.5000 0.6506 0.4594 0.4022 0.5122 0.6562 0.5391 0.3125 0.4465

gen-key 0.1407 0.1984 0.0447 0.3022 0.2108 0.2857 0.2367 0.1723 0.3284 0.3815 0.2032 0.4398 0.1039 0.2345

Table 5: Phenotyping results for CNN and Naive Bayes (NB), test-pheno. Best performing models for CNN
data augmentation experiments are highlighted in bold. We report results for the models trained with: real data
augmented with generated gen data, real data only, 2 × real data upsampled twice, gen data only, gen-key
data without traces of the input real data.

As expected, automatic evaluation scores
show that for both test sets our model gener-
ates context preserving pieces of the real text
from the input (e.g., ROUGE-L = 67.74 for
test-gen-pheno, ROUGE-L = 48.47 for
test-gen-temp). The proximity of average
lengths of sentences for the generated text and the
real text supports this statement.

As automatic metrics perform only a shallow
comparison, we also manually reviewed a sample
of texts. In general, most of the generated text
preserves the main meaning of the original text
adding or dropping some details. Incomprehen-
sible generated sentences are rare.

Table 3 shows examples of the generated
text for both datasets. In examples 1 and 3,
Transformer generates text with a meaning
very close to the original one (e.g., no evidence
of ≈ did not reveal, for test-gen-pheno). Ex-
amples 2 and 4 are “bad” modifications. In gen-
eral, such examples are infrequent. For instance in
Example 2, the real phrase unable to walk without
losing blood is incorrectly modified into a walk of
losing blood. However, the main sense of losing
blood is preserved.

Overall, our observations indicate that the
generation methodology successfully adapts to
changes in generation conditions.

5.2 Extrinsic Evaluation

Phenotype Classification. Table 5 shows results
of our text classification experiments. They indi-
cate that the artificial training data used as compli-
mentary to the real training data is in general ben-
eficial for the CNN model (e.g., av. F-score=0.50
for real + gen > 0.47 for real). real +gen
setup also outperforms the model trained using
larger volume data, where the training data was
repeated two times (2 × real). Overall, real
+gen outperforms real for 9 phenotypes out
of 13 with an average ∆F-score=0.06, while 2
× real for 6 phenotypes with an average ∆F-
score=0.04 only.

To get further insights into the actual informa-
tiveness of the generated data, we study the per-
formance of both CNN and NB in a series of se-
tups where the artificial training data fully replace
the real training data. To be more precise, we
study: (a) gen setup, where the full generated
data with traces of input key phrases are used as
the training data; and (b) gen-key setup, where
the generated text without traces of input data is
used as the training data (see Figure 4). The re-
sults of these experiments are in Table 5, lower
part. They show that average performances of
gen and real tend to be comparable for each al-
gorithm (e.g., ∆ avg. F-score=0.03 for both CNN
and NB). The gen-key setup results in a signif-



246

F-score Features (words)

real 0.5614 chest, 20, given, 11, hours, time, history, admission, continued, capsule, needed, 25, disease, refills, follow,
negative, started, status, disp, days, release, discharge, ml, stable, hct, prior, dr, showed, 40, fax, neg, telephone,
likely, 15, glucose, wbc, home, renal, care, seen, iv, 24, acute, urine, post, noted, artery, 14, year, unit, tube, inr,
bid, 50, edema, units, plt, insulin, known, course, pulmonary, mild, did, dose

gen 0.5000 follow, 12, fax, renal, admission, care, telephone, prior, artery, bid, acute, dr, unit, known, time, post, likely,
seen, neg, discharge, iv, insulin, tube, units, admitted, placed, year, 11, 25, 13, pulmonary, urine, dose, delayed,
mild, chronic, transferred, edema, lower, pressure, heart, course, fluid, failure, ventricular, aortic, abdominal, 50,
discharged, medications, valve, evidence, noted, increased

gen-key 0.2857 blood, day, mg, 10, 07, date, pt, 10pm, refills, 100, 20, tablet, needed, started, ct, plt, 12, 30, inr, 11, 25, 13, dr,
times, 50, sig, 213, 24, patient, daily, 40, 500, telephone, release, transferred, negative, discharged, 81, follow,
final, admitted, 15, 30pm, time, fax, hours, delayed, normal, placed, history, 20am, seen, breath, 00, did, 18,
15pm, evidence, 80, admission, consulted, home, wbc, po, hct, bedtime, shortness

Table 6: Top-30 words contributing the most to the Advanced Lung Disease phenotype detection using
Naive Bayes.

icant performance drop (of F-score=0.2 on aver-
age). However, the gen-key text still potentially
bears some relevant information that allows both
CNN and NB have comparable performance for
this setup.

Figure 4: Example of creating gen gen-key data –
the generated text without traces of input data (para-
phrased)

Taking advantage of the easy interpretability of
the NB model, we analyse the words that con-
tribute the most to classification decisions (highest
likelihoods given the positive class) for the Adv.
Lung Disease as a an example of a phenotype
with an average frequency for the dataset. Ta-
ble 6 displays those words in order of importance
for real, gen and gen-key. As expected, for
real and gen with higher F-score values, there
are more relevant medical terms: e.g., pulmonary
and chest. For gen-key, there are words more
distantly related to the phenotype: e.g., ct and
breath.

Temporal Evaluation. For the i2b2 dataset,
we focus only on the evaluation for the OVERLAP
temporal relation between events as the most
well-represented group. Inspired by the SOTA
solutions for the temporal relations extraction
task (Tourille et al., 2017), we provide only the
text spans that link the two events as inputs to our
models. This setup is particularly beneficial to as-
sess the utility of the generated text (see Figure 5).

Figure 5: Example of an input to our models for tempo-
ral relations extraction – a text span that links the two
events (paraphrased).

As mentioned earlier, for this dataset we guide the
text generation with event text spans. Thus, for
this setup, we take only the text between those real
text spans essentially copied from the input. This
allows us to better assess the utility only of what
was generated.8

Table 7 reports results for our experiments with
the i2b2 dataset. They are similar to the ones
performed for the phenotyping dataset. Note that
we reduce the initial training set provided by the
task due to particularities of our generation pro-
cedure. In our data augmentation experiments we
add this reduced generated data to all the provided
real training data (real all).

The results show that real all + gen
(F-score=0.62) outperforms the real setup (F-
score=0.59), as well as the upsampled setup (2
× real all, F-score=0.58). This confirms
the utility of our data augmentation procedure
for the BiLSTM model. Results for gen and
real reduced are again comparable for BiL-

8However, it should be noted here that the generated text
between two events may still contain other event spans copied
from the input, especially for the cases when events are in
different sentences.



247

BiLSTM
real all + gen 0.6217

real all 0.5896
2 × real all 0.5803

gen 0.5138
real reduced 0.5312

NB

gen 0.5769
real reduced 0.5024

Table 7: Temporal relations extraction for OVERLAP
for CNN and Naive Bayes (NB), test-temp. Only
the real/generated text between events serves as input.
Best performing models for data augmentation experi-
ments are highlighted in bold. We report results for the
models trained using: real all training data from
the i2b2 task augmented with the generated gen data,
real all data only, 2 × real all data upsampled
twice, real reduced data only, gen data only.

STM. For NB, we even observe an improvement
of ∆F-score=0.08 for gen as compared to real
reduced for NB. This may be explained by a
stronger semantic signal in the generated data.
Overall, our results demonstrate the potential of
developing a model that would generate artificial
medical data for a series of NLP tasks.

6 Discussion

Our study is designed as a proof-of-concept and
the main objective of this work is to study the util-
ity of using SOTA approaches for generating artifi-
cial EHR data and to evaluate the impact of using
this to augment real data for common NLP tasks
in the clinical domain. Our results are promis-
ing. From a preliminary manual analysis, most
meaning is preserved in the generated texts. For
both extrinsic evaluation tasks (phenotype classifi-
cation, and temporal relation classification), using
generated text to augment real data in the train-
ing phase improved results. Moreover, for both
tasks, results using only generated data was com-
parable to those using only real data, further indi-
cating usefulness.

To our knowledge, this is the first study looking
at the problem of generating longer clinical text,
and that is extrinsically evaluated on two down-
stream NLP tasks. Although the MIMIC data is
comprehensive, it represents a particular type of
clinical documentation from an ICU setting, in
further work we plan to extend to other clinical
domains.

If artificial data was to be used for further down-
stream tasks, particularly those that are intended to
support secondary uses in a clinical research set-
ting, further analysis is needed to assess the clin-
ical validity of the generated text. This would
require domain expertise. For instance, the tem-
poral relation classification problem imposes dif-
ferent constraints as compared with the document
classification task, which might require other ap-
proaches for designing the text generation models.
Moreover, other temporal information representa-
tion models have been proposed in other studies,
for other use-cases, such as the CONTAINS rela-
tion in the THYME corpus (Styler IV et al., 2014).
In future studies, we will invite clinicians to re-
view the generated text with a focus on clinical va-
lidity aspects, as well as study further downstream
NLP tasks. We will also study additional alter-
native metrics for intrinsic evaluation, such as the
modified CIDEr metric proposed by Lee (2018).

7 Conclusion

In this work, we attempt to generate artificial train-
ing data for two downstream clinical NLP tasks:
text classification and temporal relation extraction.
We propose a generic methodology to guide the
generation in both cases. Our experiments show
the utility of artificial data for neural NLP mod-
els in data augmentation setups. Our generation
methodology holds promise for the development
of a more universal approach that will allow med-
ical text generation for an even wider range of
biomedical NLP tasks. We also plan to further in-
vestigate the validity and utility of artificial data.
We think thus, that artificial data generation is an
approach that has the potential to solve current
data accessibility issues associated with biomed-
ical NLP.

Acknowledgments

This work was partly funded by EPSRC Heal-
tex Feasibility Funding (Towards Shareable Data
in Clinical Natural Language Processing: Gen-
erating Synthetic Electronic Health Records).
The third author has received support from the
Swedish Research Council (2015-00359), and the
Marie Skłodowska Curie Actions, Cofund, Project
INCA 600398. We would like to thank the anony-
mous reviewers for their helpful comments.



248

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
International Conference on Learning Representa-
tions (ICLR).

Elizabeth Clark, Yangfeng Ji, and Noah A. Smith.
2018. Neural text generation in stories using en-
tity representations as context. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 2250–2260. Association for Computa-
tional Linguistics.

Sergey Edunov, Myle Ott, Michael Auli, and David
Grangier. 2018. Understanding back-translation at
scale. arXiv preprint arXiv:1808.09381.

Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 889–898. Association for Computa-
tional Linguistics.

Sebastian Gehrmann, Franck Dernoncourt, Yeran
Li, Eric T Carlson, Joy T Wu, Jonathan Welt,
John Foote, Edward T Moseley, David W Grant,
Patrick D Tyler, and Leo A Celi. 2018. Comparing
deep learning and concept extraction based meth-
ods for patient phenotyping from clinical narratives.
PLoS ONE, 13(2):e0192360.

S Hochreiter and J Schmidhuber. 1997. Long short-
term memory. Neural Computation, 9(8):1735–
1780.

Baoyu Jing, Pengtao Xie, and Eric Xing. 2018. On the
Automatic Generation of Medical Imaging Reports.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2577–2586. Association for
Computational Linguistics.

Alistair E W Johnson and Tom J Pollard. 2016. The
MIMIC-III Clinical Database.

Alistair E W Johnson, Tom J Pollard, Lu Shen, Li-
wei H Lehman, Mengling Feng, Mohammad Ghas-
semi, Benjamin Moody, Peter Szolovits, Leo An-
thony Celi, and Roger G Mark. 2016. MIMIC-III,
a freely accessible critical care database. Scientific
Data, 3:160035.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. CoRR, abs/1408.5882.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. OpenNMT:

Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67–72. Association for Computational
Linguistics.

Scott H. Lee. 2018. Natural language generation for
electronic health records. npj Digital Medicine,
1(1):63.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In ACL workshop
on Text Summarization Branches Out.

Peter J. Liu. 2018. Learning to write notes in electronic
health records. CoRR, abs/1808.02622.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311–318.

Fabian Pedregosa, Gaël Varoquaux, Vincent Michel
Alexandre Gramfort, Bertrand Thirion, Olivier
Grisel, Peter Prettenhofer Mathieu Blondel, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Édouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.

Nanyun Peng, Marjan Ghazvininejad, Jonathan May,
and Kevin Knight. 2018. Towards controllable story
generation. In Proceedings of the First Workshop on
Storytelling, pages 43–49. Association for Compu-
tational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Alberto Poncelas, Dimitar Shterionov, Andy Way,
Gideon Maillette de Buy Wenniger, and Peyman
Passban. 2018. Investigating backtranslation in neu-
ral machine translation. In Proceedings of th 21st
International Conference of the European Associa-
tion for Machine Translation (EAMT).

Stuart Rose, Dave Engel, Nick Cramer, and Wendy
Cowley. 2010. Automatic keyword extraction from
individual documents. Text Mining: Applications
and Theory, pages 1 – 20.

William F Styler IV, Steven Bethard, Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, and James Pustejovsky. 2014. Temporal an-
notation in the clinical domain. Transactions of the
Association for Computational Linguistics, 2:143–
154.

Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner.
2013a. Annotating temporal information in clini-
cal narratives. Journal of biomedical informatics,
46 Suppl(0):S5–S12.

https://arxiv.org/abs/1409.0473
https://arxiv.org/abs/1409.0473
http://aclweb.org/anthology/N18-1204
http://aclweb.org/anthology/N18-1204
https://aclweb.org/anthology/D18-1045
https://aclweb.org/anthology/D18-1045
http://aclweb.org/anthology/P18-1082
https://doi.org/10.1371/journal.pone.0192360
https://doi.org/10.1371/journal.pone.0192360
https://doi.org/10.1371/journal.pone.0192360
http://bioinf.jku.at/publications/older/2604.pdf
http://bioinf.jku.at/publications/older/2604.pdf
http://aclweb.org/anthology/P18-1240
http://aclweb.org/anthology/P18-1240
http://dx.doi.org/10.13026/C2XW26
http://dx.doi.org/10.13026/C2XW26
http://dx.doi.org/10.1038/sdata.2016.35 http://10.0.4.14/sdata.2016.35
http://dx.doi.org/10.1038/sdata.2016.35 http://10.0.4.14/sdata.2016.35
http://arxiv.org/abs/1408.5882
http://arxiv.org/abs/1408.5882
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
http://aclweb.org/anthology/P17-4012
http://aclweb.org/anthology/P17-4012
https://doi.org/10.1038/s41746-018-0070-0
https://doi.org/10.1038/s41746-018-0070-0
http://research.microsoft.com/~cyl/download/papers/WAS2004.pdf
http://research.microsoft.com/~cyl/download/papers/WAS2004.pdf
http://arxiv.org/abs/1808.02622
http://arxiv.org/abs/1808.02622
http://www.aclweb.org/anthology/P02-1040
http://www.aclweb.org/anthology/P02-1040
http://www.jmlr.org/papers/v12/pedregosa11a.html
http://aclweb.org/anthology/W18-1505
http://aclweb.org/anthology/W18-1505
https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/
http://eamt2018.dlsi.ua.es/proceedings-eamt2018.pdf
http://eamt2018.dlsi.ua.es/proceedings-eamt2018.pdf
https://doi.org/10.1002/9780470689646.ch1
https://doi.org/10.1002/9780470689646.ch1
https://www.aclweb.org/anthology/Q14-1012
https://www.aclweb.org/anthology/Q14-1012
https://www.ncbi.nlm.nih.gov/pubmed/23872518
https://www.ncbi.nlm.nih.gov/pubmed/23872518


249

Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner.
2013b. Evaluating temporal relations in clinical
text: 2012 i2b2 challenge. Journal of the American
Medical Informatics Association, 20(5):806–813.

Hanna Suominen, Liyuan Zhou, Leif Hanlen, and
Gabriela Ferraro. 2015. Benchmarking Clinical
Speech Recognition and Information Extraction:
New Data, Methods, and Evaluations. JMIR Med-
ical Informatics, 3(2):e19.

Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.
2014. Sequence to sequence learning with neural
networks. In Advances in Neural Information Pro-
cessing Systems 27, pages 3104–3112.

Julien Tourille, Olivier Ferret, Aurélie Névéol, and
Xavier Tannier. 2017. Neural architecture for tem-
poral relation extraction: A bi-lstm approach for de-
tecting narrative containers. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
224–230, Vancouver, Canada. Association for Com-
putational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran As-
sociates, Inc.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sen-
nrich, and Ivan Titov. 2019. Analyzing Multi-Head
Self-Attention: Specialized Heads Do the Heavy
Lifting, the Rest Can Be Pruned. In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics, Florence, Italy. Associa-
tion for Computational Linguistics.

Sam Wiseman, Stuart M Shieber, and Alexander M
Rush. 2018. Learning neural templates for text gen-
eration. arXiv preprint arXiv:1808.10122.

https://academic.oup.com/jamia/article/20/5/806/726374
https://academic.oup.com/jamia/article/20/5/806/726374
https://doi.org/10.2196/medinform.4321
https://doi.org/10.2196/medinform.4321
https://doi.org/10.2196/medinform.4321
https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
http://aclweb.org/anthology/P17-2035
http://aclweb.org/anthology/P17-2035
http://aclweb.org/anthology/P17-2035
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://arxiv.org/pdf/1905.09418.pdf
https://arxiv.org/pdf/1905.09418.pdf
https://arxiv.org/pdf/1905.09418.pdf
https://www.aclweb.org/anthology/D18-1356
https://www.aclweb.org/anthology/D18-1356

