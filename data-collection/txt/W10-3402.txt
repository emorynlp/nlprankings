



















































SemanticNet-Perception of Human Pragmatics


Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 2–11,
Beijing, August 2010

SemanticNet-Perception of Human Pragmatics   

Amitava Das
1
 and Sivaji Bandyopadhyay

2
 

Department of Computer Science and Engineering  

Jadavpur University 

amitava.santu@gmail.com1 sivaji_cse_ju@yahoo.com2  

 

Abstract 

SemanticNet is a semantic network of 

lexicons to hold human pragmatic 

knowledge. So far Natural Language 

Processing (NLP) research patronized 

much of manually augmented lexicon 

resources such as WordNet. But the 

small set of semantic relations like 

Hypernym, Holonym, Meronym and 

Synonym etc are very narrow to cap-

ture the wide variations human cogni-

tive knowledge. But no such informa-

tion could be retrieved from available 

lexicon resources. SemanticNet is the 

attempt to capture wide range of con-

text dependent semantic inference 

among various themes which human 

beings perceive in their pragmatic 

knowledge, learned by day to day cog-

nitive interactions with the surrounding 

physical world. SemanticNet holds 

human pragmatics with twenty well es-

tablished semantic relations for every 

pair of lexemes. As every pair of rela-

tions cannot be defined by fixed num-

ber of certain semantic relation labels 

thus additionally contextual semantic 

affinity inference in SemanticNet could 

be calculated by network distance and 

represented as a probabilistic score. 

SemanticNet is being presently devel-

oped for Bengali language. 

1 Historical Motivation 

Semantics (from Greek "σηµαντικός" - seman-

tikos) is the study of meaning, usually in lan-

guage. The word "semantics" itself denotes a 

range of ideas, from the popular to the highly 

technical. It is often used in ordinary language 

to denote a problem of understanding that 

comes down to word selection or connotation. 

We studied with various Psycholinguistics ex-

periments to understand how human natural 

intelligence helps to understand general se-

mantic from nature. Our study was to under-

stand the human psychology about semantics 

beyond language. We were haunting for the 

intellectual structure of the psychological and 

neurobiological factors that enable humans to 

acquire, use, comprehend and produce natural 

languages. Let’s come with an example of 

simple conversation about movie between two 

persons. 
Person A: Have you seen the 

movie ‘No Man's Land’? How 

is it? 

Person B: Although it is 

good but you should see 

‘The Hurt Locker’? 

May be the conversation looks very casual, 

but our intension was to find out the direction 

of the decision logic on the Person B’s brain. 

We start digging to find out the nature of hu-

man intelligent thinking. A prolonged discus-

sion with Person B reveals that the decision 

logic path to recommend a good movie was as 

the Figure 1. The highlighted red paths are the 

shortest semantic affinity distances of the hu-

man brain. 

We call it semantic thinking. Although the 

derivational path of semantic thinking is not 

such easy as we portrait in Figure 1 but we 

keep it easier for understandability. Actually a 

human try to figure out the closest semantic 

affinity node into his pragmatics knowledge by 

natural intelligence. In the previous example 

Person B find out with his intelligence that No 

Man's Land is a war movie and got Oscar 

2



award. Oscar award generally cracked by Hol-

lywood movies and thus Person B start search-

ing his pragmatics network to find out a movie 

fall into war genre, from Hollywood and may 

be got Oscar award. Person B finds out the 

name of a movie The Hurt Locker at nearer 

distance into his pragmatics knowledge net-

work which is an optimized recommendation 

that satisfy all the criteria. Noticeably Person B 

didn’t choice the other paths like Bollywood, 

Foreign movie etc. 

 
Figure 1: Semantic Thinking 

And thus our aim was to develop a computa-

tional lexicon structure for semantics as human 

pragmatics knowledge. We spare long time to 

find out the most robust structure to represent 

pragmatics knowledge properly and it should 

be easy understandable for next level of search 

and usability. 

We look into literature that probably direct 

to the direction of our ideological thinking. We 

found that in the year of 1996 Push Singh and 

Marvin Minsky proposed the field has shat-

tered into subfields populated by researchers 

with different goals and who speak very differ-

ent technical languages. Much has been 

learned, and it is time to start integrating what 

we've learned, but few researchers are widely 

versed enough to do so. They had a proposal 

for how to do so in their ConceptNet work. 

They developed lexicon resources like Con-

ceptNet (Liu and Singh, 2004). ConceptNet- 

ConceptNet is a large-scale semantic network 

(over 1.6 million links) relating a wide variety 

of ordinary objects, events, places, actions, and 

goals by only 20 different link types, mined 

from corpus. 

The present task of developing SemanticNet 

is to capture semantic affinity knowledge of 

human pragmatics as a lexicon database.  We 

extend our vision from the human common 

sense (as in ConceptNet) to human pragmatics 

and have proposed semantic relations for every 

pair of lexemes that cannot be defined by fixed 

number of certain semantic relation labels. 

Contextual semantic affinity inference in Se-

manticNet could be calculated by network dis-

tance and represented as a probabilistic score. 

SemanticNet is being presently developed for 

Bengali language. 

2 Semantic Roles 

The ideological study of semantic roles started 

age old ago since Panini’s karaka theory that 

assigns generic semantic roles to words in a 

natural language sentence. Semantic roles are 

generally domain specific in nature such as 

FROM_DESTINATION,TO_DESTINATION, 

DEPARTURE_TIME etc. Verb-specific se-

mantic roles have also been defined such as 

EATER and EATEN for the verb eat. The 

standard datasets that are used in various Eng-

lish SRL systems are: PropBank (Palmer et al., 

2005), FrameNet (Fillmore et al., 2003) and 

VerbNet (Kipper et al., 2006). These collec-

tions contain manually developed well-trusted 

gold reference annotations of both syntactic 

and predicate-argument structures.  

PropBank defines semantic roles for each 

verb. The various semantic roles identified 

(Dowty, 1991) are Agent, patient or theme etc. 

In addition to verb-specific roles, PropBank 

defines several more general roles that can ap-

ply to any verb (Palmer et al., 2005). 

FrameNet is annotated with verb frame se-

mantics and supported by corpus evidence. 

The frame-to-frame relations defined in Fra-

meNet are Inheritance, Perspective_on, Sub-

frame, Precedes, Inchoative_of, Causative_of 

and Using. Frame development focuses on pa-

raphrasability (or near paraphrasability) of 

words and multi-words.  

VerbNet annotated with thematic roles refer 

to the underlying semantic relationship be-

tween a predicate and its arguments. The se-

mantic tagset of VerbNet consists of tags as 

agent, patient, theme, experiencer, stimulus, 

instrument, location, source, goal, recipient, 

benefactive etc. 

It is evident from the above discussions that 

no adequate semantic role set exists that can be 

defines across various domains. Hence pro-

3



posed SemanticNet does not only rely on fixed 

type of semantics roles as ConceptNet. For 

semantic relations we followed the 20 relations 

defined in ConceptNet. Additionally we pro-

posed semantic relations for every pair of lex-

icons cannot be defined by exact semantic role 

and thus we formulated a probabilistic score 

based technique. Semantic affinity in Seman-

ticNet could be calculated by network distance. 

Details could be found in relevant Section 8. 

3 Corpus 

Present SemanticNet has been developed for 

Bengali language. Resource acquisition is one 

of the most challenging obstacles to work with 

electronically resource constrained languages 

like Bengali. Although Bengali is the sixth
1
 

popular language in the World, second in India 

and the national language in Bangladesh.  

There was another issue drive us long way 

to find out the proper corpus for the develop-

ment of SemanticNet. As the notion is to cap-

ture and store human pragmatic knowledge so 

the hypothesis was chosen corpus should not 

be biased towards any specific domain know-

ledge as human pragmatic knowledge is not 

constricted to any domain rather it has a wide 

spread range over anything related to universe 

and life on earth. Additionally it must be larger 

in size to cover mostly available general con-

cepts related to any topic. After a detail analy-

sis we decided it is better to choose NEWS 

corpus as various domains knowledge like Pol-

itics, Sports, Entertainment, Social Issues, 

Science, Arts and Culture, Tourism, Adver-

tisement, TV schedule, Tender, Comics and 

Weather etc are could be found only in NEWS 

corpus.  

Statistics NEWS 
Total no. of news documents in the 

corpus 
108,305 

Total no. of sentences in the corpus 2,822,737 

Avg no. of sentences in a document 27 

Total no. of wordforms in the corpus 33,836,736 

Avg. no. of wordforms in a document 313 

Total no. of distinct wordforms in the 

corpus 
467,858 

Table 1:  Bengali Corpus Statistics 

                                                 
1
 

http://en.wikipedia.org/wiki/List_of_languages_by_

number_of_native_speakers 

Fortunately such corpus development could 

be found in (Ekbal and Bandyopadhyay, 2008) 

for Bengali. We obtained the corpus from the 

authors. The Bengali NEWS corpus consisted 

of consecutive 4 years of NEWS stories with 

various sub domains as reported above. For the 

present task we have used the Bengali NEWS 

corpus, developed from the archive of a lead-

ing Bengali NEWS paper
2
 available on the 

Web. The NEWS corpus is quite larger in size 

as reported in Table 1. 

4 Annotation 

From the collected document set 200 docu-

ments have been chosen randomly for the an-

notation task. Three annotators (Mr. X, Mr. Y 

and Mr. Z) participated in the present task.  

Annotators were asked to annotate the theme 

words (topical expressions) which best de-

scribe the topical snapshot of the document. 

The agreement of annotations among three 

annotators has been evaluated. The agreements 

of tag values at theme words level is reported 

in Table 2. 

 
Annotators X vs. Y X Vs. Z Y Vs. Z Avg 

Percentage 82.64% 71.78% 80.47% 78.3% 

All Agree 75.45% 

Table 2: Agreement of annotators at theme 

words level 

5 Theme Identification 

Term Frequency (TF) plays a crucial role to 

identify document relevance in Topic-Based 

Information Retrieval. The motivation behind 

developing Theme detection technique is that 

in many documents relevant words may not 

occur frequently or irrelevant words may occur 

frequently. Moreover for the lexicon affinity 

inference, topic or theme words are the only 

strong clue to start with. The Theme detection 

technique has been proposed to resolve these 

issues to identify discourse level most relevant 

thematic nodes in terms of word or lexicon 

using a standard machine learning technique. 

The machine learning technique used here is 

Conditional Random Field (CRF)
3
. The theme 

word detection has been defined as a sequence 

                                                 
2
 http://www.anandabazar.com/ 

3
 http://crfpp.sourceforge.net 

4



labeling problem using various useful depend-

ing features. Depending upon the series of in-

put features, each word is tagged as either 

Theme Word (TW) or Other (O). 

5.1 Feature Organization 

The set of features used in the present task 

have been categorized as Lexico-Syntactic, 

Syntactic and Discourse level features. These 

are listed in the Table 3 below and have been 

described in the subsequent subsections. 

 

Types Features 

Lexico-Syntactic 

POS 

Frequency 

Stemming 

Syntactic 
Chunk Label 

Dependency Parsing Depth 

Discourse Level 

Title of the Document 

First Paragraph 

Term Distribution 

Collocation 

Table 3: Features 

5.2 Lexico-Syntactic Features 

5.2.1 Part of Speech (POS) 

It has been shown by Das and Bandyopadhyay, 

(2009), that theme bearing words in sentences 

are mainly adjective, adverb, noun and verbs 

as other POS categories like pronoun, preposi-

tion, conjunct, article etc. have no relevance 

towards thematic semantic of any document. 

The detail of the POS tagging system chosen 

for the present task could be found in (Das and 

Bandyopadhyay 2009). 

5.3 Frequency 

Frequency always plays a crucial role in identi-

fying the importance of a word in the docu-

ment or corpus. The system generates four 

separate high frequent word lists after function 

words are removed for four POS categories: 

adjective, adverb, verb and noun. Word fre-

quency values are then effectively used as a 

crucial feature in the Theme Detection tech-

nique. 

5.4 Stemming 

Several words in a sentence that carry thematic 

information may be present in inflected forms. 

Stemming is necessary for such inflected 

words before they can be searched in appropri-

ate lists. Due to non availability of good stem-

mers in Indian languages especially in Bengali, 

a stemmer based on stemming cluster tech-

nique has been used as described in (Das and 

Bandyopadhyay, 2010). This stemmer analyz-

es prefixes and suffixes of all the word forms 

present in a particular document. Words that 

are identified to have the same root form are 

grouped in a finite number of clusters with the 

identified root word as cluster center.  

5.5 Syntactic Features 

5.5.1 Chunk Label 

We found that Chunk level information is very 

much effective to identify lexicon inference 

affinity. As an example: 

 

(������ ���	�)/NP (����
�� 
�����)/NP (����)/NP 

(������)/JJP (।)/SYM 
The movies released by Sa-

tyajit Roy are excellent. 

 

In the above example two lexicons 

“����/release” and “���/movie” are collo-
cated in a chunk and they are very much se-

mantically neighboring in human pragmatic 

knowledge. Chunk feature effectively used in 

supervised classifier. Chunk labels are defined 

as B-X (Beginning), I-X (Intermediate) and E-

X (End), where X is the chunk label. In the 

task of identification of Theme expressions, 

chunk label markers play a crucial role. Fur-

ther details of development of chunking sys-

tem could be found in (Das and Bandyopad-

hyay 2009).  

5.5.2 Dependency Parser 

Dependency depth feature is very useful to 

identify Theme expressions. A particular 

Theme word generally occurs within a particu-

lar range of depth in a dependency tree. Theme 

expressions may be a Named Entity (NE: per-

son, organization or location names), a com-

mon noun (Ex: accident, bomb blast, strike etc) 

or words of other POS categories. It has been 

observed that depending upon the nature of 

Theme expressions it can occur within a cer-

tain depth in the dependency tree in the sen-

tences. A statistical dependency parser has 

5



been used for Bengali as described in (Ghosh 

et al., 2009). 

5.6 Discourse Level Features 

5.6.1 Positional Aspect 

Depending upon the position of the thematic 

clue, every document is divided into a number 

of zones. The features considered for each 

document are Title words of the document, the 

first paragraph words and the words from the 

last two sentences. A detailed study was done 

on the Bengali news corpus to identify the 

roles of the positional aspect features of a doc-

ument (first paragraph, last two sentences) in 

the detection of theme words. The importance 

of these positional features has been described 

in the following section.  

5.6.2 Title Words 

It has been observed that the Title words of a 

document always carry some meaningful the-

matic information. The title word feature has 

been used as a binary feature during CRF 

based machine learning. 

5.6.3 First Paragraph Words 

People usually give a brief idea of their beliefs 

and speculations about any related topic or 

theme in the first paragraph of the document 

and subsequently elaborate or support their 

ideas with relevant reasoning or factual infor-

mation. Hence first paragraph words are in-

formative in the detection of Thematic Expres-

sions.  

5.6.4 Words From Last Two Sentences 

It is a general practice of writing style that 

every document concludes with a summary of 

the overall story expressed in the document. 

We found that it is very obvious that every 

document ended with dense theme/topic words 

in the last two sentences. 

5.6.5 Term Distribution Model 

An alternative to the classical TF-IDF weight-

ing mechanism of standard IR has been pro-

posed as a model for the distribution of a word. 

The model characterizes and captures the in-

formativeness of a word by measuring how 

regularly the word is distributed in a document. 

Thus the objective is to estimate  that measures 

the distribution pattern of the k occurrences of 

the word wi in a document d. Zipf's law de-
scribes distribution patterns of words in an en-

tire corpus. In contrast, term distribution mod-

els capture regularities of word occurrence in 

subunits of a corpus (e.g., documents, para-

graphs or chapters of a book). A good under-

standing of the distribution patterns is useful to 

assess the likelihood of occurrences of a theme 

word in some specific positions (e.g., first pa-

ragraph or last two sentences) of a unit of text. 

Most term distribution models try to character-

ize the informativeness of a word identified by 

inverse document frequency (IDF). In the 

present work, the distribution pattern of a word 

within a document formalizes the notion of 

theme inference informativeness. This is based 

on the Poisson distribution. Significant Theme 

words are identified using TF, Positional and 

Distribution factor. The distribution function 

for each theme word in a document is eva-

luated as follows: 

( )1 1
1 1

( ) / ( ) /
n n

d i i i i i

i i

f w S S n TW TW n− −
= =

= − + −∑ ∑  

where n=number of sentences in a document 

with a particular theme word Si=sentence id of 

the current sentence containing the theme word 

and Si-1=sentence id of the previous sentence 

containing the query term, 
iTW is the positional 

id of current Theme word and 
1iTW − is the posi-

tional id of the previous Theme word. 

5.6.6 Collocation 

Collocation with other thematic 

words/expressions is undoubtedly an important 

clue for identification of theme sequence pat-

terns in a document. As we used chunk level 

collocation to capture thematic words (as de-

scribed in 5.5.1) and in this section we are in-

troducing collocation feature as inter-chunk 

collocation or discourse level collocation with 

various granularity as sentence level, para-

graph level or discourse level.  

6 Theme Clustering 

Theme clustering algorithms partition a set of 

documents into finite number of topic based 

groups or clusters in terms of theme 

words/expressions. The task of document clus-

tering is to create a reasonable set of clusters 

6



for a given set of documents. A reasonable 

cluster is defined as the one that maximizes the 

within-cluster document similarity and mini-

mizes between-cluster similarities. There are 

two principal motivations for the use of this 

technique in the theme clustering setting: effi-

ciency, and the cluster hypothesis. 

The cluster hypothesis (Jardine and van 

Rijsbergen, 1971) takes this argument a step 

further by asserting that retrieval from a clus-

tered collection will not only be more efficient, 

but will in fact improve retrieval performance 

in terms of recall and precision. The basic no-

tion behind this hypothesis is that by separat-

ing documents according to topic, relevant 

documents will be found together in the same 

cluster, and non-relevant documents will be 

avoided since they will reside in clusters that 

are not used for retrieval. Despite the plausibil-

ity of this hypothesis, there is only mixed ex-

perimental support for it. Results vary consi-

derably based on the clustering algorithm and 

document collection in use (Willett, 1988). We 

employ the clustering hypothesis only to 

measure inter-document level thematic affinity 

inference on semantics. 

Application of the clustering technique to 

the three sample documents results in the fol-

lowing theme-by-document matrix, A, where 

the rows represent various documents and the 

columns represent the themes politics, sport, 

and travel.  

election cricket hotel

A parliament sachin vacation

governor soccer tourist

 
 =  
  

 

The similarity between vectors is calculated 

by assigning numerical weights to these words 

and then using the cosine similarity measure as 

specified in the following equation.  

, ,

1

, .
N

k j k j i k i j

i

s q d q d w w
→ → → →

=

 
= = × 

 
∑ ---- (1) 

This equation specifies what is known as the 

dot product between vectors.  Now, in general, 

the dot product between two vectors is not par-

ticularly useful as a similarity metric, since it is 

too sensitive to the absolute magnitudes of the 

various dimensions. However, the dot product 

between vectors that have been length norma-

lized has a useful and intuitive interpretation: it 

computes the cosine of the angle between the 

two vectors. When two documents are identic-

al they will receive a cosine of one; when they 

are orthogonal (share no common terms) they 

will receive a cosine of zero. Note that if for 

some reason the vectors are not stored in a 

normalized form, then the normalization can 

be incorporated directly into the similarity 

measure as follows.  

Of course, in situations where the document 

collection is relatively static, it makes sense to 

normalize the document vectors once and store 

them, rather than include the normalization in 

the similarity metric. 

, ,1

2 2

, ,1 1

,

N

i k i ji
k j

N N

i k i ki i

w w
s q d

w w

→ →
=

= =

× 
= 

  ×

∑

∑ ∑
 ----(2) 

Calculating the similarity measure and using 

a predefined threshold value, documents are 

classified using standard bottom-up soft clus-

tering k-means technique. The predefined thre-

shold value is experimentally set as 0.5 as 

shown in Table 4. 

 

ID Theme 1 2 3 

1 
��	
� 
(administration) 0.63 0.12 0.04 

1 
������ 
(good-government) 

0.58 0.11 0.06 

1 
���� 
(society) 

0.58 0.12 0.03 

1 
��� 
(law) 

0.55 0.14 0.08 

2 
������ 
(research) 

0.11 0.59 0.02 

2 
��� 
(college) 

0.15 0.55 0.01 

2 
����	 
(higher study) 

0.12 0.66 0.01 

3 
������ 
(jehadi) 

0.13 0.05 0.58 

3 
����� 
(mosque) 

0.05 0.01 0.86 

3 
�	����  
(New Delhi) 0.12 0.04 0.65 

3 
�	��� 
(Kashmir) 

0.03 0.01 0.93 

Table 4: Five cluster centroids (mean jµ
→

). 

A set of initial cluster centers is necessary in 

the beginning. Each document is assigned to 

the cluster whose center is closest to the doc-

ument. After all documents have been as-

signed, the center of each cluster is recom-

puted as the centroid or mean µ
→

 (where µ
→

 is 

7



the clustering coefficient) of its members that 

is ( )1/
j

j x c
c xµ

→ →

∈
= ∑ . The distance function 

is the cosine vector similarity function. 

Table 4 gives an example of theme centroids 

by the K-means clustering. Bold words in 

Theme column are cluster centers. Cluster cen-

ters are assigned by maximum clustering coef-

ficient. For each theme word, the cluster from 

Table 4 is still the dominating cluster. For ex-

ample, “��	
�” has a higher membership 
probability in cluster1 than in other clusters. 

But each theme word also has some non-zero 

membership in all other clusters. This is useful 

for assessing the strength of association be-

tween a theme word and a topic. Comparing 

two members of the cluster2, “�	���” and 
“��	����”, it is seen that “��	����” is strongly 
associated with cluster2 (p=0.65) but it has 

some affinity with other clusters as well (e.g., 

p =0.12 with the cluster1). This is a good ex-

ample of the utility of soft clustering. These 

non-zero values are still useful for calculating 

vertex weight during Semantic Relational 

Graph generation. 

7 Semantic Relational Graph 

Representation of input text document(s) in the 

form of graph is the key to our design prin-

ciple. The idea is to build a document graph 

G=<V,E> from a given source document 

d D∈ . At this preprocessing stage, text is 
tokenized, stop words are eliminated, and 

words are stemmed. Thus, the text in each 

document is split into fragments and each 

fragment is represented with a vector of consti-

tuent theme words. These text fragments be-

come the nodes V in the document graph. 

The similarity between two nodes is ex-

pressed as the weight of each edge E of the 

document graph. A weighted edge is added to 

the document graph between two nodes if they 

either correspond to adjacent text fragments in 

the text or are semantically related by theme 

words. The weight of an edge denotes the de-

gree of the semantic inference relationship. 

The weighted edges not only denote document 

level similarity between nodes but also inter 

document level similarity between nodes. Thus 

to build a document graph G, only the edges 

with edge weight greater than some predefined 

threshold value are added to G, which basical-

ly constitute edges E of the graph G. 

The Cosine similarity measure has been 

used here. In cosine similarity, each document 

d is denoted by the vector ( )V d
→

 derived from 

d, with each component in the vector for each 

Theme words. The cosine similarity between 

two documents (nodes) d1 and d2 is computed 

using their vector representations ( 1)V d
→

and 

( 2)V d
→

as equation (1) and (2) (Described in 

Section 6). Only a slight change has been done 

i.e. the dot product of two vec-

tors ( 1) ( 2)V d V d
→ →

• is defined as
1

( 1) ( 2)
M

i

V d V d
=

∑ . 

The Euclidean length of d is defined to 

be
2

1

( )
M

ii

d
V=
→

∑  where M is the total number of 

documents in the corpus. Theme nodes within 

a cluster are connected by vertex, weight is 

calculated by clustering co-efficient of those 

theme nodes. Additionally inter cluster vertex-

es are there. Cluster centers are interconnected 

with weighted vertex. The weight is calculated 

by cluster distance as measured by cosine simi-

larity measure as discussed earlier. 

To better aid our understanding of the auto-

matically determined category relationships we 

visualized this network using the Fruchterman-

Reingold force directed graph layout algorithm 

(Fruchterman and Reingold, 1991) and the 

NodeXL network analysis tool (Smith et al., 

2009)
4
. A theme relational model graph drawn 

by NoddeXL is shown in Figure 1. 

8 Semantic Distance Measurement 

Finally generated semantic relational graph is 

the desired SemanticNet that we proposed. 

Generated Bengali SemanticNet consist of al-

most 90K high frequent Bengali lexicons. Only 

four categories of POS (noun, adjective, ad-

verb and verb) considered for present genera-

tion as reported in Section 5.2.1. In the gener-

ated Bengali SemanticNet all the lexicons are 

connected with weighted vertex either directly 

                                                 
4
 Available from 

http://www.codeplex.com/NodeXL 

8



 

Figure 1: Semantic Relational Graph by NodeXL 

or indirectly. Semantic lexicon inference could 

be identified by network distance of any two 

nodes by calculating the distance in terms of 

weighted vertex. We computed the relevance 

of semantic lexicon nodes by summing up the 

edge scores of those edges connecting the node 

with other nodes in the same cluster. As cluster 

centers are also interconnected with weighted 

vertex so inter-cluster relations could be also 

calculated in terms of weighted network dis-

tance between two nodes within two separate 

clusters. As an example: 

 
Figure 2: Semantic Affinity Graph 

The lexicon semantic affinity inference from 

Figure 2 could be calculated as follows: 

0

0

0
0

( , )              ----(1) or

                 =  ---(2)

n

kk
d i j

n
m

m kk
cc

c

v
S w w

k

v
l

k

=

=

=
=

=

×

∑

∑
∑ ∏

 

where ( , )
d i j

S w w =  semantic affinity dis-

tance between two lexicons wi and wj. Equa-

tion (1) and (2) are for intra-cluster and inter-

cluster semantic distance measure respectively. 

k=number of weighted vertex between two 

lexicons wi and wj. vk is the weighted vertex 

between two lexicons. m=number of cluster 

centers between two lexicons. lc is the distance 

between cluster centers between two lexicons. 

For illustration of present technique let take 

an example: 

(Argentina, goal)=
0.5 0.3

0.4
2

+
=  

(Gun, goal)=
0.22 0.5

0.0
1 1

 
+ × 

 
=0 

It is evident from the previous example that 

the score based semantic distance can better 

illustrate lexicon affinity between Argentina 

and goal but is no lexicon affinity relation be-

tween gun and goal. 

Instead of giving only certain semantic rela-

tions like WordNet or ConceptNet the present 

relative probabilistic score based lexicon affin-

ity distance based technique can represent best 

acceptable solution for representing the human 

pragmatic knowledge. Not only ideologically 

rather the SemanticNet provide a good solution 

to any type of NLP problem. A detail analysis 

of Information retrieval system using Seman-

ticNet is detailed in evaluation section.  

Although every lexicon pair cannot be la-

beled by exact semantic role but we try to keep 

a few semantic roles to establish a crossroad 

from previous computational lexicon tech-

niques to this new one. These semantic rela-

tions may be treated as a bridge to traverse 

SemanticNet by gathering knowledge from 

other resources WordNet and ConceptNet. 

Approximately 22k (24% of overall Seman-

ticNet) lexicons are tagged with appropriate 

semantic roles by two processes as described 

below. 

9



9 Semantic Role Assignment 

Two types of methods have been taken to as-

sign pair wise lexicon semantic affinity rela-

tions. First one is derived from ConceptNet. In 

the second technique sub-graph is identified 

consisting of a nearest verb and roles are as-

signed accordingly.  

9.1 Semantic Roles from ConceptNet 

A ConceptNet API
5
 written in Java has been 

used to extract pair wise relation from Con-

ceptNet. A Bengali-English dictionary (ap-

proximately 102119 entries) has been devel-

oped using the Samsad Bengali-English dictio-

nary
6
 used here for equivalent lookup of Eng-

lish meaning of each Bengali lexicon. Ob-

tained semantic relations from ConceptNet for 

any lexicon English pair are assigned to source 

Bengali pair lexicons. As an example: 

(“Tree”,”Gree”) (“��!”,”����”) 
 

� OftenNear 
� PartOf 
� PropertyOf 
� IsA 

9.2 Verb Sub-Graph Identification 

It is an automatic process using manually 

augmented list of only 220 Bengali verbs. This 

process starts from any arbitrary node of any 

cluster and start finding any nearest verb with-

in the cluster. The system uses the manually 

augmented list of verbs as partly reported in 

Table 5. 

Verb English Gloss Probable Relations 

�	 Be IsA 
��! Have CapableOf 
"��� Have CapableOf 
#��� Made MadeOf 
����� Live LocationOf 

Table 5: Semantic Relations 

The semantic relation labels attached with 

every verb in the manually augmented list (as 

reported in Table 5) is then automatically as-

signed between each pair of lexicons.  

                                                 
5
 http://web.media.mit.edu/~hugo/conceptnet/ 

6
 

http://dsal.uchicago.edu/dictionaries/biswas_bengal

i/ 

10 Evaluation 

It is bit difficult to evaluate this type of lexicon 

resources automatically. Manual validation 

may be suggested as a better alternative but we 

prefer for a practical implementation based 

evaluation strategy.  

For evaluation of Bengali SemanticNet it is 

used in Information Retrieval task using cor-

pus from Forum for Information Retrieval 

Evaluation (FIRE)
7
 ad-hoc mono-lingual in-

formation retrieval task for Bengali language. 

Two different strategies have been taken. First 

a standard IR technique with TF-IDF, zonal 

indexing and ranking based technique (Ban-

dyopadhyay et al., 2008) has been taken. 

Second technique uses more or less same strat-

egy along with query expansion technique us-

ing SemanticNet (Although the term Seman-

ticNet was not mentioned there) as a resource 

(Bhaskar et al., 2010).  

Only the following evaluation metrics have 

been listed for each run: mean average preci-

sion (MAP), Geometric Mean Average Preci-

sion (GM-AP), (document retrieved relevant 

for the topic) R-Precision (R-Prec), Binary pre-

ferences (Bpref) and Reciprical rank of top 

relevant document (Recip_Rank). The evalua-

tion strategy follows the global standard as 

Text Retrieval Conference (TREC)
8
 metrics. It 

is clearly evident from the system results as 

reported in Table 6 that SemanticNet is a better 

way to solve lexicon semantic affinity.  

Scores 
Bengali IR using 

IR SemanticNet 

MAP 0.0200 0.4002 

GM_AP 0.0004 0.3185 

R-Prec 0.0415 0.3894 

Bpref 0.0583 0.3424 

Recip_Rank 0.4432 0.6912 

Table 6: Information Retrieval using Seman-

ticNet 

Evaluation result shows effectiveness of de-

veloped SemanticNet in IR. Further analysis 

                                                 
7
 http://www.isical.ac.in/~clia/index.html 

8
 http://trec.nist.gov/ 

10



revealed that general query expansion tech-

nique generally used WordNet synonyms as a 

resource. But in reality “$�	” and “প���” 
could not be clustered in one cluster though 

they represent same semantic of ‘heart’. First 

one used in general context whereas the second 

one used only in literature. If there is any 

problem to understand Bengali let come with 

an example of English. Conceptually "you" 

and "thy" could be mapped in same cluster as 

they both represent the semantic of 2
nd

 person 

but in reality "thy" simply refers to the 

literature of the great English poet Shakes-

peare. Standard lexicons cannot discriminate 

this type of fine-grained semantic differences. 

11 Conclusion and Future Task 

Experimental result of Information Retrieval 

using SemanticNet proves it is a better solution 

rather than any existing lexicon resources. The 

development strategy employs less human in-

terruption rather a general architecture of 

Theme identification or Theme Clustering 

technique using easily extractable linguistics 

knowledge. The proposed technique could be 

replicated for any new language.  

SemanticNet could be useful any kind of In-

formation Retrieval technique, Information 

Extraction technique, and topic based Summa-

rization and we hope for newly identified NLP 

sub disciplines such as Stylometry or Author-

ship detection and plagiarism detection etc. 

Our future task will be in the direction of 

different experiments of NLP as mentioned 

above to profoundly establish the efficiency of 

SemanticNet. Furthermore we will try to de-

velop SemanticNet for many other languages. 

References 

Bandhyopadhyay S., Das A., Bhaskar P.. English 

Bengali Ad-hoc Monolingual Information Re-

trieval Task Result at FIRE 2008. In Working 

Note of Forum for FIRE-2008. 

Bhaskar P., Das A.,Pakray P.and Bandyopadhyay 

S.(2010). Theme Based English and Bengali Ad-

hoc Monolingual Information Retrieval in FIRE 

2010, In FIRE-2010. 

Das A. and Bandyopadhyay S. (2009). Theme De-

tection an Exploration of Opinion Subjectivity. 

In Proceeding of Affective Computing & Intelli-

gent Interaction (ACII). 

Das A. and Bandyopadhyay S. (2010). Morpholog-

ical Stemming Cluster Identification for Bangla, 

In Knowledge Sharing Event-1: Task 3: Mor-

phological Analyzers and Generators, January, 

2010, Mysore. 

Ekbal A., Bandyopadhyay S (2008). A Web-based 

Bengali News Corpus for Named Entity Recog-

nition. Language Resources and Evaluation 

Journal. pages 173-182, 2008 

Fillmore Charles J., Johnson Christopher R., and 

Petruck Miriam R. L.. 2003. Background to 

FrameNet. International Journal of Lexicogra-

phy, 16:235–250. 

Fruchterman Thomas M. J. and  Reingold Edward 

M.(1991). Graph drawing by force-directed 

placement. Software: Practice and Experience, 

21(11):1129–1164. 

Ghosh A., Das A., Bhaskar P., Bandyopadhyay 

S.(2009). Dependency Parser for Bengali: the JU 

System at ICON 2009. In NLP Tool Contest 

ICON 2009, December 14th-17th, Hyderabad. 

Jardine, N. and van Rijsbergen, C. J. (1971). The 

use of hierarchic clustering in information re-

trieval. Information Storage and Retrieval, 7, 

217-240. 

Kipper Karin, Korhonen Anna, Ryant Neville, and 

Palmer Martha. Extending VerbNet with Novel 

Verb Classes. LREC 2006.  

Liu Hugo and Singh Push (2004). ConceptNet: a 

practical commonsense reasoning toolkit. BT 

Technology Journal, 22(4):211-226. 

Palmer Martha, Gildea Dan, Kingsbury Paul, The 

Proposition Bank: A Corpus Annotated with 

Semantic Roles, Computational Linguistics 

Journal, 31:1, 2005. 

Singh Push and Williams William (2003). LifeNet: 

a propositional model of ordinary human activi-

ty. In the Proc. Of DC-KCAP 2003.  

Singh Push, Barry Barbara, and Liu Hugo (2004). 

Teaching machines about everyday life. BT 

Technology Journal, 22(4):227-240. 

Smith Marc, Ben Shneiderman, Natasa Milic-

Frayling, Eduarda Mendes Rodrigues, Vladimir 

Barash, Cody Dunne, Tony Capone, Adam Per-

er, and Eric Gleave. 2009. Analyzing (social 

media) networks with NodeXL. In C&T ’09: 

Proc. Fourth International Conference on Com-

munities and Technologies, LNCS.  Springer. 

Willerr, P. (1988). Recent trends in hierarchic doc-

ument clustering: A critical review. Information 

Processing and Management, 24(5), 577-597. 

11


