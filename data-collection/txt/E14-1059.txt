



















































Discriminating Rhetorical Analogies in Social Media


Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 560‚Äì568,
Gothenburg, Sweden, April 26-30 2014. c¬©2014 Association for Computational Linguistics

 Discriminating Rhetorical Analogies in Social Media 

 

Christoph Lofi Christian Nieke Nigel Collier 
National Inst. of Informatics TU Braunschweig National Inst. of Informatics 

2-1-2 Hitotsubashi, 

Chiyoda-ku, Tokyo 

101-8430, Japan 

M√ºhlenpfordtstr. 23 

38106 Braunschweig 

Germany  

2-1-2 Hitotsubashi, 

Chiyoda-ku, Tokyo  

101-8430, Japan 

lofi@nii.ac.jp nieke@ifis.cs.tu-bs.de       collier@nii.ac.jp 

Abstract 

Analogies are considered to be one of the core 
concepts of human cognition and communica-

tion, and are very efficient at encoding com-

plex information in a natural fashion. How-

ever, computational approaches towards large-

scale analysis of the semantics of analogies are 

hampered by the lack of suitable corpora with 

real-life example of analogies. In this paper we 

therefore propose a workflow for discriminat-

ing and extracting natural-language analogy 

statements from the Web, focusing on analo-

gies between locations mined from travel re-

ports, blogs, and the Social Web. For realizing 
this goal, we employ feature-rich supervised 

learning models which we extensively evalu-

ate. We also showcase a crowd-supported 

workflow for building a suitable Gold dataset 

used for this purpose. The resulting system is 

able to successfully learn to identify analogies 

to a high degree of accuracy (F-Score 0.9) by 

using a high-dimensional subsequence feature 

space. 

1 Introduction 

Analogies are one of the core concepts of human 

cognition (Hofstadter, 2001), and it has been sug-

gested that analogical inference is  the ‚Äúthing that 
makes us smart‚Äù (Gentner, 2003). An analogy can 

be seen as a pattern of speech leading to a cogni-

tive process that transfers some high-level mean-
ing from one particular subject (often called the 

analogue or the source) to another subject, usually 

called the target. When using analogies, one em-
phasizes that the ‚Äúessence‚Äù of source and target is 

similar, i.e. their most discriminating and proto-

typical processes and properties are perceived in a 

similar way. 
The nature of analogies has been discussed and 

studied since the ancient Greeks, however compu-

tational approaches are still rather limited and in 

their infancy. One reason for this is that text cor-

pora containing analogies are crucial to study the 
syntactic and semantic patterns of analogies in or-

der to make progress on automated understanding 

techniques. For example, to learn about their dis-

tribution and the attribute-value pairs that are 
compared. However, to the best of our knowledge, 

no such corpus is freely available. We will there-

fore in this paper present a method for creating 
such a corpus in an efficient fashion, and make our 

corpus available for further research efforts. 

As an example, consider this brief statement: 

‚ÄúWest Shinjuku (a Tokyo district) is like Lower Manhat-

tan‚Äù It allows readers who know New York, but 

not Tokyo, to infer some of the more significant 
properties of the unknown district (e.g., it is an im-

portant business district, hosts the headquarters of 

many companies, features many skyscrapers, 
etc.). However, automatically understanding anal-

ogies is surprisingly hard due to the extensive do-

main knowledge required in order to perform ana-

logical reasoning. For example, an analogy repos-
itory containing such domain knowledge has to 

provide information on which attributes of source 

and target are generally considered comparable. In 
contrast to Linked Open Data or typical ontolo-

gies, such analogical knowledge is consensual, i.e. 

there is no undisputable truth to analogical infor-

mation, but a statement can be considered ‚Äúgood‚Äù 
analogical knowledge if its‚Äô semantics are per-

ceived similarly by enough people (Lofi & Nieke, 

2013). For example, while many properties of 
West Shinjuku and Lower Manhattan are dissimi-

lar, nonetheless most people will immediately rec-

ognize dominant similarities. 
In order to build an analogy repositories, a large 

number of actual analogy statements reflecting the 

diversity of people‚Äôs opinions are required for 

analysis. In this paper, we make a start on this task 
by proposing a workflow for reliably extracting 

such statements by using feature-rich supervised 

560



learning models, and demonstrate its effectiveness 

for analogies between different places. Our contri-

butions in this paper are as follows: 

ÔÇ∑ First, we build a suitable Gold corpus for train-
ing and testing supervised learning models, fo-
cusing on analogies between places. This cor-

pus will be based upon content mined from 

search engines and social media.  

ÔÇ∑ We show the effectiveness, but also the chal-
lenges of crowd-sourcing as a technique for 

screening and refining potential Gold corpus 

documents. This process results in multi-sen-
tence text snippets containing an analogy ex-

tracted from these documents.  

ÔÇ∑ We design and evaluate supervised learning 
models with rich feature sets to recognize anal-

ogy statements automatically, allowing us to 
substitute crowd-sourcing with automated 

techniques for further expanding the corpus.  

ÔÇ∑ We extensively evaluate our models, and dis-
cuss their strengths and shortcomings. 

2 Processing Analogies 

There exist several approaches for modeling and 

capturing the semantics of analogies, among them 
many formal ones relying, for example, on struc-

tural mapping (Gentner, 1983). These types of ap-

proaches aim at mapping characteristics and rela-
tionships of source and target, usually relying on 

factual domain knowledge given in propositional 

networks. One example typically used in this con-

text is the Rutherford analogy ‚ÄúAtoms are like the So-

lar System‚Äù, which can be derived by outlining sim-

ilarities between the nucleus and the sun, which 
are both heavy masses in the center of their respec-

tive system, and electrons and planets, which re-

volve around the center attracted by a strong force 
(here, the coulomb force is analog to the gravita-

tional force). This model resulted in several theo-

retical computational models (e.g. (Gentner & 

Gunn, 2001)).  
The most extensively researched subset of analo-

gies are 4-term analogies between two word pairs 

(mason, stone)::(carpenter, wood). Here, processing 

analogies boils down to measuring the relational 
similarity of the word pairs, i.e. a mason works 

with stone as a carpenter works with wood.  

However, measuring the similarity between enti-

ties or relationships is a difficult task. While most 
structure-mapping approaches rely on processing 

facts, e.g. as extracted from ontologies or 

knowledge networks, supporters of perceptual 
analogies claim that this similarity has to be meas-

ured on a high perceptional level (Chalmers, 

French, & Hofstadter, 1992), i.e. there can be an 

analogy if people perceive or believe relations or 

properties to be as similar even if there are no hard 

facts supporting it (Kant, 1790), or even when 

facts oppose it. More formally, two entities ùê¥ and 
ùêµ can be seen as being analogous (written as ùê¥ ‚à∑
ùêµ) when their relevant relationships and properties 
are perceived sufficiently similar (Lofi & Nieke, 

2013). This type of consensual analogy is of high 

relevance in natural communication (in fact, most 

analogies we discovered in our data are of this 
type), but very hard to learn as there are no corpora 

for studying analogies readily available. Further-

more, this definition opens up other challenges: 
What are the relevant characteristics between two 

entities? When are they perceived as being simi-

lar? And when does an analogy hold true?  
With this work, we aim at paving the way for fu-

ture research on this challenging set of problems 

by providing a workflow for mining analogy ex-

amples from the Web and Social Media. To illus-
trate this, consider the following example ex-

tracted from our Gold corpus:   

‚ÄúTokyo, like Disneyland, is sterile. It‚Äôs too clean and 
really safe, which are admirable traits, but also unre-
alistic. Tokyo is like a bubble where people can live 
their lives in a very naive and enchanted way because 
real problems do not exist.‚Äù  

(No. 5310 in corpus) 

This perceptual analogy between Tokyo and Dis-

neyland is hard to explain when only relying on 

typical structured knowledge like Linked Open 
Data or ontologies, and thus requires specialized 

data repositories which can be built up using real-

world examples as provided by our approach.   

Unfortunately, actually detecting the use of an 
analogies in natural text, a requirement for build-

ing sufficiently large test corpora, is not an easy 

task, as there are only subtle syntactic and mor-
phological differences between an analogy and a 

simple comparison. These differences cannot be 

grasped by simple classification models. For ex-

ample, while many rhetorical analogies contain 

phrases as ‚Äúis like‚Äù or ‚Äú, like‚Äù, as for example in 

‚ÄúWest Shinjuku is like Lower Manhattan‚Äù or ‚ÄúTokyo is 

like Disneyland as it is very sterile‚Äù there is a plethora 

of very similar sentences which do not express an 

analogy (‚ÄúShinjuku is like this: ‚Ä¶‚Äù or ‚ÄúTokyo, like the 

rest of Japan, ‚Ä¶‚Äù). These subtle differences, which 

are hard to grasp with handcrafted patterns and are 

often found in the surrounding context, can be 

modeled by our approach as outlined in section 5. 

561



3 Related Work 

There exist several works on the semantics of 

analogies from a cognitive, philosophical, or lin-

guistic perspective, such as (Dedre Gentner, Keith 
J. Holyoak, & Boicho N. Kokinov, 2001), 

(Itkonen, 2005), or (Shelley, 2003).   

Hearst-like patterns (Hearst, 1992), which we use 
as a first and very crude filter during the construc-

tion of the Gold dataset, have frequently been em-

ployed in recent years, especially in the area of ex-

tracting hyponyms, e.g., (Snow, Jurafsky, & Ng, 
2004) which also aims at learning new extraction 

patterns based on word dependency trees. But also 

approaches for dealing with analogies are fre-
quently based on patterns applied to text corpora. 

Most of these approaches are tailored for solving 

general analogy challenges given in a 4-term mul-

tiple-choice format, and are usually evaluated on 
the US-based SAT challenge dataset (part of the 

standardized aptitude test for college admission). 

SAT challenges are in 4-term analogy form, e.g. 

‚Äúostrich is to bird AS a) cub is to bear OR b) lion is to cat‚Äù, 

and the focus of those approaches is on heuristi-

cally assessing similarity of two given words 

pairs, to find the statistically more plausible an-
swer. For example, (Bollegala, Matsuo, & 

Ishizuka, 2009), (Nakov & Hearst, 2008), or 

(Turney, 2008) approach this challenge by using 

pattern-based Web search and subsequent analysis 
of the resulting snippets. In contrast to these ap-

proaches, we do not focus on word pair similarity, 

but given one entity, we aim at finding other enti-
ties which are seen as analogous in a specific do-

main (in our case analogies between locations and 

places). Being focused on a special domain often 

renders approaches relying on thesauri like Word-
Net or CoreLex unusable, as many of the words 

relevant to the domain are simply not contained. 

Closely related to analogy processing is the detec-
tion of metaphors or metonyms, which are a spe-

cial form of analogy. Simplified, a metaphor is an 

analogy between two entities with the additional 
semantics that one entity can substitute the other 

and vice versa). While early approaches to meta-

phor identification relied on hand-crafted patterns 

(Wilks, 1978), newer ones therefore heavily ex-
ploit the interchangeability of the entities (Beust, 

Ferrari, & Perlerin, 2003) or (Shutova, 2010), and 

cannot be used for general analogy processing 
without extensive adoption. These approaches of-

ten also rely on some reasoning techniques based 

on thesauri, but also other approaches based on 

                                                   
1 http://data.l3s.de/dataset/analogy-text-snippets 

mining and corpus analysis became popular. For 

example in (Shutova, Sun, & Korhonen, 2010) a 

system is presented which, starting from a small 

seed set of manually annotated metaphorical ex-
pressions, is capable of harvesting a large number 

of metaphors of similar syntactic structure from a 

corpus. 
Detecting analogies also has some similarities 

with relation extraction, e.g. (Bunescu & Mooney, 

2006) using Subsequence Kernels. However, the 
task is slightly more difficult than simply mining 

for a ‚Äúsimilar_to‚Äù relation, which is addressed by 

our approach in section 5. 

4 Building the Gold Dataset 

As the goal of this paper is to supply the tools for 

creating a large corpus of analogies from the Web, 

we require a reliable mechanism for automatically 

classifying if a text snippet contains an analogy or 
not. Such classification requires a Gold dataset 

which we construct in this section and which we 

make available to the community for download1. 
As we expect the number of analogies in a com-

pletely random collection of web documents to be 

extremely low, we first start by collecting a set of 
web documents that are likely to contain an anal-

ogy by applying some easy-to-implement but ra-

ther coarse techniques as follows: 

In order to obtain a varied set of text snippets (i.e. 
short excerpts from larger Web documents), we 

first used a Web search engine (Google Search 

API) with simple Hearst-like patterns for crawling 
potentially relevant websites. These patterns were 

selected manually based on analysis of sample 

Web data by three experts. In contrast to other ap-
proaches relying on extraction patters, e.g. 

(Turney, 2008) or (Bollegala et al., 2009), our pat-

terns are semi-open, e.g. ‚Äú# * similar to * as‚Äù, where 

# is replaced by one of 19 major cities we used for 

corpus extraction. * is a wildcard, therefore only 
one entity of the analogy is fixed by the pattern. 

Each pattern is created by combining one base part 

(in this case, ‚Äú# * similar to *‚Äù) with an extension 
part (‚Äúas‚Äù). We used 17 different base parts, and 14 
different extensions, resulting in 238 different ex-

traction patterns before inserting the city names. 

Using Web search, we initially obtained 109,121 
search results and used them to crawl 22,360 doc-

uments, for which we extracted the text snippets 

surrounding the occurrence of the pattern (2 pre-

ceding and 2 succeeding sentences). The intention 
of our open Hearst-like patterns is to obtain a wide 

562



variety of text snippets which are not limited to 

simple analogy cases, so most snippets obtained 

will actually not be analogies at all. Therefore, ad-

ditional filtering is required to find those which do 
actually contain an analogy between places. Un-

like e.g. (Turney, 2008) where patterns of the form 

‚Äú[0..1] X [0..3] Y [0..1]‚Äù, with X and Y two given en-

tities, are used, we chose a more general approach 
and filtered out all snippets not containing at least 

two different locations (and hence no place anal-

ogy, locations provided by Stanford CoreNLP 

NER tagger), which left 14,141 snippets.  
Since we lacked the means to manually classify all 

of these snippets as a Gold set, we randomly se-

lected a subset of 8000 snippets, and performed a 
crowd-sourcing based filtering to detect potential 

analogies, as described in the following.  

 Crowd-Sourcing-Based Filtering 

Under certain circumstances, crowd-sourcing can 
be very effective for handling large tasks requiring 

human intelligence without relying on expensive 

experts. In contrast to using expert annotators, 

crowd-workers are readily and cheaply available 
even for ad-hoc tasks. In this paper, we used mi-

cro-task crowd-sourcing, i.e. a central platform 

like for example Amazon Mechanical Turk2  or 
CrowdFlower3 assigns small tasks (called HITs, 

human-intelligence tasks) to workers for monetary 

compensation. HITs usually consist of multiple 

work units taking only a few minutes to process, 
and therefore pay few cents.   

Crowd-sourcing has been shown to be effective 

for language processing related tasks, e.g. in 
(Snow, O‚ÄôConnor, Jurafsky, & Ng, 2008)  it was 

used to annotate text corpora, and the authors 

found that for this task, the combination of three 
crowd judgments roughly provides the quality of 

one expert worker. However, the quality can vary 

due to potential incompetence and maliciousness 

of workers, making quality control mandatory. 
The two basic tools for quality control in crowd-

sourcing are majority votes and Gold units, which 

are both used in our process. Gold units are tasks 
for which the correct answer is known, and they 

are transparently mixed into normal HITs distrib-

uted to workers. If workers repeatedly provide an 
incorrect judgment for gold units, they are consid-

ered malicious, are not paid, and their judgments 

are excluded from the results.  

Therefore, we continued to classify the selected 
8,000 snippets using 90 gold units. 5 snippets are 

grouped within each HIT, for which we pay USD 

                                                   
2 https://www.mturk.com/  

$0.04. For each snippet, 3 judgments are elicited. 

In total, 336 workers participated in categorizing 

87 snippets on average (some top contributors cat-

egorized up to 1,975 snippets). As a result 895 
snippets are classified as containing an analogy 

with a confidence of over 90% (confidence is 

computed as a weighted majority vote of worker 
judgments and worker reliability; with worker re-

liability resulting from workers failing or passing 

gold units in previous tasks).  
A brief manual inspection showed that these re-

sults cannot be trusted blindly (a correctness of 

78% compared to an expert judgment was meas-

ured in a small sample), so we performed an expert 
inspection on all potential analogy snippets, revis-

ing the crowd judgments where necessary. Fur-

thermore, we manually tagged the names of the 
analogous locations. This resulted in 542 snippets 

which are now manually judged as analogies and 

353 snippets that were manually judged as not be-
ing an analogy. For this task, worker performance 

is extremely asymmetrical as it is much easier for 

crowd-workers to reach an agreement for negative 

examples than for positive ones, and there were 
3,023 snippets classified as no analogies with 

100% confidence. This intuition was supported by 

a short evaluation in which we sampled 314 
(10.3%) random snippets from this set and found 

none that had been misclassified. Therefore, the 

negative examples of our Gold set consist of the 

snippets manually re-classified by our expert an-
notators, and the snippets which had been classi-

fied with 100% confidence by the crowd-workers. 

This leaves out 4,082 snippets for which no clear 
consensus could be reached, and which are thus 

excluded from the Gold set.  

5 Classifiers and Feature Extraction 

Using crowd-sourcing for finding analogy state-

ments is a tedious and still quite expensive task. 

Therefore, we aim at automating the processes of 
detecting analogies in a given text snippet by de-

signing multiple rich feature sets for machine 

learning-based classification models, allowing us 
to discover new analogies quicker and cheaper. 

 Dataset Description 

Our complete Gold dataset of 3,918 text snippets 

shows a ratio of positive to negative examples of 

roughly 1:8. For training and evaluation, we per-
form four stratified random selections on the Gold 

set to obtain 4 training sets with 2/3 of the overall 

3 http://crowdflower.com/ 

563



size (2,611), and respective test sets with 1/3 size 

(1,307). In each set, the original ratio between pos-

itive example (analogies) and negative examples 

(not analogies) is retained. We prefer this ap-
proach over n-fold cross-validation as some of our 

models are expensive to train.  

All snippets in the Gold set consist of 5 sentences, 
with 105 words per snippet on average. This aver-

age does not significantly vary between positively 

and negatively classified snippets (94 vs. 106). 
The overall vocabulary contains 31,878 unique 

words, with 6,960 words in the positive and 

30,234 in the negative subset. 5,316 of these 

words are shared between both sets (76% of those 
in the Gold set). This observation implies that the 

language in our snippets is highly varied and far 

from saturated (for the significantly smaller posi-
tive set, 12.84 new words per snippet are added to 

the vocabulary on average, while for the larger 

negative subset, this value only drops to 8.95). 
This situation looks similar for locations, which 

play a central role in this classification task: the 

overall number of different locations encountered 

in all snippets is 2,631, with 0.86 new locations 
per snippet in the positive set and 0.73 in the neg-

ative set. On average, there are 3.18 locations 

mentioned in a given snippet, again with no sig-
nificant differences in the positive and negative 

subset (3.67 vs. 3.10). Please refer to Table 1 for 

exhaustive statistics.  

 Unigram (Bag-of-Word) Feature Model 

As our evaluation baseline, we use a straight-for-
ward unigram (bag-of-word) feature model for 

training a support vector machine. No stop words 

are removed, and the feature vectors are normal-
ized to the average length of training snippets. 

Furthermore, we only retain the 5000 most fre-

quent features, and skip any which occur only in a 
single snippet. For this experiments (and all other 

later experiments using a SVM), we used the 

LibSVM implementation (Chang & Lin, 2011) 

with a linear kernel due to the size of the feature 
space. 

  N-Gram-based Feature Model  

Our first approach to increasing classification 

quality of the baseline is expanding the feature 
space to also include n-grams. We tested different 

versions of this model with lexical word-level n-

grams, part-of-speech n-grams, and both of them 

simultaneously. In all cases, we include n-grams 
with a length of 1 to 4 words, and similar to the 

                                                   
4 http://nlp.stanford.edu/software/corenlp.shtml 

baseline, the top-5000 features are retained and 

values are normalized to the training snippet 

length, with a minimal frequency of 2. The re-
quired part-of-speech labels are obtained by using 

the Stanford CoreNLP library4.The three resulting 

feature models have been trained and evaluated 

with three classification algorithms which are 
known to provide good performance in similar 

classification tasks: a support vector machine clas-

sifier (as in 5.2), a Na√Øve Bayes classifier (from the 
Weka library5), and Weka‚Äôs J48 implementation 

of the C4.5 classifier (Quinlan, 1993) (with prun-

ing confidence 0.25 and min. leaf distance 2). 

 Shortest Path Feature Model 

In this subsection we design the Shortest Path fea-

ture model, a model aiming at exploiting some of 

the specific properties of place analogies. By def-

inition, only text snippets featuring two different 
places can be a place analogy. The Shortest Path 

model furthermore assumes that both these loca-

tions occur in a single sentence (which is tested in 
6.3), and that there is a meaningful lexical or 

grammatical dependency between these occur-

rences. For actually building our feature space, we 

rely on typed dependency parses (Marneffe, 
MacCartney, & Manning, 2006) of the snippets, 

and extract the shortest path in the resulting de-

pendency tree between both locations (also using 
Stanford CoreNLP). This path represents the col-

lapsed and propagated dependencies between both 

locations, i.e. basic tokens as ‚Äúon‚Äù or ‚Äúby‚Äù are inte-

grated in the edge labels and don‚Äôt appear as 

nodes. We considered three variations of this ap-
proach: paths built using lexical labels, path with 

part-of-speech labels, and a combination of both. 

During the construction of our Gold set, we man-
ually annotated the two relevant places for all 

analogies. Therefore this approach can be applied 

5 http://www.cs.waikato.ac.nz/ml/weka/ 

Table 1: Characteristics of Gold Data 

characteristic all  positive negative 

# of snippets 3,918 542 3,376 

# of snippets in 

training set 

2,611 361 2,250 

# of snippets in test 
set 

1,307 181 1,126 

vocabulary size 31,878 6,960 30,234 

voc. / #snippets 8.14 12.84 8.95 

location   
vocabulary size 

2,631 468 2,459 

loc.voc. / #snipts. 0.67 0.86 0.73 

# words / s.       + 105 94 106 

# locations / s. 3.18 3.67 3.10 
+ #/s.: average count per snippet  

564



directly for positive training examples. However, 

for negative snippets, no relevant locations have 

been annotated. Hence, for all negative snippets in 

training and all snippets in the test set, we assume 
that all locations which appear in a snippet (as de-

termined by a NER tagger) are relevant, and we 

extract all shortest paths between any of them. On 
average this results in 5.6 paths extracted from any 

given snippet. The extracted paths are generalized 

by replacing the locations with a generic, and the 
final feature model results from constructing a bi-

nary feature representing whether a given path oc-

curs or not. 

As with the n-gram-based feature model, we train 
and evaluate SVM, Na√Øve Bayes, and J48 classifi-

ers with our feature vector (parameters as in 5.3). 

Please note that building this model is computa-
tionally significantly more expensive than the n-

gram-based approach as it requires named entity 

recognition, and typed dependency parsing (we re-
quired roughly 30 minutes per training / test set on 

our Intel i7 laptop). 

 Subsequence Pattern Feature Model 

Basically, this approach aims at creating some-

thing similar to the most common sub forests of 
all snippets, or skip-grams (Guthrie, Allison, Liu, 

Guthrie, & Wilks, 2006), i.e. results can be seen as 

a hybrid between ‚Äútree patterns‚Äù (as e.g. the Short-
est Path) and n-grams. The intention is to avoid the 

problem of overly local patterns, allowing the pat-

terns to work even in the presence of fill words and 

subsequences added to a sentence. For this, we uti-
lize the PrefixSpan algorithm (Pei et al., 2001) to 

detect common, reappearing subsequence in the 

training set, i.e. sequences of words that appear in 
a given order, ignoring anything in-between. In 

contrast to the shortest path approach, this model 

focuses on multiple sentences simultaneously, and 
therefore is a significant contribution over state-

of-the-art techniques. 

As before, we used lexical, part-of-speech, and 

combined features. The general idea of this ap-
proach is to use the PrefixSpan algorithm to mine 

subsequence patterns from positive gold snippets 

(the primitives), and use these as binary features in 
a classification step, for which we trained three 

classifiers as described in 5.3.  

In case of the lexical labels, we use the PrefixSpan 

algorithm to return all subsequences that appear at 
least 10 times (this value is dependent on charac-

teristics of the dataset and has to be tuned manu-

ally) in the relevant part (i.e. the minimal set of 
consecutive sentences that include both locations) 

of the positive training set snippets. Depending on 

the training set used, this resulted in about 40k 

common subsequences. To avoid unspecific pat-
terns, we filtered out all sequences that did not 

contain both locations, which reduces the number 

to about 15k in average. We then replaced the ac-

tual locations with a generic, which allows build-
ing a regular expression from the pattern that al-

lows any number of words in-between each part of 

the sequence. Before applying a pattern to an un-
known snippet, we also replace all (NER tagged) 

locations with a generic. For example, ‚ÄúLOCA-

TION * is * like * LOCATION‚Äù would match ‚ÄúTokyo 

is also a lot like Seoul‚Äù using regular expressions. 

The part-of-speech version is similar to the lexical 

one, but tries to create more generic and open pat-

terns by mining subsequences from the POS rep-
resentation of the relevant snippet part. For filter-

ing, all patterns that do not contain two ‚ÄòNNP‚Äô tags 

and appear less than 60 times are removed (the fil-

ter threshold is increased as POS patterns are more 
generic). We get around 60k to 80k patterns be-

fore, and ~10k to 20k primitive patterns after fil-

tering which are used as binary features. Finally, 
we merged lexical and POS patterns and thus al-

lowed the classifiers to use any of the features. A 

strongly truncated version of a rule tree created us-
ing J48 classification with POS subsequence prim-

itives is shown in Figure 1. Please note that due to 

the open nature of the primitives and their inde-

pendence, combining several of them in a feature 
vector will create extremely complex patterns 

quite easily. Even a vector that contains only the 

patterns *A*B* and *A*C* would create matches 

for ABC, ACB, ABAC, ACAB, AACB, AABC and allow 

any kind of additional fill words in between. How-
ever, this approach is computationally expensive 

(testing/training was around 6 hours on average). 

6 Evaluation 

In the following we evaluate the effectiveness of 

our analogy classifiers and models. We primarily 

rely on the informedness measure (Powers, 2007) 

for quantifying performance. In contrast to using 
only precision, recall, or F-Measure, it respects all 

 
Figure 1: Example Classification Tree 

*NNP*NNP* * *NNP*NNP*NN*NN* *

*NNP*NNP*.* *

*NN*NN*NN*NNP*IN*NNP* *

NOT-ANALOGY 
(243.0/8.0 correct)

ANALOGY 
(281.0/3.0 correct)

NO MATCH

NO MATCH

MATCH

MATCH

NO MATCH

NOT-ANALOGY 
(1938.0/22.0 correct)

*NNP*IN*NNP*NN* *

MATCH

MATCH

565



error types, false positives (FP) and false negatives 

(FN), but also true positives (TP) and true nega-

tives (TN), making it a fair and unbiased measure 

for classification. Furthermore, it compensates bi-
ased class distributions in datasets, e.g. as in our 

dataset the ratio of positive to negative snippets is 

1:8, even an ‚Äúalways no‚Äù classifier has a correct-
ness of 85%, but will have an informedness of 0. 

Informedness is given by: 
ùëñùëõùëìùëúùëüùëöùëíùëëùëõùëíùë†ùë† = ùëüùëíùëêùëéùëôùëô + ùëñùëõùë£ùëÖùëíùëêùëéùëôùëô ‚àí 1 

with: ùëüùëíùëêùëéùëôùëô =  
ùëáùëÉ

ùëáùëÉ+ùêπùëÅ
   and  ùëñùëõùë£ùëíùëüùë†ùëíùëÖùëíùëêùëéùëôùëô =  

ùëáùëÅ

ùëáùëÅ+ùêπùëÉ
 

In Table 2, we provide the average informedness, 

the percentage of correctly classified snippets, F-

measure, precision, recall, and inverse recall (true 

negative rate) for all experiments. A discussion of 
these results follows in the next section. 

 Classifier Performance 

Our straight-forward baseline approach, using uni-

grams and an SVM classifier results in a reasona-
ble informedness of 0.5. Expanding the feature 

space to lexical n-grams slightly increases perfor-

mance, while using more generic part-of-speech 
n-grams results in weaker results. Combining 

both, however, generally leads to better classifica-

tion results. When comparing different classifica-

tion algorithms, it shows that SVMs are most in-
formed when classifying n-grams-based features, 

followed by J48. Both techniques will result in 

moderate recall values around 0.5 and precision 
around 0.6, with a rather high true negative (inv. 

Recall rate) of 0.9. This changes quite signifi-

cantly for Na√Øve Bayes, which is more likely to 

classify a snippet as positive, therefore leading to 

higher recall values, but also much lower in-
formedness, precision, and inverse recall. Conse-

quently, the best approach is using SVM with a 

lexical-POS combined feature space, leading to an 
informedness of 0.55. 

Shortest Path was intended to achieve higher pre-

cision results by exploiting additional semantic 
knowledge of the underlying problem. Unfortu-

nately, it performs poorly if not used with a SVM, 

but even then it achieves inferior overall results 

than the best n-gram approach (informedness 0.4). 
This is due to some of its necessary assumptions 

not holding true (see section 6.4). 

In contrast, our subsequence-based model 
achieves a higher informedness score of 0.85 and 

0.87 in the best cases. While the lexical variants 

perform not as well, the more generic variants us-
ing POS allow for reliable classification. Combin-

ing the lexical and the POS features does unfortu-

nately not increase the performance further (quite 

contrary, the scores generally decrease for com-
bined features). A possible explanation is overfit-

ting caused by the increased feature space.  

  Significance Tests 

As our Gold set is of limited size, we performed 
statistical tests to investigate whether the differ-

ences reported in the last subsection are actually 

Table 2: Classifier Result Comparison with respect to the Gold classification 

Classifier Informed. % Correct F-Measure Precision Recall Inv. Recall 

Always No  0.00 0.85 - - 0 1 

Unigram Lexical SVM 0.50 0.88 0.59 0.63 0.55 0.94 

n-Gram Lexical SVM 0.53 0.89 0.63 0.68 0.58 0.95 

n-Gram  POS SVM 0.42 0.87 0.52 0.58 0.48 0.94 

n-Gram Lex & POS SVM 0.55 0.90 0.65 0.73 0.59 0.96 

n-Gram  Lexical Na√Øve Bayes 0.33 0.48 0.36 0.22 0.93 0.41 

n-Gram  POS Na√Øve Bayes 0.38 0.61 0.39 0.26 0.81 0.58 

n-Gram  Lex & POS Na√Øve Bayes 0.48 0.75 0.47 0.35 0.73 0.75 

n-Gram  Lexical J48 (C4.5) 0.45 0.87 0.55 0.58 0.52 0.93 

n-Gram  POS J48 (C4.5) 0.37 0.85 0.47 0.51 0.45 0.92 

n-Gram  Lex & POS J48 (C4.5) 0.44 0.87 0.54 0.57 0.52 0.93 

Shortest Path SVM 0.40  0.90 0.53 0.71 0.43 0.97 

Shortest Path Na√Øve Bayes 0.27  0.87 0.40 0.55 0.32 0.96 

Shortest Path J48 (C4.5) 0.26 0.89 0.40 0.77 0.27 0.99 

Subseq. Lexical  SVM 0.39 0.87 0.24 0.51 0.46 0.94 

Subseq. Lexical Na√Øve Bayes 0.53 0.79 0.49 0.36 0.73 0.80 

Subseq. Lexical J48 (C4.5) 0.34 0.86 0.44 0.47 0.41 0.93 

Subseq. POS SVM 0.84 0.97 0.87 0.89 0.85 0.98 

Subseq. POS Na√Øve Bayes 0.72 0.81 0.57 0.41 0.93 0.79 

Subseq. POS J48 (C4.5) 0.85 0.97 0.90 0.93 0.86 0.99 

Subseq. Lex & POS SVM 0.77 0.95 0.83 0.87 0.79 0.98 

Subseq. Lex & POS Na√Øve Bayes 0.70 0.80 0.56 0.41 0.91 0.79 

Subseq. Lex & POS J48 (C4.5) 0.87 0.97 0.90 0.92 0.88 0.98 
 

 566



significant or result from noise. We used an in-

stance-based test relying on the theory of approx-

imate randomization (Noreen, 1989)6 to perform 

100k iterations of randomized testing of the hy-
pothesis that the pairwise performance differences 

of selected approaches are actually significant (ex-

cluding those pairs where the significance is obvi-
ous). First, we compared our baseline, lexical uni-

grams with SVM to using lexical n-grams to test 

whether using n-grams actually contributed to the 
quality, and found the difference to be significant 

(sign-test p<0.024). However, for SVM-based 

classification, the higher reported performance for 

also including POS features in addition to lexical 
n-grams could not be shown to be significant 

(p>0.4). Finally, we tested if the choice between 

SVM or J48 is significant for our two best subse-
quence-based approaches, and confirmed this 

clearly (sign-test: p<0.006). According to the re-

ported subsequence results, combining lexical fea-
tures with part-of-speech features counter-intui-

tively lowers the performance when using SVM or 

Na√Øve Bayes and the positive effect on J48 was 

shown to be insignificant (p>0.68). Therefore, we 
assume that lexical features don‚Äôt make a substan-

tial contribution when POS features are present. 

 Error Analysis 

For only 2,845 of all 3,918 snippets, two different 
locations (regardless of their relevance to the anal-

ogy) are mentioned in the same sentence. This se-

verely hampers the effectiveness of our Shortest 

Path approach, which is limited to cases where 
both locations appear in the same sentence. Those 

snippets (344 on average / test set) are then classi-

fied as ‚Äúnot analogy‚Äù, decreasing the recall. The 
overall impact of this shortcoming is still low, as 

only 4% of these snippets are analogies. Our other 

approaches are unaffected. 
Interestingly, we see what one might call the ‚Äúin-

verse problem‚Äù when using the other two models 

(n-gram and subsequence) that search for the pres-

ence of certain terms or sequences, but do not ex-
plicitly connect them to the locations. They tend 

to create false positives by detecting statements 

that contain 2 locations and an analogy, but not 
between these locations. Consider: 

‚ÄúThey say New York is the City of Dreams. I say Lon-
don is the theatre where it all happens‚Äù  

(No. 5627 in corpus). 

Another source for false positives is when an anal-
ogy is not stated, but is requested: 

                                                   
6 Implementation at: http://www.clips.ua.ac.be/scripts/art 

‚ÄúWhat districts of Paris are similar to Shepherd's 
Bush or Ealing (both in West London‚Ä¶‚Äù  

(No. 8505 in corpus) 

7 Summary and Outlook 

We demonstrated approaches for discriminating 

analogy statements from the Web and Social Me-

dia. Our two major contributions are: a) We cre-

ated a Gold dataset containing 3,918 example text 
snippets, of which 542 are positively identified as 

analogies. This dataset was extracted from 109k 

potential documents resulting from a Web search 
with manually crafted Hearst-like patterns. The 

dataset was consequently refined by using a com-

bination of filters, crowd-sourcing, and expert 

judgments. We also discussed the challenges aris-
ing from a crowd-sourcing in such a setting.b) Us-

ing the Gold dataset, we designed and evaluated a 

set of machine learning models for classifying text 
snippets automatically with respect to containing 

place analogies. Besides more traditional n-gram 

based models, we also designed novel models re-
lying on feature spaces resulting from shortest 

path analysis of the typed dependency tree, and 

high-dimensional feature spaces built from fil-

tered subsequence patterns mined using the Pre-
fixSpan algorithm. In an exhaustive evaluation, 

the latter approach, which bridges between lexical 

and structural features, could be shown to provide 
significantly superior performance with a maxi-

mal informedness of 0.87 compared to 0.55 for the 

next best approach.  
In future work, classification performance can be 

further increased by better handling of current 

problem cases, e.g. analogies with out-of-domain 

targets (analogies between locations and other en-
tity classes, analogies between other entities but 

unrelated locations nearby, etc.) or ambiguous 

sentence constructions. Also, our approach can be 
adopted to other domains relevant to Web-based 

information systems like movies, cars, books, or 

e-commerce products in general. 

However, the more challenging next step is actu-
ally analyzing the semantics of the retrieved anal-

ogies, i.e. extracting the triggers of why people 

chose to compare the source and target. Achieving 
this challenge will allow building analogy reposi-

tories containing perceived similarities between 

entities and is a mandatory building block for ac-
tually implementing an analogy-enabled infor-

mation system.  

567



References  

Beust, P., Ferrari, S., & Perlerin, V. (2003). NLP model 

and tools for detecting and interpreting 

metaphors in domain-specific corpora. In Conf. 

on Corpus Linguistics. Lancaster, UK. 

Bollegala, D. T., Matsuo, Y., & Ishizuka, M. (2009). 

Measuring the similarity between implicit 

semantic relations from the web. In 18th Int. Conf. 

on World Wide Web (WWW). Madrid, Spain. 

doi:10.1145/1526709.1526797 
Bunescu, R. C., & Mooney, R. J. (2006). Subsequence 

Kernels for Relation Extraction. In Conf. on 

Advances in Neural Information Processing 

Systems (NIPS). Vancouver, Canada. 

Chalmers, D. J., French, R. M., & Hofstadter, D. R. 

(1992). High-level perception, representation, 

and analogy: A critique of artificial intelligence 

methodology. Journal of Experimental & 

Theoretical Artificial Intelligence, 4(3), 185‚Äì211. 

doi:10.1080/09528139208953747 

Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library 
for support vector machines. ACM Transactions 

on Intelligent Systems and Technology (TIST), 

2(3). 

Dedre Gentner, Keith J. Holyoak, & Boicho N. 

Kokinov (Eds.). (2001). The analogical mind: 

perspectives from cognitive science (Vol. 0, p. 

541). MIT Press. 

Gentner, D. (1983). Structure-mapping: A theoretical 

framework for analogy. Cognitive science, 7, 

155‚Äì170. 

Gentner, D. (2003). Why We‚Äôre So Smart. In Language 

in Mind: Advances in the Study of Language and 
Thought (pp. 195‚Äì235). MIT Press. 

Gentner, D., & Gunn, V. (2001). Structural alignment 

facilitates the noticing of differences. Memory & 

Cognition, 29(4), 565‚Äì77. 

Guthrie, D., Allison, B., Liu, W., Guthrie, L., & Wilks, 

Y. (2006). A closer look at skip-gram modelling. 

In Int. Conf. on Language Resources and 

Evaluation (LREC). Genoa, Italy. 

Hearst, M. A. (1992). Automatic acquisition of 

hyponyms from large text corpora. In Int. Conf. 

on Computational Linguistics (COLING). Nantes, 
France. 

Hofstadter, D. R. (2001). Analogy as the Core of 

Cognition. In The Analogical Mind (pp. 499‚Äì

538). 

Itkonen, E. (2005). Analogy as structure and process: 

Approaches in linguistics, cognitive psychology 

and philosophy of science. John Benjamins Pub 

Co. 

Kant, I. (1790). Critique of Judgement. 

Lofi, C., & Nieke, C. (2013). Modeling Analogies for 

Human-Centered Information Systems. In 5th Int. 

Conf. On Social Informatics (SocInfo). Kyoto, 
Japan. 

Marneffe, M.-C. de, MacCartney, B., & Manning, C. D. 

(2006). Generating typed dependency parses 

from phrase structure parses. In Int. Conf. on 

Language Resources and Evaluation (LREC). 

Genoa, Italy. 

Nakov, P., & Hearst, M. A. (2008). Solving relational 

similarity problems using the web as a corpus. In 

Proc. of ACL:HLT. Columbus, USA. 

Noreen, E. W. (1989). Computer-intensive Methods for 

Testing Hypotheses: An Introduction. John Wiley 

& Sons, New York, NY, USA. 

Pei, J., Han, J., Mortazavi-asl, B., Pinto, H., Chen, Q., 

Dayal, U., & Hsu, M. (2001). PrefixSpan: Mining 
Sequential Patterns Efficiently by Prefix-

Projected Pattern Growth. IEEE Computer 

Society. 

Powers, D. M. W. (2007). Evaluation: From Precision, 

Recall and F-Factor to ROC, Informedness, 

Markedness & Correlation. Flinders University 

Adelaide Technical Report SIE07001. 

Quinlan, R. (1993). C4.5: Programs for Machine 

Learning. San Mateo, USA: Morgan Kaufmann 

Publishers, Inc. 

Shelley, C. (2003). Multiple Analogies In Science And 
Philosophy. John Benjamins Pub. 

Shutova, E. (2010). Models of metaphor in NLP. In 

Annual Meeting of the Association for 

Computational Linguistics (ACL). 

Shutova, E., Sun, L., & Korhonen, A. (2010). Metaphor 

identification using verb and noun clustering. In 

Int. Conf. on Computational Linguistics 

(COLING). Beijing, China. 

Snow, R., Jurafsky, D., & Ng, A. (2004). Learning 

syntactic patterns for automatic hypernym 

discovery. In Advances in Neural Information 

Processing Systems (NIPS). Vancouver, Canada. 
Snow, R., O‚ÄôConnor, B., Jurafsky, D., & Ng, A. (2008). 

Cheap and fast---but is it good? Evaluating non-

expert annotations for natural language tasks. In 

Empirical Methods in Natural Language 

Processing (EMNLP). Honolulu, USA. 

Turney, P. (2008). A uniform approach to analogies, 

synonyms, antonyms, and associations. In Int. 

Conf. on Computational Linguistics (COLING). 

Manchester, UK. 

Wilks, Y. (1978). Making preferences more active. 

Artificial Intelligence, 11(3), 197‚Äì223. 

 

568


