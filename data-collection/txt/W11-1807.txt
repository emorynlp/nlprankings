










































Robust Biomedical Event Extraction with Dual Decomposition and Minimal Domain Adaptation


Proceedings of BioNLP Shared Task 2011 Workshop, pages 46–50,
Portland, Oregon, USA, 24 June, 2011. c©2011 Association for Computational Linguistics

Robust Biomedical Event Extraction with Dual Decomposition and Minimal
Domain Adaptation

Sebastian Riedel Andrew McCallum
Department of Computer Science

University of Massachusetts, Amherst
{riedel,mccallum}@cs.umass.edu

Abstract

We present a joint model for biomedical event
extraction and apply it to four tracks of the
BioNLP 2011 Shared Task. Our model de-
composes into three sub-models that concern
(a) event triggers and outgoing arguments, (b)
event triggers and incoming arguments and
(c) protein-protein bindings. For efficient de-
coding we employ dual decomposition. Our
results are very competitive: With minimal
adaptation of our model we come in second
for two of the tasks—right behind a version
of the system presented here that includes pre-
dictions of the Stanford event extractor as fea-
tures. We also show that for the Infectious
Diseases task using data from the Genia track
is a very effective way to improve accuracy.

1 Introduction

This paper presents the UMass entry to the BioNLP
2011 shared task (Kim et al., 2011a). We introduce
a simple joint model for the extraction of biomedical
events, and show competitive results for four tracks
of the competition. Our model subsumes three
tractable sub-models, one for extracting event trig-
gers and outgoing edges, one for event triggers and
incoming edges and one for protein-protein bind-
ings. Fast and accurate joint inference is provided by
combining optimizing methods for these three sub-
models via dual decomposition (Komodakis et al.,
2007; Rush et al., 2010). Notably, our model con-
stitutes the first joint approach that explicitly pre-
dicts which protein should share the same binding
event. So far this has either been done through post-
processing heuristics (Björne et al., 2009; Riedel et

al., 2009; Poon and Vanderwende, 2010), or through
a local classifier at the end of a pipeline (Miwa et al.,
2010).

Our model is very competitive. For Genia (GE)
Task 1 (Kim et al., 2011b) we achieve the second-
best results. In addition, the best-performing FAUST
system (Riedel et al., 2011) is a variant of the model
presented here. Its advantage stems from the fact
that it uses predictions of the Stanford system (Mc-
Closky et al., 2011a; McClosky et al., 2011b), and
hence performs model combination. The same holds
for the Infectious Diseases (ID) track (Pyysalo et al.,
2011), where we come in as second right behind
the FAUST system. For the Epigenetics and Post-
translational Modifications (EPI) track (Ohta et al.,
2011) we achieve the 4th rank, partly because we did
not aim to extract speculations, negations or cellular
locations. Finally, for Genia Task 2 we rank 3rd—
with the 1st rank achieved by the FAUST system.

In the following we will briefly describe our
model and inference algorithm, as far as this is pos-
sible in limited space. Then we show our results on
the three tasks and conclude. Note we will assume
familiarity with the task, and refer the reader to the
shared task overview paper for more details.

2 Biomedical Event Extraction

Our goal is to extract biomedical events as shown
in figure 1a). To formulate the search for such
structures as an optimization problem, we represent
structures through a set of binary variables. Our rep-
resentation is inspired by previous work (Riedel et
al., 2009; Björne et al., 2009) and based on a projec-
tion of events to a labelled graph over tokens in the

46



... phosphorylation of TRAF2 inhibits binding to the CD40 ...

Phosphorylation

Regulation

Binding
Theme

Cause Theme

Theme
Theme

Regulation BindingPhosphorylation
Theme

Cause

Theme
Theme Theme

Same Binding

 2 3 4 5 6 7 8 9

b4,9

e2,Phos.
a6,9,Theme

(a)

(b)

Figure 1: (a) sentence with target event structure; (b) pro-
jection to labelled graph.

sentence, as seen figure 1b).

We will first present some basic notation to sim-
plify our exposition. For each sentence x we have
a set candidate trigger words Trig (x), and a set of
candidate proteins Prot (x). We will generally use
the indices i and l to denote members of Trig (x), the
indices p, q for members of Prot (x) and the index j
for members of Cand (x) def= Trig (x) ∪ Prot (x).

We label each candidate trigger i with an event
Type t ∈ T (with None ∈ T ), and use the binary
variable ei,t to indicate this labeling. We use binary
variables ai,l,r to indicate that between i and l there
is an edge labelled r ∈ R (with None ∈ R).

The representation so far has been used in previ-
ous work (Riedel et al., 2009; Björne et al., 2009).
Its shortcoming is that it does not capture whether
two proteins are arguments of the same binding
event, or arguments of two binding events with the
same trigger. To overcome this problem, we intro-
duce binary “same Binding” variables bp,q that are
active whenever there is a binding event that has
both p and q as arguments. Our inference algorithm
will also need, for each trigger i and protein pair p, q,
a binary variable ti,p,q that indicates that at i there is
a binding event with arguments p and q. All ti,p,q are
summarized in t.

Constructing events from solutions (e,a,b) can
be done almost exactly as described by Björne et al.
(2009). However, while Björne et al. (2009) group
arguments according to ad-hoc rules based on de-
pendency paths from trigger to argument, we simply
query the variables bp,q.

3 Model

We use the following objective to score the struc-
tures we like to extract:

s (e,a,b) def=
∑

ei,t=1

sT (i, t) +
∑

ai,j,r=1

sR (i, j, r) +∑
bp,q=1

sB (p, q)

with local scoring functions sT (i, t)
def=

〈wT, fT (i, t)〉, sR (i, j, r) def= 〈wR, fR (i, j, r)〉
and sB (p, q)

def= 〈wB, fB (p, q)〉.
Our model scores all parts of the structure in isola-

tion. It is a joint model due to the three types of con-
straints we enforce. The first type acts on trigger la-
bels and their outgoing edges. It includes constraints
such as “an active label at trigger i requires at least
one active outgoing Theme argument”. The second
type enforces consistency between trigger labels and
their incoming edges. That is, if an incoming edge
has a label that is not None, the trigger must not be
labelled None either. The third type of constraints
ensures that when two proteins p and q are part of
the same binding (as indicated by bp,q = 1), there
needs to be a binding event at some trigger i that
has p and q as arguments. We will denote the set of
structures (e,a,b) that satisfy all above constraints
as Y .

To learn w we choose the passive-aggressive
online learning algorithm (Crammer and Singer,
2003). As loss function we apply a weighted sum of
false positives and false negative labels and edges.
The weighting scheme penalizes false negatives 3.8
times more than false positives.

3.1 Features
For feature vector fT (i, t) we use a collection of
representations for the token i: word-form, lemma,
POS tag, syntactic heads, syntactic children; mem-
bership in two dictionaries used by Riedel et al.
(2009).For fR (a; i, j, r) we use representations of
the token pair (i, j) inspired by Miwa et al. (2010) .
They contain: labelled and unlabeled n-gram depen-
dency paths; edge and vertex walk features (Miwa et
al., 2010), argument and trigger modifiers and heads,
words in between (for close distance i and j). For
fB (b; p, q) we use a small subset of the token pair
representations in fR.

47



Algorithm 1 Dual Decomposition.
require:

R: max. iteration, αt: stepsizes
t← 0 λ← 0 µ← 0
repeat

(ē, ā)← bestIncoming (−λ)
(e,a)← bestOutgoing (cout (λ,µ))
(b, t)← bestBinding

(
cbind (µ)

)
λi,t ← λi,t − αt (ei,t − ēi,t)
λi,j,r ← λi,j,r − αt (ai,j,r − āi,j,r)
µ

trig
i,j,k ←

[
µ

trig
i,j,k − αt (ei,Bind − ti,j,k)

]
+

µ
arg1
i,j,k ←

[
µ

arg1
i,j,k − αt (ai,j,Theme − ti,j,k)

]
+

µ
arg2
i,j,k ←

[
µ

arg2
i,j,k − αt (ai,k,Theme − ti,j,k)

]
+

t ← t + 1
until no λ, µ changed or t > R
return(e,a,b)

3.2 Inference

Inference in our model amounts to solving

arg max
(e,a,b)∈Y

s (e,a,b) . (1)

Our approach to finding the maximizer is dual de-
composition (Komodakis et al., 2007; Rush et al.,
2010), a technique that allows us to exploit effi-
cient search algorithms for tractable substructures
of our problem. We divide the problem into three
sub-problems: (1) finding the highest-scoring trig-
ger labels and edges (e,a) such that constraints on
triggers and their outgoing edges are fulfilled; (2)
finding the highest-scoring trigger labels and edges
(ē, ā) such that constraints on triggers and their in-
coming edges are fulfilled; (3) finding the highest-
scoring pairs of proteins b to appear in the same
binding, and make binding event trigger decisions
t for these. Due to space constraints we only state
that the first two problems can be solved exactly in
O

(
n2 + nm

)
time while the last needs O

(
m2n

)
.

Here n is the number of trigger candidates and m
the number of proteins.

The subroutines to solve these three sub-problems
are combined in algorithm 1—an instantiation of
subgradient descent on the dual of an LP relaxation
of problem 1. In the first three steps in the main
loop of this algorithm, the individual sub-problems

are solved. Note that to each subroutine a parame-
ter is passed. For example, when finding the struc-
ture (ē, ā) that maximizes the objective under the
incoming edge constraints, we pass the parameter
−λ. This parameter represents a set of penalties to
be added to the objective used for the subproblem.
In this case we have penalties −λi,e to be added to
the scores of trigger-label pairs (i, e), and penalties
−λi,j,r to be added for labelled edges i

r→ j.
One way to understand dual decomposition is as

iterative tuning of the penalties such that eventu-
ally all individual solutions are consistent with each
other. In our case this would mean, among other
things, that the solutions (e,a) and (ē, ā) are iden-
tical. This tuning happens in the second part of the
main loop which updates the dual variables λ and µ.
We see, for example, how the penalties λi,e are de-
creased by ei,e− ēi,e scaled by a step-size αt. Effec-
tively this change to λi,e will decrease the score of
ēi,e within bestIn (−λ) by αt if ēi,e was true while
ei,e was false in the current solutions.1 If ēi,e was
false but ei,e was true, the score is increased by αt.
If both agree, no change is needed.

Consistency between solutions also means that
the binding decisions in b and t are consistent
with the rest of the solution. This is achieved in
algorithm 1 through tuning of the dual variables
µ but we omit details for brevity. For complete-
ness we state how the penalties used for solving
the other subproblems are set based on the dual
variables λ and µ. We set couti,t (λ,µ)

def= λi,t +
δt,Bind

∑
p,q µ

trig
i,p,q; for the case that j ∈ Prot (x) we

get couti,j,r (λ,µ)
def= λi,j,r +

∑
p µ

arg1
i,j,p +

∑
q µ

arg2
i,q,j ,

otherwise couti,j,r (λ,µ)
def= λi,j,r . For bestBind (c)

we set cbindi,p,q (µ) = −µ
trig
i,p,q − µ

arg1
i,,p,q − µ

arg2
i,,p,q.

3.3 Preprocessing

After basic tokenization and sentence segmentation,
we generate a set of protein head tokens Prot (x)
for each sentence x based on protein span defi-
nitions from the shared task. To ensure tokens
contain not more than one protein we split them
at protein boundaries. Parsing is performed using
the Charniak-Johnson parser (Charniak and John-
son, 2005) with the self-trained biomedical parsing

1We refer to Koo et al. (2010) for details on how to set αt.

48



SVT BIND REG TOT
Task 1 73.5 48.8 43.8 55.2
Task 1 (abst.) 71.5 50.8 45.5 56.1
Task 1 (full) 79.2 44.4 40.1 53.1
Task 2 71.4 38.6 39.1 51.0

Table 1: Results for the GE track, task 1 and 2;
abst.=abstract; full=full text.

model of McClosky and Charniak (2008). Finally,
based on the set of trigger words in the training data,
we generate a set of candidate triggers Trig (x).

4 Results

We apply the same model to the GE, ID and EPI
tracks, with minor modifications in order to deal
with the different event type sets T and role sets R
of each track. Training and testing together took be-
tween 30 (EPI) to 120 (GE) minutes using a single-
core implementation.

4.1 Genia
Our results for GE task 1 and 2 can be seen in table
1. We also show results for abstracts only (abst.),
and for full text only (full). Note that binding events
(BIND) and general regulation events (REG) seem
to be harder to extract in full text. Somewhat surpris-
ingly, for simple events (SVT) the opposite holds.
We also like to point out that for full text extrac-
tion we rank first—the second best FAUST system
achieves an F1 score of 52.67.

4.2 Infectious Diseases
The Infectious Diseases track differs from the Genia
track in two important ways. First, it introduces the
event type Process that is allowed to have no ar-
guments at all. Second, it comes with significantly
less training data (152 vs 908 documents). We can
accommodate the first difference by making simple
changes in our inference algorithms. For example,
for Process events we do not force the algorithm to
pick a Theme argument.

To compensate for the lack of training data we
simply add data from the GE track. This is reason-
able because annotations overlap quite significantly.
In table 2 we show the impact of mixing different
amounts of ID data (I) and GE data (G) into the
training set. We point out that adding the ID training

I/G BIND REG PRO TOT
DEV 1/0 18.6 27.1 34.3 41.5
DEV 0/1 18.2 26.8 0.00 35.5
DEV 1/1 20.0 33.1 49.3 47.2
DEV 2/1 20.0 34.5 52.0 48.5
TEST 2/1 34.6 46.4 62.3 53.4

Table 2: ID results for different amounts of ID (I) and (G)
training data.

set twice, and the GENIA set once, leads to the best
performance (I/G=2/1). Remarkably, the F1 score
for Process increases by including data, although
this data does not include any such events. This may
stem from a shared model of None arguments that is
improved with more data.

4.3 Epigenetics and Post-translational
Modifications

For this track a different set of events is to be pre-
dicted. However, it is straightforward to adapt our
model and algorithms to this setting. For brevity we
only report our total results here and omit a table
with details. The first metric (ALL) includes nega-
tion, speculation and cellular location targets. We
omitted these in our model and hence our result of
33.52 F1 is relatively weak. For the metric that ne-
glects these aspects (CORE), we achieve 64.15 F1
and come in 4th. Note that in this metric the FAUST
system, based on the model presented here, comes
in as very close second.

5 Conclusion

We have presented a robust joint model for event
extraction from biomedical text that performs well
across all tasks. Remarkably, no feature set or pa-
rameter tuning was necessary to achieve this. We
also show substantial improvements for the ID task
by adding GENIA data into the training set.

Acknowledgements
This work was supported in part by the Center for Intelli-

gent Information Retrieval. The University of Massachusetts
gratefully acknowledges the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Program
under Air Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are those
of the authors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government.

49



References
Jari Björne, Juho Heimonen, Filip Ginter, Antti Airola,

Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the Natural Language
Processing in Biomedicine NAACL 2009 Workshop
(BioNLP ’09), pages 10–18, Morristown, NJ, USA.
Association for Computational Linguistics.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ’05),
pages 173–180.

Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951–991.

Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun’ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.

Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.

Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In In ICCV.

Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).

David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
46rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL ’08).

David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Con-
ference (ACL-HLT’11), Main Conference (to appear),
Portland, Oregon, June.

David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in BioNLP 2011. In BioNLP 2011 Shared
Task.

Makoto Miwa, Rune Saetre, Jin-Dong D. Kim, and
Jun’ichi Tsujii. 2010. Event extraction with com-
plex event classification using rich features. Journal of

bioinformatics and computational biology, 8(1):131–
146, February.

Tomoko Ohta, Sampo Pyysalo, and Jun’ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.

Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Lit-
erature. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
813–821, Los Angeles, California, June. Association
for Computational Linguistics.

Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun’ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.

Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ’09), pages 41–49.

Sebastian Riedel, David McClosky, Mihai Surdeanu,
Christopher D. Manning, and Andrew McCallum.
2011. Model combination for event extraction in
BioNLP 2011. In BioNLP 2011 Shared Task.

Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In In Proc. EMNLP.

50


