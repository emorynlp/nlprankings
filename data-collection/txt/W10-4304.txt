










































Modeling User Satisfaction Transitions in Dialogues from Overall Ratings


Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 18–27,
The University of Tokyo, September 24-25, 2010. c©2010 Association for Computational Linguistics

Modeling User Satisfaction Transitions in Dialogues from Overall Ratings

Ryuichiro Higashinaka†, Yasuhiro Minami‡, Kohji Dohsaka‡, and Toyomi Meguro‡
† NTT Cyber Space Laboratories, NTT Corporation

‡ NTT Communication Science Laboratories, NTT Corporation
higashinaka.ryuichiro@lab.ntt.co.jp

{minami,dohsaka,meguro}@cslab.kecl.ntt.co.jp

Abstract

This paper proposes a novel approach
for predicting user satisfaction transitions
during a dialogue only from the ratings
given to entire dialogues, with the aim
of reducing the cost of creating refer-
ence ratings for utterances/dialogue-acts
that have been necessary in conventional
approaches. In our approach, we first
train hidden Markov models (HMMs) of
dialogue-act sequences associated with
each overall rating. Then, we combine
such rating-related HMMs into a single
HMM to decode a sequence of dialogue-
acts into state sequences representing to
which overall rating each dialogue-act is
most related, which leads to our rating pre-
dictions. Experimental results in two di-
alogue domains show that our approach
can make reasonable predictions; it signif-
icantly outperforms a baseline and nears
the upper bound of a supervised approach
in some evaluation criteria. We also
show that introducing states that represent
dialogue-act sequences that occur com-
monly in all ratings into an HMM signifi-
cantly improves prediction accuracy.

1 Introduction

In recent years, there has been intensive work
on the automatic evaluation of dialogues (Walker
et al., 1997; Möller et al., 2008). Automatic
evaluation makes it possible to predict the per-
formance of dialogue systems without the costly
process of performing surveys with human sub-
jects, leading to a rapid improvement cycle for
dialogue systems. It is also useful for detect-
ing problematic situations in an ongoing dialogue
(Walker et al., 2002; Herm et al., 2008; Kim,
2007). In these studies, the typical approach is
to train a prediction model, such as a regression
or classification model, using features represent-
ing the whole or a part of a dialogue together with
human reference labels (e.g., reference ratings).
However, creating such reference labels by hand

can be extremely costly when we want to predict
user satisfaction transitions during a dialogue be-
cause we need to create reference labels after each
utterance/dialogue-act in the training data (Engel-
brecht et al., 2009).

This paper proposes a novel approach for pre-
dicting user satisfaction transitions during a dia-
logue only from the dialogues with overall rat-
ings. The approach makes it possible to avoid
creating reference labels for utterances/dialogue-
acts and only requires a single reference label for
each dialogue. More specifically, we predict the
user satisfaction rating after each dialogue-act in a
dialogue only by using dialogues with dialogue-
level (overall) user satisfaction ratings as train-
ing data. Our basic approach is to train hid-
den Markov models (HMMs) of dialogue-act se-
quences associated with each overall rating and
combine such rating-related HMMs into a single
HMM. We use this combined HMM to decode a
sequence of dialogue-acts by the Viterbi algorithm
(Rabiner, 1990) into state sequences that indicate
from which rating-related HMM each dialogue-act
is most likely to have been generated, leading to
our rating predictions for the dialogue-acts. This
paper experimentally examines the validity of our
approach and explores several model topologies
for possible improvement.

In Section 2, we review related work on auto-
matic evaluation of dialogues. In Section 3, we
describe our approach in detail. In Section 4, we
describe the experiment we performed to verify
our approach and present the results. In Section
5, we summarize and mention future work.

2 Related Work

Regression models are typically utilized for eval-
uating the quality of an entire dialogue. Most fa-
mously, the PARADISE framework (Walker et al.,
1997) learns from data a linear regression model
that predicts dialogue-level user satisfaction from
various objective characteristics of a dialogue that
concern task success and dialogue costs. This
framework is widely used today and a number of
extensions have been proposed to improve the pre-
diction performance (Möller et al., 2008); how-

18



ever, it is not aimed at predicting user satisfaction
transitions.

Classification models are widely employed to
detect problematic situations in an ongoing dia-
logue. Walker et al. (2002) developed the Prob-
lematic Dialogue Predictor for the “How May I
Help You” system (Gorin et al., 1997) to robustly
transfer problematic calls to human operators in
call routing tasks. They derive speech recogni-
tion, language understanding, and dialogue man-
agement features from the first few turns of a dia-
logue and apply a decision tree classifier to detect
problematic calls. For a similar task, Hirschberg
et al. (2004) and Herm et al. (2008) used prosodic
and emotional features. Kim (2007) recently pro-
posed an approach for online call quality monitor-
ing so that problematic calls can be transferred to
human operators as quickly as possible rather than
waiting for the first few turns.

N-grams and HMM-based approaches have also
been actively studied. Hara et al. (2010) proposed
predicting the most likely user satisfaction level of
a dialogue by using N-grams of dialogues for each
satisfaction level in the music navigation domain.
Isomura et al. (2009) used HMMs to evaluate the
naturalness of a dialogue in their interview system.
They trained HMMs that model dialogue-act se-
quences between human subjects and used them to
evaluate human-machine dialogues by the output
probabilities of the HMMs. Recently, there have
been approaches to predict user satisfaction tran-
sitions by evaluating the quality of individual ut-
terances in a dialogue. For example, Engelbrecht
et al. (2009) predicted user satisfaction ratings af-
ter each user utterance by HMMs trained from
utterance-level features and utterance-level refer-
ence ratings.

The problem with these approaches is that they
require a lot of training data, especially when we
want to predict the quality of smaller units such
as utterances. Our aim is to reduce such cost.
Our work is similar to Engelbrecht’s work (Engel-
brecht et al., 2009) in that we use HMMs to predict
user satisfaction transitions during a dialogue but
different in that we only use dialogue-level ratings
to model dialogue-act-level user satisfaction tran-
sitions.

3 Approach

We aim to predict user satisfaction transitions only
from dialogues with overall ratings. More for-
mally, given a dialogue di of a set of dialogues
D (= {d1 . . .dN}), we want to predict the user
satisfaction rating after each dialogue-act in di,
namely, r′(da(di, 1)) . . .r′(da(di, mi)), using D
with their dialogue-level ratings r(d1) . . . r(dN).

1:speaker1 2:speaker2

Speaker HMM for Rating 1

3:speaker1 4:speaker2

Speaker HMM for Rating 2

Figure 1: SHMMs connected ergodically. In the
figure, an oval marked with speaker1/speaker2
indicates a state that emits speaker1/speaker2’s
dialogue-acts. Arrows denote transitions and
numbers before speaker1/speaker2 are state IDs.
Boxes group together the states related to a partic-
ular overall rating.

Here, da(di, l) denotes the l-th dialogue-act in di,
N the total number of dialogues, and mi the total
number of dialogue-acts in di.

Our basic idea is to train HMMs representing
dialogue-act sequences of dialogues for each over-
all rating and combine these rating-related HMMs
into a single HMM that can assign ratings for
dialogue-acts by estimating from which HMM
each dialogue-act has most likely to have been
generated by the Viterbi decoding. We use HMMs
because they can deal with sequences that evolve
over time and have been successfully utilized to
model and evaluate dialogue-act sequences (Shi-
rai, 1996; Isomura et al., 2009; Engelbrecht et
al., 2009). The generative feature of an HMM is
also useful when we want to build a probabilis-
tic dialogue manager that produces the most likely
dialogue-act sequences (Hori et al., 2008) or that
aims to maximize a reward function in partially
observable Markov decision processes (Williams
and Young, 2007; Minami et al., 2009).

When there are K levels of user satisfaction as
overall ratings, we create K HMMs each of which
is trained using the dialogue-act sequences in dia-
logues Dk ⊂ D, where Dk = {∀di, |r(di) = k}.
We use the EM-algorithm to train HMMs. Here,
we assume that each HMM has two states, each
of which emits dialogue-acts of one of the con-
versational participants. This type of HMM is
called a speaker HMM (SHMM) and has been
successfully utilized to model two-party conversa-
tion (Meguro et al., 2009).

As an illustrative example, Fig. 1 shows two
SHMMs for ratings 1 and 2 that are connected
ergodically. We can simply use these connected
SHMMs (namely, states 1, 2, 3, and 4) to decode a
sequence of dialogue-acts into state sequences and
thereby obtain rating predictions. For example, if
the optimal state sequence obtained by the Viterbi
decoding is {4, 2, 1, 3, 2}, we can convert it into
ratings <2, 1, 1, 2, 1> using the ratings associated
with the states.

19



3:speaker1 4:speaker2

1:speaker1 2:speaker2

5:speaker1 6:speaker2

Speaker HMM for Rating 1

Speaker HMM for Rating 2

Speaker HMM for All Ratings

Figure 2: SHMMs with an additional SHMM
trained from all dialogues.

Introducing Common States: The simple er-
godic model may not be sufficient for appropri-
ately assigning ratings to input dialogue-act se-
quences because it is often the case that there
are dialogue-act sequences, such as greetings and
question-answer pairs, that commonly occur in ev-
ery dialogue. If we forcefully assign a rating for
such dialogue-act sequences, it may result in de-
grading the prediction accuracy. Therefore, in
addition to the simple ergodic model, we intro-
duce another SHMM that represents dialogue-act
sequences of dialogues for all ratings (see Fig.
2). This additional SHMM models dialogue-act
sequences that occur commonly in all dialogues
and it can simply be trained using all dialogues.
Hence, we call the states in this SHMM common
states. When this SHMM is added to the ergodic
model, it may be possible to reduce the possibil-
ity of our having to forcefully assign inappropriate
scores to common dialogue-act sequences. In this
model, when the optimal state sequence is {1, 4,
5, 6, 2}, the predicted ratings become <1, 2, 0, 0,
1>. Here, we assume that the SHMM for all rat-
ings corresponds to rating 0, which is reasonable
because common dialogue-acts should not affect
ratings. The obtained ratings can also be inter-
preted as <1, 2, 2, 2, 1> when we assume that
the rating of a dialogue-act is taken over from the
previous turn.

Using Concatenated Training: We have so far
presented two model topologies, one with K
SHMMs connected ergodically and the other with
K + 1 SHMMs having an additional SHMM rep-
resenting all ratings. However, we still have a
problem; that is, we need to find optimal transi-
tion probabilities between the SHMMs of different
ratings. Our solution is to use concatenated train-
ing (Lee, 1989). The procedure for concatenated
training is illustrated in Fig. 3 and has the follow-
ing three steps.

step 1 Train an SHMM Mk (Mk ∈ M, 1 ≤
k ≤ K) using dialogues Dk, where Dk =

Copy

Rating

1
M1M1

M1M1M0M0

Retrain

Train
Rating

k

M0

MkMk

MkMk

Retrain

Train
Rating

K

M0

MKMK

MKMK

Retrain

Train

All 

Ratings
M0

Train +

M0

M1 Mk MK

AVG

Concatenate

M1+0 Mk+0 MK+0

M1M1M0M0 M0 MkMk M0 MKMK

M1+0 Mk+0 MK+0

Step 1

Step 2

Step 3

Step 2’

END

Mconcat
If the fitting has 

converged for 

all Mk+0

Split Mconcat into 

pairs again and 

retrain Mk+0

M1…MK become 

less likely to

output common 

sequences

Transition probabilities

of M0 are redistributed

between M0 and Mk

Figure 3: Three steps to combine SHMMs using
concatenated training.

{∀di|r(di) = k}, and an SHMM M0 using
all dialogues; i.e., D. Here, K means the
maximum level of user satisfaction and r(di)
the rating assigned to di.

step 2 Connect each Mk ∈ M with a copy of
M0 using equal initial and transition proba-
bilities (we call this connected model Mk+0)
and retrain Mk+0 with ∀di ∈ Dk, where
r(di) = k.

step 3 Merge all models Mk+0 (1 ≤ k ≤ K) to
produce one concatenated HMM (Mconcat).
Here, the output probabilities of the copies
of M0 are averaged over K when all models
are merged to create a combined model. If
the fitting of all Mk+0 models has converged
against the training data, exit this procedure;
otherwise, go to step 2 by connecting a copy
of M0 and Mk for all k. Here, the transi-
tion probabilities from M0 to Ml(l �= k) are
summed and equally distributed between the
copied M0’s self-loop and transitions to the
states in Mk .

In concatenated training, the transition and out-
put probabilities can be optimized between M0
and Mk , meaning that the output probabilities
of dialogue-act sequences that are common and
also found in Mk can be moved from Mk to
M0. This makes the distribution of Mk sharp (not
broad/uniform), making it likely to output only
the dialogue-acts specific to a rating k. As re-
gards M0, its distribution of output probabilities
can also be sharpened for dialogue-acts that oc-
cur commonly in all ratings. This sharpening of
distributions is likely to be helpful in assigning

20



appropriate ratings to dialogue-act sequences. In
the next section, we experimentally examine how
these proposed HMMs perform in modeling and
predicting user satisfaction transitions in dialogue.

4 Experiment

To verify our approach, we first prepared dialogue
data. Then, we trained our HMMs and compared
them with a random baseline and an upper bound
that uses a supervised approach; that is, an HMM
is trained using reference labels on the dialogue-
act level.

4.1 Dialogue Data
We used dialogues in two domains; the animal
discussion (AD) domain and the attentive listen-
ing (AL) domain. All dialogues are in Japanese.
In both domains, the data we used were text dia-
logues. We did not use spoken dialogue data be-
cause we wanted to avoid particular problems of
voice, such as filled pauses and overlaps, although
we aim to deal with spoken dialogue in the future.

4.1.1 Animal Discussion
We used the dialogue data in the AD domain that
we previously collected (Higashinaka et al., 2008).
In this domain, the system and user talk about likes
and dislikes about animals via a text chat inter-
face. The data consist of 1000 dialogues between
a dialogue system and 50 human users. Each
user conversed with the system 20 times, includ-
ing two example dialogues at the beginning. All
user/system utterances have been annotated with
dialogue-acts. There are 29 dialogue-act types in-
cluding those related to self-disclosure, question,
response, and greetings. For example, a dialogue-
act DISC-P denotes one’s self-disclosure about a
proposition P. Here, P is either like(X,A) or
dislike(X,A) where X is a conversational par-
ticipant and A a certain animal. DISC-R denotes
one’s self-disclosure of a reason for a proposition.
See (Higashinaka et al., 2008) for the details of the
dialogue-acts.

For our experiment, we created two subsets of
the data. We first extracted 180 dialogues by
taking all 18 non-example dialogues for the ini-
tial ten users sorted by user ID (AD-SUB1; 4147
user dialogue-acts and 6628 system dialogue-
acts). Then, from AD-SUB1, we randomly ex-
tracted nine dialogues per user to form another
subset of 90 dialogues (AD-SUB2; 2050 user
dialogue-acts and 3290 system dialogue-acts). An
annotator, who was not one of the authors, la-
beled AD-SUB1 with dialogue-level user satis-
faction ratings and AD-SUB2 with utterance-level
ratings. More specifically, each dialogue/utterance

Utterance (dialogue-acts) Sm Cl Wi
SYS Do you like rabbits? (DA: Q-DISC-P) 6 6 6
USR I like rabbits. They are cute.

(DA: DISC-P, DISC-R)
SYS Indeed they are cute. (DA: REPEAT) 6 6 6
SYS Tell me why you like rabbits. 6 5 6

(DA: Q-DISC-R-OTHER)
USR I like them because they are small and

warm. (DA: DISC-P-R)
SYS You like them because they are warm. 7 5 7

(DA: REPEAT)
Overall rating for the dialogue 7 5 6

Figure 4: Excerpt of a dialogue with utterance-
level user satisfaction ratings for smoothness
(Sm), closeness (Cl), and willingness (Wi) in the
AD domain. SYS and USR denote system and
user, respectively. The dialogue was translated by
the authors.

was given three different user satisfaction rat-
ings related to “Smoothness of the conversation”,
“Closeness perceived by the user towards the sys-
tem”, and “Willingness to continue the conversa-
tion”. The ratings ranged from 1 to 7, where 1
is the worst and 7 the best (see Fig. 4 for exam-
ples of utterance-level and overall ratings given by
the annotator for an excerpt of a dialogue). In a
manner similar to (Evanini et al., 2008), we used a
third-person’s user satisfaction rating for the sake
of consistency.

For utterance-level ratings, the annotator care-
fully read each utterance and gave ratings after
each system utterance according to how she would
have felt after receiving each system utterance if
she had been the user in the dialogue. To make
the situation more realistic, she was not allowed
to look down at the dialogue after the current ut-
terance. At the beginning of a dialogue, the rat-
ings always started from four (neutral). When the
annotator gave dialogue-level ratings, she looked
through the entire dialogue and rated its quality
(smoothness, closeness, and willingness) accord-
ing to how she would have felt after having had
the dialogue in question.

4.1.2 Attentive Listening
We collected human-human listening-oriented di-
alogues in a manner similar to (Meguro et al.,
2009). In this AL domain, a listener attentively
listens to the other in order to satisfy the speaker’s
desire to speak and to make himself/herself heard.
We collected such listening-oriented dialogues us-
ing a website where users taking the roles of lis-
teners and speakers were matched up to have con-
versations. There were ten listeners who always
stayed at the website and 37 speakers who could
talk to them anytime the listeners were available.
They were all paid for their participation. A con-
versation was done through a text-chat interface.

21



The use of facial and other non-linguistic expres-
sions were not allowed for analysis purposes. The
participants were instructed to end the conversa-
tion approximately after ten minutes. Within a
three-week period, each speaker was instructed to
have at least two conversations a day, resulting in
our collecting 1260 listening-oriented dialogues.

Two independent annotators labeled each utter-
ance with 40 dialogue-act types, including those
related to self-disclosure, question, internal argu-
ment, sympathy, and information giving. The
inter-annotator agreement was reasonable, with
0.57 in Cohen’s κ. Although we cannot describe
the full details of our dialogue-acts for lack of
space, we have dialogue-acts DISC-EVAL-POS for
one’s self-disclosure of his/her positive evalua-
tion towards a certain entity, DISC-EXP for one’s
self-disclosure of his/her experience, and SELF-Q-
DESIRE for one’s internal argument about his/her
desire (e.g., “Have I ever wanted to go abroad?”).
We used the dialogue-act annotation of one of the
annotators in this work.

An annotator gave dialogue-level user satis-
faction ratings to all 1260 dialogues (AL-ALL;
31779 speaker dialogue-acts and 28681 listener
dialogue-acts). Then, we made a subset of the
data by randomly selecting ten dialogues for
each of the ten listeners to obtain 100 dialogues
(AL-SUB1; 2453 speaker dialogue-acts and 2197
listener dialogue-acts). Finally, the annotator
gave utterance-level ratings to AL-SUB1. The
utterance-level ratings were given only after lis-
teners’ utterances. The annotator gave three rat-
ings as in the AD domain; namely, smoothness,
closeness, and good listening. Instead of willing-
ness, we have a “good listener” criterion asking
for how good the annotator thinks the listener is
from the viewpoint of attentive listening; for ex-
ample, how well the listener is making it easy for
the speaker to speak. All ratings ranged from 1 to
7. See Fig. 5 for a sample dialogue in the AL do-
main with utterance-level and overall ratings given
by the annotator.

4.2 Training HMMs
From the dialogue data and their dialogue-level
ratings, we created our proposed HMMs. We had
five topology variations:

ergodic0: The simple ergodic model with no ad-
ditional SHMM for all ratings. See Fig.
1 for the topology. This HMM has 7
SHMMs connected ergodically with equal
initial/transition probabilities.

ergodic1: The simple ergodic model with an ad-
ditional SHMM for all ratings. See Fig. 2
for the topology. This HMM has 8 (7 +

Utterance (dialogue-acts) Sm Cl GL
LIS You know, in spring, Japanese food tastes de-

licious. (DA: DISC-EVAL-POS)
5 5 5

SPK This time every year, I make a plan to go on
a healthy diet. But . . . (DA: DISC-HABIT)

LIS Uh-huh (DA: ACK) 6 5 6
SPK The temperature goes up suddenly!

(DA: INFO)
SPK It’s always too late! (DA: DISC-EVAL-NEG)
LIS Clothing worn gets less and less while not be-

ing able to lose weight. (DA: DISC-FACT)
6 6 6

SPK Well, people around me soon get used to my
body shape though. (DA: DISC-FACT)

Overall rating for the dialogue 7 7 7

Figure 5: Excerpt of a dialogue with utterance-
level user satisfaction ratings for smoothness
(Sm), closeness (Cl), and good listener (GL) in the
AL domain. SPK and LIS denote speaker and lis-
tener, respectively. Both the speaker and listener
are human.

1) SHMMs connected ergodically with equal
initial/transition probabilities.

ergodic2: Same as ergodic1 except that the num-
ber of common states is doubled so that com-
mon dialogue-act sequences can be more ac-
curately modeled. Note that without concate-
nated training, SHMMs for each rating may
also have sharp distributions for common se-
quences. One possible solution to avoid this
is to sharpen the distributions of common
states by increasing its number of states.

concat1: 8 (7 + 1) SHMMs combined using con-
catenated training. See Fig. 3 for the topol-
ogy.

concat2: Same as concat1 except that the number
of common states is doubled.

[See Appendices A and B for the actual examples
of the obtained models]

4.2.1 Baseline and Upper Bound
We created the following baseline (random) and
upper bound (supervised) models for comparison:

random: This outputs ratings 1–7 at random.

supervised: This is an HMM trained in a man-
ner similar to (Engelbrecht et al., 2009). This
model is the same as ergodic0 in topology but
different in that the initial, transition, and out-
put probabilities are trained in a supervised
manner using the dialogue-acts and dialogue-
act-level reference ratings in AD-SUB2 and
AL-SUB1. Since we only have ratings for
system/listener utterances in the corpora, in
order to make training data, we assumed that
the ratings for dialogue-acts corresponding
to user/speaker utterances were the same as

22



those after the previous system/listener utter-
ances. This model simulates the ideal situ-
ation where we possess user satisfaction rat-
ings for all dialogue-acts in the data.

4.3 Evaluation Procedure
We performed a ten-fold cross validation. We first
separated utterance-level labeled data (i.e., AD-
SUB2 or AL-SUB1) into 10 disjoint sets. Then,
for each set S, we used dialogue-level labeled
data (i.e., AD-SUB1 or AL-ALL) excluding S
for training HMMs. Here, ‘supervised’ only used
the utterance-level labeled data excluding S for
training. Then, we made the models (i.e., er-
godic0, ergodic1, ergodic2, concat1, concat2, ran-
dom and supervised) output rating sequences for
the dialogue-acts in S and evaluated them with the
reference ratings in S. We repeated this process
ten times to evaluate the overall performance.

Since utterance-level ratings are provided only
after system/listener utterances, we only evaluated
ratings after dialogue-acts corresponding to sys-
tem/listener utterances. When a system/listener
utterance contained multiple dialogue-acts, the
dialogue-acts were assumed to have the same rat-
ing as that utterance. When the output rating
sequences contain 0, which can be the case for
ergodic1–2 and concat1–2, the 0 is replaced by the
most previous non-zero rating. When 0 is found at
the beginning of a dialogue, it remained 0. Al-
though our reference ratings always started with
four (cf. Section 4.1.1), we did not use this in-
formation to fill initial zeros because we wanted
to evaluate the prediction accuracy when we do
not have any prior knowledge. Since some mod-
els may benefit from avoiding evaluating dialogue-
acts at the beginning because of these zeros, we
simply compared the rating sequences where all
models produced non-zero values. For exam-
ple, when we have three output rating sequences
<0,5,6,0,4>, <0,0,1,2,0>, and <1,2,3,4,5> for a
given dialogue-act sequence, the zeros that follow
non-zero values are first filled with their preceed-
ing values, and thereby we obtain <0,5,6,6,4>,
<0,0,1,2,2>, and <1,2,3,4,5>. Then, by cropping
the common non-zero span, we obtain <6,6,4>,
<1,2,2>, and <3,4,5>, and use these rating se-
quences for evaluation.

4.3.1 Evaluation Criteria
We used two kinds of evaluation criteria: one for
evaluating individual matches and the other for
evaluating distributions.

Evaluating Individual Matches: We used the
match rate and mean absolute error to evaluate the
matching of reference and hypothesis rating se-

quences. They are derived by the equations shown
below. In the equations, R (= {R1 . . .RL}) and
H (= {H1 . . .HL}) denote reference and hypoth-
esis rating sequences for a dialogue, respectively.
L is the length of R and H (Note that they have
the same length).

• Match Rate (MR)

MR(R, H) =
1
L

L∑

i=1

match(Ri, Hi), (1)

where ‘match’ returns 1 or 0 depending on
whether a rating in R matches that in H .

• Mean Absolute Error (MAE)

MAE(R, H) =
1
L

L∑

i=1

|Ri −Hi|. (2)

Evaluating Distributions: In generative mod-
els, it is important that the output distribution
matches that of the reference. Therefore, we ad-
ditionally use Kullback-Leibler divergence, match
rate per rating, and mean absolute error per rat-
ing. The Kullback-Leibler divergence evaluates
the shape of output distributions. The match rate
per rating and mean absolute error per rating eval-
uate how accurately each individual rating can
be predicted; namely, the accuracy for predict-
ing dialogue-acts with one rating is equally val-
ued with those for other ratings irrespective of the
distribution of ratings in the reference. It is im-
portant to use these metrics in the practical as well
as information theoretic sense because it is no use
predicting only easy-to-guess ratings; we should
be able to correctly predict rare but still important
cases. For example, rating 1 in human-human di-
alogue is quite rare; however, predicting it is very
important for detecting problematic situations in a
dialogue.

• Kullback-Leibler Divergence (KL)

KL(R, H) =
K∑

r=1

P(H, r) · log(P(H, r)
P(R, r)

), (3)

where K is the maximum user satisfaction rating
(i.e. 7 in this experiment), R and H denote the se-
quentially concatenated reference/hypothesis rat-
ing sequences of the entire dialogues, and P(∗, r)
denotes the occurrence probability that a rating r
is found in an arbitrary rating sequence.

• Match Rate per rating (MR/r)

MR/r(R, H) =
1
K

K∑

r=1

∑

i∈{i|Ri=r}
match(Ri, Hi)

∑

i∈{i|Ri=r}
1

,

(4)

23



Criterion random ergodic0 ergodic1 ergodic2 concat1 concat2 supervised

Smoothness

MR 0.142e0e1 0.111 0.111 0.157e0e1 0.153 0.199e0e1r 0.275c1e0e1e2r
MAE 1.988e0e1 2.212 2.212 1.980 1.936e0e1 1.870e0e1 1.420c1c2e0e1e2r
KL 0.287 0.699 0.699 0.562 0.280 0.369 0.162
MR/r 0.143 0.137 0.137 0.176 0.136 0.177 0.217
MAE/r 2.286 2.414 2.414 2.152 2.301 2.206 1.782

Closeness

MR 0.143 0.129 0.129 0.171e0e1 0.174 0.189e0e1 0.279c1c2e0e1e2r
MAE 2.028 2.066 2.066 1.964 1.798e0e1r 1.886 1.431c1c2e0e1e2r
KL 0.195 0.449 0.449 0.261 0.138 0.263 0.092
MR/r 0.143 0.156 0.156 0.170 0.155 0.164 0.231
MAE/r 2.283 2.236 2.236 2.221 2.079 2.067 1.702

Willingness

MR 0.143e0e1 0.112 0.112 0.180e0e1 0.152 0.183e0e1 0.283c1c2e0e1e2r
MAE 2.005 2.133 2.133 1.962 1.801e0e1r 1.882 1.403c1c2e0e1e2r
KL 0.225 0.568 0.568 0.507 0.238 0.255 0.125
MR/r 0.143 0.152 0.152 0.192 0.181 0.167 0.224
MAE/r 2.286 2.258 2.258 2.107 1.958 2.164 1.705

Table 1: The match rate (MR), mean absolute error (MAE), Kullback-Leibler divergence (KL), match
rate per rating (MR/r) and mean absolute error per rating (MAE/r) for our proposed HMMs, the random
baseline, and the upper bound (supervised) for the AD domain. ‘e0–e2’, ‘c1–c2’, and ‘r’ indicate the sta-
tistical significance (p<0.01) over ergodic0–2, concat1–2, and random, respectively. Bold font indicates
the best value within each row (except for ‘supervised’).

where Ri and Hi denote ratings at i-th positions.

• Mean Absolute Error per rating (MAE/r)

MAE/r(R, H) =
1
K

K∑

r=1

∑

i∈{i|Ri=r}
|Ri −Hi|

∑

i∈{i|Ri=r}
1

.

(5)

4.4 Evaluation Results
Tables 1 and 2 show the evaluation results for the
AD and AL domains, respectively. The MR and
MAE values are averaged over all dialogues. To
compare the means of the MR and MAE, we per-
formed a non-parametric multiple comparison test
[Steel-Dwass test (Dwass, 1960)]. We did not per-
form a statistical test for other criteria because it
was difficult to perform sample-wise comparison
for distributions. Naturally, ‘supervised’ is the
best performing model for all criteria in both do-
mains. Therefore, we focus on how much our pro-
posed models differ from the baseline (random)
and the upper bound (supervised).

In the AD domain, we find that ergodic0 and er-
godic1 performed rather poorly and concat1 and
concat2 performed fairly well, significantly out-
performing the random baseline. However, it is
also clear that we still need a great deal of im-
provement for our models to reach the level of
‘supervised’. A promising sign is that concat2
is not significantly different from ‘supervised’ in
smoothness. Here, ergodic0 and ergodic1 re-
turned the exact same results. This means that the
state transition paths did not go through the com-
mon states at all in ergodic1, suggesting that the
common states in ergodic1 have very broad out-
put distributions and the optimal path could not
go through the common states, instead preferring

other states having sharper distributions. How-
ever, this phenomenon was rightly avoided by in-
troducing more common states as seen in the re-
sults for ergodic2; nonetheless, as the results for
concat1 and concat2 indicate, the transition prob-
abilities have to be trained appropriately to obtain
better results.

In the AL domain, although the tendency of
the evaluation results is the same as that for the
AD domain, concat2 is clearly the best perform-
ing model. It outperformed other models in al-
most all cases except for “Good Listener” for
which concat1 performed better. In fact, the MR/r
and MAE/r of concat1 are quite close to those of
‘supervised’, suggesting the potential of our ap-
proach.

Overall, although we still need further improve-
ment in order for our models to be closer to the
upper bound, we showed that we can, to some ex-
tent, predict user satisfaction transitions in a dia-
logue only from overall ratings of dialogues using
our proposed HMMs. We also showed that model
topologies and learning methods can make signif-
icant differences. Especially, we found the intro-
duction of common states to be crucial in making
appropriate models for prediction. Since our mod-
els, especially concat2, significantly outperformed
the baseline, we believe that our approach can be
one of the viable options for automatically predict-
ing user satisfaction transitions when there exist
only overall rating data.

5 Summary and Future Work

We presented a novel approach for modeling user
satisfaction transitions only from dialogues with
overall ratings. The experimental results show that
it is possible to predict user satisfaction transi-

24



Criterion random ergodic0 ergodic1 ergodic2 concat1 concat2 supervised

Smoothness

MR 0.143e0e1e2 0.069 0.069 0.131e0e1 0.173e0e1 0.243c1e0e1e2r 0.439c1c2e0e1e2r
MAE 1.868e0e1e2 2.519 2.519 2.433 1.687e0e1e2r 1.594e0e1e2r 0.802c1c2e0e1e2r
KL 0.989 2.253 2.253 2.319 0.851 0.753 0.087
MR/r 0.141 0.118 0.118 0.156 0.161 0.167 0.231
MAE/r 2.289 2.500 2.500 2.492 2.093 2.077 1.868

Closeness

MR 0.143e0e1 0.050 0.050 0.175e0e1 0.158e0e1 0.263c1e0e1e2r 0.425c1c2e0e1e2r
MAE 1.849e0e1e2 2.357 2.357 2.316 1.778e0e1e2 1.562e0e1e2r 0.890c1c2e0e1e2r
KL 1.022 2.137 2.137 2.220 1.155 0.909 0.109
MR/r 0.143 0.090 0.090 0.122 0.117 0.159 0.237
MAE/r 2.281 2.577 2.577 2.811 2.260 2.039 1.972

Good Listener

MR 0.143e0e1 0.075 0.075 0.145e0e1 0.199e0e1 0.206e0e1e2 0.422c1c2e0e1e2r
MAE 1.890e0e1e2 2.237 2.237 2.150 1.634e0e1e2r 1.634e0e1e2r 0.852c1c2e0e1e2r
KL 0.945 1.738 1.738 1.782 0.924 0.824 0.087
MR/r 0.143 0.121 0.121 0.184 0.224 0.200 0.227
MAE/r 2.284 2.358 2.358 2.236 1.911 2.083 1.769

Table 2: Evaluation results for the AL domain. See Table 1 for the notations in the table.

tions to some extent by our approach and that in-
troducing common states and concatenated train-
ing can significantly improve prediction accuracy.
For improvement, we plan to explore new dialogic
features for emissions, different topologies, and
other optimization functions, such as discrimina-
tive ones. We also need to validate our approach
using dialogue-act recognition results instead of
hand-labeled dialogue-acts. We also want to ap-
ply our approach to sequence mining in dialogues
where we have categories instead of ratings for di-
alogues. It is also necessary to test whether our
HMMs can be generalized over different raters,
since user satisfaction ratings may differ greatly
among individuals. Although there remain such
issues, we believe we have presented a new di-
rection in automatic evaluation of dialogues and
the experimental results show that our approach is
promising.

References
Meyer Dwass. 1960. Some k-sample rank-order tests. In

Ingram Olkin et al., editor, Contributions to Probability
and Statistics, pages 198–202. Stanford University Press.

Klaus-Peter Engelbrecht, Florian Gödde, Felix Hartard,
Hamed Ketabdar, and Sebastian Möller. 2009. Model-
ing user satisfaction with hidden Markov models. In Proc.
SIGDIAL, pages 170–177.

Keelan Evanini, Phillip Hunter, Jackson Liscombe, David
Suendermann, Krishna Dayanidhi, and Roberto Pierac-
cini. 2008. Caller experience: A method for evaluating
dialog systems and its automatic prediction. In Proc. SLT,
pages 129–132.

Allen L. Gorin, Giuseppe Riccardi, and Jerry H. Wright.
1997. How may I help you? Speech Communication,
23(1-2):113–127.

Sunao Hara, Norihide Kitaoka, and Kazuya Takeda. 2010.
Estimation method of user satisfaction using N-gram-
based dialog history model for spoken dialog system. In
Proc. LREC, pages 78–83.

Ota Herm, Alexander Schmitt, and Jackson Liscombe. 2008.
When calls go wrong: How to detect problematic calls
based on log-files and emotions? In Proc. INTER-
SPEECH, pages 463–466.

Ryuichiro Higashinaka, Kohji Dohsaka, and Hideki Isozaki.

2008. Effects of self-disclosure and empathy in human-
computer dialogue. In Proc. SLT, pages 109–112.

Julia Hirschberg, Diane Litman, and Marc Swerts. 2004.
Prosodic and other cues to speech recognition failures.
Speech Communication, 43:155–175.

Chiori Hori, Kiyonori Ohtake, Teruhisa Misu, Hideki Kash-
ioka, and Satoshi Nakamura. 2008. Dialog management
using weighted finite-state transducers. In Proc. INTER-
SPEECH, pages 211–214.

Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii. 2009.
Evaluation method of non-task-oriented dialogue system
using HMM. IEICE Transactions on Information and Sys-
tems, J92-D(4):542–551.

Woosung Kim. 2007. Online call quality monitoring for
automating agent-based call centers. In Proc. INTER-
SPEECH, pages 130–133.

Kai-Fu Lee. 1989. Automatic speech recognition: the de-
velopment of the SPHINX system. Kluwer Academic Pub-
lishers.

Toyomi Meguro, Ryuichiro Higashinaka,Kohji Dohsaka,Ya-
suhiro Minami, and Hideki Isozaki. 2009. Analysis of
listening-oriented dialogue for building listening agents.
In Proc. SIGDIAL, pages 124–127.

Yasuhiro Minami, Akira Mori, Toyomi Meguro, Ryuichiro
Higashinaka, Kohji Dohsaka, and Eisaku Maeda. 2009.
Dialogue control algorithm for ambient intelligence based
on partially observable Markov decision processes. In
Proc. IWSDS, pages 254–263.

Sebastian Möller, Klaus-Peter Engelbrecht, and Robert
Schleicher. 2008. Predicting the quality and usability of
spoken dialogue services. Speech Communication, 50(8-
9):730–744.

Lawrence R. Rabiner. 1990. A tutorial on hidden Markov
models and selected applications in speech recognition.
Readings in speech recognition, 53(3):267–296.

Katsuhiko Shirai. 1996. Modeling of spoken dialogue with
and without visual information. In Proc. ICSLP, vol-
ume 1, pages 188–191.

Marilyn A. Walker, Diane Litman, Candace A. Kamm, and
Alicia Abella. 1997. PARADISE: A framework for evalu-
ating spoken dialogue agents. In Proc. EACL, pages 271–
280.

Marilyn A. Walker, Irene Langkilde-Geary, Helen Wright
Hastie, Jerry Wright, and Allen Gorin. 2002. Automat-
ically training a problematic dialogue predictor for a spo-
ken dialogue system. Journal of Artificial Intelligence Re-
search, 16(1):293–319.

Jason D. Williams and Steve Young. 2007. Partially ob-
servable Markov decision processes for spoken dialog sys-
tems. Computer Speech & Language, 21(2):393–422.

25



Appendix A. HMM obtained by concat2 for Willingness rating in the AD domain.
This HMM is the model obtained for one of the folds in the experiment. Square and oval states emit
a system’s dialogue-act and a user’s dialogue-act, respectively. Emissions (dialogue-acts) are shown in
each state as a table with their probabilities. Only the emissions and transitions over the probability of
0.1 are displayed for the sake of brevity. Here, ‘pi’ denotes initial probability.

ra
ti

n
g

:0

ra
ti

n
g

:1
ra

ti
n

g
:2

ra
ti

n
g

:3
ra

ti
n

g
:4

ra
ti

n
g

:5

ra
ti

n
g

:6
ra

ti
n

g
:7

S
Y

S
T

E
M

 (
p

i:
 0

.0
0

)
A

C
K

0
.3

5
D

IS
C

-A
G

R
E

E
-P

0
.1

4
D

IS
C

-D
IS

A
G

R
E

E
-P

0
.1

4
R

E
PE

A
T

0
.1

0

S
Y

S
T

E
M

 (
p

i:
 1

.0
0

)
D

IS
C

-R
-O

T
H

E
R

0
.1

7
Q

-D
IS

C
-R

0
.2

3
Q

-D
IS

C
-R

-O
T

H
E

R
0

.2
4

0
.3

5

U
S

E
R

 (
p

i:
 0

.0
0

)
A

C
K

0
.1

4
D

IS
C

-R
-O

T
H

E
R

0
.3

0

0
.1

3

0
.3

7

U
S

E
R

 (
p

i:
 0

.0
0

)
D

IS
C

-P
0

.4
0

D
IS

C
-R

0
.2

6

0
.3

1

0
.6

1

0
.1

2

0
.4

9

0
.1

5

S
Y

S
T

E
M

 (
p

i:
 0

.0
0

)
A

C
K

0
.8

1
E

M
P

0
.1

1

0
.2

7

0
.2

6

U
S

E
R

 (
p

i:
 0

.0
0

)
A

C
K

0
.3

0
D

IS
C

-O
T

H
E

R
0

.1
4

O
T

H
E

R
0

.2
7

Q
-D

IS
C

-P
0

.1
0

0
.3

3

0
.1

2

0
.2

60
.5

6

S
Y

S
T

E
M

 (
p

i:
 0

.0
0

)
A

C
K

0
.7

7
Q

-D
IS

C
-P

0
.1

4

0
.2

1

0
.2

2

0
.2

2
0

.1
1

U
S

E
R

 (
p

i:
 0

.0
0

)
O

T
H

E
R

0
.2

9
Q

-D
IS

C
-O

T
H

E
R

0
.2

0
Q

-D
IS

C
-R

0
.2

3

0
.2

3

0
.2

3

0
.1

6

0
.5

9

S
Y

S
T

E
M

 (
p

i:
 0

.0
0

)
D

IS
C

-A
G

R
E

E
-P

0
.3

3
D

IS
C

-P
0

.1
3

G
O

O
D

B
Y

E
0

.1
1

Q
-D

IS
C

-P
0

.2
2

Q
-D

IS
C

-P
-O

P
E

N
0

.1
8

0
.1

9

0
.2

2

U
S

E
R

 (
p

i:
 0

.0
0

)
D

IS
C

-P
0

.4
6

O
T

H
E

R
0

.2
6

Q
-D

IS
C

-R
0

.1
8

0
.4

6

0
.6

2

0
.1

2

0
.2

1

S
Y

S
T

E
M

 (
p

i:
 0

.0
0

)
D

IS
C

-D
IS

A
G

R
E

E
-P

0
.4

6
D

IS
C

-P
0

.2
0

D
IS

C
-P

-R
0

.1
2

G
O

O
D

B
Y

E
0

.1
5

0
.5

4

0
.1

9

U
S

E
R

 (
p

i:
 0

.0
0

)
D

IS
C

-P
-R

0
.1

0
G

R
E

E
T

IN
G

0
.1

4
Q

-D
IS

C
-O

T
H

E
R

0
.2

1
Q

-D
IS

C
-R

0
.1

9
R

E
S

0
.1

0

0
.2

2

0
.6

1

0
.2

00
.1

5

S
Y

S
T

E
M

 (
p

i:
 0

.0
0

)
D

IS
C

-A
G

R
E

E
-P

0
.2

8
D

IS
C

-R
0

.1
0

E
M

P
0

.1
4

R
E

PE
A

T
0

.1
7

0
.5

1

0
.1

5

0
.1

4

S
Y

S
T

E
M

 (
p

i:
 0

.0
0

)
D

IS
C

-D
IS

A
G

R
E

E
-P

0
.3

5
D

IS
C

-P
0

.1
3

D
IS

C
-R

0
.1

5
Q

-D
IS

C
-P

-O
P

E
N

0
.1

5

0
.2

5 0
.1

1
0

.3
2

0
.1

6

U
S

E
R

 (
p

i:
 0

.0
0

)
D

IS
C

-D
IS

A
G

R
E

E
-O

T
H

E
R

0
.1

6
D

IS
C

-O
T

H
E

R
0

.2
7

D
IS

C
-P

-R
0

.1
4

E
M

P
0

.1
8

R
E

PE
A

T
0

.1
6

0
.1

7

0
.5

2

0
.1

7

0
.1

9

S
Y

S
T

E
M

 (
p

i:
 0

.0
0

)
D

IS
C

-P
0

.1
4

D
IS

C
-R

0
.1

5
D

IS
C

-R
-O

T
H

E
R

0
.4

5
G

O
O

D
B

Y
E

0
.1

1
Q

-D
IS

C
-P

0
.1

6

0
.1

6

0
.1

6
0

.1
8

U
S

E
R

 (
p

i:
 0

.0
0

)
A

C
K

0
.3

1
D

IS
C

-O
T

H
E

R
0

.2
3

E
M

P
0

.1
0

R
E

PE
A

T
0

.1
0

0
.4

6

0
.5

1

0
.2

0

U
S

E
R

 (
p

i:
 0

.0
0

)
D

IS
C

-R
-O

T
H

E
R

0
.4

1
G

O
O

D
B

Y
E

0
.1

2
Q

-D
IS

C
-O

T
H

E
R

0
.2

2
R

E
PE

A
T

0
.1

2

0
.3

7

0
.1

6

0
.4

5

26



Appendix B. HMM obtained by concat1 for Good Listener rating in the AL domain.

This HMM is the model obtained for one of the folds in the experiment. Square and oval states emit a lis-
tener’s dialogue-act and a speaker’s dialogue-act, respectively. We find DICS-EVAL-NEG (self-disclosure
of one’s evaluation with a negative polarity) in the rating score 1 and DICS-EVAL-POS in the rating score
7, indicating that it may be better to make speakers talk about positive evaluations to be a good listener.

ra
ti

n
g

:0

ra
ti

n
g

:1
ra

ti
n

g
:2

ra
ti

n
g

:3
ra

ti
n

g
:4

ra
ti

n
g

:5
ra

ti
n

g
:6

ra
ti

n
g

:7

L
IS

T
E

N
E

R
 (

pi
: 

0.
16

)
G

R
E

E
T

IN
G

0
.1

3
D

IS
C

-F
A

C
T

0
.1

0
D

IS
C

-E
V

A
L

-P
O

S
0

.1
1

0
.2

2

S
P

E
A

K
E

R
 (

pi
: 

0.
41

)
G

R
E

E
T

IN
G

0
.1

3
D

IS
C

-F
A

C
T

0
.1

5
SY

N
PA

T
H

Y
0

.1
5

0
.3

5
0

.3
2

0
.2

6

L
IS

T
E

N
E

R
 (

pi
: 

0.
01

)
G

R
E

E
T

IN
G

0
.2

8
D

IS
C

-F
A

C
T

0
.1

9
IN

F
O

0
.1

5

0
.1

2

0
.2

4

0
.3

2

S
P

E
A

K
E

R
 (

pi
: 

0.
14

)
G

R
E

E
T

IN
G

0
.2

3
Q

-F
A

C
T

0
.1

3
D

IS
C

-E
V

A
L

-N
E

G
0

.1
2

0
.3

3

0
.2

4

0
.1

2

0
.3

2

0
.3

3

L
IS

T
E

N
E

R
 (

pi
: 

0.
00

)
D

IS
C

-F
A

C
T

0
.2

1
IN

F
O

0
.1

8

0
.1

6

0
.2

5

0
.2

3

S
P

E
A

K
E

R
 (

p
i:

 0
.0

0
)

IN
F

O
0

.2
2

Q
-F

A
C

T
0

.1
2

0
.3

6

0
.2

7

0
.1

4

0
.3

5

0
.2

4

L
IS

T
E

N
E

R
 (

pi
: 

0.
00

)
D

IS
C

-F
A

C
T

0
.3

2
Q

-F
A

C
T

0
.2

4
T

H
A

N
K

0
.1

0
Q

-I
N

F
O

0
.1

1

0
.1

5

0
.3

4

0
.1

8

S
P

E
A

K
E

R
 (

p
i:

 0
.0

0
)

D
IS

C
-F

A
C

T
0

.3
4

D
IS

C
-E

V
A

L
-N

E
G

0
.1

2

0
.3

3

0
.3

9

0
.1

3

0
.2

1

0
.2

7

L
IS

T
E

N
E

R
 (

pi
: 

0.
04

)
G

R
E

E
T

IN
G

0
.2

3
SY

N
PA

T
H

Y
0

.1
6

Q
-F

A
C

T
0

.1
4

0
.1

8

0
.3

8

0
.2

2

S
P

E
A

K
E

R
 (

pi
: 

0.
10

)
G

R
E

E
T

IN
G

0
.3

1
Q

-F
A

C
T

0
.1

0
D

IS
C

-E
V

A
L

-N
E

G
0

.1
5

0
.2

3

0
.2

4

0
.1

6

0
.4

1

0
.1

9

L
IS

T
E

N
E

R
 (

pi
: 

0.
00

)
SY

N
PA

T
H

Y
0

.1
7

D
IS

C
-E

V
A

L
-P

O
S

0
.2

0

0
.1

6

0
.2

3

0
.2

3

S
P

E
A

K
E

R
 (

p
i:

 0
.0

0
)

D
IS

C
-F

A
C

T
0

.2
7

SY
N

PA
T

H
Y

0
.2

0
D

IS
C

-E
V

A
L

-P
O

S
0

.1
7

0
.3

9

0
.2

2

0
.1

5

0
.3

1

0
.3

2

L
IS

T
E

N
E

R
 (

pi
: 

0.
00

)
SY

N
PA

T
H

Y
0

.1
9

D
IS

C
-E

V
A

L
-P

O
S

0
.2

4
C

O
N

F
IR

M
0

.1
4

0
.2

1

0
.2

4

0
.2

2

S
P

E
A

K
E

R
 (

p
i:

 0
.0

0
)

SY
N

PA
T

H
Y

0
.2

6
D

IS
C

-E
V

A
L

-P
O

S
0

.2
3

IN
F

O
0

.2
5

D
IS

C
-E

X
P

0
.1

1

0
.3

3

0
.2

1

0
.2

1

0
.2

9

0
.2

9

L
IS

T
E

N
E

R
 (

pi
: 

0.
04

)
G

R
E

E
T

IN
G

0
.2

4
D

IS
C

-E
V

A
L

-P
O

S
0

.2
2

IN
F

O
0

.1
1

C
O

N
F

IR
M

0
.1

3

0
.1

7

0
.2

4

0
.2

2

S
P

E
A

K
E

R
 (

pi
: 

0.
10

)
G

R
E

E
T

IN
G

0
.1

8
D

IS
C

-F
A

C
T

0
.2

6
D

IS
C

-E
V

A
L

-P
O

S
0

.1
6

IN
F

O
0

.1
8

0
.3

8

0
.2

6

0
.1

2

0
.3

1

0
.3

0

27


