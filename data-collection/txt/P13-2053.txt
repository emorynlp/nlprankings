



















































Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 294–299,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Context-Dependent Multilingual Lexical Lookup for Under-Resourced
Languages

Lian Tze Lim*†
*SEST, KDU College Penang

Georgetown, Penang, Malaysia
liantze@gmail.com

Lay-Ki Soon and Tek Yong Lim
†FCI, Multimedia University

Cyberjaya, Selangor, Malaysia
{lksoon,tylim}@mmu.edu.my

Enya Kong Tang
Linton University College

Seremban, Negeri Sembilan, Malaysia
enyakong1@gmail.com

Bali Ranaivo-Malançon
FCSIT, Universiti Malaysia Sarawak,
Kota Samarahan, Sarawak, Malaysia
mbranaivo@fit.unimas.my

Abstract

Current approaches for word sense dis-
ambiguation and translation selection typ-
ically require lexical resources or large
bilingual corpora with rich information
fields and annotations, which are often
infeasible for under-resourced languages.
We extract translation context knowledge
from a bilingual comparable corpora of a
richer-resourced language pair, and inject
it into a multilingual lexicon. The multilin-
gual lexicon can then be used to perform
context-dependent lexical lookup on texts
of any language, including under-resourced
ones. Evaluations on a prototype lookup
tool, trained on a English–Malay bilingual
Wikipedia corpus, show a precision score
of 0.65 (baseline 0.55) and mean recip-
rocal rank score of 0.81 (baseline 0.771).
Based on the early encouraging results,
the context-dependent lexical lookup tool
may be developed further into an intelligent
reading aid, to help users grasp the gist of
a second or foreign language text.

1 Introduction

Word sense disambiguation (WSD) is the task of
assigning sense tags to ambiguous lexical items
(LIs) in a text. Translation selection chooses target
language items for translating ambiguous LIs in a
text, and can therefore be viewed as a kind of WSD
task, with translations as the sense tags. The trans-
lation selection task may also be modified slightly
to output a ranked list of translations. This then re-
sembles a dictionary lookup process as performed
by a human reader when reading or browsing a text
written in a second or foreign language. For conve-
nience’s sake, we will call this task (as performed

via computational means) context-dependent lexi-
cal lookup. It can also be viewed as a simplified
version of the Cross-Lingual Lexical Substitution
(Mihalcea et al., 2010) and Cross-Lingual Word
Sense Disambiguation (Lefever and Hoste, 2010)
tasks, as defined in SemEval-2010.

There is a large body of work around WSD and
translation selection. However, many of these ap-
proaches require lexical resources or large bilin-
gual corpora with rich information fields and an-
notations, as reviewed in section 2. Unfortunately,
not all languages have equal amounts of digital re-
sources for developing language technologies, and
such requirements are often infeasible for under-
resourced languages.

We are interested in leveraging richer-resourced
language pairs to enable context-dependent lexical
lookup for under-resourced languages. For this pur-
pose, we model translation context knowledge as a
second-order co-occurrence bag-of-words model.
We propose a rapid approach for acquiring them
from an untagged, comparable bilingual corpus
of a (richer-resourced) language pair in section 3.
This information is then transferred into a multilin-
gual lexicon to perform context-dependent lexical
lookup on input texts, including those in an under-
resourced language (section 4). Section 5 describes
a prototype implementation, where translation con-
text knowledge is extracted from a English–Malay
bilingual corpus to enrich a multilingual lexicon
with six languages. Results from a small experi-
ment are presented in 6 and discussed in section 7.
The approach is briefly compared with some related
work in section 8, before concluding in section 9.

2 Typical Resource Requirements for
Translation Selection

WSD and translation selection approaches may be
broadly classified into two categories depending

294



on the type of learning resources used: knowledge-
and corpus-based. Knowledge-based approaches
make use of various types of information from
existing dictionaries, thesauri, or other lexical re-
sources. Possible knowledge sources include defi-
nition or gloss text (Banerjee and Pedersen, 2003),
subject codes (Magnini et al., 2001), semantic net-
works (Shirai and Yagi, 2004; Mahapatra et al.,
2010) and others.

Nevertheless, lexical resources of such rich con-
tent types are usually available for medium- to rich-
resourced languages only, and are costly to build
and verify by hand. Some approaches therefore
turn to corpus-based approaches, use bilingual cor-
pora as learning resources for translation selection.
(Ide et al., 2002; Ng et al., 2003) used aligned cor-
pora in their work. As it is not always possible to
acquire parallel corpora, comparable corpora, or
even independent second-language corpora have
also been shown to be suitable for training pur-
poses, either by purely numerical means (Li and Li,
2004) or with the aid of syntactic relations (Zhou
et al., 2001). Vector-based models, which capture
the context of a translation or meaning, have also
been used (Schütze, 1998; Papp, 2009). For under-
resourced languages, however, bilingual corpora of
sufficient size may still be unavailable.

3 Enriching Multilingual Lexicon with
Translation Context Knowledge

Corpus-driven translation selection approaches typ-
ically derive supporting semantic information from
an aligned corpus, where a text and its translation
are aligned at the sentence, phrase and word level.
However, aligned corpora can be difficult to ob-
tain for under-resourced language pairs, and are
expensive to construct.

On the other hand, documents in a comparable
corpus comprise bilingual or multilingual text of
a similar nature, and need not even be exact trans-
lations of each other. The texts are therefore un-
aligned except at the document level. Comparable
corpora are relatively easier to obtain, especially
for richer-resourced languages.

3.1 Overview of Multilingual Lexicon

Entries in our multilingual lexicon are organised as
multilingual translation sets, each corresponding to
a coarse-grained concept, and whose members are
LIs from different languages {L1, . . . , LN} con-
veying the same concept. We denote an LI as

«item», sometimes with the 3-letter ISO language
code in underscript when necessary: «item»eng. A
list of 3-letter ISO language codes used in this pa-
per is given in Appendix A.

For example, following are two translation sets
containing different senses of English «bank» (‘fi-
nancial institution’ and ‘riverside land’):

TS 1 = {«bank»eng, «bank»msa, «银行»zho, . . .}
TS 2 = {«bank»eng, «tebing»msa, «岸»zho, . . .}.

Multilingual lexicons with under-resourced lan-
guages can be rapidly bootstrapped from simple
bilingual translation lists (Lim et al., 2011). Our
multilingual lexicon currently contains 24371 En-
glish, 13226 Chinese, 35640 Malay, 17063 French,
14687 Thai and 5629 Iban LIs.

3.2 Extracting Translation Context
Knowledge from Comparable Corpus

We model translation knowledge as a bag-of-words
consisting of the context of a translation equiva-
lence in the corpus. We then run latent seman-
tic indexing (LSI) (Deerwester et al., 1990) on a
comparable bilingual corpora. A vector is then ob-
tained for each LI in both languages, which may
be regarded as encoding some translation context
knowledge.

While LSI is more frequently used in informa-
tion retrieval, the translation knowledge acquisi-
tion task can be recast as a cross-lingual indexing
task, following (Dumais et al., 1997). The underly-
ing intuition is that in a comparable bilingual cor-
pus, a document pair about finance would be more
likely to contain English «bank»eng and Malay
«bank»msa (‘financial institution’), as opposed to
Malay «tebing»msa (‘riverside’). The words ap-
pearing in this document pair would then be an
indicative context for the translation equivalence
between «bank»eng and «bank»msa. In other words,
the translation equivalents present serve as a kind
of implicit sense tag.

Briefly, a translation knowledge vector is ob-
tained for each multilingual translation set from a
bilingual comparable corpus as follows:

1. Each bilingual pair of documents is merged
as one single document, with each LI tagged
with its respective language code.

2. Pre-process the corpus, e.g. remove closed-
class words, perform stemming or lemmati-
sation, and word segmentation for languages
without word boundaries (Chinese, Thai).

295



3. Construct a term-document matrix (TDM), us-
ing the frequency of terms (each made up by
a LI and its language tag) in each document.
Apply further weighting, e.g. TF-IDF, if nec-
essary.

4. Perform LSI on the TDM. A vector is then
obtained for every LI in both languages.

5. Set the vector associated with each translation
set to be the sum of all available vectors of its
member LIs.

4 Context-Dependent Lexical Lookup

Given an input text in language Li (1 ≤ i ≤ N ),
the lookup module should return a list of multilin-
gual translation set entries, which would contain
L1, L2, . . . , LN translation equivalents of LIs in
the input text, wherever available. For polysemous
LIs, the lookup module should return translation
sets that convey the appropriate meaning in context.

For each input text segment Q (typically a sen-
tence), a ‘query vector’, VQ is computed by taking
the vectorial sum of all open class LIs in the in-
put Q. For each LI l in the input, the list of all
translation sets containing l, is retrieved into TS l.
TS l is then sorted in descending order of

CSim(Vt, VQ) =
Vt · VQ
|Vt| × |VQ|

(i.e. the cosine similarity between the query vector
VQ and the translation set candidate t’s vector) for
all t ∈ TS l.

If the language of input Q is not present in
the bilingual training corpus (e.g. Iban, an under-
resourced language spoken in Borneo), VQ is then
computed as the sum of all vectors associated with
all translation sets in TS l. For example, given the
Iban sentence ‘Lelaki nya tikah enggau emperaja
iya, siko dayang ke ligung’ (‘he married his sweet-
heart, a pretty girl’), VQ would be computed as

VQ =
∑

V (lookup(«lelaki»iba))

+
∑

V (lookup(«tikah»iba))

+
∑

V (lookup(«emperaja»iba))

+
∑

V (lookup(«dayang»iba))

+
∑

V (lookup(«ligung»iba))

where the function lookup(w) returns the transla-
tion sets containing LI w.

5 Prototype Implementation

We have implemented LEXICALSELECTOR, a pro-
totype context-dependent lexical lookup tool in
Java, trained on a English–Malay bilingual cor-
pus built from Wikipedia articles. Wikipedia ar-
ticles are freely available under a Creative Com-
mons license, thus providing a convenient source
of bilingual comparable corpus. Note that while
the training corpus is English–Malay, the trained
lookup tool can be applied to texts of any language
included in the multilingual dictionary.

Malay Wikipedia articles1 and their correspond-
ing English articles of the same topics2 were first
downloaded. To form the bilingual corpus, each
Malay article is concatenated with its correspond-
ing English article as one document.

The TDM constructed from this corpus con-
tains 62 993 documents and 67 499 terms, includ-
ing both English and Malay items. The TDM is
weighted by TF-IDF, then processed by LSI using
the Gensim Python library3. The indexing process,
using 1000 factors, took about 45 minutes on a
MacBook Pro with a 2.3GHz processor and 4GB
RAM. The vectors obtained for each English and
Malay LIs were then used to populate the transla-
tion context knowledge vectors of translation set
in a multilingual lexicon, which comprise six lan-
guages: English, Malay, Chinese, French, Thai and
Iban.

As mentioned earlier, LEXICALSELECTOR can
process texts in any member languages of the mul-
tilingual lexicon, instead of only the languages of
the training corpus (English and Malay). Figure 1
shows the context-depended lexical lookup out-
puts for the Iban input ‘Lelaki nya tikah enggau
emperaja iya, siko dayang ke ligung’. Note that
«emperaja» is polysemous (‘rainbow’ or ‘lover’),
but is successfully identified as meaning ‘lover’ in
this sentence.

6 Early Experimental Results

80 input sentences containing LIs with translation
ambiguities were randomly selected from the Inter-
net (English, Malay and Chinese) and contributed
by a native speaker (Iban). The test words are:

• English «plant» (vegetation or factory),

1http://dumps.wikimedia.org/mswiki/
2http://en.wikipedia.org/wiki/Special:

Export
3http://radimrehurek.com/gensim/

296



Figure 1: LEXICALSELECTOR output for Iban input ‘Lelaki nya tikah enggau emperaja iya, siko dayang
ke ligung’. Only top ranked translation sets are shown.

• English «bank» (financial institution or river-
side land),

• Malay «kabinet» (governmental Cabinet or
household furniture),

• Malay «mangga» (mango or padlock),
• Chinese «谷» (gù, valley or grain) and
• Iban «emperaja» (rainbow or lover).
Each test sentence was first POS-tagged auto-

matically based on the Penn Treebank tagset. The
English test sentences were lemmatised and POS-
tagged with the Stanford Parser.4 The Chinese test
sentences segmented with the Stanford Chinese
Word Segmenter tool.5 For Malay POS-tagging,
we trained the QTag tagger6 on a hand-tagged
Malay corpus, and applied the trained tagger on our
test sentences. As we lacked a Iban POS-tagger,
the Iban test sentences were tagged by hand. LIs
of each language and their associated vectors can
then be retrieved from the multilingual lexicon.

The prototype tool LEXICALSELECTOR then
computes the CSim score and ranks potential trans-
lation sets for each LI in the input sentences
(ranking strategy wiki-lsi). The baseline strat-
egy (base-freq) selects the translation set whose
members occur most frequently in the bilingual
Wikipedia corpus.

As a comparison, the English, Chinese and
Malay test sentences were fed to Google Trans-
late7 and translated into Chinese, Malay and En-
glish. (Google Translate does not support Iban
currently.) The Google Translate interface makes
available the ranked list of translation candidates
for each word in an input sentence, one language

4http://www-nlp.stanford.edu/software/
lex-parser.shtml

5http://nlp.stanford.edu/software/segmenter.
shtml

6http://phrasys.net/uob/om/software
7http://translate.google.com on 3 October 2012

at a time.The translated word for each of the input
test word can therefore be noted. The highest rank
of the correct translation for the test words in En-
glish/Chinese/Malay are used to evaluate goog-tr.

Two metrics were used in this quick evaluation.
The first metric is by taking the precision of the first
translation set returned by each ranking strategy,
i.e. whether the top ranked translation set contains
the correct translation of the ambiguous item. The
precision metric is important for applications like
machine translation, where only the top-ranked
meaning or translation is considered.

The results may also be evaluated similar to a
document retrieval task, i.e. as a ranked lexical
lookup for human consumption. This is measured
by the mean reciprocal rank (MRR), the average
of the reciprocal ranks of the correct translation set
for each input sentence in the test set T :

MRR =
1

|T |

|T |∑

i=1

1

ranki

The results for the three ranking strategies are
summarised in Table 1. For the precision metric,
wiki-lsi scored 0.650 when all 80 input sen-
tences are tested, while the base-freq baseline
scored 0.550. goog-tr has the highest precision
at 0.797. However, if only the Chinese and Malay
inputs — which has less presence on the Inter-
net and ‘less resource-rich’ than English — were
tested (since goog-tr cannot accept Iban inputs),
wiki-lsi and goog-tr actually performs equally
well at 0.690 precision.

In our evaluation, the MRR score of wiki-lsi
is 0.810, while base-freq scored 0.771.
wiki-lsi even outperforms goog-tr when
only the Chinese and Malay test sentences are
considered for the MRR metric, as goog-tr

297



Table 1: Precision and MRR scores of context-
dependent lexical lookup

Incl. Eng. & Iban W/o Eng. & Iban
Strategy

Precision MRR Precision MRR

wiki-lsi 0.650 0.810 0.690 0.845
base-freq 0.550 0.771 0.524 0.762
goog-tr 0.797 0.812 0.690 0.708

did not present the correct translation in its list
of alternative translation candidates for some
test sentences. This suggests that the LSI-based
translation context knowledge vectors would be
helpful in building an intelligent reading aid.

7 Discussion

wiki-lsi performed better than base-freq for
both the precision and the MRR metrics, although
further tests is warranted, given the small size of
the current test set. While wiki-lsi is not yet
sufficiently accurate to be used directly in an MT
system, it is helpful in producing a list of ranked
multilingual translation sets depending on the input
context, as part of an intelligent reading aid. Specif-
ically, the lookup module would have benefited if
syntactic information (e.g. syntactic relations and
parse trees) was incorporated during the training
and testing phase. This would require more time
in parsing the training corpus, as well as assuming
that syntactic analysis tools are available to pro-
cess test sentences of all languages, including the
under-resourced ones.

Note that even though the translation context
knowledge vectors were extracted from an English–
Malay corpus, the same vectors can be applied on
Chinese and Iban input sentences as well. This
is especially significant for Iban, which otherwise
lacks resources from which a lookup or disambigua-
tion tool can be trained. Translation context knowl-
edge vectors mined via LSI from a bilingual com-
parable corpus, therefore offers a fast, low cost and
efficient fallback strategy for acquiring multilin-
gual translation equivalence context information.

8 Related Work

Basile and Semeraro (2010) also used Wikipedia
articles as a parallel corpus for their participation
in the SemEval 2010 Cross-Lingual Lexical Sub-
stitution task. Both training and test data were for
English–Spanish. The idea behind their system

is to count, for each potential Spanish candidate,
the number of documents in which the target En-
glish word and the Spanish candidate occurs in
an English–Spanish document pair. In the task’s
‘best’ evaluation (which is comparable to our ‘Preci-
sion’ metric), Basile and Semeraro’s system scored
26.39 precision on the trial data and 19.68 preci-
sion on the SemEval test data. This strategy of
selecting the most frequent translation is similar to
our base-freq baseline strategy.

Sarrafzadeh et al. (2011) also tackled the prob-
lem of cross-lingual disambiguation for under-
resourced language pairs (English–Persian) using
Wikipedia articles, by applying the one sense per
collocation and one sense per discourse heuristics
on a comparable corpus. The authors incorporated
English and Persian wordnets in their system, thus
achieving 0.68 for the ‘best sense’ (‘Precision’)
evaluation. However, developing wordnets for new
languages is no trivial effort, as acknowledged by
the authors.

9 Conclusion

We extracted translation context knowledge from a
bilingual comparable corpus by running LSI on the
corpus. A context-dependent multilingual lexical
lookup module was implemented, using the cosine
similarity score between the vector of the input
sentence and those of candidate translation sets to
rank the latter in order of relevance. The precision
and MRR scores outperformed Google Translate’s
lexical selection for medium- and under-resourced
language test inputs. The LSI-backed translation
context knowledge vectors, mined from bilingual
comparable corpora, thus provide an fast and af-
fordable data source for building intelligent reading
aids, especially for under-resourced languages.

Acknowledgments

The authors thank Multimedia University and Uni-
versiti Malaysia Sarawak for providing support and
resources during the conduct of this study. We also
thank Panceras Talita for helping to prepare the
Iban test sentences for context-dependent lookup.

A 3-Letter ISO Language Codes

Code Language Code Language

eng English msa Malay
zho Chinese fra French
tha Thai iba Iban

298



References
Satanjeev Banerjee and Ted Pedersen. 2003. Extended

gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Con-
ference on Artificial Intelligence, pages 805–810.

Pierpaolo Basile and Giovanni Semeraro. 2010. UBA:
Using automatic translation and Wikipedia for cross-
lingual lexical substitution. In Proceedings of the
5th International Workshop on Semantic Evaluation
(SemEval 2010), pages 242–247, Uppsala, Sweden.

Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.

Susan T. Dumais, Michael L. Littman, and Thomas K.
Landauer. 1997. Automatic cross-language re-
trieval using latent semantic indexing. In AAAI97
Spring Symposium Series: Cross Language Text and
Speech Retrieval, pages 18–24, Stanford University.

Nancy Ide, Tomaz Erjavec, and Dan Tufiş. 2002.
Sense discrimination with parallel corpora. In Pro-
ceedings of the SIGLEX/SENSEVAL Workshop on
Word Sense Disambiguation: Recent Successes and
Future Directions, pages 54–60, Philadelphia, USA.

Els Lefever and Véronique Hoste. 2010. SemEval-
2010 Task 3: Cross-lingual word sense disambigua-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval 2010), Upp-
sala, Sweden.

Hang Li and Cong Li. 2004. Word translation disam-
biguation using bilingual bootstrapping. Computa-
tional Linguistics, 30(1):1–22.

Lian Tze Lim, Bali Ranaivo-Malançon, and Enya Kong
Tang. 2011. Low cost construction of a multilingual
lexicon from bilingual lists. Polibits, 43:45–51.

Bernardo Magnini, Carlo Strapparava, Giovanni Pez-
zulo, and Alfio Gliozzo. 2001. Using domain
information for word sense disambiguation. In
Proceedings of the 2nd International Workshop on
Evaluating Word Sense Disambiguation Systems
(SENSEVAL-2), pages 111–114, Toulouse, France.

Lipta Mahapatra, Meera Mohan, Mitesh M. Khapra,
and Pushpak Bhattacharyya. 2010. OWNS: Cross-
lingual word sense disambiguation using weighted
overlap counts and Wordnet based similarity mea-
sures. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval 2010), Upp-
sala, Sweden.

Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. SemEval-2010 Task 2: Cross-lingual lexical
substitution. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval 2010),
Uppsala, Sweden.

Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting parallel texts for word sense disambigua-
tion: An empirical study. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 455–462, Sapporo, Japan.

Gyula Papp. 2009. Vector-based unsupervised word
sense disambiguation for large number of contexts.
In Václav Matoušek and Pavel Mautner, editors,
Text, Speech and Dialogue, volume 5729 of Lec-
ture Notes in Computer Science, pages 109–115.
Springer Berlin Heidelberg.

Bahareh Sarrafzadeh, Nikolay Yakovets, Nick Cercone,
and Aijun An. 2011. Cross-lingual word sense dis-
ambiguation for languages with scarce resources. In
Proceedings of the 24th Canadian Conference on
Advances in Artificial Intelligence, pages 347–358,
St. John’s, Canada.

Hinrich Schütze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97–123.

Kiyoaki Shirai and Tsunekazu Yagi. 2004. Learn-
ing a robust word sense disambiguation model us-
ing hypernyms in definition sentences. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING 2004), pages 917–
923, Geneva, Switzerland. Association for Compu-
tational Linguistics.

Ming Zhou, Yuan Ding, and Changning Huang. 2001.
Improviging translation selection with a new transla-
tion model trained by independent monolingual cor-
pora. Computational Linguistics and Chinese lan-
guage Processing, 6(1):1–26.

299


