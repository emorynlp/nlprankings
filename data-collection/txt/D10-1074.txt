










































Towards Conversation Entailment: An Empirical Investigation


Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756–766,
MIT, Massachusetts, USA, 9-11 October 2010. c©2010 Association for Computational Linguistics

Towards Conversation Entailment: An Empirical Investigation

Chen Zhang Joyce Y. Chai
Department of Computer Science and Engineering

Michigan State University
East Lansing, MI 48824, USA

{zhangch6, jchai}@cse.msu.edu

Abstract

While a significant amount of research has
been devoted to textual entailment, automated
entailment from conversational scripts has re-
ceived less attention. To address this limi-
tation, this paper investigates the problem of
conversation entailment: automated inference
of hypotheses from conversation scripts. We
examine two levels of semantic representa-
tions: a basic representation based on syntac-
tic parsing from conversation utterances and
an augmented representation taking into con-
sideration of conversation structures. For each
of these levels, we further explore two ways of
capturing long distance relations between lan-
guage constituents: implicit modeling based
on the length of distance and explicit mod-
eling based on actual patterns of relations.
Our empirical findings have shown that the
augmented representation with conversation
structures is important, which achieves the
best performance when combined with ex-
plicit modeling of long distance relations.

1 Introduction

Textual entailment has received increasing attention
in recent years (Dagan et al., 2005; Bar-Haim et al.,
2006; Giampiccolo et al., 2007; Giampiccolo et al.,
2008; Bentivogli et al., 2009). Given a segment from
a textual document, the task of textual entailment is
to automatically determine whether a given hypoth-
esis can be entailed from the segment. The capa-
bility of such kind of inference can benefit many
text-based applications such as information extrac-
tion and question answering.

Textual entailment has mainly focused on infer-
ence from written text in monologue. Recent years
also observed an increasing amount of conversa-
tional data such as conversation scripts of meetings,
call center records, court proceedings, as well as on-
line chatting. Although conversation is a form of
language, it is different from monologue text with
several unique characteristics. The key distinctive
features include turn-taking between participants,
grounding between participants, different linguistic
phenomena of utterances, and conversation impli-
catures. Traditional approaches dealing with tex-
tual entailment were not designed to handle these
unique conversation behaviors and thus to support
automated entailment from conversation scripts.

Example 1:
Conversation Segment:

B: My mother also was very very independent.
She had her own, still had her own little house
and still driving her own car,

A: Yeah.
B: at age eighty-three.

Hypothesis:
(1) B’s mother is eighty-three.
(2) B is eighty-three.

To address this limitation, our previous
work (Zhang and Chai, 2009) has initiated an
investigation on the problem of conversation en-
tailment. The problem was formulated as follows:
given a conversation discourse D and a hypothesis
H concerning its participant, the goal was to identify
whether D entails H. For instance, as in Example
1, the first hypothesis can be entailed from the

756



conversation segment while the second hypothesis
cannot. While our previous work has provided
some interesting preliminary observations, it mostly
focused on data collection and initial experiments
and analysis using a small set of development data.
It is not clear whether the previous results are
generally applicable, how different components in
the entailment framework interact with each other,
and how different representations may influence the
entailment outcome.

To reach a better understanding of conversation
entailment, we conducted a further investigation
based on the larger set of test data collected in our
previous work (Zhang and Chai, 2009). We specifi-
cally examined two levels of representations: a basic
representation based on syntactic parsing from con-
versation utterances and an augmented representa-
tion taking into consideration of conversation struc-
tures. For each of these levels, we further explored
two ways of capturing long distance relations: (1)
implicit modeling based on the length of distance
and (2) explicit modeling based on actual patterns
of relations. Our empirical findings have shown that
augmented representation with conversation struc-
tures is important in conversation entailment. Com-
bining conversation structures with explicit model-
ing of long distance relations results in the best per-
formance.

2 Related Work

Our work here is related to recent advances in tex-
tual entailment, automated processing of conversa-
tion scripts, and our initial investigation on conver-
sation entailment.

There is a large body of work on textual en-
tailment initiated by the Pascal Recognizing Tex-
tual Entailment (RTE) Challenges (Dagan et al.,
2005; Bar-Haim et al., 2006; Giampiccolo et al.,
2007; Giampiccolo et al., 2008; Bentivogli et al.,
2009). Different approaches have been developed,
for example, based on logic proving (Tatu and
Moldovan, 2005; Bos and Markert, 2005; Raina et
al., 2005) and graph match (Haghighi et al., 2005;
de Salvo Braz et al., 2005; MacCartney et al., 2006).
Supervised learning approaches have also been ap-
plied to measure the similarities between training
and testing pairs (Zanzotto and Moschitti, 2006). In

the most recent RTE Challenge (Bentivogli et al.,
2009), the best system achieves 73.5% of accuracy,
while the median performance among all partici-
pants is 60.4%. These results indicate that, while
progress has been made, textual entailment remains
a challenging problem.

As more and more conversation data becomes
available, researchers have investigated automated
processing of conversation data to acquire useful
information, for example, related to opinions (So-
masundaran et al., 2007; Somasundaran et al.,
2008; Somasundaran et al., 2009), biographic at-
tributes (Garera and Yarowsky, 2009), social net-
works (Jing et al., 2007), and agreements and
disagreements between participants (Galley et al.,
2004). Recent studies have also developed ap-
proaches to summarize conversations (Murray and
Carenini, 2008) and to model conversation struc-
tures (dialogue acts) from online Twitter conversa-
tions (Ritter et al., 2010). Here we address a dif-
ferent angle regarding conversation scripts, namely
conversation entailment.

In our previous work (Zhang and Chai, 2009),
we started an initial investigation on conversation
entailment. We have collected a dataset of 875
instances. Each instance consists of a conversa-
tion segment and a hypothesis (as described in Sec-
tion 1). The hypotheses are statements about conver-
sation participants and are further categorized into
four types: about their profile information, their be-
liefs and opinions, their desires, and their commu-
nicative intentions. We developed an approach that
is motivated by previous work on textual entailment.
We use clauses in the logic-based approaches as the
underlying representation of our system. Based on
this representation, we apply a two stage entailment
process similar to MacCartney et al. (2006) devel-
oped for textual entailment: an alignment stage fol-
lowed by an entailment stage.

Building upon our previous work, in this paper,
we systematically examine different representations
of the conversation segment and different modeling
of long distance relations between language con-
stituents. We compare the roles of these different
representations on the performance of entailment
prediction using a larger testing dataset that was not
previously evaluated. This analysis allows better un-
derstanding of the problem and provides insight on

757



potential solutions.

3 Overall Framework

In our previous work (Zhang and Chai, 2009), con-
versation entailment is formulated as the follow-
ing: given a conversation segment D which is rep-
resented by a set of clauses D = d1 ∧ . . . ∧ dm,
and a hypothesis H represented by another set of
clauses H = h1 ∧ . . . ∧ hn, the prediction on
whether D entails H is determined by the product
of probabilities that each hypothesis clause hj is
entailed from all the conversation segment clauses
d1 . . . dm as follows. This is based on a simple as-
sumption that whether a clause is entailed from a
conversation segment is conditionally independent
from other clauses.

P (D � H|D,H)
= P (D � h1, . . . , D � hn|D,h1, . . . , hn)

=
n∏

j=1

P (D � hj |D = d1 . . . dm, hj)

=
n∏

j=1

P (d1 . . . dm � hj |d1, . . . , dm, hj) (1)

A clause here is similar to a sentence in first-
order predicate calculus. It is made up by terms
and predicates. A term is either: 1) an entity
described by a noun phrase, e.g., John Lennon,
mother, or she; or 2) an action or event de-
scribed by a verb phrase, e.g., marry in “John
married Eva in 1940”. A predicate represents
either: 1) a property (i.e., unary) for a term,
e.g., Russian(company), or recently(visit);
or 2) a relation (i.e., binary) between two
terms, e.g., subj(visit, Prime Minister) and
obj(visit, Brazil) in “Prime Minister recently vis-
ited Brazil”.

Given the clause representation, we follow the
idea similar to MacCartney et al. (2006), and predict
the entailment decision in two stages of processing:
(1) an alignment model aligns terms in the hypothe-
sis to terms in the conversation segment; and (2) an
inference model predicts the entailment based on the
alignment between the hypothesis and the conversa-
tion segment.

3.1 Alignment Model

An alignment is defined as a mapping function g
between a term x in the conversation segment and a
term y in the hypothesis. g(x, y) = 1 if x and y are
aligned; otherwise g(x, y) = 0. It is possible that
multiple terms from the segment are mapped to one
term in the hypothesis (g(x1, y) = g(x2, y) = 1),
or vice versa (g(x, y1) = g(x, y2) = 1). To predict
these alignments, the problem is formulated as bi-
nary classification: given any two terms x from the
conversation and y from the hypothesis, decide the
value of their alignment function g(x, y).

3.2 Inference Model

Once an alignment between a hypothesis and a con-
versation segment is established, an inference model
is applied to predict whether the conversation seg-
ment entails the hypothesis given such alignment.
More specifically, as shown in Equation 1, given a
clause from the hypothesis hj , a set of clauses from
the conversation segment d1, . . . , dm, and an align-
ment g between them, the goal is to predict whether
d1, . . . , dm entails hj under the alignment g.

The prediction is treated differently according to
different types of clauses. If hj is a property clause
(i.e., takes one argument hj(·)), a property inference
model is applied; otherwise (i.e., relational clauses
with two arguments hj(·, ·)), a relational inference
model is applied.

In this paper we follow the same framework.
However our focus here is on the new question that
how different levels of semantic representation and
different approaches of modeling long distance rela-
tionship affect the alignment and inference models
as well as the overall entailment performance.

4 Semantic Representation

Given the clause representation described earlier,
an important question is what information from the
conversation segment should be captured and repre-
sented. To address this question, we examined two
levels of shallow semantic representation. The first
level is basic representation which only captures the
information from all the utterances in the conversa-
tion segment. The second representation includes
conversation structures (e.g., speakers and dialogue

758



acts). Next we use Example 2 to illustrate these rep-
resentations.

Example 2:
Conversation Segment:

B: Have you seen Sleeping with the Enemy?
A: No. I’ve heard that’s really great, though.
B: You have to go see that one.

Hypothesis:
B suggests A to watch Sleeping with the Enemy.

4.1 Basic Representation
The first representation is based on the syntactic
parsing from conversation utterances and we call it
a basic representation. Figure 1(a) shows an exam-
ple of dependency structures for several utterances
that are derived from the Stanford parser (Klein and
Manning, 2003), and Figure 1(b) shows the corre-
sponding clause representation. In the dependency
structure, the vertices represent entities (e.g., x1) and
actions (e.g., x3) within an utterance. They corre-
spond to terms in the clause representation. An edge
between vertices captures a dependency relation and
is represented as predicates in the clause representa-
tion. For example, the edge between x1 and x3 indi-
cates x1 is the subject of x3, which is represented by
the clause representation subj(x3, x1). Similar rep-
resentation also applies to the hypothesis as shown
in Figure 1(c), 1(d).

4.2 Augmented Representation
The second representation is built upon the basic
representation and incorporates conversation struc-
ture across turns and utterances. We call it an aug-
mented representation. Figure 2(a) shows the aug-
mented structures of the conversation segment and
Figure 2(b) shows the corresponding clause repre-
sentation. Compared to the basic representation,
there are two additional types of vertices (i.e., terms)
highlighted in the figures:

• Vertices representing utterances (e.g.,
u1 . . . u4). Their corresponding terms capture
the dialogue acts for the utterances (e.g.
u1 = yes no question). To focus our effort,
currently we only apply annotated dialogue
acts provided in the Switchboard corpus (God-
frey and Holliman, 1997). Two edges are

added to connect different utterances. The
first edge connects each utterance vertex to
the head of the corresponding utterance to
indicate the specific content of the utterance
(e.g., content(u1, x3)). The second edge con-
nects an utterance to its succeeding utterance
to indicate the temporal progression of the
conversation (e.g., follow(u2, u1)).

• Vertices representing speakers or participants
(e.g., sA, sB). One edge is added to
connect each utterance to its speaker (e.g.,
speaker(u1, sB)).

Note that since our clause representations are
mainly based on the dependency relations, they are
mostly syntactic-driven. However, it does capture
some shallow semantics such as who is the agent
(i.e., subject) or the patient (i.e., object) of an event.
The incorporation of speakers and dialogue acts in
our augmented representations provides additional
semantics of conversation discourse.

5 Modeling LDR

A critical part in predicting entailment is to recog-
nize the semantic relationship between two language
constituents, especially when these two constituents
are not directly related. In Figure 2(a), for exam-
ple, we want to recognize that x9 (You) is the (log-
ical) subject of x11 (see). Here we experimented
two ways of modeling such long distance relations
(LDR).

5.1 Implicit Modeling of LDR
The first method characterizes the relationship sim-
ply by the distance between two constituents in the
dependency structure (or augmented structure). For
example, in Figure 2(a) the distance between x11
and x9 is 3. We call this method an implicit mod-
eling of long distance relationship.

The advantage of implicit modeling is that it is
easy to implement based on the dependency struc-
ture. However, its limitation is that the distance mea-
sure does not capture sufficient information of se-
mantic relations between language constituents.

5.2 Explicit Modeling of LDR
The second way of modeling long distance relation-
ship is called explicit modeling. It uses a string to

759



B: Have you seen Sleeping with the Enemy?

A: No. I've heard that's really great, though.

B: You have to go see that one.

x9
x13

x12
x11

x10

x4 x1

x3

x2

x5

x8

x6
x7

obj(x3,x2)
subj(x3,x1)
aux(x3,x4)

x1=A
x2=Sleeping
with the Enemy
x3=seen, x4=have

obj(x11,x10)
obj(x12,x11)
obj(x13,x12)
subj(x13,x9)

x9=A, x10=one,
x11=see, x12=go, 
x13=have

subj(x7,x6)
obj(x8,x7)
subj(x8,x5)

x5=A, x6=that
x7=is really great
x8=have heard

ClausesTerms

(a) dependency structure of the conversation
utterances

B: Have you seen Sleeping with the Enemy?

A: No. I've heard that's really great, though.

B: You have to go see that one.

x9
x13

x12
x11

x10

x4 x1

x3

x2

x5

x8

x6
x7

obj(x3,x2)
subj(x3,x1)
aux(x3,x4)

x1=A
x2=Sleeping
with the Enemy
x3=seen, x4=have

obj(x11,x10)
obj(x12,x11)
obj(x13,x12)
subj(x13,x9)

x9=A, x10=one,
x11=see, x12=go, 
x13=have

subj(x7,x6)
obj(x8,x7)
subj(x8,x5)

x5=A, x6=that
x7=is really great
x8=have heard

ClausesTerms

(b) basic representation of the conver-
sation segment

x1

x2

x5

x4

x3

B suggests A to watch Sleeping with the Enemy.

subj

subj

obj

obj

B: Have you seen Sleeping with the Enemy?

A: No. I've heard that's really great, though.

B: You have to go see that one.

x9
x13

x12
x11

x10

x4 x1

x3

x2

x5

x8

x6
x7

u1

u2

u3

u4

sB

sA

(c) dependency structure of the hypothesis

obj(x3,x2), subj(x3,x1)
aux(x3,x4)

x1=A, x3=seen, x4=have
x2=Sleeping with the Enemy

obj(x11,x10), obj(x12,x11)
obj(x13,x12), subj(x13,x9)

x9=A, x10=one, x11=see
x12=go, x13=have

speaker(u4,sB)
content(u4,x13)
follow(u4,u3)

u4=viewpoint

subj(x7,x6), obj(x8,x7)
subj(x8,x5)

x5=A, x7=is really great
x6=that, x8=have heard

speaker(u3,sA)
content(u3.x8)
follow(u3,u2)

u3=statement

speaker(u2,sA)
follow(u2,u1)

u2=no_answer

speaker(u1,sB)
content(u1, x3)

sA, sB
u1=yes_no_question

ClausesTerms

subj(x4,x2)
obj(x4,x3)
subj(x5,x1)
obj(x5,x4)

x1=B, x2=A
x3=Sleeping
with the Enemy
x4=watch
x5=suggests

ClausesTerms

(d) representation of the hy-
pothesis

Figure 1: The dependency structures and corresponding basic representation of Example 2

x1

x2

x5

x4

x3

B suggests A to watch Sleeping with the Enemy.

subj

subj

obj

obj

B: Have you seen Sleeping with the Enemy?

A: No. I've heard that's really great, though.

B: You have to go see that one.

x9
x13

x12
x11

x10

x4 x1

x3

x2

x5

x8

x6
x7

u1

u2

u3

u4

sB

sA

(a) dependency and conversation structures of the conversation
segment

obj(x3,x2), subj(x3,x1)
aux(x3,x4)

x1=A, x3=seen, x4=have
x2=Sleeping with the Enemy

obj(x11,x10), obj(x12,x11)
obj(x13,x12), subj(x13,x9)

x9=A, x10=one, x11=see
x12=go, x13=have

speaker(u4,sB)
content(u4,x13)
follow(u4,u3)

u4=viewpoint

subj(x7,x6), obj(x8,x7)
subj(x8,x5)

x5=A, x7=is really great
x6=that, x8=have heard

speaker(u3,sA)
content(u3.x8)
follow(u3,u2)

u3=statement

speaker(u2,sA)
follow(u2,u1)

u2=no_answer

speaker(u1,sB)
content(u1, x3)

sA, sB
u1=yes_no_question

ClausesTerms

subj(x4,x2)
obj(x4,x3)
subj(x5,x1)
obj(x5,x4)

x1=B, x2=A
x3=Sleeping
with the Enemy
x4=watch
x5=suggests

ClausesTerms

(b) augmented representation of the conversation seg-
ment

Figure 2: The dependency and conversation structures and corresponding augmented representation of Example 2

760



describe the path from one constituent to the other:
v1e1 . . . vl−1el−1vl, where v1, . . . , vl are the vertices
on the path and e1, . . . , el−1 are the edges. Each vi
describes the type of the vertex in the dependency
structure, which is either a noun (N ), a verb (V ),
or an utterance (U ). Each ei describes whether the
edge is forward (→) or backward (←). For ex-
ample, in Figure 2(a), the path from x11 to x9 is
V → V → V ← N .

This kind of string representation of paths in syn-
tactic parse is known as a way of modeling “shal-
low semantics” between any two constituents in a
language structure. It is largely used in other NLP
tasks such as semantic role labeling (Pradhan et al.,
2008). The difference here is our paths are extracted
from dependency parses as opposed to traditional
constituent parses, and our paths also incorporate the
representation of conversation structures (e.g., utter-
ances and speakers).

6 Applications in Entailment Models

In this section we describe how different representa-
tions and modeling of LDR are used in the alignment
and inference models.

6.1 Applications in Alignment Model

Although a noun and a verb can potentially be
aligned, to simplify the problem, we restrict the
problem to the alignment between two nouns or two
verbs. We trained an alignment model for nouns and
one for verbs separately.

Table 1 summarizes a set of features used in the
alignment models. Most of these features are shared
by the model for noun alignment and the model for
verb alignment. These features include whether the
two strings are the same, two terms have the same
stem, the similarity between the two terms either
based on WordNet or distributional statistics (Lin,
1998).

To learn the alignment model for nouns, we anno-
tated the noun alignments for the development data
used in PASCAL RTE-3 Challenge (Giampiccolo et
al., 2007) and trained a logistic regression model
based on the features in Table 1. Cross-validation
on the same dataset shows relatively satisfying per-
formance (96.4% precision and 94.9% recall). In
this paper, we focus on the alignment between verbs

Noun Verb
Align. Align.

Verb be identification X
String equality X X
Stemmed equality X X
Acronym equality X
Named entity equality X
WordNet similarity X X
Distributional similarity X X
Subject consistency X
Object consistency X

Table 1: Features for alignment models

since it appears more difficult.
A major difference between noun alignment and

verb alignment is that, for verb alignment the con-
sistency of their arguments is also important. For
two events (described by two verbs) to be aligned, at
least their subjects (usually denoting the executers of
actions) and objects (usually denoting the receivers
of actions) should match to each other respectively.
Note that, although actions/events also depend on
other arguments or adjuncts, here we only consider
the subjects and objects and leave the consistency
check of other arguments/adjuncts to downstream
processes. Based on two different ways of model-
ing long distance relationship (as described in Sec-
tion 5), we explored two methods for modeling ar-
gument consistency (AC) in verb alignment models.

6.1.1 Implicit Modeling of AC
The first approach models argument consistency

based on implicit modeling of the relationship be-
tween a verb and its aligned subject/object. Specif-
ically, given a pair of verb terms (x, y) where x is
from the conversation segment and y is from the hy-
pothesis, let sy be the subject of y and sx be the
aligned entity of sy in the conversation (in case of
multiple alignments, sx is the one closest to x). The
subject consistency of the verbs (x, y) is then mea-
sured by the distance between sx and x in the de-
pendency structure. Similarly, the distance between
a verb and its aligned object is used as a measure of
the object consistency.

In Example 2, to decide whether the conversa-
tion term see (x11 in Figure 1(a), 1(b), and 2) and
the hypothesis term watch (x4 in Figure 1(c), 1(d))
should be aligned, we first identify the subject of x4
in the hypothesis, which is x2 (A). We then look for

761



x2’s alignments in the conversation segment, among
which x9 (You) is the closest to x11 (see). In Fig-
ure 2(a), we find the distance between x11 and x9 is
3.

Using the implicit modeling of argument consis-
tency, we follow the same approach as in our pre-
vious work (Zhang and Chai, 2009) and trained a
logistic regression model to predict verb alignment
based on the features in Table 1.

6.1.2 Explicit Modeling of AC
The second approach captures argument consis-

tency based on explicit modeling of the relationship
between a verb and its aligned subject (or object).
Given a pair of verb terms (x, y), let sy be the sub-
ject of y and sx be the aligned entity of sy in the
conversation closest to x, we use the string describ-
ing the path from x to sx as the feature to capture
subject consistency. For example, in Figure 2(a), the
path from x11 to x9 is V → V → V ← N .

This string representation of paths is used to cap-
ture both the subject consistency and the object con-
sistency. Since they are non-numerical features, and
the variability of their values can be extremely large,
so we applied an instance-based classification model
(e.g., k-nearest neighbor) to determine alignments
between verb terms. We measure the distance be-
tween two path features by their minimal string edit
distance, and then simply use the Euclidean distance
to measure the closeness between any two verbs.
Again this model is trained from our development
data described in Zhang and Chai (2009).

Figure 3 shows an example of alignment between
the conversation terms and hypothesis terms in Ex-
ample 2. Note that in this figure the alignment
between x5 = suggests from the hypothesis and
u4 = opinion from the conversation segment is a
pseudo alignment, which directly maps a verb term
in the hypothesis to an utterance term represented
by its dialogue act. This alignment is obtained by
following the same set of rules learned from the de-
velopment dataset as in (Zhang and Chai, 2009).

6.2 Applications in Inference Model

As mentioned earlier, once an alignment is estab-
lished, the inference model is to predict whether
each clause in the hypothesis is entailed from the
conversation segment. Two separate models were

x4=have
x5=A

x2=Sleeping 
with the Enemy

x1=A

x7=is really great

x10=one

u4=opinion

Conversation Segment

x3=Sleeping 
with the Enemy

x5=suggests

x2=A

x1=B

x4=watch

Hypothesis

x6=that

x11=see
x12=go
x13=have

x8=have heard

u3=statement
u2=no_answer
u1=yes_no_question

x3=seen

x9=A

sB
sA

Figure 3: The alignment result for Example 2

used to handle the inference of property clauses
(hj(x)) and and the inference of relational clauses
(hj(x, y)). Property clauses involve less variables
and are relatively simple, so we used the same prop-
erty inference model as in (Zhang and Chai, 2009).
Here we focus on relational inference model and ex-
amine how different modeling of long distance rela-
tionship may affect relation inference.

For a relation h between x and y to be entailed
from a conversation segment, we need to find a same
or similar relation in the conversation segment be-
tween x’s and y’s counterparts (i.e., aligned entities
of x and y in the conversation segment).

More specifically, given a relational clause from
the hypothesis, hj(x, y), we find the sets of
terms X ′ = {x′|x′ ∈ D, g(x′, x) = 1} and Y ′ =
{y′|y′ ∈ D, g(y′, y) = 1}, which are aligned with x
and y, respectively. We then find the closest re-
lation between these two sets of terms, (x∗, y∗),
such that the distance between x∗ and y∗ is the
smallest for any x∗ ∈ X ′ and y∗ ∈ Y ′. For in-
stance, in the hypothesis of Example 2 there are
terms x5=suggests and x4=watch, and a relational
clause obj(x5, x4) describing an action-object rela-
tion between them. Their counterparts in the con-

762



versation segment are X ′ = {u4=viewpoint} and
Y ′ = {x3=seen, x11=see}. So the closest pair of
terms between these two sets is u4 and x11. Conse-
quently, whether the target relational clause hj(x, y)
is entailed is determined by the relationship between
x∗ and y∗. Such relationship can be modeled either
implicitly or explicitly.

6.3 Implicit modeling of relation inference
In this model we follow the simple idea that the
shorter a path is between two terms, the more likely
these two terms have a direct relationship. So we
predefine a threshold, λL. We predict that hj(x, y) is
entailed if the distance between x∗ and y∗ is smaller
than λL. However, as can be seen, this distance does
not reflect whether the type of relationship between
x∗ and y∗ is similar to the relationship that holds be-
tween x and y.

6.4 Explicit modeling of relation inference
In order to capture more semantics from the rela-
tion between two terms, we use explicit modeling
of the relationship between terms x∗ and y∗. In
the previous example, the relationship between u4
and x11 is modeled by the path from u4 to x11,
U ← V ← V ← V .

Given this characterization, the prediction of
whether hj(x, y) is entailed from the conversation
segment is formulated as a binary classification
problem, using a k-nearest neighbor classification
model with following features:

1. Explicit modeling of long distance relationship,
i.e., the path from x∗ to y∗ in the dependency
structure of the conversation segment;

2. The types (N, V, or U) of x, y, x∗, and y∗;
3. The type of relation between x and y, for ex-

ample, obj in obj(x, y);
4. The order (i.e., before or after) between x and
y, and between x∗ and y∗;

5. The specific type of the hypothesis.

7 Evaluation and Analysis

We evaluated different model configurations using
our data1. This dataset consists of 291 development
instances and 584 testing instances. The hypotheses

1The data is available for download at http:
//links.cse.msu.edu:8000/lair/projects/
conversationentailment_data.html.

(a) Based on basic representation

(b) Based on augmented representation

Figure 4: Evaluation of verb alignment

were categorized into four types: (1) fact: profile
and social relations of conversation participants (ac-
counted for 47% of the development data and 49%
of the testing data); (2) belief: participants’ beliefs
and opinions (34% and 35%); (3) desire: partici-
pants’ desire of certain actions or outcomes (11%
and 4%); (4) intent: communicative intent that cap-
tures some perlocutionary force from one participant
to the other (e.g. A stops B from doing something;
A disagreees with B on something, 8% and 12%)

Note that in our original work (Zhang and Chai,
2009), only development data were used to show
some initial observations. Here we trained our mod-
els on the development data and results shown are
from the testing data.

7.1 Evaluation of Alignment Models

The evaluation of alignment models is based on pair-
wise decision. For each pair of terms (x, y), where
x is from a conversation segment and y is from
a hypothesis, we measure whether the model cor-
rectly predicts that the two terms should or should
not be aligned. Because the alignment classification
has extremely unbalanced classes, we use precision-
recall of true alignments as evaluation metrics.

Figure 4(a) and 4(b) shows the comparison (F-
measure) of two alignment models for verb align-

763



Figure 5: Evaluation of inference models based on different representations

ment, based on the basic representation and the aug-
mented representation, respectively. Note that we
cannot directly compare the results between these
two figures since they involve different number of
alignment instances2. Nevertheless, we can see the
overall trend within each figure: the explicit model
outperforms the implicit model. This suggests that
the explicit modeling of semantic relationship be-
tween verbs and arguments works better than the im-
plicit modeling used in previous work. Furthermore,
the improvement is most noticeable when hypothe-
ses are facts (24.8% with the basic representation
and 24.1% with the augmented representation), and
least when hypotheses are intents (12.2% with the
basic representation and 6.2% with the augmented
representation).

7.2 Evaluation of Inference Models

In order to compare different inference models, in
this section (and this section only) we use gold-
standard alignment results. They are obtained from
manual annotation in our evaluation. We evaluated
two inference models, one with implicit modeling
of long distance relationship and one with explicit
modeling. Evaluations were conducted based on
both the basic representation and the augmented rep-
resentation. Figure 5 shows the four groups of eval-
uation results.

Overall speaking, the augmented representation
outperforms the basic representation for both im-
plicit modeling and explicit modeling of long dis-
tance relationship (McNemar’s tests, p < 0.05). The
explicit model performs better than implicit model
only based on augmented representation (McNe-
mar’s test, p < 0.05).

2The alignment based on the augmented representation in
Figure 4(b) also includes pseudo alignments.

Clause Rep- Relation modeling Improve-
resentation Implicit Explicit ment
Basic 53.9% 53.9% 0
Augmented 54.8% 58.7% 3.9%

Table 2: Entailment performance with different represen-
tations and LDR modeling

The results were further broken down by different
hypothesis types. For the fact type of hypotheses,
there is no difference between different represen-
tations and modeling of long distance relationship.
This is not surprising since most hypotheses about
partipants’ profiling information can be inferred di-
rectly from the utterances. The augmented repre-
sentation affects the intent type of hypothesis most
significantly, so does the explicit modeling of long
distance relationship.

7.3 Interaction between Clause
Representations and LDR Modeling

It was shown in previous sections that the aug-
mented representation helps entailment prediction
compared to the basic representation. Here we want
to study how they interact with other entailment
components and what is their effect in the enhanced
modeling of long distance relations. Specifically, we
test the performance of implicit and explicit mod-
eling of long distance relations under two different
representation settings: the basic representation and
the augmented representation.

Table 2 compares the performance (accuracy) of
entailment models with different relationship mod-
eling. We can see that the explicit model makes im-
provement over the implicit model for augmented
representation (McNemar’s test, p < 0.05), while
no improvement is made for basic representation.
These evaluation results appear to suggest that there

764



is an interaction between clause representations and
semantic modeling of long distance relations: the
modeling of long distance relations between lan-
guage constituents appears only effective when con-
versation structure is incorporated in the representa-
tion.

It is interesting to see the difference in the predic-
tion performances on fact hypotheses and intent hy-
potheses. For fact, the most benefit of incorporating
explicit modeling of long distance relationship ap-
pears at the alignment stage, but not much at the in-
ference stage. However, this situation is different for
intent, where the benefit of explicitly modeling long
distance relationship mostly happened at the infer-
ence stage. This observation suggests that the effects
of different types of modeling may vary for different
types of hypotheses, which indicates that hypothesis
type dependent models may be beneficial.

8 Discussion and Conclusion

This paper presents an empirical investigation on
conversation entailment. We specifically examine
two levels of representation of conversation seg-
ments and two different ways of modeling long dis-
tance relations between language constituents. Our
findings indicate that, although traditional architec-
ture and approaches for textual entailment remain
important, additional representation and processing
that address conversation structures is critical. The
augmented representation with conversation struc-
tures, together with explicit modeling of semantic
relations between language constituents, results in
the best performance (58.7% accuracy).

The work here only represents an initial step to-
wards conversation entailment. Conversation phe-
nomena are rich and complex. Conversation entail-
ment is extremely difficult. Besides the same chal-
lenges faced by textual entailment, it is further com-
plicated by conversation implicature. Although our
current data enables us to start an initial investiga-
tion, its small size poses significant limitations on
technology development and evaluation. For ex-
ample, our studies have indicated hypothesis type-
dependent approaches may be beneficial, however
we do not have sufficient data to yield reasonable
models. A more systematical approach to collect
and create a larger set of data is crucial. Inno-

vative community-based approaches (e.g., through
web) for data collection and annotation can be pur-
sued in the future. As more techniques in semantic
processing (e.g., semantic role) become available,
future work should also capture deeper semantics,
address pragmatics, and incorporate richer world
knowledge.

Finally, as the technology in conversation entail-
ment is developed, its applications in NLP problems
should be explored. Example applications include
information extraction, question answering, summa-
rization from conversation scripts, and modeling of
conversation participants. These applications may
provide new insights on the nature of the conversa-
tion entailment problem and its potential solutions.

Acknowledgments

This work was supported by grant IIS-0347548 from
the National Science Foundation. We thank the
anonymous reviewers for their valuable comments
and suggestions.

References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo

Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, Venice, Italy.

Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
Proceedings of the Second Text Analysis Conference
(TAC 2009).

Johan Bos and Katja Markert. 2005. Recognising textual
entailment with logical inference. In Proceedings of
HLT-EMNLP, pages 628–635.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In PASCAL Challenges Workshop on Recognis-
ing Textual Entailment.

Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.

Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL, pages 669–676.

765



Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 710–718, Suntec, Singapore, Au-
gust.

Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1–9.

Danilo Giampiccolo, Hoa Trang Dang, Bernardog
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth pascal recognizing textual entail-
ment challenge. In Proceedings of the First Text Anal-
ysis Conference (TAC 2008).

John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Linguistic Data Consor-
tium, Philadelphia.

Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching. In
Proceedings of HLT-EMNLP, pages 387–394.

Hongyan Jing, Nanda Kambhatla, and Salim Roukos.
2007. Extracting social networks and biographical
facts from conversational speech transcripts. In Pro-
ceedings of ACL, pages 1040–1047.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ’03: Proceedings
of the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 423–430, Morristown, NJ,
USA.

Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of International Confer-
ence on Machine Learning, pages 296–304.

Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of HLT-NAACL,
pages 41–48.

Gabriel Murray and Giuseppe Carenini. 2008. Summa-
rizing spoken and written conversations. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 773–782, Hon-
olulu, Hawaii, October.

Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Compu-
tational Linguistics, 34(2):289–310.

Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI, pages
1099–1105.

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human

Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 172–180, Los An-
geles, California, June.

Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
meetings. In Proceedings of the 8th SIGdial Workshop
on Discourse and Dialogue, Antwerp, September.

Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 801–808, Manchester, UK, August.

Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 170–179, Singapore,
August.

Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of HLT-EMNLP, pages 371–378.

Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In ACL-44: Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 401–408,
Morristown, NJ, USA.

Chen Zhang and Joyce Chai. 2009. What do we know
about conversation participants: Experiments on con-
versation entailment. In Proceedings of the SIGDIAL
2009 Conference, pages 206–215.

766


