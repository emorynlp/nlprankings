



















































DeepNL: a Deep Learning NLP pipeline


Proceedings of NAACL-HLT 2015, pages 109â€“115,
Denver, Colorado, May 31 â€“ June 5, 2015. cÂ©2015 Association for Computational Linguistics

DeepNL: a Deep Learning NLP pipeline 

Giuseppe Attardi 

Dipartimento di Informatica 

UniversitÃ  di Pisa 

Pisa, Italy 

attardi@di.unipi.it 

 
  

 

Abstract 

We present the architecture of a deep learn-

ing pipeline for natural language pro-

cessing. Based on this architecture we built 

a set of tools both for creating distributional 

vector representations and for performing 

specific NLP tasks. Three methods are 

available for creating embeddings: feed-

forward neural network, sentiment specific 

embeddings and embeddings based on 

counts and Hellinger PCA. Two methods 

are provided for training a network to per-

form sequence tagging, a window approach 

and a convolutional approach. The window 

approach is used for implementing a POS 

tagger and a NER tagger, the convolutional 

network is used for Semantic Role Label-

ing. The library is implemented in Python 

with core numerical processing written in 

C++ using parallel linear algebra library for 

efficiency and scalability. 

1 Introduction 

Distributional Semantic Models (DSM) that rep-

resent words as vectors of weights over a high 

dimensional feature space (Hinton et al., 1986), 

have proved very effective in representing se-

mantic or syntactic aspects of lexicon. Incorpo-

rating such representations has allowed improv-

ing many natural language tasks. They also re-

duce the burden of feature selection since these 

models can be learned through unsupervised 

techniques from text. 

Deep learning algorithms for NLP tasks ex-

ploit distributional representation of words. In 

tagging applications such as POS tagging, NER 

tagging and Semantic Role Labeling (SRL), this 

has proved quite effective in reaching state of art 

accuracy and reducing reliance on manually en-

gineered feature selection (Collobert et al, 2011). 

Word embeddings have been exploited also 

in constituency parsing (Collobert, 2011) and 

dependency parsing (Chen and Manning, 2014).  

A further benefit of a deep learning approach 

is to allow performing multiple tasks jointly, and 

therefore reducing error propagation as well as 

improving efficiency. 

This paper presents DeepNL, an NLP pipe-

line based on a common Deep Learning architec-

ture: it consists of tools for creating embeddings, 

and tools that exploit word embeddings as fea-

tures. The current release includes a POS tagger, 

a NER, an SRL tagger and a dependency parser. 

Two methods are supported for creating em-

beddings: an approach that uses neural network 

and one using Hellinger PCA (Lebret and Col-

lobert 2014). 

2 NLP Toolkits 

A short survey of NLP toolkits is presented by 

Krithika and Akondi (2014). 

NLTK is among the most well-known and 

comprehensive NLP toolkits: it is written in Py-

thon and provides a number of basic processing 

facilities (tokenization, splitting, statistical anal-

ysis of corpora, etc.) as well as machine learning 

algorithms for classification and clustering. Cur-

rently it does not provide any tool based on word 

embeddings, however it can be interfaced to 

SENNA
1
 or it can be used in conjunction with 

Gensim
2
 which provides several algorithms for 

performing unsupervised semantic modeling 

from plain text, including word embeddings, 

random indexing, LDA (Latent Dirichlet Alloca-

tion). 

The Stanford NLP Toolkit (Manning et al., 

2014) is written in Java and provides tools for 

tokenization, sentence splitting, POS tagging, 

NER, parsing, sentiment analysis and temporal 

expression tagging. As a recent inclusion, it pro-

                                                           
1 http://ronan.collobert.com/senna/ 
2 http://radimrehurek.com/gensim/ 

109



vides a dependency parser based on neural net-

work and word embeddings (Chen et al., 2014). 

OpenNLP
3
 is a machine learning library writ-

ten in Java that supports the most common NLP 

tasks, such as tokenization, sentence segmenta-

tion, POS tagging, named entity extraction, 

chunking, parsing, and coreference resolution. 

Typically each tool built with these libraries 

uses a different approach or an most suitable al-

gorithm for the task: for example Sanford NLP 

uses Conditional Random Fields for NER while 

the POS tagger uses MaxEntropy and both re-

quire a set of rich features that need to be manu-

ally engineered. 

DeepNL differs from these toolkits since it is 

based on a common deep learning architecture: 

all tools exploit the same core neural network 

and use mostly just word embeddings as fea-

tures. For example the POS tagger and the NER 

tagger have an identical structure, and they differ 

only in the way they read/write documents and 

in the configuration of the discrete features used: 

the POS tagger uses word suffixes while the 

NER uses gazetteer dictionaries. Embeddings are 

used as features, providing a continuous rather 

than discrete representation of text. 

The ability of creating suitable embeddings 

for various tasks is critical for the proper work-

ing of the tools in DeepNL; hence the toolkit 

integrates algorithms for creating word embed-

dings from text, either in unsupervised or super-

vised fashion. 

3 Building Word Embeddings 

Word embeddings provide a low dimensional 

vector space representation for words, where 

values in each dimension may represent syntac-

tic or semantic properties. 

DeepNL provides two methods for building 

embeddings, one is based on the use of a neural 

language model, as proposed by (Turian and 

Bengio; Collobert et al., 2011; Mikolov et al., 

2010) and one based on spectral method as pro-

posed by Lebret and Collobert (2013). 

The neural language method can be hard to 

train and the process is often quite time consum-

ing, since several iterations are required over the 

whole training set. Some researchers provide 

precomputed embeddings for English
4
. The Pol-

                                                           
3 http://opennlp.apache.org/ 
4 http://ronan.collobert.com/senna/ 

http://metaoptimize.com/projects/wordreprs/ 

http://www.fit.vutbr.cz/Ëœimikolov/rnnlm/ 

http://ai.stanford.edu/Ëœehhuang/ 

yglot project (Al-Rafou et al., 2013) makes 

available embeddings for several languages, 

built from the plain text of Wikipedia in the re-

spective language, and the Python code for com-

puting them
5
, that supports GPU computations 

by means of Theano
6
. 

Mikolov et al. (2013) developed an alterna-

tive solution for computing word embeddings, 

which significantly reduces the computational 

costs. They propose two log-linear models, 

called bag of words and skip-gram model. The 

bag-of-word approach is similar to a feed-

forward neural network language model and 

learns to classify the current word in a given 

context, except that instead of concatenating the 

vectors of the words in the context window of 

each token, it just averages them, eliminating a 

network layer and reducing the data dimensions. 

The skip-gram model tries instead to estimate 

context words based on the current word. Further 

speed up in the computation is obtained by ex-

ploiting a mini-batch Asynchronous Stochastic 

Gradient Descent algorithm, splitting the training 

corpus into partitions and assigning them to mul-

tiple threads. An optimistic approach is also ex-

ploited to avoid synchronization costs: updates 

to the current weight matrix are performed con-

currently, without any locking, assuming that 

updates to the same rows of the matrix will be 

infrequent and will not harm convergence. 

The authors published single-machine multi-

threaded C++ code for computing the word vec-

tors
7
. A reimplementation of the algorithm in 

Python is included in the Genism library 

(Å˜ehÅ¯Å™ek and Petr Sojka, 2010). In order to ob-

tain comparable speed to the C++ version, they 

use Cython for interfacing a coding in C of the 

core function for training the network on a single 

sentence, which in turn exploits the BLAS li-

brary for algebraic computations. 

The DeepNL implementation is written in 

Cython
8
 and uses C++ code which exploits the 

Eigen
9
 library for efficient parallel linear algebra 

computations. Data is exchanged between 

Numpy arrays in Python and Eigen matrices by 

means of Eigen Map types. On the Cython side, 

a pointer to the location where the data of a 

Numpy array is stored is obtained with a call 

like: 

                                                           
5 https://bitbucket.org/aboSamoor/word2embeddings 
6 http://deeplearning.net/software/theano/ 
7 https://code.google.com/p/word2vec 
8 http://docs.cython.org/ 
9 http://eigen.tuxfamily.org/ 

110



<FLOAT_t*>np.PyArray_DATA(self.nn.hid

den_weights) 

and passed to a C++ method. On the C++ side 

this is turned into an Eigen matrix, with no com-

putational costs due to conversion or allocation, 

with the code: 

Map<Matrix> hidden_weights( 

hidden_weights, numHidden, numInput) 

which interprets the pointer to a double as a ma-

trix with numHidden rows and numInput col-

umns. Since Eigen by default uses column-major 

order while Numpy uses row-major order, the 

class Matrix above is declared as: 

typedef Eigen::Matrix<double, Eig-

en::Dynamic, Eigen::Dynamic, Eig-

en::RowMajor> Matrix;  

3.1 Word Embeddings through Hellinger 
PCA 

Lebret and Collobert (2013) have shown that 

embeddings can be efficiently computed from 

word co-occurence counts, applying Principal 

Component Analysis (PCA) to reduce dimen-

sionality while optimizing the Hellinger similari-

ty distance. 

Levy and Goldberg (2014) have shown simi-

larly that the skip-gram model by Mikolov et 

al.(2013) can be interpreted as implicitly factor-

izing a word-context matrix, whose values are 

the pointwise mutual information (PMI) of the 

respective word and context pairs, shifted by a 

global constant.  

DeepNL provides an implementation of the 

Hellinger PCA algorithm using Cython and the 

LAPACK library SSYEVR from Scipy
10

. 

Cooccurrence frequencies are computed by 

counting the number of times each context word 

w ïƒ D occurs after a sequence of T words: 

ğ‘(ğ‘¤|ğ‘‡) =
ğ‘(ğ‘¤, ğ‘‡)

ğ‘(ğ‘‡)
=

ğ‘›(ğ‘¤, ğ‘‡)

âˆ‘ ğ‘›(ğ‘¤, ğ‘‡)ğ‘›
 

where n(w, T) is the number of times word w 

occurs after a sequence of T words. The set D of 
context word is normally chosen as the the sub-

set of the top most frequent words in the vocabu-

lary V. 
The word co-occurrence matrix C of size 

|V|ï‚´|D| is built.  The coefficients of C are 
square rooted and then its transpose is multiplied 

by it to obtain a symmetric square matrix of size 

                                                           
10 https://docs.scipy.org/doc/scipy-

0.15.1/reference/generated/scipy.linalg.lapack.ssyevr.html 

|V|ï‚´|V|, to which PCA is applied to obtain the 
desired dimensionality reduction. 

3.2 Sentiment Specific Word Embeddings 

For the task of sentiment analysis, semantic 

similarity is not appropriate, since antonyms end 

up at close distance in the embeddings space. 

One needs to learn a vector representation where 

words of opposite polarity are further apart. 

Tang et al. (2014) propose an approach for 

learning sentiment specific word embeddings, by 

incorporating supervised knowledge of polarity 

in the loss function of the learning algorithm. 

The original hinge loss function in the algorithm 

by Collobert et al. (2011) is: 

LCW(x, x
c
) = max(0, 1 ï€­ fï±(x) + fï±(x

c
)) 

where x is an ngram and x
c
 is the same ngram 

corrupted by changing the target word with a 

randomly chosen one, fï±(Â·) is the feature function 

computed by the neural network with parameters 

Î¸. The sentiment specific network outputs a vec-

tor of 2 dimensions, one for modeling the gener-

ic syntactic/semantic aspects of words and the 

second for modeling polarity. 

A second loss function is introduced as objec-

tive for minimization: 

LSS(x, x
c
) = max(0, 1 ï€­ ï¤s(x) fï±(x)1 + 

   ï¤s(x) fï±(x
c
)1) 

where ï¤s is an indicator function reflecting the 

sentiment polarity of a sentence, 

ğ›¿ğ‘ (ğ‘¥) = {
1 ğ‘–ğ‘“ ğ‘“ğ‘”(ğ‘¥) = [1,0]

0 ğ‘–ğ‘“ ğ‘“ğ‘”(ğ‘¥) = [0,1]
 

where f
g
(x) is the gold distribution for ngram x. 

The overall hinge loss is a linear combination of 

the two: 

L(x, xc) = ï¡ï€ LCW(x, x
c
) + (1 â€“ ï¡) LSS(x, x

c
) 

The gradient for the output layer is given by the 

formula: 

(

ğœ•â„’
ğœ•ğ‘“ğœƒ(ğ‘¥)

ğœ•â„’
ğ›¿ğ‘“ğœƒ(ğ‘¥

ğ‘)

)

0

= {
(

âˆ’1

1
)  ğ‘–ğ‘“ â„’ğ¶ğ‘Š(ğ‘¥, ğ‘¥

ğ‘) > 0

(
0

0
)  otherwise

 

(

ğœ•â„’
ğœ•ğ‘“ğœƒ(ğ‘¥)

ğœ•â„’
ğ›¿ğ‘“ğœƒ(ğ‘¥

ğ‘)

)

1

= {
(

1

âˆ’1
)  ğ‘–ğ‘“ â„’ğ‘†ğ‘†(ğ‘¥, ğ‘¥

ğ‘) > 0

(
0

0
)  otherwise

 

DeepNL provides an algorithm for training po-

larized embeddings, performing gradient descent 

111



using an adaptive learning rate according to the 

AdaGrad method (Duchi et al, 2011). The algo-

rithm requires a training set consisting of sen-

tences annotated with their polarity, for example 

a corpus of tweets. The algorithm builds embed-

dings for both unigrams and ngrams at the same 

time, by performing variations on a training sen-

tence replacing not just a single word, but a se-

quence of words with either another word or an-

other ngram. 

4 Deep Learning Architecture 

DeepNL adopts a multi layer neural network 

architecture, as proposed in (Collobert et al., 

2011): 

1. Lookup layer. It maps word feature indi-
ces to a feature vector, as described be-

low. 

2. Linear layer. Fully connected network 
layer, represented by matrix M1 and in-

put bias b1. 

3. Activation layer (e.g. hardtanh) 

4. Linear layer. Fully connected network 
layer, represented by matrix M2 and in-

put bias b2 

5. Softmax layer. Computes the softmax of 
the output values, producing a probabil-

ity distribution of the outputs. 

Overall, the network computes the following 

function: 

f(x) = softmax(M2 a(M1 x + b1) + b2) 

where M1 ïƒ R
hï‚´d

, b1 ïƒ R
d
, M2 ïƒ R

oï‚´h
, b2 ïƒ R

o
, 

are the parameters, with d the dimension of the 

input, h the number of hidden units, o the num-

ber of output classes, a(ïƒ—) is the activation func-

tion. 

4.1 Lookup layer 

The first layer of the network transforms the in-

put into a feature vector representation. Individ-

ual words are represented by a tuple of K dis-

crete features, w ïƒ D1ï‚´ï‚¼ï‚´Dk, where Dk is the 
dictionary for the k-th feature. 

Each feature has its own lookup table 

ğ¿ğ‘‡ğ‘Šğ‘˜(âˆ™) , with a matrix of parameters to be 

learned ğ‘Šğ‘˜ âˆˆ  â„ğ‘‘
ğ‘˜Ã—|ğ’Ÿğ‘˜| , where Dk is the dic-

tionary for the k-th feature and d
k
 is a user speci-

fied vector size. The lookup table layer ğ¿ğ‘‡ğ‘Šğ‘˜(âˆ™) 

associates a vector of weights to each discrete 

feature f ïƒ Dk: 

ğ¿ğ‘‡ğ‘Šğ‘˜(ğ‘“) =  âŒ©ğ‘Š
ğ‘˜âŒªğ‘“

1  

where âŒ©ğ‘Šğ‘˜âŒªğ‘“
1 âˆˆ â„ğ‘‘ğ‘˜ is the fth column of W and dk 

is the word vector size (a hyper-parameter to be cho-

sen by the user). 

The feature vector for word w becomes the 

concatenation of the vectors for all features: 

ğ¿ğ‘‡ğ‘Š1(ğ‘¤1)ğ¿ğ‘‡ğ‘Š2(ğ‘¤2) â‹¯ ğ¿ğ‘‡ğ‘Šğ¾(ğ‘¤ğ‘˜) 

This vector of features for word w, is passed as 

input to the network. W
k
, M1, b1, M2 and b2 are 

the parameters to be learned by backpropagation. 

4.2 Feature Extractors 

The library has a modular architecture that al-

lows customizing a network for specific tasks, in 

particular its first layer, by supplying extractors 

for various types of features. 

An extractor is defined as a class that inherits 

from an abstract class with the following inter-

face: 

class Extractor(object): 

   def extract(self, tokens) 

   def lookup(self, feature) 

   def save(self, file) 

   def load(self, file) 

Method extract, applied to a list of tokens, ex-

tracts features from each token and returns a list 

of IDs for those features. The argument is a list 

of tokens rather than a single token, since fea-

tures might depend on consecutive tokens. For 

instance a gazetteer extractor needs to look at a 

sequence of tokens to determine whether they 

are mentioned in its dictionary. 

Method lookup returns the vector of weights 

for a given feature. Methods save/load allow 

saving and reloading the Extractor data to/from 

disk. 

Extractors currently include an Embeddings 

extractor, implementing the word lookup feature, 

a Caps, Prefix and Postfix extractors for deal-

ing with capitalization and prefix/postfix fea-

tures, a Gazetteer extractor for dealing with the 

gazetteers typically used in a NER, and a cus-

tomizable AttributeFeature extractor that ex-

tracts features from the state of a Shift/Reduce 

dependency parser, i.e. from the tokens in the 

stack or buffer as described for example in Nivre 

(2007). 

112



5 Sequence Taggers 

For sequence tagging, two approaches were pro-

posed in Collobert at al. (2011), a window ap-

proach and a sentence approach. The window 

approach assumes that the tag of a word depends 

mainly on the neighboring words, and is suitable 

for tasks like POS and NE tagging. The sentence 

approach assumes that the whole sentence must 

be taken into account by adding a convolution 

layer after the first lookup layer and is more 

suitable for tasks like SRL. 

We can train a neural network to maximize 

the log-likelihood over the training data. Denot-

ing by ï± the trainable parameters, including the 
network and the transition scores, we want to 

maximize the following log-likelihood with re-

spect to ï±: 

âˆ‘ log ğ‘(ğ‘¡|ğ‘¥, ğœƒ)

(ğ‘¥,ğ‘¡)âˆˆğ‘‡

 

where x are all training sentences and t their cor-

responding tag sequence. 

The score s(x, t, ï±) of a sequence of tags t for 

a sentence x, with parameters ï±, is given by the 
sum of the transition scores and the tag scores: 

ğ‘ (ğ‘¥, ğ‘¡, ğœƒ) = âˆ‘(ğ‘‡(ğ‘¡ğ‘–âˆ’1, ğ‘¡ğ‘–) + ğ‘“ğœƒ(ğ‘¥ğ‘– , ğ‘¡ğ‘–))

ğ‘›

ğ‘–=1

 

where T(i, j) is the score for the transition from 

tag i to tag j, and fï±(ti, xi) is the output of the 

network at word xi for tag ti. The probability of a 

sequence y for sentence x can be expressed as: 

ğ‘(ğ‘¦|ğ‘¥, ğœƒ) =
ğ‘’ğ‘ (ğ‘¥,ğ‘¦,ğœƒ)

âˆ‘ ğ‘’ğ‘ (ğ‘¥,ğ‘¡,ğœƒ)ğ‘¡
 

If we define: 

logadd
ğ‘–

ğ‘¥ğ‘– = log âˆ‘ ğ‘’
ğ‘¥ğ‘–

ğ‘–

 

the log of the conditional probability of the cor-

rect sequence y is given by: 

log ğ‘(ğ‘¦|ğ‘¥, ğœƒ) = ğ‘ (ğ‘¥, ğ‘¦, ğœƒ) âˆ’ logadd
ğ‘¡

ğ‘ (ğ‘¥, ğ‘¡, ğœƒ) 

The probability can be computed iteratively by 

defining: 

ğœ•ğ‘–(ğ‘) = logadd
ğ‘¡ğ‘–=ğ‘

ğ‘ (ğ‘¥1
ğ‘– , ğ‘¡1

ğ‘– , ğœƒ) 

= logadd
ğ‘

(ğœ•ğ‘–âˆ’1(ğ‘) + ğ‘‡(ğ‘, ğ‘)) + ğ‘“ğœƒ(ğ‘, ğ‘–)  âˆ€ğ‘ 

and finally 

logadd
ğ‘¡

ğ‘ (ğ‘¥, ğ‘¡, ğœƒ) = logadd
ğ‘

ğ›¿|ğ‘¥|(ğ‘) 

In order to avoid numeric overflows, the func-

tion logadd must be computed carefully, i.e. by 

subtracting the maximum value to the coeffi-

cients before performing exponentiation and 

then re-adding the maximum. 

The computation of the gradients can be per-

formed at once for the whole sequence exploit-

ing matrix operations whose computation can be 

optimized and parallelized using suitable linear 

algebra libraries. We implemented two versions 

of the network trainer, one in Python using 

NumPy
11

 and one in C++ using Eigen
12

. 

Here for example is the Python code for 

computing the ï¤ in the above equation: 

delta = scores 

delta[0] += transitions[-1] 

tr = transitions[:-1].T 

for i in xrange(1, len(delta)): 

  # sum by rows 

  logadd = logsumexp(delta[i-1]+tr, 

1) 

  delta[token] += logadd 

The array scores[i, j] contains the output of 

the neural network for the i-th element of the 

sequence and for tag j, delta[i, j] represents 

the sum of all scores ending at the i-th token 

with tag j; transitions[i, j] contains the 

current estimate of the probability of a transition 

from tag i to tag j.  

6 Experiments 

We tested the DeepNL sequence tagger on the 

CoNLL 2003 challenge
13

, a NER benchmark 

based on Reuters data. The tagger was trained 

with three types of features: the word embed-

dings from SENNA, a â€œcapsâ€ feature telling 

whether a word is in lowercase, uppercase, title 

case, or had at least one non-initial capital letter, 

and a gazetteer feature, based on the list provid-

ed by the organizers. The window size was set to 

5, 300 hidden variables were used and training 

was iterated for 40 epochs. In the following table 

we report the scores compared with the system 

by Ando et al. (2005) which uses a semi-

supervised approach and with the results by the 

released version of SENNA
14

: 

 

 

 

 

                                                           
11 http://www.numpy.org/ 
12 http://eigen.tuxfamily.org/ 
13 http://www.cnts.ua.ac.be/conll2003/ner/ 
14 http://ml.nec-labs.com/senna/ 

113



System F1 

Ando et al. 2005 89.31 

SENNA 89.51 

DeepNL 89.38 

Table 1. Performance on the NER task, using the 

CoNLL 2003 benchmark. 

The slight difference with SENNA might be ex-

plained by the use of different gazetteers. 

The same sequence tagger can be used for 

POS tagging. In this case the discrete features 

used are the same capitalization feature as for the 

NER and a suffix feature, which denotes whether 

a token ends with one of the 455 most frequent 

suffixes of length one or two characters in the 

training corpus. 

Table 2 presents the results achieved by the 

POS tagger trained on the Penn Treebank, com-

pared with the results of the reference system by 

Tuotanova et al. (2003), which uses rich fea-

tures, and with the original SENNA implementa-

tion. 

System Precision 

Toutanova et al. 2003 97.24 

SENNA 97.28 

DeepNL 97.12 

Table 2. Performance on the POS task, using the Penn 

Treebank, sections 0-18 for training, sections 22-24 

for testing. 

Both these experiments confirm that word em-

beddings can replace the use of complex manu-

ally engineered features for typical natural lan-

guage processing tasks. 

7 Dependency Parsing 

We have adapted to the use of embeddings our 

original transition based dependency parser 

DeSR (Attardi et al., 2009), that was already 

based on a neural network. The parser uses the 

neural network to decide which action to per-

form at each step in the analysis of a sentence. 

Looking at a short context of past analyzed to-

kens and next input tokens, it must decide 

whether the two current focus tokens can be 

connected by a dependency relation. In this case 

it performs a reduction, creating the dependency, 

otherwise it advances on the input. The original 

implementation used a large set of discrete fea-

tures to represent the current context. 

The deep learning version of the parser ex-

ploits word embedding as features and also cre-

ates a dense vector representation for the remain-

ing discrete features. A specific extractor (At-

tributeExtractor) was built for this purpose. 

8 Conclusions 

We have presented the architecture of DeepNL, 

a library for building NLP applications based on 

a deep learning architecture. The implementation 

is written in Python/Cython and uses C++ linear 

algebra libraries for efficiency and scalability, 

exploiting multithreading or GPUs where avail-

able. 

The implementation of DeepNL is available 

on GitHub
15

. 

The availability of a library that allows creat-

ing embeddings and training a deep learning ar-

chitecture using them might contribute to the 

development of further tools for linguistic analy-

sis. 

For example we are planning to build a clas-

sifier for performing identification of affirma-

tive, negative or speculative contexts in sentenc-

es. 

We are also considering additional ways of 

creating embeddings, for example to generate 

context sensitive embeddings that could provide 

word representations that disambiguate among 

word senses. 

Acknowledgements 

Partial support for this work was provided by 

project RIS (POR RIS of the Regione Toscana, 

CUP nÂ° 6408.30122011.026000160). 

References 

R. Al-Rfou, B. Perozzi, and S. Skiena. 2013. Poly-

glot: Distributed Word Representations for Multi-

lingual NLP. arXiv preprint arXiv:1307.1662. 

R. K. Ando, T. Zhang, and P. Bartlett. 2005. A 

framework for learning predictive structures from 

multiple tasks and unlabeled data. Journal of Ma-

chine Learning Research, 6:1817â€“1853. 

G. Attardi, F. Dell'Orletta, M. Simi, J. Turian.  2009. 

Accurate Dependency Parsing with a Stacked Mul-

tilayer Perceptron. In Proc. of Workshop Evalita 

2009, ISBN 978-88-903581-1-1. 

Danqi Chen and Christopher D. Manning. 2014. Fast 

and Accurate Dependency Parser using Neural 

Networks. In: Proc. of EMNLP 2014. 

R. Collobert et al. 2011. Natural Language Processing 

(Almost) from Scratch. Journal of Machine Learn-

ing Research, 12, 2461â€“2505. 

                                                           
15 https://github.com/attardi/deepnl 

114



R. Collobert and J. Weston. 2008. A unified architec-

ture for natural language processing: Deep neural 

networks with multitask learning. In ICML, 2008. 

R. Collobert. 2011. Deep Learning for Efficient Dis-

criminative Parsing. In AISTATS, 2011. 

P. S. Dhillon, D. Foster, and L. Ungar. 2011. Mul-

tiview learning of word embeddings via CCA. In 

Advances in Neural Information Processing Sys-

tems (NIPS), volume 24. 

John Duchi, Elad Hazan, and Yoram Singer. 2011. 

Adaptive subgradient methods for online learning 

and stochastic optimization. The Journal of Ma-

chine Learning Research. 

S. Hartmann, G. Szarvas, and I. Gurevych. 2011. 

Mining Multiword Terms from Wikipedia, in M.T. 

Pazienza & A. Stellato (Eds.): Semi-Automatic 

Ontology Development: Processes and Resources, 

pp. 226-258, Hershey, PA, USA: IGI Global. 

G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Dis-

tributed representations. 1986. In: Parallel distrib-

uted processing: Explorations in the microstructure 

of cognition. Volume 1: Foundations, MIT Press, 

1986. 

L.B. Krithika and Kalyana Vasanth Akondi. 2014. 

Survey on Various Natural Language Processing 

Toolkits. World Applied Sciences Journal 32 (3): 

399-402. 

RÃ©mi Lebret and Ronan Collobert. 2013. Word Em-

beddings through Hellinger PCA. Proc. of EACL 

2013. 

Omer Levy and Yoav Goldberg. 2014. Neural Word 

Embeddings as Implicit Matrix Factorization. In  

Advances in Neural Information Processing Sys-

tems (NIPS), 2014. 

Christopher D. Manning and Hinrich SchÃ¼tze. 1999. 

Foundations of Statistical Natural Language Pro-

cessing. The MIT Press. Cambridge, Massachu-

setts. 

Manning, Christopher D., Surdeanu, Mihai, Bauer, 

John, Finkel, Jenny, Bethard, Steven J., and 

McClosky, David. 2014. The Stanford CoreNLP 

Natural Language Processing Toolkit. In Proceed-

ings of 52nd Annual Meeting of the Association for 

Computational Linguistics: System Demonstra-

tions, pp. 55-60. 
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and 

Sanjeev Khudanpur. 2010. Recurrent neural net-

work based language model. 

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-

frey Dean. 2013. Efficient Estimation of Word 

Representations in Vector Space. In Proceedings 

of Workshop at ICLR, 2013. 

J. Nivre. 2007. Incremental non-projective dependen-

cy parsing, Proceedings of Human LanguageTech-

nologies: The Annual Conference of the North 

American Chapter of the Association for Computa-

tional Linguistics (NAACL HLT), Rochester, NY, 

pp. 396â€“403. 

Radim Å˜ehÅ¯Å™ek and Petr Sojka. 2010. Software 

Framework for Topic Modelling with Large Cor-

pora. In Proceedings of the LREC 2010 Workshop 

on New Challenges for NLP Frameworks, ELRA, 

Valletta, Malta, pp. 45â€“50. 

K. Toutanova, D. Klein, C. D. Manning, and Y. Sing-

er. 2003. Feature-rich part-of-speech tagging with 

a cyclic dependency network. In Conference of the 

North American Chapter of the Association for 

Computational Linguistics & Human Language 

Technologies (NAACL-HLT). 

 

115


