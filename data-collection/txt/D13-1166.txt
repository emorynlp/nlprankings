










































Prior Disambiguation of Word Tensors for Constructing Sentence Vectors


Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1590–1601,
Seattle, Washington, USA, 18-21 October 2013. c©2013 Association for Computational Linguistics

Prior Disambiguation of Word Tensors
for Constructing Sentence Vectors

Dimitri Kartsaklis
University of Oxford

Department of
Computer Science

Wolfson Building, Parks Road
Oxford, OX1 3QD, UK

dimitri.kartsaklis@cs.ox.ac.uk

Mehrnoosh Sadrzadeh
Queen Mary University of London
School of Electronic Engineering

and Computer Science
Mile End Road

London, E1 4NS, UK
mehrs@eecs.qmul.ac.uk

Abstract

Recent work has shown that compositional-
distributional models using element-wise op-
erations on contextual word vectors benefit
from the introduction of a prior disambigua-
tion step. The purpose of this paper is to
generalise these ideas to tensor-based models,
where relational words such as verbs and ad-
jectives are represented by linear maps (higher
order tensors) acting on a number of argu-
ments (vectors). We propose disambiguation
algorithms for a number of tensor-based mod-
els, which we then test on a variety of tasks.
The results show that disambiguation can pro-
vide better compositional representation even
for the case of tensor-based models. Further-
more, we confirm previous findings regarding
the positive effect of disambiguation on vec-
tor mixture models, and we compare the ef-
fectiveness of the two approaches.

1 Introduction

Distributional models of meaning have been proved
extremely useful for a number of natural language
processing tasks, ranging from thesaurus extraction
(Curran, 2004) to topic modelling (Landauer and
Dumais, 1997) and information retrieval (Manning
et al., 2008), to name just a few. These models
are based on the distributional hypothesis of Har-
ris (1968), which states that the meaning of a word
depends on its context. This idea allows the words
to be represented by vectors of statistics collected
from a sufficiently large corpus of text; each ele-
ment of the vector reflects how many times a word
co-occurs in the same context with another word
of the vocabulary. However, due to the generative

power of natural language, which is able to pro-
duce infinite new structures from a finite set of re-
sources (words), no text corpus, regardless of its
size, can provide reliable distributional representa-
tions for anything longer than single words or per-
haps very short phrases consisting of two words; in
other words, this technique cannot scale up to the
phrase or sentence level.

Much research activity has been recently dedi-
cated to provide a solution to this problem: although
the direct construction of a sentence vector is not
possible, we might still be able to synthetically cre-
ate such a vectorial representation by somehow com-
posing the vectors of the words that comprise the
sentence. Towards this goal, researchers have em-
ployed a variety of approaches that roughly fall into
two general categories. Following an influential
work (Mitchell and Lapata, 2008), the models in the
first category compute a sentence vector as a mix-
ture of the original word vectors, using simple oper-
ations such as element-wise multiplication and ad-
dition; we refer to these models as vector mixtures.
The main characteristic of these models is that they
do not distinguish between the type-logical identi-
ties of the different words: an intransitive verb, for
example, is of the same order as its subject (a noun),
and both will contribute equally to the composite
sentence vector.

However, this symmetric treatment of composi-
tion seems unjustified from a formal semantics point
of view. Words with special meanings, such as verbs
and adjectives, are usually seen as functions acting
on, hence modifying, a number of arguments rather
than lexical units of the same order as them; an
adjective, for example, is a function that returns a
modified version of its input noun. Inspired from

1590



this more-aligned-to-formal-semantics view, a sec-
ond research direction aims to represent relational
words as linear maps (tensors of various orders)
that can be applied to one or more arguments (vec-
tors). Baroni and Zamparelli (2010), for example,
model adjectives as matrices which, when matrix-
multiplied with a noun vector, will produce a vec-
torial representation of the specific adjective-noun
compound. The notion of a framework where re-
lational words are entities living in vector spaces of
higher order than nouns, which are simple vectors,
has been formalized by Coecke et al. (2010) in the
context of the abstract mathematical framework of
compact closed categories. We refer to this class of
models as tensor-based.

Regardless of the way they approach the repre-
sentation of relational words and their composition
operation, however, most current compositional-
distributional models do share a common feature:
they all rely on ambiguous vector representations,
where all the senses of a polysemous word, such as
the verb ‘file’ (which can mean register or smooth),
are merged into the same vector or tensor. At least
for the vector mixture approach, this practice has
been proved suboptimal: Reddy et al. (2011) and
Kartsaklis et al. (2013) test a number of simple mul-
tiplicative and additive models using disambiguated
vector representations on various tasks, showing that
the introduction of a disambiguation step prior to ac-
tual composition can indeed increase the quality of
the composite vectors. However, the fact that disam-
biguation can be beneficial for models based on vec-
tor mixtures is not very surprising. Both additive and
multiplicative compositions are but a kind of average
of the vectors of the words in the sentence, hence
can directly benefit from the provision of more ac-
curate starting points. Perhaps a more interesting
question, and one that the current paper aims to ad-
dress, is to what extent disambiguation can also pro-
vide benefits for tensor-based approaches, which in
general constitute more powerful models for natural
language (see discussion in Section 2).

Specifically, this paper aims to: (a) propose dis-
ambiguation algorithms for a number of tensor-
based distributional models; (b) examine the effect
of disambiguation on tensors for relational words;
and (c) meaningfully compare the effectiveness of
tensor-based against vector mixture models in a
number of tasks. Based on the generic procedure of
Schütze (1998), we propose algorithms for a num-

ber of tensor-based models, where the composition
is modelled as the application of linear maps (ten-
sor contractions). Following Mitchell and Lapata
(2008) and many others, we test our models on
two disambiguation tasks similar to that of Kintsch
(2001), and on the phrase similarity task introduced
in (Mitchell and Lapata, 2010). In almost every
case, the results show that disambiguation can make
a great difference in the case of tensor-based models;
they also reconfirm previous findings regarding the
effectiveness of the method for simple vector mix-
ture models.

2 Vectors vs tensors

The simple models of Mitchell and Lapata (2008)
constitute the easiest and perhaps the most intuitive
way of composing two or more vectors: each ele-
ment of the resulting vector is computed as the sum
or the product of the corresponding elements in the
input vectors (left part in Figure 1). In the case of
addition, the components of the output vector are
simply the cumulative scores of the corresponding
input components. So in a sense the output element
embraces both input elements, resembling a union of
the input features. On the other hand, the element-
wise multiplication of two vectors can be seen as the
intersection of their features: a zero element in one
of the input vectors will eliminate the corresponding
feature in the output, no matter how high the other
input component was. In addition to failing to iden-
tify the special roles of words in a sentence, vector
mixture models disregard grammar in another way:
the commutativity of operators make them a bag-
of-words approach, where the meaning of sentence
‘dog bites man’ is equated to that of ‘man bites dog’.

On the contrary to the above element-wise treat-
ment, a compositional approach based on linear
maps computes each element of the resulting vec-

= =

Vector mixture Tensor-based

Figure 1: Vector mixture and tensor-based models for
composition. In the latter approach, the ith element of
the output vector is the linear combination of the input
vector with the ith row of the matrix.

1591



tor via a linear combination of all the elements of
the input vector (right part of Figure 1); in other
words, possible interdependencies between differ-
ent features are also taken into account, offering (in
principle) more power. Furthermore, by design, the
bag-of-words problem is not present here. Over-
all, tensor-based models offer a more complete and
linguistically motivated solution to the problem of
composition. For example, one can consider build-
ing linear maps for prepositions and logical words,
rather than treating them as noise and discard them,
as commonly done in the vector mixture models.

3 Disambiguation in vector mixtures

For a compositional model based on vector mix-
tures, polysemy of words can be a critical factor.
Pulman (2013) and Kartsaklis et al. (2013) point out
that the element-wise combination of “ambiguous”
vectors produces results that are hard to interpret;
the composed vector is not a purely compositional
representation but a product of two tasks that take
place in parallel: composition and some amount of
disambiguation that emerges as a side-effect of the
compositional process, leaving the resulting vector
in an intermediate state.

This effect is demonstrated in Figure 2, which
shows the composition of the ambiguous verb ‘run’
(with meanings moving fast and dissolving) with the
subject ‘horse’. The first three components of our
toy vector space are related to the dissolving mean-
ing, while the last three of them to the moving fast
meaning. An ambiguous vector for ‘run’ will have
non-zero values for every component. On the other
hand, we would expect the vector for ‘horse’ to have
high values for the ‘race’, ‘gallop’, and ‘move’ com-
ponents, and very low values (but not necessarily
zero) for the dissolving-related ones—it is always
possible for the word ‘horse’ to appear in the same

colour

dissolve

painting

race

gallop

move

5

9

4

11

8

15

1

0

2

6

13

7

Ambiguous 
runhorse

=

5

0

8

66

104

105

0

0

0

11

8

15

1

0

2

6

13

7

Disambiguated
runhorse

=

0

0

0

66

104

105

Figure 2: The effect of disambiguation on vector compo-
sition. The numbers are (artificial) co-occurrence counts
of each target word with the 6 basis words on the left.

context with the word ‘painting’, for example. The
left part of Figure 2 shows what happens when the
ambiguous ‘run’ vector is used; the multiplication
with the ‘horse’ vector will produce an impure re-
sult, half affected by composition and half by disam-
biguation. However, what we really want is a vec-
tor where all the dissolving-related components will
be eliminated, since they are irrelevant to the way
the word ‘run’ is used in the sentence. In order to
achieve this, we have to introduce a disambiguation
step prior to composition (right part of Figure 2).

These ideas are experimentally verified in the
works of Reddy et al. (2011) and Kartsaklis et al.
(2013); Pulman (2013) also presents a comprehen-
sive analysis of the problem. What remains to be
seen is if disambiguation can also provide bene-
fits for the linguistically motivated setting of tensor-
based models, the principles of which are shortly
discussed in the next section.

4 Tensors as multilinear maps

A tensor is a geometric object that can be seen as the
generalization of the familiar notion of a vector in
higher dimensions. The order of a tensor is the num-
ber of its dimensions; in other words, the number of
indices we need to fully describe a random element
of the tensor. Hence, a vector is a tensor of order
1, a matrix is a tensor of order 2, and so on. Ten-
sors and multilinear maps stand in one-to-one cor-
respondence, as stated by the following well-known
“map-state” isomorphism (Bourbaki, 1989):

f : V1 → . . .→ Vj → Vk ∼= Vk⊗Vj⊗. . .⊗V1 (1)

This offers an elegant way to adopt a formal se-
mantics view of natural language in vector spaces.
Let nouns live in a basic vector space N ∈ RD; re-
turning to our previous example, an adjective then
can be seen as a map f : N → N which is isomor-
phic to N ⊗N (that is, to a matrix). In general, the
order of the tensor is equal to the number of argu-
ments plus one dimension that carries the result; so
a unary function (e.g. adjectives, intransitive verbs)
is represented by a tensor of order 2 (a matrix), a bi-
nary function (e.g. a transitive verb) as an order 3
tensor, and so on. Due to the above isomorphism,
function application (and hence our compositional
operation) becomes a generalisation of matrix mul-
tiplication, formalised in terms of the inner product.
In the case of a unary relational word, such as an
adjective, this is nothing more than the usual notion

1592



of matrix multiplication between a matrix and a vec-
tor. The generalization of this process to tensors of
higher order is known as tensor contraction. Given
two tensors of orders n and m, the tensor contrac-
tion operation will always produce a tensor of order
n+m− 2.

Let us see an example of how this works for a
simple transitive sentence. Let V ∈ RI×J×K be the
tensor of order 3 for the verb and S ∈ RI , O ∈ RK
the tensors of order 1 (vectors) for the subject and
the object of the verb, respectively. Then V ×O will
return a new tensor living in RI×J (i.e. a matrix)1;
a further interaction of this result with the subject
will return a vector for the whole transitive sentence
living in RJ . We should note that the order in which
the verb is applied to its arguments is not important;
so in general the meaning of a transitive sentence is
given by:

(V ×O)T × S = (VT × S)×O (2)

where T denotes a transpose and makes indices
match, since subject precedes the verb.

5 Creating verb tensors

In this section we review a number of proposals re-
garding concrete methods of constructing tensors for
relational words in the context of the frameworks
of Coecke et al. (2010) and Baroni and Zamparelli
(2010), which both comply to the setting of Section
4.2

Relational Following ideas from the set-theoretic
view of formal semantics, Grefenstette and
Sadrzadeh (2011a) suggest that the meaning of a
relational word should be represented as the sum
of its arguments. The meaning of adjective ‘red’,
for example, becomes the sum of the vectors of
all the nouns that ‘red’ modifies in the corpus; so−→
red =

∑
i
−−−→nouni, where i iterates through all the

occurrences of ‘red’. This can be generalised to
relational words of any arity, by summing the tensor
product of their arguments. So for a transitive verb
we have:

verb
2

=
∑

i

(
−−→
subji ⊗

−−→
obji) (3)

1The symbol × denotes tensor contraction.
2In what follows we use the case of a transitive verb as an

example; however the descriptions apply to any relational word
of any arity. A vector (an order-1 tensor) is denoted as −→x ; ten-
sors of order n > 1 are shown as xn for clarity.

where i again iterates over all occurrences of the spe-
cific verb in the corpus and the superscript denotes
the order of the tensor.

In order to achieve a more expressive represen-
tation for the sentences, the authors used the con-
vention that the arity of the head word in a sentence
will also determine the order of the sentence space;
that is, the space of intransitive sentences will be of
order 1, of transitive ones will be of order 2, and
so on. Recall from Section 4 that for the transitive
case this increases the order of the verb tensor to 4
(2 dimensions for the arguments plus another 2 for
the result). In spite of this, however, note that the
method of Equation 3 produces a matrix. The other
two dimensions of the tensor remain empty (filled
with zeros), a fact that simplifies the calculations but
also considerably weakens the expressive power of
the model. This simplification transforms Equation
2 to the following:

subj verb obj
2

= (
−−→
subj ⊗

−→
obj)� verb2 (4)

where ⊗ denotes the tensor product and � element-
wise multiplication.

Kronecker In a subsequent work (Grefenstette
and Sadrzadeh, 2011b), the same team proposes the
creation of a verb matrix as the Kronecker product
of the verb’s contextual vector with itself:

verb
2

=
−−→
verb⊗

−−→
verb (5)

Again in this model the sentence space is of order
2, and the meaning of a transitive sentence is calcu-
lated using Equation 4.

Frobenius The previous models bring the impor-
tant limitation that only sentences of the same struc-
ture can be meaningfully compared; it is not pos-
sible, for example, to compare an intransitive sen-
tence (e.g. ‘kids play’) with a transitive one (‘chil-
dren play football’), since the former is a vector and
the latter a matrix. Using Frobenius algebras, Kart-
saklis et al. (2012) provide a unified sentence space
for every sentence regardless of its type. These mod-
els turn the matrix of Equation 3 to a tensor of or-
der 3 (as required by the type-logical identities) by
copying one of the existing dimensions. When the
dimension of rows (corresponding to subjects) is
copied, the calculation of a vector for a transitive
sentence becomes:

−−−−−−−−−→
subj verb obj =

−−→
subj � (verb2 ×

−→
obj) (6)

1593



Copying the column dimension (objects) gives:

−−−−−−−−−→
subj verb obj =

−→
obj �

(
(verb

2
)T ×

−−→
subj

)
(7)

Linear regression None of the above models cre-
ate tensors that are fully populated: one or more
dimensions will always remain empty. Following
an idea first introduced by Baroni and Zamparelli
(2010) for the creation of adjective matrices, Grefen-
stette et al. (2013) use linear regression in order
to learn full tensors of order 3 for transitive verbs.
Linear regression is a supervised method of learn-
ing, so it needs a number of exemplar data points.
In the case of the adjective ‘red’, for example, we
would need a set of the form 〈−→car,

−−−−→
red car〉, 〈

−−→
shirt,

−−−−−→
red shirt〉, 〈

−−→
shoe,

−−−−−→
red shoe〉 and so on, where the sec-

ond vector in each pair is the contextual vector of the
whole phrase created exactly as if it were a single
word. The goal of the learning process is to find the
parameters adj

2
and
−→
b such that:

−−−−−−→
adj noun ≈ adj2 ×−−−→noun+

−→
b (8)

for all nouns modified by the specific adjective. In
practice, the bias

−→
b is embedded in adj

2
, hence

the above procedure provides us with a matrix for
the adjective. One can generalize this procedure to
tensors of higher order by proceeding step-wise, as
done by Grefenstette et al. (2013). For the case
of a transitive verb, they first use exemplar pairs
of the form 〈

−−→
subj,

−−−−−−−−→
subj verb obj〉 to learn a matrix

verb obj
2

for the verb phrase; then, they perform
a new training session with exemplars of the form
〈
−→
obj, verb obj

2〉, the result of which is an order 3
tensor for the verb.

6 Generic context-based disambiguation

In all of the models of Section 5, the training of a
relational word tensor is based on the set of contexts
where this word occurs. Hence, in these models the
problem of creating disambiguated versions of ten-
sors can be recasted to that of further breaking the
set of contexts in a way that each subset reflects a
different sense of the word in the corpus. If, for ex-
ample, S is the whole set of sentences for a word
w that occurs in the corpus under n different senses,
then the goal is to create n subsets S1, . . . Sn such
that S1 contains the sentences where w appears un-
der the first sense, S2 the sentences where w occurs

under the second sense, and so on. Each one of these
subsets can then be used to train a tensor for a spe-
cific sense of the target relational word.

Towards this purpose we use a variation of the ef-
fective procedure of Schütze (1998): first, each con-
text for a target word wt is represented by a context
vector of the form 1n(

−→w1 + . . . + −→wn), where −→wi is
the lexical vector of some other word wi 6= wt in the
same context. Next, we apply a clustering method
on this set of vectors in order to discover the latent
senses of wt. The assumption is that the contexts
of wt will vary according to the specific sense this
word is used: ‘bank’ as a financial institution should
appear in quite different contexts than as land.

The above procedure will give us a number of
clusters, each consisting of context vectors; we use
the centroid of each cluster as a vectorial repre-
sentation of the corresponding sense. So in our
model each wordw is initially represented by a tuple
〈−→w ,S〉, where−→w is the lexical vector of the word as
created by the usual distributional practice, and S
is a set of sense vectors (centroids of context vec-
tor clusters) produced by the above procedure. The
disambiguation of a new word w under a context C
can now be accomplished as follows: we create a
context vector −→c for C as above, and we compare it
with every sense vector of w; the word is assigned to
the sense corresponding to the closest sense vector.
Specifically, if Sw is the set of sense vectors for w,−→c the context vector for C, and d(−→v ,−→u ) our vector
distance measure, the preferred sense ŝ is given by:

ŝ = arg min
−→vs∈Sw

d(−→vs ,−→c ) (9)

For the actual clustering step we follow the set-
ting of Kartsaklis et al. (2013), which worked well in
tasks very similar to ours. Specifically, we perform
hierarchical agglomerative clustering (HAC) using
Ward’s method as the inter-cluster distance, while
the distance between vectors is measured with Pear-
son’s correlation.3 In the above work, this configura-
tion has been found to return the highest V-measure
(Rosenberg and Hirschberg, 2007) on the noun set
of SEMEVAL 2010 Word Sense Induction & Disam-
biguation Task (Manandhar et al., 2010). As con-
text for a word, we consider the sentence in which
this word occurs. The output of HAC is a dendro-
gram embedding all the possible partitionings of the

3Informal experimentation with more robust probabilistic
techniques, such as Dirichlet process gaussian mixture models,
revealed no significant benefits for our setting.

1594



data. In order to select the optimal partitioning, we
rely on the Caliński/Harabasz index (Caliński and
Harabasz, 1974), also known as variance ratio cri-
terion (VRC). VRC is calculated as the ratio of the
sum of the inter-cluster variances over the sum of
the intra-cluster variances, bearing the intuition that
the optimal partitioning should be the one that re-
sults in the most compact and maximally separated
clusters. We compute the VRC for a range of differ-
ent partitionings (from 2 to 10 clusters) and keep the
partitioning with the highest score.

7 Constructing unambiguous verb tensors

The procedure described in Section 6 provides us
with n clusters of context vectors for a target word.
Since in our case each context vector corresponds
to a distinct sentence, the output of the clustering
scheme can also be seen as n subsets of sentences,
where the word appears under different senses. It
is now quite straightforward to use this partitioning
of the training corpus in order to learn unambiguous
versions of verb tensors, as detailed below.

Relational/Frobenius Both the Relational and the
Frobenius models use the same way of creating an
initial verb matrix (Equation 3) which then they ex-
pand to a higher order tensor. Let S1 . . . Sn be the
sets of sentences returned by the clustering step for
a verb. Then, the verb tensor for the ith sense is:

verb
2
i =

∑
s∈Si

(
−−−→
subjs ⊗

−−→
objs) (10)

where subjs and objs refer to the subject/object pair
that occurred with the verb in sentence s. This can
be generalized to any arity n as follows:

word
n
i =

∑
s∈Si

n⊗
k=1

−−−→argk,s (11)

where argk,s denotes the kth argument of the target
word in sentence s.

Kronecker For a given verb v in a context C, let
−→vi be the sense vector of v given C corresponding to
the sense i returned by Equation 9. Then we have:

verb
2
i =
−→vi ⊗−→vi (12)

The generalized version to arity n is given by:

word
n
i =

n⊗
k=1

−→vi (13)

Linear regression Creating unambiguous full ten-
sors using linear regression is also quite straightfor-
ward. Let us assume again that the clustering step
for a verb v returns n sets of sentences S1 . . . Sn,
where each sentence set corresponds to a different
sense. Then, we have n different regression prob-
lems, each one of which will be trained on exemplar
pairs derived exclusively from the sentences of the
corresponding set. This will result in n verb tensors,
which will correspond to the different senses of the
verb. Generalization to higher arities is a straightfor-
ward extension of the step-wise process in Section 5
for transitive verbs.

8 Experiments

In this section we will test the effect of disambigua-
tion on the models of Section 5 in a variety of tasks.
Due to the significant methodological differences
of the linear regression model from the other ap-
proaches and the variety of its set of parameters, we
decided that it would be better if this was left as the
subject of a distinct work.

Experimental setting We train our vectors using
ukWaC (Ferraresi et al., 2008), a corpus of English
text with 2 billion words (100m sentences). We use
2000 dimensions, with weights calculated as the ra-
tio of the probability of the context word given the
target word to the probability of the context word
overall. The context here is a 5-word window on
both sides of the target word. The vectors are disam-
biguated both syntactically and semantically: first,
separate vectors have been created for different syn-
tactic usages of the same word in the corpus; for ex-
ample, the word ‘book’ has two vectors, one for its
noun sense and one for its verb sense. Furthermore,
each word is semantically disambiguated according
to the method of Section 6.

Models We compare the tensor-based models of
Section 5 with the multiplicative and additive mod-
els of Mitchell and Lapata (2008), reporting results
for both ambiguous and disambiguated versions.
For all the disambiguated models, the best sense for
each word in the sentence or phrase is first selected
by applying the procedure of Section 6 and Equa-
tion 9. If the model is based on a vector mixture, the
sense vectors corresponding to these senses are mul-
tiplied or added to form the composite representa-
tion for the sentence or phrase. For the tensor-based
models, the composite meanings are calculated ac-

1595



cording to the equations of Section 5, using verb
tensors created by the procedures of Section 7. The
semantic similarity of two phrases or sentences is
measured as the cosine distance between their com-
posite vectors. For models that return a matrix (e.g.
Relational, Kronecker), the distance is based on the
Frobenius inner product.

Implementation details Our code is mainly writ-
ten in Python and C++, and for the actual cluster-
ing step we use the Python interface of the efficient
FASTCLUSTER library (Müllner, 2013). In a shared
24-core Xeon machine with 72 GB of memory, and
with a fair amount of parallelism applied, the aver-
age processing time per word was about 4 minutes;
this is roughly translated to 12-13 hours of training
on average per dataset.

8.1 Verb disambiguation task

Perhaps surprisingly, one of the most popular tasks
for testing compositionality in distributional models
is based on disambiguation. This task, originally in-
troduced by Kintsch (2001), has been adopted by
Mitchell and Lapata (2008) and others for evaluating
the quality of composition in vector spaces. Given
an ambiguous verb such as ‘file’, the goal is to find
out to what extent the presence of an appropriate
context will disambiguate its intended meaning. The
context (e.g. a subject/object pair) is composed with
two landmark verbs corresponding to the different
senses (‘smooth’ and ‘register’) to create simple sen-
tences. The assumption is that a good compositional
model should be able to reflect that ‘woman files ap-
plication’ is closer to ‘woman registers application’
than to ‘woman smooths application’.

In this paper we test our models on two different
datasets of transitive sentences, that of Grefenstette
and Sadrzadeh (2011a) and Kartsaklis et al. (2013)4.
Specific details about the creation of the datasets can
be found in the above papers; for the purposes of
the current work it is sufficient to mention that their
main difference is that in the former the verbs and
their alternative meanings have been selected auto-
matically using the JCN metric of semantic similar-
ity (Jiang and Conrath, 1997), while in the latter the
selection was based on human judgements from the
work of Pickering and Frisson (2001). So, while

4This dataset has been created by Mehrnoosh Sadrzadeh in
collaboration with Edward Grefenstette, but remained unpub-
lished until (Kartsaklis et al., 2013).

in the first dataset many verbs cannot be consid-
ered as genuinely ambiguous (e.g. ‘say’ with mean-
ings state and allege or ‘write’ with meanings pub-
lish and spell), the landmarks in the second dataset
correspond to clearly separated senses (e.g. ‘file’
with meanings register and smooth or ‘charge’ with
meanings accuse and bill). Furthermore, subjects
and objects of this latter case are modified by appro-
priate adjectives, overall creating a richer and more
linguistically balanced dataset.

In both cases the evaluation methodology is the
same: each entry of the dataset has the form
〈subject, verb, object, high-sim landmark, low-sim
landmark〉. The context is combined with the verb
and the two landmarks, creating three simple tran-
sitive sentences. The main-verb sentence is paired
with both the landmark sentences, and these pairs
are randomly presented to human evaluators, the
duty of which is to evaluate the similarity of the sen-
tences within a pair in a scale from 1 to 7. The scores
of the compositional models are the cosine distances
(or the Frobenius inner products, in the case of ma-
trices) between the composite representations of the
sentences of each pair. As an overall score for each
model, we report its Spearman’s ρ correlation with
the human judgements. Both datasets consist of 200
pairs of sentences (10 main verbs × 2 landmarks ×
10 contexts).

Results The results for the G&S dataset are shown
in Table 1.5 The verbs-only model (BL) refers to a
non-compositional evaluation, where the similarity
between two sentences is solely based on the dis-
tance between the two verbs, without applying any
compositional step with subject and object.

The tensor-based models present much better per-
formance than the vector mixture ones, with the dis-
ambiguated version of the copy-object model sig-
nificantly higher than the relational model. By de-
sign, the copy-object model retains more informa-
tion about the objects; so this result confirms pre-
vious findings, that in this certain dataset the role
of objects is more important than that of subjects
(Kartsaklis et al., 2012). In general, the disambigua-
tion step improves the results of all the tensor-based
models except Kronecker; the effect is reversed for
the vector mixture models, where the disambiguated
versions present much worse performance (these

5For all tables in this section, � and� denote highly sta-
tistically significant differences with p < 0.001.

1596



Model Ambig. Disamb.
BL Verbs only 0.198 � 0.132
M1 Multiplicative 0.137 � 0.044
M2 Additive 0.127 � 0.047
T1 Relational 0.219 < 0.223
T2 Kronecker 0.207 � 0.061
T3 Copy-subject 0.070 � 0.122
T4 Copy-object 0.241 � 0.262

Human agreement 0.599
Difference between T4 and T1 is s.s. with p < 0.001

Table 1: Results for the G&S dataset.

Model Ambig. Disamb.
BL Verbs only 0.151 � 0.217
M1 Multiplicative 0.131 < 0.137
M2 Additive 0.085 � 0.193
T1 Relational 0.036 � 0.121
T2 Kronecker 0.159 < 0.166
T3 Copy-subject 0.035 � 0.117
T4 Copy-object 0.033 � 0.095

Human agreement 0.383
Difference between BL and M2 is s.s. with p < 0.001

Table 2: Results for the Kartsaklis et al. dataset.

findings are further discussed in Section 9).
The result of disambiguation is clearer for the

dataset of Kartsaklis et al. (Table 2). The longer
context in combination with genuinely ambiguous
verbs produces two effects: first, disambiguation is
now helpful for all models, either vector mixtures
or tensor-based; second, the disambiguation of just
the verb (verbs-only model), without any interac-
tion with the context, is sufficient to provide the best
score (0.22) with a difference statistically significant
from the second model (0.19 for disambiguated ad-
ditive). In fact, further composition of the verb with
the context decreases performance, confirming the
results reported by Kartsaklis et al. (2013) for vec-
tors trained using BNC. Given the nature of the spe-
cific task, which is designed around the ambiguity of
the verb, this result is not surprising: a direct disam-
biguation of the verb based on the rest of the con-
text should naturally constitute the best method to
achieve top performance—no composition is neces-
sary for this task to be successful.

However, when one does use a task like this in
order to evaluate compositional models (as we do
here and as is commonly the case), they implic-
itly correlate the strength of the disambiguation ef-
fect that takes place during the composition with the
quality of composition, essentially assuming that the
stronger the disambiguation, the better the composi-

tional model that produced this side-effect. Unfor-
tunately, the extent to which this assumption is valid
or not is still not quite clear; the subject is addressed
in more detail in (Kartsaklis et al., 2013). Keeping a
note of this observation, we now proceed to examine
the performance of our models in a task that does not
use disambiguation as a criterion of composition.

8.2 Phrase/sentence similarity task
Our second set of experiments is based on the phrase
similarity task of Mitchell and Lapata (2010). On
the contrary with the task of Section 8.1, this one
does not involve any assumptions about disambigua-
tion, and thus it seems like a more genuine test of
models aiming to provide appropriate phrasal or sen-
tential semantic representations; the only criterion is
the degree to which these models correctly evaluate
the similarity between pairs of sentences or phrases.
We work on the verb-phrase part of the dataset, con-
sisting of 72 short verb phrases (verb-object struc-
tures). These 72 phrases have been paired in three
different ways to form groups exhibiting various
degrees of similarity: the first group contains 36
pairs of highly similar phrases (e.g. produce effect-
achieve result), the pairs of the second group are
of medium similarity (e.g. write book-hear word),
while a last group contains low-similarity pairs (use
knowledge-provide system). The task is again to
compare the similarity scores given by the various
models for each phrase pair with those of human an-
notators. Additionally to the verb phrases task, we
also perform a richer version of the experiment us-
ing transitive sentences.

Verb phrases It can be shown that for simple verb
phrases the relational model reduces itself to the
copy-subject model; for both of these methods, the
meaning of the verb phrase is calculated according
to Equation 6. Furthermore, according to the copy-
object model the meaning of a verb phrase computed
by a verb matrix

∑
ij vij(

−→ni⊗−→nj) and an object vec-
tor
∑

j oj
−→nj becomes:

verb object
2

=
∑
ij

vijoj(
−→ni ⊗−→nj) (14)

Finally, the Kronecker model has no meaning for
verb phrases, since the vector of a verb phrase
will become (−→vs ⊗ −→vs) ×

−→
obj, which is equal to

〈−→vs |
−→
obj〉−→vs , where 〈−→vs |

−→
obj〉 denotes the inner prod-

uct between vectors of verb and object. Hence, the

1597



Model Ambig. Disamb.
BL Verbs only 0.310 � 0.420
M1 Multiplicative 0.315 � 0.448
M2 Additive 0.291 � 0.436
T1 Rel./Copy-sbj 0.340 � 0.367
T2 Copy-object 0.290 � 0.393

Human agreement 0.550
Difference between M1 and M2 is not s.s.
Difference between M1 and BL is s.s. with p < 0.001

Table 3: Results for the original M&L task.

meaning of a verb phrase becomes a scalar multipli-
cation of the meaning of its verb. As a result, the
cosine distance (used for measuring similarity) be-
tween the meanings of two verb phrases is reduced
to the distance between the vectors of their verbs,
completely dismissing the role of their objects.

Hence our models are limited to those of Table
3. The effects of disambiguation for this task are
quite impressive: the differences between the scores
of all disambiguated models and those of the am-
biguous versions are highly statistically significant
(with p < 0.001), while 4 of the 5 models present
an improvement greater than 10 units of correla-
tion. The models that benefit the most from disam-
biguation are the vector mixtures; both of these ap-
proaches perform significantly better than the best
tensor-based model (copy-object). In fact, the score
of M1 (0.45) is quite high, given that the inter-
annotator agreement is 0.55 (best score reported by
Mitchell and Lapata was 0.41 for their LDA-dilation
model).

Transitive sentences The second part of this ex-
periment aims to examine the extent to which the
above picture can change for the case of text struc-
tures longer than verb phrases. In order to achieve
this, we extend each one of the 72 verb phrases to
a full transitive sentence by adding an appropriate
subject such that the similarity relationships of the
original dataset are retained as much as possible,
so the human judgements for the verb phrase pairs
could as well be used for the transitive cases. We
worked pair-wise: for each pair of verb phrases, we
first selected one of the 5 most frequent subjects for
the first phrase; then, the subject of the other phrase
was selected by a list of synonyms of the first sub-
ject in a way that the new pair of transitive sen-
tences constitutes the least more specific version of
the given verb-phrase pair. So, for example, the pair
produce effect/achieve result became drug produce

effect/medication achieve result, while the pair pose
problem/address question became study pose prob-
lem/paper address question.6

The restrictions of the verb-phrase version do not
hold here, so we evaluate on the full set of models
(Table 4). Once more disambiguation produces bet-
ter results in all cases, with highly statistically sig-
nificant differences for all but one model. Further-
more, now the best score is delivered by one of the
tensor-based models (Kronecker), with a difference
not statistically significant from disambiguated ad-
ditive. In any case, the result suggests that as the
length of the text segments increases, the perfor-
mance of vector mixtures and tensor-based models
converges. Indeed, note how the performance of the
vector mixture models are significantly decreased
compared to the verb phrase task.

9 Discussion

The purpose of this work was twofold: our main ob-
jective was to investigate how disambiguation can
affect the compositional models which are based on
higher order vector spaces; a second, but not less
important goal, was to compare this more linguisti-
cally motivated approach to the simpler vector mix-
ture methods. Based on the experimental work pre-
sented here, we can say with enough confidence that
disambiguation as an additional step prior to com-
position is indeed very beneficial for tensor-based
models. Furthermore, our experiments confirm and
strengthen previous work (Reddy et al., 2011; Kart-
saklis et al., 2013) that showed better performance of
disambiguated vector mixture models compared to
their ambiguous versions. The positive effect of dis-
ambiguation is more evident for the vector mixture
models (especially for the additive model) than for

6The dataset will be available at http://www.cs.ox.
ac.uk/activities/compdistmeaning/.

Model Ambig. Disamb.
BL Verbs only 0.310 � 0.341
M1 Multiplicative 0.325 � 0.404
M2 Additive 0.368 � 0.410
T1 Relational 0.368 � 0.397
T2 Kronecker 0.404 < 0.412
T3 Copy-subject 0.310 � 0.337
T4 Copy-object 0.321 � 0.368

Human agreement 0.550
Difference between T2 and M2 is not s.s.

Table 4: Transitive version of M&L task.

1598



the tensor-based ones. This is expected: composite
representations created by element-wise operations
are averages, and a prior step of disambiguation can
make a great difference.

From a task perspective, the effect of disambigua-
tion was much more definite in the phrase/sentence
similarity task. This observation is really interest-
ing, since the words of that dataset were not se-
lected in order to be ambiguous in any way. The
superior performance of the disambiguated models,
therefore, implies that the proposed methodology
can improve tasks based on phrase or sentence sim-
ilarity regardless of the level of ambiguity in the
vocabulary. For these cases, the proposed disam-
biguation algorithm acts as a fine-tuning process, the
outcome of which seems to be always positive; it
can only produce better composite representations,
not worse. In general, the positive effect of dis-
ambiguation in the phrase/sentence similarity task is
quite encouraging, especially given the fact that this
task constitutes a more appropriate test for evaluat-
ing compositional models, avoiding the pitfalls of
disambiguation-based experiments (as shortly dis-
cussed in Section 8.1).

For disambiguation-based tasks similar to those
of Section 8.1, the form of dataset is very important;
hence the inferior performance of disambiguated
models in the G&S dataset, compared to the dataset
of Kartsaklis et al.. In fact, the G&S dataset was
the only one where disambiguation was not helpful
for some cases (specifically, for vector mixtures and
the Kronecker model). We believe the reason behind
this lies in the fact that the automatic selection of
landmark verbs using the JCN metric (as done with
the G&S dataset) was not very efficient for certain
cases. Note, for example, that the bare baseline of
comparing just ambiguous versions of verbs (with-
out any composition) in that dataset already achieves
a very high correlation of 0.198 with human judge-
ments (Table 1).7. This number is only 0.15 for the
Kartsaklis et al. dataset, due to the more efficient
verb selection procedure. In general, we consider
the results gained by this latter experiment more re-
liable for the specific task, the successful evaluation
of which requires genuinely ambiguous verbs.

The results are less conclusive for the second
question we posed in the beginning of this section,
regarding the comparison of the two classes of mod-

7The reported number for this baseline by Grefenstette and
Sadrzadeh (2011a) was 0.16 using vectors trained from BNC.

els. Despite the obvious benefits of the tensor-based
approaches, this work suggests for one more time
that vector mixture models might constitute a hard-
to-beat baseline; similar observations have been
made, for example, in the comparative study of Bla-
coe and Lapata (2012). However, when trying to in-
terpret the mixing results regarding the effectiveness
of the tensor-based models compared to vector mix-
tures, we need to take into account that the tensor-
based models tested in this work were all “hybrid”,
in the sense that they all involved some element
of point-wise operation; in other words, they con-
stituted a trade-off between transformational power
and complexity.

Even with this compromise, though, the study
presented in Section 8.2 implies that the effective-
ness of each method depends to some extent on the
length of the text segment: when more words are
involved, vector mixture models tend to be less ef-
fective; on the contrary, the performance of tensor-
based models seems to be proportional to the length
of the phrase or sentence—the more, the better.
These observations comply with the nature of the
approaches: “averaging” larger numbers of points
results in more general (hence less accurate) repre-
sentations; on the other hand, a larger number of
arguments makes a function (such as a verb) more
accurate.

10 Conclusion and future work

In the present paper we showed how to improve
a number of tensor-based compositional distribu-
tional models of meaning by introducing a step
of disambiguation prior to composition. Our sim-
ple algorithm (based on the procedure of Schütze
(1998)) creates unambiguous versions of tensors be-
fore these are composed with vectors of nouns in
order to construct vectors for sentences and phrases.
This algorithm is quite generic, and can be applied
to any model that follows the tensor contraction pro-
cess described in Section 4. As for future work, we
aim to investigate the application of this procedure
to the regression model of Grefenstette et al. (2013).

Acknowledgements

We would like to thank Edward Grefenstette for his
comments on the first draft of this paper, as well as
the three anonymous reviewers for their fruitful sug-
gestions. Support by EPSRC grant EP/F042728/1 is
gratefully acknowledged by the authors.

1599



References

Baroni, M. and Zamparelli, R. (2010). Nouns are
Vectors, Adjectives are Matrices. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP).

Blacoe, W. and Lapata, M. (2012). A comparison of
vector-based representations for semantic compo-
sition. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546–556, Jeju Island, Korea. As-
sociation for Computational Linguistics.

Bourbaki, N. (1989). Commutative Algebra: Chap-
ters 1-7. Srpinger Verlag, Berlin/New York.

Caliński, T. and Harabasz, J. (1974). A Dendrite
Method for Cluster Analysis. Communications in
Statistics-Theory and Methods, 3(1):1–27.

Coecke, B., Sadrzadeh, M., and Clark, S.
(2010). Mathematical Foundations for Dis-
tributed Compositional Model of Meaning. Lam-
bek Festschrift. Linguistic Analysis, 36:345–384.

Curran, J. (2004). From Distributional to Seman-
tic Similarity. PhD thesis, School of Informatics,
University of Edinburgh.

Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukWaC, a very large web-derived corpus of En-
glish. In Proceedings of the 4th Web as Corpus
Workshop (WAC-4) Can we beat Google, pages
47–54.

Grefenstette, E., Dinu, G., Zhang, Y.-Z., Sadrzadeh,
M., and Baroni, M. (2013). Multi-step regres-
sion learning for compositional distributional se-
mantics. In Proceedings of the 10th International
Conference on Computational Semantics (IWCS
2013).

Grefenstette, E. and Sadrzadeh, M. (2011a). Exper-
imental Support for a Categorical Compositional
Distributional Model of Meaning. In Proceedings
of Conference on Empirical Methods in Natural
Language Processing (EMNLP).

Grefenstette, E. and Sadrzadeh, M. (2011b). Exper-
imenting with Transitive Verbs in a DisCoCat. In
Proceedings of Workshop on Geometrical Models
of Natural Language Semantics (GEMS).

Harris, Z. (1968). Mathematical Structures of Lan-
guage. Wiley.

Jiang, J. and Conrath, D. (1997). Semantic similar-
ity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference
on Research in Computational Linguistics, pages
19–33, Taiwan.

Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2012). A unified sentence space for categori-
cal distributional-compositional semantics: The-
ory and experiments. In Proceedings of 24th
International Conference on Computational Lin-
guistics (COLING 2012): Posters, pages 549–
558, Mumbai, India. The COLING 2012 Orga-
nizing Committee.

Kartsaklis, D., Sadrzadeh, M., and Pulman, S.
(2013). Separating Disambiguation from Com-
position in Distributional Semantics. In Proceed-
ings of 17th Conference on Computational Nat-
ural Language Learning (CoNLL-2013), Sofia,
Bulgaria.

Kintsch, W. (2001). Predication. Cognitive Science,
25(2):173–202.

Landauer, T. and Dumais, S. (1997). A Solution to
Plato’s Problem: The Latent Semantic Analysis
Theory of Acquision, Induction, and Representa-
tion of Knowledge. Psychological Review.

Manandhar, S., Klapaftis, I., Dligach, D., and Prad-
han, S. (2010). Semeval-2010 task 14: Word
sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Se-
mantic Evaluation, pages 63–68. Association for
Computational Linguistics.

Manning, C., Raghavan, P., and Schütze, H. (2008).
Introduction to Information Retrieval. Cambridge
University Press.

Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics, pages 236–244.

Mitchell, J. and Lapata, M. (2010). Composition
in distributional models of semantics. Cognitive
Science, 34(8):1388–1439.

Müllner, D. (2013). fastcluster: Fast Hierarchical
Clustering Routines for R and Python. Journal of
Statistical Software, 9(53):1–18.

Pickering, M. and Frisson, S. (2001). Processing
ambiguous verbs: Evidence from eye movements.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 27(2):556.

1600



Pulman, S. (2013). Combining Compositional and
Distributional Models of Semantics. In Heunen,
C., Sadrzadeh, M., and Grefenstette, E., editors,
Quantum Physics and Linguistics: A Composi-
tional, Diagrammatic Discourse. Oxford Univer-
sity Press.

Reddy, S., Klapaftis, I., McCarthy, D., and Man-
andhar, S. (2011). Dynamic and static prototype
vectors for semantic composition. In Proceedings
of 5th International Joint Conference on Natural
Language Processing, pages 705–713.

Rosenberg, A. and Hirschberg, J. (2007). V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 410–420.

Schütze, H. (1998). Automatic Word Sense Dis-
crimination. Computational Linguistics, 24:97–
123.

1601


