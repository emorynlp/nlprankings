



















































Beam Search for Solving Substitution Ciphers


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568–1576,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Beam Search for Solving Substitution Ciphers

Malte Nuhn and Julian Schamper and Hermann Ney
Human Language Technology and Pattern Recognition

Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de

Abstract

In this paper we address the problem of
solving substitution ciphers using a beam
search approach. We present a concep-
tually consistent and easy to implement
method that improves the current state of
the art for decipherment of substitution ci-
phers and is able to use high order n-gram
language models. We show experiments
with 1:1 substitution ciphers in which the
guaranteed optimal solution for 3-gram
language models has 38.6% decipherment
error, while our approach achieves 4.13%
decipherment error in a fraction of time
by using a 6-gram language model. We
also apply our approach to the famous
Zodiac-408 cipher and obtain slightly bet-
ter (and near to optimal) results than pre-
viously published. Unlike the previous
state-of-the-art approach that uses addi-
tional word lists to evaluate possible deci-
pherments, our approach only uses a letter-
based 6-gram language model. Further-
more we use our algorithm to solve large
vocabulary substitution ciphers and im-
prove the best published decipherment er-
ror rate based on the Gigaword corpus of
7.8% to 6.0% error rate.

1 Introduction

State-of-the-art statistical machine translation
(SMT) systems use large amounts of parallel data
to estimate translation models. However, parallel
corpora are expensive and not available for every
domain.

Recently different works have been published
that train translation models using only non-
parallel data. Although first practical applications
of these approaches have been shown, the overall

decipherment accuracy of the proposed algorithms
is still low. Improving the core decipherment algo-
rithms is an important step for making decipher-
ment techniques useful for practical applications.

In this paper we present an effective beam
search algorithm which provides high decipher-
ment accuracies while having low computational
requirements. The proposed approach allows us-
ing high order n-gram language models, is scal-
able to large vocabulary sizes and can be adjusted
to account for a given amount of computational
resources. We show significant improvements in
decipherment accuracy in a variety of experiments
while being computationally more effective than
previous published works.

2 Related Work

The experiments proposed in this paper touch
many of previously published works in the deci-
pherment field.

Regarding the decipherment of 1:1 substitution
ciphers various works have been published: Most
older papers do not use a statistical approach and
instead define some heuristic measures for scoring
candidate decipherments. Approaches like (Hart,
1994) and (Olson, 2007) use a dictionary to check
if a decipherment is useful. (Clark, 1998) defines
other suitability measures based on n-gram counts
and presents a variety of optimization techniques
like simulated annealing, genetic algorithms and
tabu search.

On the other hand, statistical approaches for
1:1 substitution ciphers were published in the nat-
ural language processing community: (Ravi and
Knight, 2008) solve 1:1 substitution ciphers opti-
mally by formulating the decipherment problem as
an integer linear program (ILP) while (Corlett and
Penn, 2010) solve the problem using A∗ search.
We use our own implementation of these methods
to report optimal solutions to 1:1 substitution ci-

1568



phers for language model orders n = 2 and n = 3.
(Ravi and Knight, 2011a) report the first au-

tomatic decipherment of the Zodiac-408 cipher.
They use a combination of a 3-gram language
model and a word dictionary. We run our beam
search approach on the same cipher and report
better results without using an additional word
dictionary—just by using a high order n-gram lan-
guage model.

(Ravi and Knight, 2011b) report experiments on
large vocabulary substitution ciphers based on the
Transtac corpus. (Dou and Knight, 2012) improve
upon these results and provide state-of-the-art re-
sults on a large vocabulary word substitution ci-
pher based on the Gigaword corpus. We run our
method on the same corpus and report improve-
ments over the state of the art.

(Ravi and Knight, 2011b) and (Nuhn et al.,
2012) have shown that—even for larger vocabu-
lary sizes—it is possible to learn a full translation
model from non-parallel data. Even though this
work is currently only able to deal with substi-
tution ciphers, phenomena like reordering, inser-
tions and deletions can in principle be included in
our approach.

3 Definitions

In the following we will use the machine trans-
lation notation and denote the ciphertext with
fN1 = f1 . . . fj . . . fN which consists of cipher
tokens fj ∈ Vf . We denote the plaintext with
eN1 = e1 . . . ei . . . eN (and its vocabulary Ve re-
spectively). We define

e0 = f0 = eN+1 = fN+1 = $ (1)

with “$” being a special sentence boundary token.
We use the abbreviations V e = Ve ∪ {$} and V f
respectively.

A general substitution cipher uses a table
s(e|f) which contains for each cipher token f a
probability that the token f is substituted with the
plaintext token e. Such a table for substituting
cipher tokens {A,B,C,D} with plaintext tokens
{a, b, c, d} could for example look like

a b c d
A 0.1 0.2 0.3 0.4
B 0.4 0.2 0.1 0.3
C 0.4 0.1 0.2 0.3
D 0.3 0.4 0.2 0.1

The 1:1 substitution cipher encrypts a given
plaintext into a ciphertext by replacing each plain-
text token with a unique substitute: This means
that the table s(e|f) contains all zeroes, except for
one “1.0” per f ∈ Vf and one “1.0” per e ∈ Ve.
For example the text

abadcab

would be enciphered to

BCBADBC

when using the substitution

a b c d
A 0 0 0 1
B 1 0 0 0
C 0 1 0 0
D 0 0 1 0

In contrast to the 1:1 substitution cipher, the ho-
mophonic substitution cipher allows multiple ci-
pher tokens per plaintext token, which means that
the table s(e|f) is all zero, except for one “1.0” per
f ∈ Vf . For example the above plaintext could be
enciphered to

ABCDECF

when using the homophonic substitution

a b c d
A 1 0 0 0
B 0 1 0 0
C 1 0 0 0
D 0 0 0 1
E 0 0 1 0
F 0 1 0 0

We will use the definition

nmax = max
e

∑

f

s(e|f) (2)

to characterize the maximum number of different
cipher symbols allowed per plaintext symbol.

We formalize the 1:1 substitutions with a bijec-
tive function φ : Vf → Ve and homophonic sub-
stitutions with a general function φ : Vf → Ve.

Following (Corlett and Penn, 2010), we call
cipher functions φ, for which not all φ(f)’s are
fixed, partial cipher functions . Further, φ′ is
said to extend φ, if for all f that are fixed in φ, it
holds that f is also fixed in φ′ with φ′(f) = φ(f).

1569



The cardinality of φ counts the number of fixed
f ’s in φ.

When talking about partial cipher functions we
use the notation for relations, in which φ ⊆ Vf ×
Ve. For example with

φ = {(A, a)} φ′ = {(A, a), (B, b)}

it follows that φ ⊆1φ′ and

|φ| = 1 |φ′| = 2
φ(A) = a φ′(A) = a

φ(B) = undefined φ′(B) = b

The general decipherment goal is to obtain a
mapping φ such that the probability of the deci-
phered text is maximal:

φ̂ = argmax
φ

p(φ(f1)φ(f2)φ(f3)...φ(fN )) (3)

Here p(. . . ) denotes the language model. De-
pending on the structure of the language model
Equation 3 can be further simplified.

4 Beam Search

In this Section we present our beam search ap-
proach to solving Equation 3. We first present the
general algorithm, containing many higher level
functions. We then discuss possible instances of
these higher level functions.

4.1 General Algorithm

Figure 1 shows the general structure of the beam
search algorithm for the decipherment of substi-
tution ciphers. The general idea is to keep track
of all partial hypotheses in two arrays Hs and Ht.
During search all possible extensions of the partial
hypotheses in Hs are generated and scored. Here,
the function EXT ORDER chooses which cipher
symbol is used next for extension, EXT LIMITS
decides which extensions are allowed, and SCORE
scores the new partial hypotheses. PRUNE then se-
lects a subset of these hypotheses which are stored
to Ht. Afterwards the array Hs is copied to Ht
and the search process continues with the updated
array Hs.

Due to the structure of the algorithm the car-
dinality of all hypotheses in Hs increases in each
step. Thus only hypotheses of the same cardinality

1shorthand notation for φ′ extends φ

1: function BEAM SEARCH(EXT ORDER,
EXT LIMITS, PRUNE)

2: init sets Hs, Ht
3: CARDINALITY = 0
4: Hs.ADD((∅, 0))
5: while CARDINALITY < |Vf | do
6: f = EXT ORDER[CARDINALITY]
7: for all φ ∈ Hs do
8: for all e ∈ Ve do
9: φ′ := φ ∪ {(e, f)}

10: if EXT LIMITS(φ′) then
11: Ht.ADD(φ

′,SCORE (φ′))
12: end if
13: end for
14: end for
15: PRUNE(Ht)
16: CARDINALITY = CARDINALITY + 1
17: Hs = Ht
18: Ht.CLEAR()
19: end while
20: return best scoring cipher function in Hs
21: end function

Figure 1: The general structure of the beam
search algorithm for decipherment of substitu-
tion ciphers. The high level functions SCORE,
EXT ORDER, EXT LIMITS and PRUNE are de-
scribed in Section 4.

are compared in the pruning step. When Hs con-
tains full cipher relations, the cipher relation with
the maximal score is returned.2

Figure 2 illustrates how the algorithm explores
the search space for a homophonic substitution ci-
pher. In the following we show several instances
of EXT ORDER, EXT LIMITS, SCORE, and PRUNE.

4.2 Extension Limits (EXT LIMITS)
In addition to the implicit constraint of φ being
a function Vf → Ve, one might be interested in
functions of a specific form:

For 1:1 substitution ciphers
(EXT LIMITS SIMPLE) φ must fulfill that the
number of cipher letters f ∈ Vf that map to any
e ∈ Ve is at most one. Since partial hypotheses
violating this condition can never “recover” when
being extended, it becomes clear that these partial
hypotheses can be left out from search.

2n-best output can be implemented by returning the n best
scoring hypotheses in the final array Hs.

1570



∅

a

b

c

d

a

b
c

d

a

b
c

d

a

b
c

d

a

b
c

d

. . .

. . .

. . .

. . .

. . .

a

b
c

d

a

b
c

d

a

b
c

d

a

b
c

d

. . .

. . .

. . .

. . .

a

b
c

d

a

b
c

d

a

b
c

d

a

b
c

d

B C A D

Figure 2: Illustration of the search space explored by the beam search algorithm with cipher vocabulary
Vf = {A,B,C,D}, plaintext vocabulary Ve = {a, b, c, d}, EXT ORDER = (B,C,A,D), homophonic
extension limits (EXT LIMITS HOMOPHONIC) with nmax = 4, and histogram pruning with nkeep = 4.
Hypotheses are visualized as nodes in the tree. The x-axis represents the extension order. At each level
only those 4 hypotheses that survived the histogram pruning process are extended.

Homophonic substitution ciphers can be han-
dled by the beam search algorithm, too. Here
the condition that φ must fulfill is that the num-
ber of cipher letters f ∈ Vf that map to any
e ∈ Ve is at most nmax (which we will call
EXT LIMITS HOMOPHONIC). As soon as this con-
dition is violated, all further extensions will also
violate the condition. Thus, these partial hypothe-
ses can be left out.

4.3 Score Estimation (SCORE)
The score estimation function needs to predict
how good or bad a partial hypothesis (cipher func-
tion) might become. We propose simple heuristics
that use the n-gram counts rather than the original
ciphertext. The following formulas consider the
2-gram case. Equations for higher n-gram orders
can be obtained analogously.

With Equation 3 in mind, we want to estimate
the best possible score

N+1∏

j=1

p(φ′(fj)|φ′(fj−1)) (4)

which can be obtained by extensions φ′ ⊇ φ. By
defining counts3

Nff ′ =
N+1∑

i=1

δ(f, fi−1)δ(f ′, fi) (5)

3δ denotes the Kronecker delta.

we can equivalently use the scores
∑

f,f ′∈V f

Nff ′ log p(φ
′(f ′)|φ′(f)) (6)

Using this formulation it is easy to propose
a whole class of heuristics: We only present
the simplest heuristic, which we call TRIV-
IAL HEURISTIC. Its name stems from the fact that
it only evaluates those parts of a given φ′ that are
already fixed, and thus does not estimate any fu-
ture costs. Its score is calculated as

∑

f,f ′∈φ′
Nff ′ log p(φ

′(f ′)|φ′(f)). (7)

Here f, f ′ ∈ φ′ denotes that f and f ′ need to
be covered in φ′. This heuristic is optimistic since
we implicitly use “0” as estimate for the non fixed
parts of the sum, for which Nff ′ log p(·|·) ≤ 0
holds.

It should be noted that this heuristic can be im-
plemented very efficiently. Given a partial hypoth-
esis φ with given SCORE(φ) the score of an exten-
sion φ′ can be calculated as

SCORE(φ′) = SCORE(φ) + NEWLY FIXED(φ, φ′)
(8)

where NEWLY FIXED only includes scores for
n-grams that have been newly fixed in φ′ during
the extension step from φ to φ′.

1571



4.4 Extension Order (EXT ORDER)
For the choice which ciphertext symbol should be
fixed next during search, several possibilities ex-
ist: The overall goal is to choose an extension or-
der that leads to an overall low error rate. Intu-
itively it seems a good idea to first try to decipher
higher frequent words rather than the lowest fre-
quent ones. It is also clear that the choice of a good
extension order is dependent on the score estima-
tion function SCORE: The extension order should
lead to informative scores early on so that mislead-
ing hypotheses can be pruned out early.

In most of our experiments we will
make use of a very simple extension order:
HIGHEST UNIGRAM FREQUENCY simply fixes
the most frequent symbols first.

In case of the Zodiac-408, we use another strat-
egy that we call HIGHEST NGRAM COUNT ex-
tension order. In each step it greedily chooses
the symbol that will maximize the number of
fixed ciphertext n-grams. This strategy is use-
ful because the SCORE function we use is TRIV-
IAL HEURISTIC, which is not able to provide in-
formative scores if only few full n-grams are fixed.

4.5 Pruning (PRUNE)
We propose two pruning methods:
HISTOGRAM PRUNING sorts all hypotheses
according to their score and then keeps only the
best nkeep hypotheses.

THRESHOLD PRUNING keeps only those hy-
potheses φkeep for which

SCORE(φkeep) ≥ SCORE(φbest)− β (9)

holds for a given parameter β ≥ 0. Even though
THRESHOLD PRUNING has the advantage of not
needing to sort all hypotheses, it has proven dif-
ficult to choose proper values for β. Due to this,
all experiments presented in this paper only use
HISTOGRAM PRUNING.

5 Iterative Beam Search

(Ravi and Knight, 2011b) propose a so called “it-
erative EM algorithm”. The basic idea is to run a
decipherment algorithm—in their case an EM al-
gorithm based approach—on a subset of the vo-
cabulary. After having obtained the results from
the restricted vocabulary run, these results are used
to initialize a decipherment run with a larger vo-
cabulary. The results from this run will then be
used for a further decipherment run with an even

larger vocabulary and so on. In our large vocabu-
lary word substitution cipher experiments we it-
eratively increase the vocabulary from the 1000
most frequent words, until we finally reach the
50000 most frequent words.

6 Experimental Evaluation

We conduct experiments on letter based 1:1 sub-
stitution ciphers, the homophonic substitution ci-
pher Zodiac-408, and word based 1:1 substitution
ciphers.

For a given reference mapping φref , we eval-
uate candidate mappings φ using two error mea-
sures: Mapping Error Rate MER(φ, φref ) and
Symbol Error Rate SER(φ, φref ). Roughly
speaking, SER reports the fraction of symbols
in the deciphered text that are not correct, while
MER reports the fraction of incorrect mappings
in φ.

Given a set of symbols Veval with unigram
countsN(v) for v ∈ Veval, and the total amount of
running symbols Neval =

∑
v∈Veval

N(v) we define

MER = 1−
∑

v∈Veval

1

|Veval|
· δ(φ(v), φref (v))

(10)

SER = 1−
∑

v∈Veval

N(v)

Neval
· δ(φ(v), φref (v))

(11)

Thus the SER can be seen as a weighted form of
the MER, emphasizing errors for frequent words.
In decipherment experiments, SER will often be
lower than MER, since it is often easier to deci-
pher frequent words.

6.1 Letter Substitution Ciphers

As ciphertext we use the text of the English
Wikipedia article about History4, remove all pic-
tures, tables, and captions, convert all letters to
lowercase, and then remove all non-letter and non-
space symbols. This corpus forms the basis for
shorter cryptograms of size 2, 4, 8, 16, 32, 64, 128,
and 256—of which we generate 50 each. We make
sure that these shorter cryptograms do not end or
start in the middle of a word. We create the ci-
phertext using a 1:1 substitution cipher in which
we fix the mapping of the space symbol ’ ’. This

4http://en.wikipedia.org/wiki/History

1572



Order Beam MER [%] SER [%] RT [s]

3 10 33.15 25.27 0.01
3 100 12.00 6.95 0.06
3 1k 7.37 3.06 0.53
3 10k 5.10 1.42 5.33
3 100k 4.93 1.31 47.70
3 ∞∗ 4.93 1.31 19 700.00
4 10 55.97 48.19 0.02
4 100 18.15 14.41 0.10
4 1k 5.13 3.42 0.89
4 10k 1.55 1.00 8.57
4 100k 0.39 0.06 81.34

5 10 69.19 60.13 0.02
5 100 35.57 29.02 0.14
5 1k 10.89 8.47 1.29
5 10k 0.38 0.06 11.91
5 100k 0.38 0.06 120.38

6 10 74.65 64.77 0.03
6 100 40.26 33.38 0.17
6 1k 13.53 10.08 1.58
6 10k 2.45 1.28 15.77
6 100k 0.09 0.05 151.85

Table 1: Symbol error rates (SER), Mapping er-
ror rates (MER) and runtimes (RT) in dependence
of language model order (ORDER) and histogram
pruning size (BEAM) for decipherment of letter
substitution ciphers of length 128. Runtimes are
reported on a single core machine. Results for
beam size “∞” were obtained using A∗ search.

makes our experiments comparable to those con-
ducted in (Ravi and Knight, 2008). Note that fix-
ing the ’ ’ symbol makes the problem much eas-
ier: The exact methods show much higher com-
putational demands for lengths beyond 256 letters
when not fixing the space symbol.

The plaintext language model we use a letter
based (Ve = {a, . . . , z, }) language model trained
on a subset of the Gigaword corpus (Graff et al.,
2007).

We use extension limits fitting the 1:1 substi-
tution cipher nmax = 1 and histogram pruning
with different beam sizes.

For comparison we reimplemented the ILP ap-
proach from (Ravi and Knight, 2008) as well as
the A∗ approach from (Corlett and Penn, 2010).

Figure 3 shows the results of our algorithm for
different cipher length. We use a beam size of
100k for the 4, 5 and 6-gram case. Most remark-
ably our 6-gram beam search results are signifi-
cantly better than all methods presented in the lit-
erature. For the cipher length of 32 we obtain a
symbol error rate of just 4.1% where the optimal
solution (i.e. without search errors) for a 3-gram

2 4 8 16 32 64 128 256
0

10

20

30

40

50

60

70

80

90

100

Cipher Length
Sy

m
bo

lE
rr

or
R

at
e

(%
)

Exact 2gram
Exact 3gram
Beam 3gram
Beam 4gram
Beam 5gram
Beam 6gram

Figure 3: Symbol error rates for decipherment of
letter substitution ciphers of different lengths. Er-
ror bars show the 95% confidence interval based
on decipherment on 50 different ciphers. Beam
search was performed with a beam size of “100k”.

language model has a symbol error rate as high as
38.3%.

Table 1 shows error rates and runtimes of our
algorithm for different beam sizes and language
model orders given a fixed ciphertext length of 128
letters. It can be seen that achieving close to op-
timal results is possible in a fraction of the CPU
time needed for the optimal solution: In the 3-
gram case the optimal solution is found in 1400 th
of the time needed using A∗ search. It can also
be seen that increasing the language model order
does not increase the runtime much while provid-
ing better results if the beam size is large enough:
If the beam size is not large enough, the decipher-
ment accuracy decreases when increasing the lan-
guage model order: This is because the higher or-
der heuristics do not give reliable scores if only
few n-grams are fixed.

To summarize: The beam search method is sig-
nificantly faster and obtains significantly better re-
sults than previously published methods. Further-
more it offers a good trade-off between CPU time
and decipherment accuracy.

1573



i l i k e k i l l i n g p e o p l

e b e c a u s e i t i s s o m u c

h f u n i t i n m o r e f u n t h

a n k i l l i n g w i l d g a m e

i n t h e f o r r e s t b e c a u

s e m a n i s t h e m o a t r a n

g e r o u e a n a m a l o f a l l

t o k i l l s o m e t h i n g g i

Figure 4: First 136 letters of the Zodiac-408 cipher
and its decipherment.

6.2 Zodiac-408 Cipher

As ciphertext we use a transcription of the
Zodiac-408 cipher. It consists of 54 different sym-
bols and has a length of 408 symbols.5 The ci-
pher has been deciphered by hand before. It con-
tains some mistakes and ambiguities: For exam-
ple, it contains misspelled words like forrest (vs.
forest), experence (vs. experience), or paradice
(vs. paradise). Furthermore, the last 17 letters
of the cipher do not form understandable English
when applying the same homophonic substitution
that deciphers the rest of the cipher. This makes
the Zodiac-408 a good candidate for testing the ro-
bustness of a decipherment algorithm.

We assume a homophonic substitution cipher,
even though the cipher is not strictly homophonic:
It contains three cipher symbols that correspond
to two or more plaintext symbols. We ignore this
fact for our experiments, and count—in case of the
MER only—the decipherment for these symbols
as correct when the obtained mapping is contained
in the set of reference symbols. We use extension
limits with nmax = 8 and histogram pruning
with beam sizes of 10k up to 10M .

The plaintext language model is based on the
same subset of Gigaword (Graff et al., 2007) data
as the experiments for the letter substitution ci-
phers. However, we first removed all space sym-

5hence its name

Order Beam MER [%] SER [%] RT [s]

4 10k 71.43 67.16 222
4 100k 66.07 61.52 1 460
4 1M 39.29 34.80 12 701
4 10M 19.64 16.18 125 056

5 10k 94.64 96.57 257
5 100k 10.71 5.39 1 706
5 1M 8.93 3.19 14 724
5 10M 8.93 3.19 152 764

6 10k 87.50 84.80 262
6 100k 94.64 94.61 1 992
6 1M 8.93 2.70 17 701
6 10M 7.14 1.96 167 181

Table 2: Symbol error rates (SER), Mapping er-
ror rates (MER) and runtimes (RT) in dependence
of language model order (ORDER) and histogram
pruning size (BEAM) for the decipherment of the
Zodiac-408 cipher. Runtimes are reported on a
128-core machine.

bols from the training corpus before training the
actual letter based 4-gram, 5-gram, and 6-gram
language model on it. Other than (Ravi and
Knight, 2011a) we do not use any word lists and
by that avoid any degrees of freedom in how to in-
tegrate it into the search process: Only an n-gram
language model is used.

Figure 4 shows the first parts of the cipher and
our best decipherment. Table 2 shows the results
of our algorithm on the Zodiac-408 cipher for dif-
ferent language model orders and pruning settings.

To summarize: Our final decipherment—for
which we only use a 6-gram language model—has
a symbol error rate of only 2.0%, which is slightly
better than the best decipherment reported in (Ravi
and Knight, 2011a). They used an n-gram lan-
guage model together with a word dictionary and
obtained a symbol error rate of 2.2%. We thus ob-
tain better results with less modeling.

6.3 Word Substitution Ciphers

As ciphertext, we use parts of the JRC corpus
(Steinberger et al., 2006) and the Gigaword cor-
pus (Graff et al., 2007). While the full JRC corpus
contains roughly 180k word types and consists of
approximately 70M running words, the full Giga-
word corpus contains around 2M word types and
roughly 1.5G running words.

We run experiments for three different setups:
The “JRC” and “Gigaword” setups use the first
half of the respective corpus as ciphertext, while
the plaintext language model of order n = 3 was

1574



Setup Top MER [%] SER [%] RT [hh:mm]

Gigaword 1k 81.91 27.38 03h 10m
Gigaword 10k 30.29 8.55 09h 21m
Gigaword 20k 21.78 6.51 16h 25m
Gigaword 50k 19.40 5.96 49h 02m

JRC 1k 73.28 15.42 00h 32m
JRC 10k 15.82 2.61 13h 03m

JRC-Shuf 1k 76.83 19.04 00h 31m
JRC-Shuf 10k 15.08 2.58 13h 03m

Table 3: Word error rates (WER), Mapping error
rates (MER) and runtimes (RT) for iterative deci-
pherment run on the (TOP) most frequent words.
Error rates are evaluated on the full vocabulary.
Runtimes are reported on a 128-core machine.

trained on the second half. The “JRC-Shuf” setup
is created by randomly selecting half of the sen-
tences of the JRC corpus as ciphertext, while the
language model was trained on the complemen-
tary half of the corpus.

We encrypt the ciphertext using a 1:1 substi-
tution cipher on word level, imposing a much
larger vocabulary size. We use histogram prun-
ing with a beam size of 128 and use extension
limits of nmax = 1. Different to the previous
experiments, we use iterative beam search with
iterations as shown in Table 3.

The results for the Gigaword task are directly
comparable to the word substitution experiments
presented in (Dou and Knight, 2012). Their fi-
nal decipherment has a symbol error rate of 7.8%.
Our algorithm obtains 6.0% symbol error rate. It
should be noted that the improvements of 1.8%
symbol error rate correspond to a larger improve-
ment in terms of mapping error rate. This can also
be seen when looking at Table 3: An improvement
of the symbol error rate from 6.51% to 5.96% cor-
responds to an improvement of mapping error rate
from 21.78% to 19.40%.

To summarize: Using our beam search algo-
rithm in an iterative fashion, we are able to im-
prove the state-of-the-art decipherment accuracy
for word substitution ciphers.

7 Conclusion

We have presented a simple and effective beam
search approach to the decipherment problem. We
have shown in a variety of experiments—letter
substitution ciphers, the Zodiac-408, and word
substitution ciphers—that our approach outper-
forms the current state of the art while being con-

ceptually simpler and keeping computational de-
mands low.

We want to note that the presented algorithm is
not restricted to 1:1 and homophonic substitution
ciphers: It is possible to extend the algorithm to
solve n:m mappings. Along with more sophis-
ticated pruning strategies, score estimation func-
tions, and extension orders, this will be left for fu-
ture research.

Acknowledgements

This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. Experiments were
performed with computing resources granted by
JARA-HPC from RWTH Aachen University un-
der project “jara0040”.

References
Andrew J. Clark. 1998. Optimisation heuristics for

cryptology. Ph.D. thesis, Faculty of Information
Technology, Queensland University of Technology.

Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040–1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.

Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 266–275,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.

David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Linguistic Data Consortium, Philadelphia.

George W. Hart. 1994. To decode short cryptograms.
Communications of the Association for Computing
Machinery (CACM), 37(9):102–108, September.

Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156–164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.

Edwin Olson. 2007. Robust dictionary attack of
short simple substitution ciphers. Cryptologia,
31(4):332–342, October.

1575



Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812–819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.

Sujith Ravi and Kevin Knight. 2011a. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239–247, Portland, Oregon, June. Association for
Computational Linguistics.

Sujith Ravi and Kevin Knight. 2011b. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12–21, Portland, Oregon, USA, June.
Association for Computational Linguistics.

Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaž Erjavec, and Dan Tufiş. 2006.
The JRC-Acquis: A multilingual aligned parallel
corpus with 20+ languages. In In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC), pages 2142–2147.
European Language Resources Association.

1576


