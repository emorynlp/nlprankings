










































Entropy-based Pruning for Phrase-based Machine Translation


Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 962–971, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics

Entropy-based Pruning for Phrase-based Machine Translation

Wang Ling, João Graça, Isabel Trancoso, Alan Black
L2F Spoken Systems Lab, INESC-ID, Lisboa, Portugal

Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
{wang.ling,joao.graca,isabel.trancoso}@inesc-id.pt

awb@cs.cmu.edu

Abstract

Phrase-based machine translation models
have shown to yield better translations than
Word-based models, since phrase pairs en-
code the contextual information that is needed
for a more accurate translation. However,
many phrase pairs do not encode any rele-
vant context, which means that the transla-
tion event encoded in that phrase pair is led
by smaller translation events that are indepen-
dent from each other, and can be found on
smaller phrase pairs, with little or no loss in
translation accuracy. In this work, we pro-
pose a relative entropy model for translation
models, that measures how likely a phrase pair
encodes a translation event that is derivable
using smaller translation events with similar
probabilities. This model is then applied to
phrase table pruning. Tests show that con-
siderable amounts of phrase pairs can be ex-
cluded, without much impact on the transla-
tion quality. In fact, we show that better trans-
lations can be obtained using our pruned mod-
els, due to the compression of the search space
during decoding.

1 Introduction

Phrase-based Machine Translation Models (Koehn
et al., 2003) model n-to-m translations of n source
words to m target words, which are encoded in
phrase pairs and stored in the translation model.
This approach has an advantage over Word-based
Translation Models (Brown et al., 1993), since trans-
lating multiple source words allows the context for
each source word to be considered during trans-

lation. For instance, the translation of the En-
glish word “in” by itself to Portuguese is not ob-
vious, since we do not have any context for the
word. This word can be translated in the con-
text of “in (the box)” to “dentro”, or in the con-
text of “in (China)” as “na”. In fact, the lexical
entry for “in” has more than 10 good translations
in Portuguese. Consequently, the lexical translation
entry for Word-based models splits the probabilis-
tic mass between different translations, leaving the
choice based on context to the language model. On
the other hand, in Phrase-based Models, we would
have a phrase pair p(in the box, dentro da caixa)
and p(in china, na china), where the words “in the
box” and “in China” can be translated together to
“dentro da caixa” and “na China”, which substan-
tially reduces the ambiguity. In this case, both the
translation and language models contribute to find
the best translation based on the local context, which
generally leads to better translations.

However, not all words add the same amount of
contextual information. Using the same example for
“in”, if we add the context “(hid the key) in”, it is
still not possible to accurately identify the best trans-
lation for the word “in”. The phrase extraction algo-
rithm (Ling et al., 2010) does not discriminate which
phrases pairs encode contextual information, and ex-
tracts all phrase pairs with consistent alignments.
Hence, phrases that add no contextual information,
such as, p(hid the key in, escondeu a chave na)
and p(hid the key in, escondeu a chave dentro)
are extracted. This is undesirable because we are
populating translation models with redundant phrase
pairs, whose translations can be obtained using com-

962



binations of other phrases with the same probabil-
ities, namely p(hid the key, escondeu a chave),
p(in, dentro) and p(in, na). This is a problem
that is also found in language modeling, where
large amounts of redundant higher-order n-grams
can make the model needlessly large. For backoff
language models, multiple pruning strategies based
on relative entropy have been proposed (Seymore
and Rosenfeld, 1996) (Stolcke, 1998), where the ob-
jective is to prune n-grams in a way to minimize the
relative entropy between the model before and after
pruning.

While the concept of using relative entropy for
pruning is not new and frequently used in backoff
language models, there are no such models for ma-
chine translation. Thus, the main contribution of
our work is to propose a relative entropy pruning
model for translation models used in Phrase-based
Machine Translation. It is shown that our pruning
algorithm can eliminate phrase pairs with little or
no impact in the predictions made in our translation
model. In fact, by reducing the search space, less
search errors are made during decoding, which leads
to improvements in translation quality.

This paper is organized as follows. We describe
and contrast the state of the art pruning algorithms
in section 2. In section 3, we describe our relative-
entropy model for machine translation. Afterwards,
in section 4, we apply our model for pruning in
Phrase-based Machine Translation systems. We per-
form experiments with our pruning algorithm based
on phrase pair independence and analyse the results
in section 5. Finally, we conclude in section 6.

2 Phrase Table Pruning

Phrase table pruning algorithms are important in
translation, since they efficiently reduce the size of
the translation model, without having a large nega-
tive impact in the translation quality. This is espe-
cially relevant in environments where memory con-
straints are imposed, such as translation systems for
small devices like cellphones, and also when time
constraints for the translation are defined, such as
online Speech-to-Speech systems.

2.1 Significance Pruning

A relevant reference in phrase table pruning is the
work of (Johnson and Martin, 2007), where it is
shown that a significant portion of the phrase ta-
ble can be discarded without a considerable negative
impact on translation quality, or even positive one.
This work computes the probability, named p-value,
that the joint occurrence event of the source phrase
s and target phrase t occurring in same sentence pair
happens by chance, and are actually statistically in-
dependent. Phrase pairs that have a high p-value,
are more likely to be spurious and more prone to
be pruned. This work is followed in (Tomeh et al.,
2009), where phrase pairs are treated discriminately
based on their complexity. Significance-based prun-
ing has also been successfully applied in language
modeling in (Moore and Quirk, 2009).

Our work has a similar objective, but instead
of trying to predict the independence between the
source and target phrases in each phrase pair, we at-
tempt to predict the independence between a phrase
pair and other phrase pairs in the model.

2.2 Relevance Pruning

Another proposed approach (Matthias Eck and
Waibel, 2007) consists at collecting usage statistics
for phrase pairs. This algorithm decodes the train-
ing corpora and extracts the number of times each
phrase pair is used in the 1-best translation hypoth-
esis. Thus, phrase pairs that are rarely used during
decoding are excluded first during pruning.

This method considers the relationship between
phrase pairs in the model, since it tests whether
the decoder is more prone to use some phrase pairs
than others. However, it leads to some undesirable
pruning choices. Let us consider a source phrase
“the box in China” and 2 translation hypotheses,
where the first hypothesis uses the phrase transla-
tion p(the key in China, a chave na China) with
probability 70%, and the second hypothesis uses
two phrase translations p(the key, a chave) and
p(in China, na China) with probability 65%. This
approach will lean towards pruning the phrase pairs
in the second hypothesis, since the decoder will use
the first hypothesis. This is generally not desired,
since the 2 smaller phrase pairs can be used to trans-
late the same source sentence with a small probabil-

963



ity loss (5%), even if the longer phrase is pruned.
On the other hand, if the smaller phrases are pruned,
the longer phrase can not be used to translate smaller
chunks, such as “the key in Portugal”. This matter is
aggravated due to the fact that the training corpora is
used to decode, so longer phrase pairs will be used
more frequently than when translating unseen sen-
tences, which will make the model more biased into
pruning shorter phrase pairs.

3 Relative Entropy Model For
Phrase-based Translation Models

In this section, we shall define our entropy model
for phrase pairs. We start by introducing some no-
tation to distinguish different types of phrase pairs
and show why some phrase pairs are more redun-
dant than others. Afterwards, we illustrate our no-
tion of relative entropy between phrase pairs. Then,
we describe our entropy model, its computation and
its application to phrase table pruning.

3.1 Atomic and Composite Phrase Pairs

We discriminate between 2 types of phrase pairs:
atomic phrase pairs and composite phrase pairs.

Atomic phrase pairs define the smallest transla-
tion units, such that given an atomic phrase pair that
translates from s to t, the same translation cannot
be obtained using any combination of other phrase
pairs. Removing these phrase pairs reduces the
range of translations that our model is capable of
translating and also the possible translations.

Composite phrase pairs define translations of a
given sequence of words that can also be obtained
using atomic or other smaller composite phrase
pairs. Each combination is called a derivation or
translation hypothesis. Removing these phrase pairs
does not change the amount of sentences that the
model can translate, since all translations encoded
in these phrases can still be translated using other
phrases, but these will lead to different translation
probabilities.

Considering table 1, we can see that atomic
phrases encode one elementary translation event,
while composite phrases encode joint events that are
encoded in atomic phrase pairs. If we look at the
source phrase “in”, there is a multitude of possible
translations for this word in most target languages.

Taking Portuguese as the target language, the proba-
bility that “in” is translated to “em” is relatively low,
since it can also be translated to “no”, “na”, “den-
tro”, “dentro de” and many others.

However, if we add another word such as “Por-
tugal” forming “in Portugal”, it is more likely that
“in” is translated to “em”. Thus, we define the
joint event of “in” translating to “em” (A1) and
“Portugal” to “Portugal” (B1), denoted as A1 ∩ B1,
in the phrase pair p(in Portugal, em Portugal).
Without this phrase pair it is assumed that these
are independent events with probability given by
P (A1)P (B1)

1, which would be 10%, leading to a
60% reduction. In this case, it would be more likely,
that in Portugal is translated to no Portugal or
na Portugal, which would be incorrect.

Some words, such as “John”, forming “John in”,
do not influence the translations for the word “in”,
since it can still be translated to “em”, “no”, “na”,
“dentro” or “dentro de” depending on the word that
follows. By definition, if the presence of phrase
p(John, John) does not influence the translation of
p(in, em) and viceversa, we can say that probability
of the joint event P (A1∩C1) is equal to the product
of the probabilities of the events P (A1)P (C1).

If we were given a choice of pruning either the
composite phrase pairs p(John in, John em) or
p(in Portugal, em Portugal), the obvious choice
would be the former, since the probability of the
event encoded in that phrase pair is composed by 2
independent events, in which case the decoder will
inherently consider the hypothesis that “John in” is
translated to “John em” with the same probability. In
another words, the model’s predictions even, with-
out this phrase pair will remain the same.

The example above shows an extreme case,
where the event encoded in the phrase pair
p(John in, John em) is decomposed into indepen-
dent events, and can be removed without chang-
ing the model’s prediction. However, finding and
pruning phrase pairs that are independent, based on
smaller events is impractical, since most translation
events are not strictly independent. However, many
phrase pairs can be replaced with derivations using
smaller phrases with a small loss in the model’s pre-

1For simplicity, we assume at this stage that no reordering
model is used

964



Phrase Pair Prob Event
Atomic Phrase Pairs

in→ em 10% A1
in→ na 20% A2
in→ no 20% A3

in→ dentro 5% A4
in→ dentro de 5% A5

Portugal→ Portugal 100% B1
John→ John 100% C1

Composite Phrase Pairs
in Portugal→ em Portugal 70% A1 ∩B1

John in→ John em 10% C1 ∩A1
John in→ John na 20% C1 ∩A2
John in→ John no 20% C1 ∩A3

John in→ John dentro 5% C1 ∩A4
John in→ John dentro de 5% C1 ∩A5

Table 1: Phrase Translation Table with associated events

dictions.
Hence, we would like to define a metric for phrase

pairs that allows us evaluate how discarding each
phrase pair will affect the pruned model’s predic-
tions. By removing phrase pairs that can be derived
using smaller phrase pairs with similar probability,
it is possible to discard a significant portion of the
translation model, while minimizing the impact on
the model’s predictions.

3.2 Relative Entropy Model for Machine
Translation

For each phrase pair pa, we define the supporting
set SP (pa(s, t)) = S1, ..., Sk, where each element
Si = pi, ..., pj is a distinct derivation of pa(s, t) that
translates s to t, with probability P (Si) = P (pi) ×
...×P (pj). A phrase pair can have multiple elements
in its supporting set. For instance, the phrase pair
p(John in Portugal, John em Portugal), has 3
elements in the support set:

• S1 = {p(John, John), p(in, em), p(Portugal, Portugal)}

• S2 = {p(John, John), p(in Portugal, em Portugal)}

• S3 = {p(John in, John em), p(Portugal, Portugal)}

S1, S2 and S3 encode 3 different assumptions
about the event of translating “John in Portugal”
to “John em Portugal”. S1 assumes that the event
is composed by 3 independent events A1, B1 and
C1, S2 assumes that A1 and B1 are dependent, and

groups them into a single composite event A1 ∩B1,
which is independent from C1, and S3 groups A1
and C1 independently from B1. As expected, the
event encoded in the phrase pair p itself isA1∩B1∩
C1, which assumes thatA1,B1 andC1 are all depen-
dent. We can see that if any of the events S1, S2 or
S3 has a “similar probability” as the event coded in
the phrase pair, we can remove this phrase pair with
a minimal impact in the phrase prediction.

To formalize our notion of “similar probabil-
ity”, we apply the relative entropy or the Kullback-
Leibler divergence, and define the divergence be-
tween a pruned translation model Pp(s, t) and the
unpruned model P (s, t) as:

D(Pp||P ) = −
∑
s,t

P (s, t)log
Pp(t|s)
P (t|s)

(1)

Where Pp(t|s)P (t|s) , measures the deviation from the
probability emission from the pruned model and the
original probability from the unpruned model, for
each source-target pair s, t. This is weighted by
the frequency that the pair s, t is observed, given by
P (s, t).

Our objective is to minimize D(Pp||P ), which
can be done locally by removing phrase pairs p(s, t)
with the lowest values for −P (s, t)logPp(t|s)P (t|s) . Ide-
ally, we would want to minimize the relative entropy
for all possible source and target sentences, rather
than all phrases in our model. However, minimiz-
ing such an objective function would be intractable
due to reordering, since the probability assigned to a
phrase pair in a sentence pair by each model would
depend on the positioning of all other phrase pairs
used in the sentence. Because of these dependen-
cies, we would not be able to reduce this problem to
a local minimization problem. Thus, we assume that
all phrase pairs have the same probability regardless
of their context in a sentence.

Thus, our pruning algorithm takes a threshold δ
and prunes all phrase pairs that fail to meet the fol-
lowing criteria:

−P (s, t)logPp(t|s)
P (t|s)

> δ (2)

The main components of this function is the ratio
between the emission from the pruned model and

965



unpruned models given by Pp(t|s)P (t|s) , and the weight
given to each s, t pair given by P (s, t). In the re-
mainder of this section, we will focus on how to
model each of these components in equation 2.

3.3 Computing P (s, t)

The term P (s, t) can be seen as a weighting function
for each s, t pair. There is no obvious optimal dis-
tribution to model P (s, t). In this work, we apply 2
different distributions for P (s, t). First, an uniform
distribution, where all phrases are weighted equally.
Secondly, a multinomial function defined as:

P (s, t) =
N(s, t)

N
(3)

whereN is the number of sentence pairs in the paral-
lel data, and N(s, t) is the number of sentence pairs
where s was observed in the source sentence and t
was observed in the target sentence. Using this dis-
tribution, the model is more biased in pruning phrase
pairs with s, t pairs that do not occur frequently.

3.4 Computing Pp(t|s)P (t|s)

The computation of Pp(t|s)P (t|s) depends on how the de-
coder adapts when a phrase pair is pruned from the
model. In the case of back-off language models,
this can be solved by calculating the difference of
the logs between the n-gram estimate and the back-
off estimate. However, a translation decoder gen-
erally functions differently. In our work, we will
assume that the decoding will be performed using
a Viterbi decoder, such as MOSES (Koehn et al.,
2007), where the translation with the highest score
is chosen.

In the example above, where s=”John in Portu-
gal” and t=”John em Portugal”, the decoder would
choose the derivation with the highest probability
from s to t. Using the unpruned model, the possi-
ble derivations are either using phrase p(s, t) or one
element of its support set S1, S2 or S3. On the other
hand, on the pruned model where p(s, t) does not
exist, only S1, S2 and S3 can be used. Thus, given
a s, t pair one of three situations may occur. First, if
the probability of the phrase pair p(s, t) is lower than
the highest probability element in SP (p(s, t)), then
both the models will choose that element, in which
case, Pp(t|s)P (t|s) = 1. This can happen, if we define

features that penalize longer phrase pairs, such as
lexical weighting, or if we apply smoothing (Foster
et al., 2006). Secondly, if the probability of p(s, t)
is equal to the most likely element in SP (p(s, t)),
regardless of whether the unpruned model choses to
use p(s, t) or that element, the probability emissions
of the pruned and unpruned model will be identi-
cal. Thus, for this case Pp(t|s)P (t|s) = 1. Finally, if the
probability of p(s, t) is higher than other possible
derivations, the unpruned model will choose to emit
the probability of p(s, t), while the pruned model
will emit the most likely element in SP (p(s, t)).
Hence, the probability loss between the 2 models,
will be the ratio between the probability of p(s, t)
and the probability of the most likely element in
SP (p(s, t)).

From the example above, we can generalize the
function for Pp(t|s)P (t|s) as:∏

p′∈argmax(SP (p(s,t))) P (p
′)

P (p(s, t))
(4)

Where P (p(s, t)) denotes the probability of
p(s, t) and

∏
p′∈argmax(SP (p(s,t))) P (p

′) the most
likely sequence of phrasal translations that translates
s to t, with the probability equal to the product of all
phrase translation probabilities in that sequence.

Replacing in equation 2, our final condition that
must be satisfied for keeping a phrase pair is:

−P (s, t)log
∏

p′∈argmax(SP (p(s,t))) P (p
′)

P (p(s, t))
> δ (5)

4 Application for Phrase-based Machine
Translation

We will now show how we apply our entropy prun-
ing model in the state-of-the-art phrase-based trans-
lation system MOSES and describe the problems
that need to be addressed during the implementation
of this model.

4.1 Translation Model

The translation model in Moses is composed by
a phrase translation model and a phrase reorder-
ing model. The first one models, for each phrase
pair p(s, t), the probability of translating the s to
t by combining multiple features φi, weighted by

966



wTi , as PT (p) =
∏n

i=1 φi(p)
wTi . The reordering

model is similar, but models the local reordering be-
tween p, given the previous and next phrase accord-
ing to the target side, pP and pN , or more formally,
PR(p|pP , pN ) =

∏m
i=1 ψi(p|pP , pP )w

R
i

4.2 Building the Support Set
Essentially, implementing our model is equiva-
lent to calculating the components described in
equation 5. These are P (s, t), P (p(s|t)) and
argmax(SP (p(s, t))). Calculating the uniform dis-
tribution and multinomial distributions for P (s, t)
is simple, the uniform distribution just assumes the
same value for all s and t, and the multinomial dis-
tribution can be modeled by extracting counts from
the parallel corpora.

Calculating P (s|t) is also trivial, since it only en-
volves calculating PT (p(s, t)), which can be done
by retrieving the translation features of p and apply-
ing the weights for each feature.

The most challenging task is to calculate
argmax(SP (p(s, t))), which is similar to the de-
coding task in machine translation, where we need to
find the best translation t̂ for a sentence s, that is, t̂ =
argmaxtP (s|t)P (t). In practice, we are not search-
ing in the space of possible translations, but in the
space of possible derivations, which are sequences
of phrase translations p1(s1, t1), ..., pn(sn, tn) that
can be applied to s to generate an output t with the
score given by P (t)

∏n
i=1 P (si, ti).

Our algorithm to determine SP (p(s, t)) can be
described as an adaptation to the decoding algorithm
in Moses, where we restrict the search space to the
subspace SP (p(s, t)), that is, our search space is
only composed by derivations that output t, with-
out using p itself. This can be done using the forced
decoding algorithm proposed in (Schwartz, 2008).
Secondly, the score of a given translation hypothesis
does not depend on the language model probability
P (t), since all derivations in this search space have
the same t, thus we discard this probability from
the score function. Finally, rather than using beam
search, we exhaustively search all the search space,
to reduce the hypothesis of incurring a search error
at this stage. This is possible, since phrase pairs are
generally smaller than text (less than 8 words), and
because we are constraining the search space to t,
which is an order of magnitude smaller than the reg-

ular search space with all possible translations.

4.3 Pruning Algorithm
The algorithm to generate a pruned translation
model is shown in 1. We iterate over all phrase pairs
p1(s1, t1), ..., pn(sn, tn), decode using our forced
decoding algorithm from si to ti, to obtain the best
path S. If no path is found then it means that the pi
is atomic. Then, we prune pi based on condition 5.

Algorithm 1 Independence Pruning
Require: pruning threshold δ,

unpruned model {p1(s1, t1), ..., pn(sn, tn)}
for pi(si, ti) ∈ {p1(s1, t1), ..., pn(sn, tn)} do
S := argmax(SP (pi)) \ pi
score :=∞
if S 6= {} then
score := −P (s, t)log

∏
p′(s′,t′)∈S P (s

′|t′)
P (s|t)

end if
if score ≤ δ then
prune(pi)

end if
end for
return pruned model

The main bottle neck in this algorithm is find-
ing argmax(SP (pi)). While this appears relatively
simple and similar to a document decoding task, the
size of our task is on a different order of magni-
tude, since we need to decode every phrase pair in
the translation model, which might not be tractable
for large models with millions of phrase pairs. We
address this problem in section 5.3.

Another problem with this algorithm is that the
decision to prune each phrase pair is made assuming
that all other phrase pairs will remain in the model.
Thus, there is a chance a phrase pair p1 is pruned
because of a derivation using p2 and p3 that leads to
the same translation. However, if p3 also happens to
be pruned, such a derivation will no longer be pos-
sible. One possible solution to address this problem
is to perform pruning iteratively, from the smallest
phrase pairs (number of words) and increase the size
at each iteration. However, we find this undesirable,
since the model will be biased into removing smaller
phrase pairs, which are generally more useful, since
they can be used in multiple derivation to replace
larger phrase pairs. In the example above, the model

967



would eliminate p3 and keep p1, yet the best deci-
sion could be to keep p3 and remove p1, if p3 is also
frequently used in derivations of other phrase pairs.
Thus, we leave the problem of finding the best set of
phrases to prune as future work.

5 Experiments

We tested the performance of our system under two
different environments. The first is the small scale
DIALOG translation task for IWSLT 2010 evalua-
tion (Paul et al., 2010) using a small corpora for
the Chinese-English language pair (henceforth re-
ferred to as “IWSLT”). The second one is a large
scale test using the complete EUROPARL (Koehn,
2005) corpora for the Portuguese-English language
pair, which we will denote by “EUROPARL”.

5.1 Corpus

The IWSLT model was trained with 30K training
sentences. The development corpus and test corpus
were taken from the evaluation dataset in IWSLT
2006 (489 tuning and 500 test sentences with 7 ref-
erences). The EUROPARL model was trained using
the EUROPARL corpora with approximately 1.3M
sentence pairs, leaving out 1K sentences for tuning
and another 1K sentences for tests.

5.2 Setup

In the IWSLT experiment, word alignments were
generated using an HMM model (Vogel et al., 1996),
with symmetric posterior constraints (V. Graça et
al., 2010), using the Geppetto toolkit2. This setup
was used in the official evaluation in (Ling et al.,
2010). For the EUROPARL experiment the word
alignments were generated using IBM model 4. In
both experiments, the translation model was built
using the phrase extraction algorithm (Paul et al.,
2010), with commonly used features in Moses (Ex:
probability, lexical weighting, lexicalized reordering
model). The optimization of the translation model
weights was done using MERT tuning (Och, 2003)
and the results were evaluated using BLEU-4.

5.3 Pruning Setup

Our pruning algorithm is applied after the translation
model weight optimization with MERT. We gener-

2http://code.google.com/p/geppetto/

ate multiple translation models by setting different
values for δ, so that translation models of different
sizes are generated at intervals of 5%. We also run
the significance pruning (Johnson and Martin, 2007)
algorithm in these conditions.

While the IWSLT translation model has only
88,424 phrase pairs, for the EUROPARL exper-
iment, the translation model was composed by
48,762,372 phrase pairs, which had to be decoded.
The average time to decode each phrase pair us-
ing the full translation model is 4 seconds per sen-
tence, since the table must be read from disk due to
its size. This would make translating 48M phrase
pairs unfeasible. To address this problem, we di-
vide the phrase pairs in the translation model into
blocks of K phrase pairs, that are processed sepa-
rately. For each block, we resort to the approach
used in MERT tuning, where the model is filtered to
only include the phrase pairs that are used for trans-
lating tuning sentences. We filter each block with
phrase pairs fromK to 2K with the source sentences
sK , ..., s2K . Furthermore, since we are force de-
coding using the target sentences, we also filter the
remaining translation models using the target sen-
tences tK , ..., t2K . We used blocks of 10,000 phrase
pairs and each filtered table was reduced to less than
1% of the translation table on average, reducing the
average decoding time to 0.03 seconds per sentence.
Furthermore, each block can be processed in parallel
allowing multiple processes to be used for the task,
depending on the resources that are available.

5.4 Results

Figure 1 shows the BLEU results for different sizes
of the translation model for the IWSLT experiment
using the uniform and multinomial distributions for
P (s, t). We observe that there is a range of values
from 65% to 95% where we actually observe im-
provements caused by our pruning algorithm, with
the peak at 85% for the uniform distribution, where
we improve from 15.68 to 15.82 (0.9% improve-
ment). Between 26% and 65%, the BLEU score is
lower than the baseline at 100%, with the minimum
at 26% with 15.54, where only atomic phrase pairs
remain and both the multinomial and uniform distri-
bution have the same performance, obviously. This
is a considerable reduction in phrase table size by
sacrificing 0.14 BLEU points. Regarding the com-

968



15.5	  

15.55	  

15.6	  

15.65	  

15.7	  

15.75	  

15.8	  

15.85	  

25
%
	  

30
%
	  

35
%
	  

40
%
	  

45
%
	  

50
%
	  

55
%
	  

60
%
	  

65
%
	  

70
%
	  

75
%
	  

80
%
	  

85
%
	  

90
%
	  

95
%
	  

10
0%

	  

IWSLT	  Results	  

Uniform	  

Mul8nomial	  

Figure 1: Results for the IWSLT experiment. The x-
axis shows the percentage of the phrase table used. The
BLEU scores are shown in the y-axis. Two distributions
for P (s, t) were tested Uniform and Multinomial.

parison between the uniform and multinomial distri-
bution, we can see that both distributions yield sim-
ilar results, specially when a low number of phrase
pairs is pruned. In theory, the multinomial distri-
bution should yield better results, since the pruning
model will prefer to prune phrase pairs that are more
likely to be observed. However, longer phrase pairs,
which tend compete with other long phrase pairs on
which get pruned first. These phrase pairs gener-
ally occur only once or twice, so the multinomial
model will act similarly to the uniform model re-
garding longer phrase pairs. On the other hand, as
the model size reduces, we can see that using multi-
nomial distribution seems to start to improve over
the uniform distribution.

The comparison between our pruning model and
pruning based on significance is shown in table 2.
These models are hard to compare, since not all
phrase table sizes can be obtained using both met-
rics. For instance, the significance metric can ei-
ther keep or remove all phrase pairs that only appear
once, leaving a large gap of phrase table sizes that
cannot be attained. In the EUROPARL experiment
the sizes of the table suddenly drops from 60% to
8%. The same happens with our metric that cannot
distinguish atomic phrase pairs. In the EUROPARL
experiment, we cannot generate phrase tables with
sizes smaller than 15%. Thus, we only show re-
sults at points where both algorithms can produce
a phrase table.

Significant improvements are observed in the

Table size Significance Entropy (u) Entropy (m)
Pruning Pruning Pruning

IWSLT
57K (65%) 14.82 15.77 15.78
71K (80%) 15.14 15.76 15.77
80K (90%) 15.31 15.73 15.72

88K (100%) 15.68 15.68 15.68
EUROPARL
29M (60%) 28.64 28.82 28.91
34M (70%) 28.84 28.94 28.99
39M (80%) 28.86 28.99 28.99
44M (90%) 28.91 29.00 29.02

49M (100%) 29.18 29.18 29.18

Table 2: Comparison between Significance Pruning (Sig-
nificance Pruning) and Entropy-based pruning using the
uniform (Entropy (u) Pruning) and multinomial distribu-
tions (Entropy (m) Pruning).

IWSLT experiment, where significance pruning
does not perform as well. On the other hand, on the
EUROPARL experiment, our model only achieves
slightly higher results. We believe that this is re-
lated by the fact the EUROPARL corpora is gener-
ated from automatically aligning documents, which
means that there are misaligned sentence pairs.
Thus, many spurious phrase pairs are extracted. Sig-
nificance pruning performs well under these condi-
tions, since the measure is designed for this purpose.
In our metric, we do not have any means for detect-
ing spurious phrase pairs, in fact, spurious phrase
pairs are probably kept in the phrase table, since
each distinct spurious phrase pair is only extracted
once, and thus, they have very few derivations in
its support set. This suggests, that the significance
score can be integrated in our model to improve our
model, which we leave as future work.

John married Portugal

married 
in

in 
Portugal

married 

married 
in

John 

in 
Portugal

Portugal

a)

b)

Figure 2: Translation order in for different reordering
starting from left to right.

We believe that in language pairs such as Chinese-

969



English with large distance reorderings between
phrases are more prone to search errors and benefit
more from our pruning algorithm. To illustrate this,
let us consider the source sentence “John married
in Portugal”, and translating either using the blocks
“John”, “married” and “in Portugal” or the blocks
“John”, “married in”, “Portugal”, the first hypoth-
esis would be much more viable, since the word
“Portugal” is more relevant as the context for the
word “in”. Thus, the key choice for the decoder is
to decide whether to translate using “married” with
or without “in”, and it is only able to predict that
it is better to translate “married” by itself until it
finds that “in” is better translated with “Portugal”.
Thus, a search error occurs if the hypothesis where
“married” is translated by itself is removed. In fig-
ure 2, we can see the order that blocks are consid-
ered for different reorderings, starting from left to
right. In a), we illustrate the case for a monotonous
translation. We observe that the correct decision be-
tween translating “married in” or just “married” is
found immediately, since the blocks “Portugal” and
“in Portugal” are considered right afterwards. In this
case, it is unlikely that the hypothesis using “mar-
ried” is removed. However, if we consider that due
to reordering, “John” is translated after “married”
and before “Portugal”, which is shown in b). Then,
the correct decision can only be found after consid-
ering “John”. In this case, “John” does not have
many translations, so the likelihood of eliminating
the correct hypothesis. However, if there were many
translations for John, it is highly likely that the cor-
rect partial hypothesis is eliminated. Furthermore,
the more words exist between “married” and “Portu-
gal”, the more likely will the correct hypothesis not
exist when we reach “Portugal”. By pruning the hy-
pothesis “married in” a priori, we contribute in pre-
venting such search errors.

We observe that some categories of phrase pairs
that are systematically pruned, but these cannot
be generalized in rules, since there are many ex-
ceptions. The most obvious type of phrase pairs
are phrases with punctuations, such as “谢谢.” to
“thanks .” and “. 谢谢” to “thanks .”, since “.”
is translated independently from most contextual
words. However, this rule should not be general-
ized, since in some cases “.” is a relevant contextual
marker. For instance, the word “please” is translated

to “请” in the sentence ‘open the door, please.” and
translated to “使高兴” in “please my advisors”. An-
other example are sequences of numbers, which are
generally translated literally. For instance, “八(8)
三(3)八(8)” is translated to “eight three eight” (Ex:
“room eight three eight”). Thus, phrase pairs for
number sequences can be removed, since those num-
bers can be translated one by one. However, for se-
quences such as “一(1)八(8)”, we need a phrase pair
to represent this specifically. This is because “一(1)”
can be translated to “one”, but also to “a”, “an”, “sin-
gle”. Other exceptions include “一(1)一(1)”, which
tends to be translated as “eleven”, and which tends to
be translated to “o”, rather than “zero” in sequences
(“room eleven o five”).

6 Conclusions

We present a pruning algorithm for Machine Trans-
lation based on relative entropy, where we assess
whether the translation event encoded in a phrase
pair can be decomposed into combinations of events
encoded in other phrase pairs. We show that such
phrase pairs can be removed from the translation
model with little negative impact or even a positive
one in the overall translation quality. Tests show that
our method yields comparable or better results with
state of the art pruning algorithms.

As future work, we would like to combine our
approach with significance pruning, since both ap-
proaches are orthogonal and address different issues.
We also plan to improve the pruning step of our algo-
rithm to find the optimal set of phrase pairs to prune
given the pruning threshold.

The code used in this work will be made available.

7 Acknowledgements

This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors also wish
to thank the anonymous reviewers for many helpful
comments.

970



References

Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263–311, June.

George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’06, pages 53–61, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

J Howard Johnson and Joel Martin. 2007. Improv-
ing translation quality by discarding most of the
phrasetable. In In Proceedings of EMNLP-CoNLL’07,
pages 967–975.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 48–54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.

Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.

Wang Ling, Tiago Luı́s, João Graça, Luı́sa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ’10:
International Workshop on Spoken Language Transla-
tion, pages 313–320, Paris, France.

Stephen Vogal Matthias Eck and Alex Waibel. 2007. Es-
timating phrase pair relevance for translation model
pruning. MTSummit XI.

Robert C. Moore and Chris Quirk. 2009. Less is more:
significance-based n-gram selection for smaller, bet-
ter language models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ’09,
pages 746–755, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Michael Paul, Marcello Federico, and Sebastian Stüker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ’10: International Workshop on Spo-
ken Language Translation, pages 3–27.

Lane Schwartz. 2008. Multi-source translation methods.
In Proceedings of AMTA, pages 279–288.

Kristie Seymore and Ronald Rosenfeld. 1996. Scalable
backoff language models. In In Proceedings of ICSLP,
pages 232–235.

Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270–274.

Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. MTSummit XII, Aug.

João V. Graça, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481–504.

S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836–841. Association for
Computational Linguistics.

971


