










































Modeling Spoken Decision Making Dialogue and Optimization of its Dialogue Strategy


Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 221–224,
The University of Tokyo, September 24-25, 2010. c©2010 Association for Computational Linguistics

Modeling Spoken Decision Making Dialogue
and Optimization of its Dialogue Strategy

Teruhisa Misu, Komei Sugiura, Kiyonori Ohtake,
Chiori Hori, Hideki Kashioka, Hisashi Kawai and Satoshi Nakamura

MASTAR Project, NICT
Kyoto, Japan.

teruhisa.misu@nict.go.jp

Abstract

This paper presents a spoken dialogue frame-
work that helps users in making decisions.
Users often do not have a definite goal or cri-
teria for selecting from a list of alternatives.
Thus the system has to bridge this knowledge
gap and also provide the users with an appro-
priate alternative together with the reason for
this recommendation through dialogue. We
present a dialogue state model for such deci-
sion making dialogue. To evaluate this model,
we implement a trial sightseeing guidance sys-
tem and collect dialogue data. Then, we opti-
mize the dialogue strategy based on the state
model through reinforcement learning with a
natural policy gradient approach using a user
simulator trained on the collected dialogue
corpus.

1 Introduction
In many situations where spoken dialogue interfaces
are used, information access by the user is not a goal in
itself, but a means for decision making (Polifroni and
Walker, 2008). For example, in a restaurant retrieval
system, the user’s goal may not be the extraction of
price information but to make a decision on candidate
restaurants based on the retrieved information.

This work focuses on how to assist a user who is
using the system for his/her decision making, when
he/she does not have enough knowledge about the tar-
get domain. In such a situation, users are often un-
aware of not only what kind of information the sys-
tem can provide but also their own preference or fac-
tors that they should emphasize. The system, too, has
little knowledge about the user, or where his/her inter-
ests lie. Thus, the system has to bridge such gaps by
sensing (potential) preferences of the user and recom-
mend information that the user would be interested in,
considering a trade-off with the length of the dialogue.

We propose a model of dialogue state that consid-
ers the user’s preferences as well as his/her knowledge
about the domain changing through a decision making
dialogue. A user simulator is trained on data collected
with a trial sightseeing system. Next, we optimize
the dialogue strategy of the system via reinforcement
learning (RL) with a natural policy gradient approach.

2 Spoken decision making dialogue

We assume a situation where a user selects from a given
set of alternatives. This is highly likely in real world
situations; for example, the situation wherein a user se-
lects one restaurant from a list of candidates presented

Choose the optimal spot

1. Cherry 

Blossoms

2. Japanese

Garden

3. Easy

Access

Kinkakuji-

Temple

Ryoanji-

Temple

Nanzenji-

Temple

…

・・・・・

Goal

Criteria

Alternatives

(choices) ・・・・・

p1 p2 p3

v11 v12 v13
… …

Figure 1: Hierarchy structure for sightseeing guidance
dialogue

by a car navigation system. In this work, we deal with
a sightseeing planning task where the user determines
the sightseeing spot to visit, with little prior knowledge
about the target domain. The study of (Ohtake et al.,
2009), which investigated human-human dialogue in
such a task, reported that such consulting usually con-
sists of a sequence of information requests from the
user, presentation and elaboration of information about
certain spots by the guide followed by the user’s evalu-
ation. We thus focus on these interactions.

Several studies have featured decision support sys-
tems in the operations research field, and the typical
method that has been employed is the Analytic Hierar-
chy Process (Saaty, 1980) (AHP). In AHP, the problem
is modeled as a hierarchy that consists of the decision
goal, the alternatives for achieving it, and the criteria
for evaluating these alternatives. An example hierarchy
using these criteria is shown in Figure 1.

For the user, the problem of making an optimal de-
cision can be solved by fixing a weight vector Puser =
(p1, p2, . . . , pM ) for criteria and local weight matrix
Vuser = (v11, v12, . . . , v1M , . . . , vNM ) for alterna-
tives in terms of the criteria. The optimal alternative
is then identified by selecting the spot k with the maxi-
mum priority of

∑M
m=1 pmvkm. In typical AHP meth-

ods, the procedure of fixing these weights is often con-
ducted through pairwise comparisons for all the possi-
ble combinations of criteria and spots in terms of the
criteria, followed by weight tuning based on the re-
sults of these comparisons (Saaty, 1980). However, this
methodology cannot be directly applied to spoken dia-
logue systems. The information about the spot in terms
of the criteria is not known to the users, but is obtained
only via navigating through the system’s information.
In addition, spoken dialogue systems usually handle
several candidates and criteria, making pairwise com-
parison a costly affair.

We thus consider a spoken dialogue framework that
estimates the weights for the user’s preference (po-
tential preferences) as well as the user’s knowledge

221



about the domain through interactions of information
retrieval and navigation.

3 Decision support system with spoken
dialogue interface

The dialogue system we built has two functions: an-
swering users’ information requests and recommend-
ing information to them. When the system is requested
to explain about the spots or their determinants, it ex-
plains the sightseeing spots in terms of the requested
determinant. After satisfying the user’s request, the
system then provides information that would be helpful
in making a decision (e.g., instructing what the system
can explain, recommending detailed information of the
current topic that the user might be interested in, etc.).
Note that the latter is optimized via RL (see Section 4).

3.1 Knowledge base

Our back-end DB consists of 15 sightseeing spots as al-
ternatives and 10 determinants described for each spot.
We select determinants that frequently appear in the di-
alogue corpus of (Ohtake et al., 2009) (e.g. cherry blos-
soms, fall foliage). The spots are annotated in terms of
these determinants if they apply to them. The value of
the evaluation enm is “1” when the spot n applies to the
determinant m and “0” when it does not.

3.2 System initiative recommendation

The content of the recommendation is determined
based on one of the following six methods:
1. Recommendation of determinants based on the

currently focused spot (Method 1)
This method is structured on the basis of the user’s
current focus on a particular spot. Specifically, the
system selects several determinants related to the
current spot whose evaluation is “1” and presents
them to the user.

2. Recommendation of spots based on the cur-
rently focused determinant (Method 2)
This method functions on the basis of the focus on
a certain specific determinant.

3. Open prompt (Method 3)
The system does not make a recommendation, and
presents an open prompt.

4. Listing of determinants 1 (Method 4)
This method lists several determinants to the user in
ascending order from the low level user knowledge
Ksys (that the system estimates). (Ksys, Psys, pm
and Pr(pm = 1) are defined and explained in Sec-
tion 4.2.)

5. Listing of determinants 2 (Method 5)
This method also lists the determinants, but the or-
der is based on the user’s high preference Psys (that
the system estimates).

6. Recommendation of user’s possibly preferred
spot (Method 6)
The system recommends a spot as well as the de-
terminants that the users would be interested in
based on the estimated preference Psys. The sys-
tem selects one spot k with a maximum value of∑M

m=1 Pr(pm = 1) · ek,m. This idea is based
on collaborative filtering which is often used for
recommender systems (Breese et al., 1998). This
method will be helpful to users if the system suc-
cessfully estimates the user’s preference; however,
it will be irrelevant if the system does not.

We will represent these recommendations
through a dialogue act expression, (casys{scsys}),
which consists of a communicative act casys
and the semantic content scsys. (For exam-
ple Method1{(Spot5), (Det3,Det4,Det5)},
Method3{NULL,NULL}, etc.)
4 Optimization of dialogue strategy
4.1 Models for simulating a user
We introduce a user model that consists of a tuple of
knowledge vector Kuser, preference vector Puser, and
local weight matrix Vuser. In this paper, for simplic-
ity, a user’s preference vector or weight for determi-
nants Puser = (p1, p2, . . . , pM ) is assumed to con-
sist of binary parameters. That is, if the user is in-
terested in (or potentially interested in) the determi-
nant m and emphasizes it when making a decision,
the preference pm is set to “1”. Otherwise, it is set
to “0”. In order to represent a state that the user has
potential preference, we introduce a knowledge param-
eter Kuser = (k1, k2, . . . , kM ) that shows if the user
has the perception that the system is able to handle or
he/she is interested in the determinants. km is set to
“1” if the user knows (or is listed by system’s recom-
mendations) that the system can handle determinant m
and “0” when he/she does not. For example, the state
that the determinant m is the potential preference of a
user (but he/she is unaware of that) is represented by
(km = 0, pm = 1). This idea is in contrast to previous
research which assumes some fixed goal observable by
the user from the beginning of the dialogue (Schatz-
mann et al., 2007). A user’s local weight vnm for spot
n in terms of determinant m is set to “1”, when the
system lets the user know that the evaluation of spots is
“1” through recommendation Methods 1, 2 and 6.

We constructed a user simulator that is based on
the statistics calculated through an experiment with the
trial system (Misu et al., 2010) as well as the knowl-
edge and preference of the user. That is, the user’s com-
municative act catuser and the semantic content sc

t
user

for the system’s recommendation atsys are generated
based on the following equation:

Pr(catuser, sc
t
user|catsys, sctsys,Kuser,Puser)

= Pr(catuser|catsys)
·Pr(sctuser|Kuser,Puser, catuser, catsys, sctsys)

This means that the user’s communicative act causer
is sampled based on the conditional probability of
Pr(catuser|catsys) in (Misu et al., 2010). The seman-
tic content scuser is selected based on the user’s pref-
erence Puser under current knowledge about the de-
terminants Kuser. That is, the sc is sampled from the
determinants within the user’s knowledge (km = 1)
based on the probability that the user requests the de-
terminant of his/her preference/non-preference, which
is also calculated from the dialogue data of the trial sys-
tem.

4.2 Dialogue state expression
We defined the state expression of the user in the pre-
vious section. However the problem is that for the
system, the state (Puser,Kuser,Vuser) is not observ-
able, but is only estimated from the interactions with
the user. Thus, this model is a partially observable
Markov decision process (POMDP) problem. In or-
der to estimate unobservable properties of a POMDP

222



� �
Priors of the estimated state:
- Knowledge: Ksys = (0.22, 0.01, 0.02, 0.18, . . . )
- Preference: Psys = (0.37, 0.19, 0.48, 0.38, . . . )

Interactions (observation):
- System recommendation:
asys = Method1{(Spot5), (Det1, Det3, Det4)}
- User query:
auser = Accept{(Spot5), (Det3)}

Posterior of the estimated state:
- Knowledge: Ksys = (1.00, 0.01, 1.00, 1.00, . . . )
- Preference: Psys = (0.26, 0.19, 0.65, 0.22, . . . )

User’s knowledge acquisition:
- Knowledge: Kuser ← {k1 = 1, k3 = 1, k4 = 1}
- Local weight: Vuser ← {v51 = 1, v53 = 1, v54 =

1}� �
Figure 2: Example of state update

and handle the problem as an MDP, we introduce
the system’s inferential user knowledge vector Ksys
or probability distribution (estimate value) Ksys =
(Pr(k1 = 1), P r(k2 = 1), . . . , P r(kM = 1)) and
that of preference Psys = (Pr(p1 = 1), P r(p2 =
1), . . . , P r(pM = 1)).

The dialogue state DSt+1 or estimated user’s dia-
logue state of the step t+1 is assumed to be dependent
only on the previous state DSt, as well as the interac-
tions It = (atsys, a

t
user).

The estimated user’s state is represented as a prob-
ability distribution and is updated by each interac-
tion. This corresponds to representing the user types
as a probability distribution, whereas the work of (Ko-
matani et al., 2005) classifies users to several discrete
user types. The estimated user’s preference Psys is up-
dated when the system observes the interaction It. The
update is conducted based on the following Bayes’ the-
orem using the previous state DSt as a prior.

Pr(pm = 1|It) =
Pr(It|pm=1)Pr(pm=1)

Pr(It|pm=1)Pr(pm=1)+Pr(It|(pm=0))Pr(1−Pr(pm=1))

Here, Pr(It|pm = 1), P r(It|(pm = 0) to the right
side was obtained from the dialogue corpus of (Misu et
al., 2010). This posterior is then used as a prior in the
next state update using interaction It+1. An example
of this update is illustrated in Figure 2.

4.3 Reward function

The reward function that we use is based on the num-
ber of agreed attributes between the user preference
and the decided spot. Users are assumed to determine
the spot based on their preference Puser under their
knowledge Kuser (and local weight for spots Vuser)
at that time, and select the spot k with the maximum
priority of

∑
m kk · pk · vkm. The reward R is then

calculated based on the improvement in the number of
agreed attributes between the user’s actual (potential)
preferences and the decided spot k over the expected
agreement by random spot selection.

R =
M∑

m=1

pm · ek,m − 1
N

N∑

n=1

M∑

m=1

pm · en,m

For example, if the decided spot satisfies three prefer-
ences and the average agreement of the agreement by
random selection is 1.3, then the reward is 1.7.

4.4 Optimization by reinforcement learning
The problem of system recommendation generation is
optimized through RL. The MDP (S, A, R) is defined
as follows. The state parameter S = (s1, s2, . . . , sI) is
generated by extracting the features of the current dia-
logue state DSt. We use the following 29 features 1.
1. Parameters that indicate the # of interactions from
the beginning of the dialogue. This is approximated by
five parameters using triangular functions. 2. User’s
previous communicative act (1 if at−1user = xi, other-
wise 0). 3. System’s previous communicative act (1 if
at−1sys = yj , otherwise 0). 4. Sum of the estimated user
knowledge about determinants (

∑N
n=1 Pr(kn = 1)).

5. Number of presented spot information. 6. Expecta-
tion of the probability that the user emphasizes the de-
terminant in the current state (Pr(kn = 1)× Pr(pn =
1)) (10 parameters). The action set A consists of the
six recommendation methods shown in subsection 3.2.
Reward R is given by the reward function of subsection
4.3.

A system action asys (casys) is sampled based on the
following soft-max (Boltzmann) policy.

π(asys = k|S) = Pr(asys = k|S,Θ)

=
exp(

∑I
i=1 si · θki)∑J

j=1 exp(
∑I

i=1 si · θji)
Here, Θ = (θ11, θ12, . . . θ1I , . . . , θJI ) consists of J (#
actions) × I (# features) parameters. The parameter
θji works as a weight for the i-th feature of the ac-
tion j and determines the likelihood that the action j
is selected. This Θ is the target of optimization by RL.
We adopt the Natural Actor Critic (NAC) (Peters and
Schaal, 2008), which adopts a natural policy gradient
method as the policy optimization method.

4.5 Experiment by dialogue simulation
For each simulated dialogue session, a simulated user
(Puser,Kuser,Vuser) is sampled. A preference vector
Puser of the user is generated so that he/she has four
preferences. As a result, four parameters in Puser are
“1” and the others are “0”. This vector is fixed through-
out the dialogue episode. This sampling is conducted
based on the rate proportional to the percentage of users
who emphasize it for making decisions (Misu et al.,
2010). The user’s knowledge Kuser is also set based
on the statistics of the “percentage of users who stated
the determinants before system recommendation”. For
each determinant, we sample a random valuable r that
ranges from “0” to “1”, and km is set to “1” if r is
smaller than the percentage. All the parameters of
local weights Vuser are initialized to “0”, assuming
that users have no prior knowledge about the candi-
date spots. As for system parameters, the estimated
user’s preference Psys and knowledge Ksys are ini-
tialized based on the statistics of our trial system (Misu
et al., 2010).

We assumed that the system does not misunderstand
the user’s action. Users are assumed to continue a di-
alogue session for 20 turns2, and episodes are sampled
using the policy π at that time and the user simulator

1Note that about half of them are continuous variables and
that the value function cannot be denoted by a lookup table.

2In practice, users may make a decision at any point once
they are satisfied collecting information. And this is the rea-
son why we list the rewards in the early dialogue stage in

223



Table 1: Comparison of reward with baseline methods
Reward (±std)

Policy T = 5 T = 10 T = 15 T = 20
NAC 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
B1 0.02 (0.42) 0.13 (0.54) 0.29 (0.59) 0.34 (0.59)
B2 0.46 (0.67) 0.68 (0.65) 0.80 (0.61) 0.92 (0.56)

Table 2: Comparison of reward with discrete dialogue
state expression

Reward (±std)
State T = 5 T = 10 T = 15 T = 20
PDs 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Discrete 0.89 (0.60) 0.97 (0.56) 1.03 (0.54) 1.10 (0.52)

Table 3: Effect of estimated preference and knowledge
Reward (±std)

Policy T = 5 T = 10 T = 15 T = 20
Pref+Know 0.96 (0.53) 1.04 (0.51) 1.12 (0.50) 1.19 (0.48)
Pref only 0.94 (0.57) 0.96 (0.55) 1.02 (0.55) 1.09 (0.53)
Know only 0.96 (0.59) 1.00 (0.56) 1.08 (0.53) 1.15 (0.51)
No Pref or
Know

0.93 (0.57) 0.96 (0.55) 1.02 (0.53) 1.08 (0.52)

of subsection 4.1. In each turn, the system is rewarded
using the reward function of subsection 4.3. The pol-
icy (parameter Θ) is updated using NAC in every 2,000
dialogues.

4.6 Experimental result
The policy was fixed at about 30,000 dialogue
episodes. We analyzed the learned dialogue policy by
examining the value of weight parameter Θ. We com-
pared the parameters of the trained policy between ac-
tions3. The weight of the parameters that represent the
early stage of the dialogue was large in Methods 4 and
5. On the other hand, the weight of the parameters that
represent the latter stage of the dialogue was large in
Methods 2 and 6. This suggests that in the trained pol-
icy, the system first bridges the knowledge gap between
the user, estimates the user’s preference, and then, rec-
ommends specific information that would be useful to
the user.

Next, we compared the trained policy with the fol-
lowing baseline methods.

1. No recommendation (B1)
The system only provides the requested informa-
tion and does not generate any recommendations.

2. Random recommendation (B2)
The system randomly chooses a recommendation
from six methods.

The comparison of the average reward between the
baseline methods is listed in Table 1. Note that the ora-
cle average reward that can be obtained only when the
user knows all knowledge about the knowledge base
(it requires at least 50 turns) was 1.45. The reward by
the strategy optimized by NAC was significantly better
than that of baseline methods (n = 500, p < .01).

We then compared the proposed method with the
case where estimated user’s knowledge and preference
are represented as discrete binary parameters instead of
probability distributions (PDs). That is, the estimated
user’s preference pm of determinant m is set to “1”
when the user requested the determinant, otherwise it
is “0”. The estimated user’s knowledge km is set to

the following subsections. In our trial system, the dialogue
length was 16.3 turns with a standard deviation of 7.0 turns.

3The parameters can be interpreted as the size of the con-
tribution for selecting the action.

“1” when the system lets the user know the determi-
nant, otherwise it is “0”. Another dialogue strategy was
trained using this dialogue state expression. This result
is shown in Table 2. The proposed method that rep-
resents the dialogue state as a probability distribution
outperformed (p < .01 (T=15,20)) the method using a
discrete state expression.

We also compared the proposed method with the
case where either one of estimated preference or
knowledge was used as a feature for dialogue state in
order to carefully investigate the effect of these factors.
In the proposed method, expectation of the probabil-
ity that the user emphasizes the determinant (Pr(kn =
1) × Pr(pn = 1)) was used as a feature of dialogue
state. We evaluated the performance of the cases where
the estimated knowledge Pr(kn = 1) or estimated
preference Pr(pn = 1) was used instead of the expec-
tation of the probability that the user emphasizes the
determinant. We also compared with the case where
no preference/knowledge feature was used. This result
is shown in Table 3. We confirmed that significant im-
provement (p < .01 (T=15,20)) was obtained by taking
into account the estimated knowledge of the user.

5 Conclusion
In this paper, we presented a spoken dialogue frame-
work that helps users select an alternative from a list of
alternatives. We proposed a model of dialogue state for
spoken decision making dialogue that considers knowl-
edge as well as preference of the user and the system,
and its dialogue strategy was trained by RL. We con-
firmed that the learned policy achieved a better recom-
mendation strategy over several baseline methods.

Although we dealt with a simple recommendation
strategy with a fixed number of recommendation com-
ponents, there are many possible extensions to this
model. The system is expected to handle a more com-
plex planning of natural language generation. We also
need to consider errors in speech recognition and un-
derstanding when simulating dialogue.

References
J. Breese, D. Heckerman, and C. Kadie. 1998. ”empirical

analysis of predictive algorithms for collaborative filter-
ing”. In ”Proc. the 14th Annual Conference on Uncer-
tainty in Artificial Intelligence”, pages 43–52.

K. Komatani, S. Ueno, T. Kawahara, and H. Okuno. 2005.
User Modeling in Spoken Dialogue Systems to Generate
Flexible Guidance. User Modeling and User-Adapted In-
teraction, 15(1):169–183.

T. Misu, K. Ohtake, C. Hori, H. Kashioka, H. Kawai, and
S. Nakamura. 2010. Construction and Experiment of a
Spoken Consulting Dialogue System. In Proc. IWSDS.

K. Ohtake, T. Misu, C. Hori, H. Kashioka, and S. Nakamura.
2009. Annotating Dialogue Acts to Construct Dialogue
Systems for Consulting. In Proc. The 7th Workshop on
Asian Language Resources, pages 32–39.

J. Peters and S. Schaal. 2008. Natural Actor-Critic. Neuro-
computing, 71(7-9):1180–1190.

J. Polifroni and M. Walker. 2008. Intensional Summaries
as Cooperative Responses in Dialogue: Automation and
Evaluation. In Proc. ACL/HLT, pages 479–487.

T. Saaty. 1980. The Analytic Hierarchy Process: Planning,
Priority Setting, Resource Allocation. Mcgraw-Hill.

J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based User Simulation for
Bootstrapping a POMDP Dialogue System. In Proc.
HLT/NAACL.

224


