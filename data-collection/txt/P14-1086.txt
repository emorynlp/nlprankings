



















































Query-Chain Focused Summarization


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 913â€“922,
Baltimore, Maryland, USA, June 23-25 2014. cÂ©2014 Association for Computational Linguistics

Query-Chain Focused Summarization 

 

Tal Baumel 

Dept. of Computer Science  

Ben-Gurion University 

Beer-Sheva, Israel 

talbau@cs.bgu.ac.il 

Raphael Cohen 

Dept. of Computer Science  

Ben-Gurion University 

Beer-Sheva, Israel 

cohenrap@cs.bgu.ac.il 

Michael Elhadad 

Dept. of Computer Science  

Ben-Gurion University 

Beer-Sheva, Israel 

elhadad@cs.bgu.ac.il 

 

 

Abstract 

Update summarization is a form of multi-

document summarization where a document 

set must be summarized in the context of other 

documents assumed to be known. Efficient 

update summarization must focus on identify-

ing new information and avoiding repetition of 

known information. In Query-focused summa-

rization, the task is to produce a summary as 

an answer to a given query.  We introduce a 

new task, Query-Chain Summarization, which 

combines aspects of the two previous tasks: 

starting from a given document set, increas-

ingly specific queries are considered, and a 

new summary is produced at each step. This 

process models exploratory search: a user ex-

plores a new topic by submitting a sequence of 

queries, inspecting a summary of the result set 

and phrasing a new query at each step. We 

present a novel dataset comprising 22 query-

chains sessions of length up to 3 with 3 match-

ing human summaries each in the consumer-

health domain. Our analysis demonstrates that 

summaries produced in the context of such 

exploratory process are different from in-

formative summaries. We present an algorithm 

for Query-Chain Summarization based on a 

new LDA topic model variant.  Evaluation in-

dicates the algorithm improves on strong base-

lines. 

1 Introduction 

In the past 10 years, the general objective of 

text summarization has been refined into more 

specific tasks. Such summarization tasks include: 

(i) Generic Multi Document Summarization: 

aims at summarizing a cluster of topically related 

documents, such as the top results of a search 

engine query; (ii) in Update Summarization, a set 

of documents is summarized while assuming the 

user has already read a summary of earlier doc-

uments on the same topic; (iii) in Query-Focused 

Summarization, the summary of a documents set 

is produced to convey an informative answer in 

the context of a specific query. The importance 

of these specialized tasks is that they help us dis-

tinguish criteria that lead to the selection of con-

tent in a summary: centrality, novelty, relevance, 

and techniques to avoid redundancy. 

We present in this paper a variant summariza-

tion task which combines the two aspects of up-

date and query-focused summarization.  The task 

is related to exploratory search (Marchionini, 

2006). In contrast to classical information seek-

ing, in exploratory search, the user is uncertain 

about the information available, and aims at 

learning and understanding a new topic (White 

and Roth, 2009).  In typical exploratory search 

behavior, a user posts a series of queries, and 

based on information gathered at each step, de-

cides how to further explore a set of documents. 

The metaphor of berrypicking introduced in 

(Bates, 1989) captures this interactive process. 

At each step, the user may zoom in to a more 

specific information need, zoom out to a more 

general query, or pan sideways, in order to inves-

tigate a new aspect of the topic.  

We define Query-Chain Focused Summariza-

tion as follows: for each query in an exploratory 

search session, we aim to extract a summary that 

answers the information need of the user, in a 

manner similar to Query-Focused Summariza-

tion, while not repeating information already 

provided in previous steps, in a manner similar to 

Update Summarization. In contrast to query-

focused summarization, the context of a sum-

913



mary is not a single query, but the set of queries 

that led to the current step, their result sets and 

the corresponding summaries. 

We have constructed a novel dataset of Query-

Sets with matching manual summarizations in 

the consumer health domain (Cline and Haynes, 

2001). Queries are extracted from PubMed 

search logs (Dogan et al., 2009). We have ana-

lyzed this manual dataset and confirm that sum-

maries written in the context of berry-picking are 

markedly different from those written for similar 

queries on the same document set, but without 

the query-chain context. 

We have adapted well-known multi-document 

algorithms to the task, and present baseline algo-

rithms based on LexRank (Erkan and Radev, 

2004), KLSum and TopicSum (Haghighi and 

Vanderwende, 2009). We introduce a new algo-

rithm to address the task of Query-Chain Fo-

cused Summarization, based on a new LDA topic 

model variant, and present an evaluation which 

demonstrates it improves on these baselines. 

The paper is structured as follows. Section 2 

formulates the task of Query-Chain Focused 

Summarization. Section 3 reviews related work. 

In Section 4, we describe the data collection pro-

cess and the resulting dataset. We then present 

our algorithm, as well as the baseline algorithms 

used for evaluation. We conclude with evalua-

tion and discussion. 

2 Query- Chain Summarization 

In this work, we focus on the zoom in aspect 

of the exploratory search process described 

above. We formulate the Query-Chain Focused 

Summarization (QCFS) task as follows: 

Given an ordered chain of queries Q and a set 

of documents D , for each query Qqi ïƒ  a sum-

mary Si is generated from D answering iq  under 

the assumption that the user has already read the 

summaries Si-1 for queries 10... ï€­iqq . 

A typical example of query chain in the con-

sumer health domain we investigate includes the 

following 3 successive queries: (Causes of asth-

ma, Asthma and Allergy, Asthma and Mold Al-

lergy).   We consider a single set of documents 

relevant to the domain of Asthma as the refer-

ence set D.  The QCFS task consists of generat-

ing one summary of D as an answer to each que-

ry, so that the successive answers do not repeat 

information already provided in a previous an-

swer. 

3 Previous Work 

We first review the closely related tasks of 

Update Summarization and Query-Focused 

Summarization. We also review key summariza-

tion algorithms that we have selected as baseline 

and adapted to the QCFS task. 

Update Summarization focuses on identifying 

new information relative to a previous body of 

information, modeled as a set of documents. It 

has been introduced in shared tasks in DUC 2007 

and TAC 2008.  This task consists of producing a 

multi-document summary for a document set on 

a specific topic, and then a multi-document 

summary for a different set of articles on the 

same topic published at later dates. This task 

helps us understand how update summaries iden-

tified and focused on new information while re-

ducing redundancy compared to the original 

summaries.  

The TAC 2008 dataset includes 48 sets of 20 

documents, each cluster split in two subsets of 10 

documents (called A and B). Subset B docu-

ments were more recent. Original summaries 

were generated for the A subsets and update 

summaries were then produced for the B subsets. 

Human summaries and candidate systems are 

evaluated using the Pyramid method (Nenkova 

and Passonneau, 2004). For automatic evaluation, 

ROUGE (Lin, 2004) variants have been pro-

posed (Conroy et al., 2011).  In contrast to this 

setup, QCFS distinguishes the subsets of docu-

ments considered at each step of the process by 

facets of the underlying topic, and not by chro-

nology. In addition, the document subsets are not 

identified as part of the task in QCFS (as op-

posed to the explicit split in A and B subsets in 

Update Summarization). 

Most systems working on Update Summariza-

tion have focused on removing redundancy. Du-

alSum (Delort and Alfonseca, 2012) is notable in 

attempting to directly model novelty using a spe-

cialized topic-model to distinguish words ex-

pressing background information and those in-

troducing new information in each document. 

In Query-Focused Summarization (QFS), the 

task consists of identifying information in a doc-

ument set that is most relevant to a given query.  

914



This differs from generic summarization, where 

one attempts to identify central information.  

QFS helps us distinguish models of relevance 

and centrality.  Unfortunately, detailed analysis 

of the datasets produced for QFS indicates that 

these two notions are not strongly distinguished 

in practice: (Gupta et al., 2007) observed that in 

QFS datasets, up to 57% of the words in the doc-

ument sets were closely related to the query 

(through simple query expansion).  They note 

that as a consequence, a generic summarizer 

forms a strong baseline for such biased QFS 

tasks. 

We address this limitation of existing QFS da-

tasets in our definition of QCFS: we identify a 

chain of at least 3 related queries which focus on 

different facets of the same central topic and re-

quire the generation of distinct summaries for 

each query, with little repetition across the steps. 

A specific evaluation aspect of QFS measures 

responsiveness (how well the summary answers 

the specific query).  QFS must rely on Infor-

mation Retrieval techniques to overcome the 

scarceness of the query to establish relevance.  

As evidenced since (Daume and Marcu, 2006), 

Bayesian techniques have proven effective at this 

task: we construct a latent topic model on the 

basis of the document set and the query. This 

topic model effectively serves as a query expan-

sion mechanism, which helps assess the rele-

vance of individual sentences to the original que-

ry. 

In recent years, three major techniques have 

emerged to perform multi-document summariza-

tion: graph-based methods such as LexRank (Er-

kan and Radev, 2004) for multi document sum-

marization and Biased-LexRank (Otterbacher et 

al., 2008) for query focused summarization, lan-

guage model methods such as KLSum (Haghighi 

and Vanderwende, 2009) and variants of KLSum 

based on topic models such as BayesSum (Dau-

me and Marcu, 2006) and TopicSum (Haghighi 

and Vanderwende, 2009).   

LexRank is a stochastic graph-based method 

for computing the relative importance of textual 

units in a natural text. The LexRank algorithm 

builds a weighted graph ğº = (ğ‘‰, ğ¸) where each 
vertex in ğ‘‰ is a linguistic unit (in our case sen-
tences) and each weighted edge in ğ¸ is a measure 
of similarity between the nodes. In our imple-

mentation, we model similarity by computing the 

cosine distance between the ğ‘‡ğ¹ Ã— ğ¼ğ·ğ¹  vectors 

representing each node. After the graph is gener-

ated, the PageRank algorithm (Page et al., 1999) 

is used to determine the most central linguistic 

units in the graph. To generate a summary we 

use the ğ‘›  most central lexical units, until the 
length of the target summary is reached. This 

method has no explicit control to avoid redun-

dancy among the selected sentences, and the 

original algorithm does not address update or 

query-focused variants. Biased-LexRank (Otter-

bacher et al., 2008) makes LexRank sensitive to 

the query by introducing a prior belief about the 

ranking of the nodes in the graph, which reflects 

the similarity of sentences to the query. Pag-

eRank spreads the query similarity of a vertex to 

its close neighbors, so that we rank higher sen-

tences that are similar to other sentences which 

are similar to the query. As a result, Biased-

LexRank overcomes the lexical sparseness of the 

query and obtained state of the art results on the 

DUC 2005 dataset. 

KLSum adopts a language model approach to 

compute relevance: the documents in the input 

set are modeled as a distribution over words (the 

original algorithm uses a unigram distribution 

over the bag of words in documents D). KLSum 

is a sentence extraction algorithm: it searches for 

a subset of the sentences in D with a unigram 

distribution as similar as possible to that of the 

overall collection D, but with a limited length. 

The algorithm uses Kullback-Lieber (KL) diver-

gence ğ¾ğ¿(ğ‘ƒ||ğ‘„) = âˆ‘ logğ‘¤ (
ğ‘ƒ(ğ‘¤)

ğ‘„(ğ‘¤)
)ğ‘ƒ(ğ‘¤)  to com-

pute the similarity of the distributions. It searches 

for ğ‘†âˆ— = argmin|ğ‘†|<ğ¿ğ¾ğ¿(ğ‘ƒğ·||ğ‘ƒğ‘†). This search is 
performed in a greedy manner, adding sentences 

one by one to S until the length L is reached, and 

choosing the best sentence as measured by KL-

divergence at each step. The original method has 

no update or query focusing capability, but as a 

general modeling framework it is easy to adapt to 

a wide range of specific tasks. 

TopicSum uses an LDA-like topic model (Blei 

et al. 2003) to classify words from a number of 

document sets (each set discussing a different 

topic) as either general non-content words, topic 

specific words and document specific word (this 

category refers to words that are specific to the 

writer and not shared across the document set). 

After the words are classified, the algorithm uses 

a KLSum variant to find the summary that best 

matches the unigram distribution of topic specif-

ic words. This method improves the results of 

915



KLSum but it also has no update summary or 

query answering capabilities.  

4 Dataset Collection 

We now describe how we have constructed a 

dataset to evaluate QCFS algorithms, which we 

are publishing freely. We selected to build our 

dataset in the Consumer Health domain, a popu-

lar domain in the web (Cline and Haynes 2001) 

providing medical information at various levels 

of complexity, ranging from layman and up to 

expert information, because consumer health il-

lustrates the need for exploratory search.   

The PubMed repository, while primarily serving 

the academic community, is also used by laymen 

to ask health related questions. The PubMed que-

ry logs (Dogan et al., 2009) provide user queries 

with timestamps and anonymized user identifica-

tion. They are publically available and include 

over 600K queries per day. In this dataset, Dogan 

and Murray found that query reformulation (typ-

ical of exploratory search) is quite frequent: "In 

our dataset, 47% of all queries are followed by a 

new subsequent query. These users did not select 

any abstract or full text views from the result set. 

We make an operational assumption that these 

usersâ€™ intent was to modify their search by re-

formulating their query." We used these logs to 

extract laymen queries relating to four topics: 

Asthma, Lung Cancer, Obesity and Alzheimerâ€™s 

disease. We extracted a single day query log. 

From these, we extracted sessions which con-

tained the terms â€œAsthmaâ€, â€œLung Cancerâ€, 

â€œObesityâ€ or â€œAlzheimerâ€. Sessions containing 

search tags (such as â€œ[Author]â€) were removed 

to reduce the number of academic searches. The 

sessions were then manually examined and used 

to create zoom-in query chains of length 3 at 

most. The queries appear below: 

Asthma: 

Asthma causesâ†’ asthma allergyâ†’ asthma mold allergy; 

Asthma treatmentâ†’asthma medicationâ†’corticosteroids; 

Exercise induced asthmaâ†’ exercise for asthmatic; 

Atopic dermatitisâ†’ atopic dermatitis medicationsâ†’ atopic 
dermatitis side effects; 

Atopic dermatitisâ†’ atopic dermatitis childrenâ†’ atopic der-
matitis treatment; 

Atopic dermatitis â†’ atopic dermatitis exercise activity â†’
 atopic dermatitis treatment; 

Cancer: 

Lung cancerâ†’ lung cancer causesâ†’ lung cancer symptoms; 

Lung cancer diagnosisâ†’ lung cancer treatmentâ†’lung cancer 
treatment side effects; 

Stage of lung cancerâ†’ lung cancer staging testsâ†’ lung can-
cer TNM staging system; 

Types of lung cancerâ†’non-small cell lung cancer treat-
mentâ†’non-small cell lung cancer surgery; 

Lung cancer in womenâ†’ risk factors for lung cancer in 
womenâ†’ treatment of lung cancer in women; 

Lung cancer chemotherapyâ†’ goals of lung cancer chemo-
therapyâ†’ palliative care for lung cancer; 

Obesity: 

Salt obesityâ†’retaining fluid; 

Obesity screeningâ†’body mass indexâ†’BMI Validity; 

Childhood obesityâ†’childhood obesity low incomeâ†’chil-
dren diet and exercise; 

Causes of childhood obesityâ†’obesity and nutritionâ†’school 
lunch; 

Obesity and lifestyle changeâ†’obesity metabolismâ†’super-
foods antioxidant; 

Obesity and diabetesâ†’emergence of type 2 diabetesâ†’type 2 
diabetes and obesity in children; 

Alzheimerâ€™s disease: 

Alzheimer memoryâ†’helping retrieve memory alzheimer 
â†’alzheimer memory impairment nursing; 

Cognitive impairmentâ†’Vascular Dementiaâ†’Vascular De-
mentia difference alzheimer; 

Alzheimerâ€™s symptomsâ†’alzheimer diagnosisâ†’alzheimer 
medications; 

Semantic dementiaâ†’first symptoms dementiaâ†’first symp-
toms alzheimer; 

Figure 1: Queries Used to Construct Dataset 

We asked medical experts to construct four 

document collections from well-known and reli-

able consumer health websites relating to the 

four subjects (Wikipedia, WebMD, and the 

NHS), so that they would provide general infor-

mation relevant to the queries. 

We then asked medical students to manually 

produce summaries of these four document col-

lections for each query-chain. The medical stu-

dents were instructed construct a text of up to 

250 words that provides a good answer to each 

query in the chain. For each query in a chain the 

summarizers should assume that the person read-

ing the summaries is familiar with the previous 

916



summaries in the chain so they should avoid re-

dundancy. 

Three distinct human summaries were pro-

duced for each chain.  For each chain, one sum-

mary was produced for each of the three queries, 

where the person producing the summary was 

not shown the next steps in the chain when an-

swering the first query. 

To simulate the exploratory search of the user 

we provided the annotators with a Solr1  query 

interface for each document collection. The in-

terface allowed querying the document set, read-

ing the documents and choosing sentences which 

answer the query. After choosing the sentences, 

annotators can copy and edit the resulting sum-

mary in order to create an answer of up to 250 

words. After processing the first two query chain 

summaries, the annotators held a post-hoc dis-

cussion about the different summaries in order to 

adjust their conception of the task. 

The statistics on the collected dataset appear in 

the Tables below: 

Document sets # Docs # Sentences #Tokens / 

Unique 

Asthma  125 1,924 19,662 / 2,284 

Lung-Cancer 135 1,450 17,842 / 2,228 

Obesity 289 1,615 21,561 / 2,907 

Alzheimerâ€™s Disease 191 1,163 14,813 / 2,508 

 
Queries # Sessions # Sentences #Tokens / 

Unique 

Asthma  5 15 36 / 14 

Lung-Cancer 6 18 71 / 25 

Obesity 6 17 45 / 29 

Alzheimerâ€™s Disease 4 12 33 / 16 

 
Manual Summaries # Docs # Sentences #Tokens / 

Unique 

Asthma  45 543 6,349  / 1,011 

Lung-Cancer 54 669 8,287  / 1,130 

Obesity 51 538 7,079  / 1,270 

Alzheimerâ€™s Disease 36 385 5,031  /    966  

Table 1: Collected Dataset Size Statistics 

A key aspect of the dataset is that the same 

documents are summarized for each step of the 

chains, and we expect the summaries for each 

step to be different (that is, each answer is indeed 

responsive to the specific query it addresses). In 

addition, each answer is produced in the context 

of the previous steps, and only provides updated 

                                                 

1 http://lucene.apache.org/solr/ 

information with respect to previous answers. To 

ensure that the dataset indeed reflects these two 

aspects (responsiveness and freshness), we em-

pirically verified that summaries created for ad-

vanced queries are different from the summaries 

created for the same queries by summarizers who 

did not see the previous summaries in the chain. 

We asked from additional annotators to create 

manual summaries of advanced queries from the 

query chain without ever seeing the queries from 

the beginning of the chain. For example, given 

the chain (asthma causes â†’ asthma allergy â†’
 asthma mold allergy), we asked summarizers to 
produce an answer for the second query (asthma 

allergy) without seeing the first step, on the same 

input documents. 

We used ROUGE to perform this validation: 

ROUGE compares a summary with a set of ref-

erence summaries and source documents. We 

first computed the mean ROUGE score of the 

second query summaries. The mean ROUGE 

score is the mean score of each manual summary 

vs. all other summaries about the same query. 

We got (ğ‘Ÿ1 = 0.52, ğ‘Ÿ2 = 0.22, ğ‘Ÿğ‘ 4 = 0.13). The 
mean ROUGE scores of the same second query 

summaries by people who did not see the previ-

ous query were markedly lower: ( ğ‘Ÿ1 =
0.40, ğ‘Ÿ2 = 0.22, ğ‘Ÿğ‘ 4 = 0.01).  We only verified 
the asthma dataset in this manner. The results, 

except for the R2 test, had statistically significant 

difference with 95%  confidence interval. 
All the data, code and an annotated example can 

be found in our site2. 

We show below slightly shortened manual 

summaries created in answer to the following 

query chain: "obesity screening -> body mass 

index -> BMI Validity". We manually annotated 

the summaries to highlight how the exploration 

process develops and indicate some of the infor-

mation retrieval challenges faced when address-

ing the task: underlined terms are directly rele-

vant to the current query; bold italic terms have 

likely triggered the curiosity of the reader and led 

to asking the next query.  In this example, a user 

asks about obesity screening, and reformulates 

his query using the technical term of BMI, which 

is prevalent in the produced answer. 
 

Q1:  obesity screening 

Body mass index (BMI) is a measure used to determine 

                                                 

2 http://www.cs.bgu.ac.il/~nlpproj/QCFS/dataset.html 

917



childhood overweight and obesity. A child's weight status 

is determined using an age- and sex-specific percentile 

for BMI rather than the BMI categories used for adults 

because children's body composition varies as they age 

and varies between boys and girls. For children and ado-

lescents (aged 2-19 years): Overweight is defined as 

a BMI at or above the 85th percentile and lower than the 

95th percentile for children of the same age and 

sex.  Obesity is defined as a BMI at or above the 95th 

percentile for children of the same age and sex. BMI is not 

a perfect measure, but rather a helpful indicator. It doesn't 

consider things like being muscular or having a larger than 

average body frame and because growth patterns vary 

greatly among children.  As a result, doctors also factor a 

child's growth and development into consideration 

when screening for overweight and obesity. Obesity 

screening of adults using BMI may be less accurate for 

adults, due to the coarseness of the measurement: Heavily 

muscled athletes may have a high BMI, as many women 

who enter menopause, who tend to gain weight due to 

increased water retention. 

Q2: body mass index 

The body mass index (BMI), or Quetelet index, is a meas-

ure for human body shape based on an individu-

al's mass and height. Devised in the mid-1800s by Adolphe 

Quetelet during the course of developing "social physics", 

it is defined as an individual's body mass divided by the 

square of their height, the units being kg/m^2. BMI was 

explicitly cited as being appropriate for population studies, 

and inappropriate for individual diagnosis. BMI provides a 

simple measure of a person's thickness, allowing health 

professionals to discuss over-weight and underweight  

problems more objectively with their patients. Howev-

er, BMI has become controversial because many people, 

including physicians, have come to rely on its appar-

ent authority for medical diagnosis. However, it was origi-

nally meant to be used as a simple means of classifying 

sedentary individuals, or rather, populations, with an aver-

age body composition. For these individuals, the current 

value settings are as follows: (...). Nick Korevaar (a mathe-

matics lecturer from the University of Utah) suggests that 

instead of squaring the body height or cubing 

the body height, it would be more appropriate to use an 

exponent of between 2.3 and 2.7 (as originally noted by 

Quetelet). 

Q3: BMI Validity 

BMI has become controversial because many people, in-

cluding physicians, have come to rely on its apparent nu-

merical authority for medical diagnosis, but that was never 

the BMI's purpose; it is meant to be used as a simple 

means of classifying sedentary populations with an average 

body composition. In an article published in the July edi-

tion of 1972 of the Journal of Chronic Diseases, Ancel Keys 

explicitly cited BMI as being appropriate for population 

studies, but inappropriate for individual diagnosis. These 

ranges of BMI values are valid only as statistical categories 

While BMI is a simple, inexpensive method of screening for 

weight categories, it is not a good diagnostic tool: It does 

not take into account age, gender, or muscle mass. (...). 

Figure 2: Query Chain Summary Annotated Example 

5 Algorithms  

In this section, we first explain how we 

adapted the previously mentioned methods to the 

QCFS task, thus producing 3 strong baselines. 

We then describe our new algorithm for QCFS. 

5.1 Focused KLSum 

We adapted KLSum to QCFS by introducing 

a simple document selection step in the algo-

rithm.  The method is: given a query step ğ‘, we 
first select a focused subset of documents from 

ğ·, ğ·(ğ‘).  We then apply the usual KLSum algo-
rithm over ğ·(ğ‘). This approach does not make 
any effort to reduce redundancy from step to step 

in the query chain.  In our implementation, we 

compute ğ·(ğ‘) by selecting the top-10 documents 
in ğ· ranked by ğ‘‡ğ¹ Ã— ğ¼ğ·ğ¹ scores to the query, as 
implemented in SolR. 

5.2 KL-Chain-Update 

KL-Chain-Update is a slightly more sophisti-

cated variation of KLSum that answers a query 

chain (instead a single query). When construct-

ing a summary, we update the unigram distribu-

tion of the constructed summary so that it in-

cludes a smoothed distribution of the previous 

summaries in order to eliminate redundancy be-

tween the successive steps in the chain. For ex-

ample, when we summarize the documents that 

were retrieved as a result to the first query, we 

calculate the unigram distribution in the same 

manner as we did in Focused KLSum; but for the 

second query, we calculate the unigram distribu-

tion as if all the sentences we selected for the 

previous summary were selected for the current 

query too, with a damping factor. In this variant, 

the Unigram Distribution estimate of word X is 

computed as: 

918



(Count(ğ‘Š, ğ¶ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘†ğ‘¢ğ‘š) +
Count(ğ‘Š, ğ‘ƒğ‘Ÿğ‘’ğ‘£ğ‘–ğ‘œğ‘¢ğ‘ ğ‘†ğ‘¢ğ‘š)

ğ‘†ğ‘šğ‘œğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”ğ¹ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ
)

Length(ğ¶ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘†ğ‘¢ğ‘š) +
Length(PreviousSum âˆ© ğ¶ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘†ğ‘¢ğ‘š)

ğ‘†ğ‘šğ‘œğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”ğ¹ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ

 

5.3 ChainSum 

ChainSum is our adaptation of TopicSum to 

the QCFS task. We developed a novel Topic 

Model to identify words that are associated to the 

current query and not shared with the previous 

queries. We achieved this with the following 

model. For each query in a chain, we consider 

the documents ğ·ğ‘ which are "good answers" to 
the query; and ğ·ğ‘ƒ which are the documents used 
to answer the previous steps of the chain.  We 

assume in this model that these document subsets 

are observable (in our implementation, we select 

these subsets by ranking the documents for the 

query based on TFxIDF similarity). 

1. ğº is the general words topic, it is intended 
to capture stop words and non-topic spe-

cific vocabulary. Its distribution ğœ‘ğº  is 
drawn for all the documents from 

ğ·ğ‘–ğ‘Ÿğ‘–ğ‘â„ğ‘™ğ‘’ğ‘¡(ğ‘‰, ğœ†ğº). 

2. ğ‘†ğ‘– is the document specific topic; it repre-
sents words which are local for a specific 

document.  ğœ™ğ‘†ğ‘–  is drawn for each docu-

ment from ğ·ğ‘–ğ‘Ÿğ‘–ğ‘â„ğ‘™ğ‘’ğ‘¡(ğ‘‰, ğœ†ğ‘†ğ‘–). 

3. ğ‘ is the new content topic, which should 
capture words that are characteristic for 

ğ·ğ‘. ğœ™ğ‘ is drawn for all the documents in 
ğ·ğ‘ from ğ·ğ‘–ğ‘Ÿğ‘–ğ‘â„ğ‘™ğ‘’ğ‘¡(ğ‘‰, ğœ†ğ‘). 

4. ğ‘‚  captures old content from ğ·ğ‘ƒ , ğœ™ğ‘‚  is 
drawn for all the documents in ğ·ğ‘ƒ  from 
ğ·ğ‘–ğ‘Ÿğ‘–ğ‘â„ğ‘™ğ‘’ğ‘¡(ğ‘‰, ğœ†ğ‘‚). 

5. ğ‘… captures redundant information between 
ğ·ğ‘  and ğ·ğ‘, ğœ™ğ‘…  is drawn for all the docu-

ments in ğ·ğ‘ âˆª ğ·ğ‘ from ğ·ğ‘–ğ‘Ÿğ‘–ğ‘â„ğ‘™ğ‘’ğ‘¡(ğ‘‰, ğœ†ğ‘…). 

6. For documents from ğ·ğ‘ we draw from the 
distribution ğœ“ğ‘¡1  over topics (ğº, ğ‘, ğ‘…, ğ‘†ğ‘–) 

from a Dirichlet prior with pseudo-

counts (10.0,15.0,15.0,1.0)3 . For each 
word in the document, we draw a topic ğ‘ 
from ğœ“ğ‘¡, and a word ğ‘Š from the topic in-
dicated by ğ‘. 

                                                 

3 All pseudo-counts were selected empirically  

7. For documents from ğ·ğ‘, we draw from the 

distribution ğœ“ğ‘¡2  over topics (ğº, ğ‘‚, ğ‘…, ğ‘†ğ‘–) 

from a Dirichlet prior with pseudo-

counts  (10.0,15.0,15.0,1.0) . The words 
are drawn in the same manner as in ğ‘¡1. 

8. For documents in ğ· âˆ– (ğ·ğ‘ âˆª ğ·ğ‘) we draw 

from the distribution ğœ“ğ‘¡3  over topics 

(ğº, ğ‘†ğ‘–) from a Dirichlet prior with pseudo-
counts (10.0,1.0) . The words are also 
drawn in the same manner as in ğ‘¡1. 

The plate diagram of this generative model is 

shown in Fig.3. 

 
Figure 3 Plate Model for Our Topic Model 

We implemented inference over this topic 

model using Gibbs Sampling (we distribute the 

code of the sampler together with our dataset).  

After the topic model is applied to the current 

query, we apply KLSum only on words that are 

assigned to the new content topic. Fig.4 summa-

rizes the algorithm data flow. 

When running this topic model on our dataset, 

we observe: ğ·ğ‘  mean size was 978 words and 
375 unique words. ğ·ğ‘ƒ   mean size was 1374 
words and 436 unique words. ğ·ğ‘  and ğ·ğ‘ƒ  mean 
on average 159 words. These figures show there 

is high lexical overlap between the summaries 

answering query qi and qi+1 and highlight the 

need to distinguish new and previously exposed 

content. 

In the ChainSum model, the topic R aims at 

modeling redundant information between the 

previous summaries and the new summary.  We 

intend in the future to exploit this information to 

construct a contrastive model of content selec-

tion.  In the current version, R does not play an 

active role in content selection.  We, therefore, 

tested a variant of ChainSum that did not in-

clude ğœ‘ğ‘…  and obtained results extremely similar 
to the full model, which we report below. 

919



 

Figure 4 ChainSum Architecture 

5.4 Adapted LexRank 

In LexRank, the algorithm creates a graph 

where nodes represent the sentences from the 

text and weighted edges represent the cosine-

distance of each sentence's TFxIDF vec-

tors. After creating the graph, PageRank is run to 

rank sentences. We adapted LexRank to QCFS in 

two main ways: we extend the sentence represen-

tation scheme to capture semantic information 

and refine the model of sentences similarity so 

that it captures query answering instead of cen-

trality. We tagged each sentence with Wikipedia 

terms using the Illinois Wikifier (Ratinov et al., 

2011) and with UMLS (Bodenreider, 2004) 

terms using HealthTermFinder (Lipsky-Gorman 

and Elhadad, 2011). UMLS is a rich medical on-

tology, which is appropriate to the consumer 

health domain. 

We changed the edges scoring formula to use 

the sum of Lexical Semantic Similarity (LSS) 

functions (Li et al., 2007) on lexical terms, Wik-

ipedia terms and UMLS terms: 

ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘ˆ, ğ‘‰) = ğ¿ğ‘†ğ‘†ğ‘™ğ‘’ğ‘¥ğ‘–ğ‘ğ‘ğ‘™(ğ‘ˆ, ğ‘‰) + ğ‘

âˆ— ğ¿ğ‘†ğ‘†ğ‘¤ğ‘–ğ‘˜ğ‘–(ğ‘ˆ, ğ‘‰) + ğ‘

âˆ— ğ¿ğ‘†ğ‘†ğ‘ˆğ‘€ğ¿ğ‘†(ğ‘ˆ, ğ‘‰) 

Where: 

ğ¿ğ‘†ğ‘†(ğ‘†1, ğ‘†2) =

âˆ‘ (ğ‘€ğ´ğ‘‹ğ‘—(
ğ‘†ğ‘–ğ‘š(ğ‘Šğ‘–

1, ğ‘Šğ‘—
2)

ğ‘†ğ‘–ğ‘š(ğ‘Šğ‘–
1, ğ‘Šğ‘–

1)
)ğ¼ğ·ğ¹(ğ‘Šğ‘–

1))ğ‘–

âˆ‘ ğ¼ğ·ğ¹(ğ‘Šğ‘–
1)ğ‘–

 

Instead of using the cosine distance, in order to 

incorporate advanced word/term similarity func-

tions. For lexical terms, we used the identity 

function, for Wikipedia term we used Wikiminer 

(Milne, 2007), and for UMLS we used Ted 

Pedersen UMLS similarity function (McInnes et 

al., 2009).  Finally, instead of PageRank, we 

used SimRank (Haveliwala, 2002) to identify the 

nodes most similar to the query node and not 

only the central sentences in the graph.  

6 Evaluation 

6.1 Evaluation Dataset 

We worked on the dataset we created for 

QCFS and added semantic tags: 10% of the to-

kens had Wikipedia annotations and 33% had a 

UMLS annotation. 

6.2 Results 

 

Figure 5: ROUGE Recall Scores (with stemming and 

stop-words) 

For Focused KLSum we received ROUGE 

scores of (r1 = 0.281, r2 = 0.061, su4 = 0.100), 

KL-Chain-Update (r1 = 0.424, r2 = 0.149, su4 = 

0.193), ChainSum (r1 = 0.44988, r2 = 0.1587, 

su4 = 0.20594), ChainSum with t Simplified 

Topic model (r1 = 0.44992, r2 = 0.15814, su4 = 

0.20507) and for Modified-LexRank (r1 = 0.444, 

r2 = 0.151, su4 = 0.201). All of the modified ver-

sions of our algorithm performed better than Fo-

cused KLSum with more than 95% confidence.  

7 Conclusions 

We presented a new summarization task tai-

lored for the needs of exploratory search system. 

This task combines elements of question answer-

ing by sentence extraction with those of update 

summarization. 

The main contribution of this paper is the def-

inition of a new summarization task that corre-

sponds to exploratory search behavior and the 

contribution of a novel dataset containing human 

summaries. This dataset is annotated with Wik-

ipedia and UMLS terms for over 30% of the to-

kens. We controlled that the summaries cover 

only part of the input document sets (and are, 

therefore, properly focused) and sensitive to the 

position of the queries in the chain. 

Four methods were evaluated for the task. The 

baseline methods based on KL-Sum show a sig-

0

0.5

R1 R2 R3 R4 SU4
Focused-KLSum KLSum-Update LexRank-U

QC-LDA QC-simplified

920



nificant improvement when penalizing redun-

dancy with the previous summarization. 

This paper concentrated on â€œzoom inâ€ query 

chains, other user actions such as â€œzoom outâ€ or 

â€œswitch topicâ€ were left to future work. This pa-

per concentrated on â€œzoom inâ€ query chains, oth-

er user actions such as â€œzoom outâ€ or â€œswitch 

topicâ€ were left to future work.  The task remains 

extremely challenging, and we hope the dataset 

availability will allow further research to refine 

our understanding of topic-sensitive summariza-

tion and redundancy control. 

In future work, we will attempt to derive a 

task-specific evaluation metric that exploits the 

structure of the chains to better assess relevance, 

redundancy and contrast. 

Acknowledgments 

This work was supported by the Israeli Minis-

ter of Science (Grant #3-8705) and by the Lynn 

and William Frankel Center for Computer Sci-

ences, Ben-Gurion University.  We thank the 

reviewers for extremely helpful advice. 

References  

Marcia J. Bates. 1989. The design of browsing and 

berrypicking techniques for the online search 

interface, Online Information Review, 13(5), 407-

424.  

 

David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 

2003. Latent dirichlet allocation, the Journal of 

machine Learning research, 3, 993-1022. 

 

Olivier Bodenreider. 2004. The unified medical 

language system (UMLS): integrating biomedical 

terminology, Nucleic acids research, 32(suppl 1), 

D267-D270.  

 

John M. Conroy, Judith D. Schlesinger, and Dianne P. 

O'Leary. 2011. Nouveau-rouge: A novelty metric 

for update summarization, Computational 

Linguistics, 37(1), 1-8. 

 

Rebecca JW Cline, and Katie M. Haynes. 2001. 

Consumer health information seeking on the 

Internet: the state of the art, Health education 

research, 16(6), 671-692.  

 

Daume Hal and Daniel Marcu. 2006. Bayesian query-

focused summarization, In Proceedings of the 21st 

International Conference on Computational 

Linguistics and the 44th annual meeting of the 

Association for Computational Linguistics (pp. 

305-312). Association for Computational 

Linguistics. 

 

Jean-Yves Delort, and Enrique Alfonseca. 2012. 

DualSum: a Topic-Model based approach for 

update summarization, In Proceedings of the 13th 

Conference of the European Chapter of the 

Association for Computational Linguistics (pp. 

214-223). Association for Computational 

Linguistics.  

 

Rezarta Islamaj Dogan, G. Craig Murray, AurÃ©lie 

NÃ©vÃ©ol, and Zhiyong Lu. 2009. Understanding 

PubMedÂ® user search behavior through log 

analysis, Database: The Journal of Biological 

Databases & Curation, 2009. 

 

GÃ¼nes Erkan, and Dragomir R. Radev. 2004. 

LexRank: Graph-based lexical centrality as 

salience in text summarization, J. Artif. Intell. 

Res.(JAIR), 22(1), 457-479.  

 

Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 

2007. Measuring importance and query relevance 

in topic-focused multi-document summarization, In 

Proceedings of the 45th Annual Meeting of the 

ACL on Interactive Poster and Demonstration 

Sessions (pp. 193-196). Association for 

Computational Linguistics. 

 

Aria Haghighi, and Lucy Vanderwende. 2009. 

Exploring content models for multi-document 

summarization, In Proceedings of Human 

Language Technologies: The 2009 Annual 

Conference of the North American Chapter of the 

Association for Computational Linguistics (pp. 

362-370). Association for Computational 

Linguistics.  

 

Glen Jeh, and Jennifer Widom. 2002. SimRank: a 

measure of structural-context similarity, In 

Proceedings of the eighth ACM SIGKDD 

international conference on Knowledge discovery 

and data mining (pp. 538-543). ACM.  

 

Baoli Li, Joseph Irwin, Ernest V. Garcia, and Ashwin 

Ram. 2007. Machine learning based semantic 

inference: Experiments and Observations at RTE-

3, In Proceedings of the ACL-PASCAL Workshop 

on Textual Entailment and Paraphrasing (pp. 159-

164). Association for Computational Linguistics. 

 

Chin-Yew Lin. 2004. Rouge: A package for automatic 

evaluation of summaries, In Text Summarization 

Branches Out: Proceedings of the ACL-04 

Workshop (pp. 74-81). 

 

Sharon Lipsky-Gorman, and NoÃ©mie Elhadad 2011. 

ClinNote and HealthTermFinder: a pipeline for 

921



processing clinical notes, Columbia University 

Technical Report, Columbia University. 

 

Gary Marchionini. 2006. Exploratory search: from 

finding to understanding, Communications of the 

ACM, 49(4), 41-46.  

 

Bridget T. McInnes, Ted Pedersen, and Serguei VS 

Pakhomov. (2009). UMLS-Interface and UMLS-

Similarity: open source software for measuring 

paths and semantic similarity, AMIA Annual 

Symposium Proceedings, American Medical 

Informatics Association. 

 

David Milne. 2007. Computing semantic relatedness 

using wikipedia link structure, In Proceedings of 

the new zealand computer science research student 

conference. 

 

Ani Nenkova, and Rebecca J. Passonneau. 2004. 

Evaluating Content Selection in Summarization: 

The Pyramid Method, In HLT-NAACL (pp. 145-

152). 

 

Jahna Otterbacher, Gunes Erkan, and Dragomir R. 

Radev. 2009. Biased LexRank: Passage retrieval 

using random walks with question-based priors, 

Information Processing & Management, 45(1), 42-

54. 

 

Lawrence Page, Sergey Brin, Rajeev Motwani, and 

Terry Winograd. 1999. The PageRank citation 

ranking: bringing order to the web, 

Lev Ratinov, Dan Roth, Doug Downey, and Mike 

Anderson. 2011. Local and Global Algorithms for 

Disambiguation to Wikipedia, In ACL (Vol. 11, 

pp. 1375-1384). 

 

Ryen W. White, and Resa A. Roth. 2009. Exploratory 

search: Beyond the query-response paradigm. 

Synthesis Lectures on Information Concepts, 

Retrieval, and Services, 1(1), 1-98.  

 

 

 

922


