



















































Iterative Constrained Clustering for Subjectivity Word Sense Disambiguation


Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 269–278,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

Iterative Constrained Clustering for Subjectivity Word Sense
Disambiguation

Cem Akkaya, Janyce Wiebe
University of Pittsburgh

Pittsburgh PA, 15260, USA
{cem,wiebe}@cs.pitt.edu

Rada Mihalcea
University of North Texas
Denton TX, 76207, USA
rada@cs.unt.edu

Abstract

Subjectivity word sense disambiguation
(SWSD) is a supervised and application-
specific word sense disambiguation task
disambiguating between subjective and
objective senses of a word. Not sur-
prisingly, SWSD suffers from the knowl-
edge acquisition bottleneck. In this work,
we use a “cluster and label” strategy to
generate labeled data for SWSD semi-
automatically. We define a new algo-
rithm called Iterative Constrained Cluster-
ing (ICC) to improve the clustering purity
and, as a result, the quality of the gener-
ated data. Our experiments show that the
SWSD classifiers trained on the ICC gen-
erated data by requiring only 59% of the
labels can achieve the same performance
as the classifiers trained on the full dataset.

1 Introduction

Subjectivity lexicons (e.g., (Turney, 2002;
Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu
and Hatzivassiloglou, 2003; Kim and Hovy, 2004;
Bloom et al., 2007; Andreevskaia and Bergler,
2008; Agarwal et al., 2009)) play an important
role in opinion, sentiment, and subjectivity
analysis. These systems typically look for the
presence of clues in text. Recently, in (Akkaya
et al., 2009), we showed that subjectivity clues
are fairly ambiguous as to whether they express
subjectivity or not – words in such lexicons may
have both subjective and objective usages. We
call this problem subjectivity sense ambiguity.
Consider the following sentence containing the
clue “attack”:

(1) He was attacked by Milosevic for at-
tempting to carve out a new party from the
Socialists.

Knowing that “attack” is a subjectivity clue with
negative polarity will help a system recognize the
negative sentiment in the sentence. But for (2), the
same information is simply misleading, because
the clue is used with an objective meaning.

(2) A new treatment based on training T-cells
to attack cancerous cells ...

Any opinion analysis system which relies on a
subjectivity lexicon will be misled by subjectiv-
ity clues used with objective senses (false hits).
In (Akkaya et al., 2009), we introduced the task,
Subjectivity Word Sense Disambiguation, which is
to automatically determine which word instances
in a corpus are being used with subjective senses,
and which are being used with objective senses.
SWSD can be considered as a coarse-grained
and application-specific word sense disambigua-
tion task. We showed that sense subjectivity in-
formation about clues can be fed to subjectiv-
ity and sentiment analysis resulting in substantial
improvement for both subjectivity and sentiment
analysis by avoiding false hits.

Although SWSD is a promising tool, it suf-
fers from the knowledge acquisition bottleneck.
SWSD is defined as a supervised task, and fol-
lows a targeted approach common in the WSD lit-
erature for performance reasons. This means, for
each target clue, a different classifier is trained re-
quiring separate training data for each target clue.
It is expensive and time-consuming to obtain an-
notated datasets to train SWSD classifiers limit-
ing scalability. As a countermeasure, in (Akkaya
et al., 2011), we showed that non-expert annota-
tions collected through Amazon Mechanical Turk
(MTurk) can replace expert annotations success-
fully and might be used to apply SWSD on a large
scale.

Although non-expert annotations are cheap and
fast, they still incur some cost. In this work, we
aim to reduce the human annotation effort needed

269



to generate the same amount of subjectivity sense
tagged data by using a “cluster and label” strategy.
We hypothesize that we can obtain large sets of
labeled data by labelling clusters of instances of a
target word instead of single instances.

The main contribution of this work is a novel
constrained clustering algorithm called Iterative
Constrained Clustering (ICC) utilizing an active
constraint selection strategy. A secondary con-
tribution is a mixed word representation that is a
combination of previously proposed context rep-
resentations. We show that a “cluster and label”
strategy relying on these two proposed compo-
nents generates training data of good purity. The
resulting data has sufficient purity to train reli-
able SWSD classifiers. SWSD classifiers trained
on only 59% of the data achieve the same perfor-
mance as classifiers trained on 100% of the data,
resulting in a significant reduction in the annota-
tion effort. Our results take SWSD another step
closer to large scale application.

2 Cluster and Label

Our approach is inspired by a method lexicogra-
phers commonly employ to create sense invento-
ries, where they create inventories based on ev-
idence found in corpora. They use concordance
information to mine frequent usage patterns. (Kil-
garriff, 1997) describes this process in detail. A
lexicographer collects usages of a word in cor-
pora and groups them into coherent sets. The in-
stances in a set should have more in common with
each other than with the instances in other sets,
according to the criteria the lexicographer consid-
ers. After generating the sets, the lexicographer
codes each set as a dictionary definition based on
the common attributes of the instances. Our goal
is similar. Instead of generating dictionary defini-
tions, we are only interested in generating coher-
ent sets of usages of a word, so that we can label
each induced set – with its instances – to obtain
labeled data for SWSD. Our high-level grouping
criterion is that the instances in a cluster should be
similar subjective (objective) usages of the word.

Training data for an SWSD classifier consists
of instances of the target word tagged as having
a subjective sense (S) or an objective sense (O)
(subjectivity sense tagged data). We train a dif-
ferent SWSD classifier for each target word as in
(Akkaya et al., 2009). Thus, we need a different
training dataset for each target word. Our ultimate

goal is to reduce the human annotation effort re-
quired to create training data for SWSD classifiers.
For this purpose, we utilize a “cluster and label”
strategy relying on context clustering. Each in-
stance of a word is represented as a feature vector
(i.e., a context vector). The annotation process has
the following steps: (1) cluster the context vectors
of word instances, (2) label the induced clusters
as S or O, (3) propagate the given label to all in-
stances in a cluster.

The induced clusters represent different usage
patterns of a word. Thus, we build more than two
clusters, even though SWSD is a binary task. This
implies that two different instances of a word can
both be subjective, but end up in different clusters,
if they are different usages of the word.

Since we are labelling clusters as a whole, we
will introduce noise in the labeled data. Thus, in
developing the clustering process, we need to min-
imize that noise and find as pure clusters as possi-
ble.

The first step is to define the context representa-
tion of the instances. This is addressed in Section
3. Then, we turn in Section 4.2 to the clustering
process itself.

To evaluate our “cluster and label” strategy, we
use two gold standard subjectivity sense tagged
datasets. 1. The first one is called senSWSD gen-
erated in (Akkaya et al., 2009) and the second
one is called mturkSWSD generated in (Akkaya
et al., 2011). They consist of subjectivity sense
tagged data for disjoint sets of 39 and 90 words,
respectively. In this paper, we opt to use the
smaller dataset senSWSD as our development set,
on which we evaluate various context representa-
tions (in Section 3) and our proposed constrained
clustering algorithm (in Section 4.2). Then, on
mturkSWSD, we evaluate the quality of semi-
automatically generated data for SWSD classifi-
cation (in Section 4.3.2).

3 Context Representations

There has been much work on context representa-
tions of words for various NLP tasks. Clustering
word instances in order to discriminate senses of
a word is called Word Sense Discrimination. Con-
text representations for this task rely on two main
types of models: distributional semantic models
(DSM) and feature-based models.

1Available at http://mpqa.cs.pitt.edu/
corpora

270



(Schutze, 1998), which is still a competi-
tive model for word-sense discrimination by con-
text clustering, relies on a distributional semantic
model (DSM) (Turney and Pantel, 2010; Sahlgren,
2006; Bullinaria and Levy, 2007). A DSM is usu-
ally a word-to-word co-occurrence matrix – also
called semantic space – such that each row repre-
sents the distribution of a target word in a large
text corpus. Each row gives the semantic sig-
nature of a word, which is basically a high di-
mensional numeric vector. Note that this high di-
mensional vector represents word types, not word
tokens. Thus, it cannot model a word instance
in context. For token-based treatment, (Schutze,
1998) utilizes a second-order representation by av-
eraging co-occurrence vectors of the words (cor-
responding to rows of the co-occurrence matrix)
that occur in that particular context. It is impor-
tant to note that (Schutze, 1998) uses an addi-
tive model for compositional representation. Re-
cently, in (Akkaya et al., 2012), we found that a
DSM built using multiplicative composition – pro-
posed by (Mitchell and Lapata, 2010) for a differ-
ent task – gives better performance than the model
described by (Schutze, 1998).

We test both methods in this paper, using the
same semantic space. The space is built from a
corpus consisting of 120 million tokens. The rows
of the space correspond to word forms and the
columns correspond to word lemmas present in the
corpus. We adopt the parameters for our semantic
space from (Mitchell and Lapata, 2010): window
size of 10 and dimension size of 2000 (i.e., the
2000 most frequent lemmas). We do not filter out
stop words, since they have been shown to be use-
ful for various semantic similarity tasks in (Bulli-
naria and Levy, 2007). We use positive point-wise
mutual information to compute values of the vec-
tor components, which has also been shown to be
favourable in (Bullinaria and Levy, 2007).

Purandere and Pedersen is the prominent repre-
sentative of feature-based models. (Purandare and
Pedersen, 2004) creates context vectors from local
feature representations similar to the feature vec-
tors found in supervised WSD. In this work, we
use the following features from (Mihalcea, 2002)
to build the local feature representation: (1) the
target word itself and its part of speech, (2) sur-
rounding context of 3 words and their part of
speech, (3) the head of the noun phrase, (4) the
first noun and verb before the target word, (5) the
first noun and verb after the target word.

skew local dsm add dsm mul mix rep
average 79.90 80.50 80.50 83.53 85.23
appear-v 53.83 54.85 54.85 57.40 69.39
fine-a 70.07 72.26 70.07 74.45 75.18
interest-n 54.41 54.78 55.88 81.62 81.62
restraint-n 70.45 71.97 75.00 71.21 81.82

Table 1: Evaluation of Various Context Representations

3.1 Evaluation of Context Representations
In this section, we evaluate context representations
for the context clustering task on the subjectivity
sense tagged data, senSWSD. The evaluation is
done separately for each word.

We use the same clustering algorithm for all
context representations: agglomerative hierarchi-
cal clustering with average linkage criteria. In all
our experiments throughout the paper, we fix the
cluster size to 7 as it is done in (Purandare and
Pedersen, 2004). We think that is reasonable num-
ber since SENSEVAL III reports that the average
number of senses per word is 6.47. We choose
cluster purity as our evaluation metric. To com-
pute cluster purity, we assign each cluster to a
sense label, which is the most frequent one in the
cluster. The number of the correctly assigned in-
stances divided by the number of all the clustered
instances gives us cluster purity.

Row 1 of Table 1 holds the cumulative results
over all the words in senSWSD (micro averages).
The table also reports detailed results for 4 sample
selected words from senSWSD. skew stands for
the percentage of the most frequent label. dsm add
is the representation based on (Schutze, 1998),
dsm mul stands for the representation as described
in (Akkaya et al., 2012) and local features is the
local feature representation based on (Purandare
and Pedersen, 2004). The results show that among
dsm mul, dsm add, and local features; dsm mul
performs the best.

When we look at the context clustering re-
sults for single words separately, we observe
that the performance of different representations
vary. There is not a single winner among all
words. Thus, perhaps choosing one single repre-
sentation for all the words is not optimal. Hav-
ing that in mind, we try merging the dsm mul
and local features representations. We leave out
dsm add representation, since both dsm mul and
dsm add rely on the same type of semantic infor-
mation (i.e., a DSM). We hypothesize that the two

271



representations, one relying on a semantic space
and the other relying on local WSD features, may
complement each other.

To merge the representations, we concatenate
the two feature vectors into one. First, however,
we normalize each vector to unit length, since the
individual vectors have different scales and would
have unequal contribution, otherwise. We call this
mixed representation mix rep.

In Table 1, we see that, overall, mix rep per-
forms better than all the other representations. The
improvement is statistically significant at the p <
.05 level on a paired t-test. We observe that, even
when mix rep does not perform the best, it is never
bad. mix rep is the winner or ties for the winner
for 25 out of 39 words. This number is 13, 13, and
15 for dsm add, dsm mul and local features, re-
spectively. For the words for which mix rep is not
the winner, it is, on average, 1.47 points lower than
the winner. This number is 4.22, 6.83, and 7.07
for the others. The results provide evidence that
mix rep is consistently good and reliable. Thus, in
our experiments, mix rep will be our choice as the
context representation.

4 Clustering Process

We now turn to the clustering process. In a “clus-
ter and label” strategy, in order to be able to label
clusters, we need to annotate some of the instances
in each cluster. Then, we can accept the majority
label found in a cluster as its label. Thus, some
manual labelling is required, preferably a small
amount.

We propose to provide this small amount of an-
notated data prior to clustering, and then perform
semi-supervised clustering. This way the provided
labels will guide the clustering algorithm to gener-
ate the clusters that are more suitable for our end
task, namely clusters where subjective and objec-
tive instances are grouped together.

4.1 Constrained Clustering

Constrained clustering (Grira et al., 2004) also
known as semi-supervised clustering is a recent
development in the clustering literature. In addi-
tion to the similarity information required by un-
supervised clustering, constrained clustering re-
quires pairwise constraints. There are two types
of constraints: (1) must-link and (2) cannot-link
constraints. A must-link constraint dictates that
two instances should be in the same cluster and a

cannot-link dictates that two instances should not
be in the same cluster. In this work, we only con-
sider cannot-links, because of the definition of our
SWSD task. Two instances sharing the same label
do not need to be in the same cluster, since the in-
duced clusters represent different usage patterns of
a word. For example, two instances labeled S need
not be similar to each other. They can be different
usages, both having a subjective meaning. On the
other hand, if two instances are labeled having op-
posing labels, we do not want them to be in the
same cluster. Thus, we utilize cannot-links but not
must-links.

Constraints can be obtained from domain
knowledge or from available instance labels. In
our work, constraints are generated from instance
labels. Each instance pair with opposing labels is
considered to be cannot-linked.

There are two general strategies to incorporate
constraints into clustering. The first is to adapt
the similarity between instances (Xing et al., 2002;
Klein et al., 2002) by adjusting the underlying dis-
tance metric. The main idea is to make the dis-
tance between must-linked instances – their neigh-
bourhoods – smaller and the distance between
cannot-linked instances – their neighbourhoods –
larger. The second strategy is modifying the clus-
tering algorithm itself so that search is biased to-
wards a partitioning for which the constraints hold
(Wagstaff and Cardie, 2000; Basu et al., 2002;
Demiriz et al., 1999).

Our proposed constrained clustering method re-
lies on some ideas from (Klein et al., 2002). Thus,
we explain it in more detail. (Klein et al., 2002)
utilizes agglomerative hierarchical clustering with
complete-linkage. The algorithm imposes con-
straints by changing the distance matrix accord-
ing to the given constraints. The distances be-
tween must-linked instances are set to 0. That is
not enough by itself, since if two instances are
must-linked, other instances close to them should
also get closer to each other. This means there is
a need to propagate the constraints. This is done
by calculating the shortest path between all the in-
stances and updating the distance matrix accord-
ingly. To impose cannot-links, the distance be-
tween two cannot-linked instances is set to some
large number. The complete-linkage property
indirectly propagates the cannot-link constraints,
since it will not allow two clusters to be merged if
they contain instances that are cannot-linked.

Although previous work report on average sub-

272



stantial improvement in the clustering purity,
(Davidson et al., 2006) shows that even if the
constraints are generated from gold-standard data,
some constraint sets can decrease clustering pu-
rity. The results vary significantly depending on
the specific set of constraints used. To our knowl-
edge, there have been two approaches for select-
ing informative constraint sets (Basu et al., 2004;
Klein et al., 2002). The method described in
(Basu et al., 2004) uses the farthest-first traversal
scheme. That strategy is not suitable in our setting,
since we have only two labels. After selecting
just one instance from both labels, this method be-
comes the same as random selection. The strategy
described in (Klein et al., 2002) is more general.
At first, the hierarchical clustering algorithm fol-
lows in a unconstrained fashion until some moder-
ate number of clusters are remaining. Then, the al-
gorithm starts to request constraints between roots
whenever two clusters are merged.

4.2 Iterative Constrained Clustering

Our proposed algorithm is closely related to (Klein
et al., 2002). We share the same backbone:
(1) the agglomerative hierarchical clustering with
complete-linkage and (2) the mechanism to im-
pose cannot-link constraints described in Section
4.1. For our algorithm, we implement a second
mechanism for imposing constraints proposed by
(Xing et al., 2002) (Section 4.2.1) and use both
mechanisms in combination. We also propose a
novel constraint selection method (Section 4.2.2).

4.2.1 Imposing Constraints
(Klein et al., 2002) imposes cannot-link con-
straints by adjusting the distance between cannot-
linked pairs heuristically and by relying on com-
plete linkage for propagation. Although this ap-
proach was shown to be effective, we believe it
does not make full use of the provided constraints.
We believe that learning a new distance metric will
result in more reliable distance estimates between
all instances. For this purpose, we learn a Maha-
lanobis distance function following the method de-
scribed in (Davis et al., 2007). (Davis et al., 2007)
formulate the problem of distance metric learn-
ing as minimizing the differential relative entropy
between two multivariate Gaussians under con-
straints. Note that using distance metric learning
for imposing constraints was previously proposed
by (Xing et al., 2002). (Xing et al., 2002) pose
metric learning as a convex optimization problem.

The reason we choose the metric learning method
(Davis et al., 2007) over (Xing et al., 2002) is that
it is computationally more efficient.

(Klein et al., 2002) has a favourable property we
want to keep. The constraints are imposed strictly,
meaning that no cannot-linked instances can ap-
pear in the same cluster. I.e., they are hard con-
straints. In the case of metric learning, the con-
straints are not imposed strictly. In a new learned
distance metric, two cannot-linked instances will
be relatively distant, but there is no guarantee they
will not end up in the same cluster. Although we
think that metric learning makes a better use of
provided constraints, we do not want to lose the
benefit of hard constraints. Thus, we use both
mechanisms in combination to impose constraints.
We first learn a Mahalanobis distance based on the
provided constraints. Then, we compute distance
matrix and employ the mechanism proposed by
(Klein et al., 2002) on the learned distance matrix.

4.2.2 Active Constraint Generation
As mentioned before, the choice of the set of con-
straints affects the quality of the end clustering. In
this work, we define a novel method to choose in-
formative instances, which we believe will have
maximum impact on the end cluster quality, when
they are labeled and used to generate constraints
for our task. We use an iterative approach. Each
iteration consists of three steps: (1) generating
clusters by the process described in Section 4.2.1
imposing available constraints, (2) choosing the
most informative instance, considering the cluster
boundaries, and acquiring its label, (3) extending
the available constraints with the ones we generate
from the newly labeled instance.

We consider an instance to be informative if
there is a high probability that the knowledge of
its label may change the cluster boundaries. The
more probable that change is, the more informa-
tive is the instance. The basic idea is that if an
instance is in a cluster holding instances of type
a and it is close to another cluster holding in-
stances of type b, that instance is most likely mis-
clustered. Thus, it should be queried. Our hypoth-
esis is that, in each iteration, the algorithm will
choose the most problematic – informative – in-
stance that will end up changing cluster bound-
aries. This will result in each iteration in a more
reliable distance metric, which in return will pro-
vide more reliable estimates of problematic in-
stances in future iterations. The imposed con-

273



Algorithm 1 Iterative Constrained Clustering
1: C = cluster(I)
2: I{L} = labelprototypes(C)
3: while

∣∣I{L}∣∣ < stop do
4: Con = createconstraints(I{L})
5: Matrixdist = learnmetric(I, Con)
6: C = constraintedcluster(Matrixdist, Con)
7: L = labelmostinformative(C)
8: I{L} = I{L} ∪ L
9: end while

10: propagatelabels(I{L}, C) {C...Clusters; Con...Constraints;
I...Instances; I{L}...Labeled Instances; Matrixdist...Distance Matrix}

straints will move the clustering in each iteration
towards better separation of S and O instances.

To define informativeness, we define a scoring
function, which is used to score each data point on
its goodness. The lower the score, the more likely
it is that the instance is mis-clustered. Choosing
the data point with the lowest score will likely
change clustering borders in the next iteration.
Our scoring function is based on the silhouette co-
efficient, a popular unsupervised cluster validation
metric to measure goodness (Tan et al., 2005) of
a cluster member. Basically, the silhouette score
assigns a cluster member that is close to another
cluster a lower score, and a cluster member that
is closer to the cluster center a higher score. That
is partly what we want. In addition, we do not
want to penalize a cluster member that is close to
another cluster having members with the same la-
bel. For this purpose, we calculate the silhouette
score only over clusters with an opposing label
(i.e., holding members with an opposing label). In
addition, we consider only instances labeled so far
when computing the score. We call this new coef-
ficient silhconst. It is computed as follows: (1) for
an instance i, compute its average distance from
the other instances in its cluster xi which are al-
ready labeled, (2) for an instance i, compute its
average distance from the labeled instances of the
clusters from an opposing label and take the mini-
mum of these averages yi, (3) compute the silhou-
ette coefficient as (yi-xi) / max(yi,xi).

The silhconst coefficient has favourable proper-
ties. First, it scores members that are close to
a cluster with an opposing label lower than the
members that are close to a cluster with the same
label. According to our definition, these mem-
bers are more informative. Figure 1 holds a sam-
ple cluster setting. The shape of a member de-
notes its label and its fill denotes whether or not it
has been queried. In this example, silhconst scores

3 
1 

2 

Figure 1: Behaviour of selection function

members 2 and 3 lower than 1. Thus, member 1
will not be selected, which is the right decision in
this example. Both members 2 and 3 are close to
clusters with an opposing label. In this example
silhconst scores member 3 lower, which is farther
away from already labeled members in the clus-
ter. Thus, member 3 will be selected to be labeled.
This type of behaviour results in an explorative
strategy.

The active selection strategy proposed by (Klein
et al., 2002) is single pass. Thus, it does not have
the opportunity to observe the complete cluster
structure before choosing constraints. We hypoth-
esize that our strategy will provide more informa-
tive constraints, since it has the advantage of be-
ing able to base the decision of which constraints
to generate on fully observed cluster structure in
each iteration.

We call our proposed algorithm Iterative Con-
strained Clustering (ICC). In our final implemen-
tation, ICC starts by simply clustering the in-
stances without any constraints. The algorithm
queries the label of the prototypical member –
the member closest to the cluster center – of each
cluster. Then, the described iterations begin. Al-
gorithm 1 contains the complete ICC algorithm.
Note that line 6 is equivalent to the algorithm of
(Klein et al., 2002).

4.3 Experiments
This section gives details on experiments to evalu-
ate the purity of the semi-automatically generated
subjectivity sense tagged data by our “cluster and
label” strategy. We carry out detailed analysis to
quantify the effect of the proposed active selec-
tion strategy and of metric learning on the purity
of the generated data. We compare our active se-
lection strategy to random selection and also to
(Klein et al., 2002). The comparison is done on
the senSWSD dataset. SenSWSD consists of three
subsets, SENSEVAL I,II and III. Since we devel-

274



Figure 2: Label Purity – ICC vs. random selection

oped our active selection algorithm on the SEN-
SEVAL I subset, we use only SENSEVAL II and
III subsets for comparison. We apply ICC to each
word in the comparison set separately, and report
cumulative results for the purity of the generated
data. We report results for different percentages of
the queried data amount (e.g. 10% means that the
algorithm queried 10% of the data to create con-
straints). This way, we obtain a learning curve.
We fix the cluster number to 7 as in the context
representation experiments.

4.3.1 Effect of Active Selection Strategy
Figure 2 holds the comparison of ICC with
silhconst selection to a random selection baseline.
“majority” stands for majority label frequency in
the dateset. We see that silhconst performs better
than the random selection. By providing labels to
only 25% of the data, we can achieve 87.67% pure
fully labeled data.

For comparison, we also evaluate the perfor-
mance of (Klein et al., 2002) with their active con-
straint selection strategy as described in Section
4.1. Note that originally (Klein et al., 2002) re-
quests the constraint between two roots. In our
setting, it requests labels of the roots and then gen-
erates constraints from the obtained labels. Since
we have a binary task, querying labels makes more
sense. This has the advantage that more con-
straints from each request are obtained. More-
over, it allows a direct comparison to our algo-
rithm. (Klein et al., 2002) does not use any metric
learning. Thus, we run our algorithm also without
metric learning, in order to compare the effective-
ness of both active selection strategies fairly. In
Figure 3, we see that silhconst performs better than
the active selection strategy described in (Klein et
al., 2002). We also see that metric learning results

Figure 3: Label Purity – ICC vs. Klein

Figure 4: SWSD accuracy on ICC generated data

in a big improvement. In addition, metric learn-
ing results in a smoother learning curve, which is
a favourable property for a real-world application.

4.3.2 SWSD on semi-automatically generated
annotations

Now that we have a tool to generate training data
for SWSD, we want to evaluate it on the actual
SWSD task. We want to see if the obtained purity
is enough to create reliable SWSD classifiers. For
this purpose, we test ICC on mturkSWSD dataset.

For each word in our dataset, we conduct 10-
fold cross-validation experiments. ICC is ap-
plied to training folds to label instances semi-
automatically. We train SWSD classifiers on the
generated training fold labels and test the classi-
fiers on the corresponding test fold. We distin-
guish between queried instances and propagated
labels. The queried instances are weighted as
1 and the instances with propagated labels are
weighted by their silhconst score, since that mea-
sure gives the goodness of an instance. The score
is defined between -1 and 1. This score is normal-
ized between 0 and 1, before it is used as a weight.
SVM classifiers from the Weka package (Witten
and Frank., 2005) with its default settings are used

275



as in (Akkaya et al., 2011).
We implement two baselines. The first is sim-

ple random sampling and the second is uncer-
tainty sampling, which is an active learning (AL)
method. We use “simple margin” selection as de-
scribed in (Tong and Koller, 2001). It selects, in
each iteration, the instance closest to the decision
boundary of the trained SVM. Each method is run
until it reaches the accuracy of training fully on
the gold-standard data. ICC reaches that bound-
ary when provided only 59% of the labels in the
dataset. For uncertainty sampling and random
sampling, these values are 92% and 100%, respec-
tively. In Figure 4, we see the SWSD accuracy for
different queried data percentages. “full” stands
for training fully on gold-standard data. We see
that training SWSD on semi-automatically labeled
data by ICC does consistently better than uncer-
tainty sampling and random sampling.

It is surprising to see that uncertainty sampling
overall does not do better than random sampling.
We believe that it might be because of sampling
bias. During AL, as more and more labels are
obtained, the training set quickly diverges from
the underlying data distribution. (Schütze et al.,
2006) states that AL can explore the feature space
in such a biased way that it can end up ignoring en-
tire clusters of unlabeled instances. We think that
SWSD is highly prone for the mentioned missed
cluster problem because of its unique nature. As
mentioned, SWSD is a binary task where we dis-
tinguish between subjective and objective usages
of a subjectivity word. Although the classifica-
tion is binary, the underlying usages are grouped
into multiple clusters corresponding to senses of
the word. It is possible that two groups of usages
which are represented quite differently in the fea-
ture space are both subjective or objective. More-
over, one usage group might be closer to a usage
group from the opposing label than to a group with
the same label.

We see that our method reduces the annotation
amount by 36% in comparison to uncertainty sam-
pling and by 41% in comparison to random sam-
pling to reach the performance of the SWSD sys-
tem trained on fully annotated data.

5 Related Work

One related line of research is constrained clus-
tering also known as semi-supervised clustering
(Xing et al., 2002; Wagstaff and Cardie, 2000;

Grira et al., 2004; Demiriz et al., 1999). It has
been applied to various datasets and tasks such
as image and document categorization. To our
knowledge, we are the first to utilize constrained
clustering for a difficult NLP task.

There have been only two previous works se-
lecting constraints for constrained clustering ac-
tively (Basu et al., 2004; Klein et al., 2002). The
biggest difference of our approach is that it is iter-
ative as opposed to single pass.

Active Learning (AL) (Settles, 2009; Settles
and Craven, 2008; Hwa, 2004; Tong and Koller,
2001) builds another important set of related work.
Our method is inspired by uncertainty sampling.
We accomplish active selection in the clustering
setting.

6 Conclusions

In this paper, we explore a “cluster and la-
bel” strategy to reduce the human annotation ef-
fort needed to generate subjectivity sense-tagged
data. In order to keep the noise in the semi-
automatically labeled data minimal, we investigate
different feature space types and evaluate their ex-
pressiveness. More importantly, we define a new
algorithm called iterative constrained clustering
(ICC) with an active constraint selection strategy.
We show that we can obtain a fairly reliable la-
beled data when we utilize ICC.

We show that the active selection strategy
we propose outperforms a previous approach by
(Klein et al., 2002) for generating subjectivity
sense-tagged data. Training SWSD classifiers on
ICC generated data improves over random sam-
pling and uncertainty sampling (Tong and Koller,
2001). We can achieve on mturkSWSD 36% an-
notation reduction over uncertainty sampling and
41% annotation reduction over random sampling
in order to reach the performance of SWSD clas-
sifiers trained on fully annotated data.

To our knowledge, this work is the first applica-
tion of constrained clustering to a hard NLP prob-
lem. We showcase the power of constrained clus-
tering. We hope that the same “cluster and label”
strategy will be applicable to Word Sense Disam-
biguation. This will be part of our future work.

7 Acknowledgments

This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046.

276



References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.

2009. Contextual phrase-level polarity analysis us-
ing lexical affect scoring and syntactic N-grams. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 24–
32, Athens, Greece, March. Association for Compu-
tational Linguistics.

Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
190–199, Singapore, August. Association for Com-
putational Linguistics.

Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of
subjectivity word sense disambiguation on contex-
tual opinion analysis. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 87–96, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.

Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2012. Utilizing semantic composition in distribu-
tional semantic models for word sense discrimina-
tion and word sense disambiguation. In ICSC, pages
45–51.

Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Over-
coming domain dependence in sentiment tagging.
In Proceedings of ACL-08: HLT, pages 290–298,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.

Sugato Basu, Arindam Banerjee, and R. Mooney.
2002. Semi-supervised clustering by seeding. In
In Proceedings of 19th International Conference on
Machine Learning (ICML-2002).

Sugato Basu, Arindam Banerjee, and Raymond J.
Mooney. 2004. Active semi-supervision for pair-
wise constrained clustering. In SDM.

Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308–315, Rochester, NY.

John Bullinaria and Joseph Levy. 2007. Ex-
tracting semantic representations from word
co-occurrence statistics: A computational
study. Behavior Research Methods, 39:510–526.
10.3758/BF03193020.

Ian Davidson, Kiri Wagstaff, and Sugato Basu. 2006.
Measuring constraint-set utility for partitional clus-
tering algorithms. In PKDD, pages 115–126.

Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra,
and Inderjit S. Dhillon. 2007. Information-theoretic
metric learning. In Proceedings of the 24th interna-
tional conference on Machine learning, ICML ’07,
pages 209–216, New York, NY, USA. ACM.

Ayhan Demiriz, Kristin Bennett, and Mark J. Em-
brechts. 1999. Semi-supervised clustering using
genetic algorithms. In In Artificial Neural Networks
in Engineering (ANNIE-99, pages 809–814. ASME
Press.

Nizar Grira, Michel Crucianu, and Nozha Boujemaa.
2004. Unsupervised and semi-supervised cluster-
ing: a brief survey. In in A Review of Ma-
chine Learning Techniques for Processing Multime-
dia Content, Report of the MUSCLE European Net-
work of Excellence.

Rebecca Hwa. 2004. Sample selection for statis-
tical parsing. Comput. Linguist., 30(3):253–276,
September.

Adam Kilgarriff. 1997. I dont believe in word senses.
Computers and the Humanities, 31(2):91–113.

Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the Twentieth International Conference on Compu-
tational Linguistics (COLING 2004), pages 1267–
1373, Geneva, Switzerland.

D. Klein, K. Toutanova, I.T. Ilhan, S.D. Kamvar, and
C. Manning. 2002. Combining heterogeneous clas-
sifiers for word-sense disambiguation. In Proceed-
ings of the ACL Workshop on ”Word Sense Dis-
ambiguatuion: Recent Successes and Future Direc-
tions, pages 74–80, July.

R. Mihalcea. 2002. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan, August.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.

A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning (CoNLL
2004), Boston.

Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2003), pages
105–112, Sapporo, Japan.

Magnus Sahlgren. 2006. The Word-Space Model: us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.

Hinrich Schütze, Emre Velipasaoglu, and Jan O. Ped-
ersen. 2006. Performance thresholding in practical
text classification. In Proceedings of the 15th ACM
international conference on Information and knowl-
edge management, CIKM ’06, pages 662–671, New
York, NY, USA. ACM.

277



H. Schutze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97–124.

Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’08, pages 1070–1079, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Burr Settles. 2009. Active Learning Literature Survey.
Technical Report 1648, University of Wisconsin–
Madison.

Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2005. Introduction to Data Mining, (First Edi-
tion). Addison-Wesley Longman Publishing Co.,
Inc., Boston, MA, USA.

Simon Tong and Daphne Koller. 2001. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45–66.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. March.

Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02), pages 417–424, Philadelphia,
Pennsylvania.

Kiri Wagstaff and Claire Cardie. 2000. Clustering
with instance-level constraints. In Proceedings of
the Seventeenth International Conference on Ma-
chine Learning (ICML-2000), pages 1103–1110.

Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal taxonomies for sen-
timent analysis. In Proceedings of CIKM-05, the
ACM SIGIR Conference on Information and Knowl-
edge Management, Bremen, DE.

I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, June.

Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and
Stuart J. Russell. 2002. Distance metric learning
with application to clustering with side-information.
In NIPS, pages 505–512.

Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003), pages 129–136, Sapporo, Japan.

278


