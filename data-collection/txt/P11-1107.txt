










































Metagrammar engineering: Towards systematic exploration of implemented grammars


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1066–1076,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics

Metagrammar Engineering:
Towards systematic exploration of implemented grammars

Antske Fokkens
Department of Computational Linguistics, Saarland University &

German Research Center for Artificial Intelligence (DFKI) Project Office Berlin
Alt-Moabit 91c, 10559 Berlin, Germany

afokkens@coli.uni-saarland.de

Abstract

When designing grammars of natural lan-
guage, typically, more than one formal anal-
ysis can account for a given phenomenon.
Moreover, because analyses interact, the
choices made by the engineer influence the
possibilities available in further grammar de-
velopment. The order in which phenomena
are treated may therefore have a major impact
on the resulting grammar. This paper proposes
to tackle this problem by using metagrammar
development as a methodology for grammar
engineering. I argue that metagrammar engi-
neering as an approach facilitates the system-
atic exploration of grammars through compar-
ison of competing analyses. The idea is illus-
trated through a comparative study of auxil-
iary structures in HPSG-based grammars for
German and Dutch. Auxiliaries form a cen-
tral phenomenon of German and Dutch and
are likely to influence many components of
the grammar. This study shows that a spe-
cial auxiliary+verb construction significantly
improves efficiency compared to the standard
argument-composition analysis for both pars-
ing and generation.

1 Introduction

One of the challenges in designing grammars of nat-
ural language is that, typically, more than one for-
mal analysis can account for a given phenomenon.
The criteria for choosing between competing analy-
ses are fairly clear (observational adequacy, analyti-
cal clarity, efficiency), but given that analyses of dif-
ferent phenomena interact, actually evaluating anal-
yses on those criteria in a systematic manner is far

from straightforward. The standard methodology in-
volves either picking one analysis, and seeing how
it goes, then backing out if it does not work out,
or laboriously adapting a grammar to two versions
supporting different analyses (Bender, 2010). The
former approach is not in any way systematic, in-
creasing the risk that the grammar is far from opti-
mal in terms of efficiency. The latter approach po-
tentially causes the grammar engineer an amount of
work that will not scale for considering many differ-
ent phenomena.

This paper proposes a more systematic and
tractable alternative to grammar development: meta-
grammar engineering. I use “metagrammar” as a
generic term to refer to a system that can generate
implemented grammars. The key idea is that the
grammar engineer adds alternative plausable anal-
yses for linguistic phenomena to a metagrammar.
This metagrammar can generate all possible com-
binations of these analyses automatically, creating
different versions of a grammar that cover the same
phenomena. The engineer can test directly how
competing analyses for different phenomena inter-
act, and determine which combinations are possible
(after minor adaptations) and which analyses are in-
compatible.

The idea of metagrammar engineering is illus-
trated here through a case study of word order and
auxiliaries in Germanic languages, which forms the
second goal of this paper. Auxiliaries form a central
phenomenon of German and Dutch and are likely to
influence many components of the grammar. The re-
sults show that the analysis of auxiliary+verb struc-
tures presented in Bender (2010) significantly im-1066



proves efficiency of the grammar compared to the
standard argument-composition analysis within the
range of phenomena studied. Because future re-
search is needed to determine whether the auxil-
iary+verb alternative can interact properly with ad-
ditional phenomena and still lead to more efficient
results than argument-composition, it is particularly
useful to have a grammar generator that can auto-
matically create grammars with either of the two
analyses.

The remainder of this paper starts with the case
study. Section 2 provides a description of the con-
text of the study. The relevant linguistic properties
and alternative analyses are described in Sections
3 and 4. After evaluating and discussing the case
study’s results, I return to the general approach of
metagrammar engineering. Section 6 presents re-
lated work on metagrammars. It is followed by a
conclusion and discussion on using metagrammars
as a methodology for grammar engineering.

2 A metagrammar for Germanic
Languages

2.1 The LinGO Grammar Matrix

The LinGO Grammar Matrix (Bender et al., 2002;
Bender et al., 2010) provides the main context for
the experiments described in this paper. To begin
with, its further development plays a significant role
for the motivation of the present study. More impor-
tantly, the Germanic metagrammar is implemented
as a special branch of the LinGO Grammar Matrix
and uses a significant amount of its code.

The Grammar Matrix customization system al-
lows users to derive a starter grammar for a particu-
lar language from a common multi-lingual resource
by specifying linguistic properties through a web-
based questionnaire. The grammars are intended for
parsing and generation with the LKB (Copestake,
2002) using Minimal Recursion Semantics (Copes-
take et al., 2005, MRS) as parsing output and gener-
ation input. After the starter grammar has been cre-
ated, its development continues independently: en-
gineers can thus make modifications to their gram-
mar without affecting the multi-lingual resource.

Internally, the customization system works as fol-
lows: The web-based questionnaire registers lin-
guistic properties in a file called “choices” (hence-

forth choices file). The customization system takes
this choices file as input to create grammar frag-
ments, using so-called “libraries” that contain imple-
mentations of cross-linguistically variable phenom-
ena. Depending on the definitions provided in the
choices file, different analyses are retrieved from the
customization system’s libraries. The language spe-
cific implementations inherit from a core grammar
which handles basic phrase types, semantic compo-
sitionality and general infrastructure, such as feature
geometry (Bender et al., 2002).

The present study is part of a larger effort to im-
prove the customization library for auxiliary struc-
tures in free word order and verb second languages.
It examines whether Bender’s observations concern-
ing an improved analysis for auxiliaries in Wambaya
(Bender, 2010) also hold for Germanic languages. A
more elaborate study of German and Dutch (includ-
ing both Flemish and (Northern) Dutch, which have
slightly different word order constraints) is informa-
tive, because these languages are well-described and
known to have distinctly challenging word order be-
havior.

2.2 Germanic branch

In order to create grammars for Germanic lan-
guages, a specialized branch of the Grammar Ma-
trix customization system was developed. This Ger-
manic grammars generator uses the Grammar Ma-
trix’s facilities to generate types in type description
language (tdl). At present, the generator uses the
Grammar Matrix analyses for agreement and case
marking as well as basics from its morphotactics,
coordination and lexicon implementations.

In the first stage, the word order library and aux-
iliary implementation were extended to cover two
alternative analyses for Germanic word order (see
Section 4). The coordination library was adapted to
ensure correct interactions with the new word order
analyses and agreement. The morphotactics library
was extended to cover Dutch and Flemish interac-
tions between word order and morphology. Finally,
the lexicon and verbal case pattern implementations
were extended to cover ditransitive verbs.

Both versions of word order analyses can be
tweaked to include or exclude a rarely occurring
variant of partial VP fronting (see Section 4.3) re-
sulting in four distinct grammars for each of the1067



Vorfeld LB Mittelfeld RB Nachfeld
Der Mann hat den Jungen gesehen nach der Party
The man.nom has the boy.acc seen after the party
Der Mann hat den Jungen nach der Party gesehen
Den Jungen hat der Mann gesehen nach der Party
Nach der Party hat der Mann den Jungen gesehen
Den Jungen gesehen hat der Mann nach der Party
Gesehen hat der Mann den Jungen nach der Party
The man saw the boy after the party

Table 1: Basic structure of German word order (not exhaustive)

languages under investigation. These 12 grammars
cover Dutch, Flemish and German main clauses with
up to three core arguments.1

3 Germanic word order

3.1 German word order

Topological fields (Erdmann, 1886; Drach, 1937)
form the easiest way to describe German word or-
der. The sentence structure for declarative main
clauses, consists of five topological fields: Vorfeld
(“pre-field”), Left Bracket (LB), Mittelfeld (“middle
field”), Right Bracket (RB) and the Nachfeld (“after
field”). A subset of permissible alternations in Ger-
man are provided in Table 1. The last two sentences
present an example of partial VP fronting.

The fields are defined with regard to verbal forms,
which are placed in the Left and Right Brackets.
Each topological field has word order restrictions
of its own. The Vorfeld must contain exactly one
constituent in an affirmative main clause. The Left
Bracket contains the finite verb and no other ele-
ments. Other verbal forms (if not fronted to the Vor-
feld) must be placed in the Right Bracket. Most non-
verbal elements are placed in the Mittelfeld. When
main verbs are placed in the Vorfeld, their object(s)
may stay in the Mittelfeld. This kind of partial VP
fronting is illustrated by the last example in Table 1.
The Nachfeld typically contains subordinate clauses
and sometimes adverbial phrases.

In German, the respective order between the verbs
in the Right Bracket is head-final, i.e. auxiliaries fol-
low their complements. The only exception is the

1The grammar generation system also creates Danish gram-
mars. Danish results are not presented, because the language
does not pose the challenges explained in Section 4.

auxiliary flip: under certain conditions in subordi-
nate clauses, the finite verb precedes all other verbal
forms.

3.2 Dutch word order

Dutch word order reveals the same topological fields
as German. There are two main differences between
the languages where word order is concerned. First,
whereas the order of arguments in the German Mit-
telfeld allows some flexibility depending on infor-
mation structure, Dutch argument order is fixed, ex-
cept for the possibility of placing any argument in
the Vorfeld. A related aspect is that Dutch is less
flexible as to what partial VPs can be placed in the
Vorfeld.

The second difference is the word order in the
Right Bracket. The order of auxiliaries and their
complements is less rigid in Dutch and typically
auxiliary-complement, the inverse of German order.
Most Dutch auxiliaries can occur in both orders, but
this may be restricted according to their verb form.
Four groups of auxiliary verbs can be distinguished
that have different syntactic restrictions.

1. Verbs selecting for participles which may ap-
pear on either side of their complement (e.g.
hebben (“have”), zijn (“be”)).

2. Verbs selecting for participles which prefer to
follow their complement and must do so if they
are in participle form themselves (e.g. blijven
(“remain”), krijgen (“get”)).

3. Modals selecting for infinitives which prefer to
precede their complement and must do so if
they appear in infinitive form themselves.1068



VF LB MF RB
De man zou haar kunnen hebben gezien
the man would her.acc can have seen
De man zou haar gezien kunnen hebben
%De man zou haar kunnen gezien hebben
The man should have been able to see her

Table 2: Variations of Dutch auxiliary order

4. Verbs selecting for “to infinitives” which must
precede their complement.

While there is some variation among speakers,
the generalizations above are robust. The permitted
variations assuming a verb of the 3rd and 1st cate-
gory in the right bracket are presented in Table 2.2

The variant %De man zou haar kunnen gezien
hebben is typical of speakers from Belgium (Hae-
seryn, 1997); speakers from the Netherlands tend to
regard such structures as ungrammatical. Our sys-
tem can both generate a Flemish grammar accepting
all of the above and a (Northern) Dutch grammar,
rejecting the third variant.

4 Alternative auxiliary approaches

This section presents the alternative analyses for
auxiliary-verb structures in Germanic languages
compared in this study. For reasons of space, I limit
my description to an explanation of the differences
and relevance of the compared analyses.3

4.1 Argument-composition

The standard analysis for German and Dutch
auxiliaries in HPSG is a so-called “argument-
composition” analysis (Hinrichs and Nakazawa,
1994), which I will explain through the following
Dutch example:4

(1) Ik
I

zou
would

het
the

boek
book

willen
want

lezen.
read.

“I would like to read the book.”

In the sentence above, the auxiliary willen “want”
separates the verb lezen “read” from its object het

2Note that the same orders as in the Right Brackets may also
occur in the Vorfeld (with or without the object).

3Details of the implementations can be found by using the
metagrammar, which can be found on my homepage.

4Hinrichs and Nakazawa (1994) present an analysis for the
German auxiliary flip. The relevant observations are the same.

2

6

6

6

6

4

VAL

2

6

6

6

6

4

SUBJ 1

COMPS

*

2

6

4

HEAD verb

VAL

"

SUBJ 1

COMPS 2

#

3

7

5
, 2

+

3

7

7

7

7

5

3

7

7

7

7

5

Figure 1: Standard Auxiliary Subcategorization

boek “the book”. A parser respecting surface order
can thus not combine lezen and het boek before com-
bining willen and lezen.

The argument-composition analysis was intro-
duced to make sure that het boek can be picked up
as the object of the embedded verb lezen. The sub-
categorization of an auxiliary under this analysis is
presented in Figure 1. The subject of the auxiliary
is identical to the subject of the auxiliary’s com-
plement. Its complement list consists of the con-
catenation of the verbal complement and any com-
plement this verbal complement may select for. In
the sentence above, willen will add the subject and
the object of lezen to its own subcatorization lists.5

This standard solution for auxiliary-verb structures
is (with minor differences) also what is provided by
the Matrix customization system.

Argument-composition can capture the grammat-
ical behavior of auxiliaries in German and Dutch.
However, grammaticality and coverage is not all
that matters for grammars of natural language. Ef-
ficiency remains an important factor, and argument-
composition has some undesirable properties on this
level. The problem lies in the fact that lexical en-
tries of auxiliaries have underspecified elements on
their subcategorization lists. With the current chart
parsing and chart generation algorithms (Carroll and
Oepen, 2005), an auxiliary in a language with flex-
ible word order will speculatively add edges to the
chart for potential analyses with the adjacent con-
stituent as subject or complement. Because the
length of the lists are underspecified as well, it can
continue wrongly combining with all elements in the
string. In the worse case scenario, the number of
edges created by an auxiliary grows exponentially in
the number of words and constituents in the string.
The efficiency problem is even worse for generation:
while the parser is restricted by the surface order of

5In the semantic representation, both arguments will be di-
rectly related to the main verb exclusively.1069



`

i
´

2

4VAL

"

SUBJ 〈〉

COMPS
D

ˆ

HEAD verb
˜

E

#

3

5

`

ii
´

2

6

6

6

6

6

6

4

VAL

"

SUBJ 1

COMPS 2

#

HEAD-DTR|VAL| COMPS 3

NON-HEAD-DTR 3

"

VAL

"

SUBJ 1

COMPS 2

##

3

7

7

7

7

7

7

5

Figure 2: Auxiliary lexical type (i) and Auxiliary+verb
construction (ii) under alternative analysis

the string, the generator will attempt to combine all
lexical items suggested by the input semantics, as
well as lexical items with empty semantics, in ran-
dom order.

4.2 Aux+verb construction

Bender (Bender, 2010)6 presents an alternative ap-
proach to auxiliary-verb structures for the Australian
language Wambaya. The analysis introduces auxil-
iaries that only subcategorize for one verbal com-
plement, not raising any of the complement’s ar-
guments or its subject. Auxiliaries combine with
their complement using a special auxiliary+verb
rule. Figure 2 presents this alternative solution. In
principle, the new analysis uses the same technique
as argument composition. The difference is that the
auxiliary now starts out with only one element in its
subcategorization lists and can only combine with
potential verbal complements that are appropriately
constrained. The structure that combines the auxil-
iary with its complement places the remaining ele-
ments on the complement’s SUBJ and COMPS lists
on the respective lists of the newly formed phrase,
as can be seen in Figure 2 (ii). The constraints on
raised arguments are known when the construction
applies. The efficiency problem sketched above is
thus avoided.

4.3 A small wrinkle: partial VP fronting

In its basic form, the auxiliary+verb structure cannot
handle partial VP fronting where the main verb is
placed in first position leaving one or more verbal

6Bender credits the key idea behind this analysis to Dan
Flickinger (Bender, 2010).

forms in the verbal cluster, as illustrated in (2) for
Dutch:

(2) Gezien
Seen

zou
should

de
the

man
man

haar
her

kunnen
can

hebben.
have

“The man should have been able to see her.”

The problem is that hebben “have” cannot com-
bine with gezien “seen”, because they are sepa-
rated by the head of the clause. Because the verb
hebben cannot combine with its complement, it can-
not raise its complement’s arguments either: the
auxiliary+verb analysis only permits raising when
auxiliary and complement combine.

This shortcoming is no reason to immediately dis-
miss the proposal. Structures such as (2) are ex-
tremely rare. The difference in coverage of a parser
that can and a parser that cannot handle such struc-
tures is likely to be tiny, if present at all, nor is it
vital for a sentence generator to be able to produce
them. However, a correct grammar should be able to
analyze and produce all grammatical structures.

I implemented an additional version of the aux-
iliary+verb construction using two rather complex
rules that capture examples such as (2). Because
the structure in (2) also presented difficulties for
the argument-composition analysis in Dutch, I tested
both of the analyses with and without the inclusion
of these structures. In the ideal case, the full cov-
erage version will remain efficient enough as the
grammar grows. But if this turns out not to be the
case, the decision can be made to exclude the ad-
ditional rule from the grammar or to use it as a ro-
bustness rule that is only called when regular rules
fail. Given the metagrammar engineering approach,
it will be straightforward to decide at a later point to
exclude the special rule, if corpus studies reveal this
is favourable.

5 Grammars and evaluation

5.1 Experimental set-up

As described above, the Germanic metagrammar is
a branch of the customization system. As such, it
takes a choices file as input to create a grammar. The
basic choices files for Dutch and German were cre-
ated through the LinGO Grammar Matrix web inter-1070



Complete Set Reduced Set
Positive Total Positive Total Av.

s s s s w/s
Du 177 14654 138 14591 6.61
Fl 195 14654 156 14606 6.61
Ge 116 6926 84 6914 6.65

Table 3: Number of test examples (s) used in evaluation
and average words per sentence (w/s)

face.7 The choices files defined artificial grammars
with a dummy vocabulary. The system can produce
real fragments of the languages, but strings repre-
senting syntactic properties through dummy vocab-
ulary were used to give better control over ambiguity
facilitating the evaluation of coverage and overgen-
eration of the grammars. The grammars have a lexi-
con of 9-10 unambiguous dummy words.

The created choices files were extended offline to
define those properties that the Germanic metagram-
mar captures, but are not incorporated in the Matrix
customization system. This included word order of
the auxiliary and complement, fixed or free argu-
ment order, influence of inflection on word order,
a more elaborate case hierarchy, ditransitive verbs,
and the choice of auxiliary/verb analysis. Four
choices files with different combinations of analy-
ses were created for each language, resulting in 12
choices files in total.

A basic test suite was developed that covers in-
transitive, transitive and ditransitive main clauses
with up to three auxiliaries. The German set was
based on a description provided by Kathol (2000),
Dutch and Flemish were based on Haeseryn (1997).
For each verb and auxiliary combination, all permis-
sible word orders were defined based on descriptive
resources. In order to make sure the grammars do
not reveal unexpected forms of overgeneration, all
possible ungrammatical orders were automatically
generated. Table 3 provides the sizes of the test
suites. Each language has both a complete set for
the 6 grammars that provide full coverage, and a re-
duced set for the 6 grammars that can not handle
split verbal clusters (see Section 4.3 for the motiva-
tion to test grammars that do not have full coverage).

7http://www.delph-in.net/matrix/
customize/

Each grammar was created using the metagram-
mar, ensuring that all components except the com-
peting analyses were held constant among compared
grammars. The [incr tsdb()] competence and per-
formance profiling environment (Oepen, 2001) was
used in combination with the LKB to evaluate pars-
ing performance of the individual grammars on the
test suites. For each grammar, the number of re-
quired parsing tasks, memory (space) and CPU time
per sentence, as well as the number of passive edges
created during an average parse were compared.
Performance on language generation was evaluated
using the LKB.

5.2 Parsing results

Table 4 presents the results from the parsing ex-
periment. Note that all directly compared gram-
mars have the same empirical coverage (100% cov-
erage and 0% overgeneration on the phenomena in-
cluded in the test suites). The comparison there-
fore addresses the effect on efficiency of the al-
ternative analyses. Three tests per grammar were
carried out: one on positive data, one on nega-
tive data and one on the complete dataset. Re-
sults were similar for all three sets, with slightly
larger differences in efficiency for negative exam-
ples. For reasons of space, only the results on pos-
itive examples are presented, which are more rele-
vant for most applications involving parsing. The
results show that the auxiliary+verb (aux+v) leads to
a more efficient grammar according to all measures
used. There is an average reduction of 73.2% in per-
formed tasks, 56.3% in produced passive edges and
32.9% in memory when parsing grammatical exam-
ples using the auxiliary+verb structure compared to
argument-composition. CPU-time per sentence also
improved significantly, but, due to the short average
sentence length (5-10 words) the value is too small
for exact comparison with [incr tsdb()].

5.3 Sentence generation evaluation

The complete coverage versions of Dutch and Ger-
man were used to create the exhaustive set of sen-
tences with an intransitive, transitive and ditransitive
verb combined with none, one or two auxiliaries but
rapidly loses ground when one or more auxiliaries8

8All auxiliaries in the grammars contribute an ep.1071



Average Performed Tasks
Compl. Cov. Gram. No Split Cl. Gram.
arg-comp aux+v arg-comp aux+v

Du 524 149 480 134
Fl 529 150 483 137
Ge 684 148 486 136

Average Created Edges
Compl. Cov. Gram. No Split Cl. Gram.
arg-comp aux+v arg-comp aux+v

Du 58 25 52 25
Fl 58 26 52 25
Ge 67 23 52 24

Average Memory Use (kb)
Compl. Cov. Gram. No Split Cl. Gram.
arg-comp aux+v arg-comp aux+v

Du 9691 6692 8944 6455
Fl 9716 6717 8989 6504
Ge 10289 5675 8315 5468

Average CPU Time (s)
Compl. Cov. Gram. No Split Cl. Gram.
arg-comp aux+v arg-comp aux+v

Du 0.04 0.02 0.03 0.01
Fl 0.04 0.02 0.03 0.01
Ge 0.06 0.01 0.04 0.01

Table 4: Parsing results positive examples

from a total of 18 MRSs. The input MRSs were ob-
tained by parsing a sentence with canonical word or-
der. Both versions provide the same set of sentences
as output, confirming their identical empirical cover-
age. Table 5 presents the number of edges required
by the generator to produce the full set of generated
sentences from a given MRS. The cells with no num-
ber represent conditions under which the LKB gen-
erator reaches the maximum limit of edges, set at
40,000, without completing its exhaustive search.

The grammar using argument-composition is
slightly more efficient when there are no aux-
iliaries, are added, in particular when sentence
length increases: For ditransitive verbs (dv), the
Dutch argument-composition grammar maxes out
the 40,000 edge limit with two auxiliaries, whereas
the auxiliary+verb grammar creates 910 edges, a
manageable number. Due to the more liberal order
of arguments, results are even worse for German:
the argument-composition grammar reaches its limit
with the first auxiliary for ditransitive verbs. These
results indicate that the auxiliary+verb analysis is

Required edges
Du No Aux 1 Aux 2 Aux

arg-c aux+v arg-c aux+v arg-c aux+v
iv 54 57 221 99 792 248
tv 124 141 1311 211 7455 500
dv 212 230 14968 378 – 910
Ge No Aux 1 Aux 2 Aux

arg-c aux+v arg-c aux+v arg-c aux+v
iv 54 57 295 84 1082 165
tv 130 142 4001 212 18473 422
dv 306 351 – 608 – 1379

Table 5: Performance on Sentence Generation

strongly preferable where natural language genera-
tion is concerned.

5.4 In summary

The results of the experiment presented above show
that avoiding underspecified subcategorization lists,
as found in the standard argument-composition anal-
ysis, significantly increases the efficiency of the
grammar for both parsing and generation. On av-
erage, they show a reduction of 73.2% in performed
tasks, 56.3% in produced passive edges and 32.9%
in memory for parsing. In generation experiments,
results are even more impressive: the reduction of
edges for German sentences with one auxiliary and
a ditransitve verb is at least 98.5%. These results
show that the auxiliary+verb alternative should be
considered seriously as an alternative to the HPSG
standard analysis of argument-composition, though
further investigation in a larger context is needed be-
fore final conclusions can be drawn.

Future work will focus on increasing the cover-
age of the grammars, as well as the number of al-
ternative options explored. In particular, both ap-
proaches for auxiliaries should be compared us-
ing alternative analyses for verb-second word order
found in other HPSG-based grammars, such as the
GG (Müller and Kasper, 2000; Crysmann, 2005),
Grammix (Müller, 2009; Müller, 2008) and Cheetah
(Cramer and Zhang, 2009) for German, and Alpino
(Bouma et al., 2001) for Dutch. These grammars
may use approaches that somewhat reduce the prob-
lem of argument-composition, leading to less sig-
nificant differences between the auxiliary+verb and
argument-composition analyses. On the other hand,
planned extensions that cover modification and sub-1072



ordinate clauses will increase local ambiguities. The
advantage of the auxiliary+verb analysis is likely to
become more important as a result.

In addition to providing a clearer picture of aux-
iliary structures, these extensions will also lead to
a better insight into efforts involved in using gram-
mar generation to explore alternative versions of a
grammar over time. In particular, it should pro-
vide an indication of the feasibility of maintaining
a higher number of competing analyses as the gram-
mar grows. After providing background on related
metagrammar projects and their goals, I will elabo-
rate on the importance of systematic exploration of
grammars in the discussion.

6 Related work

Metagrammars (or grammar generators) have been
established in the field for over a decade. This sec-
tion provides an overview of the goals and set-up of
some of the most notable projects.

The MetaGrammar project (Candito, 1998; de la
Clergerie, 2005; Kinyon et al., 2006) started as
an effort to encode syntactic knowledge in an ab-
stract class hierarchy. The hierarchy can contain
cross-linguistically invariable properties and syntac-
tic properties that hold across frameworks (Kinyon
et al., 2006). The factorized descriptions of Meta-
Grammar support Tree-Adjoining Grammars (Joshi
et al., 1975, TAG) as well as Lexical Functional
Grammars (Bresnan, 2001, LFG). The eXtensible
MetaGrammar (Crabbé, 2005, XMG) defines its
MetaGrammar as classes that are part of a multiple
inheritance hierarchy. Kinyon et al. (Kinyon et al.,
2006) use XMG to perform a cross-linguistic com-
parison of verb-second structures. Their study fo-
cuses on code-sharing between the languages, but
does not address the problem of competing analyses
investigated in this paper.

The GF Resource Grammar Library (Ranta, 2009)
is a multi-lingual linguistic resource that contains a
set of syntactic analyses implemented in GF (Gram-
matical Framework). The purpose of the library is
to allow engineers working on NLP applications to
write simple grammar rules that can call more com-
plex syntactic implementations from the grammar li-
brary. The grammar library is written by researchers
with linguistic expertise. It makes extensive use of

code sharing: general categories and constructions
that are used by all languages are implemented in
a core syntax grammar. Each language9 has its own
lexicon and morphology, as well as a set of language
specific syntactic structures. Code sharing also takes
place between the subset of languages explored, in
particular by means of common modules for Ro-
mance languages and for Scandanavian languages.

PAWS creates PC-PATR (McConnel, 1995) gram-
mars based on field linguists’ input. The main
purpose of PAWS lies in descriptive grammar writ-
ing and “computer-assisted related language adap-
tation”, where the grammar is used to map words
from a text in a source language to a target language.
PAWS differs from the other projects discussed here,
because grammar engineering or syntactic research
are not the main focus of the project.

The LinGO Grammar Matrix, described in Sec-
tion 2.1, is most closely related to the work pre-
sented in this paper. Like the other projects reviewed
here, the Grammar Matrix does not offer alterna-
tive analyses for the same phenomenon. Moreover,
starter grammars created by the Grammar Matrix are
developed manually and individually after their cre-
ation. The approach taken in this paper differs from
the original goal of the Grammar Matrix in that it
continues the development of new grammars within
the system, introducing a novel application for meta-
grammars. By using a metagrammar to store alter-
native analyses, grammars can be explored system-
atically over time. As such, the paper introduces a
novel methodology for grammar engineering. The
discussion and conclusion will elaborate on the ad-
vantages of the approach.

7 Discussion and conclusion

7.1 The challenge of choosing the right analysis

As mentioned in the introduction, most phenomena
in natural languages can be accounted for by more
than one formal analysis. An engineer may imple-
ment alternative solutions and test the impact on the
grammar concerning interaction with other phenom-
ena (Bierwisch, 1963; Müller, 1999; Bender, 2008;
Bender et al., 2011) and efficiency to decide between
analyses.

9Ranta (Ranta, 2009) reports that GF is developed for four-
teen languages, and more are under development.1073



However, it is not feasible to carry out compara-
tive tests by manually creating different versions of a
grammar every time a decision about an implemen-
tation is made. Moreover, even if such a study were
carried out at each stage, only the interaction with
the current state of the grammar would be tested.
This has two undesirable consequences. First, op-
tions may be rejected that would have worked per-
fectly well if different decisions had been made in
the past. Second, because each decision is only
based on the current state of the grammar, the result-
ing grammar is partially (or even largely) a product
of the order in which phenomena are treated.10

For grammar engineers with practical applica-
tions in mind, this is undesirable because the re-
sulting grammar may end up far from optimal. For
grammar writers that use engineering to find valid
linguistic analyses, the problem is even more seri-
ous: if there is a truth in a declarative grammar,
surely, this should not depend on the order in which
phenomena are treated.

7.2 Metagrammar engineering

This paper proposes to systematically explore anal-
yses throughout the development of a grammar by
writing a metagrammar (or grammar generator),
rather than directly implementing the grammar. A
metagrammar can contain several different analyses
for the same phenomenon. After adding a new phe-
nomenon to the metagrammar, the engineer can au-
tomatically generate versions of the grammar con-
taining different combinations of previous analyses.
As a result, the engineer can not only systematically
explore how alternative analyses interact with the
current grammar, but also continue to explore inter-
actions with phenomena added in the future. Espe-
cially for alternative approaches to basic properties
of the language, such as the auxiliary-verb structures
examined in this study, parallel analyses may pre-
vent the cumbersome scenario of changing a deeply
embedded property of a large grammar.

An additional advantage is that the engineer can
use the methodology to make different versions of
the grammar depending on its intended application.

10It is, of course, possible to go back and change old anal-
yses based on new evidence. In practice, the large effort in-
volved will only be undertaken if the advantages are apparent
beforehand.

For instance, it is possible to develop a highly re-
stricted version for grammar checking that provides
detailed feedback on detected errors (Bender et al.,
2004), next to a version with fewer constraints to
parse open text.

As far as finding optimal solutions is concerned,
it must be noted that this approach does not guar-
antee a perfect result, partially because there is no
guarantee the grammar engineer will think of the
perfect solution for each phenomenon, but mainly
because it is not maintainable to implement all pos-
sible alternatives for each phenomenon and make
them interact correctly with all other variations in
the grammar. The grammar engineer still needs to
decide which alternatives are the most promising
and therefore the most important to implement and
maintain. The resulting grammar therefore partially
remains a result of the order in which phenomena
are implemented. Nevertheless, the grammar engi-
neer can keep and try out solutions in parallel for
a longer time, increasing the possibility of explor-
ing more alternative versions of the grammar. These
additional investigations allow for better informed
decisions to stop exploring certain analyses. In ad-
dition, by breaking up analyses into possible alter-
natives, chances are that the resulting metagrammar
will be more modular than a directly written gram-
mar would have been, which facilitates exploring al-
ternatives further.

In sum, even though metagrammar engineering
does not completely solve the challenge of complete
explorations of a grammar’s possibilities, it does fa-
cilitate this process so that finding optimal solutions
becomes more likely, leading to better supported
choices among alternatives and a more scientific ap-
proach to grammar development.

Acknowledgments.

The work described in this paper has been sup-
ported by the project TAKE (Technologies for Ad-
vanced Knowledge Extraction), funded under con-
tract 01IW08003 by the German Federal Ministry
of Education and Research. Emily M. Bender, Lau-
rie Poulson, Christoph Zwirello, Bart Cramer, Kim
Gerdes and three anonymous reviewers provided
valuable feedback that resulted in significant im-
provement of the paper. Naturally, all remaining er-
rors are my own.1074



References

Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sutcliffe,
editors, Proceedings of the Workshop on Grammar
Engineering and Evaluation at the 19th International
Conference on Computational Linguistics, pages 8–
14, Taipei, Taiwan.

Emily M. Bender, Dan Flickinger, Stephan Oepen, An-
nemarie Walsh, and Tim Baldwin. 2004. Arboretum:
Using a precision grammar for grammar checking in
call. In Proceedings of the InSTIL/ICAL Symposium:
NLP and Speech Technologies in Advance Language
Learning Systems, Venice, Italy.

Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language & Compu-
tation, 8(1):23–72.

Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2011. Grammar engineering and linguistic hypoth-
esis testing. In Emily M. Bender and Jennifer E.
Arnold, editors, Language from a Cognitive Perspec-
tive: Grammar, Usage and Processing, pages 5–29.
Stanford: CSLI Publications, Palo Alto, USA.

Emily M. Bender. 2008. Grammar engineering for
linguistic hypothesis testing. In Nicholas Gaylord,
Alexis Palmer, and Elias Ponvert, editors, Proceedings
of the Texas Linguistics Society X Conference: Compu-
tational Linguistics for Less-Studied Languages, pages
16–36, Stanford. CSLI Publications.

Emily M. Bender. 2010. Reweaving a grammar for
Wambaya: A case study in grammar engineering for
linguistic hypothesis testing. Linguistic Issues in Lan-
guage Technology, 3(3):1–34.

Manfred Bierwisch. 1963. Grammatik des deutschen
Verbs, volume II of Studia Grammatica. Akademie
Verlag.

Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide coverage computational analysis
of Dutch. In Computational Linguistics in the Nether-
lands CLIN 2000.

Joan Bresnan. 2001. Lexical Functional Syntax. Black-
well Publishers, Oxford.

Marie-Helene Candito. 1998. Building parallel LTAG
for French and Italian. In Proceedings of the 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, Volume 1, pages 211–
217, Montreal, Quebec, Canada. Association for Com-
putational Linguistics.

John Carroll and Stephan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In IJCNLP, Jeju Island. Springer-Verlag LNCS.

Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. an introduc-
tion. Journal of Research on Language and Computa-
tion, 3(2–3):281 – 332.

Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA.

Benoı̂t Crabbé. 2005. Représentation modulaire
et paramétrable de grammaires électroniques lexi-
calisées. Ph.D. thesis, Université de Paris 7.

Bart Cramer and Yi Zhang. 2009. Constructon of a
German HPSG grammar from a detailed treebank. In
Proceedings of the ACL 2009 Grammar Engineering
across Frameworks workshop, pages 37–45, Singa-
pore, Singapore.

Berthold Crysmann. 2005. Relative clause extraposition
in German: An efficient and portable implementation.
Research on Language and Computation, 3(1):61–82.

Éric Villemonte de la Clergerie. 2005. From metagram-
mars to factorized TAG/TIG parsers. In Proceedings
of IWPT’05, pages 190–191.

Erich Drach. 1937. Grundgedanken der Deutschen Sat-
zlehre. Diesterweg, Frankfurt am Main, Germany.

Oskar Erdmann. 1886. Grundzüge der deutschen Syntax
nach ihrer geschichtlichen Entwicklung dargestellt.
Erste Abteilung. Verlag der Cotta’schen Buchhand-
lung, Stuttgart, Germany.

Walter Haeseryn. 1997. De gebruikswaarde van de
ans voor tekstschrijvers, taaltrainers en taaladviseurs.
Tekst[blad], 3.

Erhard Hinrichs and Tsuneko Nakazawa. 1994. Lin-
earizing auxs in German verbal complexes. In John
Nerbonne, Klaus Netter, and Carl Pollard, editors,
German in HPSG. CSLI, Stanford, USA.

Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree adjunct grammars. Journal of Computer
and System Sciences, 10(1):136–163.

Andreas Kathol. 2000. Linear Syntax. Oxford Press.
Alexandra Kinyon, Owen Rambow, Tatjana Scheffler,

SinWon Yoon, and Aravind K. Joshi. 2006. The meta-
grammar goes multilingual: A cross-linguistic look at
the V2-phenomenon. In Proceedings of the Eighth In-
ternational Workshop on Tree Adjoining Grammar and
Related Formalisms, pages 17–24, Sydney, Australia.
Association for Computational Linguistics.

Stephen McConnel. 1995. PC-PATR reference manual.
Stefan Müller and Walter Kasper. 2000. HPSG analy-

sis for German. In Wolfgang Wahlster, editor, Verb-
mobil: Foundations of Speech-to-Speech translation,
pages 238 – 253, Berlin, Germany. Springer.1075



Stefan Müller. 1999. Deutsche Syntax deklarativ. Head-
Driven Phrase Structure Grammar für das Deutsche.
Max Niemeyer Verlag, Tübingen.

Stefan Müller. 2008. Depictive secondary predicates in
german and english. In Christoph Schroeder, Gerd
Hentschel, and Winfried Boeder, editors, Secondary
Predicates in Eastern European Languages and Be-
yond, number 16 in Studia Slavica Oldenburgensia,
pages 255–273, Oldenburg, Germany. BIS-Verlag.

Stefan Müller. 2009. On predication. In Stefan Müller,
editor, Proceedings of the 16th International Con-
ference on Head-Driven Phrase Structure Grammar,
Stanford, USA. CSLI Publications.

Stephan Oepen. 2001. [incr tsdb()] — competence
and performance laboratory. Technical report, DFKI,
Saarbrücken, Germany.

Aarne Ranta. 2009. The GF resource grammar library.
Linguistic Issues in Language Technology, 2(2).

1076


