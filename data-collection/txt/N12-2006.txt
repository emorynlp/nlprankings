










































Choosing an Evaluation Metric for Parser Design


Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 29–34,
Montréal, Canada, June 3-8, 2012. c©2012 Association for Computational Linguistics

Choosing an Evaluation Metric for Parser Design

Woodley Packard

sweaglesw@sweaglesw.org

Abstract

This paper seeks to quantitatively evaluate the
degree to which a number of popular met-
rics provide overlapping information to parser
designers. Two routine tasks are considered:
optimizing a machine learning regularization
parameter and selecting an optimal machine
learning feature set. The main result is that the
choice of evaluation metric used to optimize
these problems (with one exception among
popular metrics) has little effect on the solu-
tion to the optimization.

1 Introduction

The question of how best to evaluate the perfor-
mance of a parser has received considerable atten-
tion. Numerous metrics have been proposed, and
their relative merits have been debated. In this pa-
per, we seek to quantitatively evaluate the degree to
which a number of popular metrics provide overlap-
ping information for two concrete subtasks of the
parser design problem.

The motivation for this study was to confirm our
suspicion that parsing models that performed well
under one metric were likely to perform well un-
der other metrics, thereby validating the widespread
practice of using just a single metric when conduct-
ing research on improving parser performance. Our
results are cautiously optimistic on this front.1

We use the problem of selecting the best per-
former from a large space of varied but related parse

1Note that we are not suggesting that these metrics provide
redundant information for other uses, e.g. predicting utility for
any particular downstream task.

disambiguation models (“parsers” henceforth) as the
setting for our study. The parsers are all conditional
log-linear disambiguators with quadratic regulariza-
tion, coupled to the English Resource Grammar
(ERG) (Flickinger, 2000), a broad-coverage HPSG-
based hand-built grammar of English. Analyses
from the ERG consist of a syntax tree together with
an underspecified logical formula called an MRS
(Copestake et al., 2005).

The parsers differ from each other along two di-
mensions: the feature templates employed, and the
degree of regularization used. There are 57 differ-
ent sets of traditional and novel feature templates
collecting a variety of syntactic and semantic data
about candidate ERG analyses. For each set of fea-
ture templates, parsers were trained with 41 different
values for the quadratic regularization parameter, for
a total of 2337 different parsers.

The WeScience Treebank of about 9100 sentences
(Ytrestøl et al., 2009) was used both for training and
testing the parsers, with 10-fold cross validation.

We break down the problem of selecting the best
parser into two tasks. The first task is to identify
the optimal value for the regularization parameter
for each set of feature templates. The second task
is to compare the different sets of feature templates
to each other, considering only the optimal value of
the regularization parameter for each, and select the
overall best. We attack each task with each of 14
metrics, and discuss the results.

2 Prior Work

Comparisons of parser metrics have been under-
taken in the past. Carroll et al (1998) describe a

29



broad range of parser evaluation metrics, and com-
ment on their advantages and disadvantages, but do
not offer a quantitative comparison. A number of pa-
pers such as Clark and Curran (2007) have explored
the difficulty of parser comparison across different
underlying formalisms.

Crouch et al (2002) compare two variant
dependency-based metrics in some detail on a single
LFG-based parsing model, concluding that despite
some differences in the metrics’ strategies, they of-
fer similar views on the performance of their parser.

The literature specifically seeking to quantita-
tively compare a broad range of metrics across a
large array of parsers is small. Emms (2008) de-
scribes the tree-distance metric and compares the
rankings induced by several variants of that met-
ric and PARSEVAL on a collection of six statisti-
cal parsers, finding broad compatibility, but observ-
ing frequent disagreement about the relative ranks
of two parsers whose scores were only marginally
different.

3 Metrics

In our setup, the overall score a metric assigns to
a parser is the average of the scores awarded for
the parser’s analyses of each sentence in the tree-
bank (termed macro-averaging, in contrast to micro-
averaging which is also common). For sentences
where the parser selects several candidate analyses
as tied best analyses, the actual metric score used is
the average value of the metric applied to the differ-
ent tied best analyses. Fourteen metrics are consid-
ered:

• Exact Tree Match (ETM) (Toutanova et al.,
2005) - 100% if the returned tree is identical
to the gold tree, and 0% otherwise.

• Exact MRS Match (EMM) - 100% if the re-
turned MRS is equivalent to the gold MRS, and
0% otherwise.

• Average Crossing Brackets (AXB) - the num-
ber of brackets (constituents) in the returned
tree that overlap incompatibly with some
bracket in the gold tree. Sign-inverted for com-
parability to the other metrics.

• Zero Crossing Brackets (ZXB) - 100% if the
AXB score is 0, and 0% otherwise.

• Labeled PARSEVAL (LP) (Abney et al., 1991)
- the harmonic mean (F1) of the precision and
recall for comparing the set of labeled brack-
ets in the returned tree with the set of labeled
brackets in the gold tree. Labels are rule names.

• Unlabeled PARSEVAL (UP) - identical to LP,
except ignoring the labels on the brackets.

• Labeled Syntactic Dependencies (LSD) (Buch-
holz and Marsi, 2006) - the F1 for comparing
the sets of directed bilexical syntactic depen-
dencies extracted from the returned and gold
trees, labeled by the rule name that joins the
dependent to the dependee.

• Unlabeled Syntactic Dependencies (USD) -
identical to LSD, except ignoring the labels.

• Labeled Elementary Dependencies (LED) - the
F1 for comparing the sets of elementary depen-
dency triples (Oepen and Lønning, 2006) ex-
tracted from the returned and gold MRS. These
annotations are similar in spirit to those used in
the PARC 700 Dependency Bank (King et al.,
2003) and other semantic dependency evalua-
tion schemes.

• Unlabeled Elementary Dependencies (UED) -
identical to LED, except ignoring all labeling
information other than the input positions in-
volved.

• Leaf Ancestor (LA) (Sampson and Babarczy,
2003) - the average of the edit distances be-
tween the paths through the returned and gold
trees from root to each leaf.

• Lexeme Name Match (LNM) - the percentage
of input words parsed with the gold lexeme2.

• Part-of-Speech Match (POS) - the percentage
of input words parsed with the gold part of
speech.

• Node Count Match (NCM) - 100% if the gold
and returned trees have exactly the same num-
ber of nodes, and 0% otherwise.

2In the ERG, lexemes are detailed descriptions of the syn-
tactic and semantic properties of individual words. There can
be multiple candidate lexemes for each word with the same part
of speech.

30



 24

 26

 28

 30

 32

 34

 36

 38

 40

 42

 0.001  0.01  0.1  1  10  100  1000

E
x
a

c
t 

M
a

tc
h

 A
c
c
u

ra
c
y
 (

%
)

Regularization Variance Parameter

Regularized Performance of pcfg baseline

pcfg baseline

Figure 1: ETM for “pcfg baseline”

Note that the last three metrics are not commonly
used in parser evaluation, and we have no reason
to expect them to be particularly informative. They
were included for variety – in a sense serving as con-
trols, to see how informative a very unsophisticated
metric can be.

4 Optimizing the Regularization
Parameter

The first half of our problem is: given a set of fea-
ture templates T , determine the optimal value for the
regularization parameter λ. We interpret the word
“optimal” relative to each of our 14 metrics. This is
quite straightforward: to optimize relative to metric
µ, we simply evaluate µ(M(T, λ)) for each value of
λ, where M(T, λ) is a parser trained using feature
templates T and regularization parameter λ, and de-
clare the value of λ yielding the greatest value of
µ the winner. Figure 1 shows values of the ETM
as a function of the regularization parameter λ for
T = “pcfg baseline”3; as can easily be seen, the op-
timal value is approximately λ̂µ = 2.

We are interested in how λ̂µ varies with different
choices of µ. Figure 2 shows all 14 metrics as func-
tions of λ for the same T = “pcfg baseline.” The
actual scores from the metrics vary broadly, so the
vertical axes of the superimposed plots have been
rescaled to allow for easier comparison.

A priori we might expect the optimal λ̂µ to be

3Note that we are not actually considering a PCFG here; in-
stead we are looking at a conditional log-linear model whose
features are shaped like PCFG configurations.

-1.5

-1

-0.5

 0

 0.5

 1

 0.001  0.01  0.1  1  10  100  1000

Z
-S

c
o

re
s

Regularization

Z-Score Comparison of Metrics

Figure 2: Z-scores for all metrics for “pcfg baseline”

quite different for different µ, but this does not turn
out to be the case. The curves for all of the met-
rics peak in roughly the same place, with one no-
ticeable outlier (AXB). The actual peak4 regulariza-
tion parameters for the 14 metrics were all in the
range [1.8, 3.9] except for the outlier AXB, which
was 14.8.

Relative to the range under consideration, the op-
timal regularization parameters can be seen by in-
spection to depend very little on the metric. Near the
optima, the graphs are all quite flat, and we calcu-
lated that by choosing the optimal regularization pa-
rameter according to any of the metrics (with the ex-
ception of the outlier AXB), the maximum increase
in error rate visible through the other metrics was
1.6%. If we ignore LNM, POS and NCM (the non-
standard metrics we included for variety) in addition
to AXB, the maximum increase in error rate result-
ing from using an alternate metric to optimize the
regularization parameter drops to 0.41%.

“pcfg baseline” is just one of 57 sets of feature
templates. However, the situation is essentially the
same with each of the remaining 56. The average
maximum error rate increase observed across all of
the sets of feature templates when optimizing on any
metric (including AXB, LNM, POS and NCM) was
2.54%; on the worst single set of feature templates it
was 6.7%. Excluding AXB, the average maximum
error rate increase was 1.7%. Additionally exclud-

4Due to noisiness near the tops of the graphs, the reported
optimum regularization parameters are actually the averages of
the best 3 values. We attribute the noise to the limited size of
our corpus.

31



ing LNM, POS and NCM it was 0.81%.
Given the size of the evaluation corpus we are

using, the significance of an error rate increase of
0.81% is very marginal. We conclude that, at least
in circumstances similar to ours, the choice of met-
ric used to optimize regularization parameters is not
important, provided we avoid AXB and the variety
metrics LNM, POS and NCM.

5 Choosing a Set of Feature Templates

The second half of our problem is: given a col-
lection T of different sets of feature templates, se-
lect the optimal performer. Again, we interpret
the word “optimal” relative to each of our 14 met-
rics, and the selection is straightforward: given
a metric µ, we first form a set of parsers P =
{M(T, arg maxλ µ(M(T, λ))) : T ∈ T } and then
select arg maxp∈P µ(p). That is, we train parsers
using the µ-optimal regularization parameter for
each T ∈ T , and then select the µ-optimal parser
from that set.

In our experiments, all 14 of the metrics ranked
the same set of feature templates as best.

It is also interesting to inspect the order that each
metric imposes on P . There was some disagree-
ment between the metrics about this order. We com-
puted pairwise Spearman rank correlations coeffi-
cients5 for the different metrics. As with the task
of choosing a regularization parameter, the metrics
AXB, LNM, POS and NCM were outliers. The av-
erage pairwise Spearman rank correlation exclud-
ing these metrics was 0.859 and the minimum was
0.761.

An alternate method of quantifying the degree of
agreement is described below.

5.1 Epsila

Consider two metrics µ : P 7→ R and ρ : P 7→ R.
Assume for simplicity that for both µ and ρ, larger
values are better and 100 is perfect. If x, y ∈ P
then the error rate reduction from y to x under µ
is µ∗(x, y) = µ(x)−µ(y)100−µ(y) . Let �µ,ρ be the smallest
number such that ∀x, y ∈ P : µ∗(x, y) > �µ,ρ ⇒

5The Spearman rank correlation coefficient of two metrics
is defined as the Pearson correlation coefficient of the ranks the
metrics assign to the elements of P . It takes values between−1
and 1, with larger values indicating higher ranking agreement.

ρ∗(x, y) > 0. Informally, this says for all pairs of
parsers x and y, if x is at least �µ,ρ better than y when
evaluated under µ, then we are guaranteed that x is
at least a tiny bit better than y when evaluated under
ρ. For an unrestricted domain of parsers, we are not
guaranteed that such epsila exist or are small enough
to be interesting. However, since our P is finite, we
can find an � that will provide the required property
at least within P .

�µ,ρ serves as a measure of how similar µ and ρ
are: if �µ,ρ is small, then small improvements seen
under µ will be visible as improvements under ρ,
whereas if �µ,ρ is large, then small improvements
seen under µ may in fact be regressions when evalu-
ating with ρ.

We computed pairwise epsila for our 14 metrics.
A large portion of pairwise epsila were around 5%,
with some being considerably smaller or larger.

5.2 Clustering
In order to make sense of the idea that these ep-
sila provide a similarity measure, we applied Quality
Threshold clustering (Heyer et al., 1999) to discover
maximal clusters of metrics within which all pair-
wise epsila are smaller than a given threshold. Small
thresholds produce many small clusters, while larger
thresholds produce fewer, larger clusters.

At a 1% threshold, almost all of the metrics form
singleton clusters; that is, a 1% error rate reduction
on any given metric is generally not enough to guar-
antee that any other metrics will see any error reduc-
tion at all. The exceptions were that {ETM, EMM}
formed a cluster, and {UED, LED} formed a cluster.

Increasing the threshold to 3%, a new cluster
{USD, LSD} forms (indicating that a 3% error rate
reduction in USD always is visible as some level
of error rate reduction in LSD, and vice versa), and
ZXB joins the {ETM, EMM} cluster.

By the time we reach a 5% threshold, the major-
ity (7 out of 11) of the “standard” parser evaluation
metrics have merged into a single cluster, consisting
of {ETM, EMM, ZXB, LA, LSD, UED, LED}. The
PARSEVAL metrics form a cluster of their own {UP,
LP}.

Increasing the threshold even more to 10% causes
10 out of 11 “standard” evaluation metrics to cluster
together; the only holdout is AXB (average number
of crossing brackets), which does not join the cluster

32



-0.2

 0

 0.2

 0.4

 0.6

 0.8

 1

 1.2

 1.4

 1.6

 1.8

 2

 2  4  6  8  10  12  14

Z
-S

c
o

re
s

Metric

Z-Score Comparison of Feature Sets

Figure 3: Z-scores for all feature sets on the Y axis (one
line per feature set); different metrics on the X axis. The
“control” metrics and the outlier AXB are on the far right
end.

even at a 20% threshold.

5.3 Visualization

To qualitatively illustrate the degree of variation in
scores attributable to differences in metric as op-
posed to differences in feature sets, and the extent of
the metrics’ agreements in ranking the feature sets,
we plotted linearly rescaled scores from the metrics
(at their optimum regularization parameter value) in
two ways.

In Figure 3, the scores of each feature set are plot-
ted as a function of which metric is being used. To
the extent that the lines are horizontal, the metrics
provide identical information. To the extent that the
lines do not cross, the metrics agree about the rela-
tive ordering of the feature sets. Note that the three
control metrics and the outlier metric AXB are plot-
ted on the far right of the figure, and show signifi-
cantly more line crossings.

In Figure 4, the score from each metric is plot-
ted as a function of which feature set is being evalu-
ated, sorted in increasing order of the LP metric. As
can be seen, the increasing trend of the LP metric
is clearly mirrored in all the other metrics graphed,
although there is a degree of variability.

6 Conclusions

From both subtasks, we saw that the Average Cross-
ing Brackets metric (AXB) is a serious outlier. We
cannot say whether it provides complementary in-

-0.2

 0

 0.2

 0.4

 0.6

 0.8

 1

 1.2

 1.4

 1.6

 1.8

 2

 0  10  20  30  40  50  60

Z
-S

c
o

re
s

Feature Set

Z-Score Comparison of Metrics

Figure 4: Z-scores for all metrics except AXB, LNM,
POS and NCM on the Y axis (one line per metric); dif-
ferent feature sets on the X axis.

formation or actually misleading information; in-
deed, that might depend on the nature of the down-
stream application.

We can say with confidence that for the subtask of
optimizing a regularization parameter, there is very
little difference between the popular metrics {ETM,
EMM, ZXB, LA, LP, UP, LSD, USD, LED, UED}.

For the subtask of choosing the optimal set of fea-
ture templates, there was even greater agreement: all
14 metrics arrived at the same result. Although they
did not impose the exact same rankings, the rankings
were similar. It is interesting (and entertaining) that
even the three “control” metrics (LNM, POS and
NCM) selected the same optimal feature set. It is
particularly surprising that even the absurdly simple
NCM metric, which does nothing but check whether
two trees have the same number of nodes, irrespec-
tive of their structure or labels, when averaged over
thousands of items, can identify the best feature set.

Our findings agree with (Crouch et al., 2002)’s
suggestion that different metrics can offer similar
views on error rate reduction.

Clustering based on epsila at the 5% and 10%
thresholds showed interesting insights as well. We
demonstrated that a 5% error rate reduction as seen
on any of {ETM, EMM, ZXB, LA, LSD, UED,
LED} is also visible from the others (although the
popular PARSEVAL metrics were outliers at this
threshold). This has the encouraging implication
that a decision made on the basis of strong evidence
from just one metric is not likely to be contradicted

33



by evaluations by other metrics. However, we must
point out that the precise values of these thresholds
are dependent on our setup. They would likely be
larger if a significantly larger number of parsers or a
significantly more varied group of parsers were con-
sidered, and conversely would perhaps be smaller if
a larger evaluation corpus were used (reducing the
noise).

Our data only directly apply to the tasks of se-
lecting the value of the regularization parameter and
selecting feature templates for a conditional log-
likelihood model for parsing with the ERG. How-
ever, we expect the results to generalize at least to
similar tasks with other precision grammars, and
probably treebank-derived parsers as well. Explo-
ration of how well these results hold for other tasks
and for other types of parsers is an excellent subject
for future research.

References

S. Abney, D. Flickinger, C. Gdaniec, C. Grishman,
P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-
vans, M. Liberman, et al. 1991. Procedure for quan-
titatively comparing the syntactic coverage of English
grammars. In Proceedings of the workshop on Speech
and Natural Language, pages 306–311. Association
for Computational Linguistics.

Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149–164,
New York City, June. Association for Computational
Linguistics.

J. Carroll, T. Briscoe, and A. Sanfilippo. 1998. Parser
evaluation: a survey and a new proposal. In Proceed-
ings of the 1st International Conference on Language
Resources and Evaluation, pages 447–454.

S. Clark and J. Curran. 2007. Formalism-independent
parser evaluation with CCG and DepBank. In An-
nual Meeting-Association for Computational Linguis-
tics, volume 45, page 248.

A. Copestake, D. Flickinger, C. Pollard, and I.A. Sag.
2005. Minimal recursion semantics: An introduction.
Research on Language & Computation, 3(4):281–332.

R. Crouch, R.M. Kaplan, T.H. King, and S. Riezler.
2002. A comparison of evaluation metrics for a broad-
coverage stochastic parser. In Beyond PARSEVAL
workshop at 3rd Int. Conference on Language Re-
sources an Evaluation (LREC 2002).

Martin Emms. 2008. Tree distance and some other
variants of evalb. In Bente Maegaard Joseph Mari-
ani Jan Odjik Stelios Piperidis Daniel Tapias Nicoletta
Calzolari (Conference Chair), Khalid Choukri, edi-
tor, Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC’08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.

Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6(01):15–28.

L.J. Heyer, S. Kruglyak, and S. Yooseph. 1999. Explor-
ing expression data: identification and analysis of co-
expressed genes. Genome research, 9(11):1106.

T.H. King, R. Crouch, S. Riezler, M. Dalrymple, and
R. Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th Interna-
tional Workshop on Linguistically Interpreted Corpora
(LINC-03), pages 1–8.

S. Oepen and J.T. Lønning. 2006. Discriminant-based
MRS banking. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006).

G. Sampson and A. Babarczy. 2003. A test of the leaf-
ancestor metric for parse accuracy. Natural Language
Engineering, 9(04):365–380.

K. Toutanova, C.D. Manning, D. Flickinger, and
S. Oepen. 2005. Stochastic HPSG parse disambigua-
tion using the Redwoods corpus. Research on Lan-
guage & Computation, 3(1):83–105.

Gisle Ytrestøl, Dan Flickinger, and Stephan Oepen.
2009. Extracting and Annotating Wikipedia Sub-
Domains. Towards a New eScience Community Re-
source. In Proceedings of the Seventh Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries, Groningen, The Netherlands.

34


