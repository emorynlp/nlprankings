



















































Exploiting Machine-Transcribed Dialog Corpus to Improve Multiple Dialog States Tracking Methods


Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 189–196,
Seoul, South Korea, 5-6 July 2012. c©2012 Association for Computational Linguistics

Exploiting Machine-Transcribed Dialog Corpus to Improve Multiple Dialog
States Tracking Methods

Sungjin Lee1,2 and Maxine Eskenazi1
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania

2Computer Science and Engineering, Pohang University of Science and Technology, South Korea
{sungjin.lee, max}@cs.cmu.edu1, junion@postech.ac.kr2

Abstract

This paper proposes the use of unsuper-
vised approaches to improve components of
partition-based belief tracking systems. The
proposed method adopts a dynamic Bayesian
network to learn the user action model directly
from a machine-transcribed dialog corpus. It
also addresses confidence score calibration to
improve the observation model in a unsuper-
vised manner using dialog-level grounding in-
formation. To verify the effectiveness of the
proposed method, we applied it to the Let’s Go
domain (Raux et al., 2005). Overall system
performance for several comparative models
were measured. The results show that the pro-
posed method can learn an effective user ac-
tion model without human intervention. In
addition, the calibrated confidence score was
verified by demonstrating the positive influ-
ence on the user action model learning process
and on overall system performance.

1 Introduction

With present Automatic Speech Recognition (ASR)
and Spoken Language Understanding (SLU) errors,
it is impossible to directly observe the true user goal
and action. It is crucial, therefore, to efficiently infer
this true state from erroneous observations over mul-
tiple dialog turns. The Partially Observable Markov
Decision Process (POMDP) framework has offered
a well-founded theory for this purpose (Henderson
et al., 2008; Thomson and Young, 2010a; Williams
and Young, 2007; Young et al., 2010). Several
approximate methods have also emerged to tackle
the vast complexity of representing and maintaining

belief states, e.g., partition-based approaches (Ga-
sic and Young, 2011; Williams, 2010; Young et
al., 2010) and Bayesian network (BN)-based meth-
ods (Raux and Ma, 2011; Thomson and Young,
2010a). The partition-based approaches attempt to
group user goals into a small number of partitions
and split a partition only when a distinction is re-
quired by observations. This property endows it
with the high scalability that is suitable for fairly
complex domains. However, the parameter learn-
ing procedures for the partition-based methods is
still limited to hand-crafting or the use of a sim-
ple maximum likelihood estimation (Keizer et al.,
2008; Roy et al., 2000; Thomson and Young, 2010a;
Williams, 2008). In contrast, several unsupervised
methods which do not require human transcription
and annotation have been recently proposed to learn
BN-based models (Jurcicek et al., 2010; Syed and
Williams, 2008; Thomson et al., 2010b). In this pa-
per we describe an unsupervised process that can be
applied to the partition-based methods. We adopt a
dynamic Bayesian network to learn the user action
model which defines the likelihood of user actions
for a given context. In addition, we propose a simple
confidence score calibration method to improve the
observation model which represents the probability
of an observation given the true user action.

This paper is structured as follows. Section 2 de-
scribes previous research and the novelty of our ap-
proach. Section 3 and Section 4 elaborate on our
proposed unsupervised approach. Section 5 explains
the experimental setup. Section 6 presents and dis-
cusses the results. Finally, Section 7 concludes with
a brief summary and suggestions for future research.

189



2 Background and Related Work

In order to reduce the complexity of the belief states
over the POMDP states, the following factorization
of the belief state has been commonly applied to the
belief update procedure (Williams et al., 2005):

b(gt,ut,ht)

∝ p(ot|ut)︸ ︷︷ ︸
observation model

∑

ht−1

p(ht|ht−1,ut, st)︸ ︷︷ ︸
dialog history model

p(ut|gt, st,ht−1)︸ ︷︷ ︸
user action model

∑

gt−1

p(gt|gt−1, st−1)︸ ︷︷ ︸
user goal model∑

ut−1

b(gt−1,ut−1,ht−1)

(1)

where gt, st,ut,ht,ot represents the user goal, the
system action, the user action, the dialog history,
and the observed user action for each time slice, re-
spectively. The user goal model describes how the
user goal evolves. In the partition-based approaches,
this model is further approximated by assuming that
the user does not change their mind during the dia-
log (Young et al., 2010):

∑

gt−1

p(gt|gt−1, st−1) = p(pt|pt−1) (2)

where pt is a partition from the current turn. The di-
alog history model indicates how the dialog history
changes and can be set deterministically by simple
discourse rules, for example:

p(ht = Informed|ht−1,ut, st) ={
1 if ht−1 = Informed or ut = Inform(·),
0 otherwise.

(3)

The user action model defines how likely user ac-
tions are. By employing partitions, this can be ap-
proximated by the bigram model of system and user
action at the predicate level, and the matching func-
tion (Keizer et al., 2008):

p(ut|gt, st,ht−1)
∝ p(T (ut)|T (st)) · M(ut,pt, st)

(4)

where T (·) denotes the predicate of the action
and M(·) indicates whether or not the user action

matches the partition and system action. However,
it turned out that the bigram user action model did
not provide an additional gain over the improve-
ment achieved by the matching function according
to (Keizer et al., 2008). This might indicate that
it is necessary to incorporate more historical infor-
mation. To make use of historical information in
an unsupervised manner, the Expectation Maximiza-
tion algorithm was adopted to obtain maximum like-
lihood estimates (Syed and Williams, 2008). But
these methods still require a small amount of tran-
scribed data to learn the observation confusability,
and they suffer from overfitting as a general prop-
erty of maximum likelihood. To address this prob-
lem, we propose a Bayesian learning method, which
requires no transcribed data.

The observation model represents the probability
of an observation given the true user action. The
observation model is usually approximated with the
confidence score computed from the ASR and SLU
results:

p(ot|ut) ≈ p(ut|ot) (5)

It is therefore of vital importance that we obtain the
most accurate confidence score as possible. We pro-
pose an efficient method that can improve the confi-
dence score by calibrating it using grounding infor-
mation.

3 User Action Model

To learn the user action model, a dynamic Bayesian
network is adopted with several conditional inde-
pendence assumptions similar to Equation 1. This
gives rise to the graphical structure shown in Fig-
ure 1. As mentioned in Section 2, the user ac-
tion model deals with actions at the predicate level1.
This abstract-level handling enables the user action
model to employ exact inference algorithms such as
the junction tree algorithm (Lauritzen and Spiegel-
halter, 1988) for more efficient reasoning over the
graphical structure.

1To keep the notation uncluttered, we will omit T (·).

190



Figure 1: The graphical structure of the dynamic
Bayesian network for the user action model. The shaded
items are observable and the transparent ones are latent.

The joint distribution for this model is given by

p(S,H,U,O|Θ)
= p(h0|π)

∏

t

p(ut|st,ht−1,φ)

· p(ht|ht−1,ut,η)p(ot|ut, ζ)

(6)

where a capital letter stands for the set of
corresponding random variables, e.g., U =
{u1, . . . ,uN}, and Θ = {π,φ,η, ζ} denotes the
set of parameters governing the model2.

Unlike previous research which learns ζ using
maximum likelihood estimation, we use a determin-
istic function that yields a fraction of an observed
confidence score in accordance with the degree of
agreement between ut and ot:

p(ot|ut) = CS(ot) ·
( |ot ∩ ut|
|ot ∪ ut|

)
+ � (7)

where CS(·) returns the confidence score of the as-
sociated observation. As mentioned above, π and
η are deterministically set by simple discourse rules
(Equation 3). This only leaves the user action model
φ to be learned. In a Bayesian model, any unknown
parameter is given a prior distribution and is ab-
sorbed into the set of latent variables, thus it is not
feasible to directly evaluate the posterior distribution
of the latent variables and the expectations with re-
spect to this distribution. Therefore a determinis-
tic approximation, called mean field theory (Parisi,
1988), is applied.

In mean field theory, the family of posterior distri-
butions of the latent variables is assumed to be par-
titioned into disjoint groups:

q(Z) =

M∏

i=1

qi(Zi) (8)

2Here, a uniform prior distribution is assigned on S

where Z = {z1, . . . , zN} denotes all latent variables
including parameters and Zi is a disjoint group.
Amongst all distributions q(Z) having the form of
Equation 8, we then seek the member of this family
for which the divergence from the true posterior dis-
tribution is minimized. To achieve this, the follow-
ing optimization with respect to each of the qi(Zi)
factors is to be performed in turn (Bishop, 2006):

ln q∗j (Zj) = Ei 6=j
[
ln(X,Z)

]
+ const (9)

where X = {x1, . . . ,xN} denotes all observed vari-
ables and Ei 6=j means an expectation with respect to
the q distributions over all groups Zi for i 6= j.

Now we apply the mean field theory to the user
model. Before doing so, we need to introduce the
prior over the parameter φ which is a product of
Dirichlet distributions3.

p(φ) =
∏

k

Dir(φk|α0k)

=
∏

k

C(α0k)
∏

l

φ
α0k−1
k,l

(10)

where k represents the joint configuration of all of
the parents and C(α0k) is the normalization constant
for the Dirichlet distribution. Note that for symme-
try we have chosen the same parameter α0k for each
of the components.

Next we approximate the posterior distribution,
q(H,U,φ) using a factorized form, q(H,U)q(φ).
Then we first apply Equation 9 to find an expression
for the optimal factor q∗(φ):

3Note that priors over parameters for deterministic distribu-
tions (e.i., π,η,and ζ) are not necessary.

191



ln q∗(φ) = EH,U
[
ln p(S,H,U,O,Θ)

]
+ const

= EH,U

[∑

t

ln p(ut|st,ht−1,φ)
]

+ ln p(φ) + const

=
∑

t

∑

i,j,k

(
EH,U

[
δi,j,k

]
lnφi,j,k

)

+
∑

i,j,k

(αoi,j,k − 1) lnφi,j,k + const

=
∑

i,j,k

((
EH,U[ni,j,k] + (α

o
i,j,k − 1)

)

· lnφi,j,k
)
+ const

(11)

where δ(·, ·) denotes Kronecker delta and δi,j,k de-
notes δ(st, i)δ(ht−1, j) δ(ut, k). ni,j,k is the num-
ber of times where , st = i,ht−1 = j, and ut = k.
This leads to a product of Dirichlet distributions by
taking the exponential of both sides of the equation:

q∗(φ) =
∏

i,j

Dir(φi,j |αi,j),

αi,j,k = α
0
i,j,k + EH,U[ni,j,k]

(12)

To evaluate the quantity EH,U[ni,j,k], Equation 9
needs to be applied once again to obtain an op-
timal approximation of the posterior distribution
q∗(H,U).

ln q∗(H,U) = Eφ
[
ln p(S,H,U,O,Θ)

]
+ const

= Eφ

[∑

t

ln p(ut|st,ht−1,φ)

+ ln p(ht|ht−1,ut)

+ ln p(ot|ut)
]
+ const

=
∑

t

(
Eφ
[
ln p(ut|st,ht−1,φ)

]

+ ln p(ht|ht−1,ut)

+ ln p(ot|ut)
)
+ const

(13)

where Eφ
[
ln p(ut|st,ht−1,φ)

]
can be obtained us-

ing Equation 12 and properties of the Dirichlet dis-
tribution:

Eφ
[
ln p(ut|st,ht−1,φ)

]

=
∑

i,j,k

δi,j,kEφ
[
lnφi,j,k

]

=
∑

i,j,k

δi,j,k(ψ(αi,j,k)− ψ(α̂i,j))
(14)

where ψ(·) is the digamma function with α̂i,j =∑
k αi,j,k. Because computing EH,U[ni,j,k] is

equivalent to summing each of the marginal poste-
rior probabilities q∗(ht−1,ut) with the same con-
figuration of conditioning variables, this can be
done efficiently by using the junction tree algorithm.
Note that the expression on the right-hand side for
both q∗(φ) and q∗(H,U) depends on expectations
computed with respect to the other factors. We
will therefore seek a consistent solution by cycling
through the factors and replacing each in turn with a
revised estimate.

4 Confidence Score Calibration

As shown in Section 2, we can obtain a better obser-
vation model by improving confidence score accu-
racy. Since the confidence score is usually computed
using the ASR and SLU results, it can be enhanced
by adding dialog-level information. Basically, the
confidence score represents how likely it is that the
recognized input is correct. This means that a well-
calibrated confidence score should satisfy that prop-
erty such that:

p(ut = a|ot = a) '
∑

k δ(uk, a)δ(ok, a)∑
k δ(ok, a)

(15)

However, the empirical distribution on the right side
of this equation often does not well match the con-
fidence score measure on the left side. If a large
corpus with highly accurate annotation was used, a
straightforward remedy for this problem would be to
construct a mapping function from the given confi-
dence score measure to the empirical distribution.
This leads us to propose an unsupervised method
that estimates the empirical distribution and con-
structs the mapping function which is fast enough
to run in real time. Note that we will not construct

192



Figure 2: Illustrations of confidence score calibration for the representative concepts in the Let’s Go domain

a mapping function for each instance, but rather
for each concept, since the former could cause se-
vere data sparseness. In order to estimate the em-
pirical distribution in an unsupervised manner, we
exploit grounding information4 as true labels. We
first parse dialog logs to look for the grounding in-
formation that the users have provided. Each time
we encounter grounding information that includes
the constraints used in the backend queries, this is
added to the list. If two actions contradict each other,
the later action overwrites the earlier one. Then,
for each observation in the data, we determine its
correctness by comparing it with the grounding in-
formation. Next, we gather two sets of confidence
scores with respect to correctness, on which we ap-
ply a Gaussian kernel-based density estimation. Af-

4Specifically, we used explicitly confirmed information by
the system for this study

ter that, we scale the two estimated densities by their
total number of elements to see how the ratio of cor-
rect ones over the sum of correct and incorrect ones
varies according to the confidence score. The ratio
computed above will be the calibrated score:

c′ =
dc(c)

dc(c) + dinc(c)
(16)

where c′ indicates the calibrated confidence score
and c is the input confidence score. dc(·) denotes
the scaled density for the correct set and dinc(·) is
the scaled density for the incorrect set.

Note that this approach tends to yield a more
conservative confidence score since correct user ac-
tions can exist, even though they may not match
the grounding information. Finally, in order to effi-
ciently obtain the calibrated score for a given confi-
dence score, we employ the sparse Bayesian regres-
sion (Tipping, 2001) with the Gaussian kernel. By

193



virtue of the sparse representation, we only need to
consider a few so-called relevance vectors to com-
pute the score:

y(x) =
∑

xn∈RV
wnk(x,xn) + b (17)

where RV denotes the set of relevance vectors,
|RV | � |{xn}|. k(·, ·) represents a kernel function
and b is a bias parameter. Figure 2 shows the afore-
mentioned process for several representative con-
cepts in the Let’s Go domain.

5 Experimental Setup

To verify the proposed method, three months of data
from the Let’s Go domain were used to train the
user action model and the observation model. The
training data consists of 2,718 dialogs and 23,044
turns in total. To evaluate the user action model,
we compared overall system performance with three
different configurations: 1) the uniform distribution,
2) the user action model without historical infor-
mation5 which is comparable to the bigram model
of (Keizer et al., 2008), 3) the user action model with
historical information included. For system perfor-
mance evaluation, we used a user simulator (Lee and
Eskenazi, 2012) which provides a large number of
dialogs with statistically similar conditions. Also,
the simulated user enables us to examine how per-
formance changes over a variety of error levels. This
simulated user supports four error levels and each
model was evaluated by generating 2,000 dialogs at
each error level. System performance was measured
in terms of average dialog success rate. A dialog is
considered to be successful if the system provides
the bus schedule information that satisfies the user
goal.

To measure the effectiveness of the calibration
method, we conducted two experiments. First, we
applied the calibration method to parameter learn-
ing for the user action model by using the calibrated
confidence score in Equation 7. We compared the
log-likelihood of two models, one with calibration
and the other without calibration. Second, we com-
pared overall system performance with four differ-
ent settings: 1) the user action model with histori-

5This model was constructed by marginalizing out the his-
torical variables.

cal information and the observation model with cal-
ibration, 2) the user action model with historical in-
formation and the observation model without cali-
bration, 3) the user action model without historical
information and the observation model with calibra-
tion, 4) the user action model without historical in-
formation and the observation model without cali-
bration.

6 Results

The effect of parameter learning of the user action
model on average dialog success rate is shown in
Figure 3. While, in the previous study, the bigram
model unexpectedly did not show a significant ef-
fect, our result here indicates that our comparable
model, i.e. the model with historical information ex-
cluded, significantly outperformed the baseline uni-
form model. The difference could be attributed to
the fact that the previous study did not take tran-
scription errors into consideration, whereas our ap-
proach handles the problem by treating the true user
action as hidden. However, we cannot directly com-
pare this result with the previous study since the tar-
get domains are different. The model with historical
information included also consistently surpassed the
uniform model. Interestingly, there is a noticeable
trend: the model without historical information per-
forms better as the error level increases. This result
may indicate that the simpler model is more robust

Figure 3: The effect of parameter learning of each user
action model on overall system performance. The error
bar represents standard error.

194



Figure 4: The effect of confidence score calibration on
the log-likelihood of the user action model during the
training process.

Figure 5: The effect of confidence score calibration for
the observation model on overall system performance.
The error bar shows standard error.

to error. Although average dialog success rates be-
came almost zero at error level four, this result is a
natural consequence of the fact that the majority of
the dialogs in this corpus are failed dialogs.

Figure 4 shows the effect of confidence score
calibration on the log-likelihood of the user action
model during the training process. To take into ac-
count the fact that different confidence scores result
in different log-likelihoods regardless of the qual-
ity of the confidence score, we shifted both log-
likelihoods to zero at the beginning. This modifica-

tion more clearly shows how the quality of the confi-
dence score influences the log-likelihood maximiza-
tion process. The result shows that the calibrated
confidence score gives greater log-likelihood gains,
which implies that the user action model can better
describe the distribution of the data.

The effect of confidence score calibration for the
observation model on average dialog success rate is
presented in Figure 5. For both the user action model
with historical information included and excluded,
the application of the confidence score calibration
consistently improved overall system performance.
This result implies the possibility of automatically
improving confidence scores in a modularized man-
ner without introducing a dependence on the under-
lying methods of ASR and SLU.

7 Conclusion

In this paper, we have presented novel unsupervised
approaches for learning the user action model and
improving the observation model that constitute the
partition-based belief tracking method. Our pro-
posed method can learn a user action model directly
from a machine-transcribed spoken dialog corpus.
The enhanced system performance shows the effec-
tiveness of the learned model in spite of the lack of
human intervention. Also, we have addressed con-
fidence score calibration in a unsupervised fashion
using dialog-level grounding information. The pro-
posed method was verified by showing the positive
influence on the user action model learning process
and the overall system performance evaluation. This
method may take us a step closer to being able to
automatically update our models while the system is
live. Although the proposed method does not deal
with N-best ASR results, the extension to support
N-best results will be one of our future directions,
as soon as the Let’s Go system uses N-best ASR re-
sults.

Acknowledgments

This work was supported by the second Brain Korea
21 project.

References
C. Bishop, 2006. Pattern Recognition and Machine

Learning. Springer.

195



M. Gasic and S. Young, 2011. Effective handling
of dialogue state in the hidden information state
POMDP-based dialogue manager. ACM Transactions
on Speech and Language Processing, 7(3).

J. Henderson, O. Lemon, K. Georgila, 2008. Hybrid Re-
inforcement / Supervised Learning of Dialogue Poli-
cies from Fixed Datasets. Computational Linguistics,
34(4):487-511.

F. Jurcicek, B. Thomson and S. Young, 2011. Natu-
ral Actor and Belief Critic: Reinforcement algorithm
for learning parameters of dialogue systems modelled
as POMDPs. ACM Transactions on Speech and Lan-
guage Processing, 7(3).

S. Keizer, M. Gasic, F. Mairesse, B. Thomson, K. Yu, S.
Young, 2008. Modelling User Behaviour in the HIS-
POMDP Dialogue Manager. In Proceedings of SLT.

S. Lauritzen and D. J. Spiegelhalter, 1988. Local Com-
putation and Probabilities on Graphical Structures and
their Applications to Expert Systems. Journal of
Royal Statistical Society, 50(2):157–224.

S. Lee and M. Eskenazi, 2012. An Unsuper-
vised Approach to User Simulation: toward Self-
Improving Dialog Systems. In Proceedings of SIG-
DIAL. http://infinitive.lti.cs.cmu.edu:9090.

G. Parisi, 1988. Statistical Field Theory. Addison-
Wesley.

A. Raux, B. Langner, D. Bohus, A. W Black, and M.
Eskenazi, 2005. Let’s Go Public! Taking a Spoken
Dialog System to the Real World. In Proceedings of
Interspeech.

A. Raux and Y. Ma, 2011. Efficient Probabilistic Track-
ing of User Goal and Dialog History for Spoken Dia-
log Systems. In Proceedings of Interspeech.

N. Roy, J. Pineau, and S. Thrun, 2000. Spoken dia-
logue management using probabilistic reasoning. In
Proceedings of ACL.

U. Syed and J. Williams, 2008. Using automatically
transcribed dialogs to learn user models in a spoken
dialog system. In Proceedings of ACL.

B. Thomson and S. Young, 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech & Language,
24(4):562-588.

B. Thomson, F. Jurccek, M. Gasic, S. Keizer, F. Mairesse,
K. Yu, S. Young, 2010. Parameter learning for
POMDP spoken dialogue models. In Proceedings of
SLT.

M. Tipping, 2001. Sparse Bayesian Learning and
the Relevance Vector Machine. Journal of Machine
Learning Research, 1:211–244.

J. Williams, P. Poupart, and S. Young, 2005. Factored
Partially Observable Markov Decision Processes for
Dialogue Management. In Proceedings of Knowledge
and Reasoning in Practical Dialogue Systems.

J. Williams and S. Young, 2007. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393-422.

J. Williams, 2008. Exploiting the ASR N-best by track-
ing multiple dialog state hypotheses. In Proceedings
of Interspeech.

J. Williams, 2010. Incremental partition recombination
for efficient tracking of multiple dialog states. In Pro-
ceedings of ICASSP.

S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson and K. Yu, 2010. The Hidden
Information State Model: a practical framework for
POMDP-based spoken dialogue management. Com-
puter Speech and Language, 24(2):150–174.

196


