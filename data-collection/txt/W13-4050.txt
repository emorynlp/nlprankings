



















































Spoken Dialog Systems for Automated Survey Interviewing


Proceedings of the SIGDIAL 2013 Conference, pages 329–333,
Metz, France, 22-24 August 2013. c©2013 Association for Computational Linguistics

Spoken Dialog Systems for Automated Survey Interviewing 

Michael Johnston1, Patrick Ehlen2, Frederick G. Conrad3, Michael F. Schober4,  
Christopher Antoun3, Stefanie Fail4, Andrew Hupp3, Lucas Vickers4,  

Huiying Yan3, Chan Zhang3 
AT&T Labs Research, Florham Park, NJ, USA1, AT&T, San Francisco, CA, USA2 

Survey Research Center, University of Michigan, Ann Arbor, USA3 
The New School, New York, NY, USA4 

johnston@research.att.com, ehlen@research.att.com,  
fconrad@umich.edu, schober@newschool.edu,  

antoun@umich.edu, stefaniefail@gmail.com, ahupp@umich.edu,  
lucasvickers@gmail.com, yanhuier@umich.edu, chanzh@umich.edu 

Abstract 

We explore the plausibility of using automated 
spoken dialog systems (SDS) for administer-
ing survey interviews. Because the goals of a 
survey dialog system differ from more tradi-
tional information-seeking and transactional 
applications, different measures of task accu-
racy and success may be warranted. We report 
a large-scale experimental evaluation of an 
SDS that administered survey interviews with 
questions drawn from government and social 
scientific surveys. We compare two dialog 
confirmation strategies: (1) a traditional strate-
gy of explicit confirmation on low-confidence 
recognition; and (2) no confirmation. With ex-
plicit confirmation, the small percentage of re-
sidual errors had little to no impact on survey 
data measurement. Even without confirmation, 
while there are significantly more errors, im-
pact on the substantive conclusions of the sur-
vey is still very limited. 

1 Introduction 
Survey interviews play a critical role in the oper-
ation of government and commerce. Large-scale 
social scientific surveys provide key indicators of 
the success or failure of economic and social pol-
icies, driving critical policy and funding deci-
sions. Market research surveys are key in evalu-
ating products and services for business. 

Survey interviews are typically conducted ei-
ther via telephone or face-to-face by skilled hu-
man interviewers. But ongoing changes in com-
munication technology threaten the viability of 
these methods. As people migrate from landline 
telephony to mobile-only (Ehlen and Ehlen 
2007) and Voice-over-IP (Fuchs 2008) as prima-
ry modes of communication, they undermine the 
effectiveness of traditional survey sampling 
techniques that rely on random selection of num-

bers within a dial code. Telephone respondents 
were once reachable at a fixed geographic loca-
tion in a largely predictable conversational envi-
ronment. Now they are increasingly mobile, and 
more apt to prefer asynchronous communication. 
Thus it is imperative to understand how these 
changing behaviors affect survey results. 

The work described here is part of a larger re-
search project (see Schober et al. 2012; Conrad 
et al. 2013) that investigates the viability of four 
different modes for administering a survey inter-
view over a smartphone: automated voice, hu-
man voice, automated SMS text, and human 
SMS text. Here we focus specifically on the au-
tomated voice mode and explore the use of a 
spoken dialog system for survey administration. 

Spoken dialog systems are widely used in te-
lephony applications such as customer service, 
information access, and transaction fulfillment. 
They are also now common in virtual assistant 
applications for smartphones and mobile devices. 
But survey designers seeking automation have 
mostly eschewed spoken dialog in favor of tex-
tual web surveys or touchtone DTMF response 
systems. A preliminary comparison of spoken 
dialog and touchtone survey systems is available 
in Bloom (2008), and Stent et al. (2007) offer an 
evaluation of a spoken dialog system for aca-
demic course ratings. The work presented here 
describes the first large-scale investigation into 
spoken dialog technology as a viable means of 
administering the kinds of surveys that produce 
official statistics and social scientific data.  

Survey interview designers should be interest-
ed in using spoken dialog systems for several 
reasons. The most obvious reason is to curtail the 
error and bias that human interviewers are 
known to introduce to survey results data. Dec-
ades of research and investment led to “standard-
ized interviewing techniques” to reduce this error 
(Fowler and Mangione 1990), and limit a survey 

329



interviewer’s ability to offer help or clarification 
in ways that might affect results. Automated dia-
log systems can be thought of as the ultimate in 
standardization, as they can be designed to pro-
vide exactly the same interaction possibilities to 
all respondents. In effect, everyone can be inter-
viewed by the same “interviewer.” Or, if survey 
designers want to allow clarification in an inter-
view, an automated spoken dialog system can 
ensure that the same possibilities are available to 
all respondents (Schober and Conrad 1997). 

Unlike systems that use human interviewers, 
there is marginal additional cost per interview 
after the initial investment of building a system. 
This offers significant potential for cost savings 
in large cross-sectional samples or repeated panel 
surveys, such as the U.S. Current Population 
Survey or the American Community Survey. Re-
peated data collection allows refinement and re-
training of speech models to improve perfor-
mance. Spoken dialog system surveys can be 
administered on demand at any time of day, al-
lowing a better fit with respondents’ circum-
stances and schedules. Compared to asynchro-
nous text-based interviews like web or paper-
and-pencil surveys, spoken dialog systems can 
capture richer verbal paradata (Couper 2009) or 
process data like pauses, disfluencies and proso-
dy (Ehlen et al. 2007). Finally, survey tasks fit 
nicely within the limitations of current recogni-
tion and dialog technology, since they tend to 
have a purposefully structured and controlled 
interaction flow and generally require only a lim-
ited number of responses to each question. 

While spoken dialog systems have the poten-
tial to remove data error that is introduced by 
variation in human interviewer behaviors, they 
also introduce risks to survey data quality due to 
speech recognition and understanding error.  
Numerous strategies for mitigating error have 
been explored in research on dialog systems 
(Bohus and Rudnicky 2005, Litman et al. 2006). 
One approach is to use either an explicit or im-
plicit confirmation of the user's input. Following 
previous research showing that explicit confir-
mation is less confusing for users (Shin et al. 
2002), we adopt an explicit confirmation strate-
gy, which is also more in keeping with standard-
ized interview techniques. 

The effects of speech recognition and under-
standing errors may be different in a survey dia-
log system than in most current spoken dialog 
applications. One consideration is speaker initia-
tive, and the stake of the user in the interaction. 
In systems for customer service, information ac-

cess, or transactions, the user generally initiates 
contact with the system and seeks to accomplish 
a task where the system’s recognition accuracy 
will affect success of the user’s own goal. But in 
a survey dialog, the system initiates contact, and 
most respondents do not have a stake in whether 
the designers of the survey system succeed at 
collecting high quality data from them.  

This is a key point where a survey interview-
ing system might differ from traditional SDS: 
From the survey researchers’ perspective, the 
critical question is not whether individual users 
achieve some goal, but rather the extent to which 
individual errors in system recognition and un-
derstanding affect the distribution of responses 
across the population sample, affecting the quali-
ty of the estimates produced. If recognition errors 
do not affect the substantive conclusions based 
on the survey data, then survey researchers 
should be able to tolerate the imprecision of 
recognition error. This situation makes survey 
system evaluation rather different from how one 
would expect to evaluate the task success of a 
traditional SDS, like a customer service system.  

In Section 2, we characterize the content of the 
survey items, describe the dialog strategy, and 
provide examples of interaction. Section 3 de-
scribes the technical architecture of the survey 
dialog system. We provide experimental evalua-
tion in Section 4, and conclusions in Section 5. 

2 Survey interview dialogs 
After an initial question assessing whether the 
respondent is in an environment where it is safe 
for them to talk, our system administers a series 
of 32 questions drawn from major U.S. social 
surveys, including the Behavioral Risk Factor 
Surveillance System (BRFSS), National Survey 
of Drug Use and Health (NSDUH), General So-
cial Survey (GSS), and the Pew Internet and 
American Life Project. The sample transcribed 
dialogs in Appendix 1 illustrate various features 
of interaction with the system. Question types 
include Yes/No, categorical (where users pick 
from a specified set of response options), and 
numerical questions. Some categorical items are 
grouped into battery questions with the same re-
sponse options for all the items. 

The system supports explicit requests to repeat 
the question or ask for help, and mimics a 
“standardized interviewing” style of interaction 
that trained interviewers would use to repeat or 
clarify a question when the answer is rejected or 
requires confirmation. Thresholds set on acoustic 
and language confidence scores are used to de-

330



cide whether to reject, explicitly confirm, or ac-
cept a response. The final question in the dialog 
in Appendix A (“Thinking about …”) illustrates 
the importance of confirmation in ensuring the 
correct survey response is recorded. In this case, 
the system misrecognized “None” for “Nine,” 
but this was caught by the explicit confirmation 
prompt. Two terms are introduced in the final 
example that we will return to in the evaluation. 
First hypothesis indicates the speech recognition 
and semantic result produced by the system the 
first time the question is asked. Last hypothesis 
indicates the speech recognition and semantic 
result that the system produced the last time the 
question was asked within the segment.  

3 System Architecture 
The survey dialog system is directly integrated 
with a custom-built survey data case collection 
management system (PAMSS). When a survey 
case is administered, the case management sys-
tem makes an HTTP request to a voice gateway, 
which initiates a call to the respondent. When the 
respondent answers, it bridges the call to a spo-
ken dialog system running within the AT&T 
WatsonSM speech platform. The system uses pre-
recorded prompts for survey questions and re-
prompts. Confirmations for numeric responses 
combine prompts with TTS output. 

 
Figure 1: Survey Dialog System Architecture 

Users’ spoken inputs are recognized using state-
specific grammars for each question. Data were 
not initially available for training statistical mod-
els, so SRGS (Hunt and McGlashan 2004) 
grammars were built for each answer. These 
were tuned in an initial pilot phase. The gram-
mars included standard responses for the ques-
tion, along with common paraphrases and fram-
ing words from the question. In the Watson plat-
form, a dialog manager  (built in Python) is inte-
grated with ASR and TTS engines. Questions to 
be administered are represented in a declarative 
format in a survey item specification along with 

references to the appropriate prompts and gram-
mars. The dialog manager interprets this specifi-
cation to administer the survey and control the 
interaction flow. As the user responds to ques-
tions, the answers are posted back to the survey 
case management system.  

4 Experimental Evaluation 
We evaluated the survey dialog system as part of 
the first phase of a larger experiment comparing 
different survey interaction modes (Schober et al. 
2012). In this phase, 642 subjects were recruited 
from Craigslist, Facebook, Google Ads, and 
Amazon Mechanical Turk. A web-based screener 
application verified respondents to be over 21 
and collected their zip code. Of these, 158 re-
spondents were randomly assigned to the auto-
mated voice condition. A $20 iTunes gift card 
was given as an incentive after completion of a 
post-interview web questionnaire. This included 
multiple-choice questions examining user satis-
faction with their experience. In total there were 
8,228 spoken inputs over the 158 respondent dia-
logs. These responses were transcribed, coded, 
and annotated for semantic content. 

The questions we sought to answer were: 
What is the performance of a spoken dialog sys-
tem on a typical survey task? What impact does 
speech recognition and concept error have on 
overall survey estimates? Does an automated 
survey system benefit from implementing a tradi-
tional confirmation strategy, where responses 
with low confidence scores are verified with con-
firmation dialog? We also examine the impact of 
dialog length and confirmation prompts on a 
qualitative measure of user satisfaction. 

4.1 ASR and concept accuracy 
We evaluated overall word, sentence, and con-
cept accuracy for all 8,228 spoken utterances to 
the system, shown in the first row of Table 1. 

Accuracy: Word  Sentence  Concept  
All  80% 78.2% 90.3% 
First  81.2% 78% 88.9% 
Last 88.5% 85.4% 95.6% 

Table 1: System Performance 

An input is “concept accurate” if the semantic 
value assigned by the system exactly matches 
that assigned by the annotator. First shows the 
performance on the first response made by a user 
to each question before any confirmation dialog. 
Last shows performance on the last time each 
question was asked. Concept accuracy on last 
responses is 95.6%, showing that the confirma-

AT&T$WATSONSM$SPEECH$PLATFORM$SURVEY$CASE$$
MANAGEMENT$$
SYSTEM$$
(ISR,$PARSONS)$

ASR$

NLU$

DIALOG$
MANAGER$

GRAMMARS$ PROMPTS$

AUDIO/
TTS$

SURVEY$$
ITEMS$

SPEC$(XML)$

CASE$DB$

SURVEY$
RESPONSE$

DB$

VOICE$GATEWAY$
(ASTERISK)$

h9p$$
request:$
iniDate$
voice$
survey$ Request$

handler$

calls$$
respondent$

Telephony$$
Gateway$

bridges$
to$$
Watson$

•  Manages$cases$
•  Collects$survey$

results$

survey$results$returned$to$
case$management$as$
quesDons$completed$

LOGS$

331



tion strategy resulted in a 60% relative reduction 
in error compared to the first response. 

4.2 Impact of Errors on Survey Estimates 
Recognition error is undoubtedly a key factor in 
overall user experience. But unlike dialog sys-
tems for information access, search, and transac-
tions, the most important factor in a survey dia-
log system is the impact of errors on the quality 
of the estimates derived from the survey. To ex-
amine the impact of the residual 4.4% concept 
error on overall survey error, we compared an-
swer distributions derived from the system hy-
pothesis for the last response versus the annota-
tion of the last response using paired t-tests. 

For the 18 categorical questions, we conducted 
t-tests comparing the counts for each response 
option of each question. For all 18 questions (a 
total of 77 response categories) none of the dif-
ferences were statistically significant (p<0.05). 
For the 14 numerical questions, for only one 
(“Number of times shopping in a grocery store in 
the last month”) did the interpretations differ 
significantly (Annotated: 7.8 times, Hypothesis: 
7.6 times, p=0.04).1 This is strong evidence that 
speech recognition errors in this system did not 
have a major effect on survey estimates. 

How much survey error would have occurred 
without the dialog strategy?  To test this, we 
compared the annotated last response to the sys-
tem hypothesis for the first response, simulating 
an interaction without confirmation dialog, and 
thus lower recognition accuracy—see Table 1 
(This is not a perfect simulation, as we have no 
independent evidence on whether the first or fi-
nal response is true). There would indeed have 
been more survey error without dialog, although 
the overall level was still surprisingly low. For 
the 18 categorical questions, 14 of the 77 re-
sponse categories show significant differences 
(p<0.05). For the 14 numerical questions, two 
showed significant differences.  

4.3 User Satisfaction 
One of the post-interview questionnaire items 
provided a qualitative measure of user satisfac-
tion: “Overall, how satisfied were you with the 
interview?” The results were: Very satisfied 
(47.3%), Somewhat satisfied (41.8%), Somewhat 
dissatisfied (7.1%), and Very dissatisfied (0.6%).  
We examined the impact of various dialog fea-
tures that seemed on intuitive grounds plausibly 

                                                
1 If we treat the two interpretations as independent samples, 
the response distributions did not differ significantly at all. 

connected with satisfaction: average number of 
turns per question, average number of clarifica-
tion prompts per session, and average number of 
no input response prompts. We conducted a se-
ries of logistic regressions with one variable con-
trolled at a time to see the extent to which each 
of these features affected satisfaction. A Chi-
squared test was used to measure significance. 
All three features were significant predictors 
when comparing Somewhat/Very Dissatisfied to 
Very/Somewhat satisfied (Table 2). 

Feature Odds ratio  SE p 
# turns per Q 10.411 0.787 0.003 
# clarifications  1.043 0.033 0.024 
# no input  2.001 0.176 <0.001 

Table 2: User satisfaction regression 

5 Conclusion 
Our results demonstrate the viability of conduct-
ing survey interviews of the sort from which im-
portant national statistics are derived with spoken 
dialog systems.  In our system, the speech recog-
nition errors (with an overall concept recognition 
rate of 95.6%) did not substantially affect the 
error of the survey estimates; for only one of 32 
questions was there a significant difference in the 
survey estimate determined by the automated 
spoken dialog system compared to the annotated 
result. Of course, we don’t know whether these 
results generalize to dialog systems with other 
features, different questions, or different re-
spondents; much remains to be learned.  

Nonetheless, our results provide some guid-
ance for improving respondent satisfaction and 
minimizing survey error in future development 
of survey dialog systems.  For example, for nu-
merical questions, which generally involve larger 
numbers of response options, recognition errors 
may be reduced by adopting the strategy of ask-
ing the respondent to select among categories 
representing ranges (e.g. “none”, “1 to 5 times”, 
“6 to 10 times”).  Recognition performance could 
be improved by tuning confirmation strategies, 
e.g. applying a tighter confidence threshold for 
numerical vs. categorical questions. In a broad 
scale application of a repeated spoken dialog 
survey, greater amounts of data could be availa-
ble for training statistical models for the respons-
es, for improved recognition accuracy and fur-
ther reduced concept error. Finally, it is worth 
exploring the trade-offs for survey error and re-
spondent satisfaction between adding potentially 
frustrating confirmation dialog and accepting 
lower-confidence recognition for subsequent 
human annotation and processing.  

332



Acknowledgments: NSF #SES-1025645 and SES-
1026225 to Conrad and Schober. 

References  
Jonathan Bloom. 2008. The Speech IVR as a Survey 

Interviewing Methodology. In Conrad and Schober 
(eds.), Envisioning the Survey Interview of the Fu-
ture. Wiley, New York. 

Dan Bohus and Alex Rudnicky. 2005. Sorry, I didn’t 
Catch That: An Investigation of Non-
Understanding Errors and Recovery Strategies. 
Proceedings of SIGdial-2005, Lisbon, Portugal. 

Frederick G. Conrad, Michael F. Schober, Chan 
Zhang, Huiying Yan, Lucas Vickers, Michael 
Johnston, Andrew L. Hupp, Lloyd Hemingway, 
Stefanie Fail, Patrick Ehlen, and Chris Antoun. 
2013. Mode Choice on an iPhone Increases Survey 
Data Quality. 68th Annual Conference of the 
American Association for Public Opinion Research 
(AAPOR), Boston, MA. 

Mick P. Couper, 2009. The Role of Paradata in Meas-
uring and Reducing Measurement Error in Surveys. 
NCRM Network for Methodological Innovation 
2009: The Use of Paradata in UK Social Surveys. 

John Ehlen and Patrick Ehlen. 2007. Cellular-Only 
Substitution in the United States as Lifestyle Adop-
tion. Public Opinion Quarterly: Special Issue Vol 
71 (5), pp. 717-733.  

Patrick Ehlen, Michael Schober, and Frederick G. 
Conrad. 2007. Modeling Speech Disfluency to 
Predict Conceptual Misalignment in Speech Sur-
vey Interfaces. Discourse Processes 44:3, pp. 245–
265. 

Floyd J. Fowler and Thomas W. Mangione 1990. 
Standardized Survey Interviewing; Minimizing In-
terviewer Related Error. Sage Publications, CA. 

Marek Fuchs, 2008. Mobile Web Surveys: A Prelimi-
nary Discussion of Methodological Implications. In 
Conrad and Schober (eds.), Envisioning the Survey 
Interview of the Future. Wiley, New York. 

Andrew Hunt and Scott McGlashan. 2004. Speech 
Recognition Grammar Specification Version 1.0. 
W3C Recommendation 16 March 2004. 
http://www.w3.org/TR/speech-grammar/. 

Diane Litman, Julia Hirschberg, and M. G. J. Swerts. 
2006. Characterizing and Predicting Corrections in 
Spoken Dialogue Systems. Computational Linguis-
tics 32:3, pp. 417-438. 

Michael F. Schober and Frederick G. Conrad. 
1997. Does conversational interviewing reduce 
survey measurement error? Public Opinion Quar-
terly, 61, pp. 576-602.  

Michael F. Schober, Frederick G. Conrad, Chris An-
toun, Carroll, Patrick Ehlen, Stefanie Fail, Andrew 

L. Hupp, Michael Johnston, Courtney Kellner, 
Kelly Nichols, Leif Percifield, Lucas Vickers, Hui-
ying Yan, and Chan Zhang. 2012. Disclosure and 
Quality of Answers in Text and Voice Interviews 
on iPhones. 67th Annual Conference of the Ameri-
can Association for Public Opinion Research 
(AAPOR) Orlando, FL. 

Jongho Shin, Shrikanth Narayanan, Laurie Gerber, 
Abe Kazemzadeh, and Dani Byrd. 2002. Analysis 
of User Behavior under Error Conditions in Spo-
ken Dialogs. Proceedings of ICSLP-2002. 

Amanda Stent, Svetlana Stenchikova, and Matthew 
Marge. 2006. Dialog systems for surveys: The 
Rate-a-Course system. Proceedings of the 
IEEE/ACL 2006 Workshop on Spoken Language 
Technology. pp. 210-213. 

Appendix – Sample Survey Dialogs 
I: Have you smoked at least 100 cigarettes in your 
entire life? Yes or no. 
R: No 
I: Thanks 
 
I: Now I'd like to ask you some questions specific to 
your diet. For the next group of questions, I'll ask you 
how much you favor or oppose engaging in certain 
behaviors. After I say the behavior, please say one of 
the following: strongly favor, somewhat favor, neither 
favor nor oppose, somewhat oppose or strongly op-
pose. The first behavior...avoiding fast food. 
R: Strongly oppose 
I: You said strongly oppose, right? 
R: Yes 
I: Got it 
I: The next behavior: Maintaining a healthy diet 
R: Repeat 
I: The next behavior: Maintaining a healthy diet 
R: Strongly agree 
I: Sorry, I didn't hear you. When thinking about 
'maintaining a healthy diet', please say one of the fol-
lowing: strongly favor, somewhat favor, neither favor 
nor oppose, somewhat oppose or strongly oppose.  
R: Somewhat favor 
I: You said somewhat favor, right?  
R: Yes  
I: Thanks 
… 
I: Thinking about the time since your eighteenth 
birthday (including the recent past that you've already 
told us about), how many male partners have you had 
sex with?  
R: None  (First hypothesis: “Nine”) 
I: I think you said '9'. Is that right? Yes or No.  
R: No  
I: Thinking about the time since your eighteenth 
birthday (including the recent past that you've already 
told us about), how many male partners have you had 
sex with?  
R: Zero (Last hypothesis: “Zero”) 
I: Thanks 

333


