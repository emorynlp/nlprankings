Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 967–975,

Beijing, August 2010

967

A Multiple-Domain Ontology Builder 

Sara Salem 

Samir AbdelRahman 

Computer Science Department 

Faculty of Computers and Information - Cairo University 
{s.salem,s.abdelrahman@fci-cu.edu.eg} 

  
 
 
   

  
 
 
  

 

 

Abstract 

The  interpretation  of  a  multiple-domain 
text corpus as a single ontology leads to 
misconceptions.  This  is  because  some 
concepts  may  be  syntactically  equal; 
though, they are semantically lopsided in 
different  domains.  Also,  the  occurrences 
of a domain concept in a large multiple-
domain  corpus  may  not  gauge  correctly 
the  concept  significance.  This  paper 
tackles the mentioned problems and pro-
poses a novel ontology builder to extract 
separate domain specific ontologies from 
such a corpus. The builder contribution is 
to sustain each domain specific concepts 
and  relations  to  get  precise  answers  for 
user questions. We extend a single ontol-
ogy  builder  named  Text2Onto  to  apply 
our  thought.  We  fruitfully  enhance  it  to 
answer,  more  precisely,  questions  on  a 
subset of AQUAINT corpus. 

Introduction 

1 
Domain  ontology  is  a  knowledge  representation 
of the domain as a set of concepts and relations. 
Ontology notion always presents handy semantic 
solutions  for  various  hot  research  areas  such  as 
Semantic  Web,  Informational  Retrieval,  and 
Question Answering. 

Currently,  automatic  ontology  builders  pre-
sume that the given corpus has a single domain. 
When used with a multiple-domain corpus, these 
builders generate 1 large ontology for the whole 
corpus. Dramatically, this causes 2 domain mis-
conception problems. First, the ontology concep-
tual  model  becomes  imprecise  for  the  common 
concepts  in  various  domains  having  different 

semantics.  Second,  the  relevance  weights  as-
signed to the concepts do not measure precisely 
their significance in specific domains. 

This  paper  presents  a  promising  solution  for 
the  mentioned  problems.  The  proposed  solution 
is  an  integrated  2-layer  ontology  builder.  The 
ontology  layers  are:  1)  the  conceptual  layer, 
which has the key concepts and relations of each 
separate domain, and 2) the general layer, which 
maintains  the  general  domain  information  re-
garding related persons, organizations, locations, 
and  dates.  Our  proposed  2-layer  ontology  im-
proves  the  extracted  answers  for  single-domain 
and  cross-domain  questions.  We  successfully 
prove our thought against Text2Onto builder. 

Ontology extraction from a domain corpus has 
been targeted by many researchers. The core ex-
traction  approaches  can  be  classified  into  3  ap-
proaches.  The  first  approach  is  to  build  the  on-
tology  from  scratch  (Buitelaar  et  al.,  2004;  Ci-
miano  and  Völker,  2005).  The  second approach 
is to extend a predefined general ontology, such 
as  WordNet,  with  possible  application  domain 
concepts  and  relations  (Navigli  and  Velardi, 
2004). The last approach is to build ontology as a 
composition  of  other  predefined  ontologies  (Ci-
miano  et  al.,  2006).  Moreover,  as  an  ontology 
building  design  decision,  the  resultant  ontology 
is  either  a  single  layer  ontology  or  a  multi-
layered  ontology  (Benslimane  et  al.,  2000;  Du-
montier and Villanueva-Rosales, 2007). 

The  paper  is  organized  as  follows:  Section  2 
introduces  some  related  systems;  Section  3  ex-
plains  the  misconceptions  due  to  extracting  a 
single  ontology  from  a  multiple-domain  corpus; 
Section 4 describes our proposed builder; Section 
5  illustrates  our  Question  Answering  system, 
which is used for the evaluation; Section 6 states 
our  evaluation  results;  and  Section  7  draws  our 
conclusion and directions for the future work. 

968

2  Related Work 
There are 3 main approaches for ontology build-
ing,  namely  building  from  scratch,  extending  a 
general  ontology,  or  building  an  ontology  as  a 
composition of other predefined ontologies. 

Text2Onto  (Cimiano  and  Völker,  2005)  ap-
plies  the  first  approach.  It  is  a  framework  for 
learning  ontologies  automatically  from  textual 
data. It implements diverse linguistic and statis-
tical  techniques  to  extract  domain  concepts  and 
relations. It combines results from different tech-
niques,  and  it  represents  the  extracted  ontology 
elements  in  a  so  called  Probabilistic  Ontology 
Model (POM), which assigns a confidence value 
for each learnt element. 

OntoLT (Buitelaar et al., 2004) is another ex-
ample  of  building  from  scratch.  It  is  a  Protégé1 
plug-in that extracts ontology from text by defin-
ing a set of mapping rules. The rules map certain 
linguistic structures in an annotated text into on-
tological  elements.  The  extracted  elements  are 
validated  by  the  user  before  being  inserted  into 
the ontology. 

OntoLearn  (Navigli  and  Velardi,  2004)  em-
ploys the second approach. It is a framework for 
trimming and extending general purpose ontolo-
gies, like WordNet, with specific domain termi-
nologies and taxonomies. It extracts domain ter-
minologies,  and  it  uses  a  relevance  measure  to 
keep  out  non-relevant  terms.  OntoLearn  uses  a 
novel  technique,  called  SSI,  to  assign  a  domain 
specific  term  to  the  correct  sense  in  a  general 
ontology. 

The third approach is proposed in (Cimiano et 
al.,  2006).  It  presents  a  system  that  integrates 
several  heterogeneous  semantic  sources  into  1 
ontology,  which  is  used  to  extract  answers  for 
user queries from various knowledge sources. 

As a design decision, the ontology may consist 
of  a  single  layer  or  of  multiple  layers.  Bensli-
mane  et  al.  (2000)  apply  the  multiple-layer  ap-
proach  for  manually  generating  a  set  of  interre-
lated ontology layers; each layer models a spatial 
domain  specific  function.  Also,  Dumontier  and 
Villanueva-Rosales (2007) suggest a 3-layer on-
tology  design.  The  first  layer  (primitive  layer) 
defines the basic domain concepts and relations. 
The second layer (complex layer) imposes more 
complex  domain  restrictions  on  the  primitive 
                                                 
1 http://protege.stanford.edu/  

layer. The top layer (application layer) maintains 
application specific restrictions. 

Our  builder  constructs  a  layered  ontology 
from scratch. Its main distinguished features are: 
1)  generating  separate  domain  specific  ontolo-
gies  from  a  multiple-domain  corpus,  2)  extract-
ing  general  domain  information,  in  addition  to 
core domain conceptual information, and 3) it is 
an automatic multi-layered ontology builder, un-
like  other  automatic  builders,  which  generate 
single layer ontologies. 

Our system can extend current builders, which 
extract  ontologies  from  textual  data,  allowing 
them  to  handle  a  multiple-domain  corpus.  We 
selected  Text2Onto  because  it  is  an  automatic 
ontology builder, and it implements a variety of 
algorithms  to  extract  many  types  of  ontology 
elements.  We  use  a  news  corpus  as  a  multiple-
domain corpus since it contains documents from 
different domains like Politics, Sports, Arts, and 
Finance. 
3  Ontology Misconceptions 
Building a single ontology for a given corpus is a 
familiar method. However, when dealing with a 
multiple-domain corpus, the builder usually suf-
fers from the following 2 problems: 

First, the ontology conceptual model becomes 
imprecise in the definition of common concepts 
that  are  semantically  lopsided  in  different  do-
mains. For example, the concept "wall street" in 
the Finance domain is defined as a financial in-
stitution, and it is in the Arts domain defined as a 
movie.  It is inaccurate to define the concept with 
2  totally  different  meanings  in  1  ontology.  It  is 
also  incorrect  to  ignore  a  definition  of  them. 
When using Text2Onto for that example, it gene-
rates only 1 definition for "wall street" as a sub-
class-of "institution". 

Second,  when  weighing  concepts 

in  a 
multiple-domain  corpus,  the  relevance  weights 
assigned  to  the  concepts  do  not  indicate  the 
significance of each concept in a certain domain. 
As a result, some core domain specific concepts 
may have low weights with respect to the whole 
corpus. For example the concept "trading" has a 
low  weight 
in  a  multiple-domain  corpus; 
although,  it  is  a  main  concept  in  the  Finance 
domain 
(Section  6.2).  This  gives  wrong 
indication of the concept importance to the user.  

969

Input Corpus 
 

Clustering 

Clustered corpus

 

 

 

The General Layer

General Layer 

Generator

 

Conceptual Layer 

 Generator 

The Conceptual Layer 

Figure 1. The Multiple-Domain Ontology Builder. 

 

4  The Proposed Ontology Builder 
Our  builder  aims  to  extract  precise  ontologies, 
which  model  possible  knowledge  in  a  multiple-
domain  corpus.  A  domain  corpus,  mostly,  not 
only contains information about the core domain 
concepts and their relations, but it also contains 
general  domain  information  such  as  dates  of 
events and names of persons, locations, or organ-
izations  participating  in  the  domain.  Existing 
ontology  builders  either  ignore  this  general  in-
formation or they provide a limited implementa-
tion to extract it. 
4.1  System Overview 
The input to our builder (Figure 1) is a multiple-
domain  corpus.  The  first  step  is  the  clustering 
operation,  which  divides  the  given  corpus  doc-
uments  into  clusters  that  are  different  among 
each other with high internal similarity. The next 
step  is  the  conceptual  layer  generation.  In  this 
step, we use Text2Onto to extract a separate on-
tology  for  each  domain.  Finally,  the  general 
layer generator uses each domain corpus and the 
conceptual  layer  ontology  to  extract  relations 
among  the  concepts  and  the  Named  Entities  in 
that domain.  
4.2  The Conceptual Layer 
The first step in constructing the conceptual layer 
is  the  clustering  operation.  We  separate  a  mul-
tiple-domain corpus into various domain specific 
corpora  such  that  the  domain  concepts  are 
weighted based on their significance in that do-
main;  also,  the  common  concepts  in  different 
domains are separated. We favored a hierarchical 
clustering  technique  over  a  flat  clustering  one. 
That  was  because  the  number  of  resulting  clus-

ters should be known as a parameter in the latter. 
However,  the  number  of  corpus  domains  might 
be unknown in our case. 

We  employ  the  agglomerative  hierarchical 
clustering technique (Manning et al., 2008). The 
technique starts with each document as a single-
ton cluster, and then it successively merges pairs 
of  similar  clusters  until  all  clusters  are  merged 
into  1  cluster.  We  use  the  vector  space  model 
(Manning  et  al.,  2008)  to  represent  each  docu-
ment as a vector of terms' weights. The weight of 
a term w in a document d is calculated using the 
TF-IDF measure (Equation 1). 

 (cid:1846)(cid:1832)(cid:1835)(cid:1830)(cid:1832)(cid:4666)(cid:1875),(cid:1856)(cid:4667)(cid:3404)(cid:1846)(cid:1832)(cid:4666)(cid:1875),(cid:1856)(cid:4667)(cid:1499)(cid:1864)(cid:1867)(cid:1859) (cid:1840)(cid:1830)(cid:1832)(cid:4666)(cid:1875)(cid:4667)        (cid:4666)1(cid:4667) 

 
Where  N  is  the  corpus  size,  TF  (w,d)  is  the 
number of occurrences of the term w in the doc-
ument d, and DF (w) is the number of documents 
containing the term w.  

The similarity between 2 documents is calcu-
lated using the Cosine Similarity measure (Equa-
tion 2). 

(cid:1848)(cid:4666)(cid:1856)1(cid:4667) .(cid:1848)(cid:4666)(cid:1856)2(cid:4667)
||(cid:1848)(cid:4666)(cid:1856)1(cid:4667)||(cid:1499)||(cid:1848)(cid:4666)(cid:1856)2(cid:4667)||       (cid:4666)2(cid:4667) 

 (cid:1855)(cid:1867)(cid:1871)(cid:1861)(cid:1866)(cid:1857)(cid:4666)(cid:1856)1,(cid:1856)2(cid:4667)(cid:3404)

 
Where V(d) is the terms' weights vector for the 
document  d,  ||V(d)||  is  the  Euclidean  length  of 
the  vector  V(d),  and  the  numerator  is  the  dot 
product of the 2 vectors.  

The similarity between 2 clusters is calculated 
using  the  UPGMA  measure  (Steinbach  et  al., 
2000) (Equation 3). 

 

970

(cid:1871)(cid:1861)(cid:1865)(cid:4666)(cid:1829)1,(cid:1829)2(cid:4667)(cid:3404)∑

(cid:1855)(cid:1867)(cid:1871)(cid:1861)(cid:1866)(cid:1857)(cid:4666)(cid:1856)1,(cid:1856)2(cid:4667)
(cid:1871)(cid:1861)(cid:1878)(cid:1857)(cid:4666)(cid:1829)1(cid:4667)(cid:1499)(cid:1871)(cid:1861)(cid:1878)(cid:1857)(cid:4666)(cid:1829)2(cid:4667)           (cid:4666)3(cid:4667) 
(cid:3031)(cid:2869)(cid:1488)(cid:3004)(cid:2869)
(cid:3031)(cid:2870)(cid:1488)(cid:3004)(cid:2870)

 
We use the UPGMA measure to cluster a sub-
set  of  DMOZ2 data  (1322  documents,  in  4  do-
mains),  and  it  performs  F-Measure  of  0.86. 
Steinbach et al. (2000) describe how to calculate 
F-Measure for a hierarchy of clusters. 

The combination similarity is the similarity of 
2 merged clusters. We use this measure to cut the 
clusters  hierarchy  into  M  clusters  by  grouping 
ones  having  a  minimum  combination  similarity 
of the threshold value €3. After clustering, we use 
Text2Onto to generate an ontology for each clus-
ter (domain). 
4.3  The General Layer 
Text2Onto performs well in extracting ontology 
elements such as concepts, sub-class-of relations, 
instance-of  relations,  and  part-of  relations.  Un-
fortunately, it performs inaccurately in extracting 
general domain information such as Named Enti-
ties  and  numeric  information.  There  are  3  rea-
sons for such misconception. First, proper nouns 
are  not  extracted  as  concepts.  Second,  numeric 
data is ignored. Third, restricted patterns are ap-
plied  for  the  relations  of  Named  Entities,  that 
include only verb relations like [(NP |PNP) verb 
(NP|PNP)]  and  instance-of  relations  like  [NP 
such  as  PNP],  [such  NP  as  PNP],  and  [PNP 
(NP)]. 

Because  of  the  above  reasons,  we  propose  a 
highly  flexible  pattern  based  relation  extractor. 
In our system, a pattern is a sequence of tags in 
the  form  of  a  regular  expression.  The  possible 
tags are the normal POS tags like NN, VB, JJ, IN 
besides  the  following  5  tags  CONCEPT,  PER-
SON,  LOCATION,  ORGANIZATION,  and 
DATE.  This  criterion  is  called  Mixed  Tagging. 
Currently,  dates  are  the  only  data  containing 
numbers  extracted  by  our  builder,  but  we  can 
easily extend it to handle more numeric data.  

The  Mixed  Tagging  operation  inputs  are  a 
document  and  the  related  conceptual  ontology 
(Figure  2).  The  operation  output  is  a  mixed 
tagged  document.  The  tagged  text  is  then  pro-
vided  to  the  Relations  Extractor  to  take  out  all 
                                                 
2 http://www.dmoz.org/ 
3 For clustering 600 AQUAINT documents, we use €=0.55 
resulting in 7 Clusters (Secion 6.4). 

Input 

Mixed 
Tagging 

Tagged  
Text 

 
Relations 

Relations 
Extractor 

Patterns 

 

Figure 2. The General Relations Extraction. 
 

relations  matching  our  current  predefined  pat-
terns. Example patterns are listed in Table 1; the 
first 2 patterns are verb relations, and the last 2 
are noun relations. 

The  regular  expression  ([.{1,12}]){0,5}  is 
used  to  limit  the  maximum  number  of  tokens 
between the subject, the object, and the relation 
to  5  tokens.  The  expression  [NN.?.?]  matches 
any noun tag, and [VB.?] matches any verb tag. 

After  extracting  the  relations  in  all  domain 
documents,  the  domain  general  ontology  is 
created. It imports the corresponding conceptual 
ontology  to  model  the  relations  among  Named 
Entities and concepts. 

 
([PERSON]) ([.{1,12}]){0,5}([VB.?])+ 
([.{1,12}]){0,5}([CONCEPT]) 
([ORGANIZATION])([.{1,12}]){0,5} 
([DATE])([.{1,12}]){0,5}([VB.?])+ 
([PERSON])([.{1,12}]){0,5} 
([NN.?.?])+([.{1,12}]){0,5}([DATE]) 
([NN.?.?])+([.{1,12}]){0,5}([PERSON]) 
([.{1,12}]){0,5}([ORGANIZATION]) 
Table 1. Sample Relation Patterns. 
5  Question Answering System 
Based on (Brank et al., 2005), a generated ontol-
ogy can be evaluated using 4 different ways: 1) 
by  a  human  who  assesses  it  based  on  specific 
criteria, 2) by a comparison with the source data, 
3) by a comparison with a golden standard, or 4) 
by using the ontology in an application and mea-
suring  the  application  performance.  We  chose 
the  last  option  because  the  manual  human  as-
sessment  and  the  comparison  with  the  source 
data are time consuming. Also, there is no golden 
standard ontology for a multiple-domain corpus. 
Recently,  researchers  have  studied  the  use  of 
ontologies 
the  user 
questions.  AquaLog  (Lopez  et  al.,  2007)  and 

to  extract  answers 

to 

971

Our Ontology 

 
 
 

Indexer 

Triples  
Finder 

Ontology 

Index 

Concepts  

URIs 

Concepts 

Finder 

Question

Question  
Parser 

Question 
Elements 

Ontology 
 Triples 

Triples  
Weighting 

Weighted 

Triples 

Answer  
Extraction 

Weighted 
Answer(s) 

 

Figure 3. The Question Answering System. 

 

PowerAqua  (Lopez  et  al.,  2009)  are  both 
ontology  based  Question  Answering  systems. 
from  various 
PowerAqua  extracts  answers 
the  web,  unlike 
ontologies  available  on 
AquaLog,  which  extracts  answers 
from  1 
configurable ontology. 
5.1  System Description 
We  implemented  our  simple  Question  Answer-
ing system handling who, when, where, and what 
questions.  In  the  following,  we  describe  the 
components of the system (Figure 3). 

The Indexer: to make it easier for the system 
to locate the question concepts, an index is gen-
erated for our layered ontology. All concepts in 
different ontologies containing a certain stem are 
grouped in an index entry in the index file. The 
form of an index entry is as follows: 

 

Stem,(Concept URI)+ 

 

The Question Parser: this component parses 
the user question, and it extracts 4 elements from 
it.  First,  the  answer  type;  it  can  be  PERSON, 
LOCATION,  ORGANIZATION,  DATE,  or 
ANY  based  on  the  question  type  such  as  who, 
where,  when,  or  what.  Second,  the  answer  re-
striction; it is used to limit the answers of what 
questions.  For  example,  the  answers  for  "what 
sport  …?"  question  are  restricted  only  to  the 
sport types. Third, the question target; it defines 
the thing in which the question is interested. The 
fourth  element  is  the  relation;  it  contains  the 
main verb(s) in the question. As an example, the 
elements of the question "What sport does Jenni-
fer Capriati play?" are: the answer type (ANY), 
the restriction (sport), the question target (Jenni-
fer Capriati), and the relation (play). 

For  a  compound  (2-clause)  question  such  as 
"What  countries  have  Rhodes  Scholars  come 

 

from  and  has  the  Hale  Bopp  comet  visible?", 
each question clause is parsed as a separate ques-
tion; finally, the answer extraction step intersects 
the answers of both clauses. 

The Concepts Finder: using the ontology in-
dex,  it  locates  concepts  containing  the  stems  of 
the question target and the restriction (if exists). 

The  Triples  Weighting: 

The  Triples  Finder:  it  extracts  the  triples 
which contain the question target concepts either 
as subjects or as objects. If the question is a defi-
nition  question  like  "What  is  something?",  the 
triple finder extracts only the sub-class-of triples. 
triples  are 
weighted based on their similarity to the question 
using our similarity criterion (Equation 4): 

(cid:1838)(cid:1861)(cid:1866)(cid:4666)(cid:1853),(cid:1854)(cid:4667)
(cid:3028)(cid:1488)(cid:3018)(cid:3029)(cid:1488)(cid:3021)(cid:1838)(cid:4666)(cid:1843)(cid:4667)(cid:1499)(cid:1838)(cid:4666)(cid:1846)(cid:4667)                              (cid:4666)4(cid:4667) 

 (cid:1871)(cid:1861)(cid:1865)(cid:4666)(cid:1843),(cid:1846)(cid:4667)(cid:3404)∑

the 

Where  Q  and  T  are  sets  of  the  bag-of-words 
for  the  question  relation  and  the  triple  relation 
respectively, Lin(a,b) is a measure for the seman-
tic similarity between a and b based on WordNet 
(Lin, 1998), and L(x) is the number of elements 
in the set x. 

The Answer Extraction: this component first 
filters  out  the  triples  mismatching  the  expected 
answer type. Then, if there is no restriction ele-
ment,  it  extracts  the  answer  from  the  weighted 
triples  by  considering  the  triple  object  if  the 
question target is the subject, and vice versa. The 
extracted  answer  from  a  triple  is  assigned  the 
same triple weight. If the question has a restric-
tion element, the answer(s) will be limited to the 
sub concepts of the restriction element. A weight 
(Equation  5)  is  assigned  to  each  sub  concept  s 
based on its similarity to the extracted triples as 
follows: 

 

972

(cid:1849)(cid:4666)(cid:1871)(cid:4667)(cid:3404)∑

(cid:1871)(cid:1861)(cid:1865)(cid:4666)(cid:1845),(cid:1846)(cid:4667)
(cid:3021)(cid:1488)(cid:3019)(cid:1838)(cid:4666)(cid:1844)(cid:4667)

                                     (cid:4666)5(cid:4667) 

 
Where R is the set of extracted triples, S and T 
are the sets of bag-of-words for the sub concept 
and  the  triple  relation  respectively,  sim(S,T)  is 
calculated  using  Equation  4,  and  L(R)  is  the 
number of elements in R.  

For a compound question, the list of resulting 
answers  contains  only  the  common  answers  ex-
tracted for the 2 clauses. 
6  Evaluation and Discussion 
In our evaluation, we assess: 1) the enhancement 
of  the  concepts'  weights  in  a  specific  domain 
corpus,  2)  the  enhancement  of  modeling  com-
mon concepts in different domains with different 
semantics, and 3) the performance of our  Ques-
tion Answering system. The assessment is done 
through a comparison between our approach and 
Text2Onto. 

In  the  development  of  our  builder,  we  used 
Text2Onto 4 ,  Stanford  Part-Of-Speech  Tagger 
(POS  Tagger)5,  Stanford  Named  Entity  Recog-
nizer  (NER)6,  and  Jena7.  In  the  Question  Ans-
wering  system,  we  also  used  the  Java  WordNet 
Similarity  Library  (JWSL) 8;  it  implements  the 
Lin measure. 
6.1  Data Set 
Our evaluation is based on the AQUAINT9 cor-
pus  (Graff,  2002).  It  is  an  English  news  corpus 
containing documents from the New York Times 
News Service, the Xinhua News Service, and the 
Associated  Press  Worldstream  News  Service. 
The  Question  Answering  track  in  TREC10 (The 
Text  REtrieval  Conference)  provides  a  set  of 
questions on AQUAINT corpus along with their 
answers. 
6.2  Concepts Weights Enhancement 
For  this  experiment,  we  generated  a  corpus  for 
the  3  domains,  namely  Finance,  Sports,  and 
                                                 
4 http://code.google.com/p/text2onto/  
5 http://nlp.stanford.edu/software/tagger.shtml  
6 http://nlp.stanford.edu/software/CRF-NER.shtml  
7 http://jena.sourceforge.net/  
8 http://grid.deis.unical.it/similarity/  
9http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalo
gId=LDC2002T31  
10 http://trec.nist.gov/data/qa.html  

Movies,  from  AQUAINT  documents,  such  that 
each  domain  has  equal  number  of  documents. 
We measured the concepts' significance weights 
when  using  Text2Onto  to  generate  a  single  on-
tology for the whole corpus and when using our 
builder  to  generate  3  different  domains  ontolo-
gies.  We  consider  3  measures  implemented  in 
Text2Onto, namely the Relative Term Frequency 
(RTF), the Entropy, and the TF-IDF. 

The RTF for a concept w is the probability of 
the  concept  occurrence  in  the  corpus  (Equation 
6). 

 (cid:1868)(cid:4666)(cid:1875)(cid:4667)(cid:3404) (cid:1840)(cid:1867).(cid:1867)(cid:1858) (cid:1867)(cid:1855)(cid:1855)(cid:1873)(cid:1870)(cid:1857)(cid:1866)(cid:1855)(cid:1857)(cid:1871) (cid:1867)(cid:1858) (cid:1875)
(cid:1840)(cid:1867).(cid:1867)(cid:1858) (cid:1853)(cid:1864)(cid:1864) (cid:1855)(cid:1867)(cid:1870)(cid:1868)(cid:1873)(cid:1871) (cid:1855)(cid:1867)(cid:1866)(cid:1855)(cid:1857)(cid:1868)(cid:1872)(cid:1871)             (cid:4666)6(cid:4667) 
 (cid:1831)(cid:1866)(cid:1872)(cid:4666)(cid:1875)(cid:4667)(cid:3404)(cid:1868)(cid:4666)(cid:1875)(cid:4667)(cid:1499)log(cid:1868)(cid:4666)(cid:1875)(cid:4667)                               (cid:4666)7(cid:4667) 
(cid:1831)(cid:1866)(cid:1872)(cid:4666)(cid:1875)(cid:4667)
 (cid:1840)(cid:3435)(cid:1831)(cid:1866)(cid:1872)(cid:4666)(cid:1875)(cid:4667)(cid:3439)(cid:3404)
(cid:1839)(cid:1861)(cid:1866) (cid:1831)(cid:1866)(cid:1872)(cid:3398)(cid:1839)(cid:1853)(cid:1876) (cid:1831)(cid:1866)(cid:1872)               (cid:4666)8(cid:4667) 

The entropy and the normalized entropy for a 
concept w are calculated as follows (Equations 7 
and 8 respectively): 

 
In  Section  4.2,  we  mention  how  to  calculate 
the TF-IDF value for a term w in a document d 
(Equation 1). The TF-IDF weight and the norma-
lized  TF-IDF  weight  for  a  concept  w  in  the 
whole  corpus  are  calculated  as  follows  (Equa-
tions 9 and 10 respectively): 

 

 (cid:1846)(cid:1832)(cid:1835)(cid:1830)(cid:1832)(cid:4666)(cid:1875)(cid:4667)(cid:3404)∑
 (cid:1840)(cid:3435)(cid:1846)(cid:1832)(cid:1835)(cid:1830)(cid:1832)(cid:4666)(cid:1875)(cid:4667)(cid:3439)(cid:3404)

(cid:1846)(cid:1832)(cid:1835)(cid:1830)(cid:1832)(cid:4666)(cid:1875),(cid:1856)(cid:4667)
(cid:3031)(cid:1488)(cid:3005) (cid:1840)
(cid:1846)(cid:1832)(cid:1835)(cid:1830)(cid:1832)(cid:4666)(cid:1875)(cid:4667)
(cid:3493)∑
(cid:1846)(cid:1832)(cid:1835)(cid:1830)(cid:1832)(cid:4666)(cid:1855)(cid:1861)(cid:4667)(cid:2870)
(cid:3030)(cid:3036)(cid:1488)(cid:3004)

                   (cid:4666)9(cid:4667) 
        (cid:4666)10(cid:4667) 

 
Where D is the set of documents containing w, 
N is the corpus size, and C is the set of all con-
cepts in the corpus.  

Since the concept weight is proportional to its 
occurrences in the corpus with respect to the oth-
er  concepts,  the  fair  distribution  of  the  occur-
rences leads to precise weight calculation. In the 
specific domain corpus, the distribution is  more 
reasonable than in multiple-domain corpus.  

973

Domain 

Concept 

Entropy 

TF-IDF 

Finance 

Sports 

Movies 

Stock 
Trading 
Shares 
Economy 
Sport 
Baseball 
League 
Football 
Actor 
Movie Industry 
Music 
Home Video 

Text2 
Onto 
0.181 
0.155 
0.100 
0.100 
0.822 
0.321 
0.299 
0.205 
0.525 
0.230 
0.205 
0.038 

Our 
Builder 
0.999 
0.670 
0.670 
0.670 
0.974 
0.389 
0.363 
0.251 
0.613 
0.362 
0.326 
0.066 

Text2 
Onto 
0.053 
0.044 
0.036 
0.026 
0.344 
0.147 
0.134 
0.085 
0.150 
0.098 
0.085 
0.012 

Our 
Builder 
0.103 
0.139 
0.139 
0.051 
0.379 
0.190 
0.174 
0.111 
0.194 
0.263 
0.230 
0.032 

RTF 
Text2 
Onto 
0.001 
0.001 
0.000 
0.000 
0.012 
0.003 
0.003 
0.002 
0.007 
0.002 
0.002 
0.000 

Our 
Builder 
0.020 
0.010 
0.010 
0.010 
0.019 
0.006 
0.005 
0.003 
0.022 
0.011 
0.009 
0.001 

Table 2. Concepts Weights Comparison between Our Builder and Text2Onto. 

This fact can be verified easily from Table 2. 
The  3  measures  give  higher  weights  in  the  do-
main specific ontologies than in a single ontolo-
gy for the whole corpus. 
6.3  Modeling Common Concepts 
To study the enhancement in modeling common 
concepts  having  different  meaning  in  different 
domains, we chose 5 concepts as samples (Table 
3).  For  each  concept,  we  selected  documents 
from  AQUAINT  and  from  the  Wikipedia  con-
cerning the concepts in 2 different domains. 

In this experiment, the single ontology gener-
ated by Text2Onto contains only 1 definition for 
each concept namely wall_street is_a institution, 
marijuana  is_a  drug,  bear  is_a  mammal,  jaguar 
is_a  cat,  and  world_war  is_a  war.  On  the  other 
hand, our builder maintains both concept defini-
tions in different ontologies. 

A movie 

Definition 1  Definition 2 

 
Concept 
Wall Street  A financial  
Institution 
A drug 
A Mammal 
A big cat 

Marijuana 
A song 
The bear 
A movie 
Jaguar 
A car 
World War  A war 
A museum 
Table 3. Sample of Lopsided Concepts. 
6.4  Question Answering Enhancement 
The experiment includes common concepts defi-
nition  questions,  single-domain  questions,  and 
cross-domain questions. 

To  illustrate  the  effect  of  the  common  con-
cepts  misconception  problem  solved  by  our 
builder against Text2Onto, we generated 5 defi-
nition  questions  for  the  5  concepts  in  Table  3, 
like  "what  is  wall  street?",  "what  is  marijua-
na?"…etc. 

For  the  single-domain  questions,  we  used  a 
subset  of  AQUAINT  corpus  composed  of  600 
documents clustered into 7 domains using com-
bination  similarity  threshold  value  of  0.55.  We 
selected  60  factoid  questions  from  TREC  2004 
questions  having  their  answers  in  these  docu-
ments. Examples of single-domain questions are: 

•  Who discovered prions? 
•  When was the IFC established? 
In  addition  to  factoid  questions,  TREC  2004 
also includes list questions. The answers of each 
question  are  aggregated  from  multiple  docu-
ments. We used these questions in generating 10 
cross-domain questions. Each question combines 
2 of TREC list questions such that the 2 list ques-
tions are in different domains. Examples of these 
questions are: 

•  What cities have an Amtrak terminal and 

have Crip gangs? 

•  What countries are Burger King located in 

and have IFC financed projects? 

Evaluation Criteria: the accuracy (A) (Equa-
tion  11)  is  used  for  evaluating  single-domain 
questions because each factoid question has only 
1 correct answer.  

 

974

(cid:1827)(cid:3404)(cid:1840)(cid:1867).(cid:1867)(cid:1858) (cid:1855)(cid:1867)(cid:1870)(cid:1870)(cid:1857)(cid:1855)(cid:1872) (cid:1853)(cid:1866)(cid:1871)(cid:1875)(cid:1857)(cid:1870)(cid:1871)
(cid:1840)(cid:1867).(cid:1867)(cid:1858) (cid:1869)(cid:1873)(cid:1857)(cid:1871)(cid:1872)(cid:1861)(cid:1867)(cid:1866)(cid:1871)

                 (cid:4666)11(cid:4667) 

 

 

The  definition  and  cross-domain  questions 
have multiple correct answers. The average Pre-
cision (P), Recall (R), and F-Measure (F) (Equa-
tions 12, 13, and 14 respectively) of all questions 
are used for our evaluation. 

 (cid:1842)(cid:3404) (cid:1840)(cid:1867).(cid:1867)(cid:1858) (cid:1855)(cid:1867)(cid:1870)(cid:1870)(cid:1857)(cid:1855)(cid:1872) (cid:1853)(cid:1866)(cid:1871)(cid:1875)(cid:1857)(cid:1870)(cid:1871)
(cid:1840)(cid:1867).(cid:1867)(cid:1858) (cid:1870)(cid:1857)(cid:1872)(cid:1870)(cid:1861)(cid:1857)(cid:1874)(cid:1857)(cid:1856) (cid:1853)(cid:1866)(cid:1871)(cid:1875)(cid:1857)(cid:1870)(cid:1871)                  (cid:4666)12(cid:4667) 
 (cid:1844)(cid:3404)(cid:1840)(cid:1867).(cid:1867)(cid:1858) (cid:1855)(cid:1867)(cid:1870)(cid:1870)(cid:1857)(cid:1855)(cid:1872) (cid:1853)(cid:1866)(cid:1871)(cid:1875)(cid:1857)(cid:1870)(cid:1871)
(cid:1840)(cid:1867).(cid:1867)(cid:1858) (cid:1853)(cid:1855)(cid:1872)(cid:1873)(cid:1853)(cid:1864) (cid:1853)(cid:1866)(cid:1871)(cid:1875)(cid:1857)(cid:1870)(cid:1871)                       (cid:4666)13(cid:4667) 
 (cid:1832)(cid:3404)2(cid:1499)(cid:1842)(cid:1499)(cid:1844)
(cid:1842)(cid:3397)(cid:1844)                                                      (cid:4666)14(cid:4667) 

Table 4 shows that, in the definition questions, 
we  achieve  F-Measure  of  1,  while  Text2Onto 
achieves  0.5.  This  is  because  our  builder  main-
tains the 2 different definitions of each concept, 
unlike Text2Onto, which contains only one. 

 
Questions 
Type 
Definition 
Questions 

Our 
Ontology 
P=1.0 
R=1.0 
F=1.0 

Text2Onto 
Ontology 
P=0.5 
R=0.5 
F=0.5 
A=0.05% 
P=0 
R=0 
F=0 

Single-Domain   A=68% 
P=0.49 
Cross-Domain 
 
R=0.59 
F=0.44 

Table 4. Question Answering Evaluation. 
 
In the single-domain questions, using our on-
tology, we could answer 41 questions while us-
ing Text2Onto ontology we could answer only 3 
questions ("what particle is a quark?", "what are 
prions made of?", and "What is the treatment of 
cataract?"). The low coverage of Named Entities 
in Text2Onto hinders it from answering correctly 
any  question  of  types  Who,  When,  and  Where. 
This indicates the enhancement introduced by the 
proposed  general  layer  for  modeling  accurately 
more  domain  information.  In  the  cross-domain 
questions, we achieve F-Measure of 0.44. None 
of  the  cross-domain  questions  are  answered  us-
ing  Text2Onto  ontology  due  to  the  mentioned 
Named Entity coverage problem. 

results 

are  better 

Although  our 

than 
Text2Onto,  there  is  a  room  for  more  improve-
ments.  There  are  4  main  sources  for  retrieving 
wrong  or  incomplete  answers  (Table  5).  Some 
relations are not extracted because their elements 
(subject,  relation,  and  object)  are  not  near 
enough  from  each  other  in  the  text,  so  none  of 
our  patterns  or  Text2Onto  patterns  could  match 
them.  This  is  the  source  of  65%  of  the  errors. 
Missed  Named  Entities  or  wrongly  tagged  ones 
cause 16% of the errors. Some relations are not 
extracted because co-reference has not been han-
dled  yet.  That  leads  to  12%  of  the  total  errors. 
Finally, in the factoid questions, we consider the 
answer with the highest weight to be the correct 
answer; 7% of the answers are extracted but with 
lower weights.  

Error percentage 
65% 
16% 
12% 
7% 

 
Error Type 
No matching pattern 
NER Error 
Co-Reference 
Low answer weight 
Table 5. Answer Error Sources. 
 
Based  on  the  mentioned  experiments,  our 
builder  outperforms  Text2Onto 
in  Question 
Answering. In addition, it can be used skillfully 
to  enhance  other  Natural  Language  Processing 
applications  such  as  Information  Retrieval  from 
multiple-domain  data.  Our  initial  results  using 
220  queries  on  600  AQUAINT  documents 
records 0.35 F-Measure against Lucene11, which 
achieves 0.18. 
7  Conclusion and Future Work 
This paper presents the misconception problems 
when interpreting a multiple-domain corpus in a 
single ontology. A novel ontology builder is pre-
sented  handling  these  problems  by  generating 
separate  domain  ontologies  describing  core  and 
general domain information. 

Currently,  we  hand  on  improving  our  builder 
relation  extractor  to  answer  more  TREC  ques-
tions by automatically learning patterns from text 
and by handling co-reference. Moreover, we are 
working  to  enhance  the  performance  of  our  In-
formation Retrieval system. 
 
                                                 
11 http://lucene.apache.org/  

975

Manning,  Christopher  D.,  Prabhakar  Raghavan,  and 
Hinrich Schütze. 2008. Introduction to Information 
edition. 
Retrieval. 
http://nlp.stanford.edu/IR-book/information-
retrieval-book.html. Cambridge University Press. 

Online 

Navigli,  Roberto,  and  Paola Velardi. 2004. Learning 
Domain  Ontologies  from  Document  Warehouses 
and Dedicated Web Sites. In the Journal of Compu-
tational Linguistics, volume 30: 151-179. 

Steinbach,  Michael,  George  Karypis,  and  Vipin  Ku-
mar. 2000. A Comparison of Document Clustering 
Techniques. Technical Report #00-034, University 
of Minnesota. 
 

References 
Benslimane,  D.,  E.  Leclercq,  M.  Savonnet,  M.-N. 
Terrasse, and K. Yétongnon. 2000. On the Defini-
tion  of  Generic  Multi-layered  Ontologies  for  Ur-
ban  Applications.  In  the  International  Journal  of 
Computers, Environment, and Urban Systems, vo-
lume 24: 191-214. 

Brank, Janez, Marko Grobelnik, and Dunja Mladenić. 
2005.  A  Survey  of  Ontology  Evaluation  Tech-
niques. In  the  Proceedings  of  the  8th  International 
Multiconference on Information Society: 166-169. 
Buitelaar,  Paul,  Daniel  Olejnik,  and  Michael  Sintek. 
2004.  A  Protégé  Plug-In  for  Ontology  Extraction 
from Text Based on Linguistic Analysis. In the Pro-
ceedings of the  1st European Semantic Web Sym-
posium: 31-44. 

Cimiano,  Philipp,  and  Johanna  Völker.  2005. 
Text2Onto -  A  Framework  for  Ontology Learning 
and  Data-driven  Change  Discovery.  In  the  Pro-
ceedings  of  the  10th  International  Conference  on 
Applications  of  Natural  Language  to  Information 
Systems: 227-238. 

Cimiano,  Philipp,  Peter  Haase,  York  Sure,  Johanna 
Völker, and Yimin Wang. 2006. Question Answer-
ing on Top of the BT Digital Library. In the Pro-
ceedings  of  the  15th  International  Conference  on 
World Wide Web: 861-862. 

Dumontier,  Michel,  and  Natalia  Villanueva-Rosales. 
2007.  Three-Layer  OWL  Ontology  Design.  In  the 
Proceedings  of  the 2nd  International  Workshop  on 
Modular  Ontologies.  CEUR  Workshop  Proceed-
ings, volume 315. 

Graff, David. 2002. The AQUAINT Corpus of English 
News  Text.  Linguistic  Data  Consortium,  Philadel-
phia. 

Lin, Dekang. 1998. An Information-Theoretic Defini-
tion of Similarity. In the Proceedings of the 15th In-
ternational Conference on Machine Learning: 296-
304. 

Lopez,  Vanessa,  Victoria  Uren,  Enrico  Motta,  and 
Michele  Pasin.  2007.  AquaLog:  An  Ontology-
driven  Question  Answering  System  for  Organiza-
tional  Semantic  Intranets.  In  the  Journal  of  Web 
Semantics, volume 5: 72-105. 

Lopez,  Vanessa,  Victoria  Uren,  Marta  Sabou,  and 
Enrico  Motta.  2009.  Cross  Ontology  Query  Ans-
wering  on  the  Semantic  Web:  An  Initial  Evalua-
tion.  In  the  Proceedings  of  the  5th  International 
Conference on Knowledge Capture: 17-24. 

 

