



















































Semi-supervised learning of morphological paradigms and lexicons


Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569–578,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

Semi-supervised learning of morphological paradigms and lexicons

Malin Ahlberg
Språkbanken

University of Gothenburg
malin.ahlberg@gu.se

Markus Forsberg
Språkbanken

University of Gothenburg
markus.forsberg@gu.se

Mans Hulden
University of Helsinki

mans.hulden@helsinki.fi

Abstract

We present a semi-supervised approach
to the problem of paradigm induction
from inflection tables. Our system ex-
tracts generalizations from inflection ta-
bles, representing the resulting paradigms
in an abstract form. The process is in-
tended to be language-independent, and
to provide human-readable generalizations
of paradigms. The tools we provide can
be used by linguists for the rapid cre-
ation of lexical resources. We evaluate the
system through an inflection table recon-
struction task using Wiktionary data for
German, Spanish, and Finnish. With no
additional corpus information available,
the evaluation yields per word form ac-
curacy scores on inflecting unseen base
forms in different languages ranging from
87.81% (German nouns) to 99.52% (Span-
ish verbs); with additional unlabeled text
corpora available for training the scores
range from 91.81% (German nouns) to
99.58% (Spanish verbs). We separately
evaluate the system in a simulated task of
Swedish lexicon creation, and show that
on the basis of a small number of inflection
tables, the system can accurately collect
from a list of noun forms a lexicon with in-
flection information ranging from 100.0%
correct (collect 100 words), to 96.4% cor-
rect (collect 1000 words).

1 Introduction

Large scale morphologically accurate lexicon con-
struction for natural language is a very time-
consuming task, if done manually. Usually, the
construction of large-scale lexical resources pre-
supposes a linguist who constructs a detailed mor-
phological grammar that models inflection, com-
pounding, and other morphological and phonolog-

ical phenomena, and additionally performs a man-
ual classification of lemmas in the language ac-
cording to their paradigmatic behavior.

In this paper we address the problem of lexicon
construction by constructing a semi-supervised
system that accepts concrete inflection tables as in-
put, generalizes inflection paradigms from the ta-
bles provided, and subsequently allows the use of
unannotated corpora to expand the inflection ta-
bles and the automatically generated paradigms.1

In contrast to many machine learning ap-
proaches that address the problem of paradigm ex-
traction, the current method is intended to produce
human-readable output of its generalizations. That
is, the paradigms provided by the system can be
inspected for errors by a linguist, and if neces-
sary, corrected and improved. Decisions made by
the extraction algorithms are intended to be trans-
parent, permitting morphological system develop-
ment in tandem with linguist-provided knowledge.

Some of the practical tasks tackled by the sys-
tem include the following:

• Given a small number of known inflection ta-
bles, extract from a corpus a lexicon of those
lemmas that behave like the examples pro-
vided by the linguist.

• Given a large number of inflection tables—
such as those provided by the crowdsourced
lexical resource, Wiktionary—generalize the
tables into a smaller number of abstract
paradigms.

2 Previous work

Automatic learning of morphology has long been a
prominent research goal in computational linguis-
tics. Recent studies have focused on unsupervised
methods in particular—learning morphology from

1Our programs and the datasets used, including the
evaluation procedure for this paper, are freely avail-
able at https://svn.spraakbanken.gu.se/clt/
eacl/2014/extract

569



unlabeled data (Goldsmith, 2001; Schone and Ju-
rafsky, 2001; Chan, 2006; Creutz and Lagus,
2007; Monson et al., 2008). Hammarström and
Borin (2011) provides a current overview of unsu-
pervised learning.

Previous work with similar semi-supervised
goals as the ones in this paper include Yarowsky
and Wicentowski (2000), Neuvel and Fulop
(2002), Clément et al. (2004). Recent machine
learning oriented work includes Dreyer and Eis-
ner (2011) and Durrett and DeNero (2013), which
documents a method to learn orthographic trans-
formation rules to capture patterns across inflec-
tion tables. Part of our evaluation uses the same
dataset as Durrett and DeNero (2013). Eskander
et al. (2013) shares many of the goals in this paper,
but is more supervised in that it focuses on learn-
ing inflectional classes from richer annotation.

A major departure from much previous work
is that we do not attempt to encode variation
as string-changing operations, say by string edits
(Dreyer and Eisner, 2011) or transformation rules
(Lindén, 2008; Durrett and DeNero, 2013) that
perform mappings between forms. Rather, our
goal is to encode all variation within paradigms
by presenting them in a sufficiently generic fash-
ion so as to allow affixation processes, phonolog-
ical alternations as well as orthographic changes
to naturally fall out of the paradigm specification
itself. Also, we perform no explicit alignment of
the various forms in an inflection table, as in e.g.
Tchoukalov et al. (2010). Rather, we base our al-
gorithm on extracting the longest common subse-
quence (LCS) shared by all forms in an inflection
table, from which alignment of segments falls out
naturally. Although our paradigm representation
is similar to and inspired by that of Forsberg et al.
(2006) and Détrez and Ranta (2012), our method
of generalizing from inflection tables to paradigms
is novel.

3 Paradigm learning

In what follows, we adopt the view that words
and their inflection patterns can be organized
into paradigms (Hockett, 1954; Robins, 1959;
Matthews, 1972; Stump, 2001). We essentially
treat a paradigm as an ordered set of functions
(f1, . . . , fn), where fi: x1, . . . , xn 7→ Σ∗, that is,
where each entry in a paradigm is a function from
variables to strings, and each function in a partic-
ular paradigm shares the same variables.

3.1 Paradigm representation
We represent the functions in what we call ab-
stract paradigm. In our representation, an ab-
stract paradigm is an ordered collection of strings,
where each string may additionally contain in-
terspersed variables denoted x1, x2, . . . , xn. The
strings represent fixed, obligatory parts of a
paradigm, while the variables represent mutable
parts. These variables, when instantiated, must
contain at least one segment, but may otherwise
vary from word to word. A complete abstract
paradigm captures some generalization where the
mutable parts represented by variables are instan-
tiated the same way for all forms in one particu-
lar inflection table. For example, the fairly simple
paradigm

x1 x1+s x1+ed x1+ing

could represent a set of English verb forms, where
x1 in this case would coincide with the infinitive
form of the verb—walk, climb, look, etc.

For more complex patterns, several variable
parts may be invoked, some of them discontinu-
ous. For example, part of an inflection paradigm
for German verbs of the type schreiben (to write)
verbs may be described as:

x1+e+x2+x3+en INFINITIVE
x1+e+x2+x3+end PRESENT PARTICIPLE
ge+x1+x2+e+x3+en PAST PARTICIPLE
x1+e+x2+x3+e PRESENT 1P SG
x1+e+x2+x3+st PRESENT 2P SG
x1+e+x2+x3+t PRESENT 3P SG

If the variables are instantiated as x1=schr,
x2=i, and x3=b, the paradigm corresponds to
the forms (schreiben, schreibend, geschrieben,
schreibe, schreibst, schreibt). If, on the other
hand, x1=l, x2=i, and x3=h, the same paradigm re-
flects the conjugation of leihen (to lend/borrow)—
(leihen, leihend, geliehen, leihe, leihst, leiht).

It is worth noting that in this representation, no
particular form is privileged in the sense that all
other forms can only be generated from some spe-
cial form, say the infinitive. Rather, in the cur-
rent representation, all forms can be derived from
knowing the variable instantiations. Also, given
only a particular word form and a hypothetical
paradigm to fit it in, the variable instantiations can
often be logically deduced unambiguously. For
example, let us say we have a hypothetical form
steigend and need to fit it in the above paradigm,
without knowing which slot it should occupy. We

570



may deduce that it must represent the present par-
ticiple, and that x1=st, x2=i, and x3=g. From this
knowledge, all other forms can subsequently be
derived.

Although we have provided grammatical in-
formation in the above table for illustrative pur-
poses, our primary concern in the current work is
the generalization from inflection tables—which
for our purposes are simply an ordered set of
word forms—to paradigms of the format dis-
cussed above.

3.2 Paradigm induction from inflection tables

The core component of our method consists of
finding, given an inflection table, the maximally
general paradigm that reflects the information in
that table. To this end, we make the assumption
that string subsequences that are shared by dif-
ferent forms in an inflection table are incidental
and can be generalized over. For example, given
the English verb swim, and a simple inflection ta-
ble swim#swam#swum,2 we make the assump-
tion that the common sequences sw and m are ir-
relevant to the inflection, and that by disregarding
these strings, we can focus on the segments that
vary within the table—in this case the variation
i∼a∼u. In other words, we can assume sw and
m to be variables that vary from word to word
and describe the table swim#swam#swum as
x1+i+x2#x1+a+x2#x1+u+x2, where x1=sw and
x2=m in the specific table.

3.2.1 Maximally general paradigms
In order to generalize as much as possible from an
inflection table, we extract from it what we call the
maximally general paradigm by:

1. Finding the longest common subsequence
(LCS) to all the entries in the inflection table.

2. Finding the segmentation into variables of
the LCS(s) (there may be several) in the in-
flection table that results in

(a) The smallest number of variables. Two
segments xy in the LCS must be part of
the same variable if they always occur
together in every form in the inflection
table, otherwise they must be assigned
separate variables.

2To save space, we will henceforth use the #-symbol as a
delimiter between entries in an inflection table or paradigm.

ring
rang
rung

[r]i[ng]
[r]a[ng]
[r]u[ng]

rng

①	Extract
     LCS

②	Fit LCS 
     to table

③	Generalize
     to paradigms

Input:
inflection
tables

swim
swam
swum

swm
[sw]i[m]
[sw]a[m]
[sw]u[m]

x1+i+x2
x1+a+x2
x1+u+x2

x1+i+x2
x1+a+x2
x1+u+x2

④	Collapse
     paradigms

x1+i+x2
x1+a+x2
x1+u+x2

}

}

}

}
Figure 1: Illustration of our paradigm generaliza-
tion algorithm. In step À we extract the LCS sep-
arately for each inflection table, attempt to find
a consistent fit between the LCS and the forms
present in the table (step Á), and assign the seg-
ments that participate in the LCS variables (step
Â). Finally, resulting paradigms that turn out to be
identical may be collapsed (step Ã) (section 3.3).

(b) The smallest total number of infixed
non-variable segments in the inflection
table (segments that occur between vari-
ables).

3. Replacing the discontinuous sequences that
are part of the LCS with variables (every
form in a paradigm will contain the same
number of variables).

These steps are illustrated in figure 1. The
first step, extracting the LCS from a collection of
strings, is the well-known multiple longest com-
mon subsequence problem (MLCS). It is known
to be NP-hard (Maier, 1978). Although the num-
ber of strings to find the LCS from may be rather
large in real-world data, we find that a few sensible
heuristic techniques allow us to solve this problem
efficiently for practical linguistic material, i.e., in-
flection tables. We calculate the LCS by calculat-
ing intersections of finite-state machines that en-
code all subsequences of all words, using the foma
finite-state toolkit (Hulden, 2009).3

While for most tables there is only one way
to segment the LCS in the various forms, some
ambiguous corner cases need to be resolved by
imposing additional criteria for the segmentation,
given in steps 2(a) and 2(b). As an example,
consider a snippet of a small conjugation table
for the Spanish verb comprar (to buy), com-
prar#compra#compro. Obviously the LCS is
compr—however, this can be distributed in two
different ways across the strings, as seen below.

3Steps 2 and 3 are implemented using more involved
finite-state techniques that we plan to describe elsewhere.

571



comprar
compra
compro

{ x1
comprar
compra
compro

{

{

x1 x2

(a) (b)

{

{

x1 x2x1

{
The obvious difference here is that in the first

assignment, we only need to declare one vari-
able x1=compr, while in the second, we need
two, x1=comp, x2=r. Such cases are resolved by
choosing the segmentation with the smallest num-
ber of variables by step 2(a).

Remaining ambiguities are resolved by mini-
mizing the total number of infixed segments. As
an illustration of where this is necessary, consider
a small extract from the Swedish noun table segel
(sail): segel#seglen#seglet. Here, the LCS, of
which there are two of equal length (sege/segl)
must be assigned to two variables where either
x1=seg and x2=e, or x1=seg and x2=l:

segel
seglen
seglet

{ {x1 x2
segel
seglen
seglet

{ {x1 x2
(a) (b)

However, in case (a), the number of infixed
segments—the l’s in the second and third form—
total one more than in the distribution in (b), where
only one e needs to be infixed in one form. Hence,
the representation in (b) is chosen in step 2(b).

The need for this type of disambiguation strat-
egy surfaces very rarely and the choice to mini-
mize infix length is largely arbitrary—although it
may be argued that some linguistic plausibility is
encoded in the minimization of infixes. However,
choosing a consistent strategy is important for the
subsequent collapsing of paradigms.

3.3 Collapsing paradigms
If several tables are given as input, and we extract
the maximally general paradigm from each, we
may collapse resulting paradigms that are identi-
cal. This is also illustrated in figure 1.

As paradigms are collapsed, we record the in-
formation about how the various variables were
interpreted prior to collapsing. That is, for the
example in figure 1, we not only store the result-
ing single paradigm, but also the information that
x1=r, x2=ng in one table and that x1=sw, x2=m
in another. This allows us to potentially recon-
struct all the inflection tables seen during learn-

Form Input Generalization

[Inf] kaufen x1+en
[PresPart] kaufend x1+end
[PastPart] gekauft ge+x1+t
[Pres1pSg] kaufe x1+e
[Pres1pPl] kaufen x1+en
[Pres2pSg] kaufst x1+st
[Pres2pPl] kauft x1+t
[Pres3pSg] kauft x1+t
[Pres3pPl] kaufen x1+en

. . . . . . . . .

x1 = kauf

Table 1: Generalization from a German example
verb kaufen (to buy) exemplifying typical render-
ing of paradigms.

ing. Storing this information is also crucial for
paradigm table collection from text, fitting unseen
word forms into paradigms, and reasoning about
unseen paradigms, as will be discussed below.

3.4 MLCS as a language-independent
generalization strategy

There is very little language-specific information
encoded in the strategy of paradigm generaliza-
tion that focuses on the LCS in an inflection
table. That is, we do not explicitly prioritize
processes like prefixation, suffixation, or left-to-
right writing systems. The resulting algorithm
thus generalizes tables that reflect concatenative
and non-concatenative morphological processes
equally well. Tables 1 and 2 show the outputs of
the method for German and Arabic verb conjuga-
tion reflecting the generalization of concatenative
and non-concatenative patterns.

3.5 Instantiating paradigms

As mentioned above, given that the variable in-
stantiations of a paradigm are known, we may gen-
erate the full inflection table. The variable instan-
tiations are retrieved by matching a word form to
one of the patterns in the paradigms. For example,
the German word form bücken (to bend down)
may be matched to three patterns in the paradigm
exemplified in table 1, and all three matches yield
the same variable instantiation, i.e., x1=bück.

Paradigms with more than one variable may
be sensitive to the matching strategy of the vari-
ables. To see this, consider the pattern x1+a+x2
and the word banana. Here, two matches are pos-
sible x1=b and x2=nana and x1=ban and x2 =na.
In other words, there are three possible matching

572



Form Input Generalization

[Past1SG] katabtu (
��I��.

��J
�
») x1+a+x2+a+x3+tu

[Past2SGM] katabta (
��I��.

��J
�
») x1+a+x2+a+x3+ta

[Past2SGF] katabti ( �I�
��.
��J
�
») x1+a+x2+a+x3+ti

[Past3SGM] kataba ( �I.
��J
�
») x1+a+x2+a+x3+a

[Past3SGF] katabat (
��I��.

��J
�
») x1+a+x2+a+x3+at

. . . . . . . . .
[Pres1SG] aktubu ( �I.

��J
�
»

�
@) a+x1+x2+u+x3+u

[Pres2SGM] taktubu ( �I.
��J
�
º��K) ta+x1+x2+u+x3+u

[Pres2SGF] taktubı̄na (
�	á�
J.�

��J
�
º��K) ta+x1+x2+u+x3+ı̄na

[Pres3SGM] yaktubu ( �I.
��J
�
º�K
) ya+x1+x2+u+x3+u

[Pres3SGF] taktubu ( �I.
��Jº��K) ta+x1+x2+u+x3+u

. . . . . . . . .

x1 = k (¼), x2 = t ( �H), x3 = b (H. )

Table 2: Generalization from an Arabic con-
jugation table involving the root /k-t-b/ from
which the stems katab (to write/past) and ktub
(present/non-past) are formed, conjugated in Form
I, past and present tenses. Extracting the longest
common subsequence yields a paradigm where
variables correspond to root radicals.

strategies:4

1. shortest match (x1 =b and x2 =nana)
2. longest match (x1 =ban and x2 =na)
3. try all matching combinations

The matching strategy that tends to be success-
ful is somewhat language-dependent: for a lan-
guage with a preference for suffixation, longest
match is typically preferred, while for others
shortest match or trying all combinations may be
the best choice. All languages evaluated in this
article have a preference for suffixation, so in our
experiments we have opted for using the longest
match for the sake of convenience. Our imple-
mentation allows for exploring all matches, how-
ever. Even though all matches were to be tried,
‘bad’ matches will likely result in implausible in-
flections that can be discarded using other cues.

4 Assigning paradigms automatically

The next problem we consider is assigning the cor-
rect paradigms to candidate words automatically.

4The number of matches may increase quickly for longer
words and many variables in the worst case: e.g. caravan
matches x1+a+x2 in three different ways.

As a first step, we match the current word to a pat-
tern. In the general case, all patterns are tried for a
given candidate word. However, we usually have
access to additional information about the candi-
date words—e.g., that they are in the base form of
a certain part of speech—which we use to improve
the results by only matching the relevant patterns.

From a candidate word, all possible inflection
tables are generated. Following this, a decision
procedure is applied that calculates a confidence
score to determine which paradigm is the most
probable. The score is a weighted combination of
the following calculations:

1. Compute the longest common suffix for the
generated base form (which may be the input
form) with previously seen base forms. If of
equal length, select the paradigm where the
suffix occurs with higher frequency.

2. Compute frequency spread over the set of
unique word forms according to the follow-
ing formula:

∑
w∈set(W ) log(freq(w) + 1)

3. Use the most frequent paradigm as a tie-
breaker.

Step 1 is a simple memory-based approach,
much in the same spirit as van den Bosch and
Daelemans (1999), where we compare the current
base form with what we have seen before.

For step 2, let us elaborate further why the
frequency spread is computed on unique word
forms. We do this to avoid favoring paradigms
that have the same word forms for many or all
inflected forms. For example, the German noun
Ananas (pineapple) has a syncretic inflection with
one repeated word form across all slots, Ananas.
When trying to assign a paradigm to an unknown
word form that matches x1, it will surely fit the
paradigm that Ananas has generated perfectly
since we have encountered every word form in that
paradigm, of which there is only one, namely x1.
Hence, we want to penalize low variation of word
forms when assigning paradigms.

The confidence score calculated is not only ap-
plicable for selecting the most probable paradigm
for a given word-form; it may also be used to rank
a list of words so that the highest ranked paradigm
is the most likely to be correct. Examples of such
rankings are found in section 5.3.

573



0 50 100 150 200
Paradigms0.70

0.75

0.80

0.85

0.90

0.95

1.00
Inflection table coverage

FI-NOUNS-ADJS

FI-VERBS

ES-VERBS

DE-NOUNS

DE-VERBS

Figure 2: Degree of coverage with varying num-
bers of paradigms.

5 Evaluation

To evaluate the method, we have conducted three
experiments. First we repeat an experiment pre-
sented in Durrett and DeNero (2013) using the
same data and experiment setup, but with our
generalization method. In this experiment, we
are given a number of complete inflection tables
scraped from Wiktionary. The task is to recon-
struct complete inflection tables from 200 held-out
base forms. For this task, we evaluate per form
accuracy as well as per table accuracy for recon-
struction. The second experiment is the same as
the first, but with additional access to an unlabeled
text dump for the language from Wikipedia.

In the last experiment we try to mimic the situa-
tion of a linguist starting out to describe a new lan-
guage. The experiment uses a large-scale Swedish
morphology as reference and evaluates how reli-
ably a lexicon can be gathered from a word list us-
ing only a few manually specified inflection tables
generalized into abstract paradigms by our system.

5.1 Experiment 1: Wiktionary

In our first experiment we start from the inflec-
tion tables in the development and test set from
Durrett and DeNero (2013), henceforth D&DN13.
Table 3 shows the number of input tables as well
as the number of paradigms that they result in af-
ter generalization and collapsing. For all cases,
the number of output paradigms are below 10%
of the number of input inflection tables. Figure
2 shows the generalization rate achieved with the
paradigms. For instance, the 20 most common re-
sulting German noun paradigms are sufficient to
model almost 95% of the 2,564 separate inflection
tables given as input.

As described earlier, in the reconstruction task,
the input base forms are compared to the abstract

Input: Output:
Data inflection abstract

tables paradigms

DE-VERBS 1827 140
DE-NOUNS 2564 70
ES-VERBS 3855 97
FI-VERBS 7049 282
FI-NOUNS-ADJS 6200 258

Table 3: Generalization of paradigms. The num-
ber of paradigms produced from Wiktionary in-
flection tables by generalization and collapsing of
abstract paradigms.

paradigms by measuring the longest common suf-
fix length for each input base form compared to
the ones seen during training. This approach is
memory-based: it simply measures the similarity
of a given lemma to the lemmas encountered dur-
ing the learning phase. Table 4 presents our results
juxtaposed with the ones reported by D&DN13.
While scoring slightly below D&DN13 for the
majority of the languages when measuring form
accuracy, our method shows an advantage when
measuring the accuracy of complete tables. In-
terestingly, the only case where we improve upon
the form accuracy of D&DN13 is German verbs,
where we get our lowest table accuracy.

Table 4 further shows an oracle score, giv-
ing an upper bound for our method that would
be achieved if we were always able to pick the
best fitting paradigm available. This upper bound
ranges from 99% (Finnish verbs) to 100% (three
out of five tests).

5.2 Experiment 2: Wiktionary and
Wikipedia

In our second experiment, we extend the previous
experiment by adding access to a corpus. Apart
from measuring the longest common suffix length,
we now also compute the frequency of the hy-
pothetical candidate forms in every generated ta-
ble and use this to favor paradigms that generate
a large number of attested forms. For this, we
use a Wikipedia dump, from which we have ex-
tracted word-form frequencies.5 In total, the num-
ber of word types in the Wikipedia corpus was
8.9M (German), 3.4M (Spanish), 0.7M (Finnish),
and 2.7M (Swedish). Table 5 presents the results,

5The corpora were downloaded and extracted as de-
scribed at http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor

574



Data Per D&DN13 Per D&DN13 Oracle accuracy
table form per form (per table)

DE-VERBS 68.0 85.0 97.04 96.19 99.70 (198/200)
DE-NOUNS 76.5 79.5 87.81 88.94 100.00 (200/200)
ES-VERBS 96.0 95.0 99.52 99.67 100.00 (200/200)
FI-VERBS 92.5 87.5 96.36 96.43 99.00 (195/200)
FI-NOUNS-ADJS 85.0 83.5 91.91 93.41 100.00 (200/200)

Table 4: Experiment 1: Accuracy of reconstructing 200 inflection tables given only base forms from
held-out data when paradigms are learned from the Wiktionary dataset. For comparison, figures from
Durrett and DeNero (2013) are included (shown as D&DN13).

Data Per Per Oracle acc.
table form per form (table)

DE-VERBS 76.50 97.87 99.70 (198/200)
DE-NOUNS 82.00 91.81 100.00 (200/200)
ES-VERBS 98.00 99.58 100.00 (200/200)
FI-VERBS 92.50 96.63 99.00 (195/200)
FI-NOUNS-ADJS 88.00 93.82 100.00 (200/200)

Table 5: Experiment 2: Reconstructing 200 held-
out inflection tables with paradigms induced from
Wiktionary and further access to raw text from
Wikipedia.

where an increased accuracy is noted for all lan-
guages, as is to be expected since we have added
more knowledge to the system. The bold numbers
mark the cases where we outperform the result in
Durrett and DeNero (2013), which is now the case
in four out of five tests for table accuracy, scoring
between 76.50% for German verbs and 98.00% for
Spanish verbs.

Measuring form accuracy, we achieve scores
between 91.81% and 99.58%. The smallest im-
provement is noted for Finnish verbs, which has
the largest number of paradigms, but also the
smallest corpus.

5.3 Experiment 3: Ranking candidates

In this experiment we consider a task where we
only have a small number of inflection tables,
mimicking the situation where a linguist has man-
ually entered a few inflection tables, allowed the
system to generalize these into paradigms, and
now faces the task of culling from a corpus—in
this case labeled with basic POS information—the
candidate words/lemmas that best fit the induced
paradigms. This would be a typical task during
lexicon creation.

We selected the 20 most frequent noun
paradigms (from a total of 346), with one in-
flection table each, from our gold standard, the

Top-1000 rank Correct/Incorrect

TOP 10% 100/0 (100.0%)
TOP 50% 489/11 (97.8%)
TOP 100% 964/36 (96.4%)

Table 6: Top-1000 rank for all nouns in SALDO

Swedish lexical resource SALDO (Borin et al.,
2013). From this set, we discarded paradigms
that lack plural forms.6 We also removed from
the paradigms special compounding forms that
Swedish nouns have, since compound informa-
tion is not taken into account in this experiment.
The compounding forms are part of the original
paradigm specification, and after a collapsing pro-
cedure after compound-form removal, we were
left with a total of 11 paradigms.

In the next step we ranked all nouns in SALDO
(79.6k lemmas) according to our confidence score,
which indicates how well a noun fits a given
paradigm. We then evaluated the paradigm assign-
ment for the top-1000 lemmas. Among these top-
1000 words, we found 44 that were outside the
20 most frequent noun paradigms. These words
were not necessarily incorrectly assigned, since
they may only differ in their compound forms; as
a heuristic, we considered them correct if they had
the same declension and gender as the paradigm,
and incorrect otherwise.

Table 6 displays the results, including a total ac-
curacy of 96.4%.

Next, we investigated the top-1000 distribution
for individual paradigms. This corresponds to the
situation where a linguist has just entered a new
inflection table and is looking for words that fit the
resulting paradigm. The result is presented in two

6The paradigms that lack plural forms are subsets of other
paradigms. In other words: when no plural forms are attested,
we would need a procedure to decide if plural forms are even
possible, which is currently beyond the scope of our method.

575



10 20 30 40 50 60 70
Top ranked H%L

2

4

6

8

10
Error rate H%L

p_kikare

p_mening

p_flicka

10 20 30 40 50 60 70
Top ranked H%L

10

20

30

40

50

60

70
Error rate H%L

p_dike

p_akademi

p_vinge

p_nyckel

Figure 3: Top-1000: high and low precision paradigms.

error rate plots: figure 3 shows the low precision
and high precision paradigms in two plots, where
error rates range from 0-2% and 16-44% for the
top 100 words.

We further investigated the worst-performing
paradigm, p akademi (academy), to determine
the reason for the high error rate for this particular
item. The main source of error (334 out of 1000) is
confusion with p akribi (accuracy), which has no
plural. However, it is on semantic grounds that the
paradigm has no plural; a native Swedish speaker
would pluralize akribi like akademi (disregard-
ing the fact that akribi is defective). The second
main type of error (210 out of 1000) is confusion
with the unseen paradigm of parti (party), which
inflects similarly to akademi, but with a differ-
ence in gender—difficult to predict from surface
forms—that manifests itself in two out of eight
word forms.

6 Future work

The core method of abstract paradigm represen-
tation presented in this paper can readily be ex-
tended in various directions. One obvious topic of
interest is to investigate the use of machine learn-
ing techniques to expand the method to completely
unsupervised learning by first clustering similar
words in raw text into hypothetical inflection ta-
bles. The plausibility of these tables could then be
evaluated using similar techniques as in our exper-
iment 2.

We also plan to explore ways to improve the
techniques for paradigm selection and ranking. In
our experiments we have, for the sake of trans-
parency, used a fairly simple strategy of suffix
matching to reconstruct tables from base forms.
A more involved classifier may be trained for this
purpose. An obvious extension is to use a clas-
sifier based on n-gram, capitalization, and other

standard features to ascertain that word forms in
hypothetical reconstructed inflection tables main-
tain similar shapes to ones seen during training.

One can also investigate ways to collapse
paradigms further by generalizing over phonolog-
ical alternations and by learning alternation rules
from the induced paradigms (Koskenniemi, 1991;
Theron and Cloete, 1997; Koskenniemi, 2013).

Finally, we are working on a separate interactive
graphical morphological tool in which we plan to
integrate the methods presented in this paper.

7 Conclusion

We have presented a language-independent
method for extracting paradigms from inflection
tables and for representing and generalizing the
resulting paradigms.7 Central to the process of
paradigm extraction is the notion of maximally
general paradigm, which we define as the in-
flection table, with all of the common string
subsequences forms represented by variables.

The method is quite uncomplicated and outputs
human-readable generalizations. Despite the rel-
ative simplicity, we obtain state-of-the art results
in inflection table reconstruction tasks from base
forms.

Because of the plain paradigm representation
format, we believe the model can be used prof-
itably in creating large-scale lexicons from a few
linguist-provided inflection tables.

7The research presented here was supported by the
Swedish Research Council (the projects Towards a
knowledge-based culturomics, dnr 2012-5738, and Swedish
Framenet++, dnr 2010-6013), the University of Gothenburg
through its support of the Centre for Language Technology
and its support of Språkbanken, and the Academy of Finland
under the grant agreement 258373, Machine learning of
rules in natural language morphology and phonology.

576



References
Lars Borin, Markus Forsberg, and Lennart Lönngren.

2013. SALDO: a touch of yin to WordNet’s yang.
Language Resources and Evaluation, May. Online
first publication; DOI 10.1007/s10579-013-9233-4.

Erwin Chan. 2006. Learning probabilistic paradigms
for morphology in a latent class model. In Proceed-
ings of the Eighth Meeting of the ACL Special Inter-
est Group on Computational Phonology and Mor-
phology, pages 69–78. Association for Computa-
tional Linguistics.

Lionel Clément, Bernard Lang, Benoı̂t Sagot, et al.
2004. Morphology based automatic acquisition of
large-coverage lexica. In LREC 04, pages 1841–
1844.

Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.

Grégoire Détrez and Aarne Ranta. 2012. Smart
paradigms and the predictability and complexity of
inflectional morphology. In Proceedings of the 13th
EACL, pages 645–653. Association for Computa-
tional Linguistics.

Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 616–627. Association
for Computational Linguistics.

Greg Durrett and John DeNero. 2013. Supervised
learning of complete morphological paradigms. In
Proceedings of NAACL-HLT, pages 1185–1195.

Ramy Eskander, Nizar Habash, and Owen Rambow.
2013. Automatic extraction of morphological lex-
icons from morphologically annotated corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1032–1043. Association for Computational Linguis-
tics.

Markus Forsberg, Harald Hammarström, and Aarne
Ranta. 2006. Morphological lexicon extraction
from raw text data. In Advances in Natural Lan-
guage Processing, pages 488–499. Springer.

John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
linguistics, 27(2):153–198.

Harald Hammarström and Lars Borin. 2011. Unsuper-
vised learning of morphology. Computational Lin-
guistics, 37(2):309–350.

Charles F Hockett. 1954. Two models of grammati-
cal description. Morphology: Critical Concepts in
Linguistics, 1:110–138.

Mans Hulden. 2009. Foma: a finite-state compiler and
library. In Proceedings of the 12th Conference of the
European Chapter of the European Chapter of the
Association for Computational Linguistics: Demon-
strations Session, pages 29–32, Athens, Greece. As-
sociation for Computational Linguistics.

Kimmo Koskenniemi. 1991. A discovery procedure
for two-level phonology. Computational Lexicol-
ogy and Lexicography: A Special Issue Dedicated
to Bernard Quemada, 1:451–46.

Kimmo Koskenniemi. 2013. An informal discovery
procedure for two-level rules. Journal of Language
Modelling, 1(1):155–188.

Krister Lindén. 2008. A probabilistic model for guess-
ing base forms of new words by analogy. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 106–116. Springer.

David Maier. 1978. The complexity of some problems
on subsequences and supersequences. Journal of the
ACM (JACM), 25(2):322–336.

Peter H. Matthews. 1972. Inflectional morphology:
A theoretical study based on aspects of Latin verb
conjugation. Cambridge University Press.

Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. Paramor: finding paradigms
across morphology. In Advances in Multilingual
and Multimodal Information Retrieval, pages 900–
907. Springer.

Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6, pages
31–40. Association for Computational Linguistics.

Robert H Robins. 1959. In defence of WP. Transac-
tions of the Philological Society, 58(1):116–144.

Patrick Schone and Daniel Jurafsky. 2001.
Knowledge-free induction of inflectional mor-
phologies. In Proceedings of the second meeting
of the North American Chapter of the Association
for Computational Linguistics on Language tech-
nologies, pages 1–9. Association for Computational
Linguistics.

Gregory T. Stump. 2001. A theory of paradigm struc-
ture. Cambridge University Press.

Tzvetan Tchoukalov, Christian Monson, and Brian
Roark. 2010. Morphological analysis by mul-
tiple sequence alignment. In Multilingual Infor-
mation Access Evaluation I. Text Retrieval Experi-
ments, pages 666–673. Springer.

Pieter Theron and Ian Cloete. 1997. Automatic acqui-
sition of two-level morphological rules. In Proceed-
ings of the fifth conference on Applied natural lan-
guage processing, pages 103–110. Association for
Computational Linguistics.

577



Antal van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In Proceed-
ings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 285–292. As-
sociation for Computational Linguistics.

David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 207–216. Association for Com-
putational Linguistics.

578


