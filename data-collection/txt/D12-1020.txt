










































A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes


Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 214–222, Jeju Island, Korea, 12–14 July 2012. c©2012 Association for Computational Linguistics

A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes

Robert V. Lindsey
University of Colorado, Boulder

robert.lindsey@colorado.edu

William P. Headden III
Two Cassowaries Inc.

headdenw@twocassowaries.com

Michael J. Stipicevic
Google Inc.

stip@google.com

Abstract

Topic models traditionally rely on the bag-
of-words assumption. In data mining appli-
cations, this often results in end-users being
presented with inscrutable lists of topical un-
igrams, single words inferred as representa-
tive of their topics. In this article, we present
a hierarchical generative probabilistic model
of topical phrases. The model simultane-
ously infers the location, length, and topic of
phrases within a corpus and relaxes the bag-
of-words assumption within phrases by using
a hierarchy of Pitman-Yor processes. We use
Markov chain Monte Carlo techniques for ap-
proximate inference in the model and perform
slice sampling to learn its hyperparameters.
We show via an experiment on human subjects
that our model finds substantially better, more
interpretable topical phrases than do compet-
ing models.

1 Introduction

Probabilistic topic models have been the focus of
intense study in recent years. The archetypal topic
model, Latent Dirichlet Allocation (LDA), posits
that words within a document are conditionally
independent given their topic (Blei et al., 2003).
This “bag-of-words” assumption is a common sim-
plification in which word order is ignored, but
one which introduces undesirable properties into
a model meant to serve as an unsupervised ex-
ploratory tool for data analysis.

When an end-user runs a topic model, the output
he or she is often interested in is a list of topical

unigrams, words probable in a topic (hence, repre-
sentative of it). In many situations, such as during
the use of the topic model for the analysis of a new
or ill-understood corpus, these lists can be insuffi-
ciently informative. For instance, if a layperson ran
LDA on the NIPS corpus, he would likely get a topic
whose most prominent words include policy, value,
and reward. Seeing these words isolated from their
context in a list would not be particularly insightful
to the layperson unfamiliar with computer science
research. An alternative to LDA which produced
richer output like policy iteration algorithm, value
function, and model-based reinforcement learning
alongside the unigrams would be much more en-
lightening. Most situations where a topic model is
actually useful for data exploration require a model
whose output is rich enough to dispel the need for
the user’s extensive prior knowledge of the data.

Furthermore, lists of topical unigrams are often
made only marginally interpretable by virtue of their
non-compositionality, the principle that a colloca-
tion’s meaning typically is not derivable from its
constituent words (Schone and Jurafsky, 2001). For
example, the meaning of compact disc as a mu-
sic medium comes from neither the unigram com-
pact nor the unigram disc, but emerges from the bi-
gram as a whole. Moreover, non-compositionality
is topic dependent; compact disc should be inter-
preted as a music medium in a music topic, and as
a small region bounded by a circle in a mathemati-
cal topic. LDA is prone to decompose collocations
into different topics and violate the principle of non-
compositionality, and its unigram lists are harder to
interpret as a result.

214



We present an extension of LDA called Phrase-
Discovering LDA (PDLDA) that satisfies two
desiderata: providing rich, interpretable output and
honoring the non-compositionality of collocations.
PDLDA is built in the tradition of the “Topical N-
Gram” (TNG) model of Wang et al. (2007). TNG is
a topic model which satisfies the first desideratum by
producing lists of representative, topically cohesive
n-grams of the form shown in Figure 1. We diverge
from TNG by our addressing the second desidera-
tum, and we do so through a more straightforward
and intuitive definition of what constitutes a phrase
and its topic. In the furtherance of our goals, we
employ a hierarchical method of modeling phrases
that uses dependent Pitman-Yor processes to ame-
liorate overfitting. Pitman-Yor processes have been
successfully used in the past in n-gram (Teh, 2006)
and LDA-based models (Wallach, 2006) for creat-
ing Bayesian language models which exploit word
order, and they prove equally useful in this scenario
of exploiting both word order and topics.

This article is organized as follows: after describ-
ing TNG, we discuss PDLDA and how PDLDA ad-
dresses the limitations of TNG. We then provide de-
tails of our inference procedures and evaluate our
model against competing models on a subset of the
TREC AP corpus (Harman, 1992) in an experi-
ment on human subjects which assesses the inter-
pretability of topical n-gram lists. The experiment
is premised on the notion that topic models should
be evaluated through a real-world task instead of
through information-theoretic measures which often
negatively correlate with topic quality (Chang et al.,
2009).

2 Background: LDA and TNG

LDA represents documents as probabilistic mixtures
of latent topics. Each wordw in a corpus w is drawn
from a distribution φ indexed by a topic z, where z is
drawn from a distribution θ indexed by its document
d. The formal definition of LDA is

θd ∼ Dirichlet (α) zi | d, θ ∼ Discrete (θd)
φz ∼ Dirichlet (β) wi | zi, φ ∼ Discrete (φzi)

where θd is document d’s topic distribution, φz is
topic z’s distribution over words, zi is the topic as-
signment of the ith token, and wi is the ith word.
α and β are hyperparameters to the Dirichlet priors.

Here and throughout the article, we use a bold font
for vector notation: for example, z is the vector of all
topic assignments, and its ith entry, zi, corresponds
to the topic assignment of the ith token in the corpus.

TNG extends LDA to model n-grams of arbitrary
length in order to create the kind of rich output for
text mining discussed in the introduction. It does
this by representing a joint distribution P (z, c|w)
where each ci is a Boolean variable that signals the
start of a new n-gram beginning at the ith token. c
partitions a corpus into consecutive non-overlapping
n-grams of various lengths. Formally, TNG differs
from LDA by the distributional assumptions

wi | wi−1, zi, ci = 1, φ ∼ Discrete(φzi)
wi | wi−1, zi, ci = 0, σ ∼ Discrete(σziwi−1)

ci | wi−1, zi−1, π ∼ Bernoulli(πzi−1wi−1)

where the new distributions πzw and σzw are en-
dowed with conjugate prior distributions: πzw ∼
Beta(λ) and σzw ∼ Dirichlet(δ). When ci = 0,
word wi is joined into a topic-specific bigram with
wi−1. When ci = 1, wi is drawn from a topic-
specific unigram distribution and is the start of a new
n-gram.

An unusual feature of TNG is that words within
a topical n-gram, a sequence of words delineated
by c, do not share the same topic. To compen-
sate for this after running a Gibbs sampler, Wang
et al. (2007) analyze each topical n-gram post hoc
as if the topic of the final word in the n-gram was
the topic assignment of the entire n-gram. Though
this design simplifies inference, we perceive it as a
shortcoming since the aforementioned principle of
non-compositionality supports the intuitive idea that
each collocation ought to be drawn from a single
topic. Another potential drawback of TNG is that
the topic-specific bigram distributions σzw share no
probability mass between each other or with the un-
igram distributions φz . Hence, observing a bigram
under one topic does not make it more likely under
another topic or make its constituent unigrams more
probable. To be more concrete, in TNG, observing
space shuttle under a topic z (or under two topics,
one for each word) regrettably does not make space
shuttle more likely under a topic z′ 6= z, nor does it
make observing shuttle more likely under any topic.
Smoothing, the sharing of probability mass between

215



matter
atoms
elements
electrons
atom
molecules
form
oxygen
hydrogen
particles
element
solution
substance
reaction
nucleus

chemical reactions
atomic number
hydrogen atoms
hydrogen atom
periodic table
chemical change
physical properties
chemical reaction
water molecules
sodium chloride
small amounts
positive charge
carbon atoms
physical change
chemical properties

like charges repel
positively charged nucleus
unlike charges attract
outer energy level
reaction takes place
negatively charged electrons
chemical change takes place
form new substances
physical change takes place
form sodium chloride
modern atomic theory
electrically charged particles
increasing atomic number
second ionization energies
higher energy levels

(a) Topic 1

president
congress
vote
party
constitution
state
members
office
government
states
elected
representatives
senate
house
washington

supreme court
new york
democratic party
vice president
political parties
national government
executive branch
civil rights
new government
political party
andrew jackson
chief justice
federal government
state legislatures
public opinion

civil rights act
civil rights movement
supreme court ruled
president theodore roosevelt
second continental congress
equal rights amendment
strong central government
sherman antitrust act
civil rights legislation
public opinion polls
major political parties
congress shall make
federal district court
supreme court decisions
american foreign policy

(b) Topic 2

words
word
sentence
write
writing
paragraph
sentences
meaning
use
subject
language
read
example
verb
topic

main idea
topic sentence
english language
following paragraph
words like
quotation marks
direct object
word processing
sentence tells
figurative language
writing process
following sentences
subject matter
standard english
use words

word processing center
word processing systems
word processing equipment
speak different languages
use quotation marks
single main idea
use words like
topic sentence states
present perfect tense
express complete thoughts
word processing software
use formal english
standard american english
collective noun refers
formal standard english

(c) Topic 3

energy
used
oil
heat
coal
use
fuel
produce
power
source
light
electricity
burn
gas
gasoline

natural resources
natural gas
heat energy
iron ore
carbon dioxide
potential energy
solar energy
light energy
fossil fuels
hot water
steam engine
large amounts
sun's energy
radiant energy
nuclear energy

nuclear power plants
nuclear power plant
important natural resources
electric power plants
called fossil fuels
important natural resource
produce large amounts
called solar energy
electric light bulb
use electrical energy
use solar energy
carbon dioxide gas
called potential energy
gas called carbon dioxide
called crude oil

(d) Topic 4

water
air
temperature
heat
liquid
gas
gases
hot
pressure
atmosphere
warm
cold
surface
oxygen
clouds

water vapor
air pollution
air pressure
warm air
cold water
earth's surface
room temperature
boiling point
drinking water
atmospheric pressure
cold war
high temperatures
liquid water
cold air
warm water

water vapor condenses
warm air rises
cold air mass
called water vapor
water vapor changes
process takes place
warm air mass
clean air act
gas called water vapor
dry spell holds
air pressure inside
sewage treatment plant
air pollution laws
high melting points
high melting point

(e) Topic 5

china
africa
india
europe
people
chinese
asia
egypt
world
rome
land
east
trade
countries
empire

middle east
western europe
north africa
mediterranean sea
years ago
roman empire
far east
southeast asia
west africa
saudi arabia
capital letter
asia minor
united states
capital city
centuries ago

2000 years ago
east india company
eastern united states
4000 years ago
southwestern united states
middle atlantic states
northeastern united states
western united states
southeastern united states
200 years ago
middle atlantic region
indus river valley
western roman empire
british north america act
coast guard station

(f) Topic 6

Figure 1: Six out of one hundred topics found by our model, PDLDA, on the Touchstone Applied Science
Associates (TASA) corpus (Landauer and Dumais, 1997). Each column within a box shows the top fifteen
phrases for a topic and is restricted to phrases of a minimum length of one, two, or three words, respectively.
The rows are ordered by likelihood.

216



  

... z
i-1

z
i

z
i+1

w
i-1

w
i+1

c
i

c
i+1

...

...
...

G

θ

π

α

λ

w
i

D

T
u

a b

ρ ε

...
...

V

|u|

Figure 2: PDLDA drawn in plate notation.

contexts, is desirable so that a model like this does
not need to independently infer the probability of
every bigram under every topic. The advantages of
smoothing are especially pronounced for small cor-
pora or for a large number of topics. In these sit-
uations, the observed number of bigrams in a given
topic will necessarily be very small and thus not sup-
port strong inferences.

3 PDLDA

A more natural definition of a topical phrase, one
which meets our second desideratum, is to have each
phrase possess a single topic. We adopt this in-
tuitive idea in PDLDA. It can also be understood
through the lens of Bayesian changepoint detection.
Changepoint detection is used in time series mod-
els in which the generative parameters periodically
change abruptly (Adams and MacKay, 2007). View-
ing a sentence as a time series of words, we posit that
the generative parameter, the topic, changes period-

ically in accordance with the changepoint indicators
c. Because there is no restriction on the number of
words between changepoints, topical phrases can be
arbitrarily long but will always have a single topic
drawn from θd.

The full definition of PDLDA is given by

wi | u ∼ Discrete(Gu)
Gu ∼ PYP(a|u|, b|u|, Gπ(u))
G∅ ∼ PYP(a0, b0, H)

zi | d, zi−1, θd, ci ∼

{
δzi−1 if ci = 0
Discrete (θd) if ci = 1

ci | wi−1, zi−1, π ∼ Bernoulli
(
πwi−1zi−1

)
with the prior distriutions over the parameters as

θd ∼ Dirichlet (α) πzw ∼ Beta (λ)
a|u| ∼ Beta (ρ) b|u| ∼ Gamma (�)

Like TNG, PDLDA assumes that the probability
of a changepoint ci+1 after the ith token depends on
the current topic zi and word wi. This causes the
length of a phrase to depend on its topic and con-
stituent words. The changepoints explicitly model
which words tend to start and end phrases in each
document. Depending on ci, zi is either set deter-
ministically to the preceding topic (when ci = 0)
or is drawn anew from θd (when ci = 1). In this
way, each topical phrase has a single topic drawn
from its document’s topic distribution. As in TNG,
the parameters πzw and θd are given conjugate priors
parameterized by λ and α.

Let u be a context vector consisting of the
phrase topic and the past m words: u , <
zi, wi−1, wi−2, . . . , wi−m >. The operator π(u) de-
notes the prefix of u, the vector with the rightmost
element of u removed. |u| denotes the length of u,
and ∅ represents an empty context. For practical rea-
sons, we pad u with a special start symbol when the
context overlaps a phrase boundary. For example,
the first word wi of a phrase beginning at a position
i necessarily has ci = 1; consequently, all the pre-
ceding words wi−j in the context vector are treated
as start symbols so that wi is effectively drawn from
a topic-specific unigram distribution.

In PDLDA, each token is drawn from a distribu-
tion conditioned on its context u. When m = 1,
this conditioning is analogous to TNG’s word dis-
tribution. However, in contrast with TNG, the word

217



  

H

Gz1

Gz1:honda

G
Øz

1

           honda

Gz1:civic

civic        
Gz2

Gz2:honda

z
2

           honda

Gz2:civic

civic        

Figure 3: Illustration of the hierarchical Pitman-Yor
process for a toy two-word vocabulary V = {honda,
civic} and two-topic (T = 2) model with m = 1.
Each node G in the tree is a Pitman-Yor process
whose base distribution is its parent node, andH is a
uniform distribution over V . When, for example, the
context is u = z1 : honda, the darkened path is fol-
lowed and the probability of the next word is calcu-
lated from the shaded node using Equation 1, which
combines predictions from all the nodes along the
darkened path.

distributions used are Pitman-Yor processes (PYPs)
linked together into a tree structure. This hierar-
chical construction creates the desired smoothing
among different contexts. The next section explains
this hierarchical distribution in more detail.

3.1 Hierarchical Pitman-Yor process

Words in PDLDA are emitted from Gu, which has
a PYP prior (Pitman and Yor, 1997). PYPs are a
generalization of the Dirichlet Process, with the ad-
dition of a discount parameter 0 ≤ a ≤ 1. When
considering the distribution of a sequence of words
w drawn iid from a PYP-distributed G, one can an-
alytically marginalize G and consider the resulting
conditional distribution of w given its parameters a,
b, and base distribution φ. This marginal can best
be understood by considering the distribution of any
wi|w1, . . . , wi−1, a, b, φ, which is characterized by
a generative process known as the generalized Chi-
nese Restaurant Process (CRP) (Pitman, 2002). In
the CRP metaphor, one imagines a restaurant with
an unbounded number of tables, where each table
has one shared dish (a draw from φ) and can seat an
unlimited number of customers. The CRP specifies a

process by which customers entering the restaurant
choose a table to sit at and, consequently, the dish
they eat. The first customer to arrive always sits at
the first table. Subsequent customers sit at an occu-
pied table k with probability proportional to ck − a
and choose a new unoccupied table with probabil-
ity proportional to b + ta, where ck is the number
of customers seated at table k and t is the number
of occupied tables in G. For our language modeling
purposes, “customers” are word tokens and “dishes”
are word types.

The hierarchical PYP (HPYP) is an intuitive re-
cursive formulation of the PYP in which the base
distribution φ is itself PYP-distributed. Figure 3
demonstrates this principle as applied to PDLDA.
The hierarchy forms a tree structure, where leaves
are restaurants corresponding to full contexts and in-
ternal nodes correspond to partial contexts. An edge
between a parent and child node represents a depen-
dency of the child on the parent, where the base dis-
tribution of the child node is its parent. This smooths
each context’s distribution like the Bayesian n-gram
model of Teh (2006), which is a Bayesian version
of interpolated Kneser-Ney smoothing (Chen and
Goodman, 1998). One ramification of this setup
is that if a word occurs in a context u, the shar-
ing makes it more likely in other contexts that have
something in common with u, such as a shared topic
or word.

The HPYP gives the following probability for a
word following the context u being w:

Pu(w | τ,a,b) =
cuw· − a|u|tuw
b|u| + cu··

+

b|u| + a|u|tu·

b|u| + cu··
Pπ(u)(w | τ,a,b) (1)

where Pπ(∅)(w|τ,a,b) = G∅(w), cuw· is the num-
ber of customers eating dish w in restaurant u, and
tuw is the number of tables serving w in restau-
rant u, and τ represents the current seating arrange-
ment. Here and throughout the rest of the paper, we
use a dot to indicate marginal counts: e.g., cuw· =∑

k cuwk where cuwk is the number of customers
eating w in u at table k. The base distribution of
G∅ was chosen to be uniform: H(w) = 1/V with V
being the vocabulary size. The above equation an in-
terpolation between distributions of context lengths

218



|u|, |u| − 1, . . . 0 and realizes the sharing of statisti-
cal strength between different contexts.

3.2 Inference

In this section, we describe Markov chain Monte
Carlo procedures to sample from P (z, c, τ |w, U),
the posterior distribution over topic assignments z,
phrase boundaries c, and seating arrangements τ
given an observed corpus w. Let U be short-
hand for α, λ,a,b. In order to draw samples
from P (z, c, τ |w, U), we employ a Metropolis-
Hastings sampler for approximate inference. The
sampler we use is a collapsed sampler (Griffiths and
Steyvers, 2004), wherein θ, φ, and G are analyti-
cally marginalized. Because we marginalize eachG,
we use the Chinese Restaurant Franchise representa-
tion of the hierarchical PYPs (Teh, 2006). However,
rather than onerously storing the table assignment
of every token in w, we store only the counts of how
many tables there are in a restaurant and how many
customers are sitting at each table in that restaurant.
We refer the inquisitive reader to the appendix of
Teh (2006) for further details of this procedure.

Our sampling strategy for a given token i in doc-
ument d is to jointly propose changes to the change-
point ci and topic assignment zi, and then to the
seating arrangement τ . Recall that according to the
model, if ci = 0, zi = zi−1; otherwise zi is gen-
erated from the topic distribution for document d.
Since the topic assignment remains the same until a
new changepoint at a position i′ is reached, each to-
ken wj for j from position i until i′ − 1 will depend
on zi because for these j, zj = zi. We call this set of
tokens the phrase suffix of the ith token and denote
it s(i). More formally, let s(i) be the maximal set
of continuous indices j ≥ i including i such that, if
j 6= i, cj = 0. That is, s(i) are the indices compris-
ing the remainder of the phrase beginning at position
i. In addition, let x(i) indicate the extended suffix
version of s(i) which includes one additional index:
x(i) , {s(i) ∪ {max (s(i)) + 1}}. In addition to
the words in the suffix s(i), the changepoint indica-
tor variables cj for j in x(i) are also conditioned on
zi. To make these dependencies more explicit, we
refer to zs(i) , zj ∀j ∈ s(i), which are constrained
by the model to share a topic.

The variables that depend directly on zi, ci are
zs(i),ws(i), cx(i). The proposal distribution first

draws from a multinomial over T + 1 options: one
option for ci = 0, zi = zi − 1; and one for ci = 1
paired with each possible zi = z ∈ 1 . . . T . This is
given by
P (zs(i), ci | z¬s(i), c¬i, τ¬s(i),w, U) ∝

∏
j∈x(i)

n
¬x(j)
zj−1wj−1cj + λcj

n
¬x(j)
zj−1wj−1· + λ0 + λ1∏

j∈s(i)

P (zj | c, z¬s(j), U) Puj (wj | τ¬s(i), U)

with

P (zj | c, z¬s(j), U) =


n
¬s(j)
dzj

+ α

n
¬s(j)
d· + Tα

if cj = 1

δzj ,zj−1 if cj = 0

where Puj (wj | τ¬s(i), U) is given by Equation 1,
T is the number of topics, n¬s(j)dz is the number of
phrases in document d that have topic z when s(j)’s
assignment is excluded, and n¬s(j)zwc is the number of
times a changepoint c has followed a word w with
topic z when s(j)’s assignments are excluded.

After drawing a proposal for ci, zs(i) for token i,
the sampler adds a customer eating wi to a table
serving wi in restaurant ui. An old table k is se-
lected with probability ∝ max(0, cuwk − a|u|) and
a new table is selected with probability ∝ (b|ui| +
a|ui|tui·)Pπ(u)(wi).

Let z′s(i), c
′
i, τ
′
s(i) denote the proposed change to

zs(i), ci, τs(i). We accept the proposal with probabil-
ity min(A, 1) where

A =
P̂ (z′s(i), c

′
i, τ
′
s(i)) Q(zs(i), ci, τs(i))

P̂ (zs(i), ci, τs(i)) Q(z
′
s(i), c

′
i, τ
′
s(i))

where Q is the proposal distribution and P̂ is the
true unnormalized distribution. P̂ differs from Q in
that the probability of each word wj and the seating
arrangement depends only on ¬s(j), as opposed to
the simplification of using ¬s(i). Almost all propos-
als are accepted; hence, this theoretically motivated
Metropolis Hastings correction step makes little dif-
ference in practice.

Because the parameters a and b have no intuitive
interpretation and we lack any strong belief about
what they should be, we give them vague priors
where ρ1 = ρ2 = 1 and �1 = 10, �2 = .1. We then

219



interleave a slice sampling algorithm (Neal, 2000)
between sweeps of the Metropolis-Hastings sampler
to learn these parameters. We chose not to do infer-
ence on α in order to make the tests of our model
against TNG more equitable.

4 Related Work

An integral part of modeling topical phrases is the
relaxation of the bag-of-words assumption in LDA.
There are many models that make this relaxation.
Among them, Griffiths and Steyvers (2005) present
a model in which words are generated either con-
ditioned on a topic or conditioned on the previous
word in a bigram, but not both. They use this to
model human performance on a word-association
task. Wallach (2006) experiments with incorpo-
rating LDA into a bigram language model. Her
model uses a hierarchical Dirichlet to share param-
eters across bigrams in a topic in a manner similar
to our use of PYPs, but it lacks a notion of the topic
being shared between the words in an n-gram. The
Hidden Topic Markov Model (HTMM) (Gruber et
al., 2007) assumes that all words in a sentence have
the same topic, and consecutive sentences are likely
to have the same topic. By dropping the indepen-
dence assumption among topics, HTMM is able to
achieve lower perplexity scores than LDA at mini-
mal additional computational costs. These models
are unconcerned with topical n-grams and thus do
not model phrases.

Johnson (2010) presents an Adaptor Grammar
model of topical phrases. Adaptor Grammars are
a framework for specifying nonparametric Bayesian
models over context-free grammars in which certain
subtrees are “memoized” or remembered for reuse.
In Johnson’s model, subtrees corresponding to com-
mon phrases for a topic are memoized, resulting in a
model in which each topic is associated with a distri-
bution over whole phrases. While it is a theoretically
elegant method for finding topical phrases, for large
corpora we found inference to be impractically slow.

5 Phrase Intrusion Experiment

Perplexity is the typical information theoretic mea-
sure of language model quality used in lieu of ex-
trinsic measures, which are more difficult and costly
to run. However, it is well known that perplexity

Trial 1 of 80 
countries 

britain 
france 

museum 

Trial 2 of 80 
air force 

beverly hills 
defense minister 

u.s. troops 

Trial 3 of 80 
fda 

book 
smoking 

cigarettes 

Trial 4 of 80 
roman catholic church 
air traffic controllers 
roman catholic priest 
roman catholic bishop 

Figure 4: Experimental setup of the phrase intrusion
experiment in which subjects must click on the n-
gram that does not belong.

scores may negatively correlate with actual quality
as assessed by humans (Chang et al., 2009). With
that fact in mind, we expanded the methodology of
Chang et al. (2009) to create a “phrase intrusion”
task that quantitatively compares the quality of the
topical n-gram lists produced by our model against
those of other models.

Each of 48 subjects underwent 80 trials of a web-
based experiment on Amazon Mechanical Turk, a
reliable (Paolacci et al., 2010) and increasingly com-
mon venue for conducting online experiments. In
each trial, a subject is presented with a randomly or-
dered list of four n-grams (cf. Figure 4). Each sub-
ject’s task is to select the intruder phrase, a spurious
n-gram not belonging with the others in the list. If,
other than the intruder, the items in the list are all
on the same topic, then subjects can easily identify
the intruder because the list is semantically cohesive
and makes sense. If the list is incohesive and has no
discernible topic, subjects must guess arbitrarily and
performance is at random.

To construct each trial’s list, we chose two top-
ics z and z′ (z 6= z′), then selected the three most
probable n-grams from z and the intruder phrase, an
n-gram probable in z′ and improbable in z. This
design ensures that the intruder is not identifiable
due solely to its being rare. Interspersed among the
phrase intrusion trials were several simple screen-
ing trials intended to affirm that subjects possessed
a minimal level of attentiveness and reading com-
prehension. For example, one such screening trial
presented subjects with the list banana, apple, tele-
vision, orange. Subjects who got any of these trials

220



Unigrams Bigrams Trigrams
0

0.2

0.4

0.6

0.8

1

M
o

d
e

l 
P

re
c
is

io
n

 

 

PDLDA
TNG
LDA

(a) Word repetition allowed within a list.
Bigrams Trigrams

0

0.2

0.4

0.6

0.8

1

M
o

d
e

l 
P

re
c
is

io
n

 

 

PDLDA
TNG

(b) Word repetition not allowed.

Figure 5: An across-subject measure of the ability to detect intruders as a function of n-gram size and model.
Excluding trials with repeated words does not qualitatively affect the results.

wrong were excluded from our analyses.

Each subject was presented with trials constructed
from the output of PDLDA and TNG for unigrams,
bigrams, and trigrams. For unigrams, we also tested
the output of the original smoothed LDA (Blei et
al., 2003). The experiment was conducted twice for
a 2,246-document subset of the TREC AP corpus
(Blei et al., 2003; Harman, 1992): the first time pro-
ceeded as described above, but the second time did
not allow word repetition within a topic’s list. The
topical phrases found by TNG and PDLDA often
revolve around a central n-gram, with other words
pre- or post- appended to it. In this intrusion exper-
iment, any n-gram not containing the central word
or phrase may be trivially identifiable, regardless of
its relevance to the topic. For example, the intruder
in Trial 4 of Figure 4 is easily identifiable even if
a subject does not understand English. This second
experiment was designed to test whether our conclu-
sions hinge on word repetition.

We used the MALLET toolbox (McCallum,
2002) for the implementations of LDA and TNG.
Each model was run with 100 topics for 5,000 it-
erations. We set m = 2, α = .01, β = .01, λ = 1,
π1 = π2 = 1, ρ1 = 10, and ρ2 = .1. For all mod-
els, we treated certain punctuation as the start of a
phrase by setting cj = 1 for all tokens j immediately
following periods, commas, semicolons, and excla-
mation and question marks. To reduce runtime, we
removed stopwords occuring in the MALLET tool-

box’s stopword list. Because TNG and LDA had
trouble with single character words not in the sto-
plist, we manually removed them before the experi-
ment. Any token immediately following a removed
word was treated as if it were the start of a phrase.

As in Chang et al. (2009), performance is mea-
sured via model precision, the fraction of subjects
agreeing with the model. It is defined as MPm,nk =∑
s
1(im,nk,s = ω

m,n
k,s )/S where ω

m,n
k,s is the index of

the intruding n-gram for subject s among the words
generated from the kth topic of model m, im,nk,s is the
intruder selected by s, and S is the number of sub-
jects. The model precisions are shown in Figure 5.
PDLDA achieves the highest precision in all condi-
tions. Model precision is low in all models, which is
a reflection of how challenging the task is on a small
corpus laden with proper nouns and low-frequency
words. Figure 5b demonstrates that the outcome of
the experiment does not depend strongly on whether
the topical n-gram lists have repeated words.

6 Conclusion

We presented a topic model which simultaneously
segments a corpus into phrases of varying lengths
and assigns topics to them. The topical phrases
found by PDLDA are much richer sources of in-
formation than the topical unigrams typically pro-
duced in topic modeling. As evidenced by the
phrase-intrusion experiment, the topical n-gram lists
that PDLDA finds are much more interpretable than

221



those found by TNG.
The formalism of Bayesian changepoint detection

arose naturally from the intuitive assumption that the
topic of a sequence of tokens changes periodically,
and that the tokens in between changepoints com-
prise a phrase. This formalism provides a principled
way to discover phrases within the LDA framework.
We presented a model embodying these principles
and showed how to incorporate dependent Pitman-
Yor processes into it.

Acknowledgements

The first author is supported by an NSF Graduate
Research Fellowship. The first and second authors
began this project while working at J.D. Power &
Associates. We are indebted to Michael Mozer, Matt
Wilder, and Nicolas Nicolov for their advice.

References
Ryan Prescott Adams and David J.C. MacKay. 2007.

Bayesian online changepoint detection. Technical re-
port, University of Cambridge, Cambridge, UK.

David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation. Jour-
nal of Machine Learning Research, 3:993–1022.

Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems (NIPS).

Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Center for Research
in Computing Technology, Harvard University.

T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235, April.

Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in Neural Information Processing
Systems 17, pages 537–544. MIT Press.

Thomas L. Griffiths, Joshua B. Tenenbaum, and Mark
Steyvers. 2007. Topics in semantic representation.
Psychological Review, 114:211–244.

Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007.
Hidden topic Markov models. Journal of Machine
Learning Research - Proceedings Track, 2:163–170.

Donna Harman. 1992. Overview of the first text re-
trieval conference (trec–1). In Proceedings of the
first Text REtrieval Conference (TREC–1), Washing-
ton DC, USA.

Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and the
Structure of Proper Names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148–1157, Uppsala, Sweden, July.
Association for Computational Linguistics.

Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211 – 240.

Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.

Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31:705–767.

Gabriele Paolacci, Jesse Chandler, and Panagiotis G.
Ipeirotis. 2010. Running experiments on Amazon
Mechanical Turk. Judgment and Decision Making,
5(5):411–419.

J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Annals of Probability, 25:855–900.

J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, Uni-
versity of California at Berkeley.

Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Lillian Lee and
Donna Harman, editors, Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Language
Processing, pages 100–108.

Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, ACL-
44, pages 985–992, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.

Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of the 23rd International
Conference on Machine Learning, pages 977–984.

Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings of
the 7th IEEE International Conference on Data Min-
ing.

222


