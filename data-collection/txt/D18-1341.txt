



















































Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3048â€“3053
Brussels, Belgium, October 31 - November 4, 2018. cÂ©2018 Association for Computational Linguistics

3048

Automatic Post-Editing of Machine Translation:
A Neural Programmer-Interpreter Approach

Thuy-Trang Vu
Faculty of Information Technology

Monash University, Austrlia
vuth0001@student.monash.edu

Gholamreza Haffari
Faculty of Information Technology

Monash University, Austrlia
first.last@monash.edu

Abstract

Automated Post-Editing (PE) is the task of
automatically correcting common and repeti-
tive errors found in machine translation (MT)
output. In this paper, we present a neural
programmer-interpreter approach to this task,
resembling the way that humans perform post-
editing using discrete edit operations, which
we refer to as programs. Our model out-
performs previous neural models for inducing
PE programs on the WMT17 APE task for
German-English up to +1 BLEU score and -
0.7 TER scores.

1 Introduction

Automatic post-editing (APE) is the automated
task that aims to correct common systematic
and repetitive errors found in machine translation
(MT) output. APE systems can also be used to
adapt general-purpose MT output to specific do-
mains without re-training MT models, or to incor-
porate information which is not available or ex-
pensive to compute at MT decoding stage. Post-
editing is considered as the modification process
of a machine translated text with a minimum labor
effort rather than re-translation from scratch.

Previous studies in neural APE have primarily
concentrated on formalizing APE as a monolin-
gual MT problem in the target language, with or
without conditioning on the source sentence (Pal
et al., 2016; Chatterjee et al., 2017). MT approach
has suffered from over-correction where APE sys-
tem performs unnecessary correction leading to
paraphasing and the degradation of the output
quality (Bojar et al., 2016, 2017).

Recent works (LibovickyÌ et al., 2016; Berard
et al., 2017) have attempted to learn the predict
of sequence of post-editing operations, e.g. inser-
tion and deletion, to induce APE programs to turn
the machine translated text into the desired out-

put. Previous program induction approaches suf-
fer from over-cautiousness, where the APE system
tends to keep the machine translated text without
any modification (Bojar et al., 2017).

In this paper, we propose a programmer-
interpreter approach to the APE task to address the
over-cautiousness problem. Our architecture in-
cludes an interpreter module, which executes the
previous editing action before generating the next
one. This is in contrast to the previous work,
where the full program is induced before it is
executed. The ability of execution immediately
at every time step provides a proper condition-
ing context based on the actual partial edited sen-
tence to assist better prediction of the next op-
eration. Moreover, the execution module can be
pre-trained on monolingual target text, enabling
our architecture to benefit from monolingual data
in addition to PE data, which is hard to obtain.
Our model is jointly trained on translation task
and APE program induction task. The multi-task
architecture allows the model to reconstruct the
source-target alignment of the black-box MT sys-
tem and inject it into post-editing task.

We compare our programmer-interpreter archi-
tecture against previous works on the English-
German APE based on the data for this task in
WMT16 and WMT17. Compared to the previous
work on APE program induction, our architecture
achieves improvements up to +1 BLEU and -0.7
TER scores. Our analysis also shows that APE
programs generated by our model are not only bet-
ter at correcting errors but also attempt to perform
more editing actions.

2 Related Work

Pal et al. (2016) has applied the SEQ2SEQ model
to APE. Their monolingual MT learned to post-
edit English-Italian Google Translation output and



3049

Figure 1: An example of PE program when executing
on MT would return the PE reference. The color de-
notes the alignment between MT, reference PE and the
APE program. The number subscript shows the edit
position in original MT sentence.

was able to reduce the preposition related er-
rors. Blindly performing edition over MT out-
put, the monolingual APE has difficulty to cor-
rect missing word or information in the source
sentence. Neural multi-source MT architectures
are applied to better capture the connection be-
tween the source sentence/machine translated text
and the PE output (LibovickyÌ et al., 2016; Varis
and Bojar, 2017; Junczys-Dowmunt and Grund-
kiewicz, 2017). Chatterjee et al. (2017) ensemble
several different models including monolingual
MT (TGTâ†’PE), bilingual MT (SRCâ†’PE), and
multi-source (SRC,TGTâ†’PE). LibovickyÌ et al.
(2016); Berard et al. (2017) have proposed learn-
ing to predict the sequence of edit operations, aka
the program, to produce the post-editing sentence
(c.f. Â§3).

Our work is motivated by Ling et al. (2017)
on learning to indirectly solve an algebraic word
problem by inducing a program which generates
the answer together with an explanation. It further
builds up on recent work on neural programmer-
interpreter (Reed and De Freitas, 2016), where a
neural network programmer learns to program an
interpreter. The architecture is then trained using
expert action trajectories as programs.

3 The NPI-APE Approach

Given a source sentence sss and a machine trans-
lated sentence mmm, the goal is to find a post-
edited sentence ttt = argmaxtttâ€² Pape(ttt

â€²|mmm,sss) where
Pape(.) is our probabilistic APE model. In our pro-
posed approach, we aim to find an editing action
sequence zzz to execute in order to generate the de-
sired post-edited sentence,

Pape(ttt|mmm,sss) =
âˆ‘
zzzâˆˆZ

Pape(ttt, zzz|mmm,sss).

We decompose the joint probability of a program
and an output as:

Pape(zzz, ttt|mmm,sss) =
|zzz|âˆ
i=1

Pprog(zi|tttâ‰¤jiâˆ’1 ,mmm,sss) (1)

Ã—Pintp(tji |mki , zi) (2)

where Pprog(zi|tttâ‰¤jiâˆ’1 ,mmm,sss) is the programmerâ€™s
probability in producing the next edit operation zi
given the post edited output tttâ‰¤jiâˆ’1 generated from
the operations so far zzzâ‰¤iâˆ’1, and Pintp(tji |mki , zi)
is the interpreterâ€™s probability of outputing tji
given the edit operation zi and the MT word mki .

Following Berard et al.(2017), our action se-
quence is performed on the MT sentence from left
to right. At each position, we can take one of
the following editing operations: (i) KEEP to keep
the word and go to the next word, (ii) DELETE to
delete the word and go to the next word, (iii) IN-
SERT(WORD) to insert a new WORD and stay in
that position, or (iv) STOP to terminate the pro-
cess. In other words, the size of the operation set
equals the size of the target vocabulary plus three,
where we add the symbols KEEP, DELETE, and
STOP as new tokens. Furthermore, ji is the num-
ber of KEEP and INSERT(WORD) operations, and
ki is the number of KEEP and DELETE operations
in the sequence of operations zzzâ‰¤i. This hard atten-
tion mechanism is the outcome of the semantics of
the operations, and injects task knowledge into the
model. Moreover, Pintp(t|m, z) is 1 if the output
word t is consistent with performing the operation
z on m, and zero otherwise.

Our decomposition of the joint probability of
a program and post-edited output is distinguished
from that proposed in (Berard et al., 2017),
Pape(ttt, zzz|mmm,sss) = Pintp(ttt|zzz,mmm)Pprog(zzz|mmm,sss). Cru-
cially, in our decomposition (eqns 1 and 2), the
programming and interpreting are interleaved at
each position, whereas in (Berard et al., 2017) the
programming is fully done before the interpreta-
tion phase and they are independent.

3.1 Neural Architecture and Joint Training
The architecture consists of three components (i)
A SEQ2SEQ model to translate the source sen-
tence to the target in the forced-decoding mode
(MT), (ii) A SEQ2SEQ model to incrementally
generate the sequence of edit operations (Action
Generator), and (iii) An RNN to summarize the
post edited sequence of words produced from the
execution of actions generated so far (Interpreter).



3050

ğ‘‚ğ‘‚ğ‘ƒğ‘ƒ

ğ‘ƒğ‘ƒğ¸ğ¸

ğ‘§ğ‘§1

ğ‘€ğ‘€ğ‘‡ğ‘‡ğ‘‡ â„ğ‘‡1 â„ğ‘‡ğ‘˜ğ‘˜ğ‘–ğ‘– â„ğ‘‡ğ‘›ğ‘›

ğ‘§ğ‘§ğ‘–ğ‘–âˆ’1 ğ‘§ğ‘§ğ‘–ğ‘–

ğ‘¡ğ‘¡1 ğ‘¡ğ‘¡ğ‘—ğ‘—ğ‘–ğ‘–âˆ’1

ğ‘£ğ‘£1 ğ‘£ğ‘£ğ‘—ğ‘—ğ‘–ğ‘–âˆ’1

Interpreter

Action Generator

ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡

ğ‘ğ‘ğ‘˜ğ‘˜ğ‘–ğ‘–

ğ‘”ğ‘”ğ‘˜ğ‘˜ğ‘–ğ‘–

â„ğ‘›ğ‘›â„ğ‘™ğ‘™â„1

SRC-TGT Translator

ğ‘†ğ‘†ğ‘…ğ‘…ğ‘…ğ‘…

ğ‘”ğ‘”1 ğ‘”ğ‘”ğ‘‡ğ‘–ğ‘–ğ‘”ğ‘”ğ‘‡ğ‘–ğ‘–âˆ’1ğ‘”ğ‘”ğ‘‡1

execute

Figure 2: Our proposed NPI-APE model

The encoder and decoder of the MT compo-
nent are a bidirectional and unidirectional LSTM,
whose states are denoted by hhhl and gggk, respec-
tively. Similarly, the encoder and decoder of the
AG (Action Generator) component are a bidirec-
tional and unidirectional LSTM, whose states are
denoted by hhhâ€²k and ggg

â€²
i, respectively. The states of

the unidirectional RNN in the interpreter are de-
noted by vvvj .

The next edit operation zi is generated from the
decoder state of the AG gggâ€²i (see Figure 2), which is
computed from the previous state gggâ€²iâˆ’1 and a con-
text including the following: (i) hhhâ€²ki where ki is
the index of the MT output word currently pro-
cessed, (ii) cccki which is the context vector from the
MT component when generating the current target
word, and (iii) vvvjiâˆ’1 which is the last hidden state
of the interpreter RNN encoding the post edited
sentence generated so far.

The model is trained jointly for the translation
task (SRCâ†’TGT), and for the post editing task
(SRC,TGTâ†’OP,PE). For the training data, we
compute the lowest-cost sequence of editing op-
erations (OP) using dynamic programming, where
the cost of insertion and deletion are 1.

4 Experiments
Dataset. We evaluate the proposed approach on
the English-to-German (En-De) post-editing task
in the IT domain using the data from WMT161 and
WMT17.2 The official WMTâ€™16 and WMTâ€™17

1http://www.statmt.org/wmt16/ape-task.html
2http://www.statmt.org/wmt17/ape-task.html

dataset contains 12K and 11K post-editing triplets
(English, translated German, post-edited German)
respectively in IT domain. We concatenated
them to an 23K triplets. A synthetic corpus
of 500K triplets (Junczys-Dowmunt and Grund-
kiewicz, 2016) is also available as additional train-
ing data. We performed our experiment in two dif-
ferent settings with and without synthetic data for
comparison with Berard et al. (2017).

The RNN in the interpreter component can be
thought of as a language model. This paves
the way to pre-train it using monolingual text.
We collect in-domain IT text from OPUS3 from
the following sections: GNOME, KDE, KDEdoc,
OpenOffice, OpenOffice3, PHP and Ubuntu. Af-
ter tokenizing, filtering out sentences containing
special characters, and removing duplications, we
obtain around 170K sentences.

Setup. There are three components in our archi-
tecture: machine translation (MT), action genera-
tor (AG), and interpreter (LM). We compare our
MT+AG+LM architecture against MT+AG4 Be-
rard et al. (2017) which does not have the LM
component. The size of the hidden dimensions
(LSTMs in the MT and AG, and simple RNN in
the LM component) as well as word embedding in
these models is set to 128.

Furthermore, we compare against monolin-
gual SEQ2SEQ (TGTâ†’PE) as well as the multi-
source SEQ2SEQ (SRC+TGTâ†’PE) (Varis and
Bojar, 2017). Monolingual SEQ2SEQ (TGTâ†’PE)
model is an attentional SEQ2SEQ model (Bah-
danau et al., 2015) that takes target sentence as
input and outputs desired PE sentence. In multi-
source SEQ2SEQ (SRC+TGTâ†’PE), we use two
encoders for source and target sentences and con-
catenate their context vectors. In both models,
the encoder and decoder contain a single layer
of bidirectional and unidirectional LSTM respec-
tively. The size of the LSTM hidden dimensions
and word embedding in these models is set to 256
and 128, respectively. This ensures almost the
same number of parameters (âˆ¼13M) in all archi-
tectures.

Training. We use a multi-task scenario to jointly
train the parameters of the components in MT+AG
as well as MT+AG+LM models. For the latter, we
warm start the embedding of the target words with

3http://opus.nlpl.eu/
4The AG decoder in MT+AG conditions a state on the last

generated action as well.



3051

dev test2016 test2017
Model TER BLEU TER BLEU TER BLEU

Original MT 24.81 62.92 24.76 62.11 24.48 62.49

12
K

TGT â†’ PE 63.76 21.32 60.96 22.11 65.13 18.13
SRC+TGT â†’ PE 51.41 34.04 48.27 35.24 50.98 31.52
MT+AG 23.74 65.95 23.53 65.22 23.77 64.34
MT+AG+LM 23.36â€  66.24 23.24â€  65.53â€  23.45â€  64.65â€ 

50
0K

+1
2K TGT â†’ PE 50.91 30.88 48.62 32.55 52.07 27.98SRC+TGT â†’ PE 30.97 53.97 30.20 53.92 32.82 50.30

MT+AG 22.82 66.51 22.87 65.67 23.58 64.35
MT+AG+LM 22.67 67.17â€  22.53â€  66.30â€  23.03â€  65.31â€ 

23
K

TGT â†’ PE 57.02 27.87 55.52 27.8 60.06 22.78
SRC+TGT â†’ PE 38.06 47.42 36.61 47.93 39.86 43.52
MT+AG 23.10 66.60 22.82 66.15 23.14 65.19
MT+AG+LM 22.61â€  67.19â€  22.42â€  66.53â€  22.84â€  65.52?â€ 

50
0K

+2
3K TGT â†’ PE 48.89 34.29 47.12 34.75 51.13 29.59SRC+TGT â†’ PE 28.61 57.47 27.79 57.61 30.34 53.26

MT+AG 22.38 67.34 22.14 66.53 22.71 65.34
MT+AG+LM 21.99?â€  67.50? 22.07? 66.67? 22.58? 65.50

Table 1: TER and BLEU scores of our model (MT+AG+LM) v.s. the rest on various data conditions for the EN-DE
post-editing task. bold: Best results within a data condition; ?: Best results across data conditions

; â€ : Statistically significant compared to MT+AG with pvalue â‰¤ 0.05.

those obtained from training the LM component
on the monolingual text5.

All models are trained with SGD, where the
learning rate is initialised to 1 and decays after
each epoch. The learning rate is decayed 0.8 after
every epoch for model trained with official post-
editing data, and 0.5 every half epoch for model
with synthetic data. All models use the same vo-
cabulary on the same data condition. The Vocabu-
lary size is 30K for large dataset experiments, and
27K/19K for the 23K/12K data conditions. In all
experiments, the best model is selected based on
TER on the validation set. For decoding, we use
beam search with the beam size of 10.

4.1 Results

Table 1 shows the result on different training
datasets to compare our model against the base-
lines. Original MT is the strong standard do-
nothing baseline, i.e. copying the MT trans-
lation as the PE output. In all settings, our
MT+AG+LM models outperforms the MT+AG
and monolingual/multi-source SEQ2SEQ models.
Specifically, our model outperform MT+AG in
500K+12K training condition by almost 1 BLEU
score on test2017.

As expected, the models trained on 23K data
perform better than those trained on 12K; further
gains are obtained by adding 500K synthetic data.

5The MT, AG, and LM components share the target word
embedding table in our model. Similarly, MT and AG com-
ponents share the target embedding in the MT+AG model.

Sentences Actions
Model Mod Prec. Mod. Prec.

de
v

MT+AG 23K 546 61.17% 986 60.45%
MT+AG+LM 23K 558 65.59% 1098 65.21%
MT+AG 500+23K 673 61.81% 1477 62.02%
MT+AG+LM 500+23K 682 65.84% 1560 68.21%

te
st

20
16

MT+AG 23K 1039 60.92% 1766 62.63%
MT+AG+LM 23K 1041 66.09% 1919 65.61%
MT+AG 500+23K 1269 63.04% 2728 63.71%
MT+AG+LM 500+23K 1251 63.87% 2814 63.50%

te
st

20
17

MT+AG 23K 897 55.85% 1535 57.79%
MT+AG+LM 23K 952 61.24% 1853 60.44%
MT+AG 500+23K 1182 54.99% 2599 55.71%
MT+AG+LM 500+23K 1180 56.78% 2653 59.67%

Table 2: Sentence precision and action precision of
models trained on 23K and 500K+23K dataset.

Interestingly, training MT+AG and MT+AG+LM
models on 23K data lead to better TER/BLEU
than those trained on 500K+12K. This implies
the importance of in-domain training data, as the
synthetic corpus is created using general domain
Common-Crawl corpus.

4.2 Analysis
We perform fine-grained analysis of the changes
made by our model vs MT+AG in order to un-
derstand the sources of improvements. For dif-
ferent data conditions, Table 2 shows the number
of modified sentences by each model as well as
the sentence level precision defined as the fraction
of sentences with improved TER. Moreover, it re-
ports the total number of actions generated by the
model on sentences with improved TER, as well
as the precision of such actions, i.e. the fraction of
those observed in the ground truth action trajecto-



3052

ries.
As reported in Berard et al. (2017), one ma-

jor challenge of predicting action sequences is the
class imbalance. The model is often too conserva-
tive about its edits, and tends to use the KEEP far
more than the INSERT and DELETE actions. Our
Programmer-Interpreter model tackles this prob-
lem, as evidenced by its comparable number of
modified sentences, but with higher sentence and
action level precision in almost all cases.

5 Conclusion

In this paper, we have presented a neural
programmer-interpreter approach to automated
post-editing of MT output. Our approach inter-
leaves generating the sequence of edit actions by a
programmer component, and executing those ac-
tions with an interpreter component. This leads
to better capturing the history of the past gener-
ated actions when generating the next action. Our
approach achieves up to +1 BLEU and -.7 TER
improvement compared to a variant in which pro-
gramming is not interleaved with execution. Fu-
ture work includes inducing macro-actions com-
posed of simpler building block actions.

Acknowledgments

The authors are grateful to the anonymous review-
ers for their helpful comments and corrections.
This work was supported by the Multi-modal Aus-
tralian ScienceS Imaging and Visualisation Envi-
ronment (MASSIVE) (www. massive.org.au), and
partially supported by a Google Faculty Award to
GH and the Australian Research Council through
DP160102686.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
International Conference on Learning Representa-
tions (ICLR) 2015.

Alexandre Berard, Laurent Besacier, and Olivier
Pietquin. 2017. LIG-CRIStAL submission for the
WMT 2017 automatic post-editing task. In Proceed-
ings of the Second Conference on Machine Trans-
lation, Volume 2: Shared Task Papers, pages 623â€“
629, Copenhagen, Denmark. Association for Com-
putational Linguistics.

OndrÌŒej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Shujian Huang,

Matthias Huck, Philipp Koehn, Qun Liu, Varvara
Logacheva, Christof Monz, Matteo Negri, Matt
Post, Raphael Rubino, Lucia Specia, and Marco
Turchi. 2017. Findings of the 2017 conference
on machine translation (WMT17). In Proceedings
of the Second Conference on Machine Translation,
Volume 2: Shared Task Papers, pages 169â€“214,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

OndrÌŒej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
on machine translation. In Proceedings of the First
Conference on Machine Translation, pages 131â€“
198, Berlin, Germany. Association for Computa-
tional Linguistics.

Rajen Chatterjee, M. Amin Farajian, Matteo Negri,
Marco Turchi, Ankit Srivastava, and Santanu Pal.
2017. Multi-source neural automatic post-editing:
FBKâ€™s participation in the WMT 2017 APE shared
task. In Proceedings of the Second Conference on
Machine Translation, Volume 2: Shared Task Pa-
pers, pages 630â€“638, Copenhagen, Denmark. Asso-
ciation for Computational Linguistics.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2016. Log-linear combinations of monolingual and
bilingual neural machine translation models for au-
tomatic post-editing. In Proceedings of the First
Conference on Machine Translation, pages 751â€“
758, Berlin, Germany. Association for Computa-
tional Linguistics.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2017. An exploration of neural sequence-to-
sequence architectures for automatic post-editing.
In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 120â€“129. Asian Feder-
ation of Natural Language Processing.

JindrÌŒich LibovickyÌ, JindrÌŒich Helcl, Marek TlustyÌ,
OndrÌŒej Bojar, and Pavel Pecina. 2016. CUNI sys-
tem for WMT16 automatic post-editing and multi-
modal translation tasks. In Proceedings of the First
Conference on Machine Translation, pages 646â€“
654, Berlin, Germany. Association for Computa-
tional Linguistics.

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 158â€“167, Van-
couver, Canada. Association for Computational Lin-
guistics.



3053

Santanu Pal, Sudip Kumar Naskar, Mihaela Vela, and
Josef van Genabith. 2016. A neural network based
approach to automatic post-editing. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics, volume 2, pages 281â€“
286.

Scott Reed and Nando De Freitas. 2016. Neural
programmer-interpreters. In Proceedings of Inter-
national Conference on Learning Representations
(ICLR) 2016.

Dusan Varis and OndrÌŒej Bojar. 2017. CUNI system for
WMT17 automatic post-editing task. In Proceed-
ings of the Second Conference on Machine Trans-
lation, Volume 2: Shared Task Papers, pages 661â€“
666, Copenhagen, Denmark. Association for Com-
putational Linguistics.


