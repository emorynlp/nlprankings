



















































Interactive Error Resolution Strategies for Speech-to-Speech Translation Systems


Proceedings of the SIGDIAL 2013 Conference, pages 142–144,
Metz, France, 22-24 August 2013. c©2013 Association for Computational Linguistics

Interactive Error Resolution Strategies for  

Speech-to-Speech Translation Systems 

 

 

Rohit Kumar, Matthew Roy, Sankaranarayanan Ananthakrishnan,  

Sanjika Hewavitharana, Frederick Choi 

Speech, Language and Multimedia Business Unit 

Raytheon BBN Technologies 

Cambridge, MA, USA 

{rkumar, mroy, sanantha, shewavit, fchoi}@bbn.com 

 

  

 

Abstract
1
 

In this demonstration, we will showcase 

BBN’s Speech-to-Speech (S2S) transla-

tion system that employs novel interac-

tion strategies to resolve errors through 

user-friendly dialog with the speaker. 

The system performs a series of analysis 

on input utterances to detect out-of-

vocabulary (OOV) named-entities and 

terms, sense ambiguities, homophones, 

idioms and ill-formed inputs. This analy-

sis is used to identify potential errors and 

select an appropriate resolution strategy. 

Our evaluation shows a 34% (absolute) 

improvement in cross-lingual transfer of 

erroneous concepts in our English to Ira-

qi-Arabic S2S system. 

1 Introduction 

Great strides have been made in Speech-to-

Speech (S2S) translation systems that facilitate 

cross-lingual spoken communication (Stallard et. 

al., 2011). However, in order to achieve broad 

domain coverage and unrestricted dialog capabil-

ity, S2S systems need to be transformed from 

passive conduits of information to active partici-

pants in cross-lingual dialogs. These active par-

ticipants must detect key causes of communica-

tion failures and recover from them in an effi-

cient, user-friendly manner. 

                                                 
Disclaimer: This paper is based upon work supported by the 

DARPA BOLT Program. The views expressed are those of 

the authors and do not reflect the official policy or position 

of the Department of Defense or the U.S. Government. 
 

Distribution Statement A (Approved for Public Release, 

Distribution Unlimited) 

Our ongoing work on eyes-free S2S systems is 

focused on detecting three types of errors that 

affect S2S systems. First, out-of-vocabulary 

(OOV) words are misrecognized as phonetically 

similar words that do not convey the intended 

concept. Second, ambiguous words such as hom-

ophones and homographs often lead to recogni-

tion and translation errors. Also, unseen idioms 

produce erroneous literal translations. Third, user 

errors such as mispronunciations and incomplete 

utterances lead to ASR errors. We will demon-

strate our interactive error resolution strategies to 

recover from each of these error types. 

Section 2 presents our system architecture. 

Section 3 describes nine interactive error resolu-

tion strategies that are the focus of this demon-

stration. An evaluation of our English to Iraqi-

Arabic S2S system is summarized in Section 4. 

2 System Architecture 

 
Figure 1 shows the architecture of our two-way 

 
 

Figure 1: BBN S2S System with Error Recovery 

in English to Iraqi-Arabic direction 

142



English to Iraqi-Arabic S2S translation system. 

In the English to Iraqi direction, the initial Eng-

lish ASR hypothesis and its corresponding trans-

lation are processed through a series of analysis 

(e.g. parsing, sense disambiguation) and error 

detection (e.g. ASR/MT confidence, Homo-

phone/Idiom/Named-Entity detection) modules. 

A detailed discussion on the various error detec-

tion modules can be found in Prasad et. al. 

(2012). A novel Inference Bridge data structure 

supports storage of these analyses in an intercon-

nected and retraceable manner. The potential 

erroneous spans are identified and ranked in an 

order of severity using this data structure. 

Based on the top ranked error, one of nine er-

ror resolution strategies (discussed in Section 3), 

is selected and executed. Each strategy is com-

posed of a sequence of steps which include ac-

tions such as TTS output, user input processing, 

translation (unconstrained or constrained) and 

other error type specific operations. This se-

quence is hand-crafted to efficiently recover 

from an error. Following a multi-expert design 

(Turunen and Hakulinen, 2003), each strategy 

represents an error-specific expert. 

3 Error Resolution Strategies 

Figure 2 illustrates the sequence of steps for the 

nine interaction strategies used by our system.  

The OOV Name and ASR Error strategies are 

designed to interactively resolve errors caused by 

OOV words (names and non-names) as well as 

other generic ASR and MT errors. When a span 

of words is identified as an OOV named-entity, 

the user is asked to confirm whether the audio 

segment corresponding to those words is a name. 

Upon user confirmation, the audio segment is 

spliced into the output target language utterance. 

This is based on the principle that audio seg-

ments containing names are understandable 

across languages. 

In the case where a generic erroneous span is 

detected, the user is asked to rephrase the utter-

ance. This strategy is suitable for handling multi-

ple error types including OOVs, mispronuncia-

tions, and generic ASR/MT errors. Additionally, 

the ASR Errors strategy has been designed to 

 

Figure 2. Interaction Strategies for Error Resolution 

143



capture a large fraction of the OOV name false 

negatives (i.e. missed detections) by allowing the 

user to indicate if the identified erroneous span is 

a name. Because of the confusability between the 

errors handled by these two strategies, we have 

found it beneficial to maintain reciprocity be-

tween them to recover from all the errors handled 

by each of these strategies. 

The four Word Sense (WS) disambiguation 

strategies resolve sense ambiguity errors. The 

underlying principle behind these strategies is 

that the sense of an ambiguous word must be 

confirmed by at least two of four possible inde-

pendent sources of evidence. These four sources 

include (a) the translation system (sense lookup 

corresponding to phrase pair associated with the 

ambiguous word), (b) a list of source-language 

contextual keywords that disambiguate a word, 

(c) the sense predicted by a sense-disambiguation 

model and (d) sense specified by the user. Be-

sides the objective to minimize user effort, these 

multiple sources are necessary because not all of 

them may be available for every ambiguous 

word. Case 1: No Mismatch strategy corresponds 

to the case where sources (a) and (c) agree. Case 

2: Filtered strategy corresponds to the case 

where (a) and (b) agree. In both of these cases, 

the system proceeds to present the translation to 

the Arabic speaker without performing any error 

resolution. If these three sources are unable to 

resolve the sense of a word, the user is asked to 

confirm the sense identified by source (a) as il-

lustrated in Case 3: Mismatch strategy. If the 

user rejects that sense, a list of senses is present-

ed to the user (Case 4: Backoff strategy). The 

user-specified sense then drives constrained de-

coding to obtain an accurate translation. 

Albeit simpler, the two homophone resolution 

strategies mimic the word sense disambiguation 

strategies in principle and design. The observed 

homophone variant produced by the ASR must 

be confirmed either by a homophone disambigu-

ation model (Case 1: No Mismatch) or by the 

user (Case 2: Mismatch). The input utterance is 

modified (if needed) by substituting the resolved 

homophone variant in the ASR output which is 

then translated and presented to the Arabic 

speaker. 

Strategies for resolving errors associated with 

idioms and incomplete utterances primarily rely 

on informing the user about these errors and elic-

iting a rephrasal. For idioms, the user is also giv-

en the choice to force a literal translation when 

appropriate. 

Following a mixed-initiative design, at all 

times, the user has the ability to rephrase their 

utterance as well as to force the system to pro-

ceed with the current translation. This allows the 

user to override system false alarms whenever 

suitable. The interface also allows the user to 

repeat the last system message which is helpful 

for comprehension of some of the synthesized 

system prompts for unfamiliar users. 

4 Summary of Evaluation 

Our S2S system equipped with the error resolu-

tion strategies discussed in the previous section 

was evaluated on 103 English utterances (25 

unique utterances repeated by multiple speakers). 

Each utterance was designed to elicit one of the 

error types listed in Section 1. 

The ASR word error rate for these utterances 

was 23%. The error detection components were 

able to identify 59% of these errors and the cor-

responding error resolution strategies were cor-

rectly triggered. 

The erroneous concepts in 13 of the 103 utter-

ances (12.6%) were translated without any error. 

Using the error resolution strategies, an addition-

al 34% of the erroneous concepts were accurate-

ly translated. This increased precision is 

achieved at the cost of user effort. On average, 

the strategies needed 1.4 clarifications turns per 

utterance. 

Besides focusing on improving the error de-

tection and resolution capabilities, we are cur-

rently working on extending these capabilities to 

two-way S2S systems. Specifically, we are de-

signing interactive strategies that engage both 

users in eyes-free cross-lingual communication. 

References 

David Stallard, Rohit Prasad, Prem Natarajan, Fred 

Choi, Shirin Saleem, Ralf Meermeier, Kriste 

Krstovski, Shankar Ananthakrishnan, and Jacob 

Devlin. 2011. The BBN TransTalk Speech-to-

Speech Translation System. Speech and Language 

Technologies, InTech, 31-52 

Rohit Prasad, Rohit Kumar, Sankaranarayanan Anan-

thakrishnan, Wei Chen, Sanjika Hewavitharana, 

Matthew Roy, Frederick Choi, Aaron Challenner, 

Enoch Kan, Arvind Neelakantan, and Premkumar 

Natarajan. 2012.  Active Error Detection and Reso-

lution for Speech-to-Speech Translation. Intl. 

Workshop on Spoken Language Translation 

(IWSLT), Hong Kong 

Markku Turunen, and Jaakko Hakulinen, 2003. Jaspis 

- An Architecture for Supporting Distributed Spo-

ken Dialogues. Proc. of Eurospeech, Geneva, Swit-

zerland 

144


