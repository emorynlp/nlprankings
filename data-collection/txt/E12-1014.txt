










































Toward Statistical Machine Translation without Parallel Corpora


Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 130–140,
Avignon, France, April 23 - 27 2012. c©2012 Association for Computational Linguistics

Toward Statistical Machine Translation without Parallel Corpora

Alexandre Klementiev Ann Irvine Chris Callison-Burch David Yarowsky
Center for Language and Speech Processing

Johns Hopkins University

Abstract

We estimate the parameters of a phrase-
based statistical machine translation sys-
tem from monolingual corpora instead of a
bilingual parallel corpus. We extend exist-
ing research on bilingual lexicon induction
to estimate both lexical and phrasal trans-
lation probabilities for MT-scale phrase-
tables. We propose a novel algorithm to es-
timate reordering probabilities from mono-
lingual data. We report translation results
for an end-to-end translation system us-
ing these monolingual features alone. Our
method only requires monolingual corpora
in source and target languages, a small
bilingual dictionary, and a small bitext for
tuning feature weights. In this paper, we ex-
amine an idealization where a phrase-table
is given. We examine the degradation in
translation performance when bilingually
estimated translation probabilities are re-
moved and show that 80%+ of the loss can
be recovered with monolingually estimated
features alone. We further show that our
monolingual features add 1.5 BLEU points
when combined with standard bilingually
estimated phrase table features.

1 Introduction

The parameters of statistical models of transla-
tion are typically estimated from large bilingual
parallel corpora (Brown et al., 1993). However,
these resources are not available for most lan-
guage pairs, and they are expensive to produce in
quantities sufficient for building a good transla-
tion system (Germann, 2001). We attempt an en-
tirely different approach; we use cheap and plen-
tiful monolingual resources to induce an end-to-
end statistical machine translation system. In par-
ticular, we extend the long line of work on in-
ducing translation lexicons (beginning with Rapp
(1995)) and propose to use multiple independent
cues present in monolingual texts to estimate lex-
ical and phrasal translation probabilities for large,
MT-scale phrase-tables. We then introduce a

novel algorithm to estimate reordering features
from monolingual data alone, and we report the
performance of a phrase-based statistical model
(Koehn et al., 2003) estimated using these mono-
lingual features.

Most of the prior work on lexicon induction
is motivated by the idea that it could be applied
to machine translation but stops short of actu-
ally doing so. Lexicon induction holds the po-
tential to create machine translation systems for
languages which do not have extensive parallel
corpora. Training would only require two large
monolingual corpora and a small bilingual dictio-
nary, if one is available. The idea is that intrin-
sic properties of monolingual data (possibly along
with a handful of bilingual pairs to act as exam-
ple mappings) can provide independent but infor-
mative cues to learn translations because words
(and phrases) behave similarly across languages.
This work is the first attempt to extend and apply
these ideas to an end-to-end machine translation
pipeline. While we make an explicit assumption
that a table of phrasal translations is given a priori,
we induce every other parameter of a full phrase-
based translation system from monolingual data
alone. The contributions of this work are:

• In Section 2.2 we analyze the challenges
of using bilingual lexicon induction for sta-
tistical MT (performance on low frequency
items, and moving from words to phrases).

• In Sections 3.1 and 3.2 we use multiple cues
present in monolingual data to estimate lexi-
cal and phrasal translation scores.

• In Section 3.3 we propose a novel algo-
rithm for estimating phrase reordering fea-
tures from monolingual texts.

• Finally, in Section 5 we systematically drop
feature functions from a phrase table and
then replace them with monolingually es-
timated equivalents, reporting end-to-end
translation quality.

130



2 Background

We begin with a brief overview of the stan-
dard phrase-based statistical machine translation
model. Here, we define the parameters which
we later replace with monolingual alternatives.
We continue with a discussion of bilingual lex-
icon induction; we extend these methods to es-
timate the monolingual parameters in Section 3.
This approach allows us to replace expensive/rare
bilingual parallel training data with two large
monolingual corpora, a small bilingual dictionary,
and ≈2,000 sentence bilingual development set,
which are comparatively plentiful/inexpensive.

2.1 Parameters of phrase-based SMT
Statistical machine translation (SMT) was first
formulated as a series of probabilistic mod-
els that learn word-to-word correspondences
from sentence-aligned bilingual parallel corpora
(Brown et al., 1993). Current methods, includ-
ing phrase-based (Och, 2002; Koehn et al., 2003)
and hierarchical models (Chiang, 2005), typically
start by word-aligning a bilingual parallel cor-
pus (Och and Ney, 2003). They extract multi-
word phrases that are consistent with the Viterbi
word alignments and use these phrases to build
new translations. A variety of parameters are es-
timated using the bitexts. Here we review the pa-
rameters of the standard phrase-based translation
model (Koehn et al., 2007). Later we will show
how to estimate them using monolingual texts in-
stead. These parameters are:

• Phrase pairs. Phrase extraction heuristics
(Venugopal et al., 2003; Tillmann, 2003;
Och and Ney, 2004) produce a set of phrase
pairs (e, f) that are consistent with the word
alignments. In this paper we assume that the
phrase pairs are given (without any scores),
and we induce every other parameter of the
phrase-based model from monolingual data.

• Phrase translation probabilities. Each
phrase pair has a list of associated fea-
ture functions (FFs). These include phrase
translation probabilities, φ(e|f) and φ(f |e),
which are typically calculated via maximum
likelihood estimation.

• Lexical weighting. Since MLE overestimates
φ for phrase pairs with sparse counts, lexi-
cal weighting FFs are used to smooth. Aver-

How

much

should

you

charge

for

your

W
ie
vie

l

so
llte

m
an

au
frg
un
d

se
in
es

Pr
ofi
ls

in Fa
ce
bo
ok

ve
rd
ie
ne
n

Facebook

profile s

d

m

m

m

d

d

Figure 1: The reordering probabilities from the phrase-
based models are estimated from bilingual data by cal-
culating how often in the parallel corpus a phrase pair
(f, e) is orientated with the preceding phrase pair in
the 3 types of orientations (monotone, swapped, and
discontinuous).

age word translation probabilities, w(ei|fj),
are calculated via phrase-pair-internal word
alignments.

• Reordering model. Each phrase pair (e, f)
also has associated reordering parameters,
po(orientation|f, e), which indicate the dis-
tribution of its orientation with respect to the
previously translated phrase. Orientations
are monotone, swap, discontinuous (Tillman,
2004; Kumar and Byrne, 2004), see Figure 1.

• Other features. Other typical features are
n-gram language model scores and a phrase
penalty, which governs whether to use fewer
longer phrases or more shorter phrases.
These are not bilingually estimated, so we
can re-use them directly without modifica-
tion.

The features are combined in a log linear model,
and their weights are set through minimum error
rate training (Och, 2003). We use the same log
linear formulation and MERT but propose alterna-
tives derived directly from monolingual data for
all parameters except for the phrase pairs them-
selves. Our pipeline still requires a small bitext of
approximately 2,000 sentences to use as a devel-
opment set for MERT parameter tuning.

131



2.2 Bilingual lexicon induction for SMT

Bilingual lexicon induction describes the class of
algorithms that attempt to learn translations from
monolingual corpora. Rapp (1995) was the first
to propose using non-parallel texts to learn the
translations of words. Using large, unrelated En-
glish and German corpora (with 163m and 135m
words) and a small German-English bilingual dic-
tionary (with 22k entires), Rapp (1999) demon-
strated that reasonably accurate translations could
be learned for 100 German nouns that were not
contained in the seed bilingual dictionary. His al-
gorithm worked by (1) building a context vector
representing an unknown German word by count-
ing its co-occurrence with all the other words
in the German monolingual corpus, (2) project-
ing this German vector onto the vector space of
English using the seed bilingual dictionary, (3)
calculating the similarity of this sparse projected
vector to vectors for English words that were con-
structed using the English monolingual corpus,
and (4) outputting the English words with the
highest similarity as the most likely translations.

A variety of subsequent work has extended the
original idea either by exploring different mea-
sures of vector similarity (Fung and Yee, 1998)
or by proposing other ways of measuring simi-
larity beyond co-occurence within a context win-
dow. For instance, Schafer and Yarowsky (2002)
demonstrated that word translations tend to co-
occur in time across languages. Koehn and Knight
(2002) used similarity in spelling as another kind
of cue that a pair of words may be translations of
one another. Garera et al. (2009) defined context
vectors using dependency relations rather than ad-
jacent words. Bergsma and Van Durme (2011)
used the visual similarity of labeled web images
to learn translations of nouns. Additional related
work on learning translations from monolingual
corpora is discussed in Section 6.

In this paper, we apply bilingual lexicon in-
duction methods to statistical machine translation.
Given the obvious benefits of not having to rely
on scarce bilingual parallel training data, it is sur-
prising that bilingual lexicon induction has not
been used for SMT before now. There are sev-
eral open questions that make its applicability to
SMT uncertain. Previous research on bilingual
lexicon induction learned translations only for a
small number of high frequency words (e.g. 100

●

●●

●

●

●
●

●

●

0 100 200 300 400 500 600

0
10

20
30

40

A
cc

ur
ac

y,
 %

Corpus Frequency

● Top 1
Top 10

Figure 2: Accuracy of single-word translations in-
duced using contextual similarity as a function of the
source word corpus frequency. Accuracy is the pro-
portion of the source words with at least one correct
(bilingual dictionary) translation in the top 1 and top
10 candidate lists.

nouns in Rapp (1995), 1,000 most frequent words
in Koehn and Knight (2002), or 2,000 most fre-
quent nouns in Haghighi et al. (2008)). Although
previous work reported high translation accuracy,
it may be misleading to extrapolate the results to
SMT, where it is necessary to translate a much
larger set of words and phrases, including many
low frequency items.

In a preliminary study, we plotted the accuracy
of translations against the frequency of the source
words in the monolingual corpus. Figure 2 shows
the result for translations induced using contex-
tual similarity (defined in Section 3.1). Unsur-
prisingly, frequent terms have a substantially bet-
ter chance of being paired with a correct transla-
tion, with words that only occur once having a low
chance of being translated accurately.1 This prob-
lem is exacerbated when we move to multi-token
phrases. As with phrase translation features esti-
mated from parallel data, longer phrases are more
sparse, making similarity scores less reliable than
for single words.

Another impediment (not addressed in this
paper) for using lexicon induction for SMT is
the number of translations that must be learned.
Learning translations for all words in the source
language requires n2 vector comparisons, since
each word in the source language vocabulary must

1For a description of the experimental setup used to pro-
duce these translations, see Experiment 8 in Section 5.2.

132



s1
s2
s3

sN-1
sN

✓

✓

✓

t1
t2
t3

tM-1
tM

✓

✓

dict.

project
✓

✓

✓

✓

✓

✓

compare

para crecer
to expand

activity of

economic
activity

policy
growth
foreign

economico

tasa
planeta

empleo
extranjero

policy
para crecer
(projected)

ES Context
Vector

Projected ES
Context Vector

EN Context
Vectors

Figure 3: Scoring contextual similarity of phrases:
first, contextual vectors are projected using a small
seed dictionary and then compared with the target lan-
guage candidates.

be compared against the vectors for all words in
the target language vocabulary. The size of the n2

comparisons hugely increases if we compare vec-
tors for multi-word phrases instead of just words.
In this work, we avoid this problem by assuming
that a limited set of phrase pairs is given a pri-
ori (but without scores). By limiting ourselves
to phrases in a phrase table, we vastly limit the
search space of possible translations. This is an
idealization because high quality translations are
guaranteed to be present. However, as our lesion
experiments in Section 5.1 show, a phrase table
without accurate translation probability estimates
is insufficient to produce high quality translations.
We show that lexicon induction methods can be
used to replace bilingual estimation of phrase- and
lexical-translation probabilities, making a signifi-
cant step towards SMT without parallel corpora.

3 Monolingual Parameter Estimation

We use bilingual lexicon induction methods to es-
timate the parameters of a phrase-based transla-
tion model from monolingual data. Instead of
scores estimated from bilingual parallel data, we
make use of cues present in monolingual data to
provide multiple orthogonal estimates of similar-
ity between a pair of phrases.

3.1 Phrasal similarity features

Contextual similarity. We extend the vector
space approach of Rapp (1999) to compute sim-
ilarity between phrases in the source and tar-
get languages. More formally, assume that
(s1, s2, . . . sN ) and (t1, t2, . . . tM ) are (arbitrarily
indexed) source and target vocabularies, respec-
tively. A source phrase f is represented with an

terrorist (en)
terrorista (es)

O
cc

ur
re

nc
es

terrorist (en)
riqueza (es)

O
cc

ur
re

nc
es

Time

Figure 4: Temporal histograms of the English phrase
terrorist, its Spanish translation terrorista, and riqueza
(wealth) collected from monolingual texts spanning a
13 year period. While the correct translation has a
good temporal match, the non-translation riqueza has
a distinctly different signature.

N - and target phrase e with an M -dimensional
vector (see Figure 3). The component values of
the vector representing a phrase correspond to
how often each of the words in that vocabulary
appear within a two word window on either side
of the phrase. These counts are collected using
monolingual corpora. After the values have been
computed, a contextual vector f is projected onto
the English vector space using translations in a
seed bilingual dictionary to map the component
values into their appropriate English vector posi-
tions. This sparse projected vector is compared
to the vectors representing all English phrases e.
Each phrase pair in the phrase table is assigned
a contextual similarity score c(f, e) based on the
similarity between e and the projection of f .

Various means of computing the component
values and vector similarity measures have been
proposed in literature (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of f ’s
contextual vector as follows:

wk = nf,k × (log(n/nk) + 1)

where nf,k and nk are the number of times sk ap-
pears in the context of f and in the entire corpus,
and n is the maximum number of occurrences of
any word in the data. Intuitively, the more fre-
quently sk appears with f and the less common
it is in the corpus in general, the higher its com-
ponent value. Similarity between two vectors is
measured as the cosine of the angle between them.

Temporal similarity. In addition to contex-
tual similarity, phrases in two languages may

133



be scored in terms of their temporal similarity
(Schafer and Yarowsky, 2002; Klementiev and
Roth, 2006; Alfonseca et al., 2009). The intu-
ition is that news stories in different languages
will tend to discuss the same world events on the
same day. The frequencies of translated phrases
over time give them particular signatures that will
tend to spike on the same dates. For instance, if
the phrase asian tsunami is used frequently dur-
ing a particular time span, the Spanish transla-
tion maremoto asiático is likely to also be used
frequently during that time. Figure 4 illustrates
how the temporal distribution of terrorist is more
similar to Spanish terrorista than to other Span-
ish phrases. We calculate the temporal similar-
ity between a pair of phrases t(f, e) using the
method defined by Klementiev and Roth (2006).
We generate a temporal signature for each phrase
by sorting the set of (time-stamped) documents in
the monolingual corpus into a sequence of equally
sized temporal bins and then counting the number
of phrase occurrences in each bin. In our exper-
iments, we set the window size to 1 day, so the
size of temporal signatures is equal to the num-
ber of days spanned by our corpus. We use cosine
distance to compare the normalized temporal sig-
natures for a pair of phrases (f, e).
Topic similarity. Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. Thus, topic or cat-
egory information associated with monolingual
data can also be used to indicate similarity be-
tween a phrase and its candidate translation. In
order to score a pair of phrases, we collect their
topic signatures by counting their occurrences in
each topic and then comparing the resulting vec-
tors. We again use the cosine similarity mea-
sure on the normalized topic signatures. In our
experiments, we use interlingual links between
Wikipedia articles to estimate topic similarity. We
treat each linked article pair as a topic and collect
counts for each phrase across all articles in its cor-
responding language. Thus, the size of a phrase
topic signature is the number of article pairs with
interlingual links in Wikipedia, and each compo-
nent contains the number of times the phrase ap-
pears in (the appropriate side of) the correspond-
ing pair. Our Wikipedia-based topic similarity
feature, w(f, e), is similar in spirit to polylingual
topic models (Mimno et al., 2009), but it is scal-
able to full bilingual lexicon induction.

3.2 Lexical similarity features

In addition to the three phrase similarity features
used in our model – c(f, e), t(f, e) and w(f, e) –
we include four additional lexical similarity fea-
tures for each of phrase pair. The first three lex-
ical features clex(f, e), tlex(f, e) and wlex(f, e)
are the lexical equivalents of the phrase-level con-
textual, temporal and wikipedia topic similarity
scores. They score the similarity of individual
words within the phrases. To compute these
lexical similarity features, we average similarity
scores over all possible word alignments across
the two phrases. Because individual words are
more frequent than multiword phrases, the accu-
racy of clex, tlex, and wlex tends to be higher than
their phrasal equivalents (this is similar to the ef-
fect observed in Figure 2).

Orthographic / phonetic similarity. The final
lexical similarity feature that we incorporate is
o(f, e), which measures the orthographic similar-
ity between words in a phrase pair. Etymolog-
ically related words often retain similar spelling
across languages with the same writing system,
and low string edit distance sometimes signals
translation equivalency. Berg-Kirkpatrick and
Klein (2011) present methods for learning cor-
respondences between the alphabets of two lan-
guages. We can also extend this idea to language
pairs not sharing the same writing system since
many cognates, borrowed words, and names re-
main phonetically similar. Transliterations can be
generated for tokens in a source phrase (Knight
and Graehl, 1997), with o(f, e) calculating pho-
netic similarity rather than orthographic.

The three phrasal and four lexical similarity
scores are incorporated into the log linear trans-
lation model as feature functions, replacing the
bilingually estimated phrase translation probabil-
ities φ and lexical weighting probabilities w. Our
seven similarity scores are not the only ones that
could be incorporated into the translation model.
Various other similarity scores can be computed
depending on the available monolingual data and
its associated metadata (see, e.g. Schafer and
Yarowsky (2002)).

3.3 Reordering

The remaining component of the phrase-based
SMT model is the reordering model. We
introduce a novel algorithm for estimating

134



Input: Source and target phrases f and e,
Source and target monolingual corpora Cf and Ce,
Phrase table pairs T = {(f (i), e(i))}Ni=1.

Output: Orientation features (pm, ps, pd).

Sf ← sentences containing f in Cf ;
Se ← sentences containing e in Ce;
(Bf ,−,−)← CollectOccurs(f,∪Ni=1f (i), Sf );
(Be, Ae, De)← CollectOccurs(e,∪Ni=1e(i), Se);
cm = cs = cd = 0;

foreach unique f ′ in Bf do
foreach translation e′ of f ′ in T do

cm = cm + #Be (e
′);

cs = cs + #Ae (e
′);

cd = cd + #De (e
′);

c← cm + cs + cd;
return ( cm

c
, cs

c
, cd

c
)

CollectOccurs(r, R, S)
B ← (); A← (); D ← ();
foreach sentence s ∈ S do

foreach occurrence of phrase r in s do
B ← B + (longest preceding r and in R);
A← A + (longest following r and in R);
D ← D + (longest discontinuous w/ r and in
R);

return (B, A, D);

Figure 5: Algorithm for estimating reordering
probabilities from monolingual data.

po(orientation|f, e) from two monolingual cor-
pora instead a bitext.

Figure 1 illustrates how the phrase pair orienta-
tion statistics are estimated in the standard phrase-
based SMT pipeline. For a phrase pair like (f =
“Profils”, e = “profile”), we count its orien-
tation with the previously translated phrase pair
(f ′ = “in Facebook”, e′ = “Facebook”) across
all translated sentence pairs in the bitext.

In our pipeline we do not have translated sen-
tence pairs. Instead, we look for monolingual
sentences in the source corpus which contain
the source phrase that we are interested in, like
f = “Profils”, and at least one other phrase
that we have a translation for, like f ′ = “in
Facebook”. We then look for all target lan-
guage sentences in the target monolingual cor-
pus that contain the translation of f (here e =
“profile”) and any translation of f ′. Figure 6 il-
lustrates that it is possible to find evidence for
po(swapped|Profils, profile), even from the non-
parallel, non-translated sentences drawn from two
independent monolingual corpora. By looking for
foreign sentences containing pairs of adjacent for-
eign phrases (f, f ′) and English sentences con-

Da
s

An
le
ge
n

ei
ne
s

Pr
ofi
ls

in Fa
ce
bo
ok

ist ei
nf
ac
h

s

What

does

your

Facebook

profile

reveal

Figure 6: Collecting phrase orientation statistics for
a English-German phrase pair (“profile”, “Profils”)
from non-parallel sentences (the German sentence
translates as “Creating a Facebook profile is easy”).

taining their corresponding translations (e, e′), we
are able to increment orientation counts for (f, e)
by looking at whether e and e′ are adjacent,
swapped, or discontinuous. The orientations cor-
respond directly to those shown in Figure 1.

One subtly of our method is that shorter and
more frequent phrases (e.g. punctuation) are more
likely to appear in multiple orientations with a
given phrase, and therefore provide poor evi-
dence of reordering. Therefore, we (a) collect
the longest contextual phrases (which also appear
in the phrase table) for reordering feature estima-
tion, and (b) prune the set of sentences so that
we only keep a small set of least frequent contex-
tual phrases (this has the effect of dropping many
function words and punctuation marks and and re-
lying more heavily on multi-word content phrases
to estimate the reordering).2

Our algorithm for learning the reordering pa-
rameters is given in Figure 5. The algorithm
estimates a probability distribution over mono-
tone, swap, and discontinuous orientations (pm,
ps, pd) for a phrase pair (f, e) from two mono-
lingual corpora Cf and Ce. It begins by calling
CollectOccurs to collect the longest match-
ing phrase table phrases that precede f in source
monolingual data (Bf ), as well as those that pre-
cede (Be), follow (Ae), and are discontinuous
(De) with e in the target language data. For each
unique phrase f ′ preceding f , we look up transla-
tions in the phrase table T. Next, we count3 how

2The pruning step has an additional benefit of minimizing
the memory needed for orientation feature estimations.

3#L(x) returns the count of object x in list L.

135



Monolingual training corpora
Europarl Gigaword Wikipedia

date range 4/96-10/09 5/94-12/08 n/a
uniq shared dates 829 5,249 n/a
Spanish articles n/a 3,727,954 59,463
English articles n/a 4,862,876 59,463
Spanish lines 1,307,339 22,862,835 2,598,269
English lines 1,307,339 67,341,030 3,630,041
Spanish words 28,248,930 774,813,847 39,738,084
English words 27,335,006 1,827,065,374 61,656,646

Spanish-English phrase table
Phrase pairs 3,093,228
Spanish phrases 89,386
English phrases 926,138
Spanish unigrams 13,216
Avg # translations 98.7
Spanish bigrams 41,426
Avg # translations 31.9
Spanish trigrams 34,744
Avg # translations 13.5

Table 1: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.

many translations e′ of f ′ appeared before, after
or were discontinuous with e in the target lan-
guage data. Finally, the counts are normalized and
returned. These normalized counts are the values
we use as estimates of po(orientation|f, e).

4 Experimental Setup

We use the Spanish-English language pair to test
our method for estimating the parameters of an
SMT system from monolingual corpora. This al-
lows us to compare our method against the nor-
mal bilingual training procedure. We expect bilin-
gual training to result in higher translation qual-
ity because it is a more direct method for learn-
ing translation probabilities. We systematically
remove different parameters from the standard
phrase-based model, and then replace them with
our monolingual equivalents. Our goal is to re-
cover as much of the loss as possible for each of
the deleted bilingual components.

The standard phrase-based model that we use
as our top-line is the Moses system (Koehn et
al., 2007) trained over the full Europarl v5 par-
allel corpus (Koehn, 2005). With the exception
of maximum phrase length (set to 3 in our ex-
periments), we used default values for all of the
parameters. All experiments use a trigram lan-
guage model trained on the English side of the
Europarl corpus using SRILM with Kneser-Ney
smoothing. To tune feature weights in minimum
error rate training, we use a development bitext
of 2,553 sentence pairs, and we evaluate per-
formance on a test set of 2,525 single-reference
translated newswire articles. These development
and test datasets were distributed in the WMT
shared task (Callison-Burch et al., 2010).4 MERT

4Specifcially, news-test2008 plus news-syscomb2009 for
dev and newstest2009 for test.

was re-run for every experiment.
We estimate the parameters of our model from

two sets of monolingual data, detailed in Table 1:

• First, we treat the two sides of the Europarl
parallel corpus as independent, monolingual
corpora. Haghighi et al. (2008) also used
this method to show how well translations
could be learned from monolingual corpora
under ideal conditions, where the contextual
and temporal distribution of words in the two
monolingual corpora are nearly identical.

• Next, we estimate the features from truly
monolingual corpora. To estimate the con-
textual and temporal similarity features, we
use the Spanish and English Gigaword cor-
pora.5 These corpora are substantially larger
than the Europarl corpora, providing 27x as
much Spanish and 67x as much English for
contextual similarity, and 6x as many paired
dates for temporal similarity. Topical simi-
larity is estimated using Spanish and English
Wikipedia articles that are paired with inter-
language links.

To project context vectors from Spanish to En-
glish, we use a bilingual dictionary containing en-
tries for 49,795 Spanish words. Note that end-to-
end translation quality is robust to substantially
reducing dictionary size, but we omit these ex-
periments due to space constraints. The con-
text vectors for words and phrases incorporate co-
occurrence counts using a two-word window on
either side.

The title of our paper uses the word towards be-
cause we assume that an inventory of phrase pairs
is given. Future work will explore inducing the

5We use the afp, apw and xin sections of the corpora.

136



BL
EU

0
5

10
15

20
25

21.87 21.54

12.86

4.00

10.52

15.35
14.02 14.78

16.85 17.50

22.92

-/-B
/B B/
-

-/B -/M t/
- o/
-

c/
-

M
/-

M
/M

BM
/B

1 2 3 4 5 6 7 8 9 10 11

            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion

5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono

11, 19   BM/B   bilingual + all mono / bilingual

Estimated Using Europarl

Estimated Using Monolingual Corpora

BL
EU

14.02

B
L
E
U

0
5

1
0

1
5

2
0

2
5

10.15

13.13 14.02 14.07

17.00 17.92
18.79

23.3623.36

18.7917.9217.00

BM
/B

M
/MM
/-

w
/-c/
-

o/
-t/--/M

0
5

10
15

20
25

BL
EU

14.0714.0213.13
10.15

18 19171615141312
BL
EU

0
5

10
15

20
25

21.87 21.54

12.86

4.00

10.52

15.35
14.02 14.78

16.85 17.50

22.92

-/-B
/B B/
-

-/B -/M t/
- o/
-

c/
-

M
/-

M
/M

BM
/B

1 2 3 4 5 6 7 8 9 10 11

            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion

5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono

11, 19   BM/B   bilingual + all mono / bilingual

Estimated Using Europarl

Estimated Using Monolingual Corpora
BL

EU

14.02

B
L
E
U

0
5

1
0

1
5

2
0

2
5

10.15

13.13 14.02 14.07

17.00 17.92
18.79

23.3623.36

18.7917.9217.00

BM
/B

M
/MM
/-

w
/-c/
-

o/
-t/--/M

0
5

10
15

20
25

BL
EU

14.0714.0213.13
10.15

18 19171615141312

Figure 7: Much of the loss in BLEU score when bilingually estimated features are removed from a Spanish-
English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equiva-
lents estimated from monolingual Europarl data (experiments 5-10). The labels indicate how the different types
of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.

BL
EU

0
5

10
15

20
25

21.87 21.54

12.86

4.00

10.52

15.35
14.02 14.78

16.85 17.50

22.92

-/-B
/B B/
-

-/B -/M t/
- o/
-

c/
-

M
/-

M
/M

BM
/B

1 2 3 4 5 6 7 8 9 10 11

            Exp      Phrase scores / orientation scores
   1        B/B       bilingual / bilingual (Moses)
   2        B/-        bilingual / distortion
   3        -/B        none / bilingual
   4        -/-         none / distortion

5, 12     -/M       none / mono
6, 13      t/-        temporal mono / distortion
7,14       o/-       orthographic mono / distortion
8, 15      c/-       contextual mono / distortion
  16        w/-      Wikipedia topical mono / distorion
9, 17      M/-      all mono / distortion
10, 18   M/M     all mono / mono

11, 19   BM/B   bilingual + all mono / bilingual

Estimated Using Europarl

Estimated Using Monolingual Corpora

BL
EU

14.02

B
L
E
U

0
5

1
0

1
5

2
0

2
5

10.15

13.13 14.02 14.07

17.00 17.92
18.79

23.3623.36

18.7917.9217.00

BM
/B

M
/MM
/-

w
/-c/
-

o/
-t/--/M

0
5

10
15

20
25

BL
EU

14.0714.0213.13
10.15

18 19171615141312

Figure 8: Performance of monolingual features de-
rived from truly monolingual corpora. Over 82% of
the BLEU score loss can be recovered.

phrase table itself from monolingual texts. Across
all of our experiments, we use the phrase table
that the bilingual model learned from the Europarl
parallel corpus. We keep its phrase pairs, but we
drop all of its scores. Table 1 gives details of the
phrase pairs. In our experiments, we estimated
similarity and reordering scores for more than 3
million phrase pairs. For each source phrase, the
set of possible translations was constrained and
likely to contain good translations. However, the
average number of possible translations was high
(ranging from nearly 100 translations for each un-
igram to 14 for each trigram). These contain a
lot of noise and result in low end-to-end transla-
tion quality without good estimates of translation
quality, as the experiments in Section 5.1 show.

Software. Because many details of our estima-
tion procedures must be omitted for space, we dis-
tribute our full set of code along with scripts for
running our experiments and output translations.
These may be downed from http://www.cs.
jhu.edu/˜anni/papers/lowresmt/

5 Experimental Results

Figures 7 and 8 give experimental results. Figure
7 shows the performance of the standard phrase-
based model when each of the bilingually esti-
mated features are removed. It shows how much
of the performance loss can be recovered using
our monolingual features when they are estimated
from the Europarl training corpus but treating
each side as an independent, monolingual cor-
pus. Figure 8 shows the recovery when using truly
monolingual corpora to estimate the parameters.

5.1 Lesion experiments

Experiments 1-4 remove bilingually estimated pa-
rameters from the standard model. For Spanish-
English, the relative contribution of the phrase-
table features (which include the phrase transla-
tion probabilities φ and the lexical weights w) is
greater than the reordering probabilities. When
the reordering probability po(orientation|f, e) is
eliminated and replaced with a simple distance-
based distortion feature that does not require a
bitext to estimate, the score dips only marginally
since word order in English and Spanish is simi-
lar. However, when both the reordering and the
phrase table features are dropped, leaving only
the LM feature and the phrase penalty, the result-
ing translation quality is abysmal, with the score
dropping a total of over 17 BLEU points.

5.2 Adding equivalent monolingual features
estimated using Europarl

Experiments 5-10 show how much our monolin-
gual equivalents could recover when the monolin-
gual corpora are drawn from the two sides of the
bitext. For instance, our algorithm for estimating

137



reordering probabilities from monolingual data (–
/M) adds 5 BLEU points, which is 73% of the po-
tential recovery going from the model (–/–) to the
model with bilingual reordering features (–/B).

Of the temporal, orthographic, and contextual
monolingual features the temporal feature per-
forms the best. Together (M/–), they recover
more than each individually. Combining mono-
lingually estimated reordering and phrase table
features (M/M) yields a total gain of 13.5 BLEU
points, or over 75% of the BLEU score loss that
occurred when we dropped all features from the
phrase table. However, these results use “mono-
lingual” corpora which have practically identical
phrasal and temporal distributions.

5.3 Estimating features using truly
monolingual corpora

Experiments 12-18 estimate all of the features
from truly monolingual corpora. Our novel al-
gorithm for estimating reordering holds up well
and recovers 69% of the loss, only 0.4 BLEU
points less than when estimated from the Europarl
monolingual texts. The temporal similarity fea-
ture does not perform as well as when it was esti-
mated using Europarl data, but the contextual fea-
ture does. The topic similarity using Wikipedia
performs the strongest of the individual features.

Combining the monolingually estimated re-
ordering features with the monolingually esti-
mated similarity features (M/M) yields a total
gain of 14.8 BLEU points, or over 82% of the
BLEU point loss that occurred when we dropped
all features from the phrase table. This is equiv-
alent to training the standard system on a bi-
text with roughly 60,000 lines or nearly 2 million
words (learning curve omitted for space).

Finally, we supplement the standard bilingually
estimated model parameters with our monolin-
gual features (BM/B), and we see a 1.5 BLEU
point increase over the standard model. There-
fore, our monolingually estimated scores capture
some novel information not contained in the stan-
dard feature set.

6 Additional Related Work

Carbonell et al. (2006) described a data-driven
MT system that used no parallel text. It produced
translation lattices using a bilingual dictionary
and scored them using an n-gram language model.

Their method has no notion of translation similar-
ity aside from a bilingual dictionary. Similarly,
Sánchez-Cartagena et al. (2011) supplement an
SMT phrase table with translation pairs extracted
from a bilingual dictionary and give each a fre-
quency of one for computing translation scores.

Ravi and Knight (2011) treat MT without paral-
lel training data as a decipherment task and learn
a translation model from monolingual text. They
translate corpora of Spanish time expressions and
subtitles, which both have a limited vocabulary,
into English. Their method has not been applied
to broader domains of text.

Most work on learning translations from mono-
lingual texts only examine small numbers of fre-
quent words. Huang et al. (2005) and Daumé and
Jagarlamudi (2011) are exceptions that improve
MT by mining translations for OOV items.

A variety of past research has focused on min-
ing parallel or comparable corpora from the web
(Munteanu and Marcu, 2006; Smith et al., 2010;
Uszkoreit et al., 2010). Others use an existing
SMT system to discover parallel sentences within
independent monolingual texts, and use them to
re-train and enhance the system (Schwenk, 2008;
Chen et al., 2008; Schwenk and Senellart, 2009;
Rauf and Schwenk, 2009; Lambert et al., 2011).
These are complementary but orthogonal to our
research goals.

7 Conclusion

This paper has demonstrated a novel set of tech-
niques for successfully estimating phrase-based
SMT parameters from monolingual corpora, po-
tentially circumventing the need for large bitexts,
which are expensive to obtain for new languages
and domains. We evaluated the performance of
our algorithms in a full end-to-end translation sys-
tem. Assuming that a bilingual-corpus-derived
phrase table is available, we were able utilize our
monolingually-estimated features to recover over
82% of BLEU loss that resulted from removing
the bilingual-corpus-derived phrase-table proba-
bilities. We also showed that our monolingual fea-
tures add 1.5 BLEU points when combined with
standard bilingually estimated features. Thus our
techniques have stand-alone efficacy when large
bilingual corpora are not available and also make
a significant contribution to combined ensemble
performance when they are.

138



References
Enrique Alfonseca, Massimiliano Ciaramita, and

Keith Hall. 2009. Gazpacho and summer rash:
lexical relationships from temporal patterns of web
search queries. In Proceedings of EMNLP.

Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-2011), Edinburgh, Scotland, UK.

Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual simi-
larity of labeled web images. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence.

Peter Brown, John Cocke, Stephen Della Pietra, Vin-
cent Della Pietra, Frederick Jelinek, Robert Mercer,
and Paul Poossin. 1988. A statistical approach to
language translation. In 12th International Confer-
ence on Computational Linguistics (CoLing-1988).

Peter Brown, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer. 1993. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311, June.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Workshop on Sta-
tistical Machine Translation.

Jaime Carbonell, Steve Klein, David Miller, Michael
Steinbaum, Tomer Grassiany, and Jochen Frey.
2006. Context-based machine translation. In Pro-
ceedings of AMTA.

Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In Proceedings of ACL/HLT, pages
157–160.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.

Hal Daumé and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Proceedings of ACL/HLT.

Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of ACL/CoLing.

Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon in-
duction from monolingual corpora via dependency
contexts and part-of-speech equivalences. In Thir-
teenth Conference On Computational Natural Lan-
guage Learning (CoNLL-2009), Boulder, Colorado.

Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In ACL 2001 Workshop

on Data-Driven Machine Translation, Toulouse,
France.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.

Fei Huang, Ying Zhang, and Stephan Vogel. 2005.
Mining key phrase translations from web corpora.
In Proceedings of EMNLP.

Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discov-
ery from multilingual comparable corpora. In Pro-
ceedings of the ACL/Coling.

Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of ACL.

Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007 Demo
and Poster Sessions.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Machine Translation Summit.

Shankar Kumar and William Byrne. 2004. Local
phrase reordering models for statistical machine
translation. In Proceedings of HLT/NAACL.

Patrik Lambert, Holger Schwenk, Christophe Ser-
van, and Sadaf Abdul-Rauf. 2011. Investigations
on translation model adaptation using monolingual
data. In Proceedings of the Workshop on Statistical
Machine Translation, pages 284–293, Edinburgh,
Scotland, UK.

David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of
EMNLP.

Dragos Stefan Munteanu and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In Proceedings of the
ACL/Coling.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.

139



Franz Joseph Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.

Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.

Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of ACL.

Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of ACL.

Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of EACL.

Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of ACL/HLT.

Vctor M. Sánchez-Cartagena, Felipe Sánchez-
Martnez, and Juan Antonio Pérez-Ortiz. 2011.
Integrating shallow-transfer rules into phrase-based
statistical machine translation. In Proceedings of
the XIII Machine Translation Summit.

Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of CoNLL.

Holger Schwenk and Jean Senellart. 2009. Transla-
tion model adaptation for an Arabic/French news
translation system by lightly-supervised training. In
MT Summit.

Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In Proceedings of IWSLT.

Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
Proceedings of HLT/NAACL.

Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT/NAACL.

Christoph Tillmann. 2003. A projection extension al-
gorithm for statistical machine translation. In Pro-
ceedings of EMNLP.

Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of CoLing.

Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.

140


