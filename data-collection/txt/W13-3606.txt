










































A Tree Transducer Model for Grammatical Error Correction


Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 43–51,
Sofia, Bulgaria, August 8-9 2013. c©2013 Association for Computational Linguistics

A Tree Transducer Model for Grammatical Error Correction

Jan Buys and Brink van der Merwe
MIH Media Lab and Computer Science Division

Stellenbosch University, South Africa
janbuys@ml.sun.ac.za, abvdm@cs.sun.ac.za

Abstract
We present an approach to grammatical er-
ror correction for the CoNLL 2013 shared
task based on a weighted tree-to-string
transducer. Rules for the transducer are
extracted from the NUCLE training data.
An n-gram language model is used to
rerank k-best sentence lists generated by
the transducer. Our system obtains a pre-
cision, recall and F1 score of 0.27, 0.1333
and 0.1785, respectively, on the official
test set. On the revised annotations, the
F1 score increases to 0.2505. Our system
ranked 6th out of the participating teams
on both the original and revised test set an-
notations.

1 Introduction

There has recently been an increase in research on
automated grammatical error detection and correc-
tion for writing by English language learners (Lea-
cock et al., 2010). In the most prominent line of
research, statistical classifiers are trained to de-
tect or correct specific error types. Features for
these classifiers are based on word context and lo-
cal syntactic information. The classifiers are com-
bined, and a language model is often used to filter
corrections. Research on this approach focusses
especially on preposition and determiner errors.
Most of the systems in the HOO 2011 and 2012
shared tasks (Dale and Kilgarriff, 2011; Dale et
al., 2012) fall under this broad approach.

In a second class of models, a model for gen-
erating corrected sentences is formulated in the
noisy-channel framework, relying strongly on a
language model to distinguish between grammati-
cal and ungrammatical candidate corrections (Lee
and Seneff, 2006; Turner and Charniak, 2007;
Park and Levy, 2011). Such models are often in-
spired by techniques developed for statistical ma-
chine translation (Brockett et al., 2006). Finally,

rule-based methods are often used in commercial
language processing systems such as word pro-
cessors. Here large hand-crafted linguistically ex-
pressive, error-tolerant grammars are used to anal-
yse sentences and identify where constraints have
been broken.

In this paper we present our system for the
CoNLL 2013 shared task in grammatical error cor-
rection (Ng et al., 2013). Our grammar correction
model is based on a tree-to-string transducer that
is specified by a set of rules that each rewrite a
tree fragment to a string of words and variables.
These rules are extracted automatically from a set
of training examples. Each training example con-
sists of an incorrect sentence, a corresponding cor-
rect sentence with its parse tree, and a word align-
ment between the incorrect and correct sentences.
During decoding the model searches for parsed
well-formed sentences that could be transformed
into a given incorrect sentence with high probabil-
ity. Sentences are split into linguistically plausible
clauses to decrease the average sentence length, in
order to improve decoding runtime. In order to
discriminate more accurately between candidate
sentence corrections an n-gram language model
trained on a large corpus of well-formed text is
used to rerank the k-best hypotheses that the trans-
ducer model generates. The tree transducer and
language model scores are weighted to maximize
the model F1 score on a validation set. After de-
coding the clauses are recombined into the original
sentence structure.

The next section describes preprocessing and
the resources used by our system. Section 3
defines weighted tree-to-string transducers. We
present the formulation of our error correction
model in section 4, and discuss decoding with it
in section 5. Section 6 describes language model
reranking. System results are given in section 7.
Finally, section 8 draws some conclusions and dis-
cuss future work.

43



2 Data Pre-processing

2.1 NUCLE
We use the pre-processed version of the NUCLE
corpus (Dahlmeier et al., 2013) released as train-
ing data for this shared task. The data con-
sist of essays, subdivided into paragraphs. Us-
ing NLTK (Bird et al., 2009), paragraphs were
split into sentences with NLTK punkt and sen-
tences were tokenized with NLTK word tokenize.
Though this sentence-splitting and tokenization is
not error-free (for example, quotation marks are
handled incorrectly in some contexts), we use it to
maintain consistency in our model. An error anno-
tation in the data consists of the start and end token
offsets in a sentence, as well as the correction that
should replace the text between the offsets.

We divide the corpus into 80% training data,
10% validation data and 10% development data.
Splitting is performed by random selection at es-
say level. For each sentence with corrections, we
refer to the original as the incorrect sentence, and
to the version with the corrections applied to it
as the correct sentence. For the purpose of train-
ing our models, all words are lowercased. As de-
scribed below, we also construct an alignment be-
tween the words of each of these sentence pairs.

The 2013 shared task focusses on five error
types: Article or determiner, preposition, noun
number, verb form, and subject-verb agreement
errors. In the training data we only apply correc-
tions of these types to obtain the correct version
of the sentences, though other error types are also
included in the error annotations. An alternative
would be to apply the corrections of other error
types to the correct and incorrect versions of the
sentences. However, we decided against that in or-
der to keep the training data realistically close to
the test data, which will also contain these other er-
rors. We do, however, correct some of the mechan-
ical errors, especially spelling errors, in the incor-
rect and correct versions of the training data, to
reduce noise that these errors may introduce into
the model.

In order to train a syntax-based model for gram-
mar correction, the correct version of the sen-
tences are parsed with the Berkeley parser (Petrov
and Klein, 2007). The Berkeley parser is a state-
of-the-art unlexicalized parser. Given that the cor-
rect side of our training data will still contain er-
rors, it is unlikely that lexicalized parsing will be
more accurate. Parser options are set to obtain left-

binarized parse trees under Viterbi decoding.

2.2 Wikipedia language model

We train the n-gram language model used in our
system on a large corpus of text extracted from
the English Wikipedia. The April 2013 Wikipedia
XML dump1 is used. This is parsed with the
gwtwiki2 Wikipedia parser, and all sentences con-
sisting of 6 or more words are extracted. These
sentences are tokenized with NLTK and lower-
cased. The corpus has about 1 500 millions words.
As vocabulary we use the 64 000 words with the
highest frequency occurrence in the corpus. A
3-gram language model is trained from the cor-
pus on this restricted vocabulary, to keep the size
of the language model reasonable. The language
model is trained and applied with the SRILM
toolkit (Stolcke, 2002). Kneser-Ney smoothing is
used to estimate the model weights.

2.3 Vocabulary

We set the vocabulary of the transducer model as
the union of the vocabulary of our language model
and the vocabulary of the words of the correct sen-
tences in the NUCLE training data. In the trans-
ducer construction we ensure that all words in this
vocabulary can be accepted. A large number of
URLs occur in the training data, as citations are
included in some of the essays. We replace these
with a <url> symbol to reduce noise in the vo-
cabulary.

In the validation, development and test data,
words that do not appear in the vocabulary are re-
placed with an <unk> symbol. We record the re-
placed words, so that after decoding they can be
replaced back to their original positions. We do
not perform automatic spelling correction on the
test data as a preprocessing step: The occurrence
of out of vocabulary words is small enough that
performing spelling correction will not have a sig-
nificant impact on the performance of our system.

We use the NLTK interface to WordNet (Miller,
1995) to find pairs of singular and plural nouns
and groups of verbs that have the same base form.
All verbs and non-proper nouns that occur in the
language model vocabulary are grouped like this.
The groups are used to construct additional rules
for noun number and verb form errors.

1http://download.wikimedia.org/enwiki/
latest/enwiki-latest-pages-articles.xml.
bz2

2http://code.google.com/p/gwtwiki/

44



3 Weighted Tree-to-string Transducers

Tree transducers are a class of automata-theoretic
models that perform transformations on tree struc-
tures. There is a rich theory concerning these mod-
els, and tree transducers with different restrictions
can compute different classes of transformations.
Algorithms for weighted variants of these trans-
ducers have recently been developed (Graehl et al.,
2008; May, 2010) and applied to syntax-based sta-
tistical machine translation.

Tree-to-string transducers are a class of tree
transducers that generalizes synchronous context-
free grammars. These transducers can be used to
transform strings into trees: For a given output
string, the decoding problem is to find the input
tree that could be transformed into the given string
with the highest probability. This decoding pro-
cess is referred to as backward application. The
formulation is due to the noisy-channel model of-
ten followed in statistical machine translation.

3.1 Definitions

We need a few preliminary definitions (the no-
tation of May (2010) is generally followed): A
ranked alphabet Σ is a finite set of symbols, each
which can take a finite set of ranks. A tree t ∈
TΣ is denoted by σ(t1, . . . , tk), where k ∈ N,
t1, . . . , tk ∈ TΣ and σ is a node of rank k. Σk
denotes the subset of Σ of all symbols with rank
k. TΣ(S) is the set of all trees in TΣ∪S where
symbols from S occur only at the leaves. We do-
nate by X = {x1, x2, . . .} a set of variables, and
Xk = {x1, . . . , xk}. With respect to Xk, a tree
u ∈ TΣ(Xk) or a sequence u ∈ (∆∪ (Q×X))∗ is
linear if each element of Xk occurs at most once
in u, and nondeleting if each element ofXk occurs
at least once in u.

Formally, a weighted extended top-down
tree-to-string transducer M is a 5-tuple
(Q,Σ,∆, R,Qd) (May, 2010, chap. 2). Q is
an alphabet of states that all have rank one. Σ
and ∆ are the ranked input and output alphabets,
respectively. Qd is the set of initial states. R
is a finite set of rules with an associated weight
function π : R → W . Each rule r ∈ R is of
the form q.t → g for q ∈ Q, t ∈ TΣ(X) and
g ∈ (∆ ∪ (Q ×X))∗. The tree t should be linear
in X , and each variable in g should also be in t.

We refer to q.t as the left hand side of a rule, and
to g as the right hand side. M is linear if the right
hand side of each rule is linear, and nondeleting if

the right hand side of each rule is nondeleting with
respect to Xk, the set of variables on the left hand
side.

For a rule r : q.t → g and e, f ∈ (∆ ∪ (Q ×
TΣ))

∗, a derivation step e ⇒r f is obtained by
replacing the left-most element of e of the form
q(s), where s matches t, by a transformation of
g, where each instance of a variable is replaced
with the corresponding subtree of s. The sequence
d = (r1, . . . , rm) is a derivation of the pair (t, s)
if t ⇒r1 t1 ⇒r2 . . . ⇒rm s, where ti ∈ (∆ ∪
(Q × TΣ))∗ for 1 ≤ i < m. The weight of d is
wt(d) = π(r1) · . . . · π(rm).

The tree transducer that we use is a weighted
linear, nondeleting top-down tree-to-string trans-
ducer. The expressive power of this transducer
class is sufficient for the transformations that we
need our model to perform. Relaxing these re-
strictions will increase the decoding complexity
of the transducer model significantly. Since we
work with binarized trees, no node will have a rank
greater than 2. Our transducer only has one state,
q. Look-ahead restrictions are added to restrict the
variables on the left hand side to match specific
constituents.

3.2 Probability model
A tree-to-string transducer can represent a condi-
tional probability model for the output sentences
given the input trees, or a joint probability model
over the input trees and output strings. We will
use a joint probability distribution for our model.
Note that there is spurious ambiguity in the model
at two levels: Firstly, it is possible that there can
be different derivations for the same tree-string
pair. However, during the application of the model
this ambiguity occurs infrequently. Secondly, the
model can generate different trees with the same
yield.

Suppose c is a correct sentence in the set C of
all possible correct sentences, and i is the given
(possibly) incorrect sentence. Let τ(c) represent
the set of all possible parse trees of c. Then we
want to find sentence

ĉ = arg max
c∈C

P (c, i) (1)

= arg max
c∈C

Σπ∈τ(c)P (π, i) (2)

The rule probabilities for the joint model are
conditioned on the root node of the rule left hand
side (and not on the entire left hand side, as would

45



be the case for a conditional model). The model is
trained from a set of derivations constructed from
the training data, as will be described below. Let
f(r) be the number of times that rule r occurs in
all the training derivations. Then the probability
estimate of a rule is

p(r|root(r)) = f(r)
Σr′:root(r′)=root(r)f(r′)

(3)

In the construction of the transducer, some rules
that do not occur in the training derivations are
added. In order to give non-zero weights to
these added rules, we apply Good-Turing smooth-
ing (Katz, 1987). This method has the advantage
of decreasing the counts of low-frequency rules
whose rule counts may provide unreliable prob-
ability estimates. For the rules of each of the root
nonterminals, the counts of rules with frequencies
between 0 and 5 are re-estimated.

4 Transducer Model Formulation

4.1 Word alignment
In order to extract rules from the training data
to perform transformations between incorrect and
correct sentences, we need to construct an align-
ment between words in each pair of correct and
incorrect sentences. This approach is similar to
aligning words in source and target language sen-
tences for statistical machine translation. We con-
struct the alignments from given sequences of edit
operations in the training data. Figure 1(a) gives
an example of a parse tree for a correct phrase,
aligned with a corresponding incorrect phrase.

Firstly, all words in a sentence that do not oc-
cur in any edits are aligned one-to-one between
the correct and incorrect sentence. In the example,
“that”, “up” and “across” are aligned in this way.
Then, for each edit annotation, we consider the in-
correct and correct phrases of that edit. Note that
in the NUCLE annotations the incorrect phrase
will always be non-empty, but the correct phrase
may be empty.

Words that occur in both the correct and incor-
rect edit phrases are aligned one-to-one. We re-
strict such alignments to prevent overlapping. In
the example in Figure 1(a), there is an edit to re-
place “the America” with “America”, so the word
“America” is aligned. Adding these alignments
may split a phrase into unaligned subphrases. If
such a subphrase is empty on either side, then the
word(s) on the other side have to be left unaligned.

(a)

SBAR

S

VP

PP

NP

NNP

America

IN

across

@VP

PRT

RP

up

VBD

sprang

WHNP

WDT

that

that sprung up across the America

(b)
(1) q.WDT (that)→ that
(2) q.VP (x0:@VP x1:PP)→ q.x0 q.x1
(3) q.VBD (sprang)→ sprung
(4) q.SBAR (x0:WHNP x1:S)→ q.x0 q.x1
(5) q.PP (x0:IN x1:NP)→ q.x0 the q.x1
(6) q.NNP (America)→ America

Figure 1: (a) Example alignment between a cor-
rect parse tree and an incorrect clause. (b) Some
rules extracted from the example.

In the example, “the” will be unaligned. But if the
subphrases are non-empty on both sides, then all
the words on the incorrect side are aligned to all
the words on the correct side of the subphrase. In
many instances, this will occur for single word re-
placements. In the example, “sprung” is aligned
with “sprang” in this manner.

4.2 Rule extraction

We follow the GHKM transducer rule extraction
algorithm described in (Galley et al., 2004) and
(Galley et al., 2006). Given a training example
(π, i, a), where π is the correct tree, i the incorrect
sentence and a the alignment, rules are extracted
for a tree-to-string derivation of (π, i) that is min-
imally consistent with the alignment a. Counts
of how many times each rule is extracted over all
the training examples are used to estimate the rule
probabilities. A training example is represented as
a directed graph as in Figure 1(a), with the edges

46



going downward.
For each of the nodes in π, we compute a span

and a complement span with respect to the nodes
in i. The span of a node n is defined by the indexes
of the first and last words in f that are reachable
from n. The spans of the leaves in the tree (the
words of the correct sentence) are defined by a,
and the spans of the other nodes can be computed
bottom-up for each node from the spans of its child
nodes. The complement span of n is the union of
the spans of all nodes that are neither ancestors nor
descendants of n. The complement spans can be
computed top-down for each node by taking the
union of the complement span of its parent and
the spans of its siblings. Nodes whose spans and
complement spans do not overlap, are called fron-
tier nodes. From each frontier node, a rule can be
extracted: The left hand side of the rule is a subtree
rooted at n. The subtree is extracted by traversing
π top-down from n, replacing all frontier nodes
reached with variables (as more rules will be ex-
tracted from there). The right hand side is formed
by the words of the span of n of the incorrect side,
with the span of each left hand side frontier node
replaced by the corresponding variable.

Figure 1(b) gives sample rules extracted from
the training example in Figure 1(a). In the example
tree, all the constituent nodes are frontier nodes,
as there are no complex rewrites. For constituents
under which no changes are made, CFG-like rules
such as (1) and (2) are extracted. In the case
where a single word is substituted (in the example,
“sprang” with “sprung”), a rule for this substitu-
tion will be extracted (3). If there were no align-
ment between these words, the algorithm would
have attached the word “sprung” to the rule headed
by SBAR (4), which would clearly not have been
linguistically sensible. In the case of the deletion
of a word in the incorrect sentence, that word will
left be unaligned (“the” in the example). The rule
for this word (5) will have as head the lowest node
that spans the words in i to the left and right of
the unaligned word – in the example, the PP node.
Rewrite rules for aligned words in phrase edits are
also extracted (6).

4.3 Additional rules

We add rules to the transducer that involve words
in the vocabulary. Whichever of these rules have
not already been extracted from the training data
will be assigned a rule count of 0, otherwise their

Non-lexicalized
q.S (x0:NP x1:VP)→ q.x0 q.x1 −0.596
q.S (x0:VP x1:VP)→ q.x0 q.x1 −5.781

q.VP (x0:VP x1:SBAR)→ q.x0 q.x1 −3.723
Word identity

q.NN (work)→ work −2.614
q.VBP (work)→ work −2.475

q.DT (the)→ the −0.183
Single word substitution

q.NN (work)→ works −4.343
q.VBP (work)→ working −4.541
q.VBZ (works)→ work −4.802

q.DT (the)→ a −3.100
q.IN (of )→ from −3.901

Phrase substitution
q.NP (DT (the) NN (right))→ rights −5.109

q.VP (VBG (being) VP (VBN (researched)))
→ under researching −6.272

Context-sensitive phrase substitution
q.VP (TO (to) VP (VB (work) x0:PP))

→ working q.x0 −6.272
q.PP (IN (in) S (VP (VBG (generating) x0:NP)))

→ to generate q.x0 −5.480
Context-sensitive word insertion and deletion

q.NP (DT (the x0:NN)→ q.x0 −2.634
q.VP (VBZ (has) x0:VP)→ q.x0 −5.202

q.VP (x0:VB x1:NP)→ q.x0 into q.x1 −5.203

Table 1: Example transducer rules by type, with
log probability weights.

rule counts will be left unchanged.
We need to ensure that there are lexical rewrite

rules for all the words in our vocabulary. For each
of the words in the vocabulary we find one or two
possible part-of-speech tags, using the NLTK POS
tagger. We add word identity rules in the form of
the examples in Table 1. An identity rule is also
added for the <unk> symbol.

Additional rules are added for noun number and
verb form errors, using the word groups extracted
from the vocabulary and WordNet. These rules
perform substitutions between singular and plural
nouns (in both directions) and between verbs with
the same base forms. Subject-verb agreement er-
rors are also concerned with the verb form in the
sentence, so added verb form rules will also be ap-
plicable to such errors. Examples of these single
word substitutions are given in Table 1. Since de-
terminer and preposition errors are restricted to a
relatively small number of possible substitutions,
we assume that all relevant rules involving these
errors have already been extracted from the train-
ing data.

See Table 1 for rule examples categorized by
the type of rewrite the rule performs. Examples
of rules for all the error types under consideration
are included. Log probability rule weights are also

47



given. Phrase substitution rules can be fully lexi-
cal (without variables) or context-sensitive (when
they have variables). Word insertions and dele-
tions will always be context-sensitive.

5 Transducer Model Decoding

5.1 Sentence to clause splitting

A challenge to our transducer model on the NU-
CLE dataset is the length of sentences. On the
training data, 46% of sentences have length greater
than 20 and 13% have length greater that 30.
The decoding time of our model increases sharply
when the length of sentences becomes greater than
20. For lengths greater that 30 decoding is not
practically feasible on our available computational
resources. In order to address this problem, we
perform linguistically motivated sentence splits to
decrease the length of sentences passed to the de-
coder. Clauses that are still longer than 30 words
are not decoded. Decoding was performed on a
desktop computer with 8GB RAM. To keep the
overall decoding time reasonable, we restrict de-
coding to take no more than 1 minute per sentence
on average.

Sentence splitting is based on constituency
parses (obtained with the Berkeley parser) of the
incorrect sentences under consideration. Sen-
tences are split at clause level, using the heuristics
described below. The goal is to extracted clauses
that have a form similar to that of full sentences.

We distinguish between S-clauses, that are indi-
cated by S, SINV and SQ parse tree constituents,
and SBAR-clauses, indicated by SBAR or SBARQ
parse tree constituents. An SBAR-clause usually
consists of an introductory subordinating conjunc-
tion or wh-word, followed by an S-clause.

We perform splits on S-clauses. A clausal split
is performed between the phrase before the start
position of the S-clause and the phrase after that
position. If the parse tree node of the S-clause
is the child of an SBAR-clause node, the split is
performed between the phrase before the starting
position of the SBAR-clause, and the phrase after
the start of the S-clause. The introductory words in
the SBAR-clause are excluded from the extracted
clauses.

Splits are also performed between some phrases
separated by a coordinating conjunction, which is
indicated by a CC tag in the parse tree. Such a split
is performed only if the CC node is a child of an S-
clause node. The phrase before the conjunction is

split from the phrase after the conjunction, while
the conjunction itself is excluded.

After decoding and reranking has been per-
formed, the clauses are recombined to reconstruct
the original sentences. For each clause the highest-
scoring correct clause is chosen. Finally, the origi-
nal case of all the words in the sentence is restored,
as all words were lowercased in the model.

5.2 k-Best decoding

When performing decoding with the transducer
model, we need to find the highest-scoring candi-
date correct sentences, so that we can in turn find
the best sentence according to the overall model.
We found that a good trade-off between speed and
accuracy is to find a list of trees of the 1000-
best derivations for a given (incorrect) sentence.
The weights of different derivations for which the
parse trees have the same yields, are summed to
find weights for each of the hypothesis sentences.
Note that this is an approximation of the summa-
tion in equation (2), which is taken over all parse
trees with the same yield.

In our implementation the weighted tree trans-
ducer package Tiburon (May and Knight, 2006)
is used. Tiburon implements generic operations
on regular tree grammars, tree-to-tree and tree-to-
string transducers. We use Tiburon to perform de-
coding in our model, using its implementation of
backwards application and k-best decoding.

The decoding algorithm implemented by
Tiburon is based on a weighted version of the
Earley parsing algorithm (May, 2010, chap. 4).
Empirically, large rules have a detrimental impact
on the decoding speed of the algorithm. To
address this problem, we extract rules from
binarized parse trees, which results in smaller
rules than using non-binarized parse trees. In
Figure 1(a), the node @VP indicates that a bi-
narization has been performed on the subtree
VP (VBD PRT PP). All remaining rules that
have more that four variables are removed.

As the search space of the model is large, we
need to apply some heuristic pruning. Following
practices used in parsing models such as Huang
and Chiang (2005), beam search is performed.
The cell limit γ, the maximum number of hypothe-
ses that can be kept at a state in the search process,
is set to 30. The beam width β is set to 10−4. This
means that if a hypothesis score is worse than β
times the score of the best partial hypothesis found

48



up to a specific point in the model, the hypothesis
is discarded. γ and β were set to make decoding
feasible on available computational resources.

The heuristic pruning may undermine some of
the advantages our model might have in taking
whole sentence analyses into account to generate
error corrections. However, we find that despite
this, the model is still able to generate hypothesis
corrections that take non-local dependencies into
consideration.

6 Language Model Reranking

Although the transducer model defines a joint
probability distribution and is therefore sufficient
to find corrections for given sentences, incorpo-
rating an n-gram language model in our system
significantly increases its performance. The main
reason for this is that the generative transducer
model alone does not have enough discriminative
power to distinguish between well-formed and un-
grammatical sentences.

6.1 Evaluation

The standard evaluation metric used for grammat-
ical error correction is precision, recall and F1
score. Changes made to a given incorrect sentence
are represented by edits. For a sample sentence,
the sufficient statistics for this evaluation metric is
the 3-tuple (#correct system edits, #system edits,
#gold standard edits). This can be summed over
all the examples being evaluated, and the preci-
sion, recall and F1 scores can be computed from
that.

The shared task uses the M2 scorer, as de-
scribed by Dahlmeier and Ng (2012). Given the
original and system sentences, possible system
edit sequences are represented with a lattice. The
edit sequence that is the best match with the gold
standard edit sequence is chosen to compute the
edit scores.

6.2 Reranking

During decoding we compute the language model
score for each of the hypothesis sentences gener-
ated by our transducer model for a given incor-
rect sentence. The log probability scores of the
transducer and language models are normalized
by the length of the incorrect sentence. In order
to weigh these two scores, the transducer score is
kept fixed, and the language model score is multi-
plied by a weight α. For a given incorrect sentence

Data set Precision Recall F1 score
Validation 0.065 0.153 0.092
Development 0.079 0.149 0.103
Test (original) 0.2700 0.1333 0.1785
Test (revised) 0.3712 0.1891 0.2505

Table 2: Model results

i and a generated set of hypothesis correct sen-
tences H(i), we want to find

ĉ = arg max
c∈H(i)

[TT (c, i) + α · LM(c)] (4)

where TT (c, i) gives the tree transducer score
and LM(c) gives the language model score. The
parameter α is set to maximize the F1 score of the
model on a validation set. Let I be the set of in-
correct sentences in this set. Then we want to find

α̂ = arg max
α

F1[Σi∈Iedits(ĉ, i, g(i))] (5)

where ĉ is given by (4) and edits is the sufficient
statistics for the F1 score of ĉ for the incorrect sen-
tence i and gold standard edits g(i).

7 Results

We now present results of the model on our val-
idation and development sets, as well as on the
official test set. A useful measure to analyze the
performance of our model is to perform oracle
reranking on the hypothesis sets generated by the
transducer model. For each sentence, the oracle
picks the hypothesis that will contribute to the best
possible F1 score. We are especially interested in
how frequently the correct sentence is among the
hypothesis sentences – this is called the hypothesis
coverage.

7.1 Development sets

On the development set, only 21% of clauses are
annotated with corrections. For clauses that have
no annotations, the hypothesis coverage is 99%,
while for clauses that have annotations the hypoth-
esis coverage is 49%. The oracle obtains a 0.64 F1
score.

We tune the value of α on the validation set to
maximize the F1 score. The best F1 score is ob-
tained with α = 1.6949. The system results on the
validation set and the development set with this α
are given in Table 2. It was found that a strong

49



Error type Development Test
recall recall

Noun number 0.2231 0.1818
Verb form 0.1839 0.1475
Article or determiner 0.1564 0.1261
Preposition 0.1655 0.0932
Subject-verb agreement 0.0957 0.1048

Table 3: Recall for each error type, on the devel-
opment set and original test set.

weight on the language model (a relatively large
α) increases the recall of the model.

A breakdown of the recall for each error type is
given in Table 3. On the development set, the best
recall is obtained for noun number errors, and the
worst for subject-verb agreement errors. A reason
for the relatively low performance on agreement
errors may be due to the constituency parse tree
representation used. In a clause, the subject noun
phrase and the predicate verb phrase, whose head
verb must agree with the subject, are in different
subtrees. This increases the difficulty in modelling
the dependency between the subject and the verb.

7.2 Test set

The test set released for this shared task consists of
1381 sentences, which we split into 2247 clauses
using the heuristic described above. The distribu-
tion of sentence lengths is very similar to that of
the training data. The number of out of vocabu-
lary words is quite small at 0.03%. The set does
not include any URLs, and the general impression
was that it is less noisy than the training data.

The system result on the test set is given in Ta-
ble 2. Scores for both the original and revised test
data annotations are given. We submitted plau-
sible corrections suggested by our system for the
gold standard revision. This contributed to a sig-
nificant increase in our model score on the revised
annotations. The model recall on the test set is
similar to that of the development on most error
types, though the preposition error recall is signifi-
cantly lower and the subject-verb agreement recall
is slightly higher. This may indicate that prepo-
sition error correction rules in the model does not
generalize well enough.

The precision of our model is significantly bet-
ter on the test set than on the development set. This
can be explained by differences in the character-
istics of the test set. The relative occurrence of

annotated errors is much higher in the test set than
in the development set: 46% of clauses have cor-
rections. It has been found previously that a low
frequency of errors increase the difficulty of the
correction task (Dahlmeier and Ng, 2011). This is
caused especially by an increase in the number of
system edits suggested for sentences that should
not be changed. Our oracle found that for sen-
tences that should not be changed, 100% of the
correct unchanged hypotheses were generated by
the tree transducer, while for sentences that should
be changed, 50% of hypothesis sets contained the
correct result. The oracle obtains a 0.75 F1 score.
The precision of the oracle model increases sig-
nificantly, from 0.65 to 0.95. Varying the choice
of α controls the trade-off between precision and
recall better on the test set than on the validation
set. These results indicate that our model is more
suited for data with the characteristics of the test
set than for data similar to the development sets.

8 Conclusion

We presented a novel approach to grammatical er-
ror correction based on tree transducers, obtaining
promising results. One of the weaknesses of our
model is handling insertions and deletions. The
model performs too many unnecessary deletions,
especially removing content words or non-article
determiners. It also has difficulty in finding edits
where insertions such as article insertions should
be performed.

For future work, ways of constructing better
rule sets for the transducer should be investigated
to take more dependencies into consideration and
to improve probability estimates. Techniques to
improve the runtime of the decoding algorithm
while minimizing the loss in accuracy caused by
heuristic pruning should be considered. Alterna-
tive approaches to reranking could also be investi-
gated. Including additional features may increase
the ability of the model to discriminate between
grammatical and ungrammatical sentences.

Acknowledgement

The financial support of MIH is acknowledged.

50



References
Steven Bird, Ewan Klein, and Edward Loper.

2009. Natural Language Processing with Python.
O’Reilly Media.

Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
SMT techniques. In Proceedings of ACL, pages
249–256.

Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of ACL, pages 915–923.

Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of HTL-NAACL, pages 568–572.

Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS corpus of learner English. In
Proceedings of the 8th Workshop on Innovative Use
of NLP for Building Educational Applications, At-
lanta, Georgia, USA.

Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, pages 242–249, Nancy, France.

Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the 7th Workshop on the Innovative Use of
NLP for Building Educational Applications, pages
54–62, Montreal, Canada.

Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of HLT-NAACL, pages 273–280.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL, pages 961–968.

Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391–427.

Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies, pages 53–64.

Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model of a speech rec-
ognizer. IEEE Transactions on Acoustics, Speech,
and Signal Processing, 35(3):400–401.

Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammat-
ical Error Detection for Language Learners. Syn-
thesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.

John Lee and Stephanie Seneff. 2006. Automatic
grammar correction for second-language learners.
In Proceedings of Interspeech, pages 1978–1981.

Jonathan May and Kevin Knight. 2006. Tiburon: A
weighted tree automata toolkit. In CIAA, volume
4094 of Lecture Notes in Computer Science, pages
102–113. Springer.

Jonathan May. 2010. Weighted Tree Automata and
Transducers for Syntactic Natural Language Pro-
cessing. Ph.D. thesis, University of Southern Cal-
ifornia.

George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39–41.

Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proceedings of CoNLL.

Y. Albert Park and Roger Levy. 2011. Automated
whole sentence grammar correction using a noisy
channel model. In Proceedings of ACL, pages 934–
944.

Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, pages 404–411.

Andreas Stolcke. 2002. SRILM: An extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901–904.

Jenine Turner and Eugene Charniak. 2007. Language
modeling for determiner selection. In Proceedings
of HTL-NAACL, pages 177–180.

51


