Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1029–1037,

Beijing, August 2010

1029

EMMA: A Novel Evaluation Metric for Morphological Analysis

Sebastian Spiegler

Intelligent Systems Group

University of Bristol

Christian Monson

Center for Spoken Language Understanding

Oregon Health & Science University

spiegler@cs.bris.ac.uk

monsonc@csee.ogi.edu

Abstract

We present a novel Evaluation Metric
for Morphological Analysis
(EMMA)
that is both linguistically appealing and
empirically sound. EMMA uses a graph-
based assignment algorithm, optimized
via integer linear programming, to match
morphemes of predicted word analyses
to the analyses of a morphologically rich
answer key. This is necessary especially
for unsupervised morphology analysis
systems which do not have access to
linguistically motivated morpheme labels.
Across 3 languages, EMMA scores of
14 systems have a substantially greater
positive correlation with mean average
precision in an information retrieval
(IR) task than do scores from the metric
currently used by the Morpho Challenge
(MC) competition series. We compute
EMMA and MC metric scores for 93
separate
from
the 2007, 2008, and 2009 MC compe-
titions, demonstrating that EMMA is
not susceptible to two types of gaming
that have plagued recent MC competi-
tions: Ambiguity Hijacking and Shared
Morpheme Padding. The EMMA eval-
uation script is publicly available from
http://www.cs.bris.ac.uk/
Research/MachineLearning/
Morphology/Resources/.

system-language pairs

Introduction

1
Words in natural language are constructed from
smaller building blocks called morphemes. For

example, the word wives breaks down into an un-
derlying stem, wife, together with a plural sufﬁx.
Analyzing the morphological structure of words
is known to beneﬁt a variety of downstream nat-
ural language (NL) tasks such as speech recogni-
tion (Creutz, 2006; Arısoy et al., 2009), machine
translation (Oﬂazer et al., 2007), and information
retrieval (McNamee et al., 2008).

A variety of automatic systems can morpholog-
ically analyze words that have been removed from
their surrounding context. These systems range
from hand-built ﬁnite state approaches (Beesley
and Karttunen, 2003) to recently proposed algo-
rithms which learn morphological structure in an
unsupervised fashion (Kurimo et al., 2007). Since
unsupervised systems do not have access to lin-
guistically motivated morpheme labels, they typ-
ically produce morphological analyses that are
closely related to the written form. Such a system
might decompose wives as wiv -es. Meanwhile,
a hand-built system might propose wife_N +Plu-
ral, or even parse wives as a hierarchical feature
structure. As morphological analysis systems pro-
duce such varied outputs, comparing decomposi-
tions from disparate systems is a challenge.

This paper describes EMMA, an Evaluation
Metric for Morphological Analysis that quantita-
tively measures the quality of a set of morpholog-
ical analyses in a linguistically adequate, empir-
ically useful, and novel fashion. EMMA evalu-
ates analyses that can be represented as a ﬂat set
of symbolic features, including hierarchical repre-
sentations, which can be projected down to a lin-
earized form (Roark and Sproat, 2007).

An automatic metric that discriminates be-
tween proposed morphological analyses should

1030

fulﬁll certain computational and linguistic crite-
ria. Computationally, the metric should:
1. Correlate with the performance of real-world
NL processing tasks which embed the morpho-
logical analyses.

2. Be Readily Computable: The metric will only
be useful if it is less time consuming and easier
to compute than the larger NL task.

3. Be Robust: The metric should be difﬁcult to
game and should accurately reﬂect the distri-
bution of predicted and true morphemes.

4. Be Readily Interpretable: When possible, the
ﬁnal numeric score should directly identify the
strengths and weaknesses of the underlying
morphological analysis system.

While accounting for these computational re-
quirements, a morphology metric should still re-
ward accurate models of linguistic structure.
In
particular, the metric should account for:
1. Morphophonology: Applying a morphological
rule may alter the surface form of stem or af-
ﬁx. In the word wives, /waivz/, a rule of mor-
phophonology voices the stem-ﬁnal /f/ of wife,
/waif/, when the plural sufﬁx is added. A met-
ric should penalize for not placing wives and
wife as forms of the same lexeme.

2. Allomorphy: A metric should capture the suc-
cessful grouping of allomorphs. The German
plural has several surface allomorphs includ-
ing -en in Zeiten (times), -e in Hunde (dogs),
and -s in Autos (cars). A metric should reward
a morphological analysis system that analyzes
the different surface forms of the German plu-
ral as underlyingly identical.

3. Syncretism:

In mirror

fashion, a metric
should reward analyses that distinguish be-
tween surface-identical syncretic morphemes:
although derives and derivations both contain
an -s morpheme, one marks 3rd person singular
and the other plural.

4. Ambiguity: Finally, a metric should account
for legitimate morphological ambiguity. In He-
brew, the written word MHGR has three vi-
able morphological segmentations: M- H- GR,
“from the foreigner”, M- HGR, “from Hagar”,

and the unsegmented form MHGR, meaning
“immigrant” (Lavie et al., 2004). Absent dis-
ambiguating context, a morphological system
should be rewarded for calling out all three
analyses for MHGR.
Morphophonology,

syncretism,
and ambiguity are all common phenomena in the
world’s languages. The ﬁrst three have all re-
ceived much discussion in theoretical linguistics
(Spencer and Zwicky, 2001), while morpholog-
ical ambiguity has signiﬁcant practical implica-
tions in NL processing, e.g. in machine translation
of morphologically complex languages (Lavie et
al., 2004; Oﬂazer et al., 2007).

allomorphy,

In Section 2 we propose the metric EMMA,
which has been speciﬁcally designed to evalu-
ate morphological analyses according to our com-
putational and linguistic criteria. Section 3 then
describes and qualitatively critiques several well-
used alternative metrics. Section 4 empirically
compares EMMA against the qualitatively-strong
metric used in the Morpho Challenge competition
series (Kurimo et al., 2009). And we conclude in
Section 5.

2 EMMA: An Evaluation Metric for

Morphological Analysis

EMMA, the metric we propose for the evalua-
tion of morphological analyses, like all the met-
rics that we consider in this paper, compares pro-
posed morphological analyses against an answer
key of deﬁnitively-analyzed words from a vocab-
ulary. Since a set of proposed analyses is likely
to use a different labeling scheme than the answer
key, especially true of the output from unsuper-
vised systems, EMMA does not perform a direct
comparison among proposed and answer analy-
ses. Instead, EMMA seeks a one-to-one relabel-
ing of the proposed morphemes that renders them
as similar as possible to the answer key. EMMA,
then, measures the degree to which proposed anal-
yses approximate an isomorphism of the answer
key analyses. For exposition, we initially assume
that, for each word, a single proposed analysis
is scored against a single unambiguous answer
analysis. We relax this restriction in Section 2.3,
where EMMA scores multiple proposed analyses

1031

against a set of legitimately ambiguous morpho-
logical analyses.

To ﬁnd the most appropriate one-to-one mor-
pheme relabeling, EMMA turns to a standard al-
gorithm from graph theory: optimal maximum
matching in a bipartite graph. A bipartite graph,
G = {X, Y ; E}, consists of two disjoint sets
of vertices, X = {x1, x2, . . . , xn} and Y = {y1,
y2, . . . , ym}, and a set of edges e(xi, yj) ∈ E
such that each edge has one end in X and the other
end in Y . In EMMA, the set, A, of all unique mor-
phemes in the answer key and the set, P , of all
unique morphemes in the proposed analyses serve
as the disjoint vertex sets of a bipartite graph.

A matching M ⊆ E in a bipartite graph is de-
ﬁned as a set of edges e(xi, yj) such that no xi
or yj is repeated. A maximum matching is a
matching where no M0 with |M0| > |M| exists.
Furthermore, a weight w(xi, yj) ∈ < may be as-
signed to each edge e(xi, yj) of a bipartite graph.
An optimal assignment is a maximum matching
which also maximizes the sum of the weights of
the edges of the matching

Xe(xi,yj )∈M

w(xi, yj) .

EMMA weights the edge between a particular
answer morpheme a ∈ A and a proposed mor-
pheme p ∈ P as the number of words, w, in the
vocabulary, V , where the answer analysis of w in-
cludes morpheme a while the proposed analysis
includes p. EMMA constructs an optimal assign-
ment maximum matching in this weighted bipar-
tite morpheme graph. The edge weights ensure
that the optimal matching will link the answer and
proposed morphemes which globally occur in the
analyses of the same words most often – restrict-
ing each answer morpheme to be represented by at
most one proposed morpheme, and each proposed
morpheme to represent at most one morpheme in
the answer key. On the one hand, the restrictions
thus imposed by bipartite matching penalize sets
of proposed analyses that do not differentiate be-
tween surface-identical syncretic morphemes. On
the other hand, the same one-to-one matching re-
strictions penalize proposed analyses that do not
conﬂate allomorphs of the same underlying mor-
pheme, whether those allomorphs are phonologi-

cally induced or not. Thus, EMMA meets our lin-
guistic criteria from Section 1 of modeling syn-
cretism, allomorphy, and morphophonology.

2.1 Maximum Matching by Integer Linear

Programming

To construct the maximum matching optimal as-
signment of answer and proposed morphemes,
EMMA uses standard integer linear programming
techniques as implemented in lpsolve (Berkelaar
et al., 2004). For the purpose of our integer pro-
gram, we represent the weight of each potential
edge of the optimal bipartite morpheme assign-
ment in a count matrix C = {cij} where cij is as-
signed the number of words w ∈ V which share
morpheme ai in the answer key and pj in the pre-
diction. We then deﬁne a binary matrix B = {bij}
of the same dimensions as C. Each bij will be set
to 1 if an edge exists from ai to pj in the optimal
maximum matching, with bij = 0 otherwise. The
integer linear program can then be deﬁned as fol-
lows:

(1)

argmax

(C · B)ij

B Xi,j
s.t.Xi
where (C · B)ij = cij · bij is the element-wise
Hadamard product.

bij ≤ 1 , Xj

bij ≤ 1 ,

bij ≥ 0 ,

2.2 Performance Measures
Having settled on a maximum matching optimal
assignment of proposed and answer morphemes,
EMMA derives a ﬁnal numeric score. Let wk
be the kth word of V ; and let Ak and Pk de-
note, respectively, the sets of morphemes in the
answer key analysis of wk and predicted analysis
of wk. Furthermore, let P ∗k denote the predicted
morphemes for wk where a morpheme pj is re-
placed by ai if bij = 1. Now that Ak and P ∗k
contain morpheme labels that are directly compa-
rable, we can deﬁne precision and recall scores
for the proposed analysis of the word wk. Preci-
sion is the fraction of correctly relabeled proposed
morphemes from among all proposed morphemes
of wk; while recall is the number of correctly rela-
beled morphemes as a fraction of the answer key

1032

analysis of wk. Precision and recall of the full vo-
cabulary are the average word-level precision and
recall:

precision =

recall =

1
|V |
1
|V |

|V |Xk
|V |Xk

|AkT P ∗k|
|P ∗k|
|AkT P ∗k|

|Ak|

,

.

(2)

(3)

Finally,
cision and recall:

f-measure is the harmonic mean of pre-

f-measure =

2 · precision · recall
precision + recall

.

(4)

2.3 Morphological Ambiguity in EMMA
Thus far we have presented EMMA for the sce-
nario where each word has a single morphological
analysis. But, as we saw in Section 1 with the He-
brew word MHGR, natural language permits sur-
face forms to have multiple legitimate morpho-
logical analyses. When a word is truly ambigu-
ous, EMMA expects an answer key to contain a
set of analyses for that word. Similarly, we per-
mit sets of proposed alternative analyses. To ex-
tend EMMA with the ability to evaluate alterna-
tive analyses we ﬁrst generalize the optimal max-
imum matching of morphemes from Section 2.1.
We then deﬁne a new integer linear program to
match answer and proposed alternative analyses.
Finally, we adjust the performance measures of
Section 2.2 to account for alternatives.

2.3.1 Ambiguity and Morpheme Matching
Let Ak,r denote the rth alternative answer anal-
ysis of the kth word with 1 ≤ r ≤ mk, and let
Pk,s denote the sth alternative prediction with
1 ≤ s ≤ nk, where mk is the number of alterna-
tive analyses in the answer key and nk the num-
ber of alternative predictions for wk. We redeﬁne
Ak =Smk
s Pk,s as the set of
all answer or, respectively, predicted morphemes
of wk across all analysis alternatives. Instead of
incrementing each cij entry in the count matrix
C by a full count, we now add
to cij for
all pairs (ai, pj) ∈ Ak × Pk. This corresponds to
counting each combination of an answer key and
predicted morpheme normalized by the number of

r Ak,r and Pk =Snk

mk·nk

1

possible pairings between proposed and answer
analysis alternatives. When both the answer and
proposed analyses consist of just a single alter-
native, cij remains unchanged. Generalized mor-
pheme matching still employs the linear program
deﬁned in Equation 1.

2.3.2 Matching of Alternative Analyses
After performing a one-to-one morpheme rela-
belling that accounts for ambiguity, we need to
extend EMMA with the ability to evaluate alterna-
tive analyses. We again turn to optimal maximum
matching in a bipartite graph: Where earlier we
matched proposed and answer morphemes, now
we match full proposed and answer analysis alter-
natives, maximizing the total number of correctly
predicted morphemes across all alternatives. Gen-
eralizing on the notation of the unambiguous case,
let P ∗k,s denote the sth alternative predicted analy-
sis of the kth word where predicted morphemes
have been replaced by their assigned answer key
morphemes. We introduce a new count matrix
C0 = {c0r,s}, where c0r,s is the count of common
morphemes of the rth answer key alternative and
sth predicted alternative. Based on Equation 1,
we calculate the binary matrix B0 = {b0r,s} which
contains the optimal assignment of the alternative
answer key and predicted analyses for wk.
2.3.3 Ambiguity and Performance Scores
We now adjust EMMA’s numeric performance
measures to account for sets of ambiguous anal-
ysis alternatives. Precision becomes

1
|V |

|V |Xk

1
nk

mkXr

nkXs

b0r,s|Ak,rT P ∗k,s|
|P ∗k,s|

,

(5)

the ratio of correctly predicted morphemes across
all predicted alternatives normalised by the num-
ber of predicted alternatives, nk, and the vocab-
ulary size, |V |. The factor b0r,s guarantees that
scores are only averaged over pairs of proposed
and answer analysis alternatives that have been as-
signed, that is, where b0r,s = 1. Recall is measured
similarly with

1
|V |

|V |Xk

1
mk

mkXr

nkXs

b0r,s|Ak,rT P ∗k,s|

|Ak,r|

.

(6)

1033

Here, we normalize by mk, the number of alterna-
tive analyses for the kth word that are listed in the
answer key. The normalisation factors 1
and 1
nk
mk
ensure that predicting too few or many alternative
analyses is penalised.

3 Other Morphology Metrics
Having presented the EMMA metric for evaluat-
ing the quality of a set of morphological analyses,
we take a step back and examine other metrics that
have been proposed. Morphology analysis metrics
can be categorized as either: 1. Directly compar-
ing proposed analyses against an answer key, or 2.
Indirectly comparing proposed and answer analy-
ses by measuring the strength of an isomorphic-
like relationship between the proposed and answer
morphemes. The proposed EMMA metric belongs
to the second category of isomorphism-based met-
rics.

3.1 Metrics of Direct Inspection
By Segmentation Point. Perhaps the most read-
ily accessible automatic evaluation metric is a di-
rect comparison of the morpheme boundary posi-
tions in proposed and answer analyses. As early
as 1974, Hafer and Weiss used the direct boundary
metric. Although intuitively simple, the segmen-
tation point method implicitly assumes that it is
possible to arrive at a valid morphological anal-
ysis by merely dividing the characters of a word
into letter sequences that can be reconcatenated to
form the original word. But, by deﬁnition, con-
catenation cannot describe non-contanative pro-
cesses like morphophonology and allomorphy.
Nor does simple segmentation adequately differ-
entiate between surface-identical syncretic mor-
phemes. Despite these drawbacks, precision and
recall of segmentation points is still used in cur-
rent morphological analysis research (Poon et al.
(2009), Snyder and Barzilay (2008), Kurimo et al.
(2006)).

Against Full Analyses. To confront the reality
of non-concatenative morphological processes, an
answer key can hold full morphological analyses
(as opposed to merely segmented surface forms).
But while a hand-built (Beesley and Karttunen,
2003) or supervised (Wicentowski , 2002) mor-
phology analysis system can directly model the

annotation standards of a particular morphologi-
cal answer key, the label given to speciﬁc mor-
phemes is ultimately an arbitrary choice that an
unsupervised morphology induction system has
no way to discover.

By Hand. On the surface, scoring proposed
analyses by hand appears to provide a way to eval-
uate the output of an unsupervised morphology
analysis system. Hand evaluation, however, does
not meet our criteria from Section 1 for a robust
and readily computable metric. It is time consum-
ing and, as Goldsmith (2001) explains, leaves dif-
ﬁcult decisions of what constitutes a morpheme to
on-the-ﬂy subjective opinion.

3.2 Metrics of Isomorphic Analysis
Recognizing the drawbacks of direct evaluation,
Schone and Jurafsky (2001), Snover et al. (2002),
and Kurimo et al. (2007) propose related measures
of morphological analysis quality that are based
on the idea of an isomorphism. For reasons that
will be clear momentarily, we refer to the Schone
and Jurafsky, Snover et al., and Kurimo et al. met-
rics as soft isomorphic measures. As discussed
in Section 2, metrics of isomorphism measure
similarities between the distribution of proposed
morphemes and the distribution of answer mor-
phemes, where proposed and answer morphemes
may be disjoint symbol sets.

Unlike the EMMA metric proposed in Section
2, the soft metrics of isomorphism do not seek
to explicitly link proposed morphemes to answer
morphemes. Instead, their metrics group sets or
pairs of words which share, in either the pro-
posed analyses or in the answer analyses, a stem
(Schone and Jurafsky, 2001; Snover, 2002), a suf-
ﬁx (Snover et al., 2002), or any arbitrary mor-
pheme (Kurimo et al., 2007). The soft met-
rics subsequently note whether these same sets or
pairs of words share any morpheme in the answer
key or, respectively, in the proposed analyses. By
foregoing a hard morpheme assignment, the soft
metrics do not adequately punish sets of proposed
and answer morphemes which fail to model syn-
cretism and/or allomorphy. For example, pro-
posed analyses that annotate 3rd person singular
and plural with a single undifferentiated +s mor-
pheme will receive recall credit for both nouns and

1034

verbs.

3.3 The Morpho Challenge Metric
The Morpho Challenge (MC) competition series
for unsupervised morphology analysis algorithms
(Kurimo et al., 2009) has used a soft metric of iso-
morphism in its most recent three years of compe-
tition: 2007, 2008, and 2009. According to Ku-
rimo et al. (2009) the Morpho Challenge (MC)
measure samples random word pairs which share
at least one common morpheme. Precision is cal-
culated by generating random word pairs from
the set of proposed analyses and then compar-
ing the analyses of the word pairs in the answer
key. The fraction of found and expected common
morphemes is normalised by the number of words
which are evaluated. Recall is deﬁned in mirror
fashion. The MC metric also normalizes preci-
sion and recall scores across sets of alternative
analyses for each word in the proposal and answer
key. To our knowledge the MC metric is the ﬁrst
isomorphism-based metric to attempt to account
for morphological ambiguity. As we show in Sec-
tion 4, however, MC’s handling of ambiguity is
easily gamed.

The MC metric does meet our criterion of being
readily computable and, as we will show in the ex-
perimental section, the metric also correlates to a
certain extent with performance on a higher-level
natural language processing task. The downside
of the MC metric, however, is robustness. In addi-
tion to MC’s crude handling of ambiguity and its
over-counting of allomorphs and syncretic mor-
phemes, the random pair sampling method that
MC uses is not independent of the set of analyses
being evaluated. If two algorithms predict differ-
ent morpheme distributions, the sampling method
will ﬁnd different numbers of word pairs. We sub-
stantiate our claim that the MC metric lacks ro-
bustness in Section 4 where we empirically com-
pare it to the EMMA metric.

4 Experimental Evaluation
To experimentally evaluate our newly proposed
EMMA metric, and to quantitatively compare the
EMMA and MC metrics, we have evaluated re-
sults of 93 system-language pairs from Morpho

Challenge 2007, 2008, and 2009.1 The evaluation
comprised three algorithms by Bernhard (2007)
and Bernhard (2009), one algorithm by Can and
Manandhar (2009), the MC baseline algorithm
Morfessor by Creutz (2006), UNGRADE by Gole-
nia et al. (2009), two algorithms by Lavallee and
Langlais (2009), one algorithm by Lignos et al.
(2009), ﬁve ParaMor versions by Monson et al.
(2008) and Monson et al. (2009), three Promodes
versions by Spiegler et al. (2009) and one al-
gorithm by Tchoukalov et al. (2009). We ran
these algorithms over six data sets available from
the MC competition: Arabic (vowelized and non-
vowelized), English, Finnish, German, and Turk-
ish. We then scored the system outputs using both
EMMA and the MC metric against an answer key
provided by MC. In Sections 2 and 3.3 we have al-
ready commented on the linguistic characteristics
of both metrics. In this section, we concentrate on
their computational performance.

Both the EMMA and MC metrics are readily
computable: Both are freely available2 and they
each take less than two minutes to run on the av-
erage desktop machines we have used. In terms
of interpretability, EMMA not only returns the
performance as precision, recall and f-measure
as MC does, but also provides predicted analy-
ses where mapped morphemes are replaced by an-
swer key morphemes. This information is help-
ful when judging results qualitatively since it ex-
poses tangible algorithmic characteristics. In Ta-
ble 1 we present the algorithms with the highest
MC and EMMA scores for each language. For
all languages, the EMMA and MC metrics place
different algorithms highest. One reason for the
signiﬁcantly different rankings that the two met-
rics provide may be the sampling of random pairs
that MC uses. Depending on the distribution of
predicted morphemes across words, the number
of random pairs, which is used for calculating the
precision, may vary. For instance, on vowelized
Arabic, Promodes 1 is evaluated over a sample
of 100 pairs where MC selected just 47 pairs for
ParaMor Mimic.

1Detailed results can be found in Spiegler (2010).
2EMMA may be downloaded from http://www.

cs.bris.ac.uk/Research/MachineLearning/
Morphology/Resources/

1035

Algorithm and year of
participation in MC

MC evaluation metric
F1
Pr.

Re.

Language

Arabic (nv)

Promodes 2
Ungrade

Finnish

English

Arabic (vw) Promodes 2
Promodes 1
Bernhard 1
Lignos
ParaMorPlusMorfessor
Lavallee rali-cof
ParaMorPlusMorfessor
Morfessor
ParaMorPlusMorfessor
Morfessor

German

Turkish

2009
2009
2009
2009
2007
2009
2008
2009
2008
2009
2008
2009

0.7789
0.7971
0.5946
0.7381
0.7850
0.7446
0.5928
0.6731
0.5562
0.6528
0.6779
0.7894

0.3980
0.1603
0.6017
0.3477
0.5763
0.4716
0.5675
0.3563
0.6077
0.3818
0.5732
0.3330

0.5268
0.2670
0.5982
0.4727
0.6647
0.5775
0.5798
0.4659
0.5808
0.4818
0.6212
0.4684

EMMA evaluation metric

Pr.

0.5356
0.7017
0.4051
0.5588
0.8029
0.9146
0.2271
0.5061
0.3633
0.7311
0.3476
0.5901

Re.

0.2444
0.2490
0.3199
0.3281
0.7460
0.6747
0.3428
0.4065
0.4948
0.5556
0.4315
0.3703

F1

0.3356
0.3675
0.3575
0.4135
0.7734
0.7766
0.2732
0.4509
0.4190
0.6314
0.3851
0.4550

Table 1: Best performing algorithms with MC and EMMA evaluation metric.

Algorithm and year of
participation in MC

MC evaluation metric
Pr.
F1

Re.

Morfessor
ParaMor
ParaMorPlusMorfessor
Paramor Morfessor Union

2009
2008
2008
2009

0.8143
0.4111
0.5928
0.4374

0.2788
0.4337
0.5675
0.5676

0.4154
0.4221
0.5798
0.4941

EMMA evaluation metric

Pr.

0.4751
0.4322
0.2271
0.3878

Re.

0.3472
0.3770
0.3428
0.4530

F1

0.4012
0.4027
0.2732
0.4178

Table 3: Gaming MC with ambiguity hijacking on Finnish.

Looking at any particular algorithm-language
pair, the EMMA and MC scores differ consider-
ably and respective raw scores are not directly
comparable. More interesting is the extent to
which both metrics correlate with real NL tasks.
Table 2 lists the Spearman rank correlation co-
efﬁcient for algorithms from MC 2009 on En-
glish, Finnish and German comparing rankings of
f-measure results returned by either MC or EMMA
against rankings using the mean average preci-
sion (MAP) of an information retrieval (IR) task.3
All MAP scores are taken from Kurimo et al.
(2009). Although both metrics positively correlate
with the IR results; EMMA’s correlation is clearly
stronger across all three languages.

To test the robustness of the EMMA and MC
metrics, we performed two experiments where we
intentionally attempt to game the metrics – ambi-
guity hijacking and shared morpheme padding. In
both experiments, the MC metric showed vulnera-
bility. Ambiguity hijacking results for Finnish ap-

3Detailed results can be found in Spiegler (2010).

pear in Table 3, other languages perform similarly.
Using both metrics, we scored the Finnish analy-
ses that were proposed by a) the Morfessor algo-
rithm alone, b) ParaMor alone, and c) two ways
of combining ParaMor and Morfessor: ParaMor-
PlusMorfessor simply lists the ParaMor and Mor-
fessor analyses as alternatives – as if each word
were ambiguous between a ParaMor and a Mor-
fessor analysis; ParaMorMorfessorUnion, on the
other hand, combines the morpheme boundary
predictions of ParaMor and Morfessor into a sin-
gle analysis. The ParaMorPlusMorfessor system
games the ambiguity mechanism of the MC met-
ric, achieving an f-measure higher than that of any
of the three other algorithms. EMMA, however,
correctly discovers that the analyses proposed by
ParaMorPlusMorfessor lie farther from an iso-
morphism to the the answer key than do the uni-
ﬁed analyses of ParaMorMorfessorUnion.

In Table 4 we show a second way of gaming
the MC metric – shared morpheme padding. We
add the same unique bogus morpheme to each
proposed analysis of every word for all systems.

1036

Language

Arabic (nv)
Arabic (vw)
English
Finnish
German
Turkish

Precision
0.91±0.02
0.85±0.04
0.36±0.08
0.57±0.08
0.43±0.08
0.58±0.09

MC evaluation

EMMA evaluation

Recall

F-measure

Precision
10.83 ± 8.33 7.20±5.10 0.91±0.05
11.17±8.81
7.13±5.23 0.89±0.07
2.02±0.66
0.73±0.15
0.63±0.10
3.07±2.47
0.87±0.19
1.19±0.68
2.90±1.45
0.84±0.16
0.80±0.17
2.95±1.65
1.19±0.37
0.85±0.08

Recall
1.30±0.07
1.21±0.06
1.05±0.08
1.12±0.10
1.09±0.08
1.07±0.04

F-measure
1.20±0.05
1.12±0.05
0.86±0.12
0.99±0.14
0.94±0.11
0.97±0.05

Table 4: Gaming MC with shared morpheme padding: Average and standard deviations of the ratio of
padded to original scores.

Padding analyses with a shared morpheme signif-
icantly increases the recall scores of the MC met-
ric. We summarize our experimental results by
calculating, for each language-algorithm pair, the
ratio of the score for the padded analyses as com-
pared to that of the original, unpadded analyses.
Table 4 reports average and standard deviation of
the ratios across all systems for each language. In
Arabic (nv. and vw.), the recall increases by 10.83
and 11.17 times, which leads to an inﬂation of f-
measure by 7.20 and 7.13 times – this is a direct
result of the soft nature of the MC isomorphism.
In contrast, EMMA’s recall scores increase much
less than MC’s do, and EMMA’s precision scores
decrease proportionately. A small change to the
set of proposed analyses does not lead to a huge
difference in f-measure – characteristic of a more
robust metric.

5 Conclusion
This paper has proposed, EMMA, a novel evalua-
tion metric for the assessment of the quality of a
set of morphological analyses. EMMA’s:
1. Coverage of the major morphological phenom-

ena,

Correlation with IR

IR vs. MC IR vs. EMMA

English
Finnish
German

0.466
0.681
0.379

0.608
0.759
0.637

Table 2: Spearman rank correlation coefﬁcient of
metrics vs. Information Retrieval (IR).

2. Correlation with performance on natural lan-

guage processing tasks, and

3. Computational robustness
all recommend the the metric as a strong and use-
ful measure – particularly when evaluating un-
supervised morphology analysis systems which,
lacking access to labeled training data, are unin-
formed of the labeling standard used in the answer
key.

Acknowledgements
We would like to acknowledge various fruitful
discussions with Aram Harrow, Alex Popa, Tilo
Burghardt and Peter Flach. The work was par-
tially sponsored by EPSRC grant EP/E010857/1
Learning the morphology of complex synthetic
languages, as well as by NSF Grant #IIS-0811745
and DOD/NGIA grant #HM1582-08-1-0038.

References
Arısoy, Ebru, Do˘gan Can, Sıddıka Parlak, Ha¸sim Sak,
and Murat Saraçlar. 2009. Turkish Broadcast News
Transcription and Retrieval. IEEE Trans. on Audio,
Speech and Lang. Proc.

Beesley, Kenneth R. and Lauri Karttunen.

2003.
Finite State Morphology. University of Chicago
Press.

Berkelaar, Michel, Kjell Eikland, and Peter Note-
baert.
2004. Open source (mixed-integer) lin-
ear programming system, version 5.1.0.0. http:
//lpsolve. sourceforge.net/.

Bernhard, Delphine.

2007. Simple morpheme la-
belling in unsupervised morpheme analysis. Work-
ing Notes, CLEF 2007 Workshop.

1037

Bernhard, Delphine. 2009. Morphonet: Exploring the
use of community structure for unsupervised mor-
pheme analysis. Working Notes, CLEF 2009 Work-
shop.

Can, Burcu and Suresh Manandhar. 2009. Unsuper-
vised learning of morphology by using syntactic cat-
egories. Working Notes, CLEF 2009 Workshop.

Creutz, Mathias. 2006.

Induction of the Morphol-
ogy of Natural Language: Unsupervised Morpheme
Segmentation with Application to Automatic Speech
Recognition. Ph.D. thesis, Helsinki University of
Technology, Espoo, Finland.

Goldsmith, John. 2001. Unsupervised learning of the
morphology of a natural language. Comp. Ling., 27.

Monson, Christian, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. Paramor and morpho challenge
2008. Working Notes, CLEF 2008 Workshop.

Monson, Christian, Kristy Hollingshead, and Brian
2009. Probabilistic paramor. Working

Roark.
Notes, CLEF 2009 Workshop.

Oﬂazer, Kemal, and ˙Ilknur Durgar El-Kahlout. 2007.
Different Representational Units in English-to-
Turkish Statistical Machine Translation. Proc. of
Statistical Machine Translation Workshop at ACL
2007.

Poon, Hoifung, Colin Cherry and Kristina Toutanova
2009. Unsupervised Morphological Segmentation
with Log-Linear Models. Proc. of ACL.

Golénia, Bruno, Sebastian Spiegler, and Peter Flach.
2009. Ungrade: unsupervised graph decomposi-
tion. Working Notes, CLEF 2009 Workshop.

Roark, Brian and Richard Sproat. 2007. Computa-
tional Approaches to Morphology and Syntax. Ox-
ford Univ. Press.

Hafer, M. A. and S. F. Weiss. 1974. Word segmenta-
tion by letter successor varieties. Inf. Storage and
Retrieval, 10.

Schone, Patrick and Daniel Jurafsky. 2001. Know-
lege-free induction of inﬂectional morphologies.
Proc. of NAACL-2001.

Kurimo, Mikko, Mathias Creutz, Matti Varjokallio,
Ebru Arisoy, Murat Saraclar. 2006. Unsupervised
segmentation of words into morphemes - Morpho
Challenge 2005. Interspeech.

Kurimo, Mikko, Mathias Creutz, and Ville Turunen.
2007. Overview of morpho challenge in CLEF
2007. Working Notes, CLEF 2007 Workshop.

Kurimo, Mikko and Ville Turunen. 2008. Unsuper-
vised Morpheme Analysis Evaluation by IR exper-
iments – Morpho Challenge 2008. Working Notes,
CLEF 2008 Workshop.

Kurimo, Mikko, Sami Virpioja, and Ville T. Turunen.
2009. Overview and results of morpho challenge
2009. Working Notes, CLEF 2009 Workshop.

Lavallee, Jean-Francois and Philippe Langlais. 2009.
Morphological Acquisition by Formal Analogy.
Working Notes, CLEF 2009 Workshop.

Lavie, Alon, Erik Peterson, Katharina Probst, Shuly
Wintner, Yaniv Eytani.
2004. Rapid Prototyp-
ing of a Transfer-based Hebrew-to-English Machine
Translation System. Proc. of TMI-2004.

Lignos, Constantine, Erwin Chan, Mitchell P. Mar-
cus, and Charles Yang. 2009. A rule-based unsu-
pervised morphology learning framework. Working
Notes, CLEF 2009 Workshop.

McNamee, Paul, Charles Nicholas, and James May-
ﬁeld. 2008. Don’t Have a Stemmer? Be Un+con-
cern+ed Proc. of the 31st Anual International ACM
SIGIR Conference 20-24 July 2008.

Snover, Matthew G., Gaja E. Jarosz and Michael R.
Brent. 2002. Unsupervised Learning of Morphol-
ogy Using a Novel Directed Search Algorithm: Tak-
ing the First Step. Proc. of the ACL-02 SIGPHON
Workshop.

Snyder, Benjamin and Regina Barzilay. 2008. Unsu-
pervised Multilingual Learning for Morphological
Segmentation. Proc. of ACL-08: HLT.

Spencer, Andrew and Arnold M. Zwicky, editors.
2001. The Handbook of Morphology. Wiley-Black-
well.

Spiegler, Sebastian, Bruno Golénia, and Peter A.
Flach. 2009. Promodes: A probabilistic genera-
tive model for word decomposition. Working Notes,
CLEF 2009 Workshop.

Spiegler, Sebastian. 2010. EMMA: A Novel Metric for
Morphological Analysis - Experimental Results in
Detail. Computer Science Department, University
of Bristol, U.K.

Tchoukalov, Tzvetan, Christian Monson, and Brian
Roark.
2009. Multiple sequence alignment for
morphology induction. Working Notes, CLEF 2009
Workshop.

Wicentowski, Richard 2002. Modeling and Learn-
ing Multilingual Inﬂectional Morphology in a Min-
imally Supervised Framework. Ph.D. thesis, The
Johns Hopkins University, Baltimore, Maryland,
U.S.A.

