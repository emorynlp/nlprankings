










































A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1169–1178,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics

A Pronoun Anaphora Resolution System based on

Factorial Hidden Markov Models

Dingcheng Li

University of Minnesota,

Twin Cities, Minnesosta

lixxx345@umn.edu

Tim Miller

University of Wisconsin

Milwaukee, Wisconsin

tmill@cs.umn.edu

William Schuler

The Ohio State University

Columbus, Ohio

schuler@ling.osu.edu

Abstract

This paper presents a supervised pronoun

anaphora resolution system based on factorial

hidden Markov models (FHMMs). The ba-

sic idea is that the hidden states of FHMMs

are an explicit short-term memory with an an-

tecedent buffer containing recently described

referents. Thus an observed pronoun can find

its antecedent from the hidden buffer, or in

terms of a generative model, the entries in the

hidden buffer generate the corresponding pro-

nouns. A system implementing this model is

evaluated on the ACE corpus with promising

performance.

1 Introduction

Pronoun anaphora resolution is the task of find-

ing the correct antecedent for a given pronominal

anaphor in a document. It is a subtask of corefer-

ence resolution, which is the process of determin-

ing whether two or more linguistic expressions in

a document refer to the same entity. Adopting ter-

minology used in the Automatic Context Extraction

(ACE) program (NIST, 2003), these expressions

are called mentions. Each mention is a reference

to some entity in the domain of discourse. Men-

tions usually fall into three categories – proper men-

tions (proper names), nominal mentions (descrip-

tions), and pronominal mentions (pronouns). There

is a great deal of related work on this subject, so

the descriptions of other systems below are those

which are most related or which the current model

has drawn insight from.

Pairwise models (Yang et al., 2004; Qiu et al.,

2004) and graph-partitioning methods (McCallum

and Wellner, 2003) decompose the task into a col-

lection of pairwise or mention set coreference de-

cisions. Decisions for each pair or each group

of mentions are based on probabilities of features

extracted by discriminative learning models. The

aforementioned approaches have proven to be fruit-

ful; however, there are some notable problems. Pair-

wise modeling may fail to produce coherent parti-

tions. That is, if we link results of pairwise deci-

sions to each other, there may be conflicting corefer-

ences. Graph-partitioning methods attempt to recon-

cile pairwise scores into a final coherent clustering,

but they are combinatorially harder to work with in

discriminative approaches.

One line of research aiming at overcoming the

limitation of pairwise models is to learn a mention-

ranking model to rank preceding mentions for a

given anaphor (Denis and Baldridge, 2007) This ap-

proach results in more coherent coreference chains.

Recent years have also seen the revival of in-

terest in generative models in both machine learn-

ing and natural language processing. Haghighi

and Klein (2007), proposed an unsupervised non-

parametric Bayesian model for coreference resolu-

tion. In contrast to pairwise models, this fully gener-

ative model produces each mention from a combina-

tion of global entity properties and local attentional

state. Ng (2008) did similar work using the same un-

supervised generative model, but relaxed head gen-

eration as head-index generation, enforced agree-

ment constraints at the global level, and assigned

salience only to pronouns.

Another unsupervised generative model was re-

cently presented to tackle only pronoun anaphora

1169



resolution (Charniak and Elsner, 2009). The

expectation-maximization algorithm (EM) was ap-

plied to learn parameters automatically from the

parsed version of the North American News Cor-

pus (McClosky et al., 2008). This model generates a

pronoun’s person, number and gender features along

with the governor of the pronoun and the syntactic

relation between the pronoun and the governor. This

inference process allows the system to keep track of

multiple hypotheses through time, including multi-

ple different possible histories of the discourse.

Haghighi and Klein (2010) improved their non-

parametric model by sharing lexical statistics at the

level of abstract entity types. Consequently, their

model substantially reduces semantic compatibility

errors. They report the best results to date on the

complete end-to-end coreference task. Further, this

model functions in an online setting at mention level.

Namely, the system identifies mentions from a parse

tree and resolves resolution with a left-to-right se-

quential beam search. This is similar to Luo (2005)

where a Bell tree is used to score and store the

searching path.

In this paper, we present a supervised pro-

noun resolution system based on Factorial Hidden

Markov Models (FHMMs). This system is moti-

vated by human processing concerns, by operating

incrementally and maintaining a limited short term

memory for holding recently mentioned referents.

According to Clark and Sengul (1979), anaphoric

definite NPs are much faster retrieved if the an-

tecedent of a pronoun is in immediately previous

sentence. Therefore, a limited short term memory

should be good enough for resolving the majority of

pronouns. In order to construct an operable model,

we also measured the average distance between pro-

nouns and their antecedents as discussed in next sec-

tions and used distances as important salience fea-

tures in the model.

Second, like Morton (2000), the current sys-

tem essentially uses prior information as a dis-

course model with a time-series manner, using a

dynamic programming inference algorithm. Third,

the FHMM described here is an integrated system,

in contrast with (Haghighi and Klein, 2010). The

model generates part of speech tags as simple struc-

tural information, as well as related semantic in-

formation at each time step or word-by-word step.

While the framework described here can be ex-

tended to deeper structural information, POS tags

alone are valuable as they can be used to incorpo-

rate the binding features (described below).

Although the system described here is evaluated

for pronoun resolution, the framework we describe

can be extended to more general coreference resolu-

tion in a fairly straightforward manner. Further, as

in other HMM-based systems, the system can be ei-

ther supervised or unsupervised. But extensions to

unsupervised learning are left for future work.

The final results are compared with a few super-

vised systems as the mention-ranking model (De-

nis and Baldridge, 2007) and systems compared in

their paper, and Charniak and Elsner’s (2009) unsu-

pervised system, emPronouns. The FHMM-based

pronoun resolution system does a better job than the

global ranking technique and other approaches. This

is a promising start for this novel FHMM-based pro-

noun resolution system.

2 Model Description

This work is based on a graphical model framework

called Factorial Hidden Markov Models (FHMMs).

Unlike the more commonly known Hidden Markov

Model (HMM), in an FHMM the hidden state at

each time step is expanded to contain more than one

random variable (as shown in Figure 1). This al-

lows for the use of more complex hidden states by

taking advantage of conditional independence be-

tween substates. This conditional independence al-

lows complex hidden states to be learned with lim-

ited training data.

2.1 Factorial Hidden Markov Model

Factorial Hidden Markov Models are an extension

of HMMs (Ghahramani and Jordan, 1997). HMMs

represent sequential data as a sequence of hidden

states generating observation states (words in this

case) at corresponding time steps t. A most likely

sequence of hidden states can then be hypothesized

given any sequence of observed states, using Bayes

Law (Equation 2) and Markov independence as-

sumptions (Equation 3) to define a full probability as

the product of a Transition Model (ΘT ) prior prob-
ability and an Observation Model (ΘO) likelihood

1170



probability.

ĥ1..T
def
= argmax

h1..T

P(h1..T | o1..T ) (1)

def
= argmax

h1..T

P(h1..T ) · P(o1..T |h1..T ) (2)

def
= argmax

h1..T

T∏

t=1

PΘT (ht |ht−1) · PΘO(ot |ht)

(3)

For a simple HMM, the hidden state corresponding

to each observation state only involves one variable.

An FHMM contains more than one hidden variable

in the hidden state. These hidden substates are usu-

ally layered processes that jointly generate the ev-

idence. In the model described here, the substates

are also coupled to allow interaction between the

separate processes. As Figure 1 shows, the hidden

states include three sub-states, op, cr and pos which

are short forms of operation, coreference feature and

part-of-speech. Then, the transition model expands

the left term in (3) to (4).

PΘT (ht |ht−1)
def
= P(opt | opt−1, post−1)

·P(crt | crt−1, opt−1)

·P(post | opt, post−1)

(4)

The observation model expands from the right

term in (3) to (5).

PΘO(ot |ht)
def
= P(ot | post, crt) (5)

The observation state depends on more than one hid-

den state at each time step in an FHMM. Each hid-

den variable can be further split into smaller vari-

ables. What these terms stand for and the motiva-

tions behind the above equations will be explained

in the next section.

2.2 Modeling a Coreference Resolver with

FHMMs

FHMMs in our model, like standard HMMs, can-

not represent the hierarchical structure of a syntac-

tic phrase. In order to partially represent this in-

formation, the head word is used to represent the

whole noun phrase. After coreference is resolved,

the coreferring chain can then be expanded to the

whole phrase with NP chunker tools.

In this system, hidden states are composed of

three main variables: a referent operation (OP),

coreference features (CR) and part of speech tags

(POS) as displayed in Figure 1. The transition model

is defined as Equation 4.

opt-1=

copy
post-1=

VBZ

ot-1=loves

et-1=

per,org

gt-1=

neu,fem

crt-1

opt=

old

post=

PRP

ot=them

gt=

fem,neu

crt

ht-1 ht

et=

org,per

nt-1=

plu,sing

nt=

sing,plu

it-1=

-,2

it=

0,2

Figure 1: Factorial HMM CR Model
The starting point for the hidden state at each time

step is the OP variable, which determines which

kind of referent operations will occur at the current

word. Its domain has three possible states: none,

new and old.

The none state indicates that the present state will

not generate a mention. All previous hidden state

values (the list of previous mentions) will be passed

deterministically (with probability 1) to the current

time step without any changes. The new state signi-

fies that there is a new mention in the present time

step. In this event, a new mention will be added to

the entity set, as represented by its set of feature val-

ues and position in the coreference table. The old

state indicates that there is a mention in the present

time state and that this mention refers back to some

antecedent mention. In such a case, the list of enti-

ties in the buffer will be reordered deterministically,

moving the currently mentioned entity to the top of

the list.

Notice that opt is defined to depend on opt−1
and post−1. This is sometimes called a switching

FHMM (Duh, 2005). This dependency can be use-

ful, for example, if opt−1 is new, in which case opt
has a higher probability of being none or old. If

1171



post−1 is a verb or preposition, opt has more proba-

bility of being old or new.

One may wonder why opt generates post, and

not the other way around. This model only roughly

models the process of (new and old) entity genera-

tion, and either direction of causality might be con-

sistent with a model of human entity generation,

but this direction of causality is chosen to represent

the effect of semantics (referents) generating syn-

tax (POS tags). In addition, this is a joint model in

which POS tagging and coreference resolution are

integrated together, so the best combination of those

hidden states will be computed in either case.

2.3 Coreference Features

Coreference features for this model refer to features

that may help to identify co-referring entities.

In this paper, they mainly include index (I),

named entity type (E), number (N) and gender (G).

The index feature represents the order that a men-

tion was encountered relative to the other mentions

in the buffer. The latter three features are well

known and described elsewhere, and are not them-

selves intended as the contribution of this work. The

novel aspect of this part of the model is the fact that

the features are carried forward, updated after ev-

ery word, and essentially act as a discourse model.

The features are just a shorthand way of represent-

ing some well known essential aspects of a referent

(as pertains to anaphora resolution) in a discourse

model.

Features Values

I positive integers from 1. . .n

G male, female, neutral, unknown

N singular, plural, unknown

E person, location, organization,

GPE, vehicle,

company, facility

Table 1: Coreference features stored with each mention.

Unlike discriminative approaches, generative

models like the FHMM described here do not have

access to all observations at once. This model must

then have a mechanism for jointly considering pro-

nouns in tandem with previous mentions, as well as

the features of those mentions that might be used to

find matches between pronouns and antecedents.

Further, higher order HMMs may contain more

accurate information about observation states. This

is especially true for coreference resolution because

pronouns often refer back to mentions that are far

away from the present state. In this case, we would

need to know information about mentions which are

at least two mentions before the present one. In

this sense, a higher order HMM may seem ideal

for coreference resolution. However, higher order

HMMs will quickly become intractable as the order

increases.

In order to overcome these limitations, two strate-

gies which have been discussed in the last section

are taken: First, a switching variable called OP is

designed (as discussed in last section); second, a

memory of recently mentioned entities is maintained

to store features of mentions and pass them forward

incrementally.

OP is intended to model the decision to use the

current word to introduce a new referent (new), refer

to an antecedent (old), or neither (none). The entity

buffer is intended to model the set of ‘activated’ en-

tities in the discourse – those which could plausibly

be referred to with a pronoun. These designs allow

similar benefits as longer dependencies of higher-

order HMMs but avoid the problem of intractability.

The number of mentions maintained must be limited

in order for the model to be tractable. Fortunately,

human short term memory faces effectively similar

limitations and thus pronouns usually refer back to

mentions not very far away.

Even so, the impact of the size of the buffer on

decoding time may be a concern. Since the buffer of

our system will carry forward a few previous groups

of coreference features plus op and pos, the compu-

tational complexity will be exorbitantly high if we

keep high beam size and meanwhile if each feature

interacts with others. Luckily, we have successfully

reduced the intractability to a workable system in

both speed and space with following methods. First,

we estimate the size of buffer with a simple count

of average distances between pronouns and their an-

tecedents in the corpus. It is found that about six is

enough for covering 99.2% of all pronouns.

Secondly, the coreference features we have used

have the nice property of being independent from

one another. One might expect English non-person

entities to almost always have neutral gender, and

1172



thus be modeled as follows:

P(et, gt | et−1, gt−1) = P(gt | gt−1, et) · P(et | et−1)
(6)

However, a few considerations made us reconsider.

First, exceptions are found in the corpus. Personal

pronouns such as she or he are used to refer to coun-

try, regions, states or organizations. Second, existing

model files made by Bergsma (2005) include a large

number of non-neutral gender information for non-

person words. We employ these files for acquiring

gender information of unknown words. If we use

Equation 6, sparsity and complexity will increase.

Further, preliminary experiments have shown mod-

els using an independence assumption between gen-

der and personhood work better. Thus, we treat each

coreference feature as an independent event. Hence,

we can safely split coreference features into sepa-

rate parts. This way dramatically reduces the model

complexity. Thirdly, our HMM decoding uses the

Viterbi algorithm with A-star beam search.

The probability of the new state of the coreference

table P(crt | crt−1, opt) is defined to be the product
of probabilities of the individual feature transitions.

P(crt | crt−1, opt) = P(it | it−1, opt)·

P(et | et−1, it, opt)·

P(gt | gt−1, it, opt)·

P(nt |nt−1, it, opt)

(7)

This supposes that the features are conditionally in-

dependent of each other given the index variable, the

operator and previous instance. Each feature only

depends on the operator and the corresponding fea-

ture at the previous state, with that set of features

re-ordered as specified by the index model.

2.4 Feature Passing

Equation 7 is correct and complete, but in fact the

switching variable for operation type results in three

different cases which simplifies the calculation of

the transition probabilities for the coreference fea-

ture table.

Note the following observations about corefer-

ence features: it only needs a probabilistic model

when opt is old – in other words, only when the

model must choose between several antecedents to

re-refer to. gt, et and nt are deterministic except

when opt is new, when gender, entity type, and num-

ber information must be generated for the new entity

being introduced.

When opt is none, all coreference variables (en-

tity features) will be copied over from the previous

time step to the current time step, and the probabil-

ity of this transition is 1.0. When opt is new, it is

changed deterministically by adding the new entity

to the first position in the list and moving every other

entity down one position. If the list of entities is

full, the least recently mentioned entity will be dis-

carded. The values for the top of the feature lists

gt, et, and nt will then be generated from feature-

specific probability distributions estimated from the

training data. When opt is old, it will probabilisti-

cally select a value 1 . . . n, for an entity list contain-
ing n items. The selected value will deterministi-

cally order the gt, nt and et lists. This distribution

is also estimated from training data, and takes into

account recency of mention. The shape of this dis-

tribution varies slightly depending on list size and

noise in the training data, but in general the probabil-

ity of a mention being selected is directly correlated

to how recently it was mentioned.

With this understanding, coreference table tran-

sition probabilities can be written in terms of only

their non-deterministic substate distributions:

P(crt | crt−1, old) = Pold(it | it−1)·
Preorder(et | et−1, it)·
Preorder(gt | gt−1, it)·
Preorder(nt |nt−1, it)

(8)

where the old model probabilistically selects the an-

tecedent and moves it to the top of the list as de-

scribed above, thus deciding how the reordering will

take place. The reorder model actually implements

the list reordering for each independent feature by

moving the feature value corresponding to the se-

lected entity in the index model to the top of that

feature’s list. The overall effect is simply the prob-

abilistic reordering of entities in a list, where each

entity is defined as a label and a set of features.

P(crt | crt−1, new) = Pnew(it | it−1)·
Pnew(gt | gt−1)·
Pnew(nt |nt−1)·
Pnew(et | et−1)

(9)

where the new model probabilistically generates a

1173



feature value based on the training data and puts it

at the top of the list, moves every other entity down

one position in the list, and removes the final item if

the list is already full. Each entity in i takes a value

from 1 to n for a list of size n. Each g can be one of

four values – male, female, neuter and unknown; n

one of three values – plural, singular and unknown

and e around eight values.

Note that post is used in both hidden states and

observation states. While it is not considered a

coreference feature as such, it can still play an im-

portant role in the resolving process. Basically, the

system tags parts of speech incrementally while si-

multaneously resolving pronoun anaphora. Mean-

while, post−1 and opt−1 will jointly generate opt.

This point has been discussed in Section 2.2.

Importantly, the pos model can help to imple-

ment binding principles (Chomsky, 1981). It is

applied when opt is old. In training, pronouns

are sub-categorised into personal pronouns, reflex-

ive and other-pronoun. We then define a vari-

able loct whose value is how far back in the list

of antecedents the current hypothesis must have

gone to arrive at the current value of it. If we

have the syntax annotations or parsed trees, then,

the part of speech model can be defined when

opt is old as Pbinding(post | loct, sloct). For ex-
ample, if post ∈ ref lexive, P(post | loct, sloct)
where loct has smaller values (implying closer men-

tions to post) and sloct = subject should have
higher values since reflexive pronouns always re-

fer back to subjects within its governing domains.

This was what (Haghighi and Klein, 2009) did and

we did this in training with the REUTERS cor-

pus (Hasler et al., 2006) in which syntactic roles

are annotated. We finally switched to the ACE

corpus for the purpose of comparison with other

work. In the ACE corpus, no syntactic roles are

annotated. We did use the Stanford parser to ex-

tract syntactic roles from the ACE corpus. But

the result is largely affected by the parsing accu-

racy. Again, for a fair comparison, we extract simi-

lar features to Denis and Baldridge (2007), which is

the model we mainly compare with. They approx-

imate syntactic contexts with POS tags surround-

ing the pronoun. Inspired by this idea, we success-

fully represent binding features with POS tags be-

fore anaphors. Instead of using P(post | loct, sloct),

we train P(post | loct, posloct) which can play
the role of binding. For example, suppose the

buffer size is 6 and loct = 5, posloct = noun.
Then, P(post = ref lexive | loct, posloct) is usu-
ally higher than P(post = pronoun | loct, posloct),
since the reflexive has a higher probability of refer-

ring back to the noun located in position 5 than the

pronoun.

In future work expanding to coreference resolu-

tion between any noun phrases we intend to inte-

grate syntax into this framework as a joint model of

coreference resolution and parsing.

3 Observation Model

The observation model that generates an observed

state is defined as Equation 5. To expand that equa-

tion in detail, the observation state, the word, de-

pends on its part of speech and its coreference fea-

tures as well. Since FHMMs are generative, we can

say part of speech and coreference features generate

the word.

In actual implementation, the observed model will

be very sparse, since crt will be split into more vari-

ables according to how many coreference features it

is composed of. In order to avoid the sparsity, we

transform the equation with Bayes’ law as follows.

PΘO(ot |ht) =
P (ot) · P(ht | ot)∑
o′ P (o

′)P(ht | o′)
(10)

=
P (ot) · P(post, crt | ot)∑
o′ P (o

′)P(post, crt | o′)
(11)

We define pos and cr to be independent of each

other, so we can further split the above equation as:

PΘO(ot |ht)
def
=

P (ot) · P(post | ot) · P(crt | ot)∑
o′ P (o

′) · P(post | o′) · P(crt | o′)

(12)

where P(crt | ot) = P(gt | ot)P(nt | ot)P(et | ot) and
P(crt | o

′) = P(gt | o
′)P(nt | o

′)P(et | o
′).

This change transforms the FHMM to a hybrid

FHMM since the observation model no longer gen-

erates the data. Instead, the observation model gen-

erates hidden states, which is more a combination

of discriminative and generative approaches. This

way facilitates building likelihood model files of fea-

tures for given mentions from the training data. The

1174



hidden state transition model represents prior proba-

bilities of coreference features associated with each

while this observation model factors in the probabil-

ity given a pronoun.

3.1 Unknown Words Processing

If an observed word was not seen in training, the

distribution of its part of speech, gender, number and

entity type will be unknown. In this case, a special

unknown words model is used.

The part of speech of unknown words

P(post |wt = unkword) is estimated using a
decision tree model. This decision tree is built

by splitting letters in words from the end of the

word backward to its beginning. A POS tag is

assigned to the word after comparisons between

the morphological features of words trained from

the corpus and the strings concatenated from the

tree leaves are made. This method is about as

accurate as the approach described by Klein and

Manning (2003).

Next, a similar model is set up for estimating

P(nt |wt = unkword). Most English words have
regular plural forms, and even irregular words have

their patterns. Therefore, the morphological features

of English words can often be used to determine

whether a word is singular or plural.

Gender is irregular in English, so model-based

predictions are problematic. Instead, we follow

Bergsma and Lin (2005) to get the distribution of

gender from their gender/number data and then pre-

dict the gender for unknown words.

4 Evaluation and Discussion

4.1 Experimental Setup

In this research, we used the ACE corpus (Phase 2) 1

for evaluation. The development of this corpus in-

volved two stages. The first stage is called EDT (en-

tity detection and tracking) while the second stage

is called RDC (relation detection and characteriza-

tion). All markables have named entity types such

as FACILITY, GPE (geopolitical entity), PERSON,

LOCATION, ORGANIZATION, PERSON, VEHI-

CLE and WEAPONS, which were annotated in the

first stage. In the second stage, relations between

1See http://projects.ldc.upenn.edu/ace/

annotation/previous/ for details on the corpus.

named entities were annotated. This corpus include

three parts, composed of different genres: newspa-

per texts (NPAPER), newswire texts (NWIRE) and

broadcasted news (BNEWS). Each of these is split

into a train part and a devtest part. For the train

part, there are 76, 130 and 217 articles in NPA-

PER, NWIRE and BNEWS respectively while for

the test part, there are 17, 29 and 51 articles respec-

tively. Though the number of articles are quite dif-

ferent for three genres, the total number of words are

almost the same. Namely, the length of NPAPER

is much longer than BNEWS (about 1200 words,

800 word and 500 words respectively for three gen-

res). The longer articles involve longer coreference

chains. Following the common practice, we used

the devtest material only for testing. Progress during

the development phase was estimated only by using

cross-validation on the training set for the BNEWS

section. In order to make comparisons with publica-

tions which used the same corpus, we make efforts

to set up identical conditions for our experiments.

The main point of comparison is Denis and

Baldridge (2007), which was similar in that it de-

scribed a new type of coreference resolver using

simple features.

Therefore, similar to their practice, we use all

forms of personal and possessive pronouns that were

annotated as ACE ”markables”. Namely, pronouns

associated with named entity types could be used in

this system. In experiments, we also used true ACE

mentions as they did. This means that pleonastics

and references to eventualities or to non-ACE enti-

ties are not included in our experiments either. In

all, 7263 referential pronouns in training data set

and 1866 in testing data set are found in all three

genres. They have results of three different systems:

SCC (single candidate classifier), TCC (twin candi-

date classifier) and RK (ranking). Besides the three

and our own system, we also report results of em-

Pronouns, which is an unsupervised system based

on a recently published paper (Charniak and Elsner,

2009). We select this unsupervised system for two

reasons. Firstly, emPronouns is a publicly available

system with high accuracy in pronoun resolution.

Secondly, it is necessary for us to demonstrate our

system has strong empirical superiority over unsu-

pervised ones. In testing, we also used the OPNLP

Named Entity Recognizer to tag the test corpus.

1175



During training, besides coreference annotation

itself, the part of speech, dependencies between

words and named entities, gender, number and index

are extracted using relative frequency estimation to

train models for the coreference resolution system.

Inputs for testing are the plain text and the trained

model files. The entity buffer used in these exper-

iments kept track of only the six most recent men-

tions. The result of this process is an annotation

of the headword of every noun phrase denoting it

as a mention. In addition, this system does not

do anaphoricity detection, so the antecedent oper-

ation for non-anaphora pronoun it is set to be none.

Finally, the system does not yet model cataphora,

about 10 cataphoric pronouns in the testing data

which are all counted as wrong.

4.2 Results

The performance was evaluated using the ratio of

the number of correctly resolved anaphors over the

number of all anaphors as a success metrics. All the

standards are consistent with those defined in Char-

niak and Elsner (2009).

During development, several preliminary experi-

ments explored the effects of starting from a simple

baseline and adding more features. The BNEWS

corpus was employed in these development exper-

iments. The baseline only includes part of speech

tags, the index feature and and syntactic roles. Syn-

tactic roles are extracted from the parsing results

with Stanford parser. The success rate of this base-

line configuration is 0.48. This low accuracy is par-

tially due to the errors of automatic parsing. With

gender and number features added, the performance

jumped to 0.65. This shows that number and gen-

der agreements play an important role in pronoun

anaphora resolution. For a more standard compari-

son to other work, subsequent tests were performed

on the gold standard ACE corpus (using the model

as described with named entity features instead of

syntactic role features). As shown in Denis and

Baldridge (2007), they employ all features we use

except syntactic roles. In these experiments, the sys-

tem got better results as shown in Table 2.

The result of the first one is obtained by running

the publicly available system emPronouns2. It is a

2the available system in fact only includes the testing part.

Thus, it may be unfair to compare emPronouns this way with

System BNEWS NPAPER NWIRE

emPronouns 58.5 64.5 60.6

SCC 62.2 70.7 68.3

TCC 68.6 74.7 71.1

RK 72.9 76.4 72.4

FHMM 74.9 79.4 74.5

Table 2: Accuracy scores for emPronouns, the single-

candidate classifier (SCC), the twin-candidate classifier

(TCC), the ranker and FHMM

high-accuracy unsupervised system which reported

the best result in Charniak and Elsner (2009).

The results of the other three systems are those

reported by Denis and Baldridge (2007). As Table 2

shows, the FHMM system gets the highest average

results.

The emPronouns system got the lowest results

partially due to the reason that we only directly

run the existing system with its existing model files

without retraining. But the gap between its results

and results of our system is large. Thus, we may

still say that our system probably can do a better job

even if we train new models files for emPronouns

with ACE corpus.

With almost exactly identical settings, why does

our FHMM system get the highest average results?

The convincing reason is that FHMM is strongly in-

fluenced by the sequential dependencies. The rank-

ing approach ranks a set of mentions using a set of

features, and it also maintains the discourse model,

but it is not processing sequentially. The FHMM

system always maintain a set of mentions as well

as a first-order dependencies between part of speech

and operator. Therefore, context can be more fully

taken into consideration. This is the main reason that

the FHMM approach achieved better results than the

ranking approach.

From the result, one point we may notice is that

NPAPER usually obtains higher results than both

BNEWS and NWIRE for all systems while BNEWS

lower than other two genres. In last section, we

mention that articles in NPAPER are longer than

other genres and also have denser coreference chains

while articles in BENEWS are shorter and have

sparer chains. Then, it is not hard to understand

why results of NPAPER are better while those of

other systems.

1176



BNEWS are poorer.

In Denis and Baldridge (2007), they also reported

new results with a window of 10 sentences for RK

model. All three genres obtained higher results than

those when with shorter ones. They are 73.0, 77.6

and 75.0 for BNEWS, NPAPER and NWIRE respec-

tively. We can see that except the one for NWIRE,

the results are still poorer than our system. For

NWIRE, the RK model got 0.5 higher. The average

of the RK is 75.2 while that of the FHMM system is

76.3, which is still the best.

Since the emPronoun system can output sample-

level results, it is possible to do a paired Student’s

t-test. That test shows that the improvement of our

system on all three genres is statistically significant

(p < 0.001). Unfortunately, the other systems only
report overall results so the same comparison was

not so straightforward.

4.3 Error Analysis

After running the system on these documents, we

checked which pronouns fail to catch their an-

tecedents. There are a few general reasons for er-

rors.

First, pronouns which have antecedents very far

away cannot be caught. Long-distance anaphora res-

olution may pose a problem since the buffer size

cannot be too long considering the complexity of

tracking a large number of mentions through time.

During development, estimation of an acceptable

size was attempted using the training data. It was

found that a mention distance of fourteen would ac-

count for every case found in this corpus, though

most cases fall well short of that distance. Future

work will explore optimizations that will allow for

larger or variable buffer sizes so that longer distance

anaphora can be detected.

A second source of error is simple misjudgments

when more than one candidate is waiting for selec-

tion. A simple case is that the system fails to distin-

guish plural personal nouns and non-personal nouns

if both candidates are plural. This is not a problem

for singular pronouns since gender features can tell

whether pronouns are personal or not. Plural nouns

in English do not have such distinctions, however.

Consequently, demands and Israelis have the same

probability of being selected as the antecedents for

they, all else being equal. If demands is closer to

they, demands will be selected as the antecedent.

This may lead to the wrong choice if they in fact

refers to Israelis. This may require better measures

of referent salience than the “least recently used”

heuristic currently implemented.

Third, these results also show difficulty resolv-

ing coordinate noun phrases due to the simplistic

representation of noun phrases in the input. Con-

sider this sentence: President Barack Obama and

his wife Michelle Obama visited China last week.

They had a meeting with President Hu in Beijing.

In this example, the pronoun they corefers with the

noun phrase President Barack Obama and his wife

Michelle Obama. The present model cannot repre-

sent both the larger noun phrase and its contained

noun phrases. Since the noun phrase is a coordinate

one that includes both noun phrases, the model can-

not find a head word to represent it.

Finally, while the coreference feature annotations

of the ACE are valuable for learning feature mod-

els, the model training may still give some mislead-

ing results. This is brought about by missing fea-

tures in the training corpus and by the data sparsity.

We solved the problem with add-one smoothing and

deleted interpolation in training models besides the

transformation in the generation order of the obser-

vation model.

5 Conclusion and Future Work

This paper has presented a pronoun anaphora resolu-

tion system based on FHMMs. This generative sys-

tem incrementally resolves pronoun anaphora with

an entity buffer carrying forward mention features.

The system performs well and outperforms other

available models. This shows that FHMMs and

other time-series models may be a valuable model

to resolve anaphora.

Acknowledgments

We would like to thank the authors and maintainers
of ranker models and emPronouns. We also would
like to thank the three anonymous reviewers. The
final version is revised based on their valuable com-
ments. Thanks are extended to Shane Bergsma, who
provided us the gender and number data distribution.
In addition, Professor Jeanette Gundel and our lab-
mate Stephen Wu also gave us support in paper edit-
ing and in theoretical discussion.

1177



References

S Bergsma. 2005. Automatic acquisition of gender

information for anaphora resolution. page 342353.

Springer.

Eugene Charniak and Micha Elsner. 2009. Em works

for pronoun anaphora resolution. In Proceedings of

the Conference of the European Chapter of the As-

sociation for Computational Linguistics (EACL-09),

Athens, Greece.

Noam Chomsky. 1981. Lectures on government and

binding. Foris, Dordercht.

H.H. Clark and CJ Sengul. 1979. In search of refer-

ents for nouns and pronouns. Memory & Cognition,

7(1):35–41.

P. Denis and J. Baldridge. 2007. A ranking approach to

pronoun resolution. In Proc. IJCAI.

Kevin Duh. 2005. Jointly labeling multiple sequences:

a factorial HMM approach. In ACL ’05: Proceedings

of the ACL Student Research Workshop, pages 19–24,

Ann Arbor, Michigan.

Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-

rial hidden markov models. Machine Learning, 29:1–

31.

A. Haghighi and D. Klein. 2007. Unsupervised coref-

erence resolution in a nonparametric bayesian model.

In Proceedings of the 45th annual meeting on Associ-

ation for Computational Linguistics, page 848.

A. Haghighi and D. Klein. 2009. Simple coreference

resolution with rich syntactic and semantic features.

In Proceedings of the 2009 Conference on Empirical

Methods in Natural Language Processing: Volume 3-

Volume 3, pages 1152–1161. Association for Compu-

tational Linguistics.

A. Haghighi and D. Klein. 2010. Coreference resolu-

tion in a modular, entity-centered model. In Human

Language Technologies: The 2010 Annual Conference

of the North American Chapter of the Association for

Computational Linguistics, pages 385–393. Associa-

tion for Computational Linguistics.

L. Hasler, C. Orasan, and K. Naumann. 2006. NPs

for events: Experiments in coreference annotation. In

Proceedings of the 5th edition of the International

Conference on Language Resources and Evaluation

(LREC2006), pages 1167–1172. Citeseer.

Dan Klein and Christopher D. Manning. 2003. Accu-

rate unlexicalized parsing. In Proceedings of the 41st

Annual Meeting of the Association for Computational

Linguistics, pages 423–430, Sapporo, Japan.

X Luo. 2005. On coreference resolution performance

metrics. pages 25–32. Association for Computational

Linguistics Morristown, NJ, USA.

A. McCallum and B. Wellner. 2003. Toward condi-

tional models of identity uncertainty with application

to proper noun coreference. In IJCAI Workshop on In-

formation Integration on the Web. Citeseer.

David McClosky, Eugene Charniak, and Mark Johnson.

2008. BLLIP North American News Text, Complete.

Linguistic Data Consortium. LDC2008T13.

T.S. Morton. 2000. Coreference for NLP applications.

In Proceedings of the 38th Annual Meeting on Associ-

ation for Computational Linguistics, pages 173–180.

Association for Computational Linguistics.

V. Ng. 2008. Unsupervised models for coreference reso-

lution. In Proceedings of the Conference on Empirical

Methods in Natural Language Processing, pages 640–

649. Association for Computational Linguistics.

US NIST. 2003. The ACE 2003 Evaluation Plan. US Na-

tional Institute for Standards and Technology (NIST),

Gaithersburg, MD.[online, pages 2003–08.

L. Qiu, M.Y. Kan, and T.S. Chua. 2004. A public ref-

erence implementation of the rap anaphora resolution

algorithm. Arxiv preprint cs/0406031.

X. Yang, J. Su, G. Zhou, and C.L. Tan. 2004. Im-

proving pronoun resolution by incorporating corefer-

ential information of candidates. In Proceedings of

the 42nd Annual Meeting on Association for Compu-

tational Linguistics, page 127. Association for Com-

putational Linguistics.

1178


