



















































Werdy: Recognition and Disambiguation of Verbs and Verb Phrases with Syntactic and Semantic Pruning


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 374–385,
October 25-29, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Werdy: Recognition and Disambiguation of Verbs and Verb Phrases
with Syntactic and Semantic Pruning

Luciano Del Corro Rainer Gemulla
Max-Planck-Institut für Informatik

Saarbrücken, Germany
{ delcorro, rgemulla, weikum }@mpi-inf.mpg.de

Gerhard Weikum

Abstract

Word-sense recognition and disambigua-
tion (WERD) is the task of identifying
word phrases and their senses in natural
language text. Though it is well under-
stood how to disambiguate noun phrases,
this task is much less studied for verbs
and verbal phrases. We present Werdy,
a framework for WERD with particular
focus on verbs and verbal phrases. Our
framework first identifies multi-word ex-
pressions based on the syntactic structure
of the sentence; this allows us to recog-
nize both contiguous and non-contiguous
phrases. We then generate a list of can-
didate senses for each word or phrase, us-
ing novel syntactic and semantic pruning
techniques. We also construct and lever-
age a new resource of pairs of senses for
verbs and their object arguments. Finally,
we feed the so-obtained candidate senses
into standard word-sense disambiguation
(WSD) methods, and boost their precision
and recall. Our experiments indicate that
Werdy significantly increases the perfor-
mance of existing WSD methods.

1 Introduction

Understanding the semantics of words and multi-
word expressions in natural language text is an
important task for automatic knowledge acquisi-
tion. It serves as a fundamental building block
in a wide area of applications, including semantic
parsing, question answering, paraphrasing, knowl-
edge base construction, etc. In this paper, we
study the task of word-sense recognition and dis-
ambiguation (WERD) with a focus on verbs and

verbal phrases. Verbs are the central element in a
sentence, and the key to understand the relations
between sets of entities expressed in a sentence.

We propose Werdy, a method to (i) automati-
cally recognize in natural language text both sin-
gle words and multi-word phrases that match en-
tries in a lexical knowledge base (KB) like Word-
Net (Fellbaum, 1998), and (ii) disambiguate these
words or phrases by identifying their senses in the
KB. WordNet is a comprehensive lexical resource
for word-sense disambiguation (WSD), covering
nouns, verbs, adjectives, adverbs, and many multi-
word expressions. In the following, the notion of
an entry refers to a word or phrase in the KB,
whereas a sense denotes the lexical synset of the
entry’s meaning in the given sentence.

A key challenge for recognizing KB entries in
natural language text is that entries often consist of
multiple words. In WordNet-3.0 more than 40%
of the entries are multi-word. Such entries are
challenging to recognize accurately for two main
reasons: First, the components of multi-word en-
tries in the KB (such as fiscal year) often consist
of components that are themselves KB entries (fis-
cal and year). Second, multi-word entries (such
as take a breath) may not appear consecutively in
a sentence (“He takes a deep breath.”). Werdy
addresses the latter problem by (conceptually)
matching the syntactic structure of the KB entries
to the syntactic structure of the input sentence.
To address the former problem, Werdy identifies
all possible entries in a sentence and passes them
to the disambiguation phase (take, breath, take a
breath, . . . ); the disambiguation phase provides
more information about which multi-word entries
to keep. Thus, our method solves the recognition
and the disambiguation tasks jointly.

Once KB entries have been identified, Werdy

374



disambiguates each entry against its possible
senses. State-of-the-art methods for WSD (Nav-
igli, 2009) work fairly well for nouns and noun
phrases. However, the disambiguation of verbs
and verbal phrases has received much less atten-
tion in the literature.

WSD methods can be roughly categorized into
(i) methods that are based on supervised training
over sense-annotated corpora (e.g., Zhong and Ng
(2010)), and (ii) methods that harness KB’s to as-
sess the semantic relatedness among word senses
for mapping entries to senses (e.g., Ponzetto and
Navigli (2010)). For these methods, mapping
verbs to senses is a difficult task since verbs tend
to have more senses than nouns. In WordNet, in-
cluding monosemous words, there are on average
1.24 senses per noun and 2.17 per verb.

To disambiguate verbs and verbal phrases,
Werdy proceeds in multiple steps. First, Werdy
obtains the set of candidate senses for each recog-
nized entry from the KB. Second, it reduces the
set of candidate entries using novel syntactic and
semantic pruning techniques. The key insight be-
hind our syntactic pruning is that each verb sense
tends to occur in a only limited number of syn-
tactic patterns. For example, the sentence “Al-
bert Einstein remained in Princeton” has a sub-
ject (“Albert Einstein”), a verb (“remained”) and
an adverbial (“in Princeton”), it follows an SVA
clause pattern. We can thus safely prune verb
senses that do not match the syntactic structure of
the sentence. Moreover, each verb sense is com-
patible with only a limited number of semantic
argument types (such as location, river, person,
musician, etc); this phenomena is called selec-
tional preference or selectional restriction. Senses
that are compatible only with argument types not
present in the sentence can be pruned. Our prun-
ing steps are based on the idea that a verb selects
the categories of its arguments both syntactically
(c-selection) and semantically (s-selection). In the
final step, Werdy employs a state-of-the-art gen-
eral WSD method to select the most suitable sense
from the remaining candidates. Since incorrect
senses have already been greatly pruned, this step
significantly gains accuracy and efficiency over
standard WSD.

Our semantic pruning technique builds on a
newly created resource of pairs of senses for verbs
and their object arguments. For example, the
WordNet verb sense 〈play-1〉 (i.e., the 1st sense of

the verb entry “play”) selects as direct object the
noun sense 〈sport-1〉. We refer to this novel re-
source as the VO Sense Repository, or VOS repos-
itory for short.1 It is constructed from the Word-
Net gloss-tags corpus, the SemCor dataset, and a
small set of manually created VO sense pairs.

We evaluated Werdy on the SemEval-2007
coarse-grained WSD task (Navigli et al., 2007),
both with and without automatic recognition of en-
tries. We found that our techniques boost state-of-
the-art WSD methods and obtain high-quality re-
sults. Werdy significantly increases the precision
and recall of the best performing baselines.

The rest of the paper is organized as follows.
Section 2 gives an overview of Werdy compo-
nents. Section 3 presents the entry recognition,
and Sections 4 and 5 discuss our novel syntac-
tic and semantic pruning techniques. Section 6
presents the Semantic VO Repository and how we
constructed it. Section 7 gives the results of our
evaluation. Section 8 discusses related work.

2 Overview of Werdy

Werdy consists of four steps: (i) entry recognition,
(ii) syntactic pruning, (iii) semantic pruning, and
(iv) word-sense disambiguation. The novel con-
tribution of this paper is in the first three steps,
and in the construction of the VO sense repository.
Each of these steps operates on the clause level,
i.e., we first determine the set of clauses present
in the input sentence and then process clauses sep-
arately. A clause is a part of a sentence that ex-
presses some statement or coherent piece of infor-
mation. Clauses are thus suitable minimal units
for automatic text understanding tasks (Del Corro
and Gemulla, 2013); see Sec.3 for details.

In the entry-recognition step (Sec. 3), Werdy
obtains for the input sentence a set of potential
KB entries along with their part-of-speech tags.
The candidate senses of each entry are obtained
from WordNet. For instance, in the sentence “He
takes a deep and long breath”, the set of potential
entries includes take (verb, 44 candidate senses),
take a breath (verb, 1 candidate sense), and breath
(noun, 5 candidate senses). Note that in contrast to
Werdy, most existing word-sense disambiguation
methods assume that entries have already been
(correctly) identified.

1The VOS repository, Werdy’s source code, and results of
our experimental study are available at http://people.
mpi-inf.mpg.de/˜corrogg/.

375



In the syntactic-pruning step (Sec. 4), we elim-
inate candidate senses that do not agree with
the syntactic structure of the clause. It is well-
established that the syntactic realization of a
clause is intrinsically related with the sense of
its verb (Quirk et al., 1985; Levin, 1993; Hanks,
1996; Baker et al., 1998; Palmer et al., 2005).
Quirk et al. (1985) identified seven possible clause
types in the English language (such as “subject
verb adverbial”, SVA). We make use of techniques
inspired by Del Corro and Gemulla (2013) to iden-
tify the clause type of each clause in the sen-
tence. We then match the clause type with the set
of WordNet frames (e.g., “somebody verb some-
thing”) that WordNet provides for each verb sense,
and prune verb senses for which there is no match.

In the semantic-pruning step (Sec. 5), we fur-
ther prune the set of candidate senses by taking the
semantic types of direct objects into account. Sim-
ilarly to the syntactic relation mentioned above,
a verb sense also imposes a (selectional) restric-
tion on the semantic type of its arguments (Quirk
et al., 1985; Levin, 1993; Hanks, 1996; Baker et
al., 1998; Palmer et al., 2005). For instance, the
verb play with sense participate in games or sports
requires an object argument of type 〈game-1〉2,
〈game-3〉, or 〈sport-1〉. Senses that do not match
the arguments found in the clause are pruned.
This step is based on the newly constructed VOS
Repository (Sec. 6). Note that when there is no di-
rect object, only the syntactic pruning step applies.

3 Entry Recognition

The key challenge in recognizing lexical KB en-
tries in text is that entries are not restricted to sin-
gle words. In addition to named entities (such as
people, places, etc.), KB’s contain multi-word ex-
pressions. For example, WordNet-3.0 contains en-
tries such as take place (verb), let down (verb),
take into account (verb), be born (verb), high
school (noun), fiscal year (noun), and Prime Min-
ister (noun). Note that each individual word in a
multi-word entry is usually also an entry by itself,
and can even be part of several multi-word en-
tries. To ensure correct disambiguation, all poten-
tial multi-word entries need to be recognized (Fin-
layson and Kulkarni, 2011), even when they do not
appear as consecutive words in a sentence.

Werdy addresses these challenges by explor-
ing the syntactic structure of both the input sen-

2We use the notation 〈WordNet entry-sense number〉.

He takes my hand and a deep breath .

nsubj poss

dobj

cc

det

amod

conj
root

Figure 1: An example dependency parse

tence and the lexical KB entries. The structure
of the sentence is captured in a dependency parse
(DP). Given a word in a sentence, Werdy con-
ceptually generates all subtrees of the DP starting
at that word, and matches them against the KB.
This process can be performed efficiently as Word-
Net entries are short and can be indexed appro-
priately. To match the individual words of a sen-
tence against the words of a KB entry, we follow
the standard approach and perform lemmatization
and stemming (Finlayson, 2014). To further han-
dle personal pronouns and possessives, we follow
Arranz et al. (2005) and normalize personal pro-
nouns (I, you, my, your, . . . ) to one’s, and reflex-
ive pronouns (myself, yourself, . . . ) to oneself.

Consider the example sentence “He takes my
hand and a deep breath”. We first identify the
clauses and their DP’s (Fig. 1) using the method
of Del Corro and Gemulla (2013), which also
processes coordinating conjunctions. We obtain
clauses “He takes my hand” and “He takes a deep
breath”, which we process separately. To obtain
possible entries for the first clause, we start with
its head word (take) and incrementally consider
its descendants (take hand, take one’s hand, . . . ).
The exploration is terminated as early as possible;
for example, we do not consider take one’s hand
because there is no WordNet entry that contains
both take and hand. For the second clause, we
start with take (found in WordNet), then expand
to take breath (not found but can occur together),
then take a breath (found), then take a deep breath
(not found, cannot occur together) and so on.

Note that the word “take” in the sentence re-
fer to two different entries and senses: “take” for
the first clause and “take a breath” for the sec-
ond clause. In this stage no decisions are made
about selecting entries and disambiguating them;
these decisions are made in the final WSD stage
of Werdy.

We tested Werdy’s entry-recognizer on the
SemEval-2007 corpus. We detected the correct en-

376



Pattern Clause type Example WN frame example [frame number]

SVi SV AE died. Somebody verb [2]
SVeA SVA AE remained in Princeton. Somebody verb PP [22]
SVcC SVC AE is smart. Somebody verb adjective [6]
SVmtO SVO AE has won the Nobel Prize. Somebody verb something [8]
SVdtOiO SVOO RSAS gave AE the Nobel Prize. Somebody verb somebody something [14]
SVctOA SVOA The doorman showed AE to his office. Somebody verb somebody PP [20]
SVctOC SVOC AE declared the meeting open. Something verb something adjective/noun [5]
S: Subject, V: Verb, C: Complement, O: Direct object, Oi: Indirect object, A: Adverbial, Vi: Intransitive verb, Vc: Copular verb,

Vc: Extended-copular verb, Vmt: Monotransitive verb, Vdt: Ditransitive verb, Vct: Complex-transitive verb

Table 1: Clause types and examples of matching WordNet frames

tries for all but two verbs (out of more than 400).
The two missed entries (take up and get rolling)
resulted from incorrect dependency parses.

4 Syntactic Pruning

Once the KB entries have been recognized, Werdy
prunes the set of possible senses of each verb entry
by considering the syntactic structure of the clause
in which the entry occurs. This pruning is based
on the observation that each verb sense may occur
only in a limited number of “clause types”, each
having specific semantic functions (Quirk et al.,
1985). When the clause type of the sentence is
incompatible with a candidate sense of an entry,
this sense is eliminated.

Werdy first detects in the input sentence the set
of clauses and their constituents. A clause con-
sists of one subject (S), one verb (V), and option-
ally an indirect object (O), a direct object (O), a
complement (C) and one or more adverbials (A).
Not all combinations of clause constituents ap-
pear in the English language. When we classify
clauses according to the grammatical function of
their constituents, we obtain only seven different
clause types (Quirk et al., 1985); see Table 1. For
example, the sentence “He takes my hand” is of
type SVO; here “He” is the subject, “takes” the
verb, and “my hand” the object. The clause type
can (in principle) be determined by observing the
verb type and its complementation (Del Corro and
Gemulla, 2013).

For instance, consider the SVA clause “The stu-
dent remained in Princeton”. The verb remain has
four senses in WN: (1) stay the same; remain in
a certain state (e.g., “The dress remained wet”),
(2) continue in a place, position, or situation (“He
remained dean for another year”), (3) be left; of
persons, questions, problems (“There remains the

question of who pulled the trigger”) or (4) stay be-
hind (“The hostility remained long after they made
up”). The first sense of remain requires an SVC
pattern; the other cases require either SV or SVA.
Our example clause is of type SVA so that we can
safely prune the first sense.

WordNet provides an important resource for ob-
taining the set of clause types that are compatible
with each sense of a verb. In particular, each verb
sense in WordNet is annotated with a set of frames
(e.g., “somebody verb something”) in which they
may occur, capturing both syntactic and semantic
constraints. There are 35 different frames in to-
tal.3 We manually assigned a set of clause types to
each frame (e.g., SVO to frame “somebody verb
something”). Table 1 shows an example frame for
each of the seven clause types. On average, each
WordNet-3.0 verb sense is associated with 1.57
frames; the maximum number of frames per sense
is 9. The distribution of frames is highly skewed:
More than 61% of the 21,649 frame annotations
belong to one of four simple SVO frames (num-
bers 8, 9, 10 and 11), and 22 out of the 35 frames
have less than 100 instances. This skew makes
the syntactic pruning step effective for non-SVO
clauses, but less effective for SVO clauses.

Werdy directly determines a set of possible
frame types for each clause of the input sentence.
Our approach is based on the clause-type detection
method of Del Corro and Gemulla (2013), but we
also consider additional information that is cap-
tured in frames but not in clause types. For ex-
ample, we distinguish different realizations of ob-
jects (such as clausal objects from non-clausal ob-
jects), which are not captured in the clause type.
Given the DP of a clause, Werdy identifies the

3See http://wordnet.princeton.edu/
wordnet/man/wninput.5WN.html.

377



Clause Object?
Q1

Complement? Adverbial?
Q2 Q3

Frames
4,6,7

Frames
1-3

Frames
1,2,12,13,22,27

No

Yes

No

YesNo

Dir. and in-
direct object?

Complement? that-clause?
infinitive/

to-infinitive?

Adverbial?

Q7 Q8 Q9 Q10

Q11

Frames
14,15

Frame
5

Frames
26,34

Frames
24,28,29,32,35

Frames
1,2,8-11,33

Frames
1,2,8-11,15-21, 30, 31,33

Yes No

Yes

No

Yes

No

Yes

Yes No

No
Yes

Figure 2: Flow chart for frame detection

set of WN frames that can potentially match the
clause as outlined in the flowchart of Fig. 2. Werdy
walks through the flowchart; for each question,
we check for the presence or absence of a specific
constituent of a clause (e.g., a direct object for Q1)
and proceed appropriately until we obtain a set of
possible frames. This set is further reduced by
considering additional information in the frames
(not shown; e.g., that the verb must end on “-ing”).
For our example clause “The student remained
in Princeton”, we first identify possible frames
{ 1, 2, 12, 13, 22, 27 } using the flowchart (Q1 no,
Q2 no, Q3 yes); using the additional information
in the frames, Werdy then further prunes this set
to { 1, 2, 22 }. The corresponding set of remaining
candidate sense for remain is as given above, i.e.,
{ 〈remain-2〉, 〈remain-3〉, 〈remain-4〉 }.

Our mapping of clause types to WordNet frames
is judiciously designed for the way WordNet is or-
ganized. For instance, frames containing adver-
bials generally do not specify whether or not the
adverbial is obligatory; here we are conservative
in that we do not prune such frames if the input
clause does not contain an adverbial. As another
example, some frames overlap or subsume each
other; e.g, frame “somebody verb something” (8)
subsumes “somebody verb that clause” (26). In
some word senses annotated with the more general
frame, the more specific one can also apply (e.g.,
〈point out-1〉 is annotated with 8 but not 26; 26
can apply), in others it does not (e.g., 〈play-1〉 is
also annotated with 8 but not 26; but here 26 can-
not apply). To ensure the effectiveness of syntactic
pruning, we only consider the frames that are di-
rectly specified in WordNet. This procedure often
produces the desired results; in a few cases, how-
ever, we do prune the correct sense (e.g., frame 26

for clause “He points out that . . . ”).

5 Semantic Pruning

A verb sense imposes a restriction on the semantic
type of the arguments it may take and vice versa
(Quirk et al., 1985; Levin, 1993; Hanks, 1996;
Baker et al., 1998; Palmer et al., 2005; Kipper et
al., 2008). This allows us to further prune the verb
candidate set by discarding verb senses whose se-
mantic argument is not present in the clause.

WordNet frames potentially allow a shallow
type pruning based on the semantics provided for
the clause constituents. However we could solely
distinguish people (“somebody”) from things
(“something”), which is too crude to obtain sub-
stantial pruning effects. Moreover, this distinction
is sometimes ambiguous.

Instead, we have developed a more powerful
approach to semantic pruning based on our VOS
repository. We remove from the verb candidate set
those senses whose semantic argument cannot be
present in the sentence. For instance, consider the
clause “The man plays football.” Suppose that we
know that the verb entry play with sense 〈play-
1〉 (“participate in sports”) takes an object of type
〈sport-1〉; i.e., we have a tuple 〈play-1, sport-1〉
in our repository. Then, we check whether any
of the possible senses of football—(i) sport or (ii)
ball—is of type 〈sport-1〉. Here the first sense has
the correct type (the second sense does not); thus
we retain 〈play-1〉 as a possible sense for the verb
entry play. Next, suppose that we consider sense
〈play-3〉 (“play on an instrument”), which accord-
ing to our corpus takes 〈instrument-6〉 as argument
(i.e., there is a tuple 〈play-3, instrument-6〉 in our
VOS repository). Since none of the senses of foot-
ball is of type 〈instrument-6〉, we can safely drop

378



〈play-3〉 from our candidate set. We perform this
procedure for every verb sense in the candidate set.

Semantic pruning makes use of both VOS
repository and the hypernym structure of the noun
senses in WordNet. For each sentence, we obtain
the possible senses of the direct-object argument
of the verb. We then consider each candidate sense
of the verb (e.g., 〈play-1〉), and check whether any
of its compatible object-argument senses (from
our repository) is a hypernym of any of the possi-
ble senses of its actual object argument (in the sen-
tence); e.g., 〈sport-1〉 is a hypernym of 〈football-
1〉. If so, we retain the verb’s candidate sense. If
not, either the candidate sense of the verb is in-
deed incompatible with the object argument in the
sentence, or our repository is incomplete. To han-
dle incompleteness to some extent, we also con-
sider hyponyms of the object-argument senses in
our repository; e.g., if we observe object sport in a
sentence and have verb-sense argument 〈football-
1〉 in our corpus, we consider this a match. If the
hyponyms lead to a match, we retain the verb’s
candidate sense; otherwise, we discard it.

6 Verb-Object Sense Repository

We use three different methods to construct the
repository. In particular, we harness the sense-
annotated WordNet glosses4 as well as the sense-
annotated SemCor corpus (Landes et al., 1998).5

The major part of the VOS repository was ac-
quired from WordNet’s gloss tags. According
to Atkins and Rundell (2008), noun definitions
should be expressed in terms of the class to which
they belong, and verb definitions should refer to
the types of the subjects or objects related to the
action. Based on this rationale, we extracted all
noun senses that appear in the gloss of each verb
sense; each of these noun senses is treated as a
possible sense of the object argument of the cor-
responding verb sense. For example, the gloss of
〈play-1〉 is “participate in games or sports;” each
noun is annotated with its senses (2 and 3 for
“games”, 1 for “sports”). We extract tuples 〈play-
1, game-2〉, 〈play-1, game-3〉, and 〈play-1, sport-
1〉 from this gloss. Note that we only extract
direct-object arguments, i.e., we do not consider
the type of the subject argument of a verb sense.
Since the constituents of the predicate are much

4http://wordnet.princeton.edu/
glosstag.shtml

5http://web.eecs.umich.edu/˜mihalcea/
downloads.html

more important than the subject to determine or
describe a verb sense, lexical resources rarely con-
tain information on the subject (Atkins and Run-
dell, 2008). Similarly, WordNet glosses typically
do not provide any information about adverbials.
Overall, we collected arguments for 8,657 verb
senses (out of WordNet’s 13,767 verb senses) and
a total of 13,050 〈verb-#, object-#〉-pairs.

We leveraged the sense-annotated SemCor cor-
pus to further extend our VOS repository. We
parsed each sentence in the corpus to obtain
the respective pairs of verb sense and object
sense. Since sentences are often more specific
than glosses, and thus less helpful for construct-
ing our repository, we generalized the so-found
object senses using a heuristic method. In particu-
lar, we first obtained all the object senses of each
verb sense, and then repeatedly generalized sets of
at least two senses that share a direct hypernym
to this hypernym. The rationale is that we only
want to generalize if we have some evidence that
a more general sense may apply; we thus require
at least two hyponyms before we generalize. Us-
ing this method, we collected arguments for 1,516
verb senses and a total of 4,131 sense pairs.

Finally, we noticed that the most frequent
senses used in the English language are usually
so general that their glosses do not contain any
relevant semantic argument. For instance, one of
the most frequent verbs is 〈see-1〉, which has gloss
“perceive by 〈sight-3〉”. The correct semantic ar-
gument 〈entity-1〉 is so general that it is omitted
from the gloss. In fact, our gloss-tag extractor
generates tuple 〈see-1, sight-3〉, which is incorrect.
We thus manually annotated the 30 most frequent
verb senses with their object argument types.

Our final repository contains arguments for
9,335 verb senses and a total of 17,181 pairs. Pairs
from SemCor tend to be more specific because
they refer to text occurrences. The assumption of
taking the nouns of the glosses as arguments seems
to be mostly correct, although some errors may
be introduced. Consider the pair 〈play-28, stream-
2〉 extracted from the gloss “discharge or direct
or be discharged or directed as if in a continu-
ous 〈stream-2〉”. Also, in some cases, the glosses
may refer to adverbials as in 〈play-14, location-1〉,
taken from gloss “perform on a certain 〈location-
1〉”. Note that if an argument is missing from our
repository, we may prune the correct sense of the
verb. If, however, there is an additional, incorrect

379



argument in the repository, the correct verb sense
is retained but pruning may be less effective.

7 Evaluation

Dataset. We tested Werdy on the SemEval-2007
coarse-grained dataset.6 It consists of five sense-
annotated documents; the sense annotations refer
to a coarse-grained version of WordNet. In addi-
tion to sense annotations, the corpus also provides
the corresponding KB entries (henceforth termed
“gold entries”) as well as a POS tag. We restrict
our evaluation to verbs that act as clause heads. In
total, 461 such verbs were recognized by ClausIE
(Del Corro and Gemulla, 2013) and the Stanford
Parser (Klein and Manning, 2003).7

WSD Algorithms. For the final step of Werdy,
we used the KB-based WSD algorithms of
Ponzetto and Navigli (2010) and It-Makes-
Sense (Zhong and Ng, 2010), a state-of-the-art
supervised system that was the best performer in
SemEval-2007. Each method only labels entries
for which it is sufficiently confident.

Simplified Extended Lesk (SimpleExtLesk). A
version of Lesk (1986). Each entry is assigned the
sense with highest term overlap between the en-
try’s context (words in the sentence) and both the
sense’s gloss (Kilgarriff and Rosenzweig, 2000)
as well as the glosses of its neighbors (Baner-
jee and Pedersen, 2003). A sense is output only
if the overlap exceeds some threshold; we used
thresholds in the range of 1–20 in our experi-
ments. There are many subtleties and details
in the implementation of SimpleExtLesk so we
used two different libraries: a Java implementation
of WordNet::Similarity (Pedersen et al., 2004),8

which we modified to accept a context string, and
DKPro-WSD (Miller et al., 2013) version 1.1.0,
with lemmatization, removal of stop words, paired
overlap enabled and normalization disabled.

Degree Centrality. Proposed by Navigli and La-
pata (2010). The method collects all paths con-
necting each candidate sense of an entry to the set
of candidate senses of the words the entry’s con-
text. The candidate sense with the highest degree
in the resulting subgraph is selected. We imple-
mented this algorithm using the Neo4j library.9

6The data is annotated with WordNet 2.1 senses; we
converted the annotations to WordNet-3.0 using DKPro-
WSD (Miller et al., 2013).

7Version 3.3.1, model englishRNN.ser.gz
8http://www.sussex.ac.uk/Users/drh21/
9http://www.neo4j.org/

We used a fixed threshold of 1 and vary the search
depth in range 1–20. We used the candidate senses
of all nouns and verbs in a sentence as context.

It-Makes-Sense (IMS). A state-of-the-art, pub-
licly available supervised system (Zhong and Ng,
2010) and a refined version of Chan et al. (2007),
which ranked first in the SemEval-2007 coarse
grained task. We modified the code to accept KB
entries and their candidate senses. We tested both
in WordNet-2.1 and 3.0; for the later we mapped
Werdy’s set of candidates to WordNet-2.1.

Most Frequent Sense (MFS). Selects the most
frequent sense (according to WordNet frequen-
cies) among the set of candidate senses of an en-
try. If there is a tie, we do not label. Note that
this procedure differs slightly from the standard of
picking the entry with the smallest sense id. We
do not follow this approach since it cannot handle
well overlapping entries.

MFS back-off. When one of the above meth-
ods fails to provide a sense label (or provides more
than one), we used the MFS method above with a
threshold of 1. This procedure increased the per-
formance in all cases.

Methodology. The disambiguation was per-
formed with respect to coarse-grained sense clus-
ters. The score of a cluster is the sum of the indi-
vidual scores of its senses (except for IMS which
provides only one answer per word); the cluster
with the highest score was selected. Our source
code and the results of our evaluation are publicly
available10.

The SemEval-2007 task was not designed for
automatic entry recognition, for each word or
multi-word expression it provides the WordNet
entry and the POS tag. We proceeded as follows
to handle multi-word entries. In the WSD step, we
considered the candidate senses of all recognized
entries that overlap with the gold entry. For exam-
ple, we considered the candidate senses of entries
take, breath, and take a breath for gold entry take
a breath.

The SemEval-2007 task uses WordNet-2.1 but
Werdy uses WordNet-3.0. We mapped both the
sense keys and clusters from WordNet-2.1 to
WordNet-3.0. All senses in WordNet-3.0 that
could not be mapped to any cluster were consider
to belong each of them to a single sense cluster.
Note that this procedure is fair: for such senses

10http://people.mpi-inf.mpg.de/
˜corrogg/

380



Algorithm Gold Pruning MFS threshold Verbs (clause heads) F1
Entry back-off /depth P R F1 points

Degree + - + 5 73.54 73.54 73.54
Centrality + + + 11 79.61 79.61 79.61 + 6.07

+ - - 5 73.99 71.58 72.77
+ + - 8 79.91 78.52 79.21 + 6.44

- - + 5 70.41 70.41 70.41
- + + 10 76.46 76.46 76.46 + 6.05

- - - 4 71.05 68.90 69.96
- + - 10 76.81 75.81 76.30 + 6.34

SimpleExtLesk + - + 6 77.28 75.27 76.26
(DKPro) + + + 5 81.90 80.48 81.18 + 4.92

+ - - 1 73.70 52.28 61.17
+ + - 1 81.99 64.21 72.02 + 10.85

- - + 5 74.33 72.57 73.44
- + + 5 79.30 77.75 78.52 + 5.08

- - - 1 69.85 50.54 58.65
- + - 1 78.69 62.20 69.48 + 10.83

SimpleExtLesk + - + 5 77.11 75.27 76.18
(WordNet::Sim) + + + 5 80.57 79.18 79.87 + 3.69

+ - - 1 74.82 68.98 71.78
+ + - 1 79.04 75.27 77.11 + 5.33

- - + 6 74.12 72.35 73.22
- + + 7 77.97 76.46 77.21 + 3.99

- - - 1 71.36 65.66 68.39
- + - 1 76.20 71.92 74.00 + 5.61

MFS + - - 1 76.61 74.62 75.60
+ + - 1 80.35 78.96 79.65 + 4.05

- - - 1 73.67 71.92 72.79
- + - 1 77.75 76.24 76.99 + 4.20

IMS + - + n.a. 79.60 79.60 79.60
(WordNet-2.1) + + + n.a. 80.04 80.04 80.04 + 0.44

- - + n.a. 76.21 75.05 75.63
- + + n.a. 77.53 76.36 76.94 + 1.31

IMS + - + n.a. 78.96 78.96 78.96
(WordNet-3.0) + + + n.a. 79.83 79.83 79.83 + 0.87

- - + n.a. 75.77 74.62 75.19
- + + n.a. 77.53 76.36 76.94 + 1.75

Table 2: Results on SemEval-2007 coarse-grained (verbs as clause heads)

the disambiguation is equivalent to a fine-grained
disambiguation, which is harder.

Results. Our results are displayed in Table 2.
We ran each algorithm with the gold KB entries
provided by in the dataset (+ in column “gold en-
try) as well as the entries obtained by our method
of Sec. 3 (-). We also enabled (+) and disabled
(-) the pruning steps as well as the MFS back-off
strategy. The highest F1 score was achieved by
SimpleExtLesk (DKPro) with pruning and MFS
back-off: 81.18 with gold entries and 78.52 with
automatic entry recognition. In all cases, our syn-
tactic and semantic pruning strategy increased per-
formance (up to +10.85 F1 points). We next dis-
cuss the impact of the various steps of Werdy in

detail.

Detailed Analysis. Table 3 displays step-by-
step results for DKPro’s SimpleExtLesk, for MFS,
as well as SimpleExtLesk with MFS back-off, the
best performing strategy. The table shows results
when only some Werdy’s steps are used. We start
from a direct use of the respective algorithm with
the gold entries of SemEval-2007 after each hor-
izontal line, and then successively add the Werdy
steps indicated in the table.

When no gold entries were provided, perfor-
mance dropped due to the increase of sense can-
didates for multi-word expressions, which include
the possible senses of the expression itself as well
as the senses of the entry’s parts that are them-

381



Steps Performed threshold P R F1 F1 points

SimpleExtLesk (DKPro)

Plain with gold entries 1 73.70 52.28 61.17

+ Entry Recognition 1 69.85 50.54 58.65 - 2.52
+ Syntactic Pruning 1 76.47 58.84 66.50 + 7.85
+ Semantic Pruning 1 78.69 62.20 69.48 + 2.98

+ Entry Recognition 1 69.85 50.54 58.65 - 2.52
+ Semantic Pruning 1 73.85 55.39 63.30 + 4.65

+ Syntactic Pruning 1 79.33 61.21 69.10 + 7.93
+ Semantic Pruning 1 81.99 64.21 72.02 + 2.92

+ Semantic Pruning 1 78.11 56.90 65.84 + 4.67

MFS

Plain with gold entries 1 76.61 74.62 75.60

+ Entry Recognition 1 73.67 71.92 72.79 - 2.81
+ Syntactic Pruning 1 75.77 74.14 74.95 + 2.16
+ Semantic Pruning 1 77.75 76.24 76.99 + 2.04

+ Entry Recognition 1 73.67 71.92 72.79 - 2.81
+ Semantic Pruning 1 77.09 75.43 76.25 + 3.46

+ Syntactic Pruning 1 78.46 76.94 77.69 + 2.09
+ Semantic Pruning 1 80.35 78.96 79.65 + 1.96

+ Semantic Pruning 1 79.91 78.02 78.95 + 3.35

SimpleExtLesk (DKPro) with MFS back-off

Plain with gold entries 6 77.28 75.27 76.26

+ Entry Recognition 6 74.33 72.57 73.44 - 2.82
+ Syntactic Pruning 5 76.65 75.00 75.82 + 2.38
+ Semantic Pruning 5 79.30 77.75 78.52 + 2.70

+ Entry Recognition 5 74.33 72.57 73.44 - 2.82
+ Semantic Pruning 5 78.19 76.51 77.34 +3.90

+ Syntactic Pruning 5 79.34 77.80 78.56 + 2.30
+ Semantic Pruning 5 81.90 80.48 81.18 + 2.62

+ Semantic Pruning 5 81.02 79.09 80.04 + 3.78

Table 3: Step-by-step results

selves WordNet entries. Our entry recognizer
tends to do a good job since it managed to cor-
rectly identify all the relevant entries except in two
cases (i.e. “take up” and “get rolling”), in which
the dependency parse was incorrect. The drop in
F1 for our automatic entry recognition was mainly
due to incorrect selection of the correct entry of a
set of alternative, overlapping entries.

Syntactic pruning did not prune the correct
sense in most cases. In 16 cases (with gold en-
tries), however, the correct sense was pruned. Five
of these senses were pruned due to incorrect de-
pendency parses, which led to incorrect frame
identification. In two cases, the sense was not
annotated with the recognized frame in WordNet,
although it seemed adequate. In the remaining
cases, a general frame from WordNet was incor-
rectly omitted. Improvements to WordNet’s frame
annotations may thus make syntactic pruning even

more effective.
Semantic pruning also improves performance.

Here the correct sense was pruned for 11 verbs,
mainly due to the noisiness and incompleteness
of our VOS repository. Without using gold en-
tries, we found in total 237 semantic matches be-
tween possible verbs senses and possible object
senses (200 with gold entries). We also found that
our manual annotations in the VOS repository (see
Sec. 6) did not affect our experiments.

The results show that syntactic and semantic
pruning are beneficial for verb sense disambigua-
tion, but also stress the necessity to improve ex-
isting resources. Ideally, each verb sense would
be annotated with both the possible clause types
or syntactic patterns in which it can occur as well
as the possible senses of its objects. Annotations
for subjects and adverbial arguments may also be
beneficial.

382



8 Related Work

WSD is a classification task where for every word
there is a set of possible senses given by some ex-
ternal resource (as a KB). Two types of methods
can be distinguished in WSD. Supervised systems
(Dang and Palmer, 2005; Dligach and Palmer,
2008; Chen and Palmer, 2009; Zhong and Ng,
2010) use a classifier to assign senses to words,
mostly relying on manually annotated data for
training. In principle, these systems suffer from
low coverage since the training data is usually
sparse. Some authors have tried to overcome this
limitation by exploiting linked resources as train-
ing data (Shen et al., 2013; Cholakov et al., 2014).

The second WSD approach corresponds to the
so-called KB methods (Agirre and Soroa, 2009;
Ponzetto and Navigli, 2010; Miller et al., 2012;
Agirre et al., 2014). They rely on a back-
ground KB (typically WordNet or extended ver-
sions (Navigli and Ponzetto, 2012)), where related
senses appear close to each other. KB-based al-
gorithms often differ in the way the KB is ex-
plored. It has been shown that a key point to en-
hance performance is the amount of semantic in-
formation in the KB (Ponzetto and Navigli, 2010;
Miller et al., 2012). Our framework fits this line of
work since it is also unsupervised and enriches the
background knowledge in order to enhance perfor-
mance of standard WSD algorithms. A compre-
hensive overview of WSD systems can be found
in Navigli (2009) and Navigli (2012).

To bring WSD to real-world applications, the
mapping between text and KB entries is a funda-
mental first step. It has been pointed that the ex-
istence of multi-word expressions imposes multi-
ple challenges to text understanding tasks (Sag et
al., 2002). The problem has been addressed by
Arranz et al. (2005) and Finlayson and Kulkarni
(2011). They find multi-word entries by match-
ing word sequences allowing some morphological
and POS variations according to predefined pat-
terns. Our method differs in that we can recognize
KB entries that appear discontinuously and in that
we do not select the correct entry but generate a
set of potential entries.

Linguists have noted the link between verb
senses and the syntactic structure and argument
types (Quirk et al., 1985; Levin, 1993; Hanks,
1996), and supervised WSD systems were devel-
oped to capture this relation (Dang and Palmer,
2005; Chen and Palmer, 2009; Dligach and

Palmer, 2008; Cholakov et al., 2014). In Dang
and Palmer (2005) and Chen and Palmer (2009),
it is shown that WSD tasks can be improved with
features that capture the syntactic structure and in-
formation about verb arguments and their types.
They use features as shallow named entity recog-
nition and the hypernyms of the possible senses
of the noun arguments. Dang and Palmer (2005)
also included features extracted from PropBank
(Palmer et al., 2005) from role labels and frames.
Dligach and Palmer (2008) generated a corpus of
verb and their arguments (both surface forms),
which was used to incorporate a semantic feature
to the supervised system.

In our work, we also incorporate syntactic and
semantic information. Instead of learning the re-
lation between the verb senses and the syntactic
structure, however we incorporate it explicitly us-
ing the WordNet frames, which provide informa-
tion about which verb sense should be consider
for a given syntactic pattern. We also incorporate
explicitly the semantic relation between each verb
sense and its arguments using our VOS repository.

Different resources of semantic arguments for
automatic text understanding tasks have been con-
structed (Baker et al., 1998; Palmer et al., 2005;
Kipper et al., 2008; Gurevych et al., 2012; Nakas-
hole et al., 2012; Flati and Navigli, 2013). In
(Baker et al., 1998; Palmer et al., 2005; Kipper
et al., 2008; Gurevych et al., 2012), the classifica-
tion of verbs and arguments is focused toward se-
mantic or thematic roles. Nakashole et al. (2012)
uses semantic types to construct a taxonomy of bi-
nary relations and Flati and Navigli (2013) col-
lected semantic arguments for given textual ex-
pressions. For instance, given the verb “break”,
they extract a pattern “break 〈body part-1〉”. In
contrast to existing resources, our VOS repository
disambiguates both the verb sense and the senses
of its arguments.

9 Conclusion

We presented Werdy, a framework for word-sense
recognition and disambiguation with a particular
focus on verbs and verbal phrases. Our results
indicate that incorporating syntactic and seman-
tic constraints improves the performance of verb
sense disambiguation methods. This stresses the
necessity of extending and improving the available
syntactic and semantic resources, such as Word-
Net or our VOS repository.

383



References
Eneko Agirre and Aitor Soroa. 2009. Personalizing

pagerank for word sense disambiguation. In Pro-
ceedings of EACL, pages 33–41.

Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.

Victoria Arranz, Jordi Atserias, and Mauro Castillo.
2005. Multiwords and word sense disambiguation.
In Computational Linguistics and Intelligent Text
Processing, volume 3406 of Lecture Notes in Com-
puter Science, pages 250–262.

B. T. Sue Atkins and Michael Rundell. 2008. The Ox-
ford Guide to Practical Lexicography. Oxford Uni-
versity Press.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of ACL, pages 86–90.

Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of IJCAI, pages 805–810.

Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: Exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of SemEval, pages 253–256.

Jinying Chen and Martha Palmer. 2009. Improving
english verb sense disambiguation performance with
linguistically motivated features and clear sense dis-
tinction boundaries. Language Resources and Eval-
uation, 43(2):181–208.

Kostadin Cholakov, Judith Eckle-Kohler, and Iryna
Gurevych. 2014. Automated verb sense labelling
based on linked lexical resources. In Proceedings of
EACL, pages 68–77.

Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL, pages 42–49.

Luciano Del Corro and Rainer Gemulla. 2013.
Clausie: clause-based open information extraction.
In Proceedings of WWW, pages 355–366.

Dmitriy Dligach and Martha Palmer. 2008. Improv-
ing verb sense disambiguation with automatically
retrieved semantic knowledge. In Proceedings of
ICSC, pages 182–189.

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.

Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of MWE, pages 20–
24.

Mark Alan Finlayson. 2014. Java libraries for access-
ing the princeton wordnet: Comparison and evalua-
tion. In Proceedings of GWC.

Tiziano Flati and Roberto Navigli. 2013. Spred:
Large-scale harvesting of semantic predicates. In
Proceedings of ACL, pages 1222–1232.

Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. Uby - a large-scale unified
lexical-semantic resource based on lmf. In Proceed-
ings of EACL, pages 580–590.

Patrick Hanks. 1996. Contextual dependency and lex-
ical sets. International Journal of Corpus Linguis-
tics, 1(1):75–98.

Adam Kilgarriff and Joseph Rosenzweig. 2000.
Framework and results for english senseval. Com-
puters and the Humanities, 34(1-2):15–48.

Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Evalua-
tion, 42(1):21–40.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL,
pages 423–430.

Shari Landes, Claudia Leacock, and Randee I. Tengi,
1998. Building Semantic Concordances. MIT
Press.

Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of SIGDOC, pages 24–26.

Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.

Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of COLING,
pages 1781–1796.

Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, Torsten
Zesch, and Iryna Gurevych. 2013. Dkpro wsd: A
generalized uima-based framework for word sense
disambiguation. In Proceedings of ACL: System
Demonstrations, pages 37–42.

Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
EMNLP, pages 1135–1145.

Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsuper-
vised word sense disambiguation. EEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(4):678–692.

384



Roberto Navigli and Simone Paolo Ponzetto. 2012.
Babelnet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193(0):217–
250.

Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval, pages 30–35.

Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Computing Surveys, 41(2):10:1–
10:69.

Roberto Navigli. 2012. A quick tour of word sense dis-
ambiguation, induction and related approaches. In
Proceedings of SOFSEM, pages 115–129.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.

Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: Measuring the

relatedness of concepts. In Proceedings of HLT-
NAACL: Demonstration Papers, pages 38–41.

Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522–1531.

Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Proceed-
ings of CICLing, pages 1–15.

Hui Shen, Razvan Bunescu, and Rada Mihalcea. 2013.
Coarse to fine grained sense disambiguation in
wikipedia. In Proceedings of *SEM, pages 22–31.

Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proceedings of ACL: System
Demonstrations, pages 78–83.

385


