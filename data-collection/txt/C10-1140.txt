Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1245–1253,

Beijing, August 2010

1245

Kernel Slicing: Scalable Online Training with Conjunctive Features

Naoki Yoshinaga

Institute of Industrial Science,

the University of Tokyo

Masaru Kitsuregawa

Institute of Industrial Science,

the University of Tokyo

ynaga@tkl.iis.u-tokyo.ac.jp

kitsure@tkl.iis.u-tokyo.ac.jp

Abstract

This paper proposes an efﬁcient online
method that trains a classiﬁer with many
conjunctive features. We employ kernel
computation called kernel slicing, which
explicitly considers conjunctions among
frequent features in computing the poly-
nomial kernel, to combine the merits of
linear and kernel-based training. To im-
prove the scalability of this training, we
reuse the temporal margins of partial fea-
ture vectors and terminate unnecessary
margin computations. Experiments on de-
pendency parsing and hyponymy-relation
extraction demonstrated that our method
could train a classiﬁer orders of magni-
tude faster than kernel-based online learn-
ing, while retaining its space efﬁciency.

Introduction

1
The past twenty years have witnessed a growing
use of machine-learning classiﬁers in the ﬁeld of
NLP. Since the classiﬁcation target of complex
NLP tasks (e.g., dependency parsing and relation
extraction) consists of more than one constituent
(e.g., a head and a dependent in dependency pars-
ing), we need to consider conjunctive features,
i.e., conjunctions of primitive features that fo-
cus on the particular clues of each constituent, to
achieve a high degree of accuracy in those tasks.
Training with conjunctive features involves a
space-time trade-off in the way conjunctive fea-
tures are handled. Linear models, such as log-
linear models, explicitly estimate the weights of
conjunctive features, and training thus requires a
great deal of memory when we take higher-order

conjunctive features into consideration. Kernel-
based models such as support vector machines, on
the other hand, ensure space efﬁciency by using
the kernel trick to implicitly consider conjunctive
features. However, training takes quadratic time
in the number of examples, even with online algo-
rithms such as the (kernel) perceptron (Freund and
Schapire, 1999), and we cannot fully exploit am-
ple ‘labeled’ data obtained with semi-supervised
algorithms (Ando and Zhang, 2005; Bellare et al.,
2007; Liang et al., 2008; Daum´e III, 2008).

We aim at resolving this dilemma in train-
ing with conjunctive features, and propose online
learning that combines the time efﬁciency of lin-
ear training and the space efﬁciency of kernel-
based training. Following the work by Goldberg
and Elhadad (2008), we explicitly take conjunc-
tive features into account that frequently appear in
the training data, and implicitly consider the other
conjunctive features by using the polynomial ker-
nel. We then improve the scalability of this train-
ing by a method called kernel slicing, which al-
lows us to reuse the temporal margins of partial
feature vectors and to terminate computations that
do not contribute to parameter updates.

We evaluate our method in two NLP tasks: de-
pendency parsing and hyponymy-relation extrac-
tion. We demonstrate that our method is orders of
magnitude faster than kernel-based online learn-
ing while retaining its space efﬁciency.

The remainder of this paper is organized as fol-
lows. Section 2 introduces preliminaries and no-
tations. Section 3 proposes our training method.
Section 4 evaluates the proposed method. Sec-
tion 5 discusses related studies. Section 6 con-
cludes this paper and addresses future work.

1246

Algorithm 1 BASE LEARNER: KERNEL PA-I
INPUT: T = {(x, y)t}|T |t=1, k : Rn × Rn 7→ R, C ∈ R+
OUTPUT: (S|T |, α|T |)
1: initialize: S0 ← ∅, α0 ← ∅
2: for t = 1 to |T | do
3:
4:


receive example (x, y)t : x ∈ Rn, y ∈ {−1, +1}
compute margin: mt(x) = Xsi∈St−1
if ‘t = max{0, 1 − ymt(x)} > 0 then

αik(si, x)

ﬀ

5:

‘t
kxk2

C,

τt ← min
αt ← αt−1 ∪ {τty}, St ← St−1 ∪ {x}
αt ← αt−1, St ← St−1

6:

else

7:
8:
9:
end if
10:
11: end for
12: return (S|T |, α|T |)

2 Preliminaries

This section ﬁrst introduces a passive-aggressive
algorithm (Crammer et al., 2006), which we use
as a base learner. We then explain fast methods of
computing the polynomial kernel.

Each example x in a classiﬁcation problem is
represented by a feature vector whose element xj
is a value of a feature function, fj ∈ F. Here, we
assume a binary feature function, fj(x) ∈ {0, 1},
which returns one if particular context data appear
in the example. We say that feature fj is active in
example x when xj = fj(x) = 1. We denote a
binary feature vector, x, as a set of active features
x = {fj | fj ∈ F, fj(x) = 1} for brevity; fj ∈ x
means that fj is active in x, and |x| represents the
number of active features in x.

2.1 Kernel Passive-Aggressive Algorithm
A passive-aggressive algorithm (PA) (Crammer et
al., 2006) represents online learning that updates
parameters for given labeled example (x, y)t ∈
T in each round t. We assume a binary label,
y ∈ {−1, +1}, here for clarity. Algorithm 1
is a variant of PA (PA-I) that incorporates a ker-
nel function, k.
In round t, PA-I ﬁrst computes
a (signed) margin mt(x) of x by using the ker-
nel function with support set St−1 and coefﬁcients
PA-I then suffers a hinge-loss,
αt−1 (Line 4).
‘t = max{0, 1 − ymt(x)} (Line 5). If ‘t > 0,
PA-I adds x to St−1 (Line 7). Hyperparameter C
controls the aggressiveness of parameter updates.
The kernel function computes a dot product in

RH space without mapping x ∈ Rn to φ(x) ∈
RH (k(x, x0) = φ(x)Tφ(x0)). We can implic-
itly consider (weighted) d or less order conjunc-
tions of primitive features by using polynomial
kernel function kd(s, x) = (sTx + 1)d. For ex-
ample, given support vector s = (s1, s2)T and
input example x = (x1, x2)T, the second-order
polynomial kernel returns k2(s, x) = (s1x1 +
s2x2 + 1)2 = 1 + 3s1x1 + 3s2x2 + 2s1x1s2x2 (∵
si, xi ∈ {0, 1}). This function thus implies map-
ping φ2(x) = (1,√3x1,√3x2,√2x1x2)T.
Although online learning is generally efﬁcient,
the kernel spoils its efﬁciency (Dekel et al., 2008).
This is because the kernel evaluation (Line 4)
takes O(|St−1||x|) time and |St−1| increases as
training continues. The learner thus takes the most
amount of time in this margin computation.

2.2 Kernel Computation for Classiﬁcation
This section explains fast, exact methods of com-
puting the polynomial kernel, which are meant to
test the trained model, (S, α), and involve sub-
stantial computational cost in preparation.

2.2.1 Kernel Inverted

Kudo and Matsumoto (2003) proposed polyno-
mial kernel inverted (PKI), which builds inverted
indices h(fj) ≡ {s| s ∈ S, fj ∈ s} from each
feature fj to support vector s ∈ S to only con-
sider support vector s relevant to given x such
that sTx 6= 0. The time complexity of PKI is
|x|Pfj∈x |h(fj)|,
O(B · |x| + |S|) where B ≡ 1
which is smaller than O(|S||x|) if x has many
rare features fj such that |h(fj)| (cid:28) |S|.
To the best of our knowledge, this is the only
exact method that has been used to speed up mar-
gin computation in the context of kernel-based on-
line learning (Okanohara and Tsujii, 2007).

2.2.2 Kernel Expansion

Isozaki and Kazawa (2002) and Kudo and Mat-
sumoto (2003) proposed kernel expansion, which
explicitly maps both support set S and given ex-
ample x ∈ Rn into RH by mapping φd imposed
by kd:

m(x) = Xsi∈S

αiφd(si)!T

φd(x) = Xfi∈xd

wi,

1247

where xd ∈ {0, 1}H is a binary feature vector
in which xd
i = 1 for (φd(x))i 6= 0, and w is a
weight vector in the expanded feature space, F d.
The weight vector w is computed from S and α:

w = Xsi∈S

αi

dXk=0

ck
dIk(sd

i ),

(1)

2 = 1, c1

2 = 3, and c2

2 = 2)1 and Ik(sd

where ck
d is a squared coefﬁcient of k-th order con-
junctive features for d-th order polynomial kernel
(e.g., c0
i ) is
i ∈ {0, 1}H whose dimensions other than those
sd
of k-th order conjunctive features are set to zero.
The time complexity of kernel expansion is
k=0(cid:0)|x|k(cid:1) ∝ |x|d, which
O(|xd|) where |xd| =Pd
can be smaller than O(|S||x|) in usual NLP tasks
(|x| (cid:28) |S| and d ≤ 4).
2.2.3 Kernel Splitting

Since kernel expansion demands a huge mem-
ory volume to store the weight vector, w, in RH

k=0(cid:0)|F|k(cid:1)), Goldberg and Elhadad (2008)
(H =Pd
only explicitly considered conjunctions among
features fC ∈ FC that commonly appear in sup-
port set S, and handled the other conjunctive fea-
tures relevant to rare features fR ∈ F \ FC by
using the polynomial kernel:

m(x) = m( ˜x) + m(x) − m( ˜x)

= Xfi∈ ˜xd

˜wi + Xsi∈SR

αik0d(si, x, ˜x), (2)

where ˜x is x whose dimensions of rare features
are set to zero, ˜w is a weight vector computed
C, and k0d(s, x, ˜x) is deﬁned as:
with Eq. 1 for F d

k0d(s, x, ˜x) ≡ kd(s, x) − kd(s, ˜x).

i x = sT

We can space-efﬁciently compute the ﬁrst term
of Eq. 2 since | ˜w| (cid:28) |w|, while we can
quickly compute the second term of Eq. 2 since
k0d(si, x, ˜x) = 0 when sT
i ˜x; we only
need to consider a small subset of the support set,

SR =SfR∈x\ ˜x h(fR), that has at least one of the
rare features, fR, appearing in x\ ˜x (|SR| (cid:28) |S|).
Counting the number of features examined, the
time complexity of Eq. 2 is O(| ˜xd| + |SR|| ˜x|).
1Following Lemma 1 in Kudo and Matsumoto (2003),
d =Pd

m=0(−1)k−m · ml` k
Pk

´´.

´`

`d

ck

l=k

m

l

3 Algorithm
This section ﬁrst describes the way kernel splitting
is integrated into PA-I (Section 3.1). We then pro-
pose kernel slicing (Section 3.2), which enables
us to reuse the temporal margins computed in the
past rounds (Section 3.2.1) and to skip unneces-
sary margin computations (Section 3.2.2).

In what follows, we use PA-I as a base learner.
Note that an analogous argument can be applied
to other perceptron-like online learners with the
additive weight update (Line 7 in Algorithm 1).

3.1 Base Learner with Kernel Splitting
A problem in integrating kernel splitting into the
base learner presented in Algorithm 1 is how to
determine FC, features among which we explic-
itly consider conjunctions, without knowing the
ﬁnal support set, S|T |. We heuristically solve
this by ranking feature f according to their fre-
quency in the training data and by using the top-
N frequent features in the training data as FC
(= {f | f ∈ F, RANK(f ) ≤ N}).2 Since S|T |
is a subset of the examples, this approximates the
selection from S|T |. We empirically demonstrate
the validity of this approach in the experiments.
We then use FC to construct a base learner with
kernel splitting; we replace the kernel computa-
tion (Line 4 in Algorithm 1) with Eq. 2 where
(S, α) = (St−1, αt−1). To compute mt( ˜x) by
using kernel expansion, we need to additionally
maintain the weight vector ˜w for the conjunctions
of common features that appear in St−1.
The additive parameter update of PA-I enables
us to keep ˜w to correspond to (St−1, αt−1).
When we add x to support set St−1 (Line 7 in
Algorithm 1), we also update ˜w with Eq. 1:

˜w ← ˜w + τty

ck
dIk( ˜xd).

dXk=0

Following (Kudo and Matsumoto, 2003), we
use a trie (hereafter, weight trie) to maintain con-
junctive features. Each edge in the weight trie is
labeled with a primitive feature, while each path

2The overhead of counting features is negligible com-
pared to the total training time. If we want to run the learner
in a purely online manner, we can alternatively choose ﬁrst
N features that appear in the processed examples as FC.

1248

represents a conjunctive feature that combines all
the primitive features on the path. The weights
of conjunctive features are retrieved by travers-
ing nodes in the trie. We carry out an analogous
traversal in updating the parameters of conjunc-
tive features, while registering a new conjunctive
feature by adding an edge to the trie.

The base learner with kernel splitting combines
the virtues of linear training and kernel-based
training. It reduces to linear training when we in-
crease N to |F|, while it reduces to kernel-based
training when we decrease N to 0. The output
is support set S|T | and coefﬁcients α|T | (option-
ally, ˜w), to which the efﬁcient classiﬁcation tech-
niques discussed in Section 2.2 and the one pro-
posed by Yoshinaga and Kitsuregawa (2009) can
be applied.

Note on weight trie construction The time and
space efﬁciency of this learner strongly depends
on the way the weight trie is constructed. We
need to address two practical issues that greatly
affect efﬁciency. First, we traverse the trie from
the rarest feature that constitutes a conjunctive
feature. This rare-to-frequent mining helps us to
avoid enumerating higher-order conjunctive fea-
tures that have not been registered in the trie, when
computing margin. Second, we use RANK(f )
encoded into a dlog128 RANK(f )e-byte string by
using variable-byte coding (Williams and Zobel,
1999) as f’s representation in the trie. This en-
coding reduces the trie size, since features with
small RANK(f ) will appear frequently in the trie.

3.2 Base Learner with Kernel Slicing
Although a base learner with kernel splitting can
enjoy the merits of linear and kernel-based train-
ing, it can simultaneously suffer from their demer-
its. Because the training takes polynomial time
in the number of common features in x (| ˜xd| =
k=0(cid:0)| ˜x|k(cid:1) ∝ | ˜x|d) at each round, we need to set
Pd
N to a smaller value when we take higher-order
conjunctive features into consideration. However,
since the margin computation takes linear time in
the number of support vectors |SR| relevant to rare
features fR ∈ F\FC, we need to set N to a larger
value when we handle a larger number of training
examples. The training thereby slows down when

we train a classiﬁer with high-order conjunctive
features and a large number of training examples.
We then attempt to improve the scalability of
the training by exploiting a characteristic of la-
beled data in NLP. Because examples in NLP tasks
are likely to be redundant (Yoshinaga and Kitsure-
gawa, 2009), the learner computes margins of ex-
amples that have many features in common. If we
can reuse the ‘temporal’ margins of partial feature
vectors computed in past rounds, this will speed
up the computation of margins.

We propose kernel slicing, which generalizes
kernel splitting in a purely feature-wise manner
and enables us to reuse the temporal partial mar-
gins. Starting from the most frequent feature f1 in
x (f1 = argminf∈x RANK(f )), we incrementally
compute mt(x) by accumulating a partial mar-
gin, mj
t (x) ≡ mt(xj) − mt(xj−1), when we add
the j-th frequent feature fj in x:

mt(x) = m0

t +

mj

t (x),

(3)

|x|Xj=1

where m0
xj has the j most frequent features in x (x0 = ∅,

t =Psi∈St−1 αikd(si, ∅) =Pi αi, and
k=0{argminf∈x\xk
ing the polynomial kernel:

xj =Fj−1

t (x) can be computed by us-

Partial margin mj

RANK(f )}).

mj

t (x) = Xsi∈St−1

αik0d(si, xj, xj−1),

(4)

or by using kernel expansion:

mj

t (x) = Xfi∈xd

j\xd

j−1

˜wi.

(5)

Kernel splitting is a special case of kernel slicing,
which uses Eq. 5 for fj ∈ FC and Eq. 4 for fj ∈
F \ FC.
3.2.1 Reuse of Temporal Partial Margins

We can speed up both Eqs. 4 and 5 by reusing
t0(x) that had

a temporal partial margin, δj
t0 = mj
been computed in past round t0(< t):

mj

t (x) = δj

t0 + Xsi∈Sj

αik0d(si, xj, xj−1), (6)

where Sj = {s| s ∈ St−1 \ St0−1, fj ∈ s}.

1249

t

Algorithm 2 KERNEL SLICING
INPUT: x ∈ 2F , St−1, αt−1, FC ⊆ F, δ : 2F 7→ N × R
OUTPUT: mt(x)
1: initialize: x0 ← ∅, j ← 1, mt(x) ← m0
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11: mt(x) ← mt(x) + mj
12: until xj 6= x
13: return mt(x)

xj ← xj−1 t {argminf∈x\xj−1
retrieve partial margin: (t0, δj
if fj ∈ F \ FC or Eq. 7 is true then
t (x) using Eq. 6 with δj
t0
else

end if
update partial margin: δ(xj) ← (t, mj

t0 ) ← δ(xj)

t (x) using Eq. 5

RANK(f )}

compute mj

compute mj

t (x))

t (x)

Eq. 6 is faster than Eq. 4,3 and can even be
faster than Eq. 5.4 When RANK(fj) is high, xj ap-
pears frequently in the training examples and |Sj|
becomes small since t0 will be close to t. When
RANK(fj) is low, xj rarely appears in the training
examples but we can still expect |Sj| to be small
since the number of support vectors in St−1\St0−1
that have rare feature fj will be small.
To compute Eq. 3, we now have the choice to
choose Eq. 5 or 6 for fj ∈ FC. Counting the
number of features to be examined in computing
mj
t (x), we have the following criteria to deter-
mine whether we can use Eq. 6 instead of Eq. 5:

1 + |Sj||xj−1| ≤ |xd

j \ xd

j−1| =

k − 1(cid:19),
dXk=1(cid:18)j − 1

where the left- and right-hand sides indicate the
number of features examined in Eq. 6 for the for-
mer and Eq. 5 for the latter. Expanding the right-
hand side for d = 2, 3 and dividing both sides with
|xj−1| = j − 1, we have:
|Sj| ≤(cid:26) 1

(d = 2)
(d = 3)

(7)

j
2

.

If this condition is met after retrieving the tem-
poral partial margin, δj
t0, we can compute partial
margin mj
t (x) with Eq. 6. This analysis reveals
3When a margin of xj has not been computed, we regard

t0 = 0 and δj

t0 = 0, which reduces Eq. 6 to Eq. 4.

4We associate partial margins with partial feature se-
quences whose features are sorted by frequent-to-rare order,
and store them in a trie (partial margin trie). This enables us
to retrieve partial margin δj

t0 for given xj in O(1) time.

that we can expect little speed-up for the second-
order polynomial kernel; we will only use Eq. 6
with third or higher-order polynomial kernel.

Algorithm 2 summarizes the margin computa-
tion with kernel slicing. It processes each feature
fj ∈ x in frequent-to-rare order, and accumulates
partial margin mj
t (x) to have mt(x). Intuitively
speaking, when the algorithm uses the partial mar-
gin, it only considers support vectors on each fea-
ture that have been added since the last evaluation
of the partial feature vector, to avoid the repetition
in kernel evaluation as much as possible.

3.2.2 Termination of Margin Computation

k

Kernel slicing enables another optimization that
exploits a characteristic of online learning. Be-
cause we need an exact margin, mt(x), only when
hinge-loss ‘t = 1−ymt(x) is positive, we can ﬁn-
ish margin computation as soon as we ﬁnd that the
lower-bound of ymt(x) is larger than one.

αi

mk

mk

αi + ˇk0d Xsi∈S−k
αi + ˆk0d Xsi∈S−k

When ymt(x) is larger than one after pro-
cessing feature fj in Eq. 3, we quickly examine
whether this will hold even after we process the
remaining features. We can compute a possible
range of partial margin mk
t (x) with Eq. 4, hav-
ing the upper- and lower-bounds, ˆk0d and ˇk0d, of
k0d(si, xk, xk−1) (= kd(si, xk) − kd(si, xk−1)):
(8)

t (x) ≤ ˆk0d Xsi∈S+
t (x) ≥ ˇk0d Xsi∈S+
where S +
k = {si | si ∈ St−1, fk ∈ si, αi > 0},
S−k = {si | si ∈ St−1, fk ∈ si, αi < 0}, ˆk0d =
(k + 1)d − kd and ˇk0d = 2d − 1 (∵ 0 ≤ sT
i xk−1 ≤
|xk−1| = k − 1, sT
i xk−1 + 1 for all
si ∈ S +
We accumulate Eqs. 8 and 9 from rare to fre-
quent features, and use the intermediate results
to estimate the possible range of mt(x) before
Line 3 in Algorithm 2.
If the lower bound of
ymt(x) turns out to be larger than one, we ter-
minate the computation of mt(x).

k ∪ S−k ).

i xk = sT

(9)

αi,

As training continues, the model becomes dis-
criminative and given x is likely to have a larger
margin. The impact of this termination will in-
crease as the amount of training data expands.

k

1250

4 Evaluation
We evaluated the proposed method in two NLP
tasks: dependency parsing (Sassano, 2004) and
hyponymy-relation extraction (Sumida et al.,
2008). We used labeled data included in open-
source softwares to promote the reproducibility of
our results.5 All the experiments were conducted
on a server with an Intel R(cid:13) XeonTM 3.2 GHz CPU.
We used a double-array trie (Aoe, 1989; Yata et
al., 2009) as an implementation of the weight trie
and the partial margin trie.

4.1 Task Descriptions
Japanese Dependency Parsing A parser inputs
a sentence segmented by a bunsetsu (base phrase
in Japanese), and selects a particular pair of bun-
setsus (dependent and head candidates); the clas-
siﬁer then outputs label y = +1 (dependent) or
−1 (independent) for the pair. The features con-
sist of the surface form, POS, POS-subcategory
and the inﬂection form of each bunsetsu, and sur-
rounding contexts such as the positional distance,
punctuations and brackets. See (Yoshinaga and
Kitsuregawa, 2009) for details on the features.

Hyponymy-Relation Extraction A hyponymy
relation extractor (Sumida et al., 2008) ﬁrst ex-
tracts a pair of entities from hierarchical listing
structures in Wikipedia articles (hypernym and
hyponym candidates); a classiﬁer then outputs la-
bel y = +1 (correct) or −1 (incorrect) for the
pair. The features include a surface form, mor-
phemes, POS and the listing type for each entity,
and surrounding contexts such as the hierarchical
distance between the entities. See (Sumida et al.,
2008) for details on the features.

4.2 Settings
Table 1 summarizes the training data for the two
tasks. The examples for the Japanese dependency
parsing task were generated for a transition-based
parser (Sassano, 2004) from a standard data set.6
We used the dependency accuracy of the parser

5The labeled data for dependency parsing is available
http://www.tkl.iis.u-tokyo.ac.jp/˜ynaga/pecco/, and
from:
the labeled data for hyponymy-relation extraction is avail-
able from: http://nlpwww.nict.go.jp/hyponymy/.

6Kyoto Text Corpus Version 4.0:

http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html.

DATA SET
|T |

(y = +1)
(y = −1)
Ave. of |x|
Ave. of |x2|
Ave. of |x3|
|F|
|F 2|
|F 3|

DEP
296,776
150,064
146,712
27.6
396.1
3558.3
64,493
3,093,768
58,361,669

REL
201,664
152,199
49,465
15.4
136.9
798.7
306,036
6,688,886
64,249,234

Table 1: Training data for dependency parsing
(DEP) and hyponymy-relation extraction (REL).

as model accuracy in this task. In the hyponymy-
relation extraction task, we randomly chosen two
sets of 10,000 examples from the labeled data for
development and testing, and used the remaining
examples for training. Note that the number of
active features, |F d|, dramatically grows when we
consider higher-order conjunctive features.
We compared the proposed method, PA-I SL
(Algorithm 1 with Algorithm 2), to PA-I KER-
NEL (Algorithm 1 with PKI; Okanohara and Tsu-
jii (2007)), PA-I KE (Algorithm 1 with kernel ex-
pansion; viz., kernel splitting with N = |F|),
SVM (batch training of support vector machines),7
and ‘1-LLM (stochastic gradient descent training
of the ‘1-regularized log-linear model: Tsuruoka
et al. (2009)). We refer to PA-I SL that does not
reuse temporal partial margins as PA-I SL∗. To
demonstrate the impact of conjunctive features on
model accuracy, we also trained PA-I without con-
junctive features. The number of iterations in PA-I
was set to 20, and the parameters of PA-I were av-
eraged in an efﬁcient manner (Daum´e III, 2006).
We explicitly considered conjunctions among top-
N (N = 125 × 2n; n ≥ 0) features in PA-I SL
and PA-I SL∗. The hyperparameters were tuned to
maximize accuracy on the development set.

4.3 Results
Tables 2 and 3 list the experimental results for
the two tasks (due to space limitations, Tables 2
and 3 list PA-I SL with parameter N that achieved
the fastest speed). The accuracy of the models
trained with the proposed method was better than
‘1-LLMs and was comparable to SVMs. The infe-

7http://chasen.org/˜taku/software/TinySVM/

1251

METHOD
PA-I
‘1-LLM
SVM
PA-I KERNEL
PA-I KE
PA-I SL∗N =4000
‘1-LLM
SVM
PA-I KERNEL
PA-I KE
PA-I SLN =250

d
1
2
2
2
2
2
3
3
3
3
3

ACC.
TIME
3s
88.56%
90.55%
340s
90.76% 29863s
8361s
90.68%
90.67%
41s
33s
90.71%
90.76%
4057s
90.93% 25912s
8704s
90.90%
90.90%
465s
262s
90.89%

MEMORY
55MB
1656MB
245MB
84MB
155MB
95MB
21,499MB
243MB
83MB
993MB
175MB

METHOD
PA-I
‘1-LLM
SVM
PA-I KERNEL
PA-I KE
PA-I SL∗N =8000
‘1-LLM
SVM
PA-I KERNEL
PA-I KE
PA-I SLN =125

d
1
2
2
2
2
2
3
3
3
3
3

ACC.
TIME
2s
91.75%
92.67%
136s
92.85% 12306s
1251s
92.91%
92.96%
27s
17s
92.88%
92.86%
779s
93.09% 17354s
1074s
93.14%
93.11%
103s
17s
93.05%

MEMORY
28MB
1683MB
139MB
54MB
143MB
77MB
14,089MB
140MB
49MB
751MB
131MB

Table 2: Training time for classiﬁers used in de-
pendency parsing task.

Table 3: Training time for classiﬁers used in
hyponymy-relation extraction task.

Figure 1: Training time for PA-I variants as a func-
tion of the number of expanded primitive features
in dependency parsing task (d = 3).

Figure 2: Training time for PA-I variants as a func-
tion of the number of expanded primitive features
in hyponymy-relation extraction task (d = 3).

rior accuracy of PA-I (d = 1) conﬁrmed the ne-
cessity of conjunctive features in these tasks. The
minor difference among the model accuracy of the
three PA-I variants was due to rounding errors.

PA-I SL was the fastest of the training meth-
ods with the same feature set, and its space efﬁ-
ciency was comparable to the kernel-based learn-
ers. PA-I SL could reduce the memory footprint
from 993MB8 to 175MB for d = 3 in the depen-
dency parsing task, while speeding up training.

Although linear training (‘1-LLM and PA-I KE)
dramatically slowed down when we took higher-
order conjunctive features into account, kernel
slicing alleviated deterioration in speed. Espe-
cially in the hyponymy-relation extraction task,
PA-I SL took almost the same time regardless of
the order of conjunctive features.

8‘1-LLM took much more memory than PA-I KE mainly
because ‘1-LLM expands conjunctive features in the exam-
ples prior to training, while PA-I KE expands conjunctive fea-
tures in each example on the ﬂy during training. Interested
readers may refer to (Chang et al., 2010) for this issue.

Figures 1 and 2 plot the trade-off between the
number of expanded primitive features and train-
ing time with PA-I variants (d = 3) in the two
tasks. Here, PA-I SP is PA-I with kernel slicing
without the techniques described in Sections 3.2.1
and 3.2.2, viz., kernel splitting. The early termi-
nation of margin computation reduces the train-
ing time when N is large. The reuse of temporal
margins makes the training time stable regardless
of parameter N. This suggests a simple, effec-
tive strategy for calibrating N; we start the train-
ing with N = |F|, and when the learner reaches
the allowed memory size, we shrink N to N/2
by pruning sub-trees rooted by rarer features with
RANK(f ) > N/2 in the weight trie.

Figures 3 and 4 plot training time with PA-I
variants (d = 3) for the two tasks as a function
of the training data size. PA-I SP inherited the de-
merit of PA-I KERNEL which takes quadratic time
in the number of examples, while PA-I SL took al-
most linear time in the number of examples.

1500

1200

900

600

300

]
s
[

e
m

i
t
g
n
i
n
i
a
r
T

0
102

PA-I SP
PA-I SL∗
PA-I SL

103

104

105

N: # of expanded primitive features

150

120

90

60

30

]
s
[

e
m

i
t
g
n
i
n
i
a
r
T

0
102

PA-I SP
PA-I SL∗
PA-I SL

103
105
N: # of expanded primitive features

104

106

1252

Figure 3: Training time for PA-I variants as a func-
tion of the number of training examples in depen-
dency parsing task (d = 3).

Figure 4: Training time for PA-I variants as a
function of the number of training examples in
hyponymy-relation extraction task (d = 3).

5 Related Work

There are several methods that learn ‘simpler’
models with fewer variables (features or support
vectors), to ensure scalability in training.

Researchers have employed feature selection
to assure space-efﬁciency in linear training. Wu
et al. (2007) used frequent-pattern mining to se-
lect effective conjunctive features prior to train-
ing. Okanohara and Tsujii (2009) revised graft-
ing for ‘1-LLM (Perkins et al., 2003) to prune use-
less conjunctive features during training. Iwakura
and Okamoto (2008) proposed a boosting-based
method that repeats the learning of rules repre-
sented by feature conjunctions. These methods,
however, require us to tune the hyperparameter to
trade model accuracy and the number of conjunc-
tive features (memory footprint and training time);
note that an accurate model may need many con-
junctive features (in the hyponymy-relation ex-
traction task, ‘1-LLM needed 15,828,122 features
to obtain the best accuracy, 92.86%). Our method,
on the other hand, takes all conjunctive features
into consideration regardless of parameter N.

Dekel et al. (2008) and Cavallanti et al. (2007)
improved the scalability of the (kernel) percep-
tron, by exploiting redundancy in the training data
to bound the size of the support set to given thresh-
old B (≥ |St|). However, Orabona et al. (2009)
reported that the models trained with these meth-
ods were just as accurate as a naive method that
ceases training when |St| reaches the same thresh-
old, B. They then proposed budget online learn-
ing based on PA-I, and it reduced the size of the
support set to a tenth with a tolerable loss of accu-

racy. Their method, however, requires O(|St−1|2)
time in updating the parameters in round t, which
disables efﬁcient training. We have proposed an
orthogonal approach that exploits the data redun-
dancy in evaluating the kernel to train the same
model as the base learner.

6 Conclusion
In this paper, we proposed online learning with
kernel slicing, aiming at resolving the space-time
trade-off in training a classiﬁer with many con-
junctive features. The kernel slicing generalizes
kernel splitting (Goldberg and Elhadad, 2008) in
a purely feature-wise manner, to truly combine the
merits of linear and kernel-based training. To im-
prove the scalability of the training with redundant
data in NLP, we reuse the temporal partial margins
computed in past rounds and terminate unneces-
sary margin computations. Experiments on de-
pendency parsing and hyponymy-relation extrac-
tion demonstrated that our method could train a
classiﬁer orders of magnitude faster than kernel-
based learners, while retaining its space efﬁciency.
We will evaluate our method with ample la-
beled data obtained by the semi-supervised meth-
ods. The implementation of the proposed algo-
rithm for kernel-based online learners is available
from http://www.tkl.iis.u-tokyo.ac.jp/˜ynaga/.

Acknowledgment We thank Susumu Yata for
providing us practical lessons on the double-array
trie, and thank Yoshimasa Tsuruoka for making
his ‘1-LLM code available to us. We are also in-
debted to Nobuhiro Kaji and the anonymous re-
viewers for their valuable comments.

600

500

400

300

200

100

]
s
[

e
m

i
t
g
n
i
n
i
a
r
T

0

0

PA-I KERNEL
PA-I SPN =250
PA-I SLN =250

50000

150000

100000
200000
|T |: # of training examples

250000

300000

200

150

100

50

]
s
[

e
m

i
t
g
n
i
n
i
a
r
T

0

0

PA-I KERNEL
PA-I SPN =125
PA-I SLN =125

50000

100000

150000

200000

|T |: # of training examples

1253

References
Ando, Rie Kubota and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research, 6:1817–1853.

Aoe, Jun’ichi. 1989. An efﬁcient digital search al-
gorithm by using a double-array structure.
IEEE
Transactions on Software Engineering, 15(9):1066–
1077.

Bellare, Kedar, Partha Pratim Talukdar, Giridhar Ku-
maran, Fernando Pereira, Mark Liberman, Andrew
McCallum, and Mark Dredze.
2007. Lightly-
supervised attribute extraction. In Proc. NIPS 2007
Workshop on Machine Learning for Web Search.

Cavallanti, Giovanni, Nicol`o Cesa-Bianchi, and Clau-
dio Gentile. 2007. Tracking the best hyperplane
with a simple budget perceptron. Machine Learn-
ing, 69(2-3):143–167.

Chang, Yin-Wen, Cho-Jui Hsieh, Kai-Wei Chang,
Michael Ringgaard, and Chih-Jen Lin. 2010. Train-
ing and testing low-degree polynomial data map-
pings via linear SVM. Journal of Machine Learning
Research, 11:1471–1490.

Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.

Daum´e III, Hal. 2006. Practical Structured Learn-
ing Techniques for Natural Language Processing.
Ph.D. thesis, University of Southern California.

Daum´e III, Hal.

2008.
constrained self training.
pages 680–688.

Cross-task knowledge-
In Proc. EMNLP 2008,

Dekel, Ofer, Shai Shalev-Shwartz, and Yoram Singer.
2008.
The forgetron: A kernel-based percep-
tron on a budget. SIAM Journal on Computing,
37(5):1342–1372.

Freund, Yoav and Robert E. Schapire. 1999. Large
margin classiﬁcation using the perceptron algo-
rithm. Machine Learning, 37(3):277–296.

Goldberg, Yoav and Michael Elhadad.

2008.
splitSVM: fast, space-efﬁcient, non-heuristic, poly-
nomial kernel computation for NLP applications. In
Proc. ACL-08: HLT, Short Papers, pages 237–240.

Isozaki, Hideki and Hideto Kazawa. 2002. Efﬁcient
support vector classiﬁers for named entity recogni-
tion. In Proc. COLING 2002, pages 1–7.

Iwakura, Tomoya and Seishi Okamoto. 2008. A fast
boosting-based learner for feature-rich tagging and
chunking. In Proc. CoNLL 2008, pages 17–24.

Kudo, Taku and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In Proc. ACL 2003,
pages 24–31.

Liang, Percy, Hal Daum´e III, and Dan Klein. 2008.
trading structure for fea-

Structure compilation:
tures. In Proc. ICML 2008, pages 592–599.

Okanohara, Daisuke and Jun’ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative
samples. In Proc. ACL 2007, pages 73–80.

Okanohara, Daisuke and Jun’ichi Tsujii. 2009. Learn-
ing combination features with L1 regularization. In
Proc. NAACL HLT 2009, Short Papers, pages 97–
100.

Orabona, Francesco, Joseph Keshet, and Barbara Ca-
puto. 2009. Bounded kernel-based online learning.
Journal of Machine Learning Research, 10:2643–
2666.

Perkins, Simon, Kevin Lacker, and James Theiler.
2003. Grafting: fast, incremental feature selection
by gradient descent in function space. Journal of
Machine Learning Research, 3:1333–1356.

Sassano, Manabu.

analysis for Japanese.
pages 8–14.

2004. Linear-time dependency
In Proc. COLING 2004,

Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in Wikipedia.
In Proc. LREC 2008, pages
2462–2469.

Tsuruoka, Yoshimasa, Jun’ichi Tsujii, and Sophia
Ananiadou.
Stochastic gradient descent
training for L1-regularized log-linear models with
cumulative penalty.
In Proc. ACL-IJCNLP 2009,
pages 477–485.

2009.

Williams, Hugh E. and Justin Zobel. 1999. Compress-
ing integers for fast ﬁle access. The Computer Jour-
nal, 42(3):193–201.

Wu, Yu-Chieh, Jie-Chi Yang, and Yue-Shi Lee. 2007.
An approximate approach for training polynomial
kernel SVMs in linear time. In Proc. ACL 2007, In-
teractive Poster and Demonstration Sessions, pages
65–68.

Yata, Susumu, Masahiro Tamura, Kazuhiro Morita,
Masao Fuketa, and Jun’ichi Aoe. 2009. Sequential
insertions and performance evaluations for double-
arrays.
In Proc. the 71st National Convention of
IPSJ, pages 1263–1264. (In Japanese).

Yoshinaga, Naoki and Masaru Kitsuregawa.

2009.
Polynomial to linear: efﬁcient classiﬁcation with
conjunctive features. In Proc. EMNLP 2009, pages
1542–1551.

