










































An Empirical Evaluation of Stop Word Removal in Statistical Machine Translation


Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 30–37,
Avignon, France, April 23 - 27 2012. c©2012 Association for Computational Linguistics

An Empirical Evaluation of Stop Word Removal                                  

in Statistical Machine Translation 

Chong Tze Yuang 
School of Computer Engi-

neering, Nanyang Technolo-

gical University, 639798 

Singapore 

tychong@ntu.edu.sg 

Rafael E. Banchs 
Institute for Infocomm Re-

search, A*STAR, 138632, 

Singapore 

rembanchs@i2r.a-

star.edu.sg 

Chng Eng Siong 
School of Computer Engi-

neering Nanyang Technolo-

gical University, 639798 

Singapore. 

aseschng@ntu.edu.sg 

 

 

 

 

 

Abstract 

In this paper we evaluate the possibility of 

improving the performance of a statistical 

machine translation system by relaxing the 

complexity of the translation task by remov-

ing the most frequent and predictable terms 

from the target language vocabulary. After-

wards, the removed terms are inserted back 

in the relaxed output by using an n-gram 

based word predictor. Empirically, we have 

found that when these words are omitted 

from the text, the perplexity of the text de-

creases, which may imply the reduction of 

confusion in the text. We conducted some 

machine translation experiments to see if 

this perplexity reduction produced a better 

translation output. While the word predic-

tion results exhibits 77% accuracy in pre-

dicting 40% of the most frequent words in 

the text, the perplexity reduction did not 

help to produce better translations. 

1 Introduction 

It is a characteristic of natural language that a 

large proportion of running words in a corpus 

corresponds to a very small fraction of the voca-

bulary. An analysis of the Brown Corpus has 

shown that the hundred most frequent words 

account for 42% of the corpus, while only 0.1% 

in the vocabulary. On the other hand, words oc-

curring only once account merely 5.7% in the 

corpus but 58% in the vocabulary (Bell et al. 

1990). This phenomenon can be explained in 

terms of Zipf’s Law, which states that the prod-

uct of word ranks and their frequencies approx-

imates a constant, i.e. word-frequency plot is 

close to a hyperbolic function, and hence the few 

top ranked words would account for a great por-

tion of the corpus. Also, it appears that the top 

ranked words are mainly function words. For 

instance, the eight most frequent words in the 

Brown Corpus are the, of, and, to, a, in, that and 

is (Bell et al. 1990). 

It is a common practice in Information Re-

trieval (IR) to filter the most frequent words out 

from processed documents (which are referred to 

as stop words), as these function words are se-

mantically non-informative and constitute weak 

indexing terms. By removing this great amount 

of stop words, not only space and time complexi-

ties can be reduced, but document content can be 

better discriminated by the remaining content 

words (Fox, 1989; Rijsbergen, 1979; Zou et al., 

2006; Dolamic & Savoy 2009). 

Inspired by the concept of stop word removal 

in Information Retrieval, in this work we study 

the feasibility of stop word removal in Statistical 

Machine Translation (SMT). Different from In-

formation Retrieval, that ranks or classifies doc-

uments; SMT hypothesizes sentences in target 

language. Therefore, without explicitly removing 

frequent words from the documents, we proposed 

to ignore such words in the target language vo-

cabulary, i.e. by replacing those words with a 

null token. We term this process as “relaxation” 

and the omitted words as “relaxed words”.  

Relaxed SMT here refers to a translation task 

in which target vocabulary words are intentional-

ly omitted from the training dataset for reducing 

translation complexity. Since the most frequent 

words are targeted to be relaxed, as a result, there 

will be vast amount of null tokens in the output 

text, which later shall be recovered in a post 

processing stage. The idea of relaxation in SMT 

is motivated by one of our experimental findings, 

in which the perplexity measured over a test set 

decreases when most frequent words are relaxed. 

For instance, a 15% of perplexity reduction is 

observed when the twenty most frequent words 

are relaxed in the English EPPS dataset. The 

reduction of perplexity allows us to conjecture 

30



about the decrease of confusion in the text, from 

which a SMT system might be benefited. 

After applying relaxed SMT, the resulting 

null tokens in the translated sentences have to be 

replaced by the corresponding words from the set 

of relaxed words. As relaxed words are chosen 

from the top ranked words, which possess high 

occurrences in the corpus, their n-gram probabili-

ties could be reliably trained to serve for word 

prediction. Also, these words are mainly function 

words and, from the human perspective, function 

words are usually much easier to predict from 

their neighbor context than content words. Con-

sider for instance the sentence the house of the 

president is very nice. Function words like the, 

of, and is, are certainly easier to be predicted than 

content words such as house, president, and nice. 

The rest of the paper is organized into four 

sections. In section 2, we discuss the relaxation 

strategy implemented for a SMT system, which 

generates translation outputs that contain null to-

kens. In section 3, we present the word predic-

tion mechanism used to recover the null tokens 

occurring in the relaxed translation outputs. In 

section 4, we present and discuss the experimen-

tal results. Finally, in section 5 we present the 

most relevant conclusion of this work. 

2 Relaxation for Machine Translation 

In this paper, relaxation refers to the replacement 

of the most frequent words in text by a null to-

ken. In the practice, a set of frequent words is 

defined and the cardinality of such set is referred 

to as the relaxation order. For example, lets the 

relaxation order be two and the two words on the 

top rank are the and is. By relaxing the sample 

sentence previously presented in the introduc-

tion, the following relaxed sentence will be ob-

tained: NULL house of NULL President NULL 

very beautiful. 

From some of our preliminary experimental 

results with the EPPS dataset, we did observe 

that a relaxation order of twenty leaded to a per-

plexity reduction of about a 15%. To see whether 

this contributes to improving the translation per-

formance, we trained a translation system by 

relaxing the top ranked words in the vocabulary 

of the target language. In this way, there will be a 

large number of words in the source language 

that will be translated to a null token. For exam-

ple: la (the in Spanish) and es (is in Spanish) will 

be both translated to a null token in English. 

This relaxation of terms is only applied to the 

target language vocabulary, and it is conducted 

after the word alignment process but before the 

extraction of translation units and the computa-

tion of model probabilities. The main objective 

of this relaxation procedure is twofold: on the 

one hand, it attempts to reduce the complexity of 

the translation task by reducing the size of the 

target vocabulary while affecting a large propor-

tion of the running words in the text; on the other 

hand, it should also help to reduce model sparse-

ness and improve model probability estimates. 

Of course, different from the Information Re-

trieval case, in which stop words are not used at 

all along the search process, in the considered 

machine translation scenario, the removed words 

need to be recovered after decoding in order to 

produce an acceptable translation. The relaxed 

word replacement procedure, which is based on 

an n-gram based predictor, is implemented as a 

post-processing step and applied to the relaxed 

machine translation output in order to produce 

the final translation result.  

Our bet here is that the gain in the translation 

step, which is derived from the relaxation strate-

gy, should be enough to compensate the error 

rate of the word prediction step, producing, in 

overall, a significant improvement in translation 

quality with respect to the non-relaxed baseline 

procedure. 

The next section describes the implemented 

word prediction model in detail. It constitutes a 

fundamental element of our proposed relaxed 

SMT approach. 

3 Frequent Word Prediction 

Word prediction has been widely studied and 

used in several different tasks such as, for exam-

ple, augmented and alternative communication 

(Wandmacher and Antoine, 2007) and spelling 

correction (Thiele et al., 2000). In addition to the 

commonly used word n-gram, various language 

modeling techniques have been applied, such as 

the semantic model (Luís and Rosa, 2002; Wand-

macher and Antoine, 2007) and the class-based 

model (Thiele et al., 2000; Zohar and Roth, 

2000; Ruch et al., 2001). 

The role of such a word predictor in our con-

sidered problem is to recover the null tokens in 

the translation output by replacing them with the 

words that best fit the sentence. This task is es-

sentially a classification problem, in which the 

most suitable relaxed word for recovering a giv-

en null token must be selected. In other words, 

�� � max��	
 �sentence�…����������…�, where 

�sentence�·�  is the probabilistic model, e.g. n-

31



gram, that estimates the likelihood of a sentence 

when a null token is recovered with word �� , 

drawn from the set of relaxed words �. The car-

dinality |�| is referred to as the relaxation order, 

e.g. |�| � 5 implies that the five most frequent 
words have been relaxed and are candidates to be 

recovered. 

Notice that the word prediction problem in 

this task is quite different from other works in the 

literature. This is basically because the relaxed 

words to be predicted in this case are mainly 

function words. Firstly, it may not be effective to 

predict a function word semantically. For exam-

ple, we are more certain in predicting equity than 

for given the occurrence of share in the sentence. 

Secondly, although class-based modeling is com-

monly used for prediction, its original intention 

is to tackle the sparseness problem, whereas our 

task focuses only on the most frequent words. 

In this preliminary work, our predicting me-

chanism is based on an n-gram model. It predicts 

the word that yields the maximum a posteriori 

probability, conditioned on its predecessors. For 

the case of the trigram model, it can be expressed 

as follows: 

 

�� � max��	
 ����|���������  (1) 

 

Often, there are cases in which more than one 

null token occur consecutively. In such cases 

predicting a null token is conditioned on the pre-

vious recovered null tokens. To prevent a predic-

tion error from being propagated, one possibility 

is to consider the marginal probability (summed 

over the relaxed word set) over the words that 

were previously null tokens. For example, if ���� 
is a relaxed word, which has been recovered 

from earlier predictions, then the prediction of �� 

should no longer be conditioned by ���� . This 
can be computed as follows: 

 

�� � max��	
� ����|�������������	
 �

max
��	


∑ ����|�������������	
   (2) 

 

The traditional n-gram model, as discussed 

previously, can be termed as the forward n-gram 

model as it predicts the word ahead. Additional-

ly, we also tested the backward n-gram to predict 

the word behind (i.e. on the left hand side of the 

target word), which can be formulated as: 

  

�� � max��	
 ����|���������  (3) 

 

and the bidirectional n-gram to predict the word 

in middle, which can be formulated as follows: 

�� � max��	
 ����|����, ����� (4) 

 

Notice that the backward n-gram model can 

be estimated from the word counts as: 

 

����|��������� �
������ ��� !�

���� ��� !�
 (5) 

 

or, it can be also approximated from the forward 

n-gram model, as follows: 

 

����|��������� �
"��� !|���� ��"��� �|���"����

"��� !|�� ��"��� ��
 

     (6) 

 

Similarly, the bidirectional n-gram model can 

be estimated from the word counts: 

 

����|��������� �
"��� �|�������"���|�����"������

∑ "��� �|�������"���|�����"������#�	$
 (7) 

 

or approximated from the forward model: 

 

����|��������� �
"��� �|�������"���|�����"������

∑ "��� �|�������"���|�����"������#�	$
 (8) 

 

The word prediction results of using the for-

ward, backward, and bidirectional n-gram mod-

els will be presented and discussed in the ex-

perimental section.  

The three n-gram models discussed so far 

predict words based on the local word ordering. 

There are two main drawbacks to this: first, only 

the neighboring words can be used for prediction 

as building higher order n-gram models is costly; 

and, second, prediction may easily fail when con-

secutive null tokens occur, especially when all 

words conditioning the prediction probability are 

recovered null tokens. Hence, instead of predict-

ing words by maximizing the local probability, 

predicting words by maximizing a global score 

(i.e. a sentence probability in this case), may be a 

better alternative.  

At the sentence level, the word predictor con-

siders all possible relaxed word permutations and 

searchs for the one that yields the maximum a 

posteriori sentence probability. For the trigram 

model, a relaxed word that maximizes the sen-

tence probability can be predicted as follows: 

 

�� � max��	
∏ ����|���������
&
�'�  (9) 

32



where, ( is the number of words in the sentence. 
Although the forward, backward, and interpo-

lated models have been shown to be applicable 

for local word prediction, they make no differ-

ence at sentence level predictions as they pro-

duce identical sentence probabilities. It is not 

hard to prove the following identity: 

 

∏ ����|���������
&
�'� � ∏ ����|���������

&
�'�

 (10) 

4 Experimental Results and Discussion 

In this section, we first highlight the Zipfian dis-

tribution in the corpus and the reduction of per-

plexity after removing the top ranked words. The 

n-gram probabilities estimated were then used 

for word prediction, and we report the resulting 

prediction accuracy at different relaxation orders. 

The performance of the SMT system with a re-

laxed vocabulary is presented and discussed in 

the last subsection of this section. 

4.1 Corpus Analysis 

The data used in our experiments is taken from 

the EPPS (Europarl Corpus). We used the ver-

sion available through the shared task of the 

2007's ACL Workshops on Statistical Machine 

Translation (Burch et al., 2007). The training set 

comprises 1.2M sentences with 35M words while 

the development set and test sets contains 2K 

sentences each. 

From the train set, we computed the twenty 

most frequent words and ranked them according-

ly. We found them to be mainly function words. 

Their counts follow closely a Zipfian distribution 

(Figure 1) and account for a vast proportion of 

the text (Figure 2). Indeed, the 40% of the run-

ning words is made up by these twenty words. 

 

 
 

Figure 1. The twenty top ranked words and their 

relative frequencies 

 

 
 

Figure 2. Cumulative relative frequencies of the 

top ranked words (up to order 20) 

 

We found that when the most frequent words 

were relaxed from the vocabulary, which means 

being replaced by a null token, the perplexity 

(measured with a trigram model) decreased up to 

15% for the case of a relaxation order of twenty 

(Figure 3). This implies that the relaxation causes 

the text becoming less confusing, which might 

benefit natural language processing tasks such as 

machine translation. 

 

 
 

Figure 3. Perplexity decreases with the relaxation 

of the most frequent words 

 

4.2 Word Prediction Accuracy 

In order to evaluate the quality of the different 

prediction strategies, we carried out some expe-

riments for replacing null tokens with relaxed 

words. For this, frequent words were dropped 

manually from text (i.e. replaced with null to-

kens) and were recovered later by using the word 

predictor. As discussed earlier, a word can be 

predicted locally, to yield maximum n-gram 

probability, or globally, to yield maximum sen-

0

1

2

3

4

5

6

7

8

th
e

co
m

m
a

p
e

ri
o

d o
f

to

a
n

d in

th
a

t a is

w
e

fo
r i

th
is o
n it

b
e

a
re a
s

h
a

v
e

R
e

la
ti

v
e

 f
re

q
u

e
n

cy
 (

%
)

Top Ranked Words

TEST

DEV

5

10

15

20

25

30

35

40

0 5 10 15 20

C
u

m
u

la
ti

v
e

 f
re

q
u

e
n

cy
(%

)

Word Rank

TEST

DEV

45

47

49

51

53

55

57

59

61

63

65

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20

P
e

rp
le

x
it

y

Relaxation Order

DEV

TEST

33



tence probability. In a real application, a text 

may comprise up to 40% of null tokens that must 

be recovered from the twenty top ranked words. 

For n-gram level prediction (local), we eva-

luated word accuracy at different orders of relax-

ation. More specifically, we tested the forward 

trigram model, the backward trigram model, the 

bidirectional model, and the linear interpolation 

between forward and backward trigram models 

(with weight 0.5). The accuracy was computed as 

the percentage of null tokens recovered success-

fully with respect to the original words. These 

results are shown in Figure 4. Notice that the 

accuracy of the relaxation of order one is 100%, 

so it has not been depicted in the plots. 

Notice from the figure how the forward and 

backward models performed very alike through-

out the different relaxation orders. This can be 

explained in terms of their similar perplexities 

(both models exhibit a perplexity of 58). Better 

accuracy was obtained by the interpolated model, 

which demonstrates the advantage of incorporat-

ing the left and right contexts in the prediction. 

Different from the interpolated model, which 

simply adds probabilities from the two models, 

the bidirectional model estimates the probability 

from the left and right contexts simultaneously 

during the training phase; thus it produces a bet-

ter result. However, due to its heavy computa-

tional cost (Equation 8), it is infeasible to apply it 

at orders higher than five. 

 

 
 

Figure 4. Accuracy of word prediction at n-gram 

level. Models incorporating left and right context 

yield about a 20% improvement over one-sided 

models. 

 

Better accuracy has been obtained for sentence-

level prediction by using a bigram model and a 

trigram model. These results are shown in Figure 

5. From the cumulative frequency showed in 

Figure 2, we could see that 40% of the words in 

text could now be predicted with an accuracy of 

about 77%. 

 

 
 

Figure 5. Accuracy of word prediction at the 

sentence level. 

 

For predicting words by maximizing the sen-

tence probability, two methods have been tried: 

first, a brute force method that attempts all possi-

ble relaxed word permutations for the occurring 

null tokens within a sentence and finds the one 

producing the largest probability estimate; and, 

second, applying Viterbi decoding over a word 

lattice, which is built from the sentence by re-

placing the arcs of null tokens with a parallel 

network of all relaxed words.  

All the arcs in the word lattice have to be 

weighted with n-gram probabilities in order to 

search for the best route. In the case of the tri-

gram model, we expand the lattice with the aid of 

the SRILM toolkit (Stolcke 2002). Both methods 

yield almost identical prediction accuracy. How-

ever, we discarded the brute force approach for 

the later experiments because of its heavy com-

putational burden. 

Figure 4 and 5 have been plotted with the 

same scale on the vertical axis for easing their 

visual comparison. The global prediction strate-

gy, which optimizes the overall sentence perplex-

ity, is much more useful for prediction as com-

pared to the local predictions. Furthermore, as 

seen from Figure 5, the global prediction has 

better resistance against higher order relaxations. 

We also observed that the local bidirectional 

model performed closely to the global bigram 

model, up to relaxation order five, for which the 

40

50

60

70

80

90

100

0 5 10 15 20

A
cc

u
ra

cy

Relaxation Order

Trigram(Forward)

Trigram(Backward)

Interpolation

Bidirectional

40

50

60

70

80

90

100

0 5 10 15 20

A
cu

u
ra

cy
 (

%
)

Relaxation Order

Bigram

Trigram

34



computation of the bidirectional model is still 

feasible. In Figure 6 we present a scaled version 

of the plots to focus on the lower orders for com-

parison purposes. Although the global bigram 

prediction makes use of all words in the sentence 

in order to yield the prediction, locally, a given 

word is only covered by two consecutive bi-

grams. Thus, the prediction of a word does not 

depend on the second word before or the second 

word after. In other words, we could see the bidi-

rectional model as a global model that is applied 

to a “short segment” (in this case, a three word 

segment). The only difference here is that the 

local bidirectional model estimates the probabili-

ties from the corpus and keeps all seen “short 

segment” probabilities in the language model (in 

our case, it is derived from forward bigram), 

while the global bigram model optimizes the 

probabilities by searching for the best two con-

secutive bigrams. The global prediction might 

only show its advantage when predicting two or 

more consecutive null tokens. 
 

 
 

Figure 6. Comparison among local bidirectional, 

local interpolation, and global bigram models. 
 

Hence, we believe that, if the bidirectional bi-

gram model is computed from counts and stored, 

it could perform as good as global bigram model 

at much faster speed (as it involves only query-

ing the language model). Similarly, a local bidi-

rectional trigram model (actually a 5-gram) may 

be comparable to a global trigram model. 

Deriving bidirectional n-gram probabilities 

from a forward model is computationally expen-

sive. In the worst case scenario, where both 

companion words are relaxed words, the compu-

tation complexity is in the order of )�|*||�|+�, 

where |*| is the vocabulary size and |�| is the 

number of relaxed words in *. Building a bidi-

rectional bigram/trigram model from scratch is 

worth to be considered. As all known language 

model toolkits do not offer this function (even 

the backward n-gram model is built by first re-

versing the sentences manually), the discount-

ing/smoothing of the trigram has to be derived. 

The methods of Good-Turing, Kneser Ney, Ab-

solute discounting, etc. (Chen and Goodman, 

1998) can be imitated. 

4.3 Translation Performance 

As frequent words have been ignored in the tar-

get language vocabulary of the machine transla-

tion system, the translation outputs will contain a 

great number of null tokens. The amount of null 

tokens should approximate the cumulative fre-

quencies shown in Figure 2. 

In this experiment, a word predictor was used 

to recover all null tokens in the translation out-

puts, and the recovered translations were eva-

luated with the BLEU metric. All BLEU scores 

have been computed up to trigram precision. 

The word predictor used was the global tri-

gram model, which was the best performing sys-

tem in word prediction experiments previously 

described. In this case, the predictor was used to 

recover the null tokens in the translation outputs. 

In order to apply the prediction mechanism as a 

post-processing step, a word lattice was built 

from each translation output, for which the null 

word arcs were expanded with the words of the 

relaxed set. Finally, the lattice was decoded to 

produce the best word list as the complete trans-

lation output. 

To evaluate whether a SMT system benefits 

from the relaxation strategy, we set up a baseline 

system in which the relaxed words in the transla-

tion output were replaced manually with null 

tokens. After that, we used the same word pre-

dictor as in the relaxed SMT case (global trigram 

predictor) for recovering the null tokens and 

regenerating the complete sentences. We then 

compared the translation output of the relaxed 

SMT system to the baseline system. 

The results for the baseline (BL) and the re-

laxed (RX) systems are shown in Figure 7. We 

evaluated the translation performance for relaxa-

tion orders of five and twenty.   

From the results shown in Figure 7, it be-

comes evident that the translation task is not 

gaining any advantage from relaxation strategy 

and did not outperform the baseline translator, 

neither at low nor at high orders of relaxation. 

87

89

91

93

95

97

99

2 3 4 5

A
cc

u
ra

cy
 (

%
)

Relaxation Order

Bidirectional 

(local)

Interpolation 

(local)

Bigram (global)

35



Notice how the BLEU score of the baseline sys-

tems are better than those of the relaxed systems. 

 

 
 

Figure 7. BLEU scores for baseline (BL) and 

relaxed (RX) translation systems at relaxation 

orders of five and twenty. 

 

We further analyzed these results by compu-

ting BLEU scores for the translation outputs 

before and after the word prediction step. These 

results are shown in Figure 8. Notice from Figure 

8 that the relaxed translators did not produce any 

better BLEU score than the corresponding base-

line systems, even before word recovery. Al-

though the text after relaxation is less confusing 

(perplexity decreases about 15% after the twenty 

most frequent words are relaxed), the resulting 

perplexity drop was not translated into a BLEU 

score improvement. 

 

 
 

Figure 8. The BLEU scores before and after 

word prediction 

 

In terms of the word predictions shown in 

Figure 8, we can see that this post-processing 

step performed consistently for the relaxed SMT 

systems, as for the baseline systems (for which 

the null tokens were inserted manually into sen-

tences). Since the word prediction is based on an 

n-gram model, we may deduce that the relaxed 

SMT system preserves the syntactic structure of 

the sentence as the null tokens in the translation 

output could be recovered as accurate as in the 

case of the baseline system. 

5 Conclusion and Future Work 

We have looked into the problem of predicting 

the most frequently occurring words in a text. 

The best of the studied word predictors, which is 

based on an n-gram language model and a global 

search at the sentence level, has achieved 77% of 

accuracy when predicting 40% words in the text. 

We also proposed the idea of relaxed SMT, 

which consists of replacing top ranked words in 

the target language vocabulary with a null token. 

This strategy was originally inspired by the con-

cept of stop word removal in Information Re-

trieval, and later motivated by the finding that 

text will become less confusing after relaxation. 

However, when relaxation is applied to the ma-

chine translation system, our results indicate that 

the relaxed translation task is performing poorer 

than the conventional non-relaxed system. In 

other words, the SMT system does not seem to 

be benefiting from the word relaxation strategy, 

at least in the case of the specific implementation 

studied here. 

As future work, we will attempt to re-tailor 

the set of relaxed words by, for instance, impos-

ing some constraints to also include some less 

frequent function words, which may not be infor-

mative to the translation system or, alternatively, 

excluding some frequent semantically important 

words from the relaxed set. This remark is based 

on the observation of the fifty most frequent 

words in the EPPS dataset, such as president, 

union, commission, European, and parliament, 

which could be harmful when ignored by the 

translation system but also easy to predict. Hence 

there is a need to study the effects of different 

sets of relaxed words on translation performance, 

as it have already been done for the search prob-

lem by researchers in the area of Information 

Retrieval (Fox, 1990; Ibrahim, 2006). 

Acknowledgements 

The authors would like to thank their correspond-

ing institutions: the Nanyang Technological Uni-

0.3

0.31

0.32

0.33

0.34

0.35

0.36

0.37

0.38

BL5 BL20 RX5 RX20

B
LE

U

Translator

DEV

TEST

0

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

BL5 BL20 RX5 RX20

B
LE

U

Translator

Before

After

36



versity and the Institute for Infocomm Research, 

for their support regarding the development and 

publishing of this work.   

References 
Andreas Stolcke, 2002, SRILM - An Extensible Lan-

guage Modeling Toolkit, in Proceedings of ICSLP, 

901-904. 

Chris Callison-Burch, Cameron Fordyce, Philipp 

Koehn, Christof Monz and Josh Schroeder, 2007, 

(Meta-)evaluation of machine translation, in Pro-

ceedings of the SMT Workshop, 136-158 

Christopher Fox, 1990, A stop list for general text, 

SIGIR Forum, 24:19-35. 

Cornelis Joost van Rijsbergen, 1979, Information 

Retrieval, Butterworth-Heinemann. 

Feng Zou, Fu Lee Wang, Xiaotie Deng and Song Han, 

2006, Automatic identification of Chinese stop 

words, Research on Comp. Science, 18:151-162. 

Frank Thiele, Bernhard Rueber and Dietrich Klakow, 

2000, Long range language models for free spel-

ling recognition, in Proceeding of the 2000 IEEE 

ICASSP, 3:1715-1718. 

Ibrahim Abu El-Khair, 2006, Effects of stop words 

elimination for Arabic information retrieval: a 

comparative study, International Journal of Com-

puting & Information Sciences, 4(3):119-133. 

João Luís and Garcia Rosa, 2002, Next word predic-

tion in a connectionist distributed representation 

system, in Proceedings of the 2002 IEEE Int. Con-

ference on Systems, Man and Cybernetics, 6-11. 

Keith Trnka, John McCaw, Debra Yarrington, Kath-

leen F. McCoy, User interaction with word predic-

tion: the effects of prediction quality, ACM 

Transaction on Accessible Computing, 1(17):1-34. 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Ljiljana Dolamic and Jacques Savoy., 2009, When 

stopword lists make the difference, Journal of the 

American Society for Information Science and 

Technology, 61(1):1-4. 

Patrick Ruch, Robert Baud and Antoine Geissbuhler, 

2001, Toward filling the gap between interactive 

and fully-automatic spelling correction using the 

linguistic context, in Proceedings of the 2001 IEEE 

International Conference on Systems, Man and 

Cybernetics, 199-204. 

Stanley F. Chen and Joshua Goodman, 1998, An 

empirical study of smoothing techniques for lan-

guage modeling, in Proceedings of the 34th Annual 

Meeting of the Association for Computational Lin-

guistics, 310-318. 

Timothy C. Bell, John G. Cleary and Ian H. Witten., 

1990, Text Compression, Prentice Hall. 

Tonio Wandmacher and Jean-Yves Antoine, 2007, 

Methods to integrate a language model with se-

mantic information for a word prediction compo-

nent, in Proceedings of the 2007 Joint Conference 

on Empirical Methods in Natural Language 

Processing and Computational Natural Language 

Learning, 506-513. 

Yair Even-Zohar and Dan Roth, 2000, A classifica-

tion approach to word prediction, in Proceedings of 

the 1st North American chapter of the Association 

for Computational Linguistics, 124-131. 

37


