










































HsH: Estimating Semantic Similarity of Words and Short Phrases with Frequency Normalized Distance Measures


Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 48–52, Atlanta, Georgia, June 14-15, 2013. c©2013 Association for Computational Linguistics

HsH: Estimating Semantic Similarity of Words and Short Phrases with
Frequency Normalized Distance Measures

Christian Wartena
Hochschule Hannover – University of Applied Sciences and Arts

Department of Information and Communication
Expo Plaza 12, 30539 Hannover, Germany

Christian.Wartena@hs-hannover.de

Abstract

This paper describes the approach of the
Hochschule Hannover to the SemEval 2013
Task Evaluating Phrasal Semantics. In or-
der to compare a single word with a two word
phrase we compute various distributional sim-
ilarities, among which a new similarity mea-
sure, based on Jensen-Shannon Divergence
with a correction for frequency effects. The
classification is done by a support vector ma-
chine that uses all similarities as features. The
approach turned out to be the most successful
one in the task.

1 Introduction

The task Evaluating Phrasal Semantics of the 2013
International Workshop on Semantic Evaluation
(Manandhar and Yuret, 2013) consists of two sub-
tasks. For the first subtask a list of pairs consisting
of a single word and a two word phrase are given.
For the English task a labeled list of 11,722 pairs
was provided for training and a test set with 3,906
unlabeled examples. For German the training set
contains 2,202 and the test set 732 pairs. The system
should be able to tell whether the two word phrase
is a definition of the single word or not. This task
is somewhat different from the usual perspective of
finding synonyms, since definitions are usually more
general than the words they define.

In distributional semantics words are represented
by context vectors and similarities of these con-
text vectors are assumed to reflect similarities of
the words they represent. We compute context vec-
tors for all words using the lemmatized version of

the Wacky Corpora for English (UKWaC, approxi-
mately 2,2 billion words) and German (DeWaC, 1,7
billion words) (Baroni et al., 2009). For the phrases
we compute the context vectors as well directly on
the base of occurrences of that phrase, as well as
by construction from the context vectors of the two
components. For the similarities between the vec-
tors we use Jensen-Shannon divergence (JSD) and
cosine similarity. Since the JSD is extremely depen-
dent on the number of occurrences of the words, we
define a new similarity measure that corrects for this
dependency. Since none of the measures gives satis-
factory results, we use all measures to train a support
vector machine that classifies the pairs.

The remainder of this paper is organized as fol-
lows. We start with an overview of related work. In
section 3 we discuss the dependence of JSD on word
frequency and introduce a new similarity measure.
Section 4 then describes the system. The results are
given in section 5 and are discussed in section 6.

2 Related Work

Though distributional similarity has widely been
studied and has become an established method to
find similar words, there is no consensus on the way
the context of a word has to be defined and on the
best way to compute the similarity between two con-
texts. In the most general definitions the context of
a word consists of a number of words and their re-
lation to the given word (Grefenstette, 1992; Curran
and Moens, 2002). In the following we will only
consider the simplest case in which there is only one
relation: the relation of being in the same sentence.
Each word can be represented by a so called con-

48



text vector in a high dimensional word space. Since
these vectors will be sparse, often dimensionality re-
duction techniques are applied. In the present pa-
per we use random indexing, introduced by Karlgren
and Sahlgren (2001) and Sahlgren (2005) to reduce
the size of the context vectors.

The way in which the context vectors are con-
structed also determines what similarity measures
are suited. For random indexing Görnerup and Karl-
gren (2010) found that best results are obtained us-
ing L1-norm or Jensen-Shannon divergence (JSD).
they also report that these measures highly correlate.
We could confirm this in a preliminary experiment
and therefore only use JSD in the following.

Recently, the question whether and how an ap-
propriate context vector for a phrase can be de-
rived from the context vectors of its components
has become a central issue in distributional seman-
tics (Clark and Pulman, 2007; Mitchell and Lap-
ata, 2008; Widdows, 2008; Clarke et al., 2008). It
is not yet clear which way of combining the vec-
tors of the components is best suited for what goals.
Giesbrecht (2010) and Mitchell and Lapata (2008)
e.g. find that for noun-noun compounds the prod-
uct of context vectors (corresponding to the intersec-
tion of contexts) and more complex tensor products
give best results, while Guevara (2011) obtains best
results for adjective-noun phrases with addition of
vectors (corresponding to union of contexts). Since
we do not (yet) have a single best similarity mea-
sure to distinguish definitions from non-definitions,
we use a combination of similarity measures to train
a model as e.g. also was done by Bär et al. (2012).

3 Frequency Dependency Correction of
Jensen-Shannon Divergence

Weeds et al. (2004) observed that in tasks in which
related words have to be found, some measures pre-
fer words with a frequency similar to that of the tar-
get word while others prefer high frequent words,
regardless of the frequency of the target word. Since
Görnerup and Karlgren (2010) found that L1-norm
and JSD give best results for similarity of random
index vectors, we are especially interested in JSD.
The JSD of two distributions p and q is given by

JSD(p, q) = 12D(p||
1
2p+

1
2q)+

1
2D(q||

1
2p+

1
2q) (1)

where D(p||q) = Σip(i) log p(i)log q(i) is the Kullback-
Leibler divergence. We will follow the usual termi-
nology of context vectors. However, we will always
normalize the vectors, such that they can be inter-
preted as probability mass distributions. According
to Weeds et al. (2004) the JSD belongs to the cat-
egory of distance measures that tends to give small
distances for highly frequent words. In Wartena et
al. (2010) we also made this observation and there-
fore we added an additional constraint on the selec-
tion of keywords that should avoid the selection of
too general words. In the present paper we try to ex-
plicitly model the dependency between the JSD and
the number of occurrences of the involved words.
We then use the difference between the JSD of the
co-occurrence vectors of two words and the JSD ex-
pected on the base of the frequency of these words
as a similarity measure. In the following we will use
the dependency between the JSD and the frequency
of the words directly. In (Wartena, 2013) we model
the JSD instead as a function of the number of non
zero values in the context vectors. The latter depen-
dency can be modeled by a simpler function, but did
not work as well with the SemEval data set.

Given two words w1 and w2 the JSD of their con-
text vectors can be modeled as a function of the min-
imum of the number of occurrences of w1 and w2.
Figure 3 shows the JSD of the context vectors of the
words of the training set and the context vector of
the definition phrase. In this figure the JSD of the
positive and the negative examples is marked with
different marks. The lower bound of the negative
examples is roughly marked by a (red) curve, that is
defined for context vectors c1 and c2 for words w1
and w2, respectively, by

JSDexp(c1, c2) = a +
1

n̂b + c
(2)

where n̂ = min(n(w1), n(w2)) with n(w) the num-
ber of occurrences of w in the corpus and with a,
b and c constants that are estimated for each set of
word pairs. For the pairs from the English training
and test set the values are: a = 0.15, b = 0.3 and
c = 0.5. Experiments on the training data showed
that the final results are not very dependent on the
exact values of these constants.

Finally, our new measure is simply defined by

JSDnorm(p, q) = JSD(p, q)− JSDexp(p, q). (3)

49



Figure 1: JSD (y-axis) of all pairs in the English training set versus the number of occurrences of the definition phrase
(x-axis) in the UkWaC-Corpus. The positives examples are marked by a +, the negative examples by a ×. Most
positive examples are hidden behind the negative ones. The solid (red) line gives the expected JSD.

4 System Description

The main assumption for our approach is, that a
word and its definition are distributionally more sim-
ilar than a word and an arbitrary definition. We use
random indexing to capture distributional properties
of words and phrases. Since similarity measures for
random index vectors have biases for frequent or in-
frequent pairs, we use a combination of different
measures. For the two-word definition phrases we
can either estimate the context vector on the base of
the two words that make up the phrase, or compute it
directly from occurrences of the whole phrase in the
corpus. The latter method has the advantage of being
independent of assumptions about semantic compo-
sition, but might have the problem that it is based
on a few examples only. Thus we use both distribu-
tions, and also include the similarities between the
single word and each of the words of the definition.

4.1 Distributions
Consider a pair (w, d) with w a word and d a defi-
nition consisting of two words: d = (d1, d2). Now
for each of the words w, d1, d2 and the multiword
d we compute context vectors using the random in-
dexing technique. The context vectors are computed
over the complete Wacky corpus. The context used
for a word are all open-class words (i.e. Noun, Verb,
Adjective, Adverb, etc. but not Auxiliary, Pronoun,
etc.) in a sentence. Each word is represented by a

random index vector of 10 000 dimensions in which
8 random positions have a non-zero value. The ran-
dom vectors of all words in all contexts are summed
up to construct context vectors (with length 10 000),
denoted vw, vd, vd1 , vd2 . In many cases there are
only very few occurrences of d, making the context
vector vd very unreliable. Thus we also compute the
vectors vaddd = vd1 + vd2 and v

mult
d = vd1 · vd2 . Fi-

nally, we also compute the general context vector (or
background distribution) vgen which is the context
vector obtained by aggregating all used contexts.

4.2 Similarities
Table 1 gives an overview of the similarities com-
puted for the context vector vw. In addition we also
compute D(vw||vgen), D(vd||vgen), D(vd1 ||vgen),
D(vd2 ||vgen). The original intuition was that the def-
inition of a word is usual given as a more general
term or hypernym. It turned out that this is not the
case. However, in combination with other features
these divergences proved to be useful for the ma-
chine learning algorithm. Finally, we also use the
direct (first-order) co-occurrence between w and d
by computing the ratio between the probability with
which we expect w and d to co-occur in one sentence
if they would be independent, and the real probabil-
ity of co-occurrence found in the corpus:

co-occurrence-ratio(w, d) =
p(w, d)

p(w) · p(d)
(4)

50



Table 1: Similarity measures used to compute the simi-
larity of a context vector of some word to various context
vectors for a phrase d = (d1, d2).

vd vd1 vd2 v
add
d v

mult
d

jsd X X X X
jsd-norm X X X X
cossim X X

Table 2: Results for English and German (no names
dataset). Results on train sets are averaged results from
10-fold cross validation. Results on the test set are the
official task results.

AUC Accuracy F-Measure
Train English 0.88 0.80 0.79
Test English - 0.80 0.79
Train German 0.90 0.83 0.82
Test German - 0.83 0.81

where p(w, d) is the probability that w and d are
found in the same sentence, and p(w), with w a word
or phrase, the probability that a sentence contains w.

For the computation of JSDnorm(vw, vaddd ) we
need the number of occurrences on which vaddd is
based. As an estimate for this number we use
max(n(d1), n(d2)). The constants a, b and c in
equation 2 are set to the following values: for
all cases a = 0.15; for JSDnorm(vw, vd) we let
b = 0.3 and c = 0.5; for JSDnorm(vw, vd1) and
JSDnorm(vw, vd2) we let b = 0.35 and c = −0.1; for
JSDnorm(vw, v

add
d ) we let b = 0.4 and c = −0.1. For

the German subtask a = 0.28 and slightly different
values for b and c were used to account for slightly
different frequency dependencies.

4.3 Combining Similarities
The 15 attributes for each pair obtained in this way
are used to train a support vector machine (SVM)
using LibSVM (Chang and Lin, 2011). Optimal pa-
rameters for the SVM were found by grid-search and
10-fold cross validation on the training data.

5 Results

In Table 2 the results are summarized. Since the
task can also be seen as a ranking task, we include
the Area Under the ROC-Curve (AUC) as a classi-
cal measure for ranking quality. We can observe that
the results are highly stable between training set and

Table 3: Results for English train set (average from 10-
fold cross validation) using one feature

feature Accuracy AUC
jsd(vw, vd) 0.50 0.57
jsdnorm(vw, vd) 0.59 0.70
jsd(vw, vd1) 0.54 0.63
jsdnorm(vw, vd1) 0.61 0.69
jsd(vw, vd2) 0.57 0.65
jsdnorm(vw, vd2) 0.63 0.71
jsd(vw, vaddd ) 0.59 0.67
jsdnorm(vw, vaddd ) 0.66 0.74
cossim(vw, vaddd ) 0.69 0.76
cossim(vw, vmultd ) 0.62 0.71
co-occ-ratio(w, d) 0.61 0.71

test set and across languages. Table 3 gives the re-
sults that are obtained on the training set using one
feature. We can observe that the normalized versions
of the JSD always perform better than the JSD itself.
Furthermore, we see that for the composed vectors
the cosine performs better than the normalized JSD,
while it performs worse than JSD for the other vec-
tors (not displayed in the table). This eventually can
be explained by the fact that we have to estimate the
number of contexts for the calculation of jsdexp.

6 Conclusion

Though there are a number of ad-hoc decisions in
the system the approach was very successful and
performed best in the SemEval task on phrasal se-
mantics. The main insight from the development
of the system is, that there is not yet a single best
similarity measure to compare random index vec-
tors. The normalized JSD turns out to be a useful
improvement of the JSD but is problematic for con-
structed context vectors, the formula in equation (2)
is rather ad hoc and the constants are just rough esti-
mates. The formulation in (Wartena, 2013) might be
a step in the right direction, but also there we are still
far away from a unbiased similarity measure with a
well founded theoretical basis.

Finally, it is unclear, what is the best way to rep-
resent a phrase in distributional similarity. Here we
use three different vectors in parallel. It would be
more elegant if we had a way to merge context vec-
tors based on direct observations of the phrase with
a constructed context vector.

51



References
Daniel Bär, Chris Biemann, Iryna Gurevych, and Torsten

Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval-2012), pages
435–440.

M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The wacky wide web: A collection of very large
linguistically processed web-crawled corpora. Lan-
guage Resources and Evaluation 43 (3): 209-226,
43(3):209–226.

C.-C. Chang and C.-J. Lin. 2011. Libsvm : a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(3):1–27.

Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of the AAAI Spring Symposium on Quan-
tum Interaction, pages 52–55.

Daoud Clarke, Rudi Lutz, and David Weir. 2008. Se-
mantic composition with quotient algebras. In Pro-
ceedings of the 9th International Conference on Com-
putational Semantics (IWCS 2011).

James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Unsuper-
vised Lexical Acquisition: Proceedings of the Work-
shop of the ACL Special Interest Group on the Lexi-
con (SIGLAX)., pages 59–66. Association of Compu-
tational Linguistics.

Eugenie Giesbrecht. 2010. Towards a matrix-based dis-
tributional model of meaning. In Proceedings of the
NAACL HLT 2010 Student Research Workshop, pages
23–28, Los Angeles, California. ACL.

Olaf Görnerup and Jussi Karlgren. 2010. Cross-lingual
comparison between distributionally determined word
similarity networks. In Proceedings of the 2010 Work-
shop on Graph-based Methods for Natural Language
Processing, pages 48–54. ACL.

Gregory Grefenstette. 1992. Use of syntactic context to
produce term association lists for text retrieval. In SI-
GIR ’92: Proceedings of the 15th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 89–97. ACM.

Emiliano Guevara. 2011. Computing semantic compo-
sitionality in distributional semantics. In Proceedings
of the 9th International Conference on Computational
Semantics (IWCS 2011), pages 135–144.

Jussi Karlgren and Magnus Sahlgren. 2001. From words
to understanding. In Foundations of Real-World Intel-
ligence, pages 294–308. CSLI Publications, Stanford,
California.

Suresh Manandhar and Deniz Yuret, editors. 2013. Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), in conjunction with

the Second Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2013), Atlanta.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL 2008, Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics, pages 236–244.

Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering,
TKE, volume 5.

Christian Wartena, Rogier Brussee, and Wouter
Slakhorst. 2010. Keyword extraction using word
co-occurrence. In Database and Expert Systems
Applications (DEXA), 2010 Workshop on, pages
54–58. IEEE.

Christian Wartena. 2013. Distributional similarity of
words with different frequencies. In Proceedings of
the Dutch-Belgian Information Retrieval Workshop,
Delft. To Appear.

Julie Weeds, David J. Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional simi-
larity. In COLING 2004, Proceedings of the 20th In-
ternational Conference on Computational Linguistics.

Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second Conference on
Quantum Interaction, Oxford, 26th 28th March 2008.

52


