



















































Dialogue focus tracking for zero pronoun resolution


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 494–503,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Dialogue focus tracking for zero pronoun resolution

Sudha Rao1,3, Allyson Ettinger2, Hal Daumé III1,3, Philip Resnik2,3
1Computer Science, 2Linguistics, 3UMIACS

University of Maryland
raosudha@cs.umd.edu, aetting@umd.edu, hal@cs.umd.edu, resnik@umd.edu

Abstract

We take a novel approach to zero pronoun res-
olution in Chinese: our model explicitly tracks
the flow of focus in a discourse. Our approach,
which generalizes to deictic references, is not
reliant on the presence of overt noun phrase
antecedents to resolve to, and allows us to ad-
dress the large percentage of “non-anaphoric”
pronouns filtered out in other approaches.
We furthermore train our model using read-
ily available parallel Chinese/English corpora,
allowing for training without hand-annotated
data. Our results demonstrate improvements
on two test sets, as well as the usefulness of
linguistically motivated features.

1 Introduction

“Pro-drop” languages like Chinese, Japanese and
Turkish allow for dropping of pronouns when the
referents of those pronouns can be inferred. English
is typically not pro-drop, but is unusual in that re-
gard: two thirds of languages documented in WALS
(Haspelmath et al., 2005) can be categorized as pro-
drop. In such languages, sentences are frequently
characterized by “zero pronouns”: gaps in the sen-
tence which in English would hold an overt pro-
noun. In some languages, verbal morphology or cl-
itics elsewhere in the sentence are sufficient to re-
solve the ambiguity of dropped pronouns; in other
languages, there is no overt marking at all in the sen-
tence and the referent of the dropped pronoun must
be resolved using pragmatic information.

Our work departs from mainstream work on zero
pronoun resolution in that we focus primarily on
the resolution of deictic zero pronouns. Unlike an

Figure 1: A conversation between a student and a teacher.
The text has been translated from Mandarin, but zero
pronouns are retained and indexed with their referent:
(T)eacher or (S)tudent.

anaphoric zero pronoun (Section 2), whose refer-
ence must be specified by a noun phrase occurring
previously in the text, a non-anaphoric zero pro-
noun refers to an entity that is salient from larger
units of discourse (such as full sentences or pas-
sages) or from the extralinguistic environment (out-
side of the text altogether). Although anaphoric zero
pronoun resolution has been the focus of most past
work (Yeh and Chen, 2007; Chen and Ng, 2014),
50% or fewer of zero pronouns in natural Chinese
text are anaphoric (Zhao and Ng, 2007; Kong and
Zhou, 2010). Our approach allows for generaliza-
tion to non-anaphoric pronouns, focusing in partic-
ular on deictic non-anaphoric zero pronouns, which

494



refer to salient entities in the environment (such as
the speaker, hearer or pragmatically accessible ref-
erent) without requiring any introduction in the pre-
ceding text. Figure 1 shows an example conversa-
tion in which zero pronouns are frequently used to
refer to speaker or listener, and would be translated
to English as “I” or “you.”

We propose a model for resolving deictic zero
pronouns that draws inspiration from ideas in Cen-
tering Theory (Grosz et al., 1995): discourses tend to
settle on a particular focus for a time, before switch-
ing. Furthermore, we presume that when a switch
happens, there is likely to be an overt cue of this.
For example, in Figure 1, the initial focus on T is
signaled with the overt second person pronoun in the
first utterance; the switch of focus to S in the third
utterance is also signaled by an overt “you.” How-
ever, at that point, the focus remains on S for several
utterances until “The last round. . . ” at which point it
switches away from the speakers. It is brought back
to S in the last utterance, which can be inferred from
the fact that S is the most recent focus that fits the
required semantic constraints.

To account for these phenomena, we develop a
novel sequential model for zero pronoun resolu-
tion that explicitly tracks the conversation focus
in a dialogue (Section 3). We test, using data from
Chinese SMS (“texting”) dialogues, the hypothesis
that our model can predict the identity of pronouns
(at a granularity of the person attribute: first, second,
or third person—with particular focus on first and
second person) based on a variety of features of the
utterance context, without reference to a particular
antecedent (Section 4.3). In this way, we address a
much higher percentage of the zero pronouns found
in Chinese texts, and particularly in dialogue.

Our second contribution is to show that one can
train a zero pronoun resolution system using su-
pervision coming from English translations of the
Chinese text (Section 2.2). This obviates the need
for expensive linguistic annotation of Chinese and
allows us to use plentiful parallel data to train our
model. Our results confirm that even though this
“translation as annotation” process is noisy, it is still
possible to learn on large amounts of “bronze stan-
dard” data.

2 Linguistic motivation

Handling zero pronouns in Chinese (or other pro-
drop language) involves two separate tasks: (1) Zero
pronoun identification: locating and marking the
gaps corresponding to zero pronouns; and (2) Zero
pronoun resolution: determining the entity referred
to by the zero pronoun. Our focus is the latter task.

Zero pronoun resolution, like general pronoun
resolution, is almost universally approached as a
problem of linking a pronoun to an overt noun
phrase antecedent in the text. However, while
some zero pronouns do have overt noun phrase an-
tecedents, many other zero pronouns do not. In
fact, (Zhao and Ng, 2007) report that just 52% of
zero pronouns in their training set (and 46% of zero
pronouns in their test set) are “anaphoric.” Kong
and Zhou (Kong and Zhou, 2010) report just 41%.
Some zero pronouns fail to link to an antecedent
because they refer to facts or events described by
larger phrases or full sentences earlier in the text,
preventing coreference with a single noun phrase.
Other zero pronouns, particularly in dialogue set-
tings, are deictic, pointing to salient entities in the
environment without requiring introduction by an
overt mention in the text.

2.1 Dialogue focus

A central principle of document cohesion that under-
lies frameworks such as Centering Theory (Grosz et
al., 1995) states that discourses tend to settle on a
particular focus for a time, before eventually switch-
ing to a new one. The status of a particular focus
within this flow of discourse is typically signaled
by the form of the expression chosen to point to it.
When a focus is introduced (or returned to), a full
(overt) noun phrase is generally used to indicate it.
While that entity remains in focus, subsequent men-
tions can be realized with less explicit forms. In En-
glish, these less explicit forms are overt pronouns. In
Chinese (and pro-drop languages more generally),
these focus continuations are generally realized as
zero pronouns.

We see in this example an illustration of these dis-
course principles:

1. In pro-drop languages (Chinese), overt pro-
nouns introduce switches in focus, while zero

495



pronouns are used while an established focus
continues.

2. In non-pro-drop languages (English), overt pro-
nouns serve the focus-continuation function.

3. There are “deictic” exceptions to these rules,
licensed by environmental salience of the ref-
erent and inferable from the meaning of the ut-
terance (final question of the example).

Importantly, these continuations and switches of
foci occur for the most part at the level of the syntac-
tic clause. This is thus the level at which we model,
assigning labels to individual clauses, which will in
turn indicate the identity of any dropped subject pro-
noun in that clause.1 In identifying focus, we remain
at the granularity of the “person” attribute (first, sec-
ond, or third person). This is the most relevant gran-
ularity for deictic pronoun resolution, as the intent
is to capture the alternation between speakers within
that dialogue (first and second person), along with
switches of focus to any referents external to the di-
alogue (third person).

2.2 Translation as annotation

Currently, most state-of-the-art machine learning
systems for Chinese zero pronoun resolution are
supervised, requiring manually resolved pronouns
for training. We hypothesize comparable distri-
bution between zero pronouns in a pro-drop lan-
guage, and overt pronouns in a non-pro-drop lan-
guage. More specifically, because non-pro-drop lan-
guages lack zero pronouns, the discourse functions
that are served by zero pronouns in pro-drop lan-
guages must in non-pro-drop languages be served by
overt pronouns.

To be more concrete, the original Mandarin SMS
conversation from Figure 1 is reproduced in Table 1,
together with a human translation into English. In-
deed, we see in the example in Figure 1 that the
zero pronouns on the Chinese side correspond to
overt pronouns on the English side. For this rea-

1Although Mandarin does license dropped object pronouns,
we focus in this paper only on subject pronouns, as the syntac-
tic subject is (a) more consistently dropped in Mandarin, and
(b) more tightly tied to the notion of focus of conversation that
motivates our model; see also a discussion of the centering hi-
erarchy in Chinese (Wang, 2011). Relatedly, we filter out pos-
sessive pronouns in subject position, as they do not point to the
topical entity represented by the full noun phrase.

son we make use of a parallel (Chinese/English) cor-
pus for training of our sequence labeling model, de-
riving the identities of missing pronouns from the
English translation of the Chinese text rather than
from coreference relations with antecedents in the
Chinese text. Our model thus does not rely on the
availability of hand-annotated data for training.

3 Our focus tracking model

Given a Chinese dialogue, our goal is to identify
zero pronouns and resolve them either as deictic
(first or second person) or non-deictic (third person).
We use off-the-shelf tools for the identification of
the zero pronouns (described in Section 4.1) and fo-
cus on the resolution task.

In our implementation, we jointly predict the fo-
cus and identify the number of the pronoun that
would be used. For instance, when S is speaking
about herself, we consider this a “1” label; when S
is speaking about her conversation partner, we con-
sider this a “2” label. This numbering corresponds
to which pronominal form would be required in En-
glish.2

3.1 Supervision via bronze standard data

We obtain “bronze” standard (as opposed to “gold”
standard) data by looking at human-produced En-
glish translations of Chinese utterances, such as
those seen in Table 1. Our label set consists of two
properties: the person being referred to (first person,
second person or third person), and whether the ref-
erence is overt or not (visible or hidden). The “vis-
ible” three labels correspond to clauses in which an
overt subject pronoun appears on the English side.
Chinese clauses bearing this label may have an overt
or a zero pronoun subject—if the Chinese side con-
tains a zero pronoun subject, then this label will be
used to determine the correct person attribute (first,
second, or third) of the unseen pronoun.

1v: Overt English first person pronoun: “I” or “we”
2v: Overt English second person pronoun: “you”
3v: Overt English third person pronoun: “he”,

“she”, “it”, “they”

2We ignore morphological issues in English dealing with
possession and grammatical role, since these are exogenous to
the resolution task.

496



Original Mandarin English Translation Label
1) Student 陈老师你还好吗 How are you, Teacher Chen? 2v
2) Teacher Ø好啊 ,你还没走 ? I am fine. You have not left yet? 1v, 2v
3) Student Ø回来有一个月Ø不敢给你说话 I have been back for a month. 1v, 1v

I didn’t dare to chat with you.
4) Student 没呢 Not yet. 1h
5) Student 美国的面试Ø进行了 4轮了 I have gone through 4 rounds of interviews for the

American (company).
1v

6) Teacher 为什么 Why? 2h
7) Student 最后一轮是跟总经理 The last round of interview is with the general manager. 3v
8) Student 还有两次网上测验 There are also two online tests. 3h
9) Teacher Ø还在面试阶段吗 Are you still in the interview phase? 2v

Table 1: Sample Chinese SMS conversation with English translation and derived labels.

However, there are plenty of utterances (e.g., Ta-
ble 1 lines 4 and 6) in which the English transla-
tion does not contain an overt subject. This can hap-
pen in English in imperative constructions, (some)
questions, and general informal communication.3 In
these cases, we introduce “hidden” person labels
whose role is to carry forward the focus from the
previous utterance. For instance, in utterance 4, even
though there is no subject on the English side, we
carry forward the fact that the most-recent referent
was “first person” and denote this with “1h.”

Because we are jointly modeling the focus shift
and the pronoun realization aspects, when the
speaker shifts, the “hidden” person must flip. For
example, in utterance (5) the Student overtly refers
to herself, yielding a label of “1v.” The next utter-
ance is by the Teacher but lacks an English subject.
The focus remains on the Student and therefore this
utterance is labeled “2h” meaning that the focus is
on the other speaker, and it is non-overt in English.

1h: subject being continued is first person
2h: subject being continued is second person
3h: subject being continued is anything else

Finally, we introduced a seventh label for instances
in which no overt subject pronoun appears on the
English side, and no focus has yet been established
from prior clauses (this applies only at the beginning
of a discourse).

None: no subject and no focus yet established

3This can also happen due to imperfect zero pronoun identi-
fication (Section 4.1).

In Table 1, the rightmost column shows the label as-
signment for the sample SMS exchange. (The utter-
ances on lines 2 and 3 contain two clauses each, and
thus two labels each.)

3.2 Features

We included in our model the following features.
Note that these features are based solely on the Chi-
nese side. Linguistic motivations for each feature
category are described.

Subject continuation: a value indicating the per-
son (1, 2, 3) of the most recent overt NP that was
a direct descendent of an IP node (the most recent
overt NP in structural subject position—including,
if overt, the subject of the current clause). The most
recent overt NP subject is a strong candidate for
coreference with a zero pronoun. This feature comes
closest to attempting antecedent selection.

Verb: the first verb in the VP that is sister to the
subject NP (the VP of which that NP is the subject).
The nature of the verb can provide information rel-
evant to inferring the identity of deictic forms. For
example: the Chinese verb guji (“reckon”) is intu-
itively biased toward first-person subject; our train-
ing data accordingly show 68% of clauses with guji
as verb feature were assigned first-person subject la-
bels.

Participant index: a value indicating the index of
the conversational participant. To capture regular-
ities, if any exist, in the pronoun use of a speaker.

Participant switch: a binary value indicating
whether the current utterance represents a change of
speaker relative to the previous clause. Switches in

497



speaker may, particularly in tandem with other fea-
tures, be informative about topic.
Object (downstream): the direct object of the VP
sister to the subject (if any). This feature exploits the
fact that pronouns occurring as direct objects within
a clause cannot be the same as the (zero) pronoun in
subject position of that clause.
Has question particle: a binary value indicating
whether the clause contains a) a question particle
or wh-word, or b) a question mark. This feature is
likely to be a strong indicator of that the subject pro-
noun is not first person (also used by (Chen and Ng,
2013)). For example, in our training data we found
that only 16% of the clauses with question particle
were marked with first person label i.e. 1v or 1h.
Bag of words: all words occurring in the clause.
Apart from the verb, other words can also be highly
informative about the nature of the subject.
Bag of parts of speech: all parts of speech occur-
ring in the clause. The structural make-up of clause
may be informative about focus, for instance in the
case of passive or possessive constructions.
Hidden subject particles: a feature indicating
whether the clause consists of a list of phrases con-
sistently tagged with empty categories on the Chi-
nese side, but consistently translated without subject
pronouns on the English side (thus likely to corre-
spond to labels 1h-3h). This feature is intended to
help the model in recognizing clauses consistently
corresponding to “hidden” labels.

In addition, for the features that consist of se-
quence (bag of words, bag of part of speech, object,
etc.) we additionally compute bigrams and trigrams.

3.3 Structured prediction

We cast the above model as a sequence labeling
problem over visible and hidden labels. We consider
each conversation segment in the SMS as an input
data sequence x = 〈x1, x2, . . . , xn〉 where each xi
corresponds to a clause in Chinese. Each clause in
Chinese is assigned a label from the label space Y
= {1v, 2v, 3v, 1h, 2h, 3h, none}. The task then is to
assign labels y = 〈y1, y2, . . . , yn〉 to the input data
sequence from the label space Y based on the fea-
tures described in Section 3.2. At training time we
assign labels to the input sequence using the “bronze
standard” method described in Section 3.1.

To train the sequence labeling model, we use an
online variant of the DAgger imitation learning al-
gorithm (Ross et al., 2011) as implemented in the
Vowpal Wabbit machine learning library (Langford
et al., 2007; Daumé III et al., 2014). DAgger, like
its predecessor SEARN (Daumé III et al., 2009),
solves structured prediction problems by transform-
ing them into sequential decision making problems.
In the case of sequence labeling, the natural order
for sequence decision making is left-to-right. At test
time, inference is performed greedily. At training
time, the learning algorithm attempts to balance be-
tween training on “oracle” states (prefixes of deci-
sions made optimally according to the true labels)
and training on “system” states (prefixes of deci-
sions made sub-optimally according to the learned
model). The online variant of DAgger balances this
trade-off by slowly transitioning from making past
decisions optimally to making them using the cur-
rently learned predictor.

4 Experiments

Our goal in our experiments is to answer the follow-
ing questions:

1. How well does the bronze-standard annotation
capture the underlying truth? (Section 4.2).

2. Is our model able to leverage both dialogue
structure and semantic content to accurately re-
solve pronouns? (Section 4.3)

3. How important are the different components
in our model in making effective predictions?
(Section 4.4)

In the following sections, we describe the exper-
iments we perform aimed at answering these ques-
tions. First, we describe the data we use for experi-
mentation.

4.1 Experimental setup

For training our focus-tracking model, we use
Chinese-English parallel data from the SMS/chat
domain available as part of training data used in the
Machine Translation task under the DARPA BOLT
project. The training data consisted of 117k sen-
tences. We test our model on heldout SMS/Chat
data consisting of 1152 sentences (hand-annotated,

498



Bronze SMS OntoNotes
Training Test Test

# tokens 1, 007, 722 8104 108, 531
# sents 129, 190 1152 9607
# dialog 3309 34 257
# types 26, 519 1747 4753

Table 2: Dataset statistics; numbers are for the Chi-
nese side of the data. English has 25% more tokens and
roughly as many types.

as described in Section 4.2), and on telephone con-
versation data from the OntoNotes corpus (Hovy et
al., 2006), consisting of 5000 sentences. Full data
statistics are provided in Table 2.

We perform zero pronoun identification using the
method of (Cai et al., 2011), which automatically re-
covers empty categories corresponding to dropped
pronouns, integrating these empty categories into
syntactic parses. Syntactic parses were obtained
with the Berkeley parser (Petrov and Klein, 2007).
These parses were then used to split the Chinese ut-
terances into single-clause units, based on IP and
CP clausal nodes. These clauses were aligned with
clauses in the English translation, which were used
to determine the identity of the clausal subject, for
extracting the 1v, 2v, . . . label for each utterance.4

For our machine learning systems, we use Vowpal
Wabbit (Langford et al., 2007) with default hyper-
parameter settings. We train on 75% of the training
data and retain 25% as development data on which to
perform early stopping. We run 20 iterations by de-
fault and take the parameters with best development
performance based on sequence labeling accuracy.

4.2 Gold standard test set
Although we can use “bronze standard” annota-
tions for learning, evaluating against a bronze stan-
dard is not directly useful. Therefore, we annotated
our test set (1152 utterances) by hand. In partic-
ular, for the SMS/chat test set, we recruited three
linguistically-informed native Mandarin speakers to
annotate Chinese clauses containing empty cate-
gories. The clauses were labeled with a person num-
ber (1,2,3) when the empty category corresponded to

4Sometimes English syntactic parses were not well-aligned
with the Chinese IP/CP nodes; in practice, we split the English
utterances based on end-of-clause punctuation and aligned Chi-
nese and English clauses based on a simple order heuristic.

Pronoun Precision Recall F-measure
1p 0.75 0.43 0.55
2p 0.61 0.32 0.42
3p 0.52 0.45 0.49

Micro-avg 0.62 0.41 0.50

Table 3: Bronze vs Gold labels

such a pronoun; or “none” in spurious cases.5

In our annotated data,6 32% of identified zero pro-
nouns were first person, 17% were second person,
25% were third person and 26% do not have a refer-
ent (were spurious). Of the correctly identified zero
pronouns, a majority of pronouns (about 2/3) are de-
ictic: referring either to the speaker or listener. The
remainder are third person and mostly anaphoric.7

Since the annotators labeled the empty categories
obtained from an automatic zero pronoun identifi-
cation method (Cai et al., 2011), 26% spurious cases
suggest that the accuracy of this method is only 74%
on the SMS test data set.

We then used these annotations to evaluate our
bronze standard label assignment method against the
gold standard judgments. Table 3 shows the pre-
cision, recall and F-measure of the bronze annota-
tions when evaluated against the gold annotations.
We use micro-averaging to average the precision, re-
call and F-measure values against different sets. In
this method we sum up the individual true positives
(TP), false positives (FP), and false negatives (FN)
of the system for different sets and then apply them
to get the statistics. For example, precision across
two sets 1 and 2 is given by (TP1 + TP2)/(TP1 +
TP2 + FP1 + FP2). We can see a fairly significant
discrepancy between our bronze labels and the gold
labels. One major—and unfortunately inevitable—
reason for this discrepancy is a high proportion of ut-
terances in the English translation data which have

5Note that under this annotation scheme, our evaluations
will be partially constrained by (Cai et al., 2011) perfor-
mance, in including no zero pronouns that were missed by that
method—however, use of the “none” label allows filtering out
of any spuriously-identified zero pronouns.

6Annotations are available at
www.umiacs.umd.edu/∼raosudha/LDC2013E83-BOLT-
P2R2-cmn-SMS-CHT-dev.annotated.

7In an in-person dialogue, a third person pronoun might be
used in a deictic manner, as in “She is really smart” while point-
ing at someone. This rarely occurs in SMS/chat because there
is no shared environment beyond the two dialogue participants.

499



been translated with the subject pronouns still ab-
sent. This is partially due to the casual nature of the
text, and partially because the quality (fluency) of
English translations in this data is at times dubious.

While there may be a systematicity to this kind of
subject omission on the English side, this was not
a factor taken into account by our human annota-
tors. So while our own model may stand a chance
at predicting “hidden” labels (no overt pronoun on
English side) in such instances, the annotators will
never assign a label of “none” to a location at which
a pronoun could reasonably have been inserted.

4.3 Overall system efficacy
In this section we discuss the overall efficacy of our
proposed method in comparison to a few alterna-
tives. These alternatives are:

Random guessing baseline. A naı̈ve system that
makes predictions uniformly at random.

Subject continuation baseline. This is a rule-
based approach that mimics the intuitions described
in Section 2. In particular, for a Chinese utterance,
we check whether the current utterance has an overt
pronominal subject. If so, we assign a label of 1v, 2v
or 3v depending on the person of this subject. If the
current utterance has a non-pronominal subject, we
assign 3h. Otherwise we “carry forward” the sub-
ject from the previous utterance, flipping the 1p/2p
as necessary when the speaker changes; these are la-
beled as 1h, 2h or 3h.

Minimal model baseline. In the minimal model,
we restrict our model to use just three features: par-
ticipant index, participant switch and subject contin-
uation feature. This is a machine learning variant of
the rule-based subject continuation baseline.

Oracle upper bound. None of the proposed mod-
els can hope to achieve 100% accuracy on this task
because the gold annotation data consists of 26%
“no pronoun” cases. Since all of our approaches
must predict a pronoun when a zero pronoun has
been identified, their performance (namely, their
precision) is upper-bounded away from 100%.

The summary of results (micro-averaged across
1p, 2p and 3p) are shown in Table 5. These results
show that on both the SMS data (on which the model
was developed) and the OntoNotes data (on which

SMS/chat OntoNotes
Micro-average Micro-average

System Pre Rec F Pre Rec F
Random 0.24 0.25 0.24 0.18 0.26 0.22
Minimal 0.42 0.23 0.30 0.30 0.07 0.11
SubjCont 0.32 0.42 0.36 0.31 0.15 0.20
Full Model 0.59 0.31 0.41 0.30 0.36 0.33
Upper Bound 0.74 1.00 0.85 0.55 1.00 0.71

Table 5: Summary of results for different comparator
models against the gold standard labels from SMS data
(left) and OntoNotes (right).

the model was applied blindly), our full model is
able to substantially outperform the baselines. In
fact, on OntoNotes, despite a potential domain mis-
match (from SMS/chat to telephone conversations),
our full model was the only baseline to beat ran-
dom guessing! Across both data sets, the minimal
model tends to have high precision and low recall;
the behavior of the other approaches varies across
the tasks. On the SMS/chat data, our model achieves
a 14% relative improvement over the best baseline;
on an OntoNotes data, a 50% relative improvement.

More specific breakdowns of performance by dif-
ferent pronouns (1p, 2p and 3p) are shown for the
subject continuation baseline and the full model in
Table 4. In these tables, we also report results when
evaluated on the OntoNotes test set in these Tables.
As we can see, the subject continuation baseline
massively overpredicts third person pronouns in the
SMS data, leading to an overall low score. In com-
parison, our model tends to have much higher pre-
cision (at the expense of recall) across the board on
the SMS data, leading to a 14% relative improve-
ment over the subject continuation baseline.

Since, to our knowledge, no prior work (see
Section 5) has focused on deictic pronoun restora-
tion, it is not possible to directly compare our re-
sults to previously published results. Although it
is an apples-to-oranges comparison, a state-of-the-
art anaphoric zero pronoun resolution system (Chen
and Ng, 2014) achieves a precision of 13.3, a re-
call of 32.2 and an F-measure of 18.8 on the tele-
phone conversation part of the OntoNotes data, but
does so addressing the complementary problem of
correctly choosing antecedents from previous overt
noun phrases.

Another reasonable comparison would be with a

500



Subject Continuation Baseline
SMS Test set OntoNotes Test set

Pronoun Precision Recall F-measure Precision Recall F-measure
1p 0.43 0.28 0.34 0.29 0.08 0.13
2p 0.27 0.19 0.22 0.28 0.11 0.16
3p 0.29 0.75 0.42 0.32 0.21 0.25

Micro-avg 0.32 0.42 0.36 0.31 0.15 0.20

Our Full Model
SMS Test set OntoNotes Test set

Pronoun Precision Recall F-measure Precision Recall F-measure
1p 0.64 0.47 0.54 0.27 0.58 0.37
2p 0.55 0.21 0.31 0.25 0.23 0.24
3p 0.50 0.18 0.27 0.40 0.28 0.33

Micro-avg 0.59 0.31 0.41 0.30 0.36 0.33

Table 4: Results across different pronoun categories for (top) subject continuation and (bottom) our full model.

SMS/chat OntoNotes
Micro-average Micro-average

System Pre Rec F Pre Rec F
Minimal (M) 0.42 0.23 0.30 0.30 0.07 0.11
M + question 0.44 0.23 0.30 0.31 0.07 0.12
M + object 0.43 0.23 0.30 0.30 0.07 0.12
M + verb 0.58 0.29 0.39 0.32 0.13 0.18
M + pos 0.52 0.30 0.38 0.23 0.27 0.25
M + bow 0.59 0.28 0.38 0.30 0.35 0.32
Full Model 0.59 0.31 0.41 0.30 0.36 0.33

Table 6: Summary of results for feature ablation
against the gold standard labels from SMS data (left) and
OntoNotes (right).

model trained on gold annotated data (instead of
bronze data). Owing to cost, we could not obtain
gold annotations for the full training set; however, a
leave-one-out cross validation on the gold annotated
test set (of SMS data) gave an F-measure of 0.47,
versus 0.41 for our model trained on bronze labels.
This suggests the noisy bronze labels are indeed use-
ful for this task.

4.4 Feature ablations

In order to investigate the individual contributions of
each of our features, we performed feature ablation
experiments, pairing our Minimal Model with a sin-
gle feature at a time and retraining the model with
this pairing. The results of these experiments can be
seen in Table 6. We see in this table that for the SMS
data, the Verb feature creates the greatest improve-
ment over the Minimal Model, followed by Bag of

Words and Bag of POS. This supports the hypoth-
esis that the verb is informative with respect to the
nature of its subject, as are the other words of the
clause, and their parts of speech. For the OntoNotes
corpus, however, the Bag of Words feature performs
best by a large margin. Interesting, although the
Bag of Words features are clearly the most useful,
the linguistically motivated features (verb/question)
performing well supports our linguistic intuitions.

5 Discussion and related work

Past approaches to zero pronoun resolution focus ex-
clusively on anaphoric zero pronouns approached as
a task of antecedent identification. Almost all work
makes use of syntactic structure, with differences
primarily in how that structure is used. (Yeh and
Chen, 2007) take a rule-based, Centering Theory-
inspired approach based on a system of constraints
to guide selection of zero pronoun antecedents. In
the same year, (Zhao and Ng, 2007) introduced a
supervised learning approach for both zero pronoun
identification and antecedent selection based on en-
gineered features; these engineered features were
replaced with a tree-kernel by (Kong and Zhou,
2010), who jointly perform zero pronoun identifi-
cation, anaphoricity determination, and antecedent
selection. Recently, (Chen and Ng, 2013) built
upon the model introduced by Zhao and Ng, intro-
ducing additional features and allowing coreference
links between multiple zero pronouns. Chen and
Ng (2013) also test their model on automatically
identified zero pronouns and automatically gener-

501



ated parse trees, thus presenting the first end-to-end
Chinese zero pronoun resolver.

These approaches are mostly complementary to
our task, since they focus on resolving anaphoric
zero pronouns (the minority!) while we focus on re-
solving deictic non-anaphoric zero pronouns. In par-
ticular, for an end-to-end system that resolves both
deictic and anaphoric zero pronouns, one could first
take our approach and whenever our model predicts
“third person,” which is often an anaphoric refer-
ence, one could apply one of these prior approaches
for further reference resolution.

The only work we are aware of that does not
require linguistically annotated data for zero pro-
noun resolution is that of (Chen and Ng, 2014).
They hypothesize that zero pronouns and overt pro-
nouns have similar distributions, and train an unsu-
pervised model on overt pronouns and then apply
this model to zero pronouns. This model performs
on par with their previous (2013) supervised model.
Despite this, their unsupervised model only agrees
with their supervised model on 55% of zero pronoun
antecedents, suggesting that this hypothesis is weak.

In particular, the complementarity of zero versus
overt pronoun usage has been studied within various
domains of linguistics. The Position of Antecedent
Hypothesis (Carminati, 2002) states that null and
overt pronouns have different antecedent selection
preferences: null pronouns prefer antecedents in
subject positions, while overt pronouns prefer an-
tecedents in non-subject positions. This hypothesis
has been supported by studies in a variety of pro-
drop languages (e.g., (Alonso-Ovalle et al., 2002)
(Kweon, 2011)). Switching of reference has been
identified as one of the main constraints regulating
use of zero versus overt pronouns in the variationist
literature (see (Cameron, 1992) for sociolinguistic
studies of the phenomenon in Spanish). The impor-
tance of topically-coherent discourse sequences—
and the role of linguistic and extralinguistic indica-
tors of such sequences—has also been examined in
child language acquisition, e.g., (Rohde and Frank,
2014).

Our main result shows that although our bronze
standard labels are noisy (Section 4.3), they are
nonetheless useful for learning to resolve deictic
pronouns. Moreover, one oft-heralded advantage of
the translation-as-annotation scheme (Carpuat and

Wu, 2007) is that it naturally integrates into a ma-
chine translation framework, since one is learning
to predict precisely what is necessary for success-
ful translation; evaluating whether this hypothesis is
true is currently an open question. One limitation of
our approach is the coarseness of the labeling gran-
ularity (1p, 2p, 3p). Our ultimate plan is to provide
all possibilities (e.g., both singular and plural for 1p,
weighted) to a machine translation system, and let
other components (e.g., language model) determine
selection. For now, we believe that there is signifi-
cant value in intrinsic evaluation of our approach for
a problem that has not previously received signifi-
cant attention.

Acknowledgments

This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0012-12-C-0015. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of DARPA. The
authors would like to thank three anonymous re-
viewers for providing helpful comments and also ac-
knowledge people from Computational Linguistics
and Information Processing (CLIP) lab at University
of Maryland for helpful discussions.

References

[Alonso-Ovalle et al.2002] Luis Alonso-Ovalle, Susana
Fernández-Solera, Lyn Frazier, and Charles Clifton.
2002. Null vs. overt pronouns and the topic-focus ar-
ticulation in Spanish. Journal of Italian Linguistics,
14:151–170.

[Cai et al.2011] Shu Cai, David Chiang, and Yoav Gold-
berg. 2011. Language-independent parsing with
empty elements. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 212–216. Association for Computa-
tional Linguistics.

[Cameron1992] Richard Cameron. 1992. Pronominal
and null subject variation in Spanish: Constraints,
dialects, and functional compensation. Ph.D. thesis,
University of Pennsylvania.

[Carminati2002] Maria Nella Carminati. 2002. The pro-
cessing of Italian subject pronouns. Ph.D. thesis, Uni-
versity of Massachusetts at Amherst.

[Carpuat and Wu2007] Marine Carpuat and Dekai Wu.

502



2007. Improving statistical machine translation using
word sense disambiguation. In In EMNLP.

[Chen and Ng2013] Chen Chen and Vincent Ng. 2013.
Chinese zero pronoun resolution: Some recent ad-
vances. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1360–1365.

[Chen and Ng2014] Chen Chen and Vincent Ng. 2014.
Chinese zero pronoun resolution: An unsupervised
probabilistic model rivaling supervised resolvers. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing. Association
for Computational Linguistics.

[Daumé III et al.2009] Hal Daumé III, John Langford,
and Daniel Marcu. 2009. Search-based structured pre-
diction. Machine learning, 75(3):297–325.

[Daumé III et al.2014] Hal Daumé III, John Langford,
and Stephane Ross. 2014. Efficient programmable
learning to search. arXiv preprint arXiv:1406.1837.

[Grosz et al.1995] Barbara J Grosz, Scott Weinstein, and
Aravind K Joshi. 1995. Centering: A framework for
modeling the local coherence of discourse. Computa-
tional linguistics, 21(2):203–225.

[Haspelmath et al.2005] Martin Haspelmath, Matthew
Dryer, David Gil, and Bernard Comrie, editors. 2005.
The World Atlas of Language Structures. Oxford Uni-
versity Press.

[Hovy et al.2006] Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2006. Ontonotes: the 90% solution. In NAACL.

[Kong and Zhou2010] Fang Kong and Guodong Zhou.
2010. A tree kernel-based unified framework for Chi-
nese zero anaphora resolution. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 882–891. Association for
Computational Linguistics.

[Kweon2011] Soo-Ok Kweon. 2011. Processing null and
overt pronoun subject in ambiguous sentences in Ko-
rean. International Journal of Linguistics, 3(1).

[Langford et al.2007] John Langford, Lihong Li, and
Alexander Strehl. 2007. Vowpal wabbit online learn-
ing project. http://hunch.net/?p=309.

[Petrov and Klein2007] S. Petrov and D. Klein. 2007.
Improved inference for unlexicalized parsing. In Pro-
ceedings of NAACL HLT 2007, pages 404–411.

[Rohde and Frank2014] Hannah Rohde and Michael C
Frank. 2014. Markers of topical discourse in child-
directed speech. Cognitive science, 38(8):1634–1661.

[Ross et al.2011] Stéphane Ross, Geoff J. Gordon, and
J. Andrew Bagnell. 2011. A reduction of imitation
learning and structured prediction to no-regret online
learning. In Proceedings of the Workshop on Artificial
Intelligence and Statistics (AIStats).

[Wang2011] Deliang Wang. 2011. Anaphora Resolution
in Chinese from the Centering Perspective. Foreign
Language Teaching and Research Press.

[Yeh and Chen2007] Ching-Long Yeh and Yi-Chun Chen.
2007. Zero anaphora resolution in Chinese with shal-
low parsing. Journal of Chinese Language and Com-
puting, 17(1):41–56.

[Zhao and Ng2007] Shanheng Zhao and Hwee Tou Ng.
2007. Identification and resolution of Chinese zero
pronouns: A machine learning approach. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods on Natural Language Processing and Computa-
tional Natural Language Learning, pages 541–550.

503


