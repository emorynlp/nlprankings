










































Extracting Narrative Timelines as Temporal Dependency Structures


Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 88–97,
Jeju, Republic of Korea, 8-14 July 2012. c©2012 Association for Computational Linguistics

Extracting Narrative Timelines as Temporal Dependency Structures

Oleksandr Kolomiyets
KU Leuven

Celestijnenlaan 200A
B-3001 Heverlee, Belgium

Oleksandr.Kolomiyets@
cs.kuleuven.be

Steven Bethard
University of Colorado

Campus Box 594
Boulder, CO 80309, USA
Steven.Bethard@
colorado.edu

Marie-Francine Moens
KU Leuven

Celestijnenlaan 200A
B-3001 Heverlee, Belgium

Sien.Moens@
cs.kuleuven.be

Abstract

We propose a new approach to characterizing
the timeline of a text: temporal dependency
structures, where all the events of a narrative
are linked via partial ordering relations like BE-
FORE, AFTER, OVERLAP and IDENTITY. We
annotate a corpus of children’s stories with tem-
poral dependency trees, achieving agreement
(Krippendorff’s Alpha) of 0.856 on the event
words, 0.822 on the links between events, and
of 0.700 on the ordering relation labels. We
compare two parsing models for temporal de-
pendency structures, and show that a determin-
istic non-projective dependency parser outper-
forms a graph-based maximum spanning tree
parser, achieving labeled attachment accuracy
of 0.647 and labeled tree edit distance of 0.596.
Our analysis of the dependency parser errors
gives some insights into future research direc-
tions.

1 Introduction

There has been much recent interest in identifying
events, times and their relations along the timeline,
from event and time ordering problems in the Temp-
Eval shared tasks (Verhagen et al., 2007; Verhagen
et al., 2010), to identifying time arguments of event
structures in the Automated Content Extraction pro-
gram (Linguistic Data Consortium, 2005; Gupta and
Ji, 2009), to timestamping event intervals in the
Knowledge Base Population shared task (Artiles et
al., 2011; Amigó et al., 2011).

However, to date, this research has produced frag-
mented document timelines, because only specific
types of temporal relations in specific contexts have

been targeted. For example, the TempEval tasks only
looked at relations between events in the same or ad-
jacent sentences (Verhagen et al., 2007; Verhagen et
al., 2010), and the Automated Content Extraction pro-
gram only looked at time arguments for specific types
of events, like being born or transferring money.

In this article, we propose an approach to temporal
information extraction that identifies a single con-
nected timeline for a text. The temporal language
in a text often fails to specify a total ordering over
all the events, so we annotate the timelines as tem-
poral dependency structures, where each event is a
node in the dependency tree, and each edge between
nodes represents a temporal ordering relation such
as BEFORE, AFTER, OVERLAP or IDENTITY. We
construct an evaluation corpus by annotating such
temporal dependency trees over a set of children’s
stories. We then demonstrate how to train a time-
line extraction system based on dependency parsing
techniques instead of the pair-wise classification ap-
proaches typical of prior work.

The main contributions of this article are:

• We propose a new approach to characterizing
temporal structure via dependency trees.

• We produce an annotated corpus of temporal
dependency trees in children’s stories.

• We design a non-projective dependency parser
for inferring timelines from text.

The following sections first review some relevant
prior work, then describe the corpus annotation and
the dependency parsing algorithm, and finally present
our evaluation results.

88



2 Related Work

Much prior work on the annotation of temporal in-
formation has constructed corpora with incomplete
timelines. The TimeBank (Pustejovsky et al., 2003b;
Pustejovsky et al., 2003a) provided a corpus anno-
tated for all events and times, but temporal relations
were only annotated when the relation was judged to
be salient by the annotator. In the TempEval compe-
titions (Verhagen et al., 2007; Verhagen et al., 2010),
annotated texts were provided for a few different
event and time configurations, for example, an event
and a time in the same sentence, or two main-clause
events from adjacent sentences. Bethard et al. (2007)
proposed to annotate temporal relations one syntactic
construction at a time, producing an initial corpus of
only verbal events linked to events in subordinated
clauses. One notable exception to this pattern of
incomplete timelines is the work of Bramsen et al.
(2006) where temporal structures were annotated as
directed acyclic graphs. However they worked on a
much coarser granularity, annotating not the order-
ing between individual events, but between multi-
sentence segments of text.

In part because of the structure of the available
training corpora, most existing temporal informa-
tion extraction models formulate temporal linking
as a pair-wise classification task, where each pair
of events and/or times is examined and classified as
having a temporal relation or not. Early work on the
TimeBank took this approach (Boguraev and Ando,
2005), classifying relations between all events and
times within 64 tokens of each other. Most of the top-
performing systems in the TempEval competitions
also took this pair-wise classification approach for
both event-time and event-event temporal relations
(Bethard and Martin, 2007; Cheng et al., 2007; UzZa-
man and Allen, 2010; Llorens et al., 2010). Systems
have also tried to take advantage of more global in-
formation to ensure that the pair-wise classifications
satisfy temporal logic transitivity constraints, using
frameworks such as integer linear programming and
Markov logic networks (Bramsen et al., 2006; Cham-
bers and Jurafsky, 2008; Yoshikawa et al., 2009; Uz-
Zaman and Allen, 2010). Yet the basic approach is
still centered around pair-wise classifications, not the
complete temporal structure of a document.

Our work builds upon this prior research, both

improving the annotation approach to generate the
fully connected timeline of a story, and improving
the models for timeline extraction using dependency
parsing techniques. We use the annotation scheme
introduced in more detail in Bethard et. al. (2012),
which proposes to annotate temporal relations as de-
pendency links between head events and dependent
events. This annotation scheme addresses the issues
of incoherent and incomplete annotations by guaran-
teeing that all events in a plot are connected along
a single timeline. These connected timelines allow
us to design new models for timeline extraction in
which we jointly infer the temporal structure of the
text and the labeled temporal relations. We employ
methods from syntactic dependency parsing, adapt-
ing them to our task by including features typical of
temporal relation labeling models.

3 Corpus Annotation

The corpus of stories for children was drawn from the
fables collection of (McIntyre and Lapata, 2009)1 and
annotated as described in (Bethard et al., 2012). In
this section we illustrate the main annotation princi-
ples for coherent temporal annotation. As an example
story, consider:

Two Travellers were on the road together,
when a Bear suddenly appeared on the
scene. Before he observed them, one made
for a tree at the side of the road, and
climbed up into the branches and hid there.
The other was not so nimble as his compan-
ion; and, as he could not escape, he threw
himself on the ground and pretended to be
dead. . . [37.txt]

Figure 1 shows the temporal dependency structure
that we expect our annotators to identify in this story.

The annotators were provided with guidelines both
for which kinds of words should be identified as
events, and for which kinds of events should be
linked by temporal relations. For identifying event
words, the standard TimeML guidelines for anno-
tating events (Pustejovsky et al., 2003a) were aug-
mented with two additional guidelines:

1Data available at http://homepages.inf.ed.ac.
uk/s0233364/McIntyreLapata09/

89



Figure 1: Event timeline for the story of the Travellers and the Bear. Nodes are events and edges are temporal relations.
Edges denote temporal relations signaled by linguistic cues in the text. Temporal relations that can be inferred via
transitivity are not shown.

• Skip negated, modal or hypothetical events (e.g.
could not escape, dead in pretended to be dead).

• For phrasal events, select the single word that
best paraphrases the meaning (e.g. in used to
snap the event should be snap, in kept perfectly
still the event should be still).

For identifying the temporal dependencies (i.e. the
ordering relations between event words), the anno-
tators were instructed to link each event in the story
to a single nearby event, similar to what has been
observed in reading comprehension studies (Johnson-
Laird, 1980; Brewer and Lichtenstein, 1982). When
there were several reasonable nearby events to choose
from, the annotators were instructed to choose the
temporal relation that was easiest to infer from the
text (e.g. preferring relations with explicit cue words
like before). A set of six temporal relations was used:
BEFORE, AFTER, INCLUDES, IS-INCLUDED, IDEN-
TITY or OVERLAP.

Two annotators annotated temporal dependency
structures in the first 100 fables of the McIntyre-
Lapata collection and measured inter-annotator agree-
ment by Krippendorff’s Alpha for nominal data (Krip-
pendorff, 2004; Hayes and Krippendorff, 2007). For
the resulting annotated corpus annotators achieved
Alpha of 0.856 on the event words, 0.822 on the links
between events, and of 0.700 on the ordering rela-
tion labels. Thus, we concluded that the temporal
dependency annotation paradigm was reliable, and
the resulting corpus of 100 fables2 could be used to

2Available from http://www.bethard.info/data/
fables-100-temporal-dependency.xml

train a temporal dependency parsing model.

4 Parsing Models

We consider two different approaches to learning a
temporal dependency parser: a shift-reduce model
(Nivre, 2008) and a graph-based model (McDonald
et al., 2005). Both models take as input a sequence
of event words and produce as output a tree structure
where the events are linked via temporal relations.
Formally, a parsing model is a function (W → Π)
where W = w1w2 . . . wn is a sequence of event
words, and π ∈ Π is a dependency tree π = (V,E)
where:

• V = W ∪ {Root}, that is, the vertex set of the
graph is the set of words in W plus an artificial
root node.

• E = {(wh, r, wd) : wh ∈ V,wd ∈ V, r ∈ R =
{BEFORE, AFTER, INCLUDES, IS INCLUDED,
IDENTITY, OVERLAP}}, that is, in the edge set
of the graph, each edge is a link between a de-
pendent word and its head word, labeled with a
temporal relation.

• (wh, r, wd) ∈ E =⇒ wd 6= Root, that is, the
artificial root node has no head.

• (wh, r, wd) ∈ E =⇒ ((w′h, r′, wd) ∈ E =⇒
wh = w

′
h∧ r = r′), that is, for every node there

is at most one head and one relation label.

• E contains no (non-empty) subset of arcs
(wh, ri, wi), (wi, rj , wj), . . . , (wk, rl, wh), that
is, there are no cycles in the graph.

90



SHIFT Move all of L2 and the head of Q onto L1
([a1 . . . ai], [b1 . . . bj ], [wkwk+1 . . .], E) → ([a1 . . . aib1 . . . bjwk], [], [wk+1 . . .], E)

NO-ARC Move the head of L1 to the head of L2
([a1 . . . aiai+1], [b1 . . . bj ], Q,E) → ([a1 . . . ai], [ai+1b1 . . . bj ], Q,E)

LEFT-ARC Create a relation where the head of L1 depends on the head of Q
Not applicable if ai+1 is the root or already has a head, or if there is a path connecting wk and ai+1
([a1 . . . aiai+1], [b1 . . . bj ], [wk . . .], E) → ([a1 . . . ai], [ai+1b1 . . . bj ], [wk . . .], E ∪ (wk, r, ai+1)

RIGHT-ARC Create a relation where the head of Q depends on the head of L1
Not applicable if wk is the root or already has a head, or if there is a path connecting wk and ai+1
([a1 . . . aiai+1], [b1 . . . bj ], [wk . . .], E) → ([a1 . . . ai], [ai+1b1 . . . bj ], [wk . . .], E ∪ (ai+1, r, wk)

Table 1: Transition system for Covington-style shift-reduce dependency parsers.

4.1 Shift-Reduce Parsing Model

Shift-reduce dependency parsers start with an input
queue of unlinked words, and link them into a tree
by repeatedly choosing and performing actions like
shifting a node to a stack, or popping two nodes from
the stack and linking them. Shift-reduce parsers are
typically defined in terms of configurations and a tran-
sition system, where the configurations describe the
current internal state of the parser, and the transition
system describes how to get from one state to another.
Formally, a deterministic shift-reduce dependency
parser is defined as (C, T,CF , INIT, TREE) where:

• C is the set of possible parser configurations ci

• T ⊆ (C → C) is the set of transitions ti from
one configuration cj to another cj+1 allowed by
the parser

• INIT ∈ (W → C) is a function from the input
words to an initial parser configuration

• CF ⊆ C are the set of final parser configura-
tions cF where the parser is allowed to terminate

• TREE ∈ (CF → Π) is a function that extracts a
dependency tree π from a final parser state cF

Given this formalism and an oracle o ∈ (C → T ),
which can choose a transition given the current con-
figuration of the parser, dependency parsing can be
accomplished by Algorithm 1. For temporal depen-
dency parsing, we adopt the Covington set of transi-
tions (Covington, 2001) as it allows for parsing the
non-projective trees, which may also contain “cross-
ing” edges, that occasionally occur in our annotated
corpus. Our parser is therefore defined as:

Algorithm 1 Deterministic parsing with an oracle.
c← INIT(W )
while c /∈ CF do
t← o(c)
c← t(c)

end while
return TREE(c)

• c = (L1, L2, Q,E) is a parser configuration,
where L1 and L2 are lists for temporary storage,
Q is the queue of input words, and E is the set
of identified edges of the dependency tree.

• T = {SHIFT,NO-ARC,LEFT-ARC,RIGHT-ARC}
is the set of transitions described in Table 1.

• INIT(W ) = ([Root], [], [w1, w2, . . . , wn], ∅)
puts all input words on the queue and the ar-
tificial root on L1.

• CF = {(L1, L2, Q,E) ∈ C : L1 = {W ∪
{Root}}, L2 = Q = ∅} accepts final states
where the input words have been moved off of
the queue and lists and into the edges in E.

• TREE((L1, L2, Q,E)) = (W ∪{Root}, E) ex-
tracts the final dependency tree.

The oracle o is typically defined as a machine learn-
ing classifier, which characterizes a parser configu-
ration c in terms of a set of features. For temporal
dependency parsing, we learn a Support Vector Ma-
chine classifier (Yamada and Matsumoto, 2003) using
the features described in Section 5.

4.2 Graph-Based Parsing Model
One shortcoming of the shift-reduce dependency
parsing approach is that each transition decision

91



Figure 2: A setting for the graph-based parsing model: an initial dense graph G (left) with edge scores SCORE(e). The
resulting dependency tree as a spanning tree with the highest score over the edges (right).

made by the model is final, and cannot be revisited to
search for more globally optimal trees. Graph-based
models are an alternative dependency parsing model,
which assembles a graph with weighted edges be-
tween all pairs of words, and selects the tree-shaped
subset of this graph that gives the highest total score
(Fig. 2). Formally, a graph-based parser follows
Algorithm 2, where:

• W ′ = W ∪ {Root}

• SCORE ∈ ((W ′×R×W ) → <) is a function
for scoring edges

• SPANNINGTREE is a function for selecting a
subset of edges that is a tree that spans over all
the nodes of the graph.

Algorithm 2 Graph-based dependency parsing
E ← {(e, SCORE(e)) : e ∈ (W ′×R×W ))}
G← (W ′, E)
return SPANNINGTREE(G)

The SPANNINGTREE function is usually defined
using one of the efficient search techniques for find-
ing a maximum spanning tree. For temporal depen-
dency parsing, we use the Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967) which
solves this problem by iteratively selecting the edge
with the highest weight and removing edges that
would create cycles. The result is the globally op-
timal maximum spanning tree for the graph (Geor-
giadis, 2003).

The SCORE function is typically defined as a ma-
chine learning model that scores an edge based on a
set of features. For temporal dependency parsing, we
learn a model to predict edge scores via the Margin
Infused Relaxed Algorithm (MIRA) (Crammer and
Singer, 2003; Crammer et al., 2006) using the set of
features defined in Section 5.

5 Feature Design

The proposed parsing algorithms both rely on ma-
chine learning methods. The shift-reduce parser
(SRP) trains a machine learning classifier as the or-
acle o ∈ (C → T ) to predict a transition t from a
parser configuration c = (L1, L2, Q,E), using node
features such as the heads of L1, L2 and Q, and
edge features from the already predicted temporal
relations in E. The graph-based maximum spanning
tree (MST) parser trains a machine learning model
to predict SCORE(e) for an edge e = (wi, rj , wk),
using features of the nodes wi and wk. The full set
of features proposed for both parsing models, de-
rived from the state-of-the-art systems for temporal
relation labeling, is presented in Table 2. Note that
both models share features that look at the nodes,
while only the shift-reduce parser has features for
previously classified edges.

6 Evaluations

Evaluations were performed using 10-fold cross-
validation on the fables annotated in Section 3. The
corpus contains 100 fables, a total of 14,279 tokens
and a total of 1136 annotated temporal relations. As

92



Feature SRP MST
Word

√∗ √∗

Lemma
√∗ √∗

Part of speech (POS) tag
√∗ √∗

Suffixes
√∗ √∗

Syntactically governing verb
√∗ √∗

Governing verb lemma
√∗ √∗

Governing verb POS tag
√∗ √∗

Governing verb POS suffixes
√∗ √∗

Prepositional phrase occurrence
√∗ √∗

Dominated by auxiliary verb?
√∗ √∗

Dominated by modal verb?
√∗ √∗

Temporal signal word is nearby?
√∗ √∗

Head word lemma
√∗ √∗

Temporal relation labels of ai and its
leftmost and rightmost dependents

√

Temporal relation labels of ai−1’s
leftmost and rightmost dependents

√

Temporal relation labels of b1 and its
leftmost and rightmost dependents

√

Table 2: Features for the shift-reduce parser (SRP) and the
graph-based maximum spanning tree (MST) parser. The√∗ features are extracted from the heads of L1, L2 and Q
for SRP and from each node of the edge for MST.

only 40 instances of OVERLAP relations were an-
notated when neither INCLUDES nor IS INCLUDED
label matched, for evaluation purposes all instances
of these relations were merged into the temporally
coarse OVERLAP relation. Thus, the total number of
OVERLAP relations in the corpus grew from 40 to
258 annotations in total.

To evaluate the parsing models (SRP and MST)
we proposed two baselines. Both are based on the
assumption of linear temporal structures of narratives
as the temporal ordering process that was evidenced
by studies in human text rewriting (Hickmann, 2003).
The proposed baselines are:

• LinearSeq: A model that assumes all events
occur in the order they are written, adding links
between each pair of adjacent events, and label-
ing all links with the relation BEFORE.

• ClassifySeq: A model that links each pair of
adjacent events, but trains a pair-wise classifier
to predict the relation label for each pair. The

classifier is a support vector machine trained us-
ing the same features as the MST parser. This is
an approximation of prior work, where the pairs
of events to classify with a temporal relation
were given as an input to the system. (Note that
Section 6.2 will show that for our corpus, apply-
ing the model only to adjacent pairs of events
is quite competitive for just getting the basic
unlabeled link structure right.)

The Shift-Reduce parser (SRP; Section 4.1) and the
graph-based, maximum spanning tree parser (MST;
Section 4.2) are compared to these baselines.

6.1 Evaluation Criteria and Metrics

Model performance was evaluated using standard
evaluation criteria for parser evaluations:

Unlabeled Attachment Score (UAS) The fraction
of events whose head events were correctly predicted.
This measures whether the correct pairs of events
were linked, but not if they were linked by the correct
relations.

Labeled Attachment Score (LAS) The fraction
of events whose head events were correctly pre-
dicted with the correct relations. This measures both
whether the correct pairs of events were linked and
whether their temporal ordering is correct.

Tree Edit Distance In addition to the UAS and
LAS the tree edit distance score has been recently in-
troduced for evaluating dependency structures (Tsar-
faty et al., 2011). The tree edit distance score
for a tree π is based on the following operations
λ ∈ Λ : Λ = {DELETE, INSERT, RELABEL}:

• λ =DELETE delete a non-root node v in π with
parent u, making the children of v the children
of u, inserted in the place of v as a subsequence
in the left-to-right order of the children of u.

• λ =INSERT insert a node v as a child of u in
π making it the parent of a consecutive subse-
quence of the children of u.

• λ =RELABEL change the label of node v in π

Any two trees π1 and π2 can be turned one into an-
other by a sequence of edit operations {λ1, ..., λn}.

93



UAS LAS UTEDS LTEDS
LinearSeq 0.830 0.581 0.689 0.549
ClassifySeq 0.830 0.581 0.689 0.549
MST 0.837 0.614∗ 0.710 0.571
SRP 0.830 0.647∗† 0.712 0.596∗

Table 3: Performance levels of temporal structure pars-
ing methods. A ∗ indicates that the model outperforms
LinearSeq and ClassifiedSeq at p < 0.01 and a † indicates
that the model outperforms MST at p < 0.05.

Taking the shortest such sequence, the tree edit dis-
tance is calculated as the sum of the edit operation
costs divided by the size of the tree (i.e. the number
of words in the sentence). For temporal dependency
trees, we assume each operation costs 1.0. The fi-
nal score subtracts the edit distance from 1 so that
a perfect tree has score 1.0. The labeled tree edit
distance score (LTEDS) calculates sequences over
the tree with all its labeled temporal relations, while
the unlabeled tree edit distance score (UTEDS) treats
all edges as if they had the same label.

6.2 Results

Table 3 shows the results of the evaluation. The
unlabeled attachment score for the LinearSeq base-
line was 0.830, suggesting that annotators were most
often linking adjacent events. At the same time,
the labeled attachment score was 0.581, indicating
that even in fables, the stories are not simply linear,
that is, there are many relations other than BEFORE.
The ClassifySeq baseline performs identically to the
LinearSeq baseline, which shows that the simple pair-
wise classifier was unable to learn anything beyond
predicting all relations as BEFORE.

In terms of labeled attachment score, both de-
pendency parsing models outperformed the base-
line models – the maximum spanning tree parser
achieved 0.614 LAS, and the shift-reduce parser
achieved 0.647 LAS. The shift-reduce parser also
outperformed the baseline models in terms of labeled
tree edit distance, achieving 0.596 LTEDS vs. the
baseline 0.549 LTEDS. These results indicate that de-
pendency parsing models are a good fit to our whole-
story timeline extraction task.

Finally, in comparing the two different depen-
dency parsing models, we observe that the shift-
reduce parser outperforms the maximum spanning

Error Type Num. %
OVERLAP→ BEFORE 24 43.7
Attach to further head 18 32.7
Attach to nearer head 6 11.0
Other types of errors 7 12.6

Total 55 100

Table 4: Error distribution from the analysis of 55 errors
of the Shift-Reduce parsing model.

tree parser in terms of labeled attachment score
(0.647 vs. 0.614). It has been argued that graph-
based models like the maximum spanning tree parser
should be able to produce more globally consistent
and correct dependency trees, yet we do not observe
that here. A likely explanation for this phenomenon
is that the shift-reduce parsing model allows for fea-
tures describing previous parse decisions (similar to
the incremental nature of human parse decisions),
while the joint nature of the maximum spanning tree
parser does not.

6.3 Error Analysis
To better understand the errors our model is still mak-
ing, we examined two folds (55 errors in total in
20% of the evaluation data) and identified the major
categories of errors:

• OVERLAP→ BEFORE: The model predicts the
correct head, but predicts its label as BEFORE,
while the correct label is OVERLAP.

• Attach to further head: The model predicts
the wrong head, and predicts as the head an
event that is further away than the true head.

• Attach to nearer head: The model predicts the
wrong head, and predicts as the head an event
that is closer than the true head.

Table 4 shows the distribution of the errors over these
categories. The two most common types of errors,
OVERLAP → BEFORE and Attach to further head,
account for 76.4% of all the errors.

The most common type of error is predicting
a BEFORE relation when the correct answer is an
OVERLAP relation. Figure 3 shows an example of
such an error, where the model predicts that the
Spendthrift stood before he saw, while the anno-
tator indicates that the seeing happened during the

94



Figure 3: An OVERLAP → BEFORE parser error. True
links are solid lines; the parser error is the dotted line.

Figure 4: Parser errors attaching to further away heads.
True links are solid lines; parser errors are dotted lines.

time in which he was standing. An analysis of these
OVERLAP→ BEFORE errors suggests that they occur
in scenarios like this one, where the duration of one
event is significantly longer than the duration of an-
other, but there are no direct cues for these duration
differences. We also observe these types of errors
when one event has many sub-events, and therefore
the duration of the main event typically includes the
durations of all the sub-events. It might be possible
to address these kinds of errors by incorporating auto-
matically extracted event duration information (Pan
et al., 2006; Gusev et al., 2011).

The second most common error type of the model
is the prediction of a head event that is further away
than the head identified by the annotators. Figure 4
gives an example of such an error, where the model
predicts that the gathering includes the smarting, in-
stead of that the gathering includes the stung. The
second error in the figure is also of the same type.
In 65% of the cases where this type of error occurs,
it occurs after the parser had already made a label
classification error such as BEFORE → OVERLAP.
So these errors may be in part due to the sequen-
tial nature of shift-reduce parsing, where early errors
propagate and cause later errors.

7 Discussion and Conclusions

In this article, we have presented an approach to tem-
poral information extraction that represents the time-

line of a story as a temporal dependency tree. We
have constructed an evaluation corpus where such
temporal dependencies have been annotated over a
set of 100 children’s stories. We have introduced two
dependency parsing techniques for extracting story
timelines and have shown that both outperform a rule-
based baseline and a prior-work-inspired pair-wise
classification baseline. Comparing the two depen-
dency parsing models, we have found that a shift-
reduce parser, which more closely mirrors the incre-
mental processing of our human annotators, outper-
forms a graph-based maximum spanning tree parser.
Our error analysis of the shift-reduce parser revealed
that being able to estimate differences in event dura-
tions may play a key role in improving parse quality.

We have focused on children’s stories in this study,
in part because they typically have simpler temporal
structures (though not so simple that our rule-based
baseline could parse them accurately). In most of our
fables, there were only one or two characters with at
most one or two simultaneous sequences of actions.
In other domains, the timeline of a text is likely to
be more complex. For example, in clinical records,
descriptions of patients may jump back and forth
between the patient history, the current examination,
and procedures that have not yet happened.

In future work, we plan to investigate how to best
apply the dependency structure approach to such
domains. One approach might be to first group
events into their narrative containers (Pustejovsky
and Stubbs, 2011), for example, grouping together all
events linked to the time of a patient’s examination.
Then within each narrative container, our dependency
parsing approach could be applied. Another approach
might be to join the individual timeline trees into a
document-wide tree via discourse relations or rela-
tions to the document creation time. Work on how
humans incrementally process such timelines in text
may help to decide which of these approaches holds
the most promise.

Acknowledgements

We would like to thank the anonymous reviewers
for their constructive comments. This research was
partially funded by the TERENCE project (EU FP7-
257410) and the PARIS project (IWT SBO 110067).

95



References
[Amigó et al.2011] Enrique Amigó, Javier Artiles, Qi Li,

and Heng Ji. 2011. An evaluation framework for aggre-
gated temporal information extraction. In SIGIR-2011
Workshop on Entity-Oriented Search.

[Artiles et al.2011] Javier Artiles, Qi Li, Taylor Cas-
sidy, Suzanne Tamang, and Heng Ji. 2011.
CUNY BLENDER TAC-KBP2011 temporal slot fill-
ing system description. In Text Analytics Conference
(TAC2011).

[Bethard and Martin2007] Steven Bethard and James H.
Martin. 2007. CU-TMP: Temporal relation classifica-
tion using syntactic and semantic features. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 129–132, Prague,
Czech Republic, June. ACL.

[Bethard et al.2007] Steven Bethard, James H. Martin, and
Sara Klingenstein. 2007. Finding temporal structure in
text: Machine learning of syntactic temporal relations.
International Journal of Semantic Computing (IJSC),
1(4):441–458, 12.

[Bethard et al.2012] Steven Bethard, Oleksandr
Kolomiyets, and Marie-Francine Moens. 2012.
Annotating narrative timelines as temporal dependency
structures. In Proceedings of the International
Conference on Linguistic Resources and Evaluation,
Istanbul, Turkey, May. ELRA.

[Boguraev and Ando2005] Branimir Boguraev and
Rie Kubota Ando. 2005. TimeBank-driven TimeML
analysis. In Annotating, Extracting and Reasoning
about Time and Events. Springer.

[Bramsen et al.2006] P. Bramsen, P. Deshpande, Y.K. Lee,
and R. Barzilay. 2006. Inducing temporal graphs.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 189–
198. ACL.

[Brewer and Lichtenstein1982] William F. Brewer and Ed-
ward H. Lichtenstein. 1982. Stories are to entertain: A
structural-affect theory of stories. Journal of Pragmat-
ics, 6(5-6):473 – 486.

[Chambers and Jurafsky2008] N. Chambers and D. Juraf-
sky. 2008. Jointly combining implicit constraints im-
proves temporal ordering. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 698–706. ACL.

[Cheng et al.2007] Yuchang Cheng, Masayuki Asahara,
and Yuji Matsumoto. 2007. NAIST.Japan: Tempo-
ral relation identification using dependency parsed tree.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 245–248,
Prague, Czech Republic, June. ACL.

[Chu and Liu1965] Y. J. Chu and T.H. Liu. 1965. On
the shortest arborescence of a directed graph. Science
Sinica, pages 1396–1400.

[Covington2001] M.A. Covington. 2001. A fundamental
algorithm for dependency parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95–102.

[Crammer and Singer2003] K. Crammer and Y. Singer.
2003. Ultraconservative online algorithms for multi-
class problems. Journal of Machine Learning Research,
3:951–991.

[Crammer et al.2006] K. Crammer, O. Dekel, J. Keshet,
S. Shalev-Shwartz, and Y. Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.

[Edmonds1967] J. Edmonds. 1967. Optimum branchings.
Journal of Research of the National Bureau of Stan-
dards, pages 233–240.

[Georgiadis2003] L. Georgiadis. 2003. Arborescence op-
timization problems solvable by Edmonds’ algorithm.
Theoretical Computer Science, 301(1-3):427–437.

[Gupta and Ji2009] Prashant Gupta and Heng Ji. 2009.
Predicting unknown time arguments based on cross-
event propagation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, ACLShort ’09, pages
369–372, Stroudsburg, PA, USA. ACL.

[Gusev et al.2011] Andrey Gusev, Nathanael Chambers,
Divye Raj Khilnani, Pranav Khaitan, Steven Bethard,
and Dan Jurafsky. 2011. Using query patterns to learn
the duration of events. In Proceedings of the Interna-
tional Conference on Computational Semantics, pages
145–154.

[Hayes and Krippendorff2007] A.F. Hayes and K. Krip-
pendorff. 2007. Answering the call for a standard
reliability measure for coding data. Communication
Methods and Measures, 1(1):77–89.

[Hickmann2003] Maya Hickmann. 2003. Children’s Dis-
course: Person, Space and Time Across Languages.
Cambridge University Press, Cambridge, UK.

[Johnson-Laird1980] P.N. Johnson-Laird. 1980. Men-
tal models in cognitive science. Cognitive Science,
4(1):71–115.

[Krippendorff2004] K. Krippendorff. 2004. Content anal-
ysis: An introduction to its methodology. Sage Publica-
tions, Inc.

[Linguistic Data Consortium2005] Linguistic Data Con-
sortium. 2005. ACE (Automatic Content Extraction)
English annotation guidelines for events version 5.4.3
2005.07.01.

[Llorens et al.2010] Hector Llorens, Estela Saquete, and
Borja Navarro. 2010. TIPSem (English and Spanish):
Evaluating CRFs and semantic roles in TempEval-2. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284–291, Uppsala, Sweden,
July. ACL.

96



[McDonald et al.2005] R. McDonald, F. Pereira, K. Rib-
arov, and J. Hajič. 2005. Non-projective dependency
parsing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523–530. ACL.

[McIntyre and Lapata2009] N. McIntyre and M. Lapata.
2009. Learning to tell tales: A data-driven approach to
story generation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
217–225. ACL.

[Nivre2008] J. Nivre. 2008. Algorithms for determinis-
tic incremental dependency parsing. Computational
Linguistics, 34(4):513–553.

[Pan et al.2006] Feng Pan, Rutu Mulkar, and Jerry R.
Hobbs. 2006. Learning event durations from event
descriptions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics, pages 393–400, Sydney, Australia, July. ACL.

[Pustejovsky and Stubbs2011] J. Pustejovsky and
A. Stubbs. 2011. Increasing informativeness in
temporal annotation. In Proceedings of the 5th
Linguistic Annotation Workshop, pages 152–160. ACL.

[Pustejovsky et al.2003a] James Pustejovsky, José
Castaño, Robert Ingria, Roser Saurý, Robert
Gaizauskas, Andrea Setzer, and Graham Katz. 2003a.
TimeML: Robust specification of event and temporal
expressions in text. In Proceedings of the Fifth
International Workshop on Computational Semantics
(IWCS-5), Tilburg.

[Pustejovsky et al.2003b] James Pustejovsky, Patrick
Hanks, Roser Saurý, Andrew See, Robert Gaizauskas,
Andrea Setzer, Dragomir Radev, Beth Sundheim,
David Day, Lisa Ferro, and Marcia Lazo. 2003b.
The TimeBank corpus. In Proceedings of Corpus
Linguistics, pages 647–656.

[Tsarfaty et al.2011] R. Tsarfaty, J. Nivre, and E. Ander-
sson. 2011. Evaluating dependency parsing: Robust
and heuristics-free cross-annotation evaluation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 385–396. ACL.

[UzZaman and Allen2010] Naushad UzZaman and James
Allen. 2010. TRIPS and TRIOS system for TempEval-
2: Extracting temporal information from text. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 276–283, Uppsala, Sweden, July.
ACL.

[Verhagen et al.2007] Marc Verhagen, Robert Gaizauskas,
Frank Schilder, Graham Katz, and James Pustejovsky.
2007. SemEval2007 Task 15: TempEval temporal rela-

tion identification. In SemEval-2007: 4th International
Workshop on Semantic Evaluations.

[Verhagen et al.2010] Marc Verhagen, Roser Saurı́, Tom-
maso Caselli, and James Pustejovsky. 2010. SemEval-
2010 Task 13: TempEval-2. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ’10, pages 57–62, Stroudsburg, PA, USA. ACL.

[Yamada and Matsumoto2003] H. Yamada and Y. Mat-
sumoto. 2003. Statistical dependency analysis with
support vector machines. In Proceedings of IWPT.

[Yoshikawa et al.2009] K. Yoshikawa, S. Riedel, M. Asa-
hara, and Y. Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
405–413. ACL.

97


