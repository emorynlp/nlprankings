



















































Semi-Supervised Learning of Concatenative Morphology


Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 78–86,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics

Semi-supervised learning of concatenative morphology

Oskar Kohonen and Sami Virpioja and Krista Lagus
Aalto University School of Science and Technology

Adaptive Informatics Research Centre
P.O. Box 15400, FI-00076 AALTO, Finland

{oskar.kohonen,sami.virpioja,krista.lagus}@tkk.fi

Abstract

We consider morphology learning in a
semi-supervised setting, where a small
set of linguistic gold standard analyses is
available. We extend Morfessor Base-
line, which is a method for unsupervised
morphological segmentation, to this task.
We show that known linguistic segmenta-
tions can be exploited by adding them into
the data likelihood function and optimiz-
ing separate weights for unlabeled and la-
beled data. Experiments on English and
Finnish are presented with varying amount
of labeled data. Results of the linguis-
tic evaluation of Morpho Challenge im-
prove rapidly already with small amounts
of labeled data, surpassing the state-of-
the-art unsupervised methods at 1000 la-
beled words for English and at 100 labeled
words for Finnish.

1 Introduction

Morphological analysis is required in many natu-
ral language processing problems. Especially, in
agglutinative and compounding languages, where
each word form consists of a combination of stems
and affixes, the number of unique word forms in
a corpus is very large. This leads to problems in
word-based statistical language modeling: Even
with a large training corpus, many of the words en-
countered when applying the model did not occur
in the training corpus, and thus there is no infor-
mation available on how to process them. Using
morphological units, such as stems and affixes, in-
stead of complete word forms alleviates this prob-
lem. Unfortunately, for many languages morpho-
logical analysis tools either do not exist or they
are not freely available. In many cases, the prob-
lems of availability also apply to morphologically
annotated corpora, making supervised learning in-
feasible.

In consequence, there has been a need for ap-
proaches for morphological processing that would
require little language-dependent resources. Due
to this need, as well as the general interest in
language acquisition and unsupervised language
learning, the research on unsupervised learning
of morphology has been active during the past
ten years. Especially, methods that perform mor-
phological segmentation have been studied exten-
sively (Goldsmith, 2001; Creutz and Lagus, 2002;
Monson et al., 2004; Bernhard, 2006; Dasgupta
and Ng, 2007; Snyder and Barzilay, 2008b; Poon
et al., 2009). These methods have shown to pro-
duce results that improve performance in several
applications, such as speech recognition and in-
formation retrieval (Creutz et al., 2007; Kurimo et
al., 2008).

While unsupervised methods often work quite
well across different languages, it is difficult to
avoid biases toward certain kinds of languages and
analyses. For example, in isolating languages, the
average amount of morphemes per word is low,
whereas in synthetic languages the amount may be
very high. Also, different applications may need
a particular bias, for example, not analyzing fre-
quent compound words as consisting of smaller
parts could be beneficial in information retrieval.
In many cases, even a small amount of labeled data
can be used to adapt a method to a particular lan-
guage and task. Methodologically, this is referred
to as semi-supervised learning.

In semi-supervised learning, the learning sys-
tem has access to both labeled and unlabeled data.
Typically, the labeled data set is too small for su-
pervised methods to be effective, but there is a
large amount of unlabeled data available. There
are many different approaches to this class of
problems, as presented by Zhu (2005). One ap-
proach is to use generative models, which spec-
ify a join distribution over all variables in the
model. They can be utilized both in unsupervised

78



and supervised learning. In contrast, discrimina-
tive models only specify the conditional distribu-
tion between input data and labels, and therefore
require labeled data. Both, however, can be ex-
tended to the semi-supervised case. For generative
models, it is, in principle, very easy to use both la-
beled and unlabeled data. For unsupervised learn-
ing one can consider the labels as missing data and
estimate their values using the Expectation Maxi-
mization (EM) algorithm (Dempster et al., 1977).
In the semi-supervised case, some labels are avail-
able, and the rest are considered missing and esti-
mated with EM.

In this paper, we extend the Morfessor Base-
line method for the semi-supervised case. Morfes-
sor (Creutz and Lagus, 2002; Creutz and Lagus,
2005; Creutz and Lagus, 2007, etc.) is one of the
well-established methods for morphological seg-
mentation. It applies a simple generative model.
The basic idea, inspired by the Minimum Descrip-
tion Length principle (Rissanen, 1989), is to en-
code the words in the training data with a lexicon
of morphs, that are segments of the words. The
number of bits needed to encode both the morph
lexicon and the data using the lexicon should be
minimized. Morfessor does not limit the num-
ber of morphemes per word form, making it suit-
able for modeling a large variety of agglutinative
languages irrespective of them being more isolat-
ing or synthetic. We show that the model can be
trained in a similar fashion in the semi-supervised
case as in the unsupervised case. However, with
a large set of unlabeled data, the effect of the su-
pervision on the results tends to be small. Thus,
we add a discriminative weighting scheme, where
a small set of word forms with gold standard ana-
lyzes are used for tuning the respective weights of
the labeled and unlabeled data.

The paper is organized as follows: First, we
discuss related work on semi-supervised learning.
Then we describe the Morfessor Baseline model
and the unsupervised algorithm, followed by our
semi-supervised extension. Finally, we present ex-
perimental results for English and Finnish using
the Morpho Challenge data sets (Kurimo et al.,
2009).

1.1 Related work

There is surprisingly little work that consider im-
proving the unsupervised models of morphology
with small amounts of annotated data. In the

related tasks that deal with sequential labeling
(word segmentation, POS tagging, shallow pars-
ing, named-entity recognition), semi-supervised
learning is more common.

Snyder and Barzilay (2008a; 2008b) consider
learning morphological segmentation with non-
parametric Bayesian model from multilingual
data. For multilingual settings, they extract 6 139
parallel short phrases from the Hebrew, Arabic,
Aramaic and English bible. Using the aligned
phrase pairs, the model can learn the segmen-
tations for two languages at the same time. In
one of the papers (2008a), they consider also
semi-supervised scenarios, where annotated data
is available either in only one language or both of
the languages. However, the amount of annotated
data is fixed to the half of the full data. This differs
from our experimental setting, where the amount
of unlabeled data is very large and the amount of
labeled data relatively small.

Poon et al. (2009) apply a log-linear, undi-
rected generative model for learning the morphol-
ogy of Arabic and Hebrew. They report results
for the same small data set as Snyder and Barzilay
(2008a) in both unsupervised and semi-supervised
settings. For the latter, they use somewhat smaller
proportions of annotated data, varying from 25%
to 100% of the total data, but the amount of unla-
beled data is still very small. Results are reported
also for a larger 120 000 word Arabic data set, but
only for unsupervised learning.

A problem similar to morphological segmen-
tation is word segmentation for the languages
where orthography does not specify word bound-
aries. However, the amount of labeled data is
usually large, and unlabeled data is just an addi-
tional source of information. Li and McCallum
(2005) apply a semi-supervised approach to Chi-
nese word segmentation where unlabeled data is
utilized for forming word clusters, which are then
used as features for a supervised classifier. Xu
et al. (2008) adapt a Chinese word segmentation
specifically to a machine translation task, by using
the indirect supervision from a parallel corpus.

2 Method

We present an extension of the Morfessor Baseline
method to the semi-supervised setting. Morfes-
sor Baseline is based on a generative probabilis-
tic model. It is a method for modeling concatena-
tive morphology, where the morphs—i.e., the sur-

79



face forms of morphemes—of a word are its non-
overlapping segments. The model parameters θ
encode a morph lexicon, which includes the prop-
erties of the morphs, such as their string represen-
tations. Each morph m in the lexicon has a proba-
bility of occurring in a word, P (M = m |θ).1 The
probabilities are assumed to be independent. The
model uses a prior P (θ), derived using the Min-
imum Description Length (MDL) principle, that
controls the complexity of the model. Intuitively,
the prior assigns higher probability to models that
store fewer morphs, where a morph is considered
stored if P (M = m |θ) > 0. During model learn-
ing, θ is optimized to maximize the posterior prob-
ability:

θMAP = arg max
θ

P (θ|DW )

= arg max
θ

{
P (θ)P (DW |θ)

}
, (1)

where DW includes the words in the training
data. In this section, we first consider sepa-
rately the likelihood P (DW |θ) and the prior P (θ)
used in Morfessor Baseline. Then we describe
the algorithms, first unsupervised and then semi-
supervised, for finding optimal model parameters.
Last, we shortly discuss the algorithm for seg-
menting new words after the model training.

2.1 Likelihood

The latent variable of the model, Z =
(Z1, . . . , Z|DW |), contains the analyses of the
words in the training data DW . An instance of
a single analysis for the j:th word is a sequence of
morphs, zj = (mj1, . . . , mj|zj |). During training,
each word wj is assumed to have only one possible
analysis. Thus, instead of using the joint distribu-
tion P (DW , Z |θ), we need to use the likelihood
function only conditioned on the analyses of the
observed words, P (DW |Z, θ). The conditional
likelihood is

P (DW |Z = z, θ)

=
|DW |∏
j=1

P (W = wj |Z = z, θ)

=
|DW |∏
j=1

|zj |∏
i=1

P (M = mji |θ), (2)

where mij is the i:th morph in word wj .

1We denote variables with uppercase letters and their in-
stances with lowercase letters.

2.2 Priors

Morfessor applies Maximum A Posteriori (MAP)
estimation, so priors for the model parameters
need to be defined. The parameters θ of the model
are:

• Morph type count, or the size of the morph
lexicon, µ ∈ Z+

• Morph token count, or the number of morphs
tokens in the observed data, ν ∈ Z+

• Morph strings (σ1, . . . , σµ), σi ∈ Σ∗

• Morph counts (τ1, . . . , τµ), τi ∈ {1, . . . , ν},∑
i τi = ν. Normalized with ν, these give

the probabilities of the morphs.

MDL-inspired and non-informative priors have
been preferred. When using such priors, morph
type count and morph token counts can be ne-
glected when optimizing the model. The morph
string prior is based on length distribution P (L)
and distribution P (C) of characters over the char-
acter set Σ, both assumed to be known:

P (σi) = P (L = |σi|)
|σi|∏
j=1

P (C = σij) (3)

We use the implicit length prior (Creutz and La-
gus, 2005), which is obtained by removing P (L)
and using end-of-word mark as an additional char-
acter in P (C). For morph counts, the non-
informative prior

P (τ1, . . . , τµ) = 1/
(

ν − 1
µ− 1

)
(4)

gives equal probability to each possible combina-
tion of the counts when µ and ν are known, as
there are

(
ν−1
µ−1

)
possible ways to choose µ positive

integers that sum up to ν.

2.3 Unsupervised learning

In principle, unsupervised learning can be per-
formed by looking for the MAP estimate with the
EM-algorithm. In the case of Morfessor Baseline,
this is problematic, because the prior only assigns
higher probability to lexicons where fewer morphs
have nonzero probabilities. The EM-algorithm has
the property that it will not assign a zero probabil-
ity to any morph, that has a nonzero likelihood in
the previous step, and this will hold for all morphs

80



that initially have a nonzero probability. In con-
sequence, Morfessor Baseline instead uses a local
search algorithm, which will assign zero probabil-
ity to a large part of the potential morphs. This
is memory-efficient, since only the morphs with
nonzero probabilities need to be stored in mem-
ory. The training algorithm of Morfessor Base-
line, described by Creutz and Lagus (2005), tries
to minimize the cost function

L(θ, z, DW ) = − lnP (θ)− lnP (DW | z, θ)
(5)

by testing local changes to z, modifying the pa-
rameters according to each change, and selecting
the best one. More specifically, one word is pro-
cessed at a time, and the segmentation that min-
imizes the cost function with the optimal model
parameters is selected:

z
(t+1)
j = arg min

zj

{
min

θ
L(θ, z(t), DW )

}
. (6)

Next, the parameters are updated:

θ(t+1) = arg min
θ

{
L(θ, z(t+1), DW )

}
. (7)

As neither of the steps can increase the cost func-
tion, this will converge to a local optimum. The
initial parameters are obtained by adding all the
words into the morph lexicon. Due to the context
independence of the morphs within a word, the op-
timal analysis for a segment does not depend on
in which context the segment appears. Thus, it is
possible to encode z as a binary tree-like graph,
where the words are the top nodes and morphs the
leaf nodes. For each word, every possible split into
two morphs is tested in addition to no split. If the
word is split, the same test is applied recursively
to its parts. See, e.g., Creutz and Lagus (2005) for
more details and pseudo-code.

2.4 Semi-supervised learning

A straightforward way to do semi-supervised
learning is to fix the analyses z for the labeled ex-
amples. Early experiments indicated that this has
little effect on the results. The Morfessor Baseline
model only contains local parameters for morphs,
and relies on the bias given by its prior to guide
the amount of segmentation. Therefore, it may not
be well suited for semi-supervised learning. The
labeled data affects only the morphs that are found
in the labeled data, and even their analyses can be

overwhelmed by a large amount of unsupervised
data and the bias of the prior.

We suggest a fairly simple solution to this by
introducing extra parameters that guide the more
general behavior of the model. The amount of
segmentation is mostly affected by the balance
between the prior and the model. The Morfes-
sor Baseline model has been developed to ensure
this balance is sensible. However, the labeled
data gives a strong source of information regarding
the amount of segmentation preferred by the gold
standard. We can utilize this information by intro-
ducing the weight α on the likelihood. To address
the problem of labeled data being overwhelmed by
the large amount of unlabeled data we introduce a
second weight β on the likelihood for the labeled
data. These weights are optimized on a separate
held-out set. Thus, instead of optimizing the MAP
estimate, we minimize the following function:

L(θ, z, DW , DW 7→A) =
− lnP (θ)
− α× lnP (DW | z, θ)
− β × lnP (DW 7→A | z, θ) (8)

The labeled training set DW 7→A may include al-
ternative analyses for some of the words. Let
A(wj) = {aj1, . . . , ajk} be the set of known anal-
yses for word wj . Assuming the training samples
are independent, and giving equal weight for each
analysis, the likelihood of the labeled data would
be

P (DW 7→A |θ)

=
|DW 7→A|∏

j=1

∏
ajk∈A(wj)

|ajk|∏
i=1

P (M = mjki |θ). (9)

However, when the analyses of the words are
fixed, the product over alternative analyses in A
is problematic, because the model cannot select
several of them at the same time. A sum over
A(wj):s would avoid this problem, but then the
logarithm of the likelihood function becomes non-
trivial (i.e., logarithm of sum of products) and too
slow to calculate during the training. Instead, we
use the hidden variable Z to select only one anal-
ysis also for the labeled samples, but now with the
restriction that Zj ∈ A(wj). The likelihood func-
tion for DW 7→A is then equivalent to Equation 2.
Because the recursive algorithm search assumes
that a string is segmented in the same way irre-
spective of its context, the labeled data can still

81



get zero probabilities. In practice, zero probabil-
ities in the labeled data likelihood are treated as
very large, but not infinite, costs.

2.5 Segmenting new words

After training the model, a Viterbi-like algorithm
can be applied to find the optimal segmentation
of each word. As proposed by Virpioja and Ko-
honen (2009), also new morph types can be al-
lowed by utilizing an approximate cost of adding
them to the lexicon. As this enables reasonable re-
sults also when the training data is small, we use a
similar technique. The cost is calculated from the
decrease in the probabilities given in Equations 3
and 4 when a new morph is assumed to be in the
lexicon.

3 Experiments

In the experiments, we compare six different vari-
ants of the Morfessor Baseline algorithm:

• Unsupervised: The classic, unsupervised
Morfessor baseline.

• Unsupervised + weighting: A held-out set
is used for adjusting the weight of the likeli-
hood α. When α = 1 the method is equiva-
lent to the unsupervised baseline. The main
effect of adjusting α is to control how many
segments per word the algorithm prefers.
Higher α leads to fewer and lower α to more
segments per word.

• Supervised: The semi-supervised method
trained with only the labeled data.

• Supervised + weighting: As above, but the
weight of the likelihood β is optimized on
the held-out set. The weight can only af-
fect which segmentations are selected from
the possible alternative segmentations in the
labeled data.

• Semi-supervised: The semi-supervised
method trained with both labeled and
unlabeled data.

• Semi-supervised + weighting: As above,
but the parameters α and β are optimized us-
ing the the held-out set.

All variations are evaluated using the linguistic
gold standard evaluation of Morpho Challenge

2009. For supervised and semi-supervised meth-
ods, the amount of labeled data is varied be-
tween 100 and 10 000 words, whereas the held-
out set has 500 gold standard analyzes. To obtain
precision-recall curves, we calculated weighted
F0.5 and F2 scores in addition to the normal F1
score. The parameters α and β were optimized
also for those.

3.1 Data and evaluation

We used the English and Finnish data sets from
Competition 1 of Morpho Challenge 2009 (Ku-
rimo et al., 2009). Both are extracted from a
three million sentence corpora. For English, there
were 62 185 728 word tokens and 384 903 word
types. For Finnish, there were 36 207 308 word
tokens and 2 206 719 word types. The complexity
of Finnish morphology is indicated by the almost
ten times larger number of word types than in En-
glish, while the number of word tokens is smaller.

We applied also the evaluation method of the
Morpho Challenge 2009: The results of the mor-
phological segmentation were compared to a lin-
guistic gold standard analysis. Precision measures
whether the words that share morphemes in the
proposed analysis have common morphemes also
in the gold standard, and recall measures the op-
posite. The final score to optimize was F-measure,
i.e, the harmonic mean of the precision and re-
call.2 In addition to the unweighted F1 score, we
have applied F2 and F0.5 scores, which give more
weight to recall and precision, respectively.

Finnish gold standards are based on FINT-
WOL morphological analyzer from Lingsoft, Inc.,
that applies the two-level model by Koskenniemi
(1983). English gold standards are from the
CELEX English database. The final test sets are
the same as in Morpho Challenge, based on 10 000
English word forms and 200 000 Finnish word
forms. The test sets are divided into ten parts for
calculating deviations and statistical significances.
For parameter tuning, we applied a small held-out
set containing 500 word forms that were not in-
cluded in the test set.

For supervised and semi-supervised training,
we created sets of five different sizes: 100, 300,
1 000, 3 000, and 10 000. They did not contain any
of the word forms in the final test set, but were
otherwise randomly selected from the words for

2Both the data sets and evaluation scripts are available
from the Morpho Challenge 2009 web page: http://www.
cis.hut.fi/morphochallenge2009/

82



Figure 1: The F-measure for English as a function
of the number of labeled training samples.

which the gold standard analyses were available.
In order to use them for training Morfessor, the
morpheme analyses were converted to segmenta-
tions using the Hutmegs package by Creutz and
Lindén (2004).

3.2 Results

Figure 1 shows a comparison of the unsupervised,
supervised and semi-supervised Morfessor Base-
line for English. It can be seen that optimiz-
ing the likelihood weight α alone does not im-
prove much over the unsupervised case, imply-
ing that the Morfessor Baseline is well suited for
English morphology. Without weighting of the
likelihood function, semi-supervised training im-
proves the results somewhat, but it outperforms
weighted unsupervised model only barely. With
weighting, however, semi-supervised training im-
proves the results significantly already for only
100 labeled training samples. For comparison,
in Morpho Challenges (Kurimo et al., 2009), the
unsupervised Morfessor Baseline and Morfessor
Categories-MAP by Creutz and Lagus (2007) have
achieved F-measures of 59.84% and 50.50%, re-
spectively, and the all time best unsupervised re-
sult by a method that does not provide alternative
analyses for words is 66.24%, obtained by Bern-
hard (2008).3 This best unsupervised result is sur-
passed by the semi-supervised algorithm at 1000
labeled samples.

As shown in Figure 1, the supervised method
obtains inconsistent scores for English with the

3Better results (68.71%) have been achieved by Monson
et al. (2008), but as they were obtained by combining of
two systems as alternative analyses, the comparison is not as
meaningful.

Figure 2: The F-measure for Finnish as a function
of the number of labeled training samples. The
semi-supervised and unsupervised lines overlap.

smallest training data sizes. The supervised al-
gorithm only knows the morphs in the training
set, and therefore is crucially dependent on the
Viterbi segmentation algorithm for analyzing new
data. Thus, overfitting to some small data sets is
not surprising. At 10 000 labeled training samples
it clearly outperforms the unsupervised algorithm.
The improvement obtained from tuning the weight
β in the supervised case is small.

Figure 2 shows the corresponding results for
Finnish. The optimization of the likelihood weight
gives a large improvement to the F-measure al-
ready in the unsupervised case. This is mainly be-
cause the standard unsupervised Morfessor Base-
line method does not, on average, segment words
into as many segments as would be appropriate for
Finnish. Without weighting, the semi-supervised
method does not improve over the unsupervised
one: The unlabeled training data is so much larger
that the labeled data has no real effect.

For Finnish, the unsupervised Morfessor Base-
line and Categories-MAP obtain F-measures of
26.75% and 44.61%, respectively (Kurimo et al.,
2009). The all time best for an unsupervised
method is 52.45% by Bernhard (2008). With op-
timized likelihood weights, the semi-supervised
Morfessor Baseline achieves higher F-measures
with only 100 labeled training samples. Fur-
thermore, the largest improvement for the semi-
supervised method is achieved already from 1000
labeled training samples. Unlike English, the su-
pervised method is quite a lot worse than the un-
supervised one for small training data. This is
natural because of the more complex morphology

83



Figure 3: Precision-recall graph for English with
varying amount of labeled training data. Parame-
ters α and β have been optimized for three differ-
ent measures: F0.5, F1 and F2 on the held-out set.
Precision and recall values are from the final test
set, error bars indicate one standard deviation.

in Finnish; good results are not achieved just by
knowing the few most common suffixes.

Figures 3 and 4 show precision-recall graphs
of the performance of the semi-supervised method
for English and Finnish. The parameters α and β
have been optimized for three differently weighted
F-measures (F0.5, F1, and F2) on the held-out set.
The weight tells how much recall is emphasized;
F1 is the symmetric F-measure that emphasizes
precision and recall alike. The graphs show that
the more there are labeled training data, the more
constrained the model parameters are: With many
labeled examples, the model cannot be forced to
achieve high precision or recall only. The phe-
nomenon is more evident in the Finnish data (Fig-
ure 3), where the same amount of words contains
more information (morphemes) than in the En-
glish data. Table 1 shows the F0.5, F1 and F2
measures numerically.

Table 2 shows the values for the F1-optimal
weights α and β that were chosen for different
amounts of labeled data using the held-out set. As
even the largest labeled sets are much smaller than
the unlabeled training set, it is natural that β ≫ α.
The small optimal α for Finnish explains why the
difference between unsupervised unweighted and
weighted versions in Figure 2 was so large. Gener-
ally, the more there is labeled data, the smaller β is
needed. A possible increase in overall likelihood
cost is compensated by a smaller α. Finnish with
100 labeled words is an exception; probably a very

Figure 4: Precision-recall graph for Finnish with
varying amount of labeled training data. Param-
eters α and β have been optimized for three dif-
ferent measures: F0.5, F1 and F2 on the held-out
set. Precision and recall values are from the final
test set, error bars indicate one standard deviation,
which here is very small.

high β would end in overlearning of the small set
words at the cost of overall performance.

4 Discussion

The method developed in this paper is a straight-
forward extension of Morfessor Baseline. In the
semi-supervised setting, it should be possible to
develop a generative model that would not require
any discriminative reweighting, but could learn,
e.g., the amount of segmentation from the labeled
data. Moreover, it would be possible to learn the
morpheme labels instead of just the segmentation
into morphs, either within the current model or as
a separate step after the segmentation. We made
initial experiment with a trivial context-free label-
ing: A mapping between the segments and mor-
pheme labels was extracted from the labeled train-
ing data. If some label did not have a correspond-
ing segment, it was appended to the previous la-
bel. E.g., if the labels for “found” are “find V
+PAST”, “found” was mapped to both labels. Af-
ter segmentation, each segment in the test data was
replaced by the most common label or label se-
quence whenever such was available. The results
using training data with 1 000 and 10 000 labeled
samples are shown in Table 3. Although preci-
sions decrease somewhat, recalls improve consid-
erably, and significant gains in F-measure are ob-
tained. A more advanced, context-sensitive label-
ing should perform much better.

84



English
labeled data F0.5 F1 F2

0 69.16 61.05 62.70
100 73.23 65.18 68.30
300 72.98 65.63 68.81

1000 71.86 68.29 69.68
3000 74.34 69.13 72.01

10000 76.04 72.85 73.89
Finnish

labeled data F0.5 F1 F2
0 56.81 49.07 53.95

100 58.96 52.66 57.01
300 59.33 54.92 57.16

1000 61.75 56.38 58.24
3000 63.72 58.21 58.90

10000 66.58 60.26 57.24

Table 1: The F0.5, F1 and F2 measures for the
semi-supervised + weighting method.

English Finnish
labeled data α β α β

0 0.75 - 0.01 -
100 0.75 750 0.01 500
300 1 500 0.005 5000

1000 1 500 0.05 2500
3000 1.75 350 0.1 1000

10000 1.75 175 0.1 500

Table 2: The values for the weights α and β
that the semisupervised algorithm chose for differ-
ent amounts of labeled data when optimizing F1-
measure.

The semi-supervised extension could easily be
applied to the other versions and extensions of
Morfessor, such as Morfessor Categories-MAP
(Creutz and Lagus, 2007) and Allomorfessor (Vir-
pioja and Kohonen, 2009). Especially the model-
ing of allomorphy might benefit from even small
amounts of labeled data, because those allomorphs
that are hardest to find (affixes, stems with irregu-
lar orthographic changes) are often more common
than the easy cases, and thus likely to be found
even from a small labeled data set.

Even without labeling, it will be interesting
to see how well the semi-supervised morphology
learning works in applications such as information
retrieval. Compared to unsupervised learning, we
obtained much higher recall for reasonably good
levels of precision, which should be beneficial to
most applications.

Segmented Labeled
English, D = 1 000
Precision 69.72% 69.30%
Recall 66.92% 72.21%
F-measure 68.29% 70.72%
English, D = 10 000
Precision 77.35% 77.07%
Recall 68.85% 77.78%
F-measure 72.86% 77.42%
Finnish, D = 1 000
Precision 61.03% 58.96%
Recall 52.38% 66.55%
F-measure 56.38% 62.53%
Finnish, D = 10 000
Precision 69.14% 66.90%
Recall 53.40% 74.08%
F-measure 60.26% 70.31%

Table 3: Results of a simple morph labeling after
segmentation with semi-supervised Morfessor.

5 Conclusions

We have evaluated an extension of the Morfessor
Baseline method to semi-supervised morphologi-
cal segmentation. Even with our simple method,
the scores improve far beyond the best unsuper-
vised results. Moreover, already one hundred
known segmentations give significant gain over
the unsupervised method even with the optimized
data likelihood weight.

Acknowledgments

This work was funded by Academy of Finland and
Graduate School of Language Technology in Fin-
land. We thank Mikko Kurimo and Tiina Lindh-
Knuutila for comments on the manuscript, and
Nokia foundation for financial support.

References

Delphine Bernhard. 2006. Unsupervised morpholog-
ical segmentation based on segment predictability
and word segments alignment. In Proceedings of the
PASCAL Challenge Workshop on Unsupervised seg-
mentation of words into morphemes, Venice, Italy.
PASCAL European Network of Excellence.

Delphine Bernhard. 2008. Simple morpheme labelling
in unsupervised morpheme analysis. In Advances in
Multilingual and Multimodal Information Retrieval,
8th Workshop of the CLEF, volume 5152 of Lec-
ture Notes in Computer Science, pages 873–880.
Springer Berlin / Heidelberg.

85



Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the Workshop on Morphological and Phonological
Learning of ACL’02, pages 21–30, Philadelphia,
Pennsylvania, USA.

Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Technical
Report A81, Publications in Computer and Informa-
tion Science, Helsinki University of Technology.

Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.

Mathias Creutz and Krister Lindén. 2004. Morpheme
segmentation gold standards for Finnish and En-
glish. Technical Report A77, Publications in Com-
puter and Information Science, Helsinki University
of Technology.

Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo,
Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Saraçlar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Transactions on Speech and
Language Processing, 5(1):1–29.

Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In the annual conference of the North
American Chapter of the ACL (NAACL-HLT).

Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society, Series B (Methodological), 39(1):1–
38.

John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–189.

Kimmo Koskenniemi. 1983. Two-level morphology: A
general computational model for word-form recog-
nition and production. Ph.D. thesis, University of
Helsinki.

Mikko Kurimo, Mathias Creutz, and Matti Varjokallio.
2008. Morpho Challenge evaluation using a linguis-
tic Gold Standard. In Advances in Multilingual and
MultiModal Information Retrieval, 8th Workshop of
the Cross-Language Evaluation Forum, CLEF 2007,
Budapest, Hungary, September 19-21, 2007, Re-
vised Selected Papers, Lecture Notes in Computer
Science , Vol. 5152, pages 864–873. Springer.

Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.

Wei Li and Andrew McCallum. 2005. Semi-
supervised sequence modeling with syntactic topic
models. In AAAI’05: Proceedings of the 20th na-
tional conference on Artificial intelligence, pages
813–818. AAAI Press.

Christian Monson, Alon Lavie, Jaime Carbonell, and
Lori Levin. 2004. Unsupervised induction of natu-
ral language morphology inflection classes. In Pro-
ceedings of the Workshop of the ACL Special Interest
Group in Computational Phonology (SIGPHON).

Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. ParaMor: Finding paradigms
across morphology. In Advances in Multilingual
and MultiModal Information Retrieval, 8th Work-
shop of the Cross-Language Evaluation Forum,
CLEF 2007, Budapest, Hungary, September 19-21,
2007, Revised Selected Papers, Lecture Notes in
Computer Science , Vol. 5152. Springer.

Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In NAACL ’09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 209–217. Association for Computational Lin-
guistics.

Jorma Rissanen. 1989. Stochastic Complexity in Sta-
tistical Inquiry, volume 15. World Scientific Series
in Computer Science, Singapore.

Benjamin Snyder and Regina Barzilay. 2008a. Cross-
lingual propagation for morphological analysis. In
AAAI’08: Proceedings of the 23rd national con-
ference on Artificial intelligence, pages 848–854.
AAAI Press.

Benjamin Snyder and Regina Barzilay. 2008b. Un-
supervised multilingual learning for morphological
segmentation. In Proceedings of ACL-08: HLT,
pages 737–745, Columbus, Ohio, June. Association
for Computational Linguistics.

Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme analysis with Allomorfessor. In
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.

Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In COLING ’08: Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics, pages 1017–1024, Morristown, NJ, USA. As-
sociation for Computational Linguistics.

Xiaojin Zhu. 2005. Semi-supervised Learning with
Graphs. Ph.D. thesis, CMU. Chapter 11, Semi-
supervised learning literature survey (updated online
version).

86


