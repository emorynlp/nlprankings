



















































Non-Monotonic Sentence Alignment via Semisupervised Learning


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 622–630,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

Non-Monotonic Sentence Alignment via Semisupervised Learning

Xiaojun Quan, Chunyu Kit and Yan Song
Department of Chinese, Translation and Linguistics

City University of Hong Kong, HKSAR, China
{xiaoquan,ctckit,[yansong]}@[student.]cityu.edu.hk

Abstract
This paper studies the problem of non-
monotonic sentence alignment, motivated
by the observation that coupled sentences
in real bitexts do not necessarily occur
monotonically, and proposes a semisuper-
vised learning approach based on two as-
sumptions: (1) sentences with high affinity
in one language tend to have their counter-
parts with similar relatedness in the other;
and (2) initial alignment is readily avail-
able with existing alignment techniques.
They are incorporated as two constraints
into a semisupervised learning framework
for optimization to produce a globally op-
timal solution. The evaluation with real-
world legal data from a comprehensive
legislation corpus shows that while exist-
ing alignment algorithms suffer severely
from non-monotonicity, this approach can
work effectively on both monotonic and
non-monotonic data.

1 Introduction

Bilingual sentence alignment is a fundamental
task to undertake for the purpose of facilitating
many important natural language processing ap-
plications such as statistical machine translation
(Brown et al., 1993), bilingual lexicography (Kla-
vans et al., 1990), and cross-language informa-
tion retrieval (Nie et al., 1999). Its objective is to
identify correspondences between bilingual sen-
tences in given bitexts. As summarized by Wu
(2010), existing sentence alignment techniques
rely mainly on sentence length and bilingual lex-
ical resource. Approaches based on the former
perform effectively on cognate languages but not
on the others. For instance, the statistical cor-
relation of sentence length between English and
Chinese is not as high as that between two Indo-
European languages (Wu, 1994). Lexicon-based

approaches resort to word correspondences in a
bilingual lexicon to match bilingual sentences. A
few sentence alignment methods and tools have
also been explored to combine the two. Moore
(2002) proposes a multi-pass search procedure us-
ing both sentence length and an automatically-
derived bilingual lexicon. Hunalign (Varga et al.,
2005) is another sentence aligner that combines
sentence length and a lexicon. Without a lexicon,
it backs off to a length-based algorithm and then
automatically derives a lexicon from the align-
ment result. Soon after, Ma (2006) develops the
lexicon-based aligner Champollion, assuming that
different words have different importance in align-
ing two sentences.

Nevertheless, most existing approaches to sen-
tence alignment follow the monotonicity assump-
tion that coupled sentences in bitexts appear in
a similar sequential order in two languages and
crossings are not entertained in general (Langlais
et al., 1998; Wu, 2010). Consequently the task of
sentence alignment becomes handily solvable by
means of such basic techniques as dynamic pro-
gramming. In many scenarios, however, this pre-
requisite monotonicity cannot be guaranteed. For
example, bilingual clauses in legal bitexts are of-
ten coordinated in a way not to keep the same
clause order, demanding fully or partially crossing
pairings. Figure 1 shows a real excerpt from a leg-
islation corpus. Such monotonicity seriously im-
pairs the existing alignment approaches founded
on the monotonicity assumption.

This paper is intended to explore the problem of
non-monotonic alignment within the framework
of semisupervised learning. Our approach is mo-
tivated by the above observation and based on
the following two assumptions. First, monolin-
gual sentences with high affinity are likely to have
their translations with similar relatedness. Follow-
ing this assumption, we propose the conception
of monolingual consistency which, to the best of

622



British Overseas citizen" ( ) means a person who has the

status of a British Overseas citizen under the British Nationality Act

1981 (1981 c. 61 U.K.)

British protected person" ( ) means a person who has

the status of a British protected person under the British Nationality Act

1981 (1981 c. 61 U.K.)

...

1. Interpretation of words and expressions

British citizen" ( ) means a person who has the status of a

British citizen under the British Nationality Act 1981 (1981 c. 61 U.K.)

British Dependent Territories citizen" ( ) means a person

who has or had the status of a British Dependent Territories citizen

under the British Nationality Act 1981 (1981 c. 61 U.K.)

British enactment" and "imperial enactment" ( ) Mean-

(a) any Act of Parliament; (b) any Order in Council; and (c) any rule,

regulation, proclamation, order, notice, rule of court, by-law or other

instrument made under or by virtue of any such Act or Order in Council

(British Overseas citizen) 1981

(1981 c. 61 U.K.)

(British Dependent Territories citizen) 1981

(1981 c. 61 U.K.)

1.

(British citizen) 1981 (1981

c. 61 U.K.)

(British enactment, imperial enactment) (a)

(b) (c)

...

"

"

"

"

"

Figure 1: A real example of non-monotonic sentence alignment from BLIS corpus.

our knowledge, has not been taken into account in
any previous work of alignment. Second, initial
alignment of certain quality can be obtained by
means of existing alignment techniques. Our ap-
proach attempts to incorporate both monolingual
consistency of sentences and bilingual consistency
of initial alignment into a semisupervised learning
framework to produce an optimal solution. Ex-
tensive evaluations are performed using real-world
legislation bitexts from BLIS, a comprehensive
legislation database maintained by the Depart-
ment of Justice, HKSAR. Our experimental results
show that the proposed method can work effec-
tively while two representatives of existing align-
ers suffer severely from the non-monotonicity.

2 Methodology

2.1 The Problem
An alignment algorithm accepts as input a bi-
text consisting of a set of source-language sen-
tences, S = {s1, s2, . . . , sm}, and a set of target-
language sentences, T = {t1, t2, . . . , tn}. Dif-
ferent from previous works relying on the mono-
tonicity assumption, our algorithm is generalized
to allow the pairings of sentences in S and T
to cross arbitrarily. Figure 2(a) illustrates mono-
tonic alignment with no crossing correspondences
in a bipartite graph and 2(b) non-monotonic align-
ment with scrambled pairings. Note that it is rela-
tively straightforward to identify the type of many-
to-many alignment in monotonic alignment using
techniques such as dynamic programming if there
is no scrambled pairing or the scrambled pairings
are local, limited to a short distance. However,
the situation of non-monotonic alignment is much

more complicated. Sentences to be merged into a
bundle for matching against another bundle in the
other language may occur consecutively or discon-
tinuously. For the sake of simplicity, we will not
consider non-monotonic alignment with many-to-
many pairings but rather assume that each sen-
tence may align to only one or zero sentence in
the other language.

Let F represent the correspondence relation be-
tween S and T , and therefore F ⊂ S × T . Let
matrix F denote a specific alignment solution of
F , where Fij is a real score to measure the likeli-
hood of matching the i-th sentence si in S against
the j-th sentence tj in T . We then define an align-
ment function A : F → A to produce the final
alignment, where A is the alignment matrix for S
and T , with Aij = 1 for a correspondence be-
tween si and tj and Aij = 0 otherwise.

2.2 Semisupervised Learning

A semisupervised learning framework is intro-
duced to incorporate the monolingual and bilin-
gual consistency into alignment scoring

Q(F ) = Qm(F ) + λQb(F ), (1)

where Qm(F ) is the term for monolingual con-
straint to control the consistency of sentences with
high affinities, Qb(F ) for the constraint of initial
alignment obtained with existing techniques, and
λ is the weight between them. Then, the optimal
alignment solution is to be derived by minimizing
the cost function Q(F ), i.e.,

F ∗ = argmin
F

Q(F ). (2)

623



s1

s2

s3

s4

s5

s6

t1

t2

t3

t4

t5

t6

(a)

s1

s2

s3

s4

s5

s6

t1

t2

t3

t4

t5

t6

(b)

Figure 2: Illustration of monotonic (a) and non-monotonic alignment (b), with a line representing the
correspondence of two bilingual sentences.

In this paper, Qm(F ) is defined as

1

4

m∑

i,j=1

Wij

n∑

k,l=1

Vkl

(
Fik√
DiiEkk

− Fjl√
DjjEll

)2
, (3)

whereW and V are the symmetric matrices to rep-
resent the monolingual sentence affinity matrices
in S and T , respectively, and D and E are the di-
agonal matrices with entries Dii =

∑
j Wij and

Eii =
∑

j Vij . The idea behind (3) is that to min-
imize the cost function, the translations of those
monolingual sentences with close relatedness re-
flected inW and V should also keep similar close-
ness. The bilingual constraint term Qb(F ) is de-
fined as

Qb(F ) =
m∑

i=1

n∑

j=1

(
Fij − Âij

)2
, (4)

where Â is the initial alignment matrix obtained
by A : F̂ → Â. Note that F̂ is the initial relation
matrix between S and T .

The monolingual constraint term Qm(F ) de-
fined above corresponds to the smoothness con-
straint in the previous semisupervised learning
work by Zhou et al. (2004) that assigns higher
likelihood to objects with larger similarity to share
the same label. On the other hand, Qb(F ) corre-
sponds to their fitting constraint, which requires
the final alignment to maintain the maximum con-
sistency with the initial alignment.

Taking the derivative of Q(F ) with respect to
F , we have

∂Q(F )
∂F

= 2F − 2SFT + 2λF − 2λÂ, (5)

where S and T are the normalized matrices of W
and V , calculated by S = D−1/2WD−1/2 and

T = E−1/2V E−1/2. Then, the optimal F ∗ is to
be found by solving the equation

(1 + λ)F ∗ − SF ∗T = λÂ, (6)

which is equivalent to αF ∗ − F ∗β = γ with
α = (1 + λ)S−1, β = T and γ = λS−1Â.
This is in fact a Sylvester equation (Barlow et al.,
1992), whose numerical solution can be found by
many classical algorithms. In this research, it is
solved using LAPACK,1 a software library for nu-
merical linear algebra. Non-positive entries in F ∗

indicate unrealistic correspondences of sentences
and are thus set to zero before applying the align-
ment function.

2.3 Alignment Function

Once the optimal F ∗ is acquired, the remaining
task is to design an alignment function A to con-
vert it into an alignment solution. An intuitive ap-
proach is to use a heuristic search for local op-
timization (Kit et al., 2004), which produces an
alignment with respect to the largest scores in
each row and each column. However, this does not
guarantee a globally optimal solution. Figure 3 il-
lustrates a mapping relation matrix onto an align-
ment matrix, which also shows that the optimal
alignment cannot be achieved by heuristic search.

Banding is another approach frequently used to
convert a relation matrix to alignment (Kay and
Röscheisen, 1993). It is founded on the observa-
tion that true monotonic alignment paths usually
lie close to the diagonal of a relation matrix. How-
ever, it is not applicable to our task due to the non-
monotonicity involved. We opt for converting a
relation matrix into specific alignment by solving

1http://www.netlib.org/lapack/

624



al
ig
n
m
en
t
m
at
ri
x

re
la
ti
o
n
m
at
ri
x

2

1 2 43 5 6 7

1

3

4

5

6

00.4 0 0.5 0 00

0.30 0 0.6 0 00

00 0 0 0 00

0.40 0 0 0.2 00

0.50 0 0 0 00.6

00.1 0 0 0 00.8

2

1 2 43 5 6 7

1

3

4

5

6

01 0 0 0 00

00 0 1 0 00

00 0 0 0 00

00 0 0 1 00

10 0 0 0 00

00 0 0 0 01

Figure 3: Illustration of sentence alignment from relation matrix to alignment matrix. The scores marked
with arrows are the best in each row/column to be used by the heuristic search. The right matrix repre-
sents the corresponding alignment matrix by our algorithm.

the following optimization

A =argmax
X

m∑

i=1

n∑

j=1

XijFij (7)

s.t.
m∑

i=1

Xij ≤ 1,
n∑

j=1

Xij ≤ 1, Xij ∈ {0, 1}

This turns sentence alignment into a problem to
be resolved by binary linear programming (BIP),
which has been successfully applied to word align-
ment (Taskar et al., 2005). Given a scoring matrix,
it guarantees an optimal solution.

2.4 Alignment Initialization
Once the above alignment function is available,
the initial alignment matrix Â can be derived from
an initial relation matrix F̂ obtained by an avail-
able alignment method. This work resorts to an-
other approach to initializing the relation matrix.
In many genres of bitexts, such as government
transcripts or legal documents, there are a certain
number of common strings on the two sides of bi-
texts. In legal documents, for example, transla-
tions of many key terms are usually accompanied
with their source terms. Also, common number-
ings can be found in enumerated lists in bitexts.
These kinds of anchor strings provide quite reli-
able information to link bilingual sentences into
pairs, and thus can serve as useful cues for sen-
tence alignment. In fact, they can be treated as a
special type of highly reliable “bilexicon”.

The anchor strings used in this work are derived
by searching the bitexts using word-level inverted
indexing, a basic technique widely used in infor-
mation retrieval (Baeza-Yates and Ribeiro-Neto,
2011). For each index term, a list of postings is

created. Each posting includes a sentence identi-
fier, the in-sentence frequency and positions of this
term. The positions of terms are intersected to find
common anchor strings. The anchor strings, once
found, are used to calculate the initial affinity F̂ij
of two sentences using Dice’s coefficient

F̂ij =
2|C1i ∩ C2j |
|C1i|+ |C2j |

(8)

where C1i and C2j are the anchor sets in si and tj ,
respectively, and | · | is the cardinality of a set.

Apart from using anchor strings, other avenues
for the initialization are studied in the evaluation
section below, i.e., using another aligner and an
existing lexicon.

2.5 Monolingual Affinity
Although various kinds of information from a
monolingual corpus have been exploited to boost
statistical machine translation models (Liu et al.,
2010; Su et al., 2012), we have not yet been
exposed to any attempt to leverage monolingual
sentence affinity for sentence alignment. In our
framework, an attempt to this can be made through
the computation of W and V . Let us take W as an
example, where the entry Wij represents the affin-
ity of sentence si and sentence sj , and it is set to
0 for i = j in order to avoid self-reinforcement
during optimization (Zhou et al., 2004).

When two sentences in S or T are not too short,
or their content is not divergent in meaning, their
semantic similarity can be estimated in terms of
common words. Motivated by this, we define Wij
(for i 6= j) based on the Gaussian kernel as

Wij = exp

(
− 1
2σ2

(
1− v

T
i vj

‖vi‖ ‖vj‖

)2)
(9)

625



where σ is the standard deviation parameter, vi
and vj are vectors of si and sj with each com-
ponent corresponding to the tf-idf value of a par-
ticular term in S (or T ), and ‖·‖ is the norm of
a vector. The underlying assumption here is that
words appearing frequently in a small number of
sentences but rarely in the others are more signifi-
cant in measuring sentence affinity.

Although semantic similarity estimation is a
straightforward approach to deriving the two affin-
ity matrices, other approaches are also feasible. An
alternative approach can be based on sentence
length under the assumption that two sentences
with close lengths in one language tend to have
their translations also with close lengths.

2.6 Discussion

The proposed semisupervised framework for non-
monotonic alignment is in fact generalized be-
yond, and can also be applied to, monotonic align-
ment. Towards this, we need to make use of sen-
tence sequence information. One way to do it is
to incorporate sentence positions into Equation (1)
by introducing a position constraint Qp(F ) to en-
force that bilingual sentences in closer positions
should have a higher chance to match one another.
For example, the new constraint can be defined as

Qp(F ) =
m∑

i=1

n∑

j=1

|pi − qj |F 2ij ,

where pi and qj are the absolute (or relative) posi-
tions of two bilingual sentences in their respective
sequences. Another way follows the banding as-
sumption that the actual couplings only appear in
a narrow band along the main diagonal of relation
matrix. Accordingly, all entries of F ∗ outside this
band are set to zero before the alignment function
is applied. Kay and Röscheisen (1993) illustrate
that this can be done by modeling the maximum
deviation of true couplings from the diagonal as
O(
√
n).

3 Evaluation

3.1 Data Set

Our data set is acquired from the Bilingual
Laws Information System (BLIS),2 an electronic
database of Hong Kong legislation maintained
by the Department of Justice, HKSAR. BLIS

2http://www.legislation.gov.hk

provides Chinese-English bilingual texts of ordi-
nances and subsidiary legislation in effect on or af-
ter 30 June 1997. It organizes the legal texts into a
hierarchy of chapters, sections, subsections, para-
graphs and subparagraphs, and displays the con-
tent of a such hierarchical construct (usually a sec-
tion) on a single web page.

By web crawling, we have collected in total
31,516 English and 31,405 Chinese web pages,
forming a bilingual corpus of 31,401 bitexts after
filtering out null pages. A text contains several to
two hundred sentences. Many bitexts exhibit par-
tially non-monotonic order of sentences. Among
them, 175 bitexts are randomly selected for man-
ual alignment. Sentences are identified based on
punctuations. OpenNLP Tokenizer3 is applied to
segment English sentences into tokens. For Chi-
nese, since there is no reliable segmenter for this
genre of text, we have to treat each Chinese char-
acter as a single token. In addition, to calculate the
monolingual sentence affinity, stemming of En-
glish words is performed with the Porter Stemmer
(Porter, 1980) after anchor string mining.

The manual alignment of the evaluation data set
is performed upon the initial alignment by Hu-
nalign (Varga et al., 2005), an effective sentence
aligner that uses both sentence length and a bilex-
icon (if available). For this work, Hunalign re-
lies solely on sentence length. Its output is then
double-checked and corrected by two experts in
bilingual studies, resulting in a data set of 1747
1-1 and 70 1-0 or 0-1 sentence pairs.

The standard deviation σ in (9) is an important
parameter for the Gaussian kernel that has to be
determined empirically (Zhu et al., 2003; Zhou et
al., 2004). In addition, theQ function also involves
another parameter λ to adjust the weight of the
bilingual constraint. This work seeks an approach
to deriving the optimal parameters without any ex-
ternal training data beyond the initial alignment. A
three-fold cross-validation is thus performed on
the initial 1-1 alignment and the parameters that
give the best average performance are chosen.

3.2 Monolingual Consistency

To demonstrate the validity of the monolingual
consistency, the semantic similarity defined by

vTi vj
‖vi‖‖vj‖ is evaluated as follows. 500 pairs of En-
glish sentences with the highest similarities are se-
lected, excluding null pairings (1-0 or 0-1 type).

3http://opennlp.apache.org/

626



0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

Similarity of English sentence pair

S
im

ila
rit

y 
of

 C
hi

ne
se

 s
en

te
nc

e 
pa

ir

Figure 4: Demonstration of monolingual consis-
tency. The horizontal axis is the similarity of En-
glish sentence pairs and the vertical is the similar-
ity of the corresponding pairs in Chinese.

Type Total
initAlign NonmoAlign

Pred Corr Pred Corr

1-0 70 662 66 70 50
1-1 1747 1451 1354 1747 1533

Table 1: Performance of the initial alignment and
our aligner, where the Pred and Corr columns are
the numbers of predicted and correct pairings.

All of these high-affinity pairs have a similarity
score higher than 0.72. A number of duplicate
sentences (e.g., date) with exceptionally high sim-
ilarity 1.0 are dropped. Also, the similarity of the
corresponding translations of each selected pair
is calculated. These two sets of similarity scores
are then plotted in a scatter plot, as in Figure 4.
If the monolingual consistency assumption holds,
the plotted points would appear nearby the diag-
onal. Figure 4 confirms this, indicating that sen-
tence pairs with high affinity in one language do
have their counterparts with similarly high affinity
in the other language.

3.3 Impact of Initial Alignment

The 1-1 initial alignment plays the role of labeled
instances for the semisupervised learning. It is
of critical importance to the learning performance.
As shown in Table 1, our alignment function pre-
dicts 1451 1-1 pairings by virtue of anchor strings,
among which 1354 pairings are correct, yielding
a relatively high precision in the non-monotonic
circumstance. It also predicts null alignment for
many sentences that contain no anchor. This ex-
plains why it outputs 662 1-0 pairings when there

20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100%
0

200

400

600

800

1000

1200

1400

1600

Percentage of initial 1−1 alignment

C
or

re
ct

ly
 d

et
ec

te
d 

1−
1 

pa
iri

ng
s

 

 

NonmoAlign
initAlign

Figure 5: Performance of non-monotonic align-
ment along the percentage of initial 1-1 alignment.

are only 70 1-0 true ones. Starting from this initial
alignment, our aligner (let us call it NonmoAlign)
discovers 179 more 1-1 pairings.

A question here is concerned with how the scale
of initial alignment affects the final alignment. To
examine this, we randomly select 20%, 40%, 60%
and 80% of the 1451 1-1 detected pairings as the
initial alignments for a series of experiments. The
random selection for each proportion is performed
ten times and their average alignment performance
is taken as the final result and plotted in Figure 5.
An observation from this figure is that the aligner
consistently discovers significantly more 1-1 pair-
ings on top of an initial 1-1 alignment, which has
to be accounted for by the monolingual consis-
tency. Another observation is that the alignment
performance goes up along the increase of the
percentage of initial alignment while performance
gain slows down gradually. When the percentage
is very low, the aligner still works quite effectively.

3.4 Non-Monotonic Alignment

To test our aligner with non-monotonic sequences
of sentences, we have them randomly scrambled
in our experimental data. This undoubtedly in-
creases the difficulty of sentence alignment, espe-
cially for the traditional approaches critically rely-
ing on monotonicity.

The baseline methods used for comparison are
Moore’s aligner (Moore, 2002) and Hunalign
(Varga et al., 2005). Hunalign is configured with
the option [-realign], which triggers a three-step
procedure: after an initial alignment, Hunalign
heuristically enriches its dictionary using word co-
occurrences in identified sentence pairs; then, it
re-runs the alignment process using the updated

627



Type
Moore Hunalign NonmoAlign

P R F1 P R F1 P R F1
1-1 0.104 0.104 0.104 0.407 0.229 0.293 0.878 0.878 0.878
1-0 0.288 0.243 0.264 0.033 0.671 0.062 0.714 0.714 0.714

Micro 0.110 0.110 0.110 0.184 0.246 0.210 0.871 0.871 0.871

Table 2: Performance comparison with the baseline methods.

dictionary. According to Varga et al (2005), this
setting gives a higher alignment quality than oth-
erwise. In addition, Hunalign can use an external
bilexicon. For a fair comparison, the identified an-
chor set is fed to Hunalign as a special bilexicon.
The performance of alignment is measured by pre-
cision (P), recall (R) and F-measure (F1). Micro-
averaged performance scores of precision, recall
and F-measure are also computed to measure the
overall performance on 1-1 and 1-0 alignment.
The final results are presented in Table 2, show-
ing that both Moore’s aligner and Hunalign under-
perform ours on non-monotonic alignment. The
particularly poor performance of Moore’s aligner
has to be accounted for by its requirement of more
than thousands of sentences in bitext input for re-
liable estimation of its parameters. Unfortunately,
our available data has not reached that scale yet.

3.5 Partially Non-Monotonic Alignment

Full non-monotonic bitexts are rare in practice.
But partial non-monotonic ones are not. Unlike
traditional alignment approaches, ours does not
found its performance on the degree of monotonic-
ity. To test this, we construct five new versions of
the data set for a series of experiments by ran-
domly choosing and scrambling 0%, 10%, 20%,
40%, 60% and 80% sentence parings. In the-
ory, partial non-monotonicity of various degrees
should have no impact on the performance of our
aligner. It is thus not surprised that it achieves
the same result as reported in last subsection.
NonmoAlign initialized with Hunalign (marked
as NonmoAlign Hun) is also tested. The experi-
mental results are presented in Figure 6. It shows
that both Moore’s aligner and Hunalign work rel-
atively well on bitexts with a low degree of non-
monotonicity, but their performance drops dra-
matically when the non-monotonicity is increased.
Despite the improvement at low non-monotonicity
by seeding our aligner with Hunalign, its per-
formance decreases likewise when the degree of
non-monotonicity increases, due to the quality de-

0 % 10% 20% 30% 40% 50% 60% 70% 80%
0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

1.1

Non−monotonic ratio

M
ic

ro
−

F
1

 

 

NonmoAlign
Hunalign
Moore
NonmoAlign_Hun

Figure 6: Performance of alignment approaches at
different degrees of non-monotonicity.

crease of the initial alignment by Hunalign.

3.6 Monotonic Alignment

The proposed alignment approach is also expected
to work well on monotonic sentence alignment.
An evaluation is conducted for this using a mono-
tonic data set constructed from our data set by
discarding all its 126 crossed pairings. Of the
two strategies discussed above, banding is used
to help our aligner incorporate the sequence in-
formation. The initial relation matrix is built with
the aid of a dictionary automatically derived by
Hunalign. Entries of the matrix are derived by
employing a similar strategy as in Varga et al.
(2005). The evaluation results are presented in Ta-
ble 3, which shows that NonmoAlign still achieves
very competitive performance on monotonic sen-
tence alignment.

4 Related Work

The research of sentence alignment originates in
the early 1990s. Gale and Church (1991) and
Brown (1991) report the early works using length
statistics of bilingual sentences. The general idea
is that the closer two sentences are in length, the
more likely they are to align. A notable difference
of their methods is that the former uses sentence

628



Type
Moore Hunalign NonmoAlign

P R F1 P R F1 P R F1
1-1 0.827 0.828 0.827 0.999 0.972 0.986 0.987 0.987 0.987
1-0 0.359 0.329 0.343 0.330 0.457 0.383 0.729 0.729 0.729

Micro 0.809 0.807 0.808 0.961 0.951 0.956 0.976 0.976 0.976

Table 3: Performance of monotonic alignment in comparison with the baseline methods.

length in number of characters while the latter in
number of tokens. Both use dynamic program-
ming to search for the best alignment. As shown in
Chen (1993) and Wu (1994), however, sentence-
length based methods suffer when the texts to be
aligned contain small passages, or the languages
involved share few cognates. The subsequent stage
of sentence alignment research is accompanied by
the advent of a handful of well-designed alignment
tools. Moore (2002) proposes a three-pass proce-
dure to find final alignment. Its bitext input is ini-
tially aligned based on sentence length. This step
generates a set of strictly-selected sentence pairs
for use to train an IBM translation model 1 (Brown
et al., 1993). Its final step realigns the bitext using
both sentence length and the discovered word cor-
respondences. Hunalign (Varga et al., 2005), orig-
inally proposed as an ingredient for building paral-
lel corpora, has demonstrated an outstanding per-
formance on sentence alignment. Like many other
aligners, it employs a similar strategy of combin-
ing sentence length and lexical data. In the ab-
sence of a lexicon, it first performs an initial align-
ment wholly relying on sentence length and then
automatically builds a lexicon based on this align-
ment. Using an available lexicon, it produces a
rough translation of the source text by converting
each token to the one of its possible counterparts
that has the highest frequency in the target corpus.
Then, the relation matrix of a bitext is built of sim-
ilarity scores for the rough translation and the ac-
tual translation at sentence level. The similarity of
two sentences is calculated in terms of their com-
mon pairs and length ratio.

To deal with noisy input, Ma (2006) proposes
a lexicon-based sentence aligner - Champollion.
Its distinctive feature is that it assigns different
weights to words in terms of their tf-idf scores,
assuming that words with low sentence frequen-
cies in a text but high occurrences in some local
sentences are more indicative of alignment. Un-
der this assumption, the similarity of any two sen-
tences is calculated accordingly and then a dy-

namic programming algorithm is applied to pro-
duce final alignment. Following this work, Li et
al. (2010) propose a revised version of Champol-
lion, attempting to improve its speed without per-
formance loss. For this purpose, the input bitexts
are first divided into smaller aligned fragments be-
fore applying Champollion to derive finer-grained
sentence pairs. In another related work by Deng et
al. (2007), a generative model is proposed, accom-
panied by two specific alignment strategies, i.e.,
dynamic programming and divisive clustering. Al-
though a non-monotonic search process that toler-
ates two successive chunks in reverse order is in-
volved, their work is essentially targeted at mono-
tonic alignment.

5 Conclusion

In this paper we have proposed and tested
a semisupervised learning approach to non-
monotonic sentence alignment by incorporating
both monolingual and bilingual consistency. The
utility of monolingual consistency in maintain-
ing the consonance of high-affinity monolingual
sentences with their translations has been demon-
strated. This work also exhibits that bilingual con-
sistency of initial alignment of certain quality is
useful to boost alignment performance. Our eval-
uation using real-world data from a legislation
corpus shows that the proposed approach outper-
forms the baseline methods significantly when the
bitext input is composed of non-monotonic sen-
tences. Working on partially non-monotonic data,
this approach also demonstrates a superior per-
formance. Although initially proposed for non-
monotonic alignment, it works well on monotonic
alignment by incorporating the constraint of sen-
tence sequence.

Acknowledgments

The research described in this paper was substan-
tially supported by the Research Grants Council
(RGC) of Hong Kong SAR, China, through the
GRF grant 9041597 (CityU 144410).

629



References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 2011.

Modern Information Retrieval: The Concepts
and Technology Behind Search, 2nd ed., Harlow:
Addison-Wesley.

Jewel B. Barlow, Moghen M. Monahemi, and Dianne P.
O’Leary. 1992. Constrained matrix Sylvester equa-
tions. In SIAM Journal on Matrix Analysis and Ap-
plications, 13(1):1-9.

Peter F. Brown, Jennifer C. Lai, Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In
Proceedings of ACL’91, pages 169-176.

Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263-
311.

Stanley F. Chen. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings of
ACL’93, pages 9-16.

Yonggang Deng, Shankar Kumar, and William Byrne.
2007. Segmentation and alignment of parallel text
for statistical machine translation. Natural Lan-
guage Engineering, 13(3): 235-260.

William A. Gale, Kenneth Ward Church. 1991. A Pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of ACL’91, pages 177-184.

Martin Kay and Martin Röscheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121-142.

Chunyu Kit, Jonathan J. Webster, King Kui Sin, Haihua
Pan, and Heng Li. 2004. Clause alignment for bilin-
gual HK legal texts: A lexical-based approach. In-
ternational Journal of Corpus Linguistics, 9(1):29-
51.

Chunyu Kit, Xiaoyue Liu, King Kui Sin, and Jonathan
J. Webster. 2005. Harvesting the bitexts of the laws
of Hong Kong from the Web. In The 5th Workshop
on Asian Language Resources, pages 71-78.

Judith L. Klavans and Evelyne Tzoukermann. 1990.
The bicord system: Combining lexical information
from bilingual corpora and machine readable dictio-
naries. In Proceedings of COLING’90, pages 174-
179.

Philippe Langlais, Michel Simard, and Jean Véronis.
1998. Methods and practical issues in evaluating
alignment techniques. In Proceedings of COLING-
ACL’98, pages 711-717.

Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.
2010. Improving statistical machine translation with
monolingual collocation. In Proceedings of ACL
2010, pages 825-833.

Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In LREC 2006, pages 489-492.

Peng Li, Maosong Sun, Ping Xue. 2010. Fast-
Champollion: a fast and robust sentence alignment
algorithm. In Proceedings of ACL 2010: Posters,
pages 710-718.

Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA 2002, page 135-144.

Jian-Yun Nie, Michel Simard, Pierre Isabelle and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the Web. In Proceedings
of SIGIR’99, pages 74-81.

Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3): 130-137.

Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xi-
aodong Shi, Huailin Dong, Qun Liu. 2012. Transla-
tion model adaptation for statistical machine trans-
lation with monolingual topic information. In Pro-
ceedings of ACL 2012, Vol. 1, pages 459-468.

Ben Taskar, Simon Lacoste-Julien and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT/EMNLP 2005,
pages 73-80.

Dániel Varga, Péter Halácsy, András Kornai, Viktor
Nagy, László Németh, Viktor Trón. 2005. Parallel
corpora for medium density languages. In Proceed-
ings of RANLP 2005, pages 590-596.

Dekai Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In Proceed-
ings of ACL’94, pages 80-87.

Dekai Wu. 2010. Alignment. Handbook of Natural
Language Processing, 2nd ed., CRC Press.

Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Ja-
son Weston, Bernhard Schlkopf. 2004. Learning
with local and global consistency. Advances in Neu-
ral Information Processing Systems, 16:321-328.

Xiaojin Zhu, Zoubin Ghahramani and John Lafferty.
2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
ICML 2003, pages 912-919.

630


