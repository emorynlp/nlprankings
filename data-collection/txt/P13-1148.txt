



















































A joint model of word segmentation and phonological variation for English word-final /t/-deletion


Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1508–1516,
Sofia, Bulgaria, August 4-9 2013. c©2013 Association for Computational Linguistics

A joint model of word segmentation and phonological variation for
English word-final /t/-deletion

Benjamin Börschinger1,3 and Mark Johnson1 and Katherine Demuth2
(1) Department of Computing, Macquarie University

(2) Department of Linguistics, Macquarie University

(3) Department of Computational Linguistics, Heidelberg University

{benjamin.borschinger, mark.johnson, katherine.demuth}@mq.edu.au

Abstract

Word-final /t/-deletion refers to a common
phenomenon in spoken English where
words such as /wEst/ “west” are pro-
nounced as [wEs] “wes” in certain con-
texts. Phonological variation like this is
common in naturally occurring speech.
Current computational models of unsu-
pervised word segmentation usually as-
sume idealized input that is devoid of
these kinds of variation. We extend a
non-parametric model of word segmenta-
tion by adding phonological rules that map
from underlying forms to surface forms
to produce a mathematically well-defined
joint model as a first step towards han-
dling variation and segmentation in a sin-
gle model. We analyse how our model
handles /t/-deletion on a large corpus of
transcribed speech, and show that the joint
model can perform word segmentation and
recover underlying /t/s. We find that Bi-
gram dependencies are important for per-
forming well on real data and for learning
appropriate deletion probabilities for dif-
ferent contexts.1

1 Introduction

Computational models of word segmentation try
to solve one of the first problems language learn-
ers have to face: breaking an unsegmented stream
of sound segments into individual words. Cur-
rently, most such models assume that the input
consists of sequences of phonemes with no pro-
nunciation variation across different occurrences
of the same word type. In this paper we describe

1The implementation of our model as well as
scripts to prepare the data will be made available at
http://web.science.mq.edu.au/~bborschi.
We can’t release our version of the Buckeye Corpus (Pitt et
al., 2007) directly because of licensing issues.

an extension of the Bayesian models of Gold-
water et al. (2009) that incorporates phonologi-
cal rules to “explain away” surface variation. As
a concrete example, we focus on word-final /t/-
deletion in English, although our approach is not
limited to this case. We choose /t/-deletion be-
cause it is a very common and well-studied phe-
nomenon (see Coetzee (2004, Chapter 5) for a
review) and segmental deletion is an interesting
test-case for our architecture. Recent work has
found that /t/-deletion (among other things) is in-
deed common in child-directed speech (CDS) and,
importantly, that its distribution is similar to that in
adult-directed speech (ADS) (Dilley et al., to ap-
pear). This justifies our using ADS to evaluate our
model, as discussed below.

Our experiments are consistent with long-
standing and recent findings in linguistics, in par-
ticular that /t/-deletion heavily depends on the im-
mediate context and that models ignoring context
work poorly on real data. We also examine how
well our models identify the probability of /t/-
deletion in different contexts. We find that models
that capture bigram dependencies between under-
lying forms provide considerably more accurate
estimates of those probabilities than correspond-
ing unigram or “bag of words” models of underly-
ing forms.

In section 2 we discuss related work on han-
dling variation in computational models and on /t/-
deletion. Section 3 describes our computational
model and section 4 discusses its performance for
recovering deleted /t/s. We look at both a sit-
uation where word boundaries are pre-specified
and only inference for underlying forms has to
be performed; and the problem of jointly finding
the word boundaries and recovering deleted un-
derlying /t/s. Section 5 discusses our findings, and
section 6 concludes with directions for further re-
search.

1508



2 Background and related work

The work of Elsner et al. (2012) is most closely
related to our goal of building a model that han-
dles variation. They propose a pipe-line archi-
tecture involving two separate generative models,
one for word-segmentation and one for phonolog-
ical variation. They model the mapping to sur-
face forms using a probabilistic finite-state trans-
ducer. This allows their architecture to handle
virtually arbitrary pronunciation variation. How-
ever, as they point out, combining the segmenta-
tion and the variation model into one joint model
is not straight-forward and usual inference proce-
dures are infeasible, which requires the use of sev-
eral heuristics. We pursue an alternative research
strategy here, starting with a single well-studied
example of phonological variation. This permits
us to develop a joint generative model for both
word segmentation and variation which we plan to
extend to handle more phenomena in future work.

An earlier work that is close to the spirit of our
approach is Naradowsky and Goldwater (2009),
who learn spelling rules jointly with a simple
stem-suffix model of English verb morphology.
Their model, however, doesn’t naturally extend to
the segmentation of entire utterances.

/t/-deletion has received a lot of attention within
linguistics, and we point the interested reader to
Coetzee (2004, Chapter 5) for a thorough review.
Briefly, the phenomenon is as follows: word-final
instances of /t/ may undergo deletion in natural
speech, such that /wEst/ “west” is actually pro-
nounced as [wEs] “wes”.2 While the frequency of
this phenomenon varies across social and dialectal
groups, within groups it has been found to be ro-
bust, and the probability of deletion depends on
its phonological context: a /t/ is more likely to
be dropped when followed by a consonant than
a vowel or a pause, and it is more likely to be
dropped when following a consonant than a vowel
as well. We point out two recent publications that
are of direct relevance to our research. Dilley et al.
(to appear) study word-final variation in stop con-
sonants in CDS, the kind of input we ideally would
like to evaluate our models on. They find that “in-
fants largely experience statistical distributions of
non-canonical consonantal pronunciation variants
[including deletion] that mirror those experienced
by adults.” This both directly establishes the need

2Following the convention in phonology, we give under-
lying forms within “/. . . /” and surface forms within “[. . . ]”.

for computational models to handle this dimension
of variation, and justifies our choice of using ADS
for evaluation, as mentioned above.

Coetzee and Kawahara (2013) provide a com-
putational study of (among other things) /t/-
deletion within the framework of Harmonic Gram-
mar. They do not aim for a joint model that also
handles word segmentation, however, and rather
than training their model on an actual corpus, they
evaluate on constructed lists of examples, mimick-
ing frequencies of real data. Overall, our findings
agree with theirs, in particular that capturing the
probability of deletion in different contexts does
not automatically result in good performance for
recovering individual deleted /t/s. We will come
back to this point in our discussion at the end of
the paper.

3 The computational model

Our models build on the Unigram and the Bigram
model introduced in Goldwater et al. (2009). Fig-
ure 1 shows the graphical model for our joint Bi-
gram model (the Unigram case is trivially recov-
ered by generating the Ui,js directly from L rather
than from LUi,j−1). Figure 2 gives the mathemati-
cal description of the graphical model and Table 1
provides a key to the variables of our model.

The model generates a latent sequence of un-
derlying word-tokens U1, . . . , Un. Each word to-
ken is itself a non-empty sequence of segments or
phonemes, and each Uj corresponds to an under-
lying word form, prior to the application of any
phonological rule. This generative process is re-
peated for each utterance i, leading to multiple
utterances of the form Ui,1, . . . , Ui,ni where ni is
the number of words in the ith utterance, and Ui,j
is the jth word in the ith utterance. Each utter-
ance is padded by an observed utterance bound-
ary symbol $ to the left and to the right, hence
Ui,0 = Ui,ni+1 = $.

3 Each Ui,j+1 is generated
conditionally on its predecessor Ui,j from LUi,j ,
as shown in the first row of the lower plate in Fig-
ure 1. Each Lw is a distribution over the pos-
sible words that can follow a token of w and L
is a global distribution over possible words, used
as back-off for all Lw. Just as in Goldwater et
al. (2009), L is drawn from a Dirichlet Process
(DP) with base distribution B and concentration

3Each utterance terminates as soon as a $ is generated,
thus determining the number of words ni in the ith utterance.
See Goldwater et al. (2009) for discussion.

1509



Figure 1: The graphical model for our joint
model of word-final /t/-deletion and Bigram
word segmentation. The corresponding math-
ematical description is given in Figure 2. The
generative process mimics the intuitively plau-
sible idea of generating underlying forms from
some kind of syntactic model (here, a Bi-
gram language model) and then mapping the
underlying form to an observed surface-form
through the application of a phonological rule
component, here represented by the collection
of rule probabilities ρc.

L |γ, α0 ∼DP (α0, B(· | γ))
Lw |L,α1 ∼DP (α1, L)
ρc |β ∼Beta(1, 1)
Ui,0 = $

Si,0 = $

Ui,j+1 |Ui,j , LUi,j ∼LUi,j
Si,j |Ui,j , Ui,j+1,ρ =PR(· | Ui,j , Ui,j+1)
Wi |Si,1, . . . , Si,ni = CAT(Si,0, . . . , Si,ni)

Figure 2: Mathematical description of our joint
Bigram model. The lexical generator B(· | γ)
is specified in Figure 3 and PR is explained in
the text below. CAT stands for concatenation
without word-boundaries, ni refers to the num-
ber of words in utterance i.

Variable Explanation
B base distribution over possible words
L back-off distribution over words
Lw distribution over words following w
Ui,j underlying form, a word
Si,j surface realization of Ui,j , a word
ρc /t/-deletion probability in context c
Wi observed segments for ith utterance

Table 1: Key for the variables in Figure 1 and
Figure 2. See Figure 3 for the definition of B.

parameter α0, and the word type specific distri-
butions Lw are drawn from a DP (L,α1), result-
ing in a hierarchical DP model (Teh et al., 2006).
The base distribution B functions as a lexical gen-
erator, defining a prior distribution over possible
words. In principle, B can incorporate arbitrary
prior knowledge about possible words, for exam-
ple syllable structure (cf. Johnson (2008)). In-
spired by Norris et al. (1997), we use a simpler
possible word constraint that only rules out se-
quences that lack a vowel (see Figure 3). While
this is clearly a simplification it is a plausible as-
sumption for English data.

Instead of generating the observed sequence of
segments W directly by concatenating the under-
lying forms as in Goldwater et al. (2009), we
map each Ui,j to a corresponding surface-form
Si,j by a probabilistic rule component PR. The
values over which the Si,j range are determined
by the available phonological processes. In the

model we study here, the phonological processes
only include a rule for deleting word-final /t/s
but in principle, PR can be used to encode a
wide variety of phonological rules. Here, Si,j ∈
{Ui,j ,DELF(Ui,j)} if Ui,j ends in a /t/, and Si,j =
Ui,j otherwise, where DELF(u) refers to the same
word as u except that it lacks u’s final segment.

We look at three kinds of contexts on which a
rule’s probability of applying depends:

1. a uniform context that applies to every word-
final position

2. a right context that also considers the follow-
ing segment

3. a left-right context that additionally takes the
preceeding segment into account

For each possible context c there is a prob-
ability ρc which stands for the probability of
the rule applying in this context. Writing

1510



γ ∼Dir(〈0.01, . . . , 0.01〉)

B(w = x1:n | γ) =
{

[
∏n

i=1 γxi ]γ#
Z if V(w)

0.0 if ¬V(w)

Figure 3: Lexical generator with possible word-
constraint for words in Σ+, Σ being the alphabet
of available phonemes. x1:n is a sequence of ele-
ments of Σ of length n. γ is a probability vector
of length |Σ| + 1 drawn from a sparse Dirichlet
prior, giving the probability for each phoneme and
the special word-boundary symbol #. The pred-
icate V holds of all sequences containing at least
one vowel. Z is a normalization constant that ad-
justs for the mass assigned to the empty and non-
possible words.

contexts in the notation familiar from genera-
tive phonology (Chomsky and Halle, 1968), our
model can be seen as implementing the fol-
lowing rules under the different assumptions:4

uniform /t/ → ∅ / ]word
right /t/ → ∅ / ]word β
left-right /t/ → ∅ / α ]word β

We let β range over V(owel), C(onsonant) and $
(utterance-boundary), and α over V and C. We
define a function CONT that maps a pair of ad-
jacent underlying forms Ui,j , Ui,j+1 to the con-
text of the final segment of Ui,j . For example,
CONT(/wEst/,/@v/) returns “C ]word V” in the
left-right setting, or simply “ ]word” in the uni-
form setting. CONT returns a special NOT con-
text if Ui,j doesn’t end in a /t/. We stipulate that
ρNOT = 0.0. Then we can define PR as follows:

PR(DELFINAL(u) | u, r)) = ρCONT(u,r)
PR(u | u, r) = 1− ρCONT(u,r)

Depending on the context setting used, our
model includes one (uniform), three (right) or six
(left-right) /t/-deletion probabilities ρc. We place a
uniform Beta prior on each of those so as to learn
their values in the LEARN-ρ experiments below.

Finally, the observed unsegmented utterances
Wi are generated by concatenating all Si,j using
the function CAT.

We briefly comment on the central intuition
of this model, i.e. why it can infer underlying

4For right there are three and for left-right six different
rules, one for every instantiation of the context-template.

from surface forms. Bayesian word segmentation
models try to compactly represent the observed
data in terms of a small set of units (word types)
and a short analysis (a small number of word
tokens). Phonological rules such as /t/-deletion
can “explain away” an observed surface type such
as [wEs]] in terms of the underlying type /wEst/
which is independently needed for surface tokens
of [wEst]. Thus, the /t/→ ∅ rule makes possi-
ble a smaller lexicon for a given number of sur-
face tokens. Obviously, human learners have ac-
cess to additional cues, such as the meaning of
words, knowledge of phonological similarity be-
tween segments and so forth. One of the advan-
tages of an explicitly defined generative model
such as ours is that it is straight-forward to grad-
ually extend it by adding more cues, as we point
out in the discussion.

3.1 Inference

Just as for the Goldwater et al. (2009) segmen-
tation models, exact inference is infeasible for
our joint model. We extend the collapsed Gibbs
breakpoint-sampler described in Goldwater et al.
(2009) to perform inference for our extended mod-
els. We refer the reader to their paper for addi-
tional details such as how to calculate the Bigram
probabilities in Figure 4. Here we focus on the
required changes to the sampler so as to perform
inference under our richer model. We consider the
case of a single surface string W , so we drop the
i-index in the following discussion.

Knowing W , the problem is to recover the un-
derlying forms U1, . . . , Un and the surface forms
S1, . . . , Sn for unknown n. A major insight in
Goldwater’s work is that rather than sampling over
the latent variables in the model directly (the num-
ber of which we don’t even know), we can instead
perform Gibbs sampling over a set of boundary
variables b1, . . . , b|W |−1 that jointly determine the
values for our variables of interest where |W | is
the length of the surface string W . For our model,
each bj ∈ {0, 1, t}, where bj = 0 indicates ab-
sence of a word boundary, bj = 1 indicates pres-
ence of a boundary and bj = t indicates pres-
ence of a boundary with a preceeding underlying
/t/. The relation between the bj and the S1, . . . , Sn
and U1, . . . , Un is illustrated in Figure 5. The re-
quired sampling equations are given in Figure 4.

1511



P (bj = 0 | b−j) ∝ P (w12,u | wl,u, b−j)× Pr(w12,s | w12,u, wr,u)× P (wr,u | w12,u, b−j ⊕ 〈wl,u, w12,u〉) (1)
P (bj = t | b−j) ∝ P (w1,t | wl,u, b−j)× Pr(w1,s | w1,t, w2,u)× P (w2,u | w1,t, b−j ⊕ 〈wl,u, w1,t〉)

× Pr(w2,s | w2,u, wr,u)× P (wr,u | w2,u, b−j ⊕ 〈wl,u, w1,t〉 ⊕ 〈w1,t, w2,u〉) (2)
P (bj = 1 | b−j) ∝ P (w1,s | wl,u, b−j)× Pr(w1,s | w1,s, w2,u)× P (w2,u | w1,s, b−j ⊕ 〈wl,u, w1,s〉)

× Pr(w2,s | w2,u, wr,u)× P (wr,u | w2,u, b−j ⊕ 〈wl,u, w1,s〉 ⊕ 〈w1,s, w2,u〉) (3)

Figure 4: Sampling equations for our Gibbs sampler, see figure 5 for illustration. bj = 0 corresponds
to no boundary at this position, bj = t to a boundary with a preceeding underlying /t/ and bj = 1 to a
boundary with no additional underlying /t/. We use b−j for the statistics determined by all but the jth

position and b−j ⊕ 〈r, l〉 for these statistics plus an additional count of the bigram 〈r, l〉. P (w | l, b)
refers to the bigram probability of 〈l, w〉 given the the statistics b; we refer the reader to Goldwater et
al. (2009) for the details of calculating these bigram probabilities and details about the required statistics
for the collapsed sampler. PR is defined in the text.

1 10 t 1
I h      i  i       t $

underlying
surface
boundaries
observed I h i i t $

I h      i       t  i       t $

Figure 5: The relation between the observed se-
quence of segments (bottom), the boundary vari-
ables b1, . . . , b|W |−1 the Gibbs sampler operates
over (in squares), the latent sequence of sur-
face forms and the latent sequence of underly-
ing forms. When sampling a new value for
b3 = t, the different word-variables in fig-
ure 4 are: w12,u=w12,s=hiit, w1,t=hit and w1,s=hi,
w2,u=w2,s=it, wl,u=I, wr,u=$. Note that we need
a boundary variable at the end of the utterance as
there might be an underlying /t/ at this position as
well. The final boundary variable is set to 1, not t,
because the /t/ in it is observed.

4 Experiments

4.1 The data

We are interested in how well our model han-
dles /t/-deletion in real data. Ideally, we’d eval-
uate it on CDS but as of now, we know of no
available large enough corpus of accurately hand-
transcribed CDS. Instead, we used the Buckeye
Corpus (Pitt et al., 2007) for our experiments,
a large ADS corpus of interviews with English
speakers that have been transcribed with relatively
fine phonetic detail, with /t/-deletion among the
things manually annotated. Pointing to the re-
cent work by Dilley et al. (to appear) we want
to emphasize that the statistical distribution of /t/-
deletion has been found to be similar for ADS and

orthographic I don’t intend to
transcript /aI R oU n I n t E n d @/
idealized /aI d oU n t I n t E n d t U/
t-drop /aI d oU n I n t E n d t U/

Figure 6: An example fragment from the Buckeye-
corpus in orthographic form, the fine transcript
available in the Buckeye corpus, a fully idealized
pronunciation with canonical dictionary pronunci-
ations and our version of the data with dropped
/t/s.

CDS, at least for read speech.
We automatically derived a corpus of 285,792

word tokens across 48,795 utterances from the
Buckeye Corpus by collecting utterances across all
interviews and heuristically splitting utterances at
speaker-turn changes and indicated silences. The
Buckeye corpus lists for each word token a man-
ually transcribed pronunciation in context as well
as its canonical pronunciation as given in a pro-
nouncing dictionary. As input to our model, we
use the canonical pronunciation unless the pronun-
ciation in context indicates that the final /t/ has
been deleted in which case we also delete the final
/t/ of the canonical pronunciation Figure 6 shows
an example from the Buckeye Corpus, indicating
how the original data, a fully idealized version
and our derived input that takes into account /t/-
deletions looks like.

Overall, /t/-deletion is a quite frequent phe-
nomenon with roughly 29% of all underlying /t/s
being dropped. The probabilities become more
peaked when looking at finer context; see Table 3
for the empirical distribution of /t/-dropping for
the six different contexts of the left-right setting.

1512



4.2 Recovering deleted /t/s, given word
boundaries

In this set of experiments we are interested in how
well our model recovers /t/s when it is provided
with the gold word boundaries. This allows us
to investigate the strength of the statistical sig-
nal for the deletion rule without confounding it
with the word segmentation performance, and to
see how the different contextual settings uniform,
right and left-right handle the data. Concretely,
for the example in Figure 6 this means that we tell
the model that there are boundaries between /aI/,
/doUn/, /IntEnd/, /tu/ and /liv/ but we don’t tell it
whether or not these words end in an underlying
/t/. Even in this simple example, there are 5 possi-
ble positions for the model to posit an underlying
/t/. We evaluate the model in terms of F-score, the
harmonic mean of recall (the fraction of underly-
ing /t/s the model correctly recovered) and preci-
sion (the fraction of underlying /t/s the model pre-
dicted that were correct).

In these experiments, we ran a total of 2500 it-
erations with a burnin of 2000. We collect sam-
ples with a lag of 10 for the last 500 iterations and
perform maximum marginal decoding over these
samples (Johnson and Goldwater, 2009), as well
as running two chains so as to get an idea of the
variance.5

We are also interested in how well the model
can infer the rule probabilities from the data, that
is, whether it can learn values for the different ρc
parameters. We compare two settings, one where
we perform inference for these parameters assum-
ing a uniform Beta prior on each ρc (LEARN-ρ)
and one where we provide the model with the em-
pirical probabilities for each ρc as estimated off
the gold-data (GOLD-ρ), e.g., for the uniform con-
dition 0.29. The results are shown in Table 2.

Best performance for both the Unigram and
the Bigram model in the GOLD-ρ condition is
achieved under the left-right setting, in line with
the standard analyses of /t/-deletion as primarily
being determined by the preceding and the follow-
ing context. For the LEARN-ρ condition, the Bi-
gram model still performs best in the left-right set-
ting but the Unigram model’s performance drops

5As manually setting the hyper-parameters for the DPs in
our model proved to be complicated and may be objected to
on principled grounds, we perform inference for them under
a vague gamma prior, as suggested by Teh et al. (2006) and
Johnson and Goldwater (2009), using our own implementa-
tion of a slice-sampler (Neal, 2003).

uniform right left-right

Unigram LEARN-ρ 56.52 39.28 23.59
GOLD-ρ 62.08 60.80 66.15

Bigram LEARN-ρ 60.85 62.98 77.76
GOLD-ρ 69.06 69.98 73.45

Table 2: F-score of recovered /t/s with known
word boundaries on real data for the three differ-
ent context settings, averaged over two runs (all
standard errors below 2%). Note how the Uni-
gram model always suffers in the LEARN-ρ condi-
tion whereas the Bigram model’s performance is
actually best for LEARN-ρ in the left-right setting.

C C C V C $ V C V V V $
empirical 0.62 0.42 0.36 0.23 0.15 0.07
Unigram 0.41 0.33 0.17 0.07 0.05 0.00
Bigram 0.70 0.58 0.43 0.17 0.13 0.06

Table 3: Inferred rule-probabilities for different
contexts in the left-right setting from one of the
runs. “C C” stands for the context where the
deleted /t/ is preceded and followed by a conso-
nant, “V $” stands for the context where it is pre-
ceded by a vowel and followed by the utterance
boundary. Note how the Unigram model severely
under-estimates and the Bigram model slightly
over-estimates the probabilities.

in all settings and is now worst in the left-right and
best in the uniform setting.

In fact, comparing the inferred probabilities
to the “ground truth” indicates that the Bigram
model estimates the true probabilities more ac-
curately than the Unigram model, as illustrated
in Table 3 for the left-right setting. The Bi-
gram model somewhat overestimates the probabil-
ity for all post-consonantal contexts but the Uni-
gram model severely underestimates the probabil-
ity of /t/-deletion across all contexts.

4.3 Artificial data experiments

To test our Gibbs sampling inference procedure,
we ran it on artificial data generated according to
the model itself. If our inference procedure fails
to recover the underlying /t/s accurately in this set-
ting, we should not expect it to work well on actual
data. We generated our artificial data as follows.
We transformed the sequence of canonical pronun-
ciations in the Buckeye corpus (which we take to
be underlying forms here) by randomly deleting
final /t/s using empirical probabilities as shown in
Table 3 to generate a sequence of artificial sur-
face forms that serve as input to our models. We

1513



uniform right left-right

Unigram LEARN-ρ 94.35 23.55 (+) 63.06
GOLD-ρ 94.45 94.20 91.83

Bigram LEARN-ρ 92.72 91.64 88.48
GOLD-ρ 92.88 92.33 89.32

Table 4: F-score of /t/-recovery with known word
boundaries on artificial data, each condition tested
on data that corresponds to the assumption, aver-
aged over two runs (standard errors less than 2%
except (+) = 3.68%)).

Unigram Bigram
LEARN-ρ 33.58 55.64
GOLD-ρ 55.92 57.62

Table 5: /t/-recovery F-scores when performing
joint word segmention in the left-right setting, av-
eraged over two runs (standard errors less than
2%). See Table 6 for the corresponding segmenta-
tion F-scores.

did this for all three context settings, always es-
timating the deletion probability for each context
from the gold-standard. The results of these exper-
iments are given in table 4. Interestingly, perfor-
mance on these artificial data is considerably bet-
ter than on the real data. In particular the Bigram
model is able to get consistently high F-scores for
both the LEARN-ρ and the GOLD-ρ setting. For
the Unigram model, we again observe the severe
drop in the LEARN-ρ setting for the right and left-
right settings although it does remarkably well in
the uniform setting, and performs well across all
settings in the GOLD-ρ condition. We take this to
show that our inference algorithm is in fact work-
ing as expected.

4.4 Segmentation experiments

Finally, we are also interested to learn how well
we can do word segmentation and underlying /t/-
recovery jointly. Again, we look at both the
LEARN-ρ and GOLD-ρ conditions but focus on the
left-right setting as this worked best in the exper-
iments above. For these experiments, we perform
simulated annealing throughout the initial 2000 it-
erations, gradually cooling the temperature from
5 to 1, following the observation by Goldwater
et al. (2009) that without annealing, the Bigram
model gets stuck in sub-optimal parts of the solu-
tion space early on. During the annealing stage,
we prevent the model from performing inference

for underlying /t/s so that the annealing stage can
be seen as an elaborate initialisation scheme, and
we perform joint inference for the remaining 500
iterations, evaluating on the last sample and av-
eraging over two runs. As neither the Unigram
nor the Bigram model performs “perfect” word
segmentation, we expect to see a degradation in
/t/-recovery performance and this is what we find
indeed. To give an impression of the impact of
/t/-deletion, we also report numbers for running
only the segmentation model on the Buckeye data
with no deleted /t/s and on the data with deleted
/t/s. The /t/-recovery scores are given in Table 5
and segmentation scores in Table 6. Again the
Unigram model’s /t/-recovery score degrades dra-
matically in the LEARN-ρ condition. Looking at
the segmentation performance this isn’t too sur-
prising: the Unigram model’s poorer token F-
score, the standard measure of segmentation per-
formance on a word token level, suggests that it
misses many more boundaries than the Bigram
model to begin with and, consequently, can’t re-
cover any potential underlying /t/s at these bound-
aries. Also note that in the GOLD-ρ condition, our
joint Bigram model performs almost as well on
data with /t/-deletions as the word segmentation
model on data that includes no variation at all.

The generally worse performance of handling
variation as measured by /t/-recovery F-score
when performing joint segmentation is consistent
with the finding of Elsner et al. (2012) who report
considerable performance drops for their phono-
logical learner when working with induced bound-
aries (note, however, that their model does not per-
form joint inference, rather the induced boundaries
are given to their phonological learner as ground-
truth).

5 Discussion

There are two interesting findings from our exper-
iments. First of all, we find a much larger differ-
ence between the Unigram and the Bigram model
in the LEARN-ρ condition than in the GOLD-ρ con-
dition. We suggest that this is due to the Unigram
model’s lack of dependencies between underlying
forms, depriving it of an important source of ev-
idence. Bigram dependencies provide additional
evidence for underlying /t/ that are deleted on the
surface, and because the Bigram model identifies
these underlying /t/ more accurately, it can also es-
timate the /t/ deletion probability more accurately.

1514



Unigram Bigram
LEARN-ρ 54.53 72.55 (2.3%)
GOLD-ρ 54.51 73.18

NO-ρ 54.61 70.12
NO-VAR 54.12 73.99

Table 6: Word segmentation F-scores for the /t/-
recovery F-scores in Table 5 averaged over two
runs (standard errors less than 2% unless given).
NO-ρ are scores for running just the word segmen-
tation model with no /t/-deletion rule on the data
that includes /t/-deletion, NO-VAR for running just
the word segmentation model on the data with no
/t/-deletions.

For example, /t/ dropping in “don’t you” yields
surface forms “don you”. Because the word bi-
gram probability P (you | don’t) is high, the bi-
gram model prefers to analyse surface “don” as
underlying “don’t”. The Unigram model does not
have access to word bigram information so the
underlying forms it posits are less accurate (as
shown in Table 2), and hence the estimate of the
/t/-deletion probability is also less accurate. When
the probabilities of deletion are pre-specified the
Unigram model performs better but still consider-
ably worse than the Bigram model when the word
boundaries are known, suggesting the importance
of non-phonological contextual effects that the Bi-
gram model but not the Unigram model can cap-
ture. This suggests that for example word pre-
dictability in context might be an important factor
contributing to /t/-deletion.

The other striking finding is the considerable
drop in performance between running on natural-
istic and artificially created data. This suggests
that the natural distribution of /t/-deletion is much
more complex than can be captured by statistics
over the phonological contexts we examined. Fol-
lowing Guy (1991), a finer-grained distinction for
the preceeding segments might address this prob-
lem.

Yet another suggestion comes from the recent
work in Coetzee and Kawahara (2013) who claim
that “[a] model that accounts perfectly for the
overall rate of application of some variable pro-
cess therefore does not necessarily account very
well for the actual application of the process to in-
dividual words.” They argue that in particular the
extremely high deletion rates typical of high fre-
quency items aren’t accurately captured when the

deletion probability is estimated across all types.
A look at the error patterns of our model on a sam-
ple from the Bigram model in the LEARN-ρ setting
on the naturalistic data suggests that this is in fact a
problem. For example, the word “just” has an ex-
tremely high rate of deletion with 17462442 = 0.71%.
While many tokens of “jus” are “explained away”
through predicting underlying /t/s, the (literally)
extra-ordinary frequency of “jus”-tokens lets our
model still posit it as an underlying form, although
with a much dampened frequency (of the 1746 sur-
face tokens, 1081 are analysed as being realiza-
tions of an underlying “just”).

The /t/-recovery performance drop when per-
forming joint word segmentation isn’t surprising
as even the Bigram model doesn’t deliver a very
high-quality segmentation to begin with, leading
to both sparsity (through missed word-boundaries)
and potential noise (through misplaced word-
boundaries). Using a more realistic generative
process for the underlying forms, for example an
Adaptor Grammar (Johnson et al., 2007), could
address this shortcoming in future work without
changing the overall architecture of the model al-
though novel inference algorithms might be re-
quired.

6 Conclusion and outlook

We presented a joint model for word segmentation
and the learning of phonological rule probabili-
ties from a corpus of transcribed speech. We find
that our Bigram model reaches 77% /t/-recovery
F-score when run with knowledge of true word-
boundaries and when it can make use of both the
preceeding and the following phonological con-
text, and that unlike the Unigram model it is able
to learn the probability of /t/-deletion in different
contexts. When performing joint word segmen-
tation on the Buckeye corpus, our Bigram model
reaches around above 55% F-score for recovering
deleted /t/s with a word segmentation F-score of
around 72% which is 2% better than running a Bi-
gram model that does not model /t/-deletion.

We identified additional factors that might help
handling /t/-deletion and similar phenomena. A
major advantage of our generative model is the
ease and transparency with which its assump-
tions can be modified and extended. For fu-
ture work we plan to incorporate into our model
richer phonological contexts, item- and frequency-
specific probabilities and more direct use of word

1515



predictability. We also plan to extend our model
to handle additional phenomena, an obvious can-
didate being /d/-deletion.

Also, the two-level architecture we present is
not limited to the mapping being defined in terms
of rules rather than constraints in the spirit of Op-
timality Theory (Prince and Smolensky, 2004); we
plan to explore this alternative path as well in fu-
ture work.

To conclude, we presented a model that pro-
vides a clean framework to test the usefulness of
different factors for word segmentation and han-
dling phonological variation in a controlled man-
ner.

Acknowledgements

We thank the anonymous reviewers for their
valuable comments. This research was sup-
ported under Australian Research Council’s Dis-
covery Projects funding scheme (project numbers
DP110102506 and DP110102593).

References
Noam Chomsky and Morris Halle. 1968. The Sound

Pattern of English. Haper & Row, New York.

Andries W. Coetzee and Shigeto Kawahara. 2013. Fre-
quency biases in phonological variation. Natural
Language and Linguisic Theory, 31:47–89.

Andries W. Coetzee. 2004. What it Means to be a
Loser: Non-Optimal Candidates in Optimality The-
ory. Ph.D. thesis, University of Massachusetts ,
Amherst.

Laura Dilley, Amanda Millett, J. Devin McAuley, and
Tonya R. Bergeson. to appear. Phonetic variation
in consonants in infant-directed and adult-directed
speech: The case of regressive place assimilation
in word-final alveolar stops. Journal of Child Lan-
guage.

Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 184–193, Jeju Island, Korea. As-
sociation for Computational Linguistics.

Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21–54.

Gregory R. Guy. 1991. Contextual conditioning in
variable lexical phonology. Language Variation and
Change, 3(2):223–39.

Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317–325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.

Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Schölkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641–648. MIT Press, Cambridge,
MA.

Mark Johnson. 2008. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398–406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.

Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of the 21st international jont
conference on Artifical intelligence, pages 1531–
1536, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.

Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705–767.

Dennis Norris, James M. Mcqueen, Anne Cutler, and
Sally Butterfield. 1997. The possible-word con-
straint in the segmentation of continuous speech.
Cognitive Psychology, 34(3):191 – 243.

Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech.

Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in Generative Gram-
mar. Blackwell.

Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566–1581.

1516


