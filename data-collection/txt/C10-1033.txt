Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 286‚Äì294,

Beijing, August 2010

286

A Discriminative Latent Variable-Based ‚ÄúDE‚Äù ClassiÔ¨Åer

for Chinese‚ÄìEnglish SMT

Jinhua Du and Andy Way
CNGL, School of Computing

Dublin City University

{jdu, away}@computing.dcu.ie

Abstract

Syntactic reordering on the source-side
is an effective way of handling word or-
der differences. The ·àá (DE) construc-
tion is a Ô¨Çexible and ubiquitous syntac-
tic structure in Chinese which is a ma-
jor source of error in translation quality.
In this paper, we propose a new classi-
Ô¨Åer model ‚Äî discriminative latent vari-
able model (DPLVM) ‚Äî to classify the
DE construction to improve the accuracy
of the classiÔ¨Åcation and hence the transla-
tion quality. We also propose a new fea-
ture which can automatically learn the re-
ordering rules to a certain extent. The ex-
perimental results show that the MT sys-
tems using the data reordered by our pro-
posed model outperform the baseline sys-
tems by 6.42% and 3.08% relative points
in terms of the BLEU score on PB-SMT
and hierarchical phrase-based MT respec-
tively. In addition, we analyse the impact
of DE annotation on word alignment and
on the SMT phrase table.

1 Introduction
Syntactic structure-based reordering has been
shown to be signiÔ¨Åcantly helpful for handling
word order issues in phrase-based machine trans-
lation (PB-SMT) (Xia and McCord, 2004; Collins
et al., 2005; Wang et al., 2007; Li et al., 2007;
Elming, 2008; Chang et al., 2009).
It is well-
known that in MT, it is difÔ¨Åcult to translate be-
tween Chinese‚ÄìEnglish because of the different

word orders (cf.
the different orderings of head
nouns and relative clauses). Wang et al. (2007)
pointed out that Chinese differs from English in
several important respects, such as relative clauses
appearing before the noun being modiÔ¨Åed, prepo-
sitional phrases often appearing before the head
they modify, etc. Chang et al.
(2009) argued
that many of the structural differences are re-
lated to the ubiquitous Chinese structural parti-
cle phrase ·àá (DE) construction, used for a wide
range of noun modiÔ¨Åcation constructions (both
single word and clausal) and other uses. They
pointed out that DE is a major source of word
order error when a Chinese sentence is translated
into English due to the different ways that the DE
construction can be translated.

In this paper, we focus on improving the clas-
siÔ¨Åcation accuracy of DE constructions in Chi-
nese as well as investigating its impact on trans-
lation quality. From the grammatical perspective,
the ·àá(DE) in Chinese represents the meaning of
‚Äúnoun modiÔ¨Åcation‚Äù which generally is shown in
the form of a Noun phrase (NP) [A DE B]. A in-
cludes all the words in the NP before DE and B
contains all the words in the NP after DE. Wang
et al. (2007) Ô¨Årst introduced a reordering of the
DE construction based on a set of rules which
were generated manually and achieved signiÔ¨Åcant
improvements in translation quality. Chang et
al. (2009) extended this work by classifying DE
into 5 Ô¨Åner-grained categories using a log-linear
classiÔ¨Åer with rich features in order to achieve
higher accuracy both in reordering and in lexical
choice. Their experiments showed that a higher

287

accuracy of the DE classiÔ¨Åcation improved the ac-
curacy of reordering component, and further indi-
rectly improved the translation quality in terms of
BLEU (Papineni et al., 2002) scores.

We regard the DE classiÔ¨Åcation as a labeling
task, and hence propose a new model to label the
DE construction using a discriminative latent vari-
able algorithm (DPLVM) (Morency et al., 2007;
Sun and Tsujii, 2009), which uses latent vari-
ables to carry additional information that may not
be expressed by those original labels and capture
more complicated dependencies between DE and
its corresponding features. We also propose a new
feature deÔ¨Åned as ‚Äútree-pattern‚Äù which can auto-
matically learn the reordering rules rather than us-
ing manually generated ones.

The remainder of this paper is organised as fol-
lows.
In section 2, we introduce the types of
word order errors caused by the DE construc-
tion. Section 3 describes the closely related work
on DE construction. In section 4, we detail our
proposed DPLVM algorithm and its adaptation to
our task. We also describe the feature templates
as well as the proposed new feature used in our
model. In section 5, the classiÔ¨Åcation experiments
are conducted to compare the proposed classiÔ¨Åca-
tion model with a log-linear model. Section 6 re-
ports comparative experiments conducted on the
NIST 2008 data set using two sets of reordered
and non-reordered data. Meanwhile, in section 7,
an analysis on how the syntactic DE reordering
affects word alignment and phrase table is given.
Section 8 concludes and gives avenues for future
work.

2 The Problem of Chinese DE

Construction Translation

Although syntactic reordering is an effective
way of signiÔ¨Åcantly improving translation quality,
word order is still a major error source between
Chinese and English translation. Take examples
in Figure 1 as an illustration. The errors of three
translation results in Figure 1 are from different
MT systems, and many errors relate to incorrect
reordering for the ·àá (DE) structure.

These three translations are from different Hi-
ero systems. Although Hiero has an inherent re-
ordering capability, none of them correctly re-

Source: ‡©Ñﬁû(local) Œ¥ ‡≠õ(a) Ÿç ‡†£ ŒΩ “Ö(bad reputation)
·àá(with) œî‡££(middle school)
Reference: ‚Äôa local middle school with a bad reputation‚Äô
Team 1: ‚Äôa bad reputation of the local secondary school‚Äô
Team 2: ‚Äôthe local a bad reputation secondary school‚Äô
Team 3: ‚Äôa local stigma secondary schools‚Äô

Figure 1: Examples of DE construction transla-
tion errors from (Chang et al., 2009)

ordered ‚Äúbad reputation‚Äù and ‚Äúmiddle school‚Äù
around the DE. Chang et al. (2009) suggested that
this is because it is not sufÔ¨Åcient to have a for-
malism which supports phrasal reordering. They
claimed it is necessary to have sufÔ¨Åcient linguis-
tic modeling, so that the system knows when and
how much to rearrange.

Figure 2 gives an example illustrating how
the reordering of DE construction inÔ¨Çuences the
translation of a Chinese sentence. We can see that
if we can properly recognise the DE construction
[A DE B] and correctly perform the reordering,
we can achieve a closer word order with English
and hence a good English translation even it is lit-
eral.

Although the Hiero system has a strong re-
ordering capability in its generalised phrases, it
still cannot process some complicated and Ô¨Çexible
cases of DE construction like those in Figure 1.
Therefore, a lot of work has gone into word re-
ordering before decoding so that the Chinese sen-
tences have a closer word order with correspond-
ing English sentences.

3 Related Work on DE Construction

To address the word order problems of the DE
construction, Wang et al. (2007) proposed a syn-
tactic reordering approach to deal with structural
differences and to reorder source language sen-
tences to be much closer to the order of tar-
get language sentences. They presented a set
of manually generated syntactic rules to deter-
mine whether a ·àá(DE) construction should be
reordered or not before translation, such as ‚ÄúFor
DNPs consisting of ‚ÄòXP+DEG‚Äô, reorder if XP is
PP or LCP‚Äù etc. (cf. (Wang et al., 2007)). The de-
Ô¨Åciency of their algorithm is that they did not fully
consider the Ô¨Çexibility of the DE construction, as
it can be translated in many different ways.

288

Figure 2: An example of DE construction reordering (extended from the original Ô¨Ågure in (Chiang,
2005))

Chang et al.

(2009) extended the work
of (Wang et al., 2007) and characterised the DE
structures into 5 Ô¨Åner-grained classes based on
their syntactic behaviour. They argued that one
possible reason why the ·àá(DE) construction re-
mains problematic is that previous work has paid
insufÔ¨Åcient attention to the many ways that the ·àá
(DE) construction can be translated, as well as the
rich structural cues which exist for these transla-
tions.

For a Chinese noun phrase [A ·àá B], it can be
categorized into one of the following Ô¨Åve classes
(cf. (Chang et al., 2009) for some real examples of
each class):

‚Ä¢ A B (label: DEAB)

In this category, A on the Chinese side is
translated as a pre-modiÔ¨Åer of B. In most
cases A is an adjectival form.

‚Ä¢ B preposition A (label: DEBprepA)

There are several cases that are translated
into the form B preposition A.

‚Ä¢ A‚Äôs B (label: DEAsB)

In this class, the English translation is an ex-
plicit s-genitive case. This class occurs much
less often but is still interesting because of
the difference from the of-genitive.

‚Ä¢ relative clause (label: DErelc)

In this class, the relative clause would be in-
troduced by a relative pronoun or be a re-
duced relative clause.

‚Ä¢ A preposition B (label: DEAprepB)

This class is another small one. The English
translations that fall into this class usually
have some number, percentage or level word
in the Chinese A.

Chang et al. (2009) used 6 kinds of features for
DE classiÔ¨Åcation, namely part-of-speech tag of
DE (DEPOS), Chinese syntactic patterns appear-
ing before DE (A-pattern), unigrams and bigrams
of POS tags(POS-ngram), sufÔ¨Åx unigram and bi-
gram of word (Lexical), Semantic class of words
(SemClass) and Re-occurrence of nouns (Topical-
ity). A conditional log-linear classiÔ¨Åer (Chang et
al., 2009) is trained to classify each DE based on
features extracted from the parsed data.

4 Discriminative Probabilistic Latent

Variable Model

4.1 Motivation
Based on the discussion so far, we can see that:

‚Ä¢ syntactic reordering of the DE construction
in Chinese is an effective way to improve the
translation quality;

‚Ä¢ classifying the DE construction into Ô¨Åner-
grained categories could achieve better re-
ordering and translation performance;

‚Ä¢ classiÔ¨Åcation accuracy of the DE construc-
tion in Chinese has a signiÔ¨Åcant impact on
SMT performance.

Driven by these three points, especially the third
one, we propose a DPLVM-based classiÔ¨Åer to im-
prove classiÔ¨Åcation accuracy. In natural language

Aozhou     shi   yu    Beihan             you     bangjiao                   DE    shaoshu    guojia      zhiyi     .

Australia   is    with  North Korea   have   diplomatic relations  that   few           countries  one of

.

Australia    is  [one of  the few countries] that [have diplomatic relations with North Korea] .

Australia is [one of the few countries]  [have diplomatic relations with North Korea] .

]B

Original:

Reference:

Literal 
Translation:

 [
 
Reordered:    [



]A
 []A
	



[




	

]B





289

processing (NLP) such as sequential labeling (Sun
and Tsujii, 2009), DPLVM demonstrated excel-
lent capability of learning latent dependencies of
the speciÔ¨Åc problems, and have outperformed sev-
eral commonly-used conventional models, such
as support vector machines, conditional random
Ô¨Åelds and hidden Markov models.

4.2 DPLVM Algorithm
In this section, we theoretically introduce the
deÔ¨Ånition and mathematical description of the
DPLVM algorithm used in NLP tasks (Sun and
Tsujii, 2009).

Given a sequence of observations x =
{x1, x2, . . . , xm} and a sequence of labels y =
{y1, y2, . . . , ym}, the task is to learn a mapping
between x and y. yi is a class label and is a mem-
ber of a set Y of possible class labels. DPLVM
also assumes a sequence of latent variables h =
{h1, h2, . . . , hm}, which is hidden in the training
examples.
The DPLVM is deÔ¨Åned as in (1) (Morency et

al., 2007; Sun and Tsujii, 2009):

P (y|x, Œò) =Xh

P (y|h, x, Œò)P (h|x, Œò)

(1)

where Œò are the parameters of the model. It can
be seen that the DPLVM equates to a CRF model
if it has only one latent variable for each label.

For the sake of efÔ¨Åciency,

the model is re-
stricted to have disjoint sets of latent variables as-
sociated with each class label. Each hj is a mem-
ber in a set Hyj of possible latent variables for the
class label yj. We deÔ¨Åne H as the union of all
Hyj sets, so sequences which have any hj 6‚àà Hyj
will by deÔ¨Ånition have P (y|x, Œò) = 0, so that the
model can be rewritten as in (2):
P (y|x, Œò) = Xh‚ààHy1√ó...Hym
where P (h|x, Œò) is deÔ¨Åned by the usual condi-
tional random Ô¨Åeld formulation, as in (3):

P (h|x, Œò)

(2)

P (h|x, Œò) =

expŒò ¬∑ f(h, x)
P‚àÄh expŒò ¬∑ f(h, x)

in which f(h, x) is a feature vector. Given a train-
ing set consisting of n labeled sequences (xi, yi),

(3)

for i = 1 . . . n, parameter estimation is performed
by optimizing the objective function in (4):

L(Œò) =

nXi=1

log P (yi|xi, Œò) ‚àí R(Œò)

(4)

The Ô¨Årst term of this equation is the conditional
log-likelihood of the training data. The second
term is a regularizer that is used for reducing over-
Ô¨Åtting in parameter estimation.

For decoding in the test stage, given a test se-
quence x, we want to Ô¨Ånd the most probable label
sequence y‚àó, as in (5):

y‚àó = arg max

y

P (y|x, Œò‚àó)

(5)

Sun and Tsujii (2009) argued that for latent con-
ditional models like DPLVMs, the best label path
y‚àó cannot directly be generated by the Viterbi al-
gorithm because of the incorporation of hidden
states. They proposed a latent-dynamic inference
(LDI) method based on A‚àó search and dynamic
programming to efÔ¨Åciently decode the optimal la-
bel sequence y‚àó. For more details of the LDI al-
gorithm, refer to (Sun and Tsujii, 2009).

In our experiments, we use the open source
toolkit of DPLVM1 and adapt it to our special
requirements based on the different features and
scenarios.

4.3 Data and DE Annotation
We use the 5 classes of DE of (Chang et al., 2009)
shown in Section 3 to label DE using our DPLVM
model.
In order to fairly compare the classiÔ¨Å-
cation performance between that of Chang et al.
(2009) and our proposed classiÔ¨Åers, we use the
same data sets and conditions to train and test
the classiÔ¨Åer. The data sets are the Chinese Tree-
bank 6.0 (LDC2007T36) and the English‚ÄìChinese
Translation Treebank 1.0 (LDC2007T02). For
more details about the data sets, refer to (Chang
et al., 2009). There are 3523 DEs in the data set,
with 543 of them in the ‚Äúother‚Äù category which do
not belong to any of the 5 pre-deÔ¨Åned classes. In
the classiÔ¨Åcation experiments, the ‚Äúother‚Äù class is
excluded2 and 2980 DEs remain, each of which

1http://www.ibis.t.u-tokyo.ac.jp/XuSun
2In the classiÔ¨Åcation experiments of Chang et al. (2009),
the ‚Äúother‚Äù class was excluded, so in order to carry out a

290

is manually annotated with DE labels for the pur-
pose of classiÔ¨Åer training and evaluation.

In order to match the training and testing con-
ditions, we used a parser trained on CTB6 exclud-
ing Ô¨Åles 1-325 to parse the data sets with DE an-
notation and extract parse-related features rather
than using gold-standard parses (same conditions
as in (Chang et al., 2009)). It is worth noting that
in the Chinese Treebank, there are two types of
POS tag for DE in NPs, namely DEC and DEG.
However, as a result of using a trained parser, the
POS tags of DE might have other values than DEC
and DEG. In our data set, there are four other POS
tags, namely {AS, DER, DEV,SP}.
4.4 Labels and Features in DPLVM Model
In our task, we use the 5 class labels of DE
constructions in NPs, namely DEAB, DEAprepB,
DEAsB, DEBprepA, DErelc.

Note that in the case of the DE construction in
Chinese, it is different from traditional sequence
labeling tasks such as POS tagging, parsing etc.
We only need to label one word in the NP struc-
ture, i.e. the ·àá(DE) in a Chinese NP [A DE B].
Therefore the sequence labeling task becomes ef-
Ô¨Åcient and speedy using the DPLVM algorithm.

Based on our task, the mathematical conditions
for DE classiÔ¨Åcation in a sequence of [A DE B]
are denoted as follows:

‚Ä¢ Sequence of Observations:
x = x1, . . . , xl, xDE, xk, . . . , xm, where
A={x1, . . . , xl}, xDE is the Chinese charac-
ter ·àá (DE), and B={xk, . . . , xm};
‚Ä¢ Set of Labels:
Y = {yi|1 ‚â§ i ‚â§ 5}, in which the Ô¨Åve labels
are DEAB, DEAprepB, DEAsB, DEBprepA,
DErelc.
‚Ä¢ Latent Variables:
h = h1, h2, . . . , hm, where m = 3 in our
task.

We did not add the sixth feature used in (Chang
et al., 2009) ‚Äì topicality ‚Äì in our classiÔ¨Åer because
we do not consider it to be a very useful in a data
set in which the sentences which are randomly
stored. In such a corpus, the content between any
adjacent sentences are irrelevant in many cases.

The new feature and the templates of all fea-

tures used in our task are deÔ¨Åned as:

DEPOS:
As mentioned in section 4.3, there are 6 kinds of
POS tags of DE. Thus, the feature template is de-
Ô¨Åned as in (5):

Tdepos = {dDE|dDE ‚àà DP}, where DP = {AS, DEC,
(5)
DEG,DER,DEV,SP}.

Tree-pattern:
Chang (2009) used an A-pattern feature which is
an indicator function that Ô¨Åres when some syn-
tactic rules are satisÔ¨Åed, such as ‚ÄúA is ADJP if
A+DE is a DNP with the form of ‚ÄòADJP+DEG‚Äô‚Äù,
etc. These rules are induced manually based on
the grammatical phenomena at hand. Here we
propose a more generalised feature deÔ¨Åned as
‚Äútree-pattern‚Äù to automatically learn the reorder-
ing from the training data.

We consider all the sub-tree structures around
DE without any word POS tags.
For exam-
ple, consider the parse structure (an example
in (Chang et al., 2009)) in (6):

(NP (NP (NR ·±ªﬁã)) (CP (IP (VP (ADVP (AD ‡¥°)) (VP (VA
‡†µ)))) (DEC ·àá)) (NP (NN ‡ÆÉ·§®) (NN ‡§®·£≥ﬁã))))))
(6)

where the tree-pattern is ‚ÄúNP NP CP IP VP ADVP
VP DEC NP‚Äù. We do not use the word POS tag
(except DE) in this feature, such as NR, AD, VA,
etc. The intention of this feature is to enable the
classiÔ¨Åer to automatically learn the structural rules
around DE. Given that the position of DE in the
parsing of [A DE B] is i, then the feature template
is deÔ¨Åned as in (7):

We employ Ô¨Åve features as well in the DPLVM
model, namely DEPOS, POS-gram, lexical fea-
tures, SemClass as well as a new feature:
tree-
pattern, which is discussed below.
fair comparison, we did so too. For the SMT experiments,
however, we kept it.

Ttree u = {ti‚àíl, . . . , ti‚àí1, ti, ti+1, . . . , ti+m}
(7)
Ttree b = {ti‚àílti‚àíl+1, . . . , ti‚àí1ti, titi+1, . . . , ti+m‚àí1ti+m}
where Ttree u is the sequence of unigrams in
connection with DE and Ttree b is the sequence of
bigrams related to DE; l and m are the window

291

sizes of A and B respectively. Generally, we use
all the unigrams and bigrams in the parsing of A
and B in our experiments. We argue that the im-
portant advantage of this feature is that it does not
depend on manually generated rules, but instead
of learns and generalises the reordering rules from
the training data directly.

POS-gram:
The POS-ngram feature adds all unigrams and bi-
grams in A and B. Given that the position of DE
is i in [A DE B], the feature template is deÔ¨Åned as
in (8):
Tpos u = {pi‚àíl, . . . , pi‚àí1, pi+1, . . . , pi+m}
Tpos b = {pi‚àílpi‚àíl+1, . . . , pi‚àí1pi+1, . . . , pi+m‚àí1pi+m}(8)
where Tpos u and Tpos b are uigrams and bigrams
in A and B. In the unigrams, we exclude the POS
of DE; in the bigrams, we include a bigram pair
across DE.

Some other features such as lexical features,
SemClass (cf. (Chang et al., 2009) for details) can
be deÔ¨Åned using similar feature template.

5 Experiments on DPLVM DE ClassiÔ¨Åer
In this section, we compare the performance of
DE classiÔ¨Åers between the DPLVM and log-linear
methods.

The accuracy of classiÔ¨Åcation is deÔ¨Åned as in

(9):

number of correctly labeled DEs

number of all DEs

√ó 100

(9)

Phrase Type

DEPOS
+A-pattern
+Tree-pattern
+POS-gram
+Lexical
+SemClass
+Topicality

Log-linear
2-A
5-A
71.0
54.8
83.7
67.9
-
-
84.9
72.1
86.5
74.9
86.7
75.1
75.4
86.9

DPLVM

5-A
56.2

-

69.6
73.6
76.4
76.8

-

2-A
72.3

-
85.2
86.5
87.9
88.3

-

Table 1: Comparison between the two classiÔ¨Åers
on 5-class and 2-class accuracy

Table 1 shows the comparison of accuracy, where
‚Äú5-A‚Äù and ‚Äú2-A‚Äù represent the accuracy of the
5-class and 2-class respectively. The 2-class is

the categorised classes of DE in (Wang et al.,
2007) which are deÔ¨Åned as ‚Äúreordered‚Äù and ‚Äúnon-
reordered‚Äù categories.
It can be seen that our
DPLVM classiÔ¨Åer outperforms the log-linear clas-
siÔ¨Åer by 1.4 absolute (1.86% and 1.61% rela-
tive respectively) points both on 5-class and 2-
class classiÔ¨Åcations. Furthermore, we see that
the DPLVM achieves signiÔ¨Åcantly better perfor-
mance than the log-linear model only with the
simple feature of ‚ÄúDEPOS‚Äù. As to the new feature
‚Äútree-pattern‚Äù, we can see that it achieves the im-
provement of 1.5% compared to the ‚ÄúA-pattern‚Äù in
terms of the accuracy of ‚Äú2-A‚Äù. This improvement
attributes to the good learning ability of DPLVM
as well as the strong generalisation capability of
the tree-pattern feature.

In terms of speed, in our task we only need to
label the Chinese character DE in the NP structure
[A DE B] rather than label the whole sentence, so
that we have a feature matrix of n √ó 1 for each
DE. Accordingly, the DPLVM classiÔ¨Åer can run
efÔ¨Åciently with low memory usage.

6 Experiments on SMT

6.1 Experimental Setting
For our SMT experiments, we used two systems,
namely Moses (Koehn et al., 2007) and Moses-
chart. The former is the state-of-the-art PB-SMT
system while the latter is a new extended sys-
tem of the Moses toolkit re-implementing the hi-
erarchical PB-SMT (HPB) model (Chiang, 2005).
The alignment is carried out by GIZA++ (Och and
Ney, 2003) and then we symmetrized the word
alignment using the grow-diag-Ô¨Ånal heuristic. Pa-
rameter tuning is performed using Minimum Error
Rate Training (Och, 2003).

The training data contains 2,159,232 sentence
pairs.The 5-gram language model is trained on the
English part of the parallel training data. The de-
velopment set (devset) is the NIST MT2006 test
set and the test set is the NIST MT2008 ‚Äúcur-
rent‚Äù test set. All the results are reported in terms
of BLEU (Papineni et al., 2002) and METEOR
(MTR) (Banerjee and Lavie, 2005) scores.

To run the DE classiÔ¨Åers, we use the Stan-
ford Chinese parser (Levy and Manning, 2003) to
parse the Chinese side of the MT training data, the

292

devset and test set.

6.2 Statistics of 5-class DE Annotation
For the DE-annotated MT experiments, after we
parse the training data, the devset and the test set,
we separately use the two DE classiÔ¨Åers to an-
notate the DE constructions in NPs in all of the
parsed data. Once the DE data are labeled, we
pre-process the Chinese data by reordering the
sentences only with ·àáBprepA and ·àárelc annota-
tions. Table 2 lists the statistics of the DE classes
in the MT training data, devset and test set using
our DPLVM classiÔ¨Åer. ‚Äú·àánon‚Äù denotes the unla-
beled ·àá(DE) which does not belong to any of the
5 classes.

6.3 Experimental Results
The experimental results from the PB-SMT and
HPB systems separately using the DPLVM and
log-linear classiÔ¨Åers are shown in Table 3.

BL
BLEU 22.42
MTR
52.03

PB-SMT

LL
23.47
53.25

LV
23.86
53.78

Moses-chart

BL
24.36
53.37

LL
24.75
53.75

LV
25.11
54.21

Table 3: Experimental results on PB-SMT and
Moses-chart. ‚ÄúBL‚Äù are the baselines; ‚ÄúLL‚Äù indi-
cates the log-linear model-based system; ‚ÄúLV‚Äù is
our DPLVM method.

The baseline systems indicate that the data is
neither categorised into DE classes nor reordered
on the Chinese side. We can see that (1) the
‚ÄúLV‚Äù method outperformed the ‚ÄúBL‚Äù and ‚ÄúLL‚Äù
by 1.44 absolute (6.42% relative), 0.39 absolute
(1.66% relative) BLEU points for PB-SMT, and
by 0.75 absolute (3.08% relative), 0.36 absolute
(1.45% relative) BLEU points for Moses-chart;
(2) the ‚ÄúLV‚Äù method achieved the improvements
for PB-SMT and Moses-chart in terms of MTR
scores compared to the ‚ÄúBL‚Äù and ‚ÄúLL‚Äù systems.
Therefore, using DE classiÔ¨Åcation and reorder-
ing on the source-side is helpful in improving
translation quality; (3) the results using DPLVM
achieve better translation quality than that of the
‚ÄúLL‚Äù processed data in terms of BLEU and ME-
TEOR (Banerjee and Lavie, 2005) scores, which
indirectly shows that DPLVM outperforms the

log-linear classiÔ¨Åcation model; and (4) the im-
provements on both PB-SMT and Moses-chart
show that the effectiveness of DE reordering is
consistent for different types of MT systems. The
results are veriÔ¨Åed by signiÔ¨Åcance test on 95%
conÔ¨Ådence interval (Zhang and Vogel, 2004).3

7 Analysis
In this section, we plan to evaluate how DE re-
ordering contributes to the improvement of trans-
lation quality in two respects, namely word align-
ment and phrase table.

7.1 Evaluating the Word Alignment
We create a word alignment test set which in-
cludes 500 sentences with human alignment anno-
tation, and then add this test set into the MT train-
ing corpus. Accordingly, the DE-reordered test set
is added into the reordered training corpus as well.
Thus, we run GIZA++ using the same conÔ¨Ågura-
tions for these two sets of data and symmetrize
the bidirectional word alignment using grow-diag
heuristic. The word alignment of the test set is
evaluated with the human annotation using Preci-
sion, Recall, F1 and AER measures. The results
are reported in Table 4.

non-reordered
reordered
Gain

P

71.67
74.02
2.35

R

62.02
62.79
0.77

F1
66.49
67.95
1.46

AER
33.44
31.98
-1.46

Table 4: Comparison of Precision, Recall, F1 and
AER scores of evaluating word alignment on orig-
inal and reordered data

We can see that in terms of the four measures,
the word alignment produced by the reordered
data is slightly better than that of the original data.
In some sense, we might say that the DE reorder-
ing is helpful in improving the word alignment of
the training data.

7.2 Evaluating the Phrase Table
Wang et al. (2007) proposed one way to indirectly
evaluate the phrase table by giving the same type
of input to the baseline and reordered systems,

3http://projectile.sv.cmu.edu/research/public/

tools/bootStrap/tutorial.htm.

293

training

devset

testset

DE-class
·àáAB
·àáAprepB
·àáAsB
·àáBprepA
·àárelc
·àánon
Total ·àá

count
312,679
6,975
13,205
658,589
316,772
46,547
1,354,767

percent (%)

count

percent (%)

count

percent (%)

23.08
0.51
0.97
47.31
23.38
3.44
100

523
9
23
956
419
97
2027

25.80
0.44
1.13
48.05
20.67
4.79
100

453
7
14
688
341
71
1574

28.78
0.44
0.89
43.71
21.66
4.51
100

Table 2: The number of different DE classes labeled for training data, devset and testset using the
DPLVM classiÔ¨Åer

with the consideration that if the reordered system
learned a better phrase table, then it may outper-
form the baseline on non-reordered inputs despite
the mismatch and vice versa. However, they did
not settle the question as to whether the reordered
system can learn better phrase tables.

We also try to use the idea of Wang et al (2007)
to carry out the phrase table evaluation on PB-
SMT,4 i.e. we tune the baseline on a reordered
devset and then evaluate on a reordered test set;
tune the reordered system on a non-reordered de-
vset and then evaluate on a non-reordered test set.
The results are shown in Table 5.

Testset

non-reordered set

reordered set

baseline
22.42
23.36

reordered

LL
22.76
23.47

DPLVM
22.85
23.86

Table 5: Comparison of BLEU scores in matched
and mismatched conditions on PB-SMT.

We Ô¨Ånd that (1) given the non-reordered test set,
the DE reordered system performs better than the
baseline system, which is consistent when differ-
ent DE classiÔ¨Åers are applied; (2) given the re-
ordered test set system, the reordered set produces
a better result than the baseline, which is also con-
sistent when different DE classiÔ¨Åers are applied;
and (3) the results from the DPLVM-based re-
ordered data are better than those from the LL-
based reordered data. From the comparison, one
might say that the reordered system was learned

4The phrases in HPB systems are different from those in
PB-SMT because they are variable-based, so we evaluate the
hierarchical phrases in (Du and Way, 2010)

a better phrase table and the reordered test set ad-
dresses the problem of word order.

To sum up, from the SMT results and the evalu-
ation results on the word alignment and the phrase
table, we can conclude that the DE reordering
methods contribute signiÔ¨Åcantly to the improve-
ments in translation quality, and it also implies
that using DE reordered data can achieve better
word alignment and phrase tables.

8 Conclusions and Future Work

In this paper, we presented a new classiÔ¨Åer: a
DPLVM model to classify the Chinese ·àá(DE)
constructions in NPs into 5 classes. We also pro-
posed a new and effective feature ‚Äì tree-pattern
‚Äì to automatically learn the reordering rules us-
ing the DPLVM algorithm. The experimental re-
sults showed that our DPLVM classiÔ¨Åer outper-
formed the log-linear model in terms of both the
classiÔ¨Åcation accuracy and MT translation quality.
In addition, the evaluation of the experimental re-
sults in section 7 indicates that the DE-reordering
approach is helpful in improving the accuracy of
the word alignment, and can also produce better
phrase pairs and thus generate better translations.
As for future work, Ô¨Årstly we plan to examine
and classify the DE constructions in other syn-
tactic structures such as VP, LCP etc. Secondly,
we plan to apply the DE-annotated approach in
a syntax-based MT system (Zollmann and Venu-
gopal, 2006) and examine the effects. We also in-
tend to improve the classiÔ¨Åcation accuracy of the
DE classiÔ¨Åer with richer features to further im-
prove translation quality.

294

Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In proceedings of the ACL‚Äô07,
pages 720‚Äì727.

Louis-Philippe Morency, Ariadna Quattoni and Trevor
Latent-dynamic Discriminative
In

Darrell.
Models for Continuous Gesture Recognition.
proceedings of CVPR‚Äô07, pages 1‚Äì8.

2007.

Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL‚Äô03, pages 160‚Äì167.

Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19‚Äì51.

Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the ACL-02, pages 311‚Äì318.

Xu Sun and Jun‚Äôichi Tsujii. 2009. Sequential Label-
ing with Latent Variables: An Exact Inference Algo-
rithm and An EfÔ¨Åcient Approximation. In Proceed-
ings of The European Chapter of the Association for
Computational Linguistics (EACL‚Äô09), pages 772-
780.

Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation.
In Proceedings of EMNLP-
CoNLL, pages 737‚Äì745.

Fei Xia and Michael McCord. 2004.

Improving a
statistical MT system with automatically learned
rewrite patterns.
In Proceedings of Coling 2004,
pages 508‚Äì514.

Ying Zhang and Stephan Vogel. 2004. Measuring
ConÔ¨Ådence Intervals for the Machine Translation
Evaluation Metrics. In Proceedings of the 10th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation (TMI), pages
85‚Äì94.

Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of HLT-NAACL 2006: Proceed-
ings of the Workshop on Statistical Machine Trans-
lation, New York, pages 138‚Äì141.

Acknowledgment
Many thanks to Dr. Pi-Chuan Chang for providing
the source code of her DE classiÔ¨Åer and manually
DE-annotated training data as well as valuable in-
struction in their use. Thanks also to Dr. Xu Sun
for the source code of his Latent Variable classi-
Ô¨Åer together with help in their use. This research
is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next
Generation Localisation (www.cngl.ie) at Dublin
City University.

References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
an automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, ACL-2005, pages 65‚Äì72.

Pi-Chuan Chang, Dan Jurafsky and Christopher D.
Manning. 2009 Disambiguating ‚ÄúDE‚Äù for Chinese-
English machine translation. In Proceedings of the
Fourth Workshop on SMT, pages 215‚Äì223.

David Chiang. 2005. A hierarchical phrase-based
In Pro-

model for statistical machine translation.
ceedings of ACL‚Äô05, pages 263‚Äì270.

Michael Collins, Philipp Koehn, and Ivona Kucerova.
newblock 2005. Clause restructuring for statisti-
cal machine translation. In Proceedings of ACL05,
pages 531‚Äì540.

Jinhua Du and Andy Way.

The impact
of source-side syntactic reordering on hierarchical
phrase-based SMT. In Proceedings of the 14th An-
nual conference of the European Association for
Machine Translation, Saint-Raphael, France.

2010.

Jakob Elming. 2008. Syntactic reordering integrated
with phrase-based SMT. In Proceedings of ACL-08
SSST-2, pages 46‚Äì54.

Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, Wade
Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A.
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In demonstration session of ACL‚Äô07, pages 177‚Äì
180.

Roger Levy and Christopher Manning. 2003.

Is it
harder to parse Chinese, or the Chinese treebank?
In Proceedings of ACL‚Äô03, pages 439‚Äì446.

