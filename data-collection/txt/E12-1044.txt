










































Coordination Structure Analysis using Dual Decomposition


Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 430–438,
Avignon, France, April 23 - 27 2012. c©2012 Association for Computational Linguistics

Coordination Structure Analysis using Dual Decomposition

Atsushi Hanamoto 1 Takuya Matsuzaki 1
1. Department of Computer Science, University of Tokyo, Japan
2. Web Search & Mining Group, Microsoft Research Asia, China
{hanamoto, matuzaki}@is.s.u-tokyo.ac.jp

jtsujii@microsoft.com

Jun’ichi Tsujii 2

Abstract

Coordination disambiguation remains a dif-
ficult sub-problem in parsing despite the
frequency and importance of coordination
structures. We propose a method for disam-
biguating coordination structures. In this
method, dual decomposition is used as a
framework to take advantage of both HPSG
parsing and coordinate structure analysis
with alignment-based local features. We
evaluate the performance of the proposed
method on the Genia corpus and the Wall
Street Journal portion of the Penn Tree-
bank. Results show it increases the per-
centage of sentences in which coordination
structures are detected correctly, compared
with each of the two algorithms alone.

1 Introduction

Coordination structures often give syntactic ambi-
guity in natural language. Although a wrong anal-
ysis of a coordination structure often leads to a
totally garbled parsing result, coordination disam-
biguation remains a difficult sub-problem in pars-
ing, even for state-of-the-art parsers.

One approach to solve this problem is a gram-
matical approach. This approach, however, of-
ten fails in noun and adjective coordinations be-
cause there are many possible structures in these
coordinations that are grammatically correct. For
example, a noun sequence of the form “n0 n1
and n2 n3” has as many as five possible struc-
tures (Resnik, 1999). Therefore, a grammatical
approach is not sufficient to disambiguate coor-
dination structures. In fact, the Stanford parser
(Klein and Manning, 2003) and Enju (Miyao and
Tsujii, 2004) fail to disambiguate a sentence I am

a freshman advertising and marketing major. Ta-
ble 1 shows the output from them and the correct
coordination structure.

The coordination structure above is obvious to
humans because there is a symmetry of conjuncts
(-ing) in the sentence. Coordination structures of-
ten have such structural and semantic symmetry
of conjuncts. One approach is to capture local
symmetry of conjuncts. However, this approach
fails in VP and sentential coordinations, which
can easily be detected by a grammatical approach.
This is because conjuncts in these coordinations
do not necessarily have local symmetry.

It is therefore natural to think that consider-
ing both the syntax and local symmetry of con-
juncts would lead to a more accurate analysis.
However, it is difficult to consider both of them
in a dynamic programming algorithm, which has
been often used for each of them, because it ex-
plodes the computational and implementational
complexity. Thus, previous studies on coordina-
tion disambiguation often dealt only with a re-
stricted form of coordination (e.g. noun phrases)
or used a heuristic approach for simplicity.

In this paper, we present a statistical analysis
model for coordination disambiguation that uses
the dual decomposition as a framework. We con-
sider both of the syntax, and structural and se-
mantic symmetry of conjuncts so that it outper-
forms existing methods that consider only either
of them. Moreover, it is still simple and requires
only O(n4) time per iteration, where n is the num-
ber of words in a sentence. This is equal to that
of coordination structure analysis with alignment-
based local features. The overall system still has a
quite simple structure because we need just slight
modifications of existing models in this approach,

430



Stanford parser/Enju
I am a ( freshman advertising ) and (
marketing major )

Correct coordination structure
I am a freshman ( ( advertising and mar-
keting ) major )

Table 1: Output from the Stanford parser, Enju and the
correct coordination structure

so we can easily add other modules or features for
future.

The structure of this paper is as follows. First,
we describe three basic methods required in the
technique we propose: 1) coordination structure
analysis with alignment-based local features, 2)
HPSG parsing, and 3) dual decomposition. Fi-
nally, we show experimental results that demon-
strate the effectiveness of our approach. We com-
pare three methods: coordination structure anal-
ysis with alignment-based local features, HPSG
parsing, and the dual-decomposition-based ap-
proach that combines both.

2 Related Work

Many previous studies for coordination disam-
biguation have focused on a particular type of NP
coordination (Hogan, 2007). Resnik (1999) dis-
ambiguated coordination structures by using se-
mantic similarity of the conjuncts in a taxonomy.
He dealt with two kinds of patterns, [n0 n1 and
n2 n3] and [n1 and n2 n3], where ni are all nouns.
He detected coordination structures based on sim-
ilarity of form, meaning and conceptual associa-
tion between n1 and n2 and between n1 and n3.
Nakov and Hearst (2005) used the Web as a train-
ing set and applied it to a task that is similar to
Resnik’s.

In terms of integrating coordination disam-
biguation with an existing parsing model, our ap-
proach resembles the approach by Hogan (2007).
She detected noun phrase coordinations by find-
ing symmetry in conjunct structure and the depen-
dency between the lexical heads of the conjuncts.
They are used to rerank the n-best outputs of the
Bikel parser (2004), whereas two models interact
with each other in our method.

Shimbo and Hara (2007) proposed an
alignment-based method for detecting and dis-
ambiguating non-nested coordination structures.

They disambiguated coordination structures
based on the edit distance between two conjuncts.
Hara et al. (2009) extended the method, dealing
with nested coordinations as well. We used their
method as one of the two sub-models.

3 Background

3.1 Coordination structure analysis with
alignment-based local features

Coordination structure analysis with alignment-
based local features (Hara et al., 2009) is a hy-
brid approach to coordination disambiguation that
combines a simple grammar to ensure consistent
global structure of coordinations in a sentence,
and features based on sequence alignment to cap-
ture local symmetry of conjuncts. In this section,
we describe the method briefly.

A sentence is denoted by x = x1...xk, where xi
is the i-th word of x. A coordination boundaries
set is denoted by y = y1...yk, where

yi =


(bl, el, br, er) (if xi is a coordinating

conjunction having left
conjunct xbl ...xel and
right conjunct xbr ...xer)

null (otherwise)

In other words, yi has a non-null value
only when it is a coordinating conjunction.
For example, a sentence I bought books and
stationary has a coordination boundaries set
(null, null, null, (3, 3, 5, 5), null).

The score of a coordination boundaries set is
defined as the sum of score of all coordinating
conjunctions in the sentence.

score(x, y) =

k∑
m=1

score(x, ym)

=
k∑

m=1

w · f(x, ym) (1)

where f(x, ym) is a real-valued feature vector of
the coordination conjunct xm. We used almost the
same feature set as Hara et al. (2009): namely, the
surface word, part-of-speech, suffix and prefix of
the words, and their combinations. We used the
averaged perceptron to tune the weight vector w.

Hara et al. (2009) proposed to use a context-
free grammar to find a properly nested coordina-
tion structure. That is, the scoring function Eq (1)

431



COORD Coordination.
CJT Conjunct.

N Non-coordination.
CC Coordinating conjunction like “and”.
W Any word.

Table 2: Non-terminals

Rules for coordinations:
COORDi,m → CJTi,jCCj+1,k−1CJTk,m
Rules for conjuncts:
CJTi,j → (COORD|N)i,j
Rules for non-coordinations:
Ni,k → COORDi,jNj+1,k
Ni,j →Wi,i(COORD|N)i+1,j
Ni,i →Wi,i
Rules for pre-terminals:
CCi,i → (and|or|but|, |; |+|+/−)i
CCi,i+1 → (, |; )i(and|or|but)i+1
CCi,i+2 → (as)i(well)i+1(as)i+2
Wi,i → ∗i

Table 3: Production rules

is only defined on the coordination structures that
are licensed by the grammar. We only slightly ex-
tended their grammar for convering more variety
of coordinating conjunctions.

Table 2 and Table 3 show the non-terminals and
production rules used in the model. The only ob-
jective of the grammar is to ensure the consistency
of two or more coordinations in a sentence, which
means for any two coordinations they must be ei-
ther non-overlapping or nested coordinations. We
use a bottom-up chart parsing algorithm to out-
put the coordination boundaries with the highest
score. Note that these production rules don’t need
to be isomorphic to those of HPSG parsing and
actually they aren’t. This is because the two meth-
ods interact only through dual decomposition and
the search spaces defined by the methods are con-
sidered separately.

This method requires O(n4) time, where n is
the number of words. This is because there are
O(n2) possible coordination structures in a sen-
tence, and the method requires O(n2) time to get
a feature vector of each coordination structure.

3.2 HPSG parsing

HPSG (Pollard and Sag, 1994) is one of the
linguistic theories based on lexicalized grammar

sign
PHON list of string

SYNSEM

synsem

LOCAL

local

CAT

category

HEAD
head
MODL synsem
MODR synsem

SUBJ list of synsem
COMPS list of synsem

SEM semantics

NONLOC
nonlocal
REL list of local
SLASH list of local

Figure 1: HPSG sign

2SUBJ    < >COMPS   < >2
HEAD
SUBJ    <    >
COMPS   < >

1

HEAD
SUBJ    < >
COMPS  < >

1

HEAD
SUBJ  
COMPS   <    |    >

1
COMPS   < >

HEAD
SUBJ  
COMPS   

1

2
3 4

3

4
2

Figure 2: Subject-Head Schema (left) and Head-
Complement Schema (right)

and unbounded dependencies. SEM feature rep-
resents the semantics of a constituent, and in this
study it expresses a predicate-argument structure.
Figure 2 presents the Subject-Head Schema

and the Head-Complement Schema1 defined in
(Pollard and Sag, 1994). In order to express gen-
eral constraints, schemata only provide sharing of
feature values, and no instantiated values.
Figure 3 has an example of HPSG parsing

of the sentence “Spring has come.” First, each
of the lexical entries for “has” and “come” are
unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
phrasal sign of the entire sentence is output on the
top of the derivation tree.

3 Acquiring HPSG from the Penn
Treebank

As discussed in Section 1, our grammar devel-
opment requires each sentence to be annotated
with i) a history of rule applications, and ii) ad-
ditional annotations to make the grammar rules
be pseudo-injective. In HPSG, a history of rule
applications is represented by a tree annotated
with schema names. Additional annotations are

1The value of category has been presented for simplicity,
while the other portions of the sign have been omitted.

Spring

HEAD  noun
SUBJ  < >
COMPS  < >

HEAD  verb
SUBJ  <    >

COMPS <                        >

5

has

HEAD  verb

SUBJ  <                         >

COMPS  < >

come

HEAD  verb
SUBJ  <    >
COMPS  < >

5
HEAD  noun
SUBJ  < >
COMPS  < >

HEAD
SUBJ  
COMPS   <    |    >

1
COMPS   < >

HEAD
SUBJ  
COMPS   

1

2
3 4

3

4
2

UnifyUnify

Head-complement
schema

Lexical entries

Spring

HEAD  noun
SUBJ  < >
COMPS  < > 2

HEAD  verb
SUBJ  <    >
COMPS  <    >

1

has

HEAD  verb
SUBJ  <    >
COMPS  < >

1

come

2

HEAD  verb
SUBJ  <    >
COMPS  < >

1

HEAD  verb
SUBJ  < >
COMPS  < >

1

subject-head

head-comp

Figure 3: HPSG parsing

required because HPSG schemata are not injec-
tive, i.e., daughters’ signs cannot be uniquely de-
termined given the mother. The following annota-
tions are at least required. First, the HEAD feature
of each non-head daughter must be specified since
this is not percolated to the mother sign. Second,
SLASH/REL features are required as described in
our previous study (Miyao et al., 2003a). Finally,
the SUBJ feature of the complement daughter in
the Head-Complement Schema must be specified
since this schema may subcategorize an unsatu-
rated constituent, i.e., a constituent with a non-
empty SUBJ feature. When the corpus is anno-
tated with at least these features, the lexical en-
tries required to explain the sentence are uniquely
determined. In this study, we define partially-
specified derivation trees as tree structures anno-
tated with schema names and HPSG signs includ-
ing the specifications of the above features.
We describe the process of grammar develop-

ment in terms of the four phases: specification,
externalization, extraction, and verification.

3.1 Specification
General grammatical constraints are defined in
this phase, and in HPSG, they are represented
through the design of the sign and schemata. Fig-
ure 1 shows the definition for the typed feature
structure of a sign used in this study. Some more
features are defined for each syntactic category al-

Figure 1: subject-head schema (left) and head-
complement schema (right); taken from Miyao et al.
(2004).

formalism. In a lexicalized grammar, quite a
small numbers of schemata are used to explain
general grammatical constraints, compared with
other theories. On the other hand, rich word-
specific characteristics are embedded in lexical
entries. Both of schemata and lexical entries
are represented by typed feature structures, and
constraints in parsing are checked by unification
among them. Figure 1 shows examples of HPSG
schema.

Figure 2 shows an HPSG parse tree of the sen-
tence “Spring has come.” First, the lexical en-
tries of “has” and “come” are joined by head-
complement schema. Unification gives the HPSG
sign of mother. After applying schemata to HPSG
signs repeatedly, the HPSG sign of the whole sen-
tence is output.

We use Enju for an English HPSG parser
(Miyao et al., 2004). Figure 3 shows how a co-
ordination structure is built in the Enju grammar.
First, a coordinating conjunction and the right
conjunct are joined by coord right schema. Af-
terwards, the parent and the left conjunct are
joined by coord left schema.

The Enju parser is equipped with a disam-
biguation model trained by the maximum entropy
method (Miyao and Tsujii, 2008). Since we do
not need the probability of each parse tree, we
treat the model just as a linear model that defines
the score of a parse tree as the sum of feature
weights. The features of the model are defined
on local subtrees of a parse tree.

The Enju parser takes O(n3) time since it uses
the CKY algorithm, and each cell in the CKY
parse table has at most a constant number of edges
because we use beam search algorithm. Thus, we
can regard the parser as a decoder for a weighted
CFG.

3.3 Dual decomposition

Dual decomposition is a classical method to solve
complex optimization problems that can be de-

432



sign
PHON list of string

SYNSEM

synsem

LOCAL

local

CAT

category

HEAD
head
MODL synsem
MODR synsem

SUBJ list of synsem
COMPS list of synsem

SEM semantics

NONLOC
nonlocal
REL list of local
SLASH list of local

Figure 1: HPSG sign

2SUBJ    < >COMPS   < >2
HEAD
SUBJ    <    >
COMPS   < >

1

HEAD
SUBJ    < >
COMPS  < >

1

HEAD
SUBJ  
COMPS   <    |    >

1
COMPS   < >

HEAD
SUBJ  
COMPS   

1

2
3 4

3

4
2

Figure 2: Subject-Head Schema (left) and Head-
Complement Schema (right)

and unbounded dependencies. SEM feature rep-
resents the semantics of a constituent, and in this
study it expresses a predicate-argument structure.
Figure 2 presents the Subject-Head Schema

and the Head-Complement Schema1 defined in
(Pollard and Sag, 1994). In order to express gen-
eral constraints, schemata only provide sharing of
feature values, and no instantiated values.
Figure 3 has an example of HPSG parsing

of the sentence “Spring has come.” First, each
of the lexical entries for “has” and “come” are
unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
phrasal sign of the entire sentence is output on the
top of the derivation tree.

3 Acquiring HPSG from the Penn
Treebank

As discussed in Section 1, our grammar devel-
opment requires each sentence to be annotated
with i) a history of rule applications, and ii) ad-
ditional annotations to make the grammar rules
be pseudo-injective. In HPSG, a history of rule
applications is represented by a tree annotated
with schema names. Additional annotations are

1The value of category has been presented for simplicity,
while the other portions of the sign have been omitted.

Spring

HEAD  noun
SUBJ  < >
COMPS  < >

HEAD  verb
SUBJ  <    >

COMPS <                        >

5

has

HEAD  verb

SUBJ  <                         >

COMPS  < >

come

HEAD  verb
SUBJ  <    >
COMPS  < >

5
HEAD  noun
SUBJ  < >
COMPS  < >

HEAD
SUBJ  
COMPS   <    |    >

1
COMPS   < >

HEAD
SUBJ  
COMPS   

1

2
3 4

3

4
2

UnifyUnify

Head-complement
schema

Lexical entries

Spring

HEAD  noun
SUBJ  < >
COMPS  < > 2

HEAD  verb
SUBJ  <    >
COMPS  <    >

1

has

HEAD  verb
SUBJ  <    >
COMPS  < >

1

come

2

HEAD  verb
SUBJ  <    >
COMPS  < >

1

HEAD  verb
SUBJ  < >
COMPS  < >

1

subject-head

head-comp

Figure 3: HPSG parsing

required because HPSG schemata are not injec-
tive, i.e., daughters’ signs cannot be uniquely de-
termined given the mother. The following annota-
tions are at least required. First, the HEAD feature
of each non-head daughter must be specified since
this is not percolated to the mother sign. Second,
SLASH/REL features are required as described in
our previous study (Miyao et al., 2003a). Finally,
the SUBJ feature of the complement daughter in
the Head-Complement Schema must be specified
since this schema may subcategorize an unsatu-
rated constituent, i.e., a constituent with a non-
empty SUBJ feature. When the corpus is anno-
tated with at least these features, the lexical en-
tries required to explain the sentence are uniquely
determined. In this study, we define partially-
specified derivation trees as tree structures anno-
tated with schema names and HPSG signs includ-
ing the specifications of the above features.
We describe the process of grammar develop-

ment in terms of the four phases: specification,
externalization, extraction, and verification.

3.1 Specification
General grammatical constraints are defined in
this phase, and in HPSG, they are represented
through the design of the sign and schemata. Fig-
ure 1 shows the definition for the typed feature
structure of a sign used in this study. Some more
features are defined for each syntactic category al-

Figure 2: HPSG parsing; taken from Miyao et al.
(2004).

Coordina(on

Le3,Conjunct Par(al,Coordina(on

Coordina(ng,
Conjunc(on

Right,
Conjunct

← coord_right_schema 

← coord_left_schema 

Figure 3: Construction of coordination in Enju

composed into efficiently solvable sub-problems.
It is becoming popular in the NLP community
and has been shown to work effectively on sev-
eral NLP tasks (Rush et al., 2010).

We consider an optimization problem

arg max
x

(f(x) + g(x)) (2)

which is difficult to solve (e.g. NP-hard), while
arg maxx f(x) and arg maxx g(x) are effectively
solvable. In dual decomposition, we solve

min
u

max
x,y

(f(x) + g(y) + u(x− y))

instead of the original problem.
To find the minimum value, we can use a sub-

gradient method (Rush et al., 2010). The subgra-
dient method is given in Table 4. As the algorithm

u(1) ← 0
for k = 1 to K do

x(k) ← arg maxx(f(x) + u(k)x)
y(k) ← arg maxy(g(y)− u(k)y)
if x = y then

return u(k)
end if
u(k+1) ← uk − ak(x(k) − y(k))

end for
return u(K)

Table 4: The subgradient method

shows, you can use existing algorithms and don’t
need to have an exact algorithm for the optimiza-
tion problem, which are features of dual decom-
position.

If x(k) = y(k) occurs during the algorithm, then
we simply take x(k) as the primal solution, which
is the exact answer. If not, we simply take x(K),
the answer of coordination structure analysis with
alignment-based features, as an approximate an-
swer to the primal solution. The answer does not
always solve the original problem Eq (2), but pre-
vious works (e.g., (Rush et al., 2010)) has shown
that it is effective in practice. We use it in this
paper.

4 Proposed method

In this section, we describe how we apply dual
decomposition to the two models.

4.1 Notation

We define some notations here. First we describe
weighted CFG parsing, which is used for both
coordination structure analysis with alignment-
based features and HPSG parsing. We follows the
formulation by Rush et al., (2010). We assume a
context-free grammar in Chomsky normal form,
with a set of non-terminals N . All rules of the
grammar are either the form A→ BC or A→ w
where A, B,C ∈ N and w ∈ V . For rules of the
form A→ w we refer to A as the pre-terminal for
w.

Given a sentence with n words, w1w2...wn, a
parse tree is a set of rule productions of the form
⟨A → BC, i, k, j⟩ where A,B, C ∈ N , and
1 ≤ i ≤ k ≤ j ≤ n. Each rule production rep-
resents the use of CFG rule A→ BC where non-
terminal A spans words wi...wj , non-terminal B

433



spans word wi...wk, and non-terminal C spans
word wk+1...wj if k < j, and the use of CFG
rule A→ wi if i = k = j.

We now define the index set for the coordina-
tion structure analysis as

Icsa = {⟨A→ BC, i, k, j⟩ : A,B, C ∈ N,
1 ≤ i ≤ k ≤ j ≤ n}

Each parse tree is a vector y = {yr : r ∈ Icsa},
with yr = 1 if rule r is in the parse tree, and yr =
0 otherwise. Therefore, each parse tree is repre-
sented as a vector in {0, 1}m, where m = |Icsa|.
We use Y to denote the set of all valid parse-tree
vectors. The set Y is a subset of {0, 1}m.

In addition, we assume a vector θcsa = {θcsar :
r ∈ Icsa} that specifies a score for each rule pro-
duction. Each θcsar can take any real value. The
optimal parse tree is y∗ = arg maxy∈Y y · θcsa
where y · θcsa =

∑
r yr · θcsar is the inner product

between y and θcsa.
We use similar notation for HPSG parsing. We

define Ihpsg , Z and θhpsg as the index set for
HPSG parsing, the set of all valid parse-tree vec-
tors and the weight vector for HPSG parsing re-
spectively.

We extend the index sets for both the coor-
dination structure analysis with alignment-based
features and HPSG parsing to make a constraint
between the two sub-problems. For the coor-
dination structure analysis with alignment-based
features we define the extended index set to be
I ′csa = Icsa

∪
Iuni where

Iuni = {(a, b, c) : a, b, c ∈ {1...n}}

Here each triple (a, b, c) represents that word
wc is recognized as the last word of the right
conjunct and the scope of the left conjunct or
the coordinating conjunction is wa...wb1. Thus
each parse-tree vector y will have additional com-
ponents ya,b,c. Note that this representation is
over-complete, since a parse tree is enough to
determine unique coordination structures for a
sentence: more explicitly, the value of ya,b,c is

1This definition is derived from the structure of a co-
ordination in Enju (Figure 3). The triples show where
the coordinating conjunction and right conjunct are in
coord right schema, and the left conjunct and partial coor-
dination are in coord left schema. Thus they alone enable
not only the coordination structure analysis with alignment-
based features but Enju to uniquely determine the structure
of a coordination.

1 if rule COORDa,c → CJTa,bCC , CJT ,c or
COORD ,c → CJT , CCa,bCJT ,c is in the parse
tree; otherwise it is 0.

We apply the same extension to the HPSG in-
dex set, also giving an over-complete representa-
tion. We define za,b,c analogously to ya,b,c.

4.2 Proposed method
We now describe the dual decomposition ap-
proach for coordination disambiguation. First, we
define the set Q as follows:

Q = {(y, z) : y ∈ Y, z ∈ Z, ya,b,c = za,b,c
for all (a, b, c) ∈ Iuni}

Therefore, Q is the set of all (y, z) pairs that
agree on their coordination structures. The coor-
dination structure analysis with alignment-based
features and HPSG parsing problem is then to
solve

max
(y,z)∈Q

(y · θcsa + γz · θhpsg) (3)

where γ > 0 is a parameter dictating the relative
weight of the two models and is chosen to opti-
mize performance on the development test set.

This problem is equivalent to

max
z∈Z

(g(z) · θcsa + γz · θhpsg) (4)

where g : Z → Y is a function that maps a
HPSG tree z to its set of coordination structures
z = g(y).

We solve this optimization problem by using
dual decomposition. Figure 4 shows the result-
ing algorithm. The algorithm tries to optimize
the combined objective by separately solving the
sub-problems again and again. After each itera-
tion, the algorithm updates the weights u(a, b, c).
These updates modify the objective functions for
the two sub-problems, encouraging them to agree
on the same coordination structures. If y(k) =
z(k) occurs during the iterations, then the algo-
rithm simply returns y(k) as the exact answer. If
not, the algorithm returns the answer of coordina-
tion analysis with alignment features as a heuristic
answer.

It is needed to modify original sub-problems
for calculating (1) and (2) in Table 4. We modified
the sub-problems to regard the score of u(a, b, c)
as a bonus/penalty of the coordination. The mod-
ified coordination structure analysis with align-
ment features adds u(k)(i, j, m) and u(k)(j+1, l−

434



u(1)(a, b, c)← 0 for all (a, b, c) ∈ Iuni
for k = 1 to K do

y(k) ← argmaxy∈Y(y · θcsa −
∑

(a,b,c)∈Iuni u
(k)(a, b, c)ya,b,c) ... (1)

z(k) ← argmaxz∈Z(z · θhpsg +
∑

(a,b,c)∈Iuni u
(k)(a, b, c)za,b,c) ... (2)

if y(k)(a, b, c) = z(k)(a, b, c) for all (a, b, c) ∈ Iuni then
return y(k)

end if
for all (a, b, c) ∈ Iuni do

u(k+1)(a, b, c)← u(k)(a, b, c)− ak(y(k)(a, b, c)− z(k)(a, b, c))
end for

end for
return y(K)

Figure 4: Proposed algorithm

w · f(x, (i, j, l,m)) to the score of the sub-
tree, when the rule production COORDi,m →
CJTi,jCCj+1,l−1CJTl,m is applied.

The modified Enju adds u(k)(i, j, l) when co-
ord left schema is applied, where word wc
is recognized as a coordinating conjunction
and left side of its scope is wa...wb, or co-
ord right schema is applied, where word wc
is recognized as a coordinating conjunction and
right side of its scope is wa...wb.

5 Experiments

5.1 Test/Training data
We trained the alignment-based coordination
analysis model on both the Genia corpus (?)
and the Wall Street Journal portion of the Penn
Treebank (?), and evaluated the performance of
our method on (i) the Genia corpus and (ii) the
Wall Street Journal portion of the Penn Treebank.
More precisely, we used HPSG treebank con-
verted from the Penn Treebank and Genia, and
further extracted the training/test data for coor-
dination structure analysis with alignment-based
features using the annotation in the Treebank. Ta-
ble ?? shows the corpus used in the experiments.

The Wall Street Journal portion of the Penn
Treebank has 2317 sentences from WSJ articles,
and there are 1356 COOD tags in the sentences,
while the Genia corpus has 1754 sentences from
MEDLINE abstracts, and there are 1848 COOD
tags in the sentences. COOD tags are further
subcategorized into phrase types such as NP-
COOD or VP-COOD. Table ?? shows the per-
centage of each phrase type in all COOD tags.
It indicates the Wall Street Journal portion of the

COORD WSJ Genia
NP 63.7 66.3
VP 13.8 11.4
ADJP 6.8 9.6
S 11.4 6.0
PP 2.4 5.1
Others 1.9 1.5

Table 6: The percentage of each conjunct type (%) of
each test set

Penn Treebank has more VP-COOD tags and S-
COOD tags, while the Genia corpus has more
NP-COOD tags and ADJP-COOD tags.

5.2 Implementation of sub-problems

We used Enju (?) for the implementation of
HPSG parsing, which has a wide-coverage prob-
abilistic HPSG grammar and an efficient parsing
algorithm, while we re-implemented Hara et al.,
(2009)’s algorithm with slight modifications.

5.2.1 Step size
We used the following step size in our algo-

rithm (Figure ??). First, we initialized a0, which
is chosen to optimize performance on the devel-
opment set. Then we defined ak = a0 · 2−ηk ,
where ηk is the number of times that L(u(k

′)) >
L(u(k

′−1)) for k′ ≤ k.

5.3 Evaluation metric

We evaluated the performance of the tested meth-
ods by the accuracy of coordination-level brack-
eting (?); i.e., we count each of the coordination
scopes as one output of the system, and the system

Figure 4: Proposed algorithm

1, m), as well as adding w · f(x, (i, j, l, m)) to
the score of the subtree, when the rule produc-
tion COORDi,m → CJTi,jCCj+1,l−1CJTl,m is
applied.

The modified Enju adds u(k)(a, b, c) when
coord right schema is applied, where word
wa...wb is recognized as a coordinating conjunc-
tion and the last word of the right conjunct is
wc, or coord left schema is applied, where word
wa...wb is recognized as the left conjunct and the
last word of the right conjunct is wc.

5 Experiments

5.1 Test/Training data

We trained the alignment-based coordination
analysis model on both the Genia corpus (Kim
et al., 2003) and the Wall Street Journal portion
of the Penn Treebank (Marcus et al., 1993), and
evaluated the performance of our method on (i)
the Genia corpus and (ii) the Wall Street Jour-
nal portion of the Penn Treebank. More precisely,
we used HPSG treebank converted from the Penn
Treebank and Genia, and further extracted the
training/test data for coordination structure analy-
sis with alignment-based features using the anno-
tation in the Treebank. Table 5 shows the corpus
used in the experiments.

The Wall Street Journal portion of the Penn
Treebank in the test set has 2317 sentences from
WSJ articles, and there are 1356 coordinations
in the sentences, while the Genia corpus in the
test set has 1764 sentences from MEDLINE ab-
stracts, and there are 1848 coordinations in the
sentences. Coordinations are further subcatego-

COORD WSJ Genia
NP 63.7 66.3
VP 13.8 11.4
ADJP 6.8 9.6
S 11.4 6.0
PP 2.4 5.1
Others 1.9 1.5

Table 6: The percentage of each conjunct type (%) of
each test set

rized into phrase types such as a NP coordination
or PP coordination. Table 6 shows the percentage
of each phrase type in all coordianitons. It indi-
cates the Wall Street Journal portion of the Penn
Treebank has more VP coordinations and S co-
ordianitons, while the Genia corpus has more NP
coordianitons and ADJP coordiations.

5.2 Implementation of sub-problems

We used Enju (Miyao and Tsujii, 2004) for
the implementation of HPSG parsing, which has
a wide-coverage probabilistic HPSG grammar
and an efficient parsing algorithm, while we re-
implemented Hara et al., (2009)’s algorithm with
slight modifications.

5.2.1 Step size

We used the following step size in our algo-
rithm (Figure 4). First, we initialized a0, which
is chosen to optimize performance on the devel-
opment set. Then we defined ak = a0 · 2−ηk ,
where ηk is the number of times that L(u(k

′)) >
L(u(k

′−1)) for k′ ≤ k.

435



Task (i) Task (ii)
Training WSJ (sec. 2–21) + Genia (No. 1–1600) WSJ (sec. 2–21)
Development Genia (No. 1601–1800) WSJ (sec. 22)
Test Genia (No. 1801–1999) WSJ (sec. 23)

Table 5: The corpus used in the experiments

Proposed Enju CSA
Precision 72.4 66.3 65.3
Recall 67.8 65.5 60.5
F1 70.0 65.9 62.8

Table 7: Results of Task (i) on the test set. The preci-
sion, recall, and F1 (%) for the proposed method, Enju,
and Coordination structure analysis with alignment-
based features (CSA)

5.3 Evaluation metric

We evaluated the performance of the tested meth-
ods by the accuracy of coordination-level bracket-
ing (Shimbo and Hara, 2007); i.e., we count each
of the coordination scopes as one output of the
system, and the system output is regarded as cor-
rect if both of the beginning of the first output
conjunct and the end of the last conjunct match
annotations in the Treebank (Hara et al., 2009).

5.4 Experimental results of Task (i)

We ran the dual decomposition algorithm with a
limit of K = 50 iterations. We found the two
sub-problems return the same answer during the
algorithm in over 95% of sentences.

We compare the accuracy of the dual decompo-
sition approach to two baselines: Enju and coor-
dination structure analysis with alignment-based
features. Table 7 shows all three results. The dual
decomposition method gives a statistically signif-
icant gain in precision and recall over the two
methods2.

Table 8 shows the recall of coordinations of
each type. It indicates our re-implementation of
CSA and Hara et al. (2009) have a roughly simi-
lar performance, although their experimental set-
tings are different. It also shows the proposed
method took advantage of Enju and CSA in NP
coordination, while it is likely just to take the an-
swer of Enju in VP and sentential coordinations.
This means we might well use dual decomposi-

2p < 0.01 (by chi-square test)

60%$

65%$

70%$

75%$

80%$

85%$

90%$

95%$

100%$

1$ 3$ 5$ 7$ 9$ 11$13$15$17$19$21$23$25$27$29$31$33$35$37$39$41$43$45$47$49$

accuracy certificates 

Figure 5: Performance of the approach as a function of
K of Task (i) on the development set. accuracy (%):
the percentage of sentences that are correctly parsed.
certificates (%): the percentage of sentences for which
a certificate of optimality is obtained.

tion only on NP coordinations to have a better re-
sult.

Figure 5 shows performance of the approach as
a function of K, the maximum number of iter-
ations of dual decomposition. The graphs show
that values of K much less than 50 produce al-
most identical performance to K = 50 (with
K = 50, the accuracy of the method is 73.4%,
with K = 20 it is 72.6%, and with K = 1 it
is 69.3%). This means you can use smaller K in
practical use for speed.

5.5 Experimental results of Task (ii)

We also ran the dual decomposition algorithm
with a limit of K = 50 iterations on Task (ii).
Table 9 and 10 show the results of task (ii). They
show the proposed method outperformed the two
methods statistically in precision and recall3.

Figure 6 shows performance of the approach as
a function of K, the maximum number of iter-
ations of dual decomposition. The convergence
speed for WSJ was faster than that for Genia. This
is because a sentence of WSJ often have a simpler
coordination structure, compared with that of Ge-
nia.

3p < 0.01 (by chi-square test)

436



COORD # Proposed Enju CSA # Hara et al. (2009)
Overall 1848 67.7 63.3 61.9 3598 61.5
NP 1213 67.5 61.4 64.1 2317 64.2
VP 208 79.8 78.8 66.3 456 54.2
ADJP 193 58.5 59.1 54.4 312 80.4
S 111 51.4 52.3 34.2 188 22.9
PP 110 64.5 59.1 57.3 167 59.9
Others 13 78.3 73.9 65.2 140 49.3

Table 8: The number of coordinations of each type (#), and the recall (%) for the proposed method, Enju,
coordination structure analysis with alignment-based features (CSA) , and Hara et al. (2009) of Task (i) on the
development set. Note that Hara et al. (2009) uses a different test set and different annotation rules, although its
test data is also taken from the Genia corpus. Thus we cannot compare them directly.

Proposed Enju CSA
Precision 76.3 70.7 66.0
Recall 70.6 69.0 60.1
F1 73.3 69.9 62.9

Table 9: Results of Task (ii) on the test set. The preci-
sion, recall, and F1 (%) for the proposed method, Enju,
and Coordination structure analysis with alignment-
based features (CSA)

COORD # Proposed Enju CSA
Overall 1017 71.6 68.1 60.7
NP 573 76.1 71.0 67.7
VP 187 62.0 62.6 47.6
ADJP 73 82.2 75.3 79.5
S 141 64.5 62.4 42.6
PP 19 52.6 47.4 47.4
Others 24 62.5 70.8 54.2

Table 10: The number of coordinations of each type
(#), and the recall (%) for the proposed method, Enju,
and coordination structure analysis with alignment-
based features (CSA) of Task (ii) on the development
set.

6 Conclusion and Future Work

In this paper, we presented an efficient method for
detecting and disambiguating coordinate struc-
tures. Our basic idea was to consider both gram-
mar and symmetries of conjuncts by using dual
decomposition. Experiments on the Genia corpus
and the Wall Street Journal portion of the Penn
Treebank showed that we could obtain statisti-
cally significant improvement in accuracy when
using dual decomposition.

We would need a further study in the follow-
ing points of view: First, we should evaluate our

60%$

65%$

70%$

75%$

80%$

85%$

90%$

95%$

100%$

1$ 3$ 5$ 7$ 9$ 11$13$15$17$19$21$23$25$27$29$31$33$35$37$39$41$43$45$47$49$

accuracy certificates 

Figure 6: Performance of the approach as a function of
K of Task (ii) on the development set. accuracy (%):
the percentage of sentences that are correctly parsed.
certificates (%): the percentage of sentences for which
a certificate of optimality is provided.

method with corpus in different domains. Be-
cause characteristics of coordination structures
differs from corpus to corpus, experiments on
other corpus would lead to a different result. Sec-
ond, we would want to add some features to coor-
dination structure analysis with alignment-based
local features such as ontology. Finally, we can
add other methods (e.g. dependency parsing) as
sub-problems to our method by using the exten-
sion of dual decomposition, which can deal with
more than two sub-problems.

Acknowledgments

The second author is partially supported by KAK-
ENHI Grant-in-Aid for Scientific Research C
21500131 and Microsoft CORE project 7.

437



References
Kazuo Hara, Masashi Shimbo, Hideharu Okuma, and

Yuji Matsumoto. 2009. Coordinate structure analy-
sis with global structural constraints and alignment-
based local features. In Proceedings of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP, pages 967–975, Aug.

Deirdre Hogan. 2007. Coordinate noun phrase dis-
ambiguation in a generative parsing model. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL 2007),
pages 680–687.

Jun-Dong Kim, Tomoko Ohta, and Jun’ich Tsujii.
2003. Genia corpus - a semantically annotated cor-
pus for bio-textmining. Bioinformatics, 19.

Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems, 15:3–10.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19:313–330.

Yusuke Miyao and Jun’ich Tsujii. 2004. Deep lin-
guistic analysis for the accurate identification of
predicate-argument relations. In Proceeding of
COLING 2004, pages 1392–1397.

Yusuke Miyao and Jun’ich Tsujii. 2008. Feature
forest models for probabilistic hpsg parsing. MIT
Press, 1(34):35–80.

Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP 2004).

Preslav Nakov and Marti Hearst. 2005. Using the web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language (HLT-
EMNLP 2005), pages 835–842.

Carl Pollard and Ivan A. Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.

Philip Resnik. 1999. Semantic similarity in a takon-
omy. Journal of Artificial Intelligence Research,
11:95–130.

Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natu-
ral language processing. In Proceeding of the con-
ference on Empirical Methods in Natural Language
Processing.

Masashi Shimbo and Kazuo Hara. 2007. A discrimi-
native learning model for coordinate conjunctions.

In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 610–619, Jun.

438


