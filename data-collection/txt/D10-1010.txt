










































Learning the Relative Usefulness of Questions in Community QA


Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 97–107,
MIT, Massachusetts, USA, 9-11 October 2010. c©2010 Association for Computational Linguistics

Learning the Relative Usefulness of Questions in Community QA

Razvan Bunescu

School of EECS

Ohio University

Athens, OH 43201, USA

bunescu@ohio.edu

Yunfeng Huang

School of EECS

Ohio University

Athens, OH 43201, USA

yh324906@ohio.edu

Abstract

We present a machine learning approach for

the task of ranking previously answered ques-

tions in a question repository with respect to

their relevance to a new, unanswered refer-

ence question. The ranking model is trained

on a collection of question groups manually

annotated with a partial order relation reflect-

ing the relative utility of questions inside each

group. Based on a set of meaning and struc-

ture aware features, the new ranking model is

able to substantially outperformmore straight-

forward, unsupervised similarity measures.

1 Introduction

Open domain Question Answering (QA) is one of

the most complex and challenging tasks in natural

language processing. In general, a question answer-

ing system may need to integrate knowledge coming

from a wide variety of linguistic processing tasks

such as syntactic parsing, semantic role labeling,

named entity recognition, and anaphora resolution

(Prager, 2006). State of the art implementations of

these linguistic analysis tasks are still limited in their

performance, with errors that compound and prop-

agate into the final performance of the QA system

(Moldovan et al., 2002). Consequently, the perfor-

mance of open domain QA systems has yet to ar-

rive at a level at which it would become a feasible

alternative to the current paradigms for information

access based on keyword searches.

Recently, community-driven QA sites such as Ya-

hoo! Answers and WikiAnswers 1 have established

1answers.yahoo.com, wiki.answers.com

a new approach to question answering that shifts

the inherent complexity of open domain QA from

the computer system to volunteer contributors. The

computer is no longer required to perform a deep

linguistic analysis of questions and generate corre-

sponding answers, and instead acts as a mediator

between users submitting questions and volunteers

providing the answers.

An important objective in community QA is to

minimize the time elapsed between the submission

of questions by users and the subsequent posting

of answers by volunteer contributors. One useful

strategy for minimizing the response latency is to

search the QA repository for similar questions that

have already been answered, and provide the cor-

responding ranked list of answers, if such a ques-

tion is found. The success of this approach de-

pends on the definition and implementation of the

question-to-question similarity function. In the sim-

plest solution, the system searches for previously

answered questions based on exact string match-

ing with the reference question. Alternatively, sites

such as WikiAnswers allow the users to mark ques-

tions they think are rephrasings (“alternate word-

ings”, or paraphrases) of existing questions. These

question clusters are then taken into account when

performing exact string matching, therefore increas-

ing the likelihood of finding previously answered

questions that are semantically equivalent to the ref-

erence question.

In order to lessen the amount of work required

from the contributors, an alternative approach is to

build a system that automatically finds rephrasings

of questions, especially since question rephrasing

97



seems to be computationally less demanding than

question answering. According to previous work in

this domain, a question is considered a rephrasing of

a reference question Q0 if it uses an alternate word-

ing to express an identical information need. For

example, Q0 and Q1 below are rephrasings of each

other, and consequently they are expected to have

the same answer.

Q0 What should I feed my turtle?

Q1 What do I feed my pet turtle?

Paraphrasings of a new question cannot always be

found in the community QA repository. We believe

that computing a ranked list of existing questions

that at least partially address the original informa-

tion need could also be useful to the user, at least

until other users volunteer to give an exact answer

to the original, unanswered reference question. For

example, in the absence of any additional informa-

tion about the reference question Q0, the expected

answers to questions Q2 and Q3 below may be seen

as partially overlapping in information content with

the expected answer for the reference question Q0.

An answer to question Q4, on the other hand, is less

likely to benefit the user, even though it has a signif-

icant lexical overlap with the reference question.

Q2 What kind of fish should I feed my turtle?

Q3 What do you feed a turtle that is the size of a

quarter?

Q4 What kind of food should I feed a turtle dove?

In this paper, we propose a supervised learning

approach to the question ranking problem, a gen-

eralization of the question paraphrasing problem in

which questions are ranked in a partial order based

on the relative information overlap between their ex-

pected answers and the expected answer of the refer-

ence question. Underlying the question ranking task

is the expectation that the user who submits a ref-

erence question will find the answers of the highly

ranked questions to be more useful than the answers

associated with the lower ranked questions. For the

reference question Q0 above, the learned ranking

model is expected to produce a partial order in which

Q1 is ranked higher than Q2, Q3 and Q4, whereas

Q2 and Q3 are ranked higher than Q4.

2 Partially Ordered Datasets for Question

Ranking

In order to enable the evaluation of question rank-

ing approaches, we have previously created a dataset

of 60 groups of questions (Bunescu and Huang,

2010b). Each group consists of a reference question

(e.g. Q0 above) that is associated with a partially or-

dered set of questions (e.g. Q1 to Q4 above). For

each reference questions, its corresponding partially

ordered set is created from questions in Yahoo! An-

swers and other online repositories that have a high

cosine similarity with the reference question. Out

of the 26 top categories in Yahoo! Answers, the 60

reference questions span a diverse set of categories.

Figure 1 lists the 20 categories covered, where each

category is shown with the number of corresponding

reference questions between parentheses.

Travel (10), Computers & Internet (6),

Beauty & Style (5), Entertainment &

Music (5), Food & Drink (5), Health (5),

Arts & Humanities (3), Cars &

Transportation (3), Consumer Electronics

(3), Pets (3), Family & Relationships

(2), Science & Mathematics (2),

Education & Reference (1), Environment

(1), Local Businesses (1), Pregnancy &

Parenting (1), Society & Culture (1),

Sports (1), Yahoo! Products (1)

Figure 1: The 20 categories represented in the dataset.

Inside each group, the questions are manually an-

notated with a partial order relation, according to

their utility with respect to the reference question.

We use the notation 〈Qi ≻ Qj |Qr〉 to encode the
fact that question Qi ismore useful than question Qj
with respect to the reference question Qr. Similarly,

〈Qi = Qj〉will be used to express the fact that ques-
tions Qi and Qj are reformulations of each other

(the reformulation relation is independent of the ref-

erence question). The partial ordering among the

questionsQ0 toQ4 above can therefore be expressed

concisely as follows: 〈Q0 = Q1〉, 〈Q1 ≻ Q2|Q0〉,
〈Q1 ≻ Q3|Q0〉, 〈Q2 ≻ Q4|Q0〉, 〈Q3 ≻ Q4|Q0〉.
Note that we do not explicitly annotate the rela-

tion 〈Q1 ≻ Q4|Q0〉, since it can be inferred based
on the transitivity of the more useful than relation:

〈Q1 ≻ Q2|Q0〉∧〈Q2 ≻ Q4|Q0〉 ⇒ 〈Q1 ≻ Q4|Q0〉.

98



REFERENCE QUESTION (Qr)

Q5 What’s a nice summer camp to go to in Florida?

PARAPHRASING QUESTIONS (P )
Q6 What camps are good for a vacation during the summer in FL?

Q7 What summer camps in FL do you recommend?

USEFUL QUESTIONS (U )
Q8 Does anyone know a good art summer camp to go to in FL?

Q9 Are there any good artsy camps for girls in FL?

Q10 What are some summer camps for like singing in Florida?

Q11 What is a good cooking summer camp in FL?

Q12 Do you know of any summer camps in Tampa, FL?

Q13 What is a good summer camp in Sarasota FL for a 12 year old?

Q14 Can you please help me find a surfing summer camp for beginners in Treasure Coast, FL?

Q15 Are there any acting summer camps and/or workshops in the Orlando, FL area?

Q16 Does anyone know any volleyball camps in Miramar, FL?

Q17 Does anyone know about any cool science camps in Miami?

Q18 What’s a good summer camp you’ve ever been to?

NEUTRAL QUESTIONS (N )
Q19 What’s a good summer camp in Canada?

Q20 What’s the summer like in Florida?

Table 1: A question group.

Also note that no relation is specified between Q2
and Q3, and similarly no relation can be inferred be-

tween these two questions. This reflects our belief

that, in the absence of any additional information re-

garding the user or the “turtle” referenced in Q0, we

cannot compare questions Q2 and Q3 in terms of

their usefulness with respect to Q0.

Table 1 shows another reference questionQ5 from

our dataset, together with its annotated group of

questions Q6 to Q20. In order to make the anno-

tation process easier and reproducible, we have di-

vided it into two levels of annotation. During the

first annotation stage, each question group is parti-

tioned manually into 3 subgroups of questions:

• P is the set of paraphrasing questions.

• U is the set of useful questions.

• N is the set of neutral questions.

A question is deemed useful if its expected answer

may overlap in information content with the ex-

pected answer of the reference question. The ex-

pected answer of a neutral question, on the other

hand, should be irrelevant with respect to the ref-

erence question. Let Qr be the reference question,

Qp ∈ P a paraphrasing question, Qu ∈ U a useful
question, and Qn ∈ N a neutral question. Then the
following relations are assumed to hold among these

questions:

1. 〈Qp ≻ Qu|Qr〉: a paraphrasing question is
more useful than a useful question.

2. 〈Qu ≻ Qn|Qr〉: a useful question is more use-
ful than a neutral question.

Note that as long as these relations hold between

the 3 types of questions, the names of the sub-

groups and their definitions are irrelevant with re-

spect to the implied set of more useful than rela-

tions, since only the implied ternary relations will

be used for training and evaluating question rank-

ing approaches. We also assume that, by tran-

sitivity, the following ternary relations also hold:

〈Qp ≻ Qn|Qr〉, i.e. a paraphrasing question is
more useful than a neutral question. Furthermore, if

Qp1 , Qp2 ∈ P are two paraphrasing questions, this
implies 〈Qp1 = Qp2 |Qr〉.

99



For the vast majority of questions, the first annota-

tion stage is straightforward and non-controversial.

In the second annotation stage, we perform a finer

annotation of relations between questions in the

middle group U . Table 1 shows two such relations
(using indentation): 〈Q8 ≻ Q9|Q5〉 and 〈Q8 ≻
Q10|Q5〉. Question Q8 would have been a rephras-
ing of the reference question, were it not for the

noun “art” modifying the focus noun phrase “sum-

mer camp”. Therefore, the information content of

the answer to Q8 is strictly subsumed in the infor-

mation content associated with the answer to Q5.

Similarly, inQ9 the focus noun phrase is further spe-

cialized through the prepositional phrase “for girls”.

Therefore, (an answer to) Q9 is less useful to Q5
than (an answer to) Q8, i.e. 〈Q8 ≻ Q9|Q5〉. Fur-
thermore, the focus “art summer camp” in Q8 con-

ceptually subsumes the focus “summer camps for

singing” in Q10, therefore 〈Q8 ≻ Q10|Q5〉.
We call this dataset simple since most of the ref-

erence questions are shorter than the other questions

in their group. We have also created a complex ver-

sion of the same dataset, by selecting as the refer-

ence question in each group a longer question from

the same group. For example, if Q0 were a reference

question, it would be replaced with a more complex

question, such as Q2, or Q3. The annotation is re-

done to reflect the relative usefulness relations with

respect to the new reference questions. We believe

that the new complex dataset is closer to the actual

distribution of questions in community QA reposi-

tories: unanswered questions tend to be more spe-

cific (longer), whereas general questions (shorter)

are more likely to have been answered already. Each

dataset is annotated by two annotators, leading to

a total of 4 datasets: Simple1, Simple2, Complex1,

and Complex2.

Table 2 presents the following statistics on the two

types of datasets (Simple, Complex) for each anno-

tator (1, 2): the total number of paraphrasings (P),
the total number of useful questions (U), the total
number of neutral questions (N ), the total number
of more useful than ordered pairs encoded in the

dataset, either explicitly or through transitivity, and

the Inter-Annotator Agreement (ITA). We compute

the ITA as the precision (P) and recall (R) with re-

spect to the more useful than ordered pairs encoded

in one annotation (Pairs1) relative to the ordered

Dataset P U N Pairs ITA
Simple1 164 775 594 11015 P: 76.6

Simple2 134 778 621 10436 R: 81.6

Complex1 103 766 664 10654 P: 71.3

Complex2 89 730 714 9979 R: 81.3

Table 2: Dataset statistics.

pairs encoded in the other annotation (Pairs2).

P =
|Pairs1 ∩ Pairs2|

Pairs1
R =

|Pairs1 ∩ Pairs2|

Pairs2

The statistics in Table 2 indicate that the second

annotator was in general more conservative in tag-

ging questions as paraphrases or useful questions.

3 Unsupervised Methods for Question

Ranking

An ideal question ranking method would take an ar-

bitrary triplet of questions Qr, Qi and Qj as input,

and output an ordering between Qi and Qj with re-

spect to the reference question Qr, i.e. one of 〈Qi ≻
Qj |Qr〉, 〈Qi = Qj |Qr〉, or 〈Qj ≻ Qi|Qr〉. One ap-
proach is to design a usefulness function u(Qi, Qr)
that measures how useful question Qi is for the ref-

erence question Qr, and define the more useful than

(≻) relation as follows:

〈Qi ≻ Qj |Qr〉 ⇔ u(Qi, Qr) > u(Qj , Qr)

If we define I(Q) to be the information need asso-
ciated with question Q, then u(Qi, Qr) could be de-
fined as a measure of the relative overlap between

I(Qi) and I(Qr). Unfortunately, the information
need is a concept that, in general, is defined only

intensionally and therefore it is difficult to measure.

For lack of an operational definition of the informa-

tion need, we will approximate u(Qi, Qr) directly
as a measure of the similarity between Qi and Qr.

The similarity between two questions can be seen as

a special case of text-to-text similarity, consequently

one possibility is to use a general text-to-text simi-

larity function such as cosine similarity in the vector

space model (Baeza-Yates and Ribeiro-Neto, 1999):

cos(Qi, Qr) =
QTi Qr

‖Qi‖‖Qr‖

100



Here, Qi and Qr denote the corresponding tf×idf
vectors.

As a measure of question similarity, one major

drawback of cosine similarity is that it is oblivious

of the meanings of words in each question. This par-

ticular problem is illustrated by the three questions

below. Q22 and Q23 have the same cosine similar-

ity with Q21, they are therefore indistinguishable in

terms of their usefulness to the reference question

Q21, even though we expect Q22 to be more use-

ful than Q23 (a place that sells hydrangea often sells

other types of plants too, possibly including cacti).

Q21 Where can I buy a hydrangea?

Q22 Where can I buy a cactus?

Q23 Where can I buy an iPad?

To alleviate the lexical chasm, we can redefine

u(Qi, Qr) to be the similarity measure proposed by
(Mihalcea et al., 2006) as follows:

mcs(Qi, Qr) =

∑

w∈{Qi}

maxSim(w,Qr) ∗ idf(w)

∑

w∈{Qi}

idf(w)
+

∑

w∈{Qr}

maxSim(w,Qi) ∗ idf(w)

∑

w∈{Qr}

idf(w)

Since scaling factors are immaterial for ranking, we

have ignored the normalization constant contained

in the original measure. For each word w ∈ Qi,
maxSim(w, Qr) computes the maximum semantic
similarity between w and any word wr ∈ Qr. The
similarity scores are weighted by the correspond-

ing idf’s, and normalized. A similar score is com-

puted for each word w ∈ Qr. The score computed
by maxSim depends on the actual function used

to compute the word-to-word semantic similarity.

In this paper, we evaluated four of the knowledge-

based measures explored in (Mihalcea et al., 2006):

wup (Wu and Palmer, 1994), res (Resnik, 1995), lin

(Lin, 1998), and jcn (Jiang and Conrath, 1997).

4 Supervised Learning for Question

Ranking

Cosine similarity, henceforth referred as cos, treats

questions as bags-of-words. The meta-measure pro-

posed in (Mihalcea et al., 2006), henceforth called

mcs, treats questions as bags-of-concepts. Both cos

and mcs ignore the syntactic relations between the

words in a question, and therefore may miss impor-

tant structural information. In the next three sec-

tions we describe a set of structural features that we

believe are relevant for judging question similarity.

These and other types of features will be integrated

in an SVM model for ranking, as described later in

Section 4.4.

4.1 Matching the Focus Words

If we consider the question Q24 below as reference,

question Q26 will be deemed more useful than Q25
when using cos or mcs because of the higher rela-

tive lexical and conceptual overlap with Q24. How-

ever, this is contrary to the actual ordering 〈Q25 ≻
Q26|Q24〉, which reflects the fact that Q25, which
expects the same answer type as Q24, should be

deemed more useful than Q26, which has a differ-

ent answer type.

Q24 What are some good thriller movies?

Q25 What are some thriller movies with happy end-

ing?

Q26 What are some good songs from a thriller

movie?

The analysis above shows the importance of us-

ing the answer type when computing the similar-

ity between two questions. However, instead of re-

lying exclusively on a predefined hierarchy of an-

swer types, we identify the question focus of a ques-

tion, defined as the set of maximal noun phrases in

the question that corefer with the expected answer

(Bunescu and Huang, 2010a). Focus nouns such as

movies and songs provide more discriminative in-

formation than general answer types such as prod-

ucts. We use answer types only for questions such

as Q27 or Q28 below that lack an explicit question

focus. In such cases, an artificial question focus is

created from the answer type (e.g. location for Q27,

or method for Q28).

101



Q27 Where can I buy a good coffee maker?

Q28 How do I make a pizza?

Let fi and fr be the focus words corresponding to

questions Qi and Qr. We introduce a focus feature

φf , and set its value to be equal with the similarity

between the focus words:

φf (Qi, Qr) = wsim(fi, fr) (1)

We use wsim to denote a generic word meaning sim-

ilarity measure (e.g. wup, res, lin or jcn). When

computing the focus feature, the non-focus word

“movie” in Q26 will not be compared with the fo-

cus word “movies” in Q24, and therefore Q26 will

have a lower value for this feature than Q25, i.e.

φf (Q26, Q24) < φf (Q25, Q24).

4.2 Matching the Main Verbs

In addition to the question focus, the main verb of

a question can also provide key information in es-

timating question-to-question similarity. We define

the main verb to be the content verb that is highest

in the dependency tree of the question, e.g. buy for

Q27, or make for Q28. If the question does not con-

tain a content verb, the main verb is defined to be the

highest verb in the dependency tree, as for example

are in Q24 to Q26. The utility of a question’s main

verb in judging its similarity to other questions can

be seen more clearly in the questions below, where

Q29 is the reference:

Q29 How can I transfer music from iTunes to my

iPod?

Q30 How can I upload music to my iPod?

Q31 How can I play music in iTunes?

The fact that upload, as the main verb of Q30, is

more semantically related to transfer is essential in

deciding that 〈Q30 ≻ Q31|Q29〉, i.e. Q30 is more
useful than Q31 to Q29.

Let vi and vr be the main verbs corresponding to

questions Qi and Qr. We introduce a main verb fea-

ture φv as follows:

φv(Qi, Qr) = wsim(vi, vr) (2)

If Q29 is considered as reference question, it is ex-

pected that the main verb feature for question Q30
will have a higher value than the main verb feature

for Q31, i.e. φf (Q31, Q29) < φf (Q30, Q29).

Figure 2: Matched dependency trees.

4.3 Matching the Dependency Trees

The question focus and the main verb are only two

of the nodes in the syntactic dependency tree of a

question. In general, all the words in a question are

important when judging its semantic similarity with

another question. We therefore propose a more gen-

eral feature that exploits the dependency structure

of the question and, in doing so, it also considers

all the words in the question, like cos and mcs. For

any given question we initially ignore the direction

of the dependency arcs and change the question de-

pendency tree to be rooted at the focus word, as il-

lustrated in Figure 2 for questions Q5 and Q9. In-

terrogative patterns such as “What is” or “Are there

any” are automatically eliminated from the depen-

dency trees. We define the dependency tree similar-

ity between two questions Qi and Qr to be a func-

tion of similarities wsim(vi, vr) computed between
aligned nodes vi ∈ Qi and vr ∈ Qr. The nodes
of two dependency trees are aligned through a func-

tion MaxMatch(ui.C, ur.C) that takes two sets of
children nodes as arguments, one from Qi and one

from Qr, and finds the maximum weighted bipartite

matching between ui.C and ur.C. Given two chil-
dren nodes vi ∈ ui.C and vr ∈ ur.C, the weight of
a potential matching between vi and vr is defined

102



simply as wsim(vi, vr). MaxMatch(ui.C, ur.C) is
furthermore constrained to match only nodes that

have compatible part-of-speech tags (e.g. nouns

are matched to nouns, verbs are matched to verbs),

and children nodes that have the same head-modifier

relationship with their parents (i.e. they are both

heads, or they are both dependents of their par-

ents). Table 3 shows the recursive algorithm used

TreeMatch(ui, ur)

[In]: Two dependency tree nodes ui, ur.
[Out]: A set of node pairsM.

1. setM← {(ui, ur)}
2. for each (vi, vr) ∈ MaxMatch(ui.C, ur.C):
3. setM←M∪ TreeMatch(vi, vr)

4. returnM

Table 3: Dependency Tree Matching.

for finding a matching between two question depen-

dency trees rooted at the focus words. The initial

arguments of the algorithm are the two focus words

ui = fi and ur = fr. Thus, the pair (fi, fr) is
the first pair of nodes to be added to the matching

M in step 1. In the next step, we compute the maxi-
mumweighted matching between the children nodes

ui.C and ur.C, and recursively call the matching al-
gorithm on pairs of matched nodes (vi, vr) fromM.
The algorithm stops when MaxMatch returns an

empty matching, which may happen when reach-

ing leaf nodes, or when no pair of children nodes

has compatible POS tags, or child-parent dependen-

cies. Figure 2 shows the results of applying the

tree matching algorithm on questions Q5 and Q9.

Matched nodes share the same index and are shown

in circles, whereas unmatched nodes are shown in

italics.

We introduce a new feature φt(Qi, Qr) whose
value is defined as the dependency tree similarity

between questions Qi and Qr. Once the optimum

matchingM(Qi, Qr) between dependency trees has
been found, φt(Qi, Qr) is computed as the nor-
malized sum of the similarities between pairs of

matched nodes vi and vr, as shown in Equations 3

and 4 below. When computing the similarity be-

tween two matched nodes, we factor in the similar-

ities between corresponding pairs of words on the

paths fi ; vi, fr ; vr between the focus words fi,

fr and the nodes vi, vr, as shown in Equation 5. This

has the effect of reducing the importance of words

that are farther away from the focus word in the de-

pendency tree.

φt(Qi, Qr) =
sim(Qi, Qr)

√

sim(Qi, Qi)sim(Qr, Qr)
(3)

sim(Qi, Qr) =
∑

(vi,vr)∈M(Qi,Qr)

sim(fi ; vi, fr ; vr) (4)

sim(u1 ; un, v1 ; vn) =

n
∏

i=1

wsim(ui, vi) (5)

If the word similarity function is normalized and

defined to return 1 for identical words, the nor-

malizer in Equation 3 becomes equivalent with
√

|Qi||Qr|. Thus, words that are left unmatched im-
plicitly decrease the dependency tree similarity.

4.4 An SVM Model for Ranking Questions

We consider learning a usefulness function

u(Qi, Qr) of the following general, linear form:

u(Qi, Qr) = w
T φ(Qi, Qr) (6)

The vector φ(Qi, Qr) is defined to contain the fol-
lowing generic features:

1. φf (Qi, Qr) = the semantic similarity between
focus words, as described in Section 4.1.

2. φv(Qi, Qr) = the semantic similarity between
main verbs, as described in Section 4.2.

3. φt(Qi, Qr) = the semantic similarity between
the dependency trees, as described in Sec-

tion 4.3.

4. cos(Qi, Qr) = the cosine similarity between the
two questions, as described in Section 3.

5. mcs(Qi, Qr) = the bag-of-concepts similarity
between the two questions, as described in Sec-

tion 3.

Each of the generic features φf , φv, φt, andmcs cor-

responds to four actual features, one for each possi-

ble choice of the word similarity function wsim (i.e.

wup, res, lin or jcn). An additional pair of features

is targeted at questions containing locations:

103



6. φl(Qi, Qr) = 1 if both questions contain loca-
tions, 0 otherwise.

7. φd(Qi, Qr) = the normalized geographical dis-
tance between the locations in Qi and Qr, 0 if

φl(Qi, Qr) = 0.

Given two location names, we first find their latitude

and longitude using Google Maps, and then com-

pute the spherical distance between them using the

haversine formula.

The corresponding parameters w will be trained

on pairs from one of the partially ordered datasets

described in Section 2. We use the kernel version of

the large-margin ranking approach from (Joachims,

2002) which solves the optimization problem in Fig-

ure 3 below. The aim of this formulation is to find a

minimize:

J(w, ξ) = 1
2
‖w‖2 + C

∑

ξrij
subject to:

w
T φ(Qi, Qr)−w

T φ(Qj , Qr) ≥ 1− ξrij
ξrij ≥ 0
∀Qr, Qi, Qj ∈ D, 〈Qi ≻ Qj |Qr〉

Figure 3: SVM ranking optimization problem.

weight vector w such that 1) the number of ranking

constraints u(Qi, Qr) ≥ u(Qj , Qr) from the train-
ing data D that are violated is minimized, and 2) the
ranking function u(Qi, Qr) generalizes well beyond
the training data. The learnedw is a linear combina-

tion of the feature vectors φ(Qi, Qr), which makes
it possible to use kernels.

5 Experimental Evaluation

We use the four question ranking datasets described

in Section 2 to evaluate the three similarity mea-

sures cos, mcs, and φt, as well as the SVM rank-

ing model. We report one set of results for each of

the four word similarity measures wup, res, lin or

jcn. Each question similarity measure is evaluated

in terms of its accuracy on the set of ordered pairs,

and the performance is averaged between the two

annotators for the Simple and Complex datasets. If

〈Qi ≻ Qj |Qr〉 is a relation specified in the anno-
tation, we consider the tuple 〈Qi, Qj , Qr〉 correctly

classified if and only if u(Qi, Qr) > u(Qj , Qr),
where u is the question similarity measure. We used

the SVMlight 2 implementation of ranking SVMs,

with a cubic kernel and the standard parameters. The

SVM ranking model was trained and tested using

10-fold cross-validation, and the overall accuracy

was computed by averaging over the 10 folds.

We used the NLTK 3 implementation of the four

similarity measures wup, res, lin or jcn. The idf val-

ues for each word were computed from frequency

counts over the entire Wikipedia. For each ques-

tion, the focus is identified automatically by an SVM

tagger trained on a separate corpus of 2,000 ques-

tions manually annotated with focus information

(Bunescu and Huang, 2010a). The SVM tagger

uses a combination of lexico-syntactic features and

a quadratic kernel to achieve a 93.5% accuracy in

a 10-fold cross validation evaluation on the 2,000

questions. The head-modifier dependencies were

derived automatically from the syntactic parse tree

using the head finding rules from (Collins, 1999).

The syntactic tree is obtained using Spear 4, a syn-

tactic parser which comes pre-trained on an addi-

tional treebank of questions. The main verb of

a question is identified deterministically using a

breadth first traversal of the dependency tree.

The overall accuracy results presented in Table 4

show that the SVM ranking model obtains by far the

best performance on both datasets, a substantial 10%

higher than cos, which is the best performing unsu-

pervised method. The random baseline – assigning

a random similarity value to each pair of questions –

results in 50% accuracy. Even though its use of word

senses was expected to lead to superior results, mcs

does not perform better than cos on this dataset. Our

implementation of mcs did however perform better

than cos on the Microsoft paraphrase corpus (Dolan

et al., 2004). One possible reason for this behav-

ior is that mcs seems to be less resilient than cos

to differences in question length. Whereas the Mi-

crosoft paraphrase corpus was specifically designed

such that “the length of the shorter of the two sen-

tences, in words, is at least 66% that of the longer”

(Dolan and Brockett, 2005), the question ranking

datasets place no constraints on the lengths of the

2svmlight.joachims.org
3www.nltk.org
4www.surdeanu.name/mihai/spear

104



Question wup res lin jcn

Dataset cos mcs φt mcs φt mcs φt mcs φt SVM

Simple 73.7 69.1 69.4 71.3 71.8 70.8 69.8 71.9 71.7 82.1

Complex 72.6 64.1 69.6 66.0 71.5 66.9 69.1 69.4 71.0 82.5

Table 4: Pairwise accuracy results.

Dataset all −φf −φv −φt −φl,d −cos −mcs −φf,t
Simple 82.1 79.3 82.0 80.2 81.5 80.3 81.4 78.5

Complex 82.5 81.3 81.3 78.7 81.8 79.2 81.8 77.4

Table 5: Ablation results.

questions. However, even though by themselves the

meaning aware mcs and the structure-and-meaning

aware φt do not outperform the bag-of-words cos,

they do help in increasing the performance of the

SVM ranking model, as can be inferred from the cor-

responding columns in Table 5. The table shows the

results of ablation experiments in which all but one

type of features are used. The results indicate that

all types of features are useful, with significant con-

tributions being brought especially by cos and the

focus related features φf,t.

The measures investigated in this paper are all

compositional and reduce the similarity computa-

tions to word level. The following question patterns

illustrate the need to design more complex similarity

measures that take into account the context of every

word in the question:

P1 Where can I find a job around 〈City 〉?

P2 What are some famous people from 〈City 〉?

P3 What is the population of 〈City 〉?

Below are three instantiations of the first question

pattern:

Q32 Where can I find a job around Anaheim, CA?

Q33 Where can I find a job around Los Angeles?

Q34 Where can I find a job around Vista, CA?

If we take Q32 as reference question, the fact that

the distance between Los Angeles and Anaheim is

smaller than the distance between Vista and Ana-

heim leads the ranking system to rank Q33 as more

useful than Q34 with respect to Q32, which is the

expected result. The preposition “around” from the

city context in the first pattern is a good indica-

tor that proximity relations are relevant in this case.

When the same three cities are used for instantiating

the other two patterns, it can be seen that the prox-

imity relations are no longer as relevant for judging

the relative usefulness of questions.

6 Future Work

We plan to integrate context dependent word sim-

ilarity measures into a more robust question util-

ity function. We also plan to make the dependency

tree matching more flexible in order to account for

paraphrase patterns that may differ in their syntactic

structure. The questions that are posted on commu-

nity QA sites often contain spelling or grammatical

errors. Consequently, we will work on interfacing

the question ranking system with a separate module

aimed at fixing orthographic and grammatical errors.

7 Related Work

The question rephrasing subtask has spawned a di-

verse set of approaches. (Hermjakob et al., 2002)

derive a set of phrasal patterns for question reformu-

lation by generalizing surface patterns acquired au-

tomatically from a large corpus of web documents.

The focus of the work in (Tomuro, 2003) is on deriv-

ing reformulation patterns for the interrogative part

of a question. In (Jeon et al., 2005), word trans-

lation probabilities are trained on pairs of seman-

tically similar questions that are automatically ex-

tracted from an FAQ archive, and then used in a

language model that retrieves question reformula-

tions. (Jijkoun and de Rijke, 2005) describe an FAQ

105



question retrieval system in which weighted com-

binations of similarity functions corresponding to

questions, existing answers, FAQ titles and pages

are computed using a vector space model. (Zhao et

al., 2007) exploit the Encarta logs to automatically

extract clusters containing question paraphrases and

further train a perceptron to recognize question para-

phrases inside each cluster based on a combination

of lexical, syntactic and semantic similarity features.

More recently, (Bernhard and Gurevych, 2008) eval-

uated various string similarity measures and vec-

tor space based similarity measures on the task of

retrieving question paraphrases from the WikiAn-

swers repository. The aim of the question search

task presented in (Duan et al., 2008) is to return

questions that are semantically equivalent or close

to the queried question, and is therefore similar to

our question ranking task. Their approach is eval-

uated on a dataset in which questions are catego-

rized either as relevant or irrelevant. Our formula-

tion of question ranking is more general, and in par-

ticular subsumes the annotation of binary question

categories such as relevant vs. irrelevant, or para-

phrases vs. non-paraphrases. Moreover, we are able

to exploit the annotated utility relations as super-

vision in a learning for ranking approach, whereas

(Duan et al., 2008) use the annotated dataset to tune

the 3 parameters of a mostly unsupervised approach.

The question ranking task was first formulated in

(Bunescu and Huang, 2010b), where an initial ver-

sion of the dataset was also described. In this pa-

per, we introduce 4 versions of the dataset, a more

general meaning and structure aware similarity mea-

sure, and a supervised model for ranking that sub-

stantially outperforms the previously proposed util-

ity measures.

8 Conclusion

We presented a supervised learning approach to the

question ranking task in which previously known

questions are ordered based on their relative util-

ity with respect to a new, reference question. We

created four versions of a dataset of 60 groups of

questions 5, each annotated with a partial order rela-

tion reflecting the relative utility of questions inside

each group. An SVM ranking model was trained

5The dataset will be made publicly available.

on the dataset and evaluated together with a set of

simpler, unsupervised question-to-question similar-

ity models. Experimental results demonstrate the

importance of using structure and meaning aware

features when computing the relative usefulness of

questions.

Acknowledgments

Wewould like to thank the anonymous reviewers for

their insightful comments.

References

Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.

Modern Information Retrieval. ACM Press, New

York.

Delphine Bernhard and Iryna Gurevych. 2008. Answer-

ing learners’ questions by retrieving question para-

phrases from social Q&A sites. In EANL ’08: Pro-

ceedings of the Third Workshop on Innovative Use of

NLP for Building Educational Applications, pages 44–

52, Morristown, NJ, USA. Association for Computa-

tional Linguistics.

Razvan Bunescu and Yunfeng Huang. 2010a. Towards a

general model of answer typing: Question focus iden-

tification. In Proceedings of The 11th International

Conference on Intelligent Text Processing and Com-

putational Linguistics (CICLing 2010), RCS Volume,

pages 231–242.

Razvan Bunescu and Yunfeng Huang. 2010b. A utility-

driven approach to question ranking in social QA.

In Proceedings of The 23rd International Conference

on Computational Linguistics (COLING 2010), pages

125–133.

Michael Collins. 1999. Head-driven Statistical Models

for Natural Language Parsing. Ph.D. thesis, Univer-

sity of Pennsylvania.

William B. Dolan and Chris Brockett. 2005. Automat-

ically constructing a corpus of sentential paraphrases.

In Proceedings of the Third International Workshop on

Paraphrasing (IWP2005), pages 9–16.

Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-

supervised construction of large paraphrase corpora:

Exploiting assively parallel news sources. In Proceed-

ings of The 20th International Conference on Compu-

tational Linguistics (COLING’04), page 350.

Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong

Yu. 2008. Searching questions by identifying question

topic and question focus. In Proceedings of ACL-08:

HLT, pages 156–164, Columbus, Ohio, June.

Ulf Hermjakob, Abdessamad Echihabi, and Daniel

Marcu. 2002. Natural language based reformulation

106



resource and web exploitation for question answering.

In Proceedings of TREC-2002.

Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.

Finding similar questions in large question and an-

swer archives. In Proceedings of the 14th ACM in-

ternational conference on Information and knowledge

management (CIKM’05), pages 84–90, NewYork, NY,

USA. ACM.

J.J. Jiang and D.W. Conrath. 1997. Semantic similarity

based on corpus statistics and lexical taxonomy. In

Proceedings of the International Conference on Re-

search in Computational Linguistics, pages 19–33.

Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving

answers from frequently asked questions pages on the

Web. In Proceedings of the 14th ACM international

conference on Information and knowledge manage-

ment (CIKM’05), pages 76–83, New York, NY, USA.

ACM.

Thorsten Joachims. 2002. Optimizing search engines us-

ing clickthrough data. In Proceedings of the Eighth

ACM SIGKDD International Conference on Knowl-

edge Discovery and Data Mining (KDD-2002), Ed-

monton, Canada.

Dekang Lin. 1998. An information-theoretic defini-

tion of similarity. In Proceedings of the Fifteenth In-

ternational Conference on Machine Learning (ICML

’98), pages 296–304, San Francisco, CA, USA. Mor-

gan Kaufmann Publishers Inc.

Rada Mihalcea, Courtney Corley, and Carlo Strappar-

ava. 2006. Corpus-based and knowledge-based mea-

sures of text semantic similarity. In Proceedings of

the 21st national conference on Artificial intelligence

(AAAI’06), pages 775–780. AAAI Press.

Dan I. Moldovan, Marius Pasca, Sanda M. Harabagiu,

and Mihai Surdeanu. 2002. Performance issues and

error analysis in an open-domain question answering

system. In Proceedings of the 40th Annual Meeting of

the Association for Computational Linguistics, pages

33–40, Philadelphia, PA, July.

John M. Prager. 2006. Open-domain question-

answering. Foundations and Trends in Information

Retrieval, 1(2):91–231.

Philip Resnik. 1995. Using information content to eval-

uate semantic similarity in a taxonomy. In IJCAI’95:

Proceedings of the 14th international joint conference

on Artificial intelligence, pages 448–453, San Fran-

cisco, CA, USA. Morgan Kaufmann Publishers Inc.

Noriko Tomuro. 2003. Interrogative reformulation pat-

terns and acquisition of question paraphrases. In Pro-

ceedings of the Second International Workshop on

Paraphrasing, pages 33–40, Morristown, NJ, USA.

Association for Computational Linguistics.

Zhibiao Wu and Martha Palmer. 1994. Verbs seman-

tics and lexical selection. In Proceedings of the 32nd

annual meeting on Association for Computational Lin-

guistics, pages 133–138, Morristown, NJ, USA. Asso-

ciation for Computational Linguistics.

Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learn-

ing question paraphrases for QA from Encarta logs. In

Proceedings of the 20th international joint conference

on Artifical intelligence (IJCAI’07), pages 1795–1800,

San Francisco, CA, USA. Morgan Kaufmann Publish-

ers Inc.

107


