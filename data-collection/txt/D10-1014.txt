










































Soft Syntactic Constraints for Hierarchical Phrase-Based Translation Using Latent Syntactic Distributions


Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147,
MIT, Massachusetts, USA, 9-11 October 2010. c©2010 Association for Computational Linguistics

Soft Syntactic Constraints for Hierarchical Phrase-based Translation
Using Latent Syntactic Distributions

Zhongqiang Huang
Institute for Advanced Computer Studies

University of Maryland
College Park, MD 20742
zqhuang@umiacs.umd.edu

Martin Čmejrek and Bowen Zhou
IBM T. J. Watson Research Center

Yorktown Heights, NY 10598
{martin.cmejrek,zhou}@us.ibm.com

Abstract

In this paper, we present a novel approach
to enhance hierarchical phrase-based machine
translation systems with linguistically moti-
vated syntactic features. Rather than directly
using treebank categories as in previous stud-
ies, we learn a set of linguistically-guided la-
tent syntactic categories automatically from a
source-side parsed, word-aligned parallel cor-
pus, based on the hierarchical structure among
phrase pairs as well as the syntactic structure
of the source side. In our model, each X non-
terminal in a SCFG rule is decorated with a
real-valued feature vector computed based on
its distribution of latent syntactic categories.
These feature vectors are utilized at decod-
ing time to measure the similarity between the
syntactic analysis of the source side and the
syntax of the SCFG rules that are applied to
derive translations. Our approach maintains
the advantages of hierarchical phrase-based
translation systems while at the same time nat-
urally incorporates soft syntactic constraints.

1 Introduction

In recent years, syntax-based translation mod-
els (Chiang, 2007; Galley et al., 2004; Liu et
al., 2006) have shown promising progress in im-
proving translation quality, thanks to the incorpora-
tion of phrasal translation adopted from the widely
used phrase-based models (Och and Ney, 2004) to
handle local fluency and the engagement of syn-
chronous context-free grammars (SCFG) to handle
non-local phrase reordering. Approaches to syntax-
based translation models can be largely categorized

into two classes based on their dependency on anno-
tated corpus (Chiang, 2007). Linguistically syntax-
based models (e.g., (Yamada and Knight, 2001; Gal-
ley et al., 2004; Liu et al., 2006)) utilize structures
defined over linguistic theory and annotations (e.g.,
Penn Treebank) and guide the derivation of SCFG
rules with explicit parsing on at least one side of
the parallel corpus. Formally syntax-based mod-
els (e.g., (Wu, 1997; Chiang, 2007)) extract syn-
chronous grammars from parallel corpora based on
the hierarchical structure of natural language pairs
without any explicit linguistic knowledge or anno-
tations. In this work, we focus on the hierarchi-
cal phrase-based models of Chiang (2007), which
is formally syntax-based, and always refer the term
SCFG, from now on, to the grammars of this model
class.

On the one hand, hierarchical phrase-based mod-
els do not suffer from errors in syntactic constraints
that are unavoidable in linguistically syntax-based
models. Despite the complete lack of linguistic
guidance, the performance of hierarchical phrase-
based models is competitive when compared to lin-
guistically syntax-based models. As shown in (Mi
and Huang, 2008), hierarchical phrase-based models
significantly outperform tree-to-string models (Liu
et al., 2006; Huang et al., 2006), even when at-
tempts are made to alleviate parsing errors using
either forest-based decoding (Mi et al., 2008) or
forest-based rule extraction (Mi and Huang, 2008).

On the other hand, when properly used, syntac-
tic constraints can provide invaluable benefits to im-
prove translation quality. The tree-to-string mod-
els of Mi and Huang (2008) can actually signif-

138



icantly outperform hierarchical phrase-based mod-
els when using forest-based rule extraction together
with forest-based decoding. Chiang (2010) also ob-
tained significant improvement over his hierarchi-
cal baseline by using syntactic parse trees on both
source and target sides to induce fuzzy (not exact)
tree-to-tree rules and by also allowing syntactically
mismatched substitutions.

In this paper, we augment rules in hierarchical
phrase-based translation systems with novel syntac-
tic features. Unlike previous studies (e.g., (Zoll-
mann and Venugopal, 2006)) that directly use ex-
plicit treebank categories such as NP, NP/PP (NP
missing PP from the right) to annotate phrase pairs,
we induce a set of latent categories to capture the
syntactic dependencies inherent in the hierarchical
structure of phrase pairs, and derive a real-valued
feature vector for each X nonterminal of a SCFG
rule based on the distribution of the latent cate-
gories. Moreover, we convert the equality test of
two sequences of syntactic categories, which are ei-
ther identical or different, into the computation of
a similarity score between their corresponding fea-
ture vectors. In our model, two symbolically dif-
ferent sequences of syntactic categories could have
a high similarity score in the feature vector repre-
sentation if they are syntactically similar, and a low
score otherwise. In decoding, these feature vectors
are utilized to measure the similarity between the
syntactic analysis of the source side and the syntax
of the SCFG rules that are applied to derive trans-
lations. Our approach maintains the advantages of
hierarchical phrase-based translation systems while
at the same time naturally incorporates soft syntactic
constraints. To the best of our knowledge, this is the
first work that applies real-valued syntactic feature
vectors to machine translation.

The rest of the paper is organized as follows.
Section 2 briefly reviews hierarchical phrase-based
translation models. Section 3 presents an overview
of our approach, followed by Section 4 describing
the hierarchical structure of aligned phrase pairs and
Section 5 describing how to induce latent syntactic
categories. Experimental results are reported in Sec-
tion 6, followed by discussions in Section 7. Sec-
tion 8 concludes this paper.

2 Hierarchical Phrase-Based Translation

An SCFG is a synchronous rewriting system gener-
ating source and target side string pairs simultane-
ously based on a context-free grammar. Each syn-
chronous production (i.e., rule) rewrites a nonter-
minal into a pair of strings, γ and α, where γ (or
α) contains terminal and nonterminal symbols from
the source (or target) language and there is a one-to-
one correspondence between the nonterminal sym-
bols on both sides. In particular, the hierarchical
model (Chiang, 2007) studied in this paper explores
hierarchical structures of natural language and uti-
lize only a unified nonterminal symbol X in the
grammar,

X → 〈γ, α,∼〉

where ∼ is the one-to-one correspondence between
X’s in γ and α, and it can be indicated by un-
derscripted co-indexes. Two example English-to-
Chinese translation rules are represented as follows:

X → 〈give the pen to me,钢笔给我〉 (1)
X → 〈giveX1 to me, X1给我〉 (2)

The SCFG rules of hierarchical phrase-based
models are extracted automatically from corpora of
word-aligned parallel sentence pairs (Brown et al.,
1993; Och and Ney, 2000). An aligned sentence pair
is a tuple (E,F,A), where E = e1 · · · en can be in-
terpreted as an English sentence of length n, F =
f1 · · · fm its translation of length m in a foreign lan-
guage, andA a set of links between words of the two
sentences. Figure 1 (a) shows an example of aligned
English-to-Chinese sentence pair. Widely adopted
in phrase-based models (Och and Ney, 2004), a pair
of consecutive sequences of words from E and F is
a phrase pair if all words are aligned only within the
sequences and not to any word outside. We call a se-
quence of words a phrase if it corresponds to either
side of a phrase pair, and a non-phrase otherwise.
Note that the boundary words of a phrase pair may
not be aligned to any other word. We call the phrase
pairs with all boundary words aligned tight phrase
pairs (Zhang et al., 2008). A tight phrase pair is the
minimal phrase pair among all that share the same
set of alignment links. Figure 1 (b) highlights the
tight phrase pairs in the example sentence pair.

139



6

5

4

3

2

1

1 2 3 4 5

(a) (b)

Figure 1: An example of word-aligned sentence pair (a)
with tight phrase pairs marked in a matrix representation
(b).

The extraction of SCFG rules proceeds as fol-
lows. In the first step, all phrase pairs below a max-
imum length are extracted as phrasal rules. In the
second step, abstract rules are extracted from tight
phrase pairs that contain other tight phrase pairs by
replacing the sub phrase pairs with co-indexed X-
nonterminals. Chiang (2007) also introduced several
requirements (e.g., there are at most two nontermi-
nals at the right hand side of a rule) to safeguard
the quality of the abstract rules as well as keeping
decoding efficient. In our example above, rule (2)
can be extracted from rule (1) with the following sub
phrase pair:

X → 〈the pen,钢笔〉

The use of a unified X nonterminal makes hier-
archical phrase-based models flexible at capturing
non-local reordering of phrases. However, such flex-
ibility also comes at the cost that it is not able to
differentiate between different syntactic usages of
phrases. Suppose rule X → 〈I am readingX1, · · · 〉
is extracted from a phrase pair with I am reading a
book on the source side whereX1 is abstracted from
the noun phrase pair . If this rule is used to translate
I am reading the brochure of a book fair, it would
be better to apply it over the entire string than over
sub-strings such as I ... the brochure of. This is be-
cause the nonterminal X1 in the rule was abstracted
from a noun phrase on the source side of the training
data and would thus be better (more informative) to
be applied to phrases of the same type. Hierarchi-
cal phrase-based models are not able to distinguish
syntactic differences like this.

Zollmann and Venugopal (2006) attempted to ad-
dress this problem by annotating phrase pairs with

treebank categories based on automatic parse trees.
They introduced an extended set of categories (e.g.,
NP+V for she went and DT\NP for great wall, an
noun phrase with a missing determiner on the left)
to annotate phrase pairs that do not align with syn-
tactic constituents. Their hard syntactic constraint
requires that the nonterminals should match exactly
to rewrite with a rule, which could rule out poten-
tially correct derivations due to errors in the syn-
tactic parses as well as to data sparsity. For exam-
ple, NP cannot be instantiated with phrase pairs of
type DT+NN, in spite of their syntactic similarity.
Venugopal et al. (2009) addressed this problem by
directly introducing soft syntactic preferences into
SCFG rules using preference grammars, but they
had to face the computational challenges of large
preference vectors. Chiang (2010) also avoided hard
constraints and took a soft alternative that directly
models the cost of mismatched rule substitutions.
This, however, would require a large number of pa-
rameters to be tuned on a generally small-sized held-
out set, and it could thus suffer from over-tuning.

3 Approach Overview

In this work, we take a different approach to intro-
duce linguistic syntax to hierarchical phrase-based
translation systems and impose soft syntactic con-
straints between derivation rules and the syntactic
parse of the sentence to be translated. For each
phrase pair extracted from a sentence pair of a
source-side parsed parallel corpus, we abstract its
syntax by the sequence of highest root categories,
which we call a tag sequence, that exactly1 domi-
nates the syntactic tree fragments of the source-side
phrase. Figure 3 (b) shows the source-side parse tree
of a sentence pair. The tag sequence for “the pen”
is simply “NP” because it is a noun phrase, while
phrase “give the pen” is dominated by a verb fol-
lowed by a noun phrase, and thus its tag sequence is
“VBP NP”.

Let TS = {ts1, · · · , tsm} be the set of all tag se-
quences extracted from a parallel corpus. The syntax
of each X nonterminal2 in a SCFG rule can be then

1In case of a non-tight phrase pair, we only abstract and
compare the syntax of the largest tight part.

2There are three X nonterminals (one on the left and two on
the right) for binary abstract rules, two for unary abstract rules,
and one for phrasal rules.

140



Tag Sequence Probability
NP 0.40
DT NN 0.35
DT NN NN 0.25

Table 1: The distribution of tag sequences forX1 inX →
〈I am reading X1, · · · 〉.

characterized by the distribution of tag sequences
~PX(TS) = (pX(ts1), · · · , pX(tsm)), based on the
phrase pairs it is abstracted from. Table 1 shows
an example distribution of tag sequences for X1 in
X → 〈I am reading X1, · · · 〉.

Instead of directly using tag sequences, as we
discussed their disadvantages above, we represent
each of them by a real-valued feature vector. Sup-
pose we have a collection of n latent syntactic cate-
gories C = {c1, · · · , cn}. For each tag sequence ts,
we compute its distribution of latent syntactic cate-
gories ~Pts(C) = (pts(c1), · · · , pts(cn)). For exam-
ple, ~P“NP VP”(C) = {0.5, 0.2, 0.3} means that the la-
tent syntactic categories c1, c2, and c3 are distributed
as p(c1) = 0.5, p(c2) = 0.2, and p(c3) = 0.3 for tag
sequence “NP VP”. We further convert the distribu-
tion to a normalized feature vector ~F (ts) to repre-
sent tag sequence ts:

~F (ts) = (f1(ts), · · · , fn(ts))

=
(pts(c1), · · · , pts(cn))
‖(pts(c1), · · · , pts(cn))‖

The advantage of using real-valued feature vec-
tors is that the degree of similarity between two tag
sequences ts and ts′ in the space of the latent syn-
tactic categories C can be simply computed as a dot-
product3 of their feature vectors:

~F (ts) · ~F (ts′) =
∑

1≤i≤n
fi(ts)fi(ts

′)

which computes a syntactic similarity score in the
range of 0 (totally syntactically different) to 1 (com-
pletely syntactically identical).

Similarly, we can represent the syntax of each X
nonterminal in a rule with a feature vector ~F (X),
computed as the sum of the feature vectors of tag

3Other measures such as KL-divergence in the probability
space are also feasible.

sequences weighted by the distribution of tag se-
quences of the nonterminal X:

~F (X) =
∑

ts∈TS
pX(ts)~F (ts)

Now we can impose soft syntactic constraints us-
ing these feature vectors when a SCFG rule is used
to translate a parsed source sentence. Given that aX
nonterminal in the rule is applied to a span with tag
sequence4 ts as determined by a syntactic parser, we
can compute the following syntax similarity feature:

SynSim(X, ts) = − log(~F (ts) · ~F (X))

Except that it is computed on the fly, this feature
can be used in the same way as the regular features
in hierarchical translation systems to determine the
best translation, and its feature weight can be tuned
in the same way together with the other features on
a held-out data set.

In our approach, the set of latent syntactic cate-
gories is automatically induced from a source-side
parsed, word-aligned parallel corpus based on the
hierarchical structure among phrase pairs along with
the syntactic parse of the source side. In what fol-
lows, we will explain the two critical aspects of
our approach, i.e., how to identify the hierarchi-
cal structures among all phrase pairs in a sentence
pair, and how to induce the latent syntactic cate-
gories from the hierarchy to syntactically explain the
phrase pairs.

4 Alignment-based Hierarchy

The aforementioned abstract rule extraction algo-
rithm of Chiang (2007) is based on the property that
a tight phrase pair can contain other tight phrase
pairs. Given two non-disjoint tight phrase pairs that
share at least one common alignment link, there are
only two relationships: either one completely in-
cludes another or they do not include one another
but have a non-empty overlap, which we call a non-
trivial overlap. In the second case, the intersection,
differences, and union of the two phrase pairs are

4A normalized uniform feature vector is used for tag se-
quences (of parsed test sentences) that are not seen on the train-
ing corpus.

141



Figure 2: A decomposition tree of tight phrase pairs with
all tight phrase pairs listed on the right. As highlighted,
the two non-maximal phrase pairs are generated by con-
secutive sibling nodes.

also tight phrase pairs (see Figure 1 (b) for exam-
ple), and the two phrase pairs, as well as their inter-
section and differences, are all sub phrase pairs of
their union.

Zhang et al. (2008) exploited this property to con-
struct a hierarchical decomposition tree (Bui-Xuan
et al., 2005) of phrase pairs from a sentence pair to
extract all phrase pairs in linear time. In this pa-
per, we focus on learning the syntactic dependencies
along the hierarchy of phrase pairs. Our hierarchy
construction follows Heber and Stoye (2001).

Let P be the set of tight phrase pairs extracted
from a sentence pair. We call a sequentially-ordered
list5 L = (p1, · · · , pk) of unique phrase pairs pi ∈ P
a chain if every two successive phrase pairs in L
have a non-trivial overlap. A chain is maximal if
it can not be extended to its left or right with other
phrase pairs. Note that any sub-sequence of phrase
pairs in a chain generates a tight phrase pair. In par-
ticular, chain L generates a tight phrase pair τ(L)
that corresponds exactly to the union of the align-
ment links in p ∈ L. We call the phrase pairs
generated by maximal chains maximal phrase pairs
and call the other phrase pairs non-maximal. Non-
maximal phrase pairs always overlap non-trivially
with some other phrase pairs while maximal phrase
pairs do not, and it can be shown that any non-
maximal phrase pair can be generated by a sequence
of maximal phrase pairs. Note that the largest tight
phrase pair that includes all alignment links in A is
also a maximal phrase pair.

5The phrase pairs can be sequentially ordered first by the
boundary positions of the source-side phrase and then by the
boundary positions of the target-side phrase.

give the pen to me .
X B B B X X

X

X

X

PP
VBP DT NN TO PRP .

NP

VP
S

give the pen to me .

(a) (b)

X
X B B B X X

X
X

X
X

X
X B B B X X

X
X

X
X

VBP

X
X B B B X X

X
X

X
X

DT NN TO PRP .

NP PP
CR

VP
S

I(!)

O(!)

X
X B B B X X

X
X

X
X

VBP DT NN TO PRP .

S
CR

NP PP
CR

O(!)

I(!)

(c) (d)

Figure 3: (a) decomposition tree for the English side of
the example sentence pair with all phrases underlined, (b)
automatic parse tree of the English side, (c) two example
binarized decomposition trees with syntactic emissions
in depicted in (d), where the two dotted curves give an
example I(·) and O(·) that separate the forest into two
parts.

Lemma 1 Given two different maximal phrase
pairs p1 and p2, exactly one of the following alter-
natives is true: p1 and p2 are disjoint, p1 is a sub
phrase pair of p2, or p2 is a sub phrase pair of p1.

A direct outcome of Lemma 1 is that there is an
unique decomposition tree T = (N,E) covering all
of the tight phrase pairs of a sentence pair, where N
is the set of maximal phrase pairs and E is the set of
edges that connect between pairs of maximal phrase
pairs if one is a sub phrase pair of another. All of the
tight phrase pairs of a sentence pair can be extracted
directly from the nodes of the decomposition tree
(these phrase pairs are maximal), or generated by se-
quences of consecutive sibling nodes6 (these phrase
pairs are non-maximal). Figure 2 shows the decom-
position tree as well as all of the tight phrase pairs
that can be extracted from the example sentence pair
in Figure 1.

We focus on the source side of the decomposition
tree, and expand it to include all of the non-phrase

6Unaligned words may be added.

142



single words within the scope of the decomposition
tree as frontiers and attach each as a child of the low-
est node that contains the word. We then abstract the
trees nodes with two symbol, X for phrases, and B
for non-phrases, and call the result the decomposi-
tion tree of the source side phrases. Figure 3 (a) de-
picts such tree for the English side of our example
sentence pair. We further recursively binarize7 the
decomposition tree into a binarized decomposition
forest such that all phrases are directly represented
as nodes in the forest. Figure 3 (c) shows two of the
many binarized decomposition trees in the forest.

The binarized decomposition forest compactly
encodes the hierarchical structure among phrases
and non-phrases. However, the coarse abstraction
of phrases with X and non-phrases with B provides
little information on the constraints of the hierarchy.
In order to bring in syntactic constraints, we anno-
tate the nodes in the decomposition forest with syn-
tactic observations based on the automatic syntactic
parse tree of the source side. If a node aligns with
a constituent in the parse tree, we add the syntactic
category (e.g., NP) of the constituent as an emitted
observation of the node, otherwise, it crosses con-
stituent boundaries and we add a designated crossing
category CR as its observation. We call the resulting
forest a syntactic decomposition forest. Figure 3 (d)
shows two syntactic decomposition trees of the for-
est based on the parse tree in Figure 3 (b). We will
next describe how to learn finer-grained X and B
categories based on the hierarchical syntactic con-
straints.

5 Inducing Latent Syntactic Categories

If we designate a unique symbol S as the new root
of the syntactic decomposition forests introduced
in the previous section, it can be shown that these
forests can be generated by a probabilistic context-
free grammar G = (V,Σ, S,R, φ), where

• V = {S,X,B} is the set of nonterminals,

• Σ is the set of terminals comprising treebank
categories plus the CR tag (the crossing cate-
gory),

7The intermediate binarization nodes are also labeled as ei-
ther X or B based on whether they exactly cover a phrase or
not.

• S ∈ V is the unique start symbol,

• R is the union of the set of production rules
each rewriting a nonterminal to a sequence of
nonterminals and the set of emission rules each
generating a terminal from a nonterminal,

• and φ assigns a probability score to each rule
r ∈ R.

Such a grammar can be derived from the set of
syntactic decomposition forests extracted from a
source-side parsed parallel corpus, with rule prob-
ability scores estimated as the relative frequencies
of the production and emission rules.

The X and B nonterminals in the grammar are
coarse representations of phrase and non-phrases
and do not carry any syntactic information at all.
In order to introduce syntax to these nonterminals,
we incrementally split8 them into a set of latent
categories {X1, · · · , Xn} for X and another set
{B1, · · · , Bn} for B, and then learn a set of rule
probabilities9 φ on the latent categories so that the
likelihood of the training forests are maximized. The
motivation is to let the latent categories learn differ-
ent preferences of (emitted) syntactic categories as
well as structural dependencies along the hierarchy
so that they can carry syntactic information. We call
them latent syntactic categories. The learned Xi’s
represent syntactically-induced finer-grained cate-
gories of phrases and are used as the set of latent
syntactic categories C described in Section 3. In re-
lated research, Matsuzaki et al. (2005) and Petrov et
al. (2006) introduced latent variables to learn finer-
grained distinctions of treebank categories for pars-
ing, and Huang et al. (2009) used a similar approach
to learn finer-grained part-of-speech tags for tag-
ging. Our method is in spirit similar to these ap-
proaches.

Optimization of grammar parameters to maximize
the likelihood of training forests can be achieved

8We incrementally split each nonterminal to 2, 4, 8, and fi-
nally 16 categories, with each splitting followed by several EM
iterations to tune model parameters. We consider 16 an appro-
priate number for latent categories, not too small to differentiate
between different syntactic usages and not too large for the extra
computational and storage costs.

9Each binary production rule is now associated with a 3-
dimensional matrix of probabilities, and each emission rule as-
sociated with a 1-dimensional array of probabilities.

143



by a variant of Expectation-Maximization (EM) al-
gorithm. Recall that our decomposition forests are
fully binarized (except the root). In the hypergraph
representation (Huang and Chiang, 2005), the hy-
peredges of our forests all have the same format10

〈(V,W ), U〉, meaning that node U expands to nodes
V and W with production rule U → VW . Given
a forest F with root node R, we denote e(U) the
emitted syntactic category at node U and LR(U) (or
PL(W ), or PR(V ))11 the set of node pairs (V,W )
(or (U, V ), or (U,W )) such that 〈(V,W ), U〉 is a hy-
peredge of the forest. Now consider node U , which
is either S, X , or B, in the forest. Let Ux be the
latent syntactic category12 of node U . We define
I(Ux) the part of the forest (includes e(U) but not
Ux) inside U , and O(Ux) the other part of the forest
(includes Ux but not e(U)) outside U , as illustrated
in Figure 3 (d). The inside-outside probabilities are
defined as:

PIN(Ux) = P (I(Ux)|Ux)
POUT(Ux) = P (O(Ux)|S)

which can be computed recursively as:

PIN(Ux) =
∑

(V,W )∈LR(U)

∑
y,z

φ(Ux → e(U))
×φ(Ux → VyWz)
×PIN(Vy)PIN(Wz)

POUT(Ux) =
∑

(V,W )∈PL(U)

∑
y,z

φ(Vy → e(V ))
×φ(Vy →WzUx)
×POUT(Vy)PIN(Wz)

+
∑

(V,W )∈PR(U)

∑
y,z

φ(Vy → e(V ))
×φ(Vy → UxWz)
×POUT(Vy)PIN(Wz)

In the E-step, the posterior probability of the oc-
currence of production rule13 Ux → VyWz is com-
puted as:

P (Ux → VyWz|F ) =

φ(Ux → e(U))
×φ(Ux → VyWz)

×POUT(Ux)PIN(Vy)PIN(Ww)
PIN(R)

10The hyperedge corresponding to the root node has a differ-
ent format because it is unary, but it can be handled similarly.
When clear from context, we use the same variable to present
both a node and its label.

11LR stands for the left and right children, PL for the parent
and left children, and PR for the parent and right children.

12We never split the start symbol S, and denote S0 = S.
13The emission rules can be handled similarly.

In the M-step, the expected counts of rule Ux →
VyWz for all latent categories Vy and Wz are accu-
mulated together and then normalized to obtain an
update of the probability estimation:

φ(Ux → VyWz) =
#(Ux → VyWz)∑

(V ′,W ′)

∑
y,z

#(Ux → VyWz)

Recall that each node U labeled asX in a forest is
associated with a phrase whose syntax is abstracted
by a tag sequence. Once a grammar is learned, for
each such node with a corresponding tag sequence
ts in forest F , we compute the posterior probability
that the latent category of node U being Xi as:

P (Xi|ts) =
POUT(Ui)PIN(Ui)

PIN(R)

This contributes P (Xi|ts) evidence that tag se-
quence ts belongs to a Xi category. When all
of the evidences are computed and accumulated in
#(Xi, ts), they can then be normalized to obtain the
probability that the latent category of ts is Xi:

pts(Xi) =
#(Xi, ts)∑
i #(Xi, ts)

As described in Section 3, the distributions of latent
categories are used to compute the syntactic feature
vectors for the SCFG rules.

6 Experiments

We conduct experiments on two tasks, English-to-
German and English-to-Chinese, both aimed for
speech-to-speech translation. The training data for
the English-to-German task is a filtered subset of the
Europarl corpus (Koehn, 2005), containing ∼300k
parallel bitext with ∼4.5M tokens on each side. The
dev and test sets both contain 1k sentences with one
reference for each. The training data for the English-
to-Chinese task is collected from transcription and
human translation of conversations in travel domain.
It consists of ∼500k parallel bitext with ∼3M to-
kens14 on each side. Both dev and test sets contain
∼1.3k sentences, each with two references. Both

14The Chinese sentences are automatically segmented into
words. However, BLEU scores are computed at character level
for tuning and evaluation.

144



corpora are also preprocessed with punctuation re-
moved and words down-cased to make them suitable
for speech translation.

The baseline system is our implementation of the
hierarchical phrase-based model of Chiang (2007),
and it includes basic features such as rule and
lexicalized rule translation probabilities, language
model scores, rule counts, etc. We use 4-gram lan-
guage models in both tasks, and conduct minimum-
error-rate training (Och, 2003) to optimize feature
weights on the dev set. Our baseline hierarchical
model has 8.3M and 9.7M rules for the English-to-
German and English-to-Chinese tasks, respectively.

The English side of the parallel data is
parsed by our implementation of the Berkeley
parser (Huang and Harper, 2009) trained on the
combination of Broadcast News treebank from
Ontonotes (Weischedel et al., 2008) and a speechi-
fied version of the WSJ treebank (Marcus et al.,
1999) to achieve higher parsing accuracy (Huang et
al., 2010). Our approach introduces a new syntactic
feature and its feature weight is tuned in the same
way together with the features in the baseline model.
In this study, we induce 16 latent categories for both
X and B nonterminals.

Our approach identifies ∼180k unique tag se-
quences for the English side of phrase pairs in both
tasks. As shown by the examples in Table 2, the syn-
tactic feature vector representation is able to identify
similar and dissimilar tag sequences. For instance,
it determines that the sequence of “DT JJ NN” is
syntactically very similar to “DT ADJP NN” while
very dissimilar to “NN CD VP”. Notice that our la-
tent categories are learned automatically to maxi-
mize the likelihood of the training forests extracted
based on alignment and are not explicitly instructed
to discriminate between syntactically different tag
sequences. Our approach is not guaranteed to al-
ways assign similar feature vectors to syntactically
similar tag sequences. However, as the experimental
results show below, the latent categories are able to
capture some similarities among tag sequences that
are beneficial for translation.

Table 3 and 4 report the experimental results
on the English-to-German and English-to-Chinese
tasks, respectively. The addition of the syntax fea-
ture achieves a statistically significant improvement
(p ≤ 0.01) of 0.6 in BLEU on the test set of the

Baseline +Syntax ∆
Dev 16.26 17.06 0.80
Test 16.41 17.01 0.60

Table 3: BLEU scores of the English-to-German task
(one reference).

Baseline +Syntax ∆
Dev 46.47 47.39 0.92
Test 45.45 45.86 0.41

Table 4: BLEU scores of the English-to-Chinese task
(two references).

English-to-German task. This improvement is sub-
stantial given that only one reference is used for each
test sentence. On the English-to-Chinese task, the
syntax feature achieves a smaller improvement of
0.41 BLEU on the test set. One potential explanation
for the smaller improvement is that the sentences on
the English-to-Chinese task are much shorter, with
an average of only 6 words per sentence, compared
to 15 words in the English-to-German task. The
hypothesis space of translating a longer sentence is
much larger than that of a shorter sentence. There-
fore, there is more potential gain from using syn-
tax features to rule out unlikely derivations of longer
sentences, while phrasal rules might be adequate for
shorter sentences, leaving less room for syntax to
help as in the case of the English-to-Chinese task.

7 Discussions

The incorporation of the syntactic feature into the
hierarchical phrase-based translation system also
brings in additional memory load and computational
cost. In the worst case, our approach requires stor-
ing one feature vector for each tag sequence and one
feature vector for each nonterminal of a SCFG rule,
with the latter taking the majority of the extra mem-
ory storage. We observed that about 90% of the
X nonterminals in the rules only have one tag se-
quence, and thus the required memory space can be
significantly reduced by only storing a pointer to the
feature vector of the tag sequence for these nonter-
minals. Our approach also requires computing one
dot-product of two feature vectors for each nonter-
minal when a SCFG rule is applied to a source span.

145



Very similar Not so similar Very dissimilar
~F (ts) · ~F (ts′) > 0.9 0.4 ≤ ~F (ts) · ~F (ts′) ≤ 0.6 ~F (ts) · ~F (ts′) < 0.1

DT JJ NN
DT NN DT JJ JJ NML NN PP NP NN

DT JJ JJ NN DT JJ CC INTJ VB NN CD VP
DT ADJP NN DT NN NN JJ RB NP IN CD

VP
VB VP PP JJ NN JJ NN TO VP

VB RB VB PP VB NN NN VB JJ WHNP DT NN
VB DT DT NN VB RB IN JJ IN INTJ NP

ADJP
JJ ADJP JJ JJ CC ADJP IN NP JJ

PDT JJ ADJP VB JJ JJ AUX RB ADJP
RB JJ ADVP WHNP JJ ADJP VP

Table 2: Examples of similar and dissimilar tag sequences.

This cost can be reduced, however, by caching the
dot-products of the tag sequences that are frequently
accessed.

There are other successful investigations to
impose soft syntactic constraints to hierarchical
phrase-based models by either introducing syntax-
based rule features such as the prior derivation
model of Zhou et al. (2008) or by imposing con-
straints on translation spans at decoding time, e.g.,
(Marton and Resnik, 2008; Xiong et al., 2009;
Xiong et al., 2010). These approaches are all or-
thogonal to ours and it is expected that they can be
combined with our approach to achieve greater im-
provement.

This work is an initial effort to investigate latent
syntactic categories to enhance hierarchical phrase-
based translation models, and there are many direc-
tions to continue this line of research. First, while
the current approach imposes soft syntactic con-
straints between the parse structure of the source
sentence and the SCFG rules used to derive the
translation, the real-valued syntactic feature vectors
can also be used to impose soft constraints between
SCFG rules when rule rewrite occurs. In this case,
target side parse trees could also be used alone or to-
gether with the source side parse trees to induce the
latent syntactic categories. Second, instead of using
single parse trees during both training and decod-
ing, our approach is likely to benefit from exploring
parse forests as in (Mi and Huang, 2008). Third,
in addition to the treebank categories obtained by
syntactic parsing, lexical cues directly available in

sentence pairs could also to explored to guide the
learning of latent categories. Last but not the least,
it would be interesting to investigate discriminative
training approaches to learn latent categories that di-
rectly optimize on translation quality.

8 Conclusion

We have presented a novel approach to enhance
hierarchical phrase-based machine translation sys-
tems with real-valued linguistically motivated fea-
ture vectors. Our approach maintains the advan-
tages of hierarchical phrase-based translation sys-
tems while at the same time naturally incorpo-
rates soft syntactic constraints. Experimental results
showed that this approach improves the baseline hi-
erarchical phrase-based translation models on both
English-to-German and English-to-Chinese tasks.
We will continue this line of research and exploit
better ways to learn syntax and apply syntactic con-
straints to machine translation.

Acknowledgements

This work was done when the first author was visit-
ing IBM T. J. Watson Research Center as a research
intern. We would like to thank Mary Harper for
lots of insightful discussions and suggestions and the
anonymous reviewers for the helpful comments.

References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della

Pietra, and Robert L. Mercer. 1993. The mathemat-

146



ics of statistical machine translation: parameter esti-
mation. Computational Linguistics.

Binh Minh Bui-Xuan, Michel Habib, and Christophe
Paul. 2005. Revisiting T. Uno and M. Yagiura’s al-
gorithm. In ISAAC.

David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.

David Chiang. 2010. Learning to translate with source
and target syntax. In ACL.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2004. What’s in a translation rule. In
HLT/NAACL.

Steffen Heber and Jens Stoye. 2001. Finding all common
intervals of k permutations. In CPM.

Liang Huang and David Chiang. 2005. Better k-best
parsing. In International Workshop on Parsing Tech-
nology.

Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.

Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In CHSLP.

Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm part-
of-speech tagger by latent annotation and self-training.
In NAACL.

Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable. In
EMNLP.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In ACL.

Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor, 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.

Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In ACL.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.

Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In EMNLP.

Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In ACL.

Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL.

Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.

Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: soft-
ening syntactic constraints to improve statistical ma-
chine translation. In NAACL.

Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.

Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009.
A syntax-driven bracketing model for phrase-based
translation. In ACL-IJCNLP.

Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In NAACL-HLT.

Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL.

Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In COLING.

Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntactic
parsing and tree kernels. In SSST.

Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
StatMT.

147


